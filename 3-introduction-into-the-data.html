<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Machine Learning: Archaeology of a Data Practice</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="Machine Learning: Archaeology of a Data Practice">
  <meta name="generator" content="bookdown 0.3 and GitBook 2.6.7">

  <meta property="og:title" content="Machine Learning: Archaeology of a Data Practice" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Machine Learning: Archaeology of a Data Practice" />
  
  
  

<meta name="author" content="Adrian Mackenzie">


<meta name="date" content="2016-12-21">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="2-preface.html">
<link rel="next" href="4-diagramming-machines.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="1-acknowledgments.html"><a href="1-acknowledgments.html"><i class="fa fa-check"></i><b>1</b> Acknowledgments</a></li>
<li class="chapter" data-level="2" data-path="2-preface.html"><a href="2-preface.html"><i class="fa fa-check"></i><b>2</b> Preface</a></li>
<li class="chapter" data-level="3" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html"><i class="fa fa-check"></i><b>3</b> Introduction: Into the Data</a><ul>
<li class="chapter" data-level="3.1" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#three-accumulations-settings-data-and-devices"><i class="fa fa-check"></i><b>3.1</b> Three accumulations: settings, data and devices</a></li>
<li class="chapter" data-level="3.2" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#who-or-what-is-a-machine-learner"><i class="fa fa-check"></i><b>3.2</b> Who or what is a machine learner?</a></li>
<li class="chapter" data-level="3.3" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#algorithmic-control-to-the-machine-learners"><i class="fa fa-check"></i><b>3.3</b> Algorithmic control to the machine learners?</a></li>
<li class="chapter" data-level="3.4" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#the-archaeology-of-operations"><i class="fa fa-check"></i><b>3.4</b> The archaeology of operations</a></li>
<li class="chapter" data-level="3.5" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#asymmetries-in-common-knowledge"><i class="fa fa-check"></i><b>3.5</b> Asymmetries in common knowledge</a></li>
<li class="chapter" data-level="3.6" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#what-cannot-be-automated"><i class="fa fa-check"></i><b>3.6</b> What cannot be automated?</a></li>
<li class="chapter" data-level="3.7" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#different-fields-in-machine-learning"><i class="fa fa-check"></i><b>3.7</b> Different fields in machine learning?</a></li>
<li class="chapter" data-level="3.8" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#the-diagram-in-critical-thought"><i class="fa fa-check"></i><b>3.8</b> The diagram in critical thought</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html"><i class="fa fa-check"></i><b>4</b> Diagramming machines</a><ul>
<li class="chapter" data-level="4.1" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#we-dont-have-to-write-programs"><i class="fa fa-check"></i><b>4.1</b> ‘We don’t have to write programs’?</a></li>
<li class="chapter" data-level="4.2" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#the-elements-of-machine-learning"><i class="fa fa-check"></i><b>4.2</b> The elements of machine learning</a></li>
<li class="chapter" data-level="4.3" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#who-reads-machine-learning-textbooks"><i class="fa fa-check"></i><b>4.3</b> Who reads machine learning textbooks?</a></li>
<li class="chapter" data-level="4.4" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#r-a-matrix-of-transformations"><i class="fa fa-check"></i><b>4.4</b> <code>R</code>: a matrix of transformations</a></li>
<li class="chapter" data-level="4.5" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#the-obdurate-mathematical-glint-of-machine-learning"><i class="fa fa-check"></i><b>4.5</b> The obdurate mathematical glint of machine learning</a></li>
<li class="chapter" data-level="4.6" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#cs229-2007-returning-again-and-again-to-certain-features"><i class="fa fa-check"></i><b>4.6</b> CS229, 2007: returning again and again to certain features</a></li>
<li class="chapter" data-level="4.7" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#the-visible-learning-of-machine-learning"><i class="fa fa-check"></i><b>4.7</b> The visible learning of machine learning</a></li>
<li class="chapter" data-level="4.8" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#the-diagram-of-an-operational-formation"><i class="fa fa-check"></i><b>4.8</b> The diagram of an operational formation</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html"><i class="fa fa-check"></i><b>5</b> Vectorisation and its consequences</a><ul>
<li class="chapter" data-level="5.1" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html#vector-space-and-geometry"><i class="fa fa-check"></i><b>5.1</b> Vector space and geometry</a></li>
<li class="chapter" data-level="5.2" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html#mixing-places"><i class="fa fa-check"></i><b>5.2</b> Mixing places</a></li>
<li class="chapter" data-level="5.3" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html#truth-is-no-longer-in-the-table"><i class="fa fa-check"></i><b>5.3</b> Truth is no longer in the table?</a></li>
<li class="chapter" data-level="5.4" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html#the-epistopic-fault-line-in-tables"><i class="fa fa-check"></i><b>5.4</b> The epistopic fault line in tables</a></li>
<li class="chapter" data-level="5.5" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html#surface-and-depths-the-problem-of-volume-in-data"><i class="fa fa-check"></i><b>5.5</b> Surface and depths: the problem of volume in data</a></li>
<li class="chapter" data-level="5.6" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html#vector-space-expansion"><i class="fa fa-check"></i><b>5.6</b> Vector space expansion</a></li>
<li class="chapter" data-level="5.7" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html#drawing-lines-in-a-common-space-of-transformation"><i class="fa fa-check"></i><b>5.7</b> Drawing lines in a common space of transformation</a></li>
<li class="chapter" data-level="5.8" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html#implicit-vectorization-in-code-and-infrastructures"><i class="fa fa-check"></i><b>5.8</b> Implicit vectorization in code and infrastructures</a></li>
<li class="chapter" data-level="5.9" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html#lines-traversing-behind-the-light"><i class="fa fa-check"></i><b>5.9</b> Lines traversing behind the light</a></li>
<li class="chapter" data-level="5.10" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html#the-vectorised-table"><i class="fa fa-check"></i><b>5.10</b> The vectorised table?</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html"><i class="fa fa-check"></i><b>6</b> Machines finding functions</a><ul>
<li class="chapter" data-level="6.1" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html#learning-functions"><i class="fa fa-check"></i><b>6.1</b> Learning functions</a></li>
<li class="chapter" data-level="6.2" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html#supervised-unsupervised-reinforcement-learning-and-functions"><i class="fa fa-check"></i><b>6.2</b> Supervised, unsupervised, reinforcement learning and functions</a></li>
<li class="chapter" data-level="6.3" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html#which-function-operates"><i class="fa fa-check"></i><b>6.3</b> Which function operates?</a></li>
<li class="chapter" data-level="6.4" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html#what-does-a-function-learn"><i class="fa fa-check"></i><b>6.4</b> What does a function learn?</a></li>
<li class="chapter" data-level="6.5" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html#observing-with-curves-the-logistic-function"><i class="fa fa-check"></i><b>6.5</b> Observing with curves: the logistic function</a></li>
<li class="chapter" data-level="6.6" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html#the-cost-of-curves-in-machine-learning"><i class="fa fa-check"></i><b>6.6</b> The cost of curves in machine learning</a></li>
<li class="chapter" data-level="6.7" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html#curves-and-the-variation-in-models"><i class="fa fa-check"></i><b>6.7</b> Curves and the variation in models</a></li>
<li class="chapter" data-level="6.8" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html#observing-costs-losses-and-objectives-through-optimisation"><i class="fa fa-check"></i><b>6.8</b> Observing costs, losses and objectives through optimisation</a></li>
<li class="chapter" data-level="6.9" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html#gradients-as-partial-observers"><i class="fa fa-check"></i><b>6.9</b> Gradients as partial observers</a></li>
<li class="chapter" data-level="6.10" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html#the-power-to-learn"><i class="fa fa-check"></i><b>6.10</b> The power to learn</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="7-probabilisation-and-the-taming-of-machines.html"><a href="7-probabilisation-and-the-taming-of-machines.html"><i class="fa fa-check"></i><b>7</b> Probabilisation and the Taming of Machines}</a><ul>
<li class="chapter" data-level="7.1" data-path="7-probabilisation-and-the-taming-of-machines.html"><a href="7-probabilisation-and-the-taming-of-machines.html#data-reduces-uncertainty"><i class="fa fa-check"></i><b>7.1</b> Data reduces uncertainty?</a></li>
<li class="chapter" data-level="7.2" data-path="7-probabilisation-and-the-taming-of-machines.html"><a href="7-probabilisation-and-the-taming-of-machines.html#machine-learning-as-statistics-inside-out"><i class="fa fa-check"></i><b>7.2</b> Machine learning as statistics inside out</a></li>
<li class="chapter" data-level="7.3" data-path="7-probabilisation-and-the-taming-of-machines.html"><a href="7-probabilisation-and-the-taming-of-machines.html#distributed-probabilities"><i class="fa fa-check"></i><b>7.3</b> Distributed probabilities</a></li>
<li class="chapter" data-level="7.4" data-path="7-probabilisation-and-the-taming-of-machines.html"><a href="7-probabilisation-and-the-taming-of-machines.html#naive-bayes-and-the-distribution-of-probabilities"><i class="fa fa-check"></i><b>7.4</b> Naive Bayes and the distribution of probabilities</a></li>
<li class="chapter" data-level="7.5" data-path="7-probabilisation-and-the-taming-of-machines.html"><a href="7-probabilisation-and-the-taming-of-machines.html#spam-when-foralln-is-too-much"><i class="fa fa-check"></i><b>7.5</b> Spam: when <span class="math inline">\(\forall{N}\)</span> is too much?</a></li>
<li class="chapter" data-level="7.6" data-path="7-probabilisation-and-the-taming-of-machines.html"><a href="7-probabilisation-and-the-taming-of-machines.html#the-improbable-success-of-the-naive-bayes-classifier"><i class="fa fa-check"></i><b>7.6</b> The improbable success of the Naive Bayes classifier</a></li>
<li class="chapter" data-level="7.7" data-path="7-probabilisation-and-the-taming-of-machines.html"><a href="7-probabilisation-and-the-taming-of-machines.html#ancestral-probabilities-in-documents-inference-and-prediction"><i class="fa fa-check"></i><b>7.7</b> Ancestral probabilities in documents: inference and prediction</a></li>
<li class="chapter" data-level="7.8" data-path="7-probabilisation-and-the-taming-of-machines.html"><a href="7-probabilisation-and-the-taming-of-machines.html#statistical-decompositions-bias-variance-and-observed-errors"><i class="fa fa-check"></i><b>7.8</b> Statistical decompositions: bias, variance and observed errors</a></li>
<li class="chapter" data-level="7.9" data-path="7-probabilisation-and-the-taming-of-machines.html"><a href="7-probabilisation-and-the-taming-of-machines.html#does-machine-learning-construct-a-new-statistical-reality"><i class="fa fa-check"></i><b>7.9</b> Does machine learning construct a new statistical reality?</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="8-patterns-and-differences.html"><a href="8-patterns-and-differences.html"><i class="fa fa-check"></i><b>8</b> Patterns and differences</a><ul>
<li class="chapter" data-level="8.1" data-path="8-patterns-and-differences.html"><a href="8-patterns-and-differences.html#splitting-and-the-growth-of-trees"><i class="fa fa-check"></i><b>8.1</b> Splitting and the growth of trees</a></li>
<li class="chapter" data-level="8.2" data-path="8-patterns-and-differences.html"><a href="8-patterns-and-differences.html#differences-in-recursive-partitioning"><i class="fa fa-check"></i><b>8.2</b> 1984: Differences in recursive partitioning</a></li>
<li class="chapter" data-level="8.3" data-path="8-patterns-and-differences.html"><a href="8-patterns-and-differences.html#limiting-differences"><i class="fa fa-check"></i><b>8.3</b> Limiting differences</a></li>
<li class="chapter" data-level="8.4" data-path="8-patterns-and-differences.html"><a href="8-patterns-and-differences.html#the-successful-dispersion-of-the-support-vector-machine"><i class="fa fa-check"></i><b>8.4</b> The successful dispersion of the support vector machine</a></li>
<li class="chapter" data-level="8.5" data-path="8-patterns-and-differences.html"><a href="8-patterns-and-differences.html#differences-blur"><i class="fa fa-check"></i><b>8.5</b> Differences blur?</a></li>
<li class="chapter" data-level="8.6" data-path="8-patterns-and-differences.html"><a href="8-patterns-and-differences.html#bending-the-decision-boundary"><i class="fa fa-check"></i><b>8.6</b> Bending the decision boundary</a></li>
<li class="chapter" data-level="8.7" data-path="8-patterns-and-differences.html"><a href="8-patterns-and-differences.html#instituting-patterns"><i class="fa fa-check"></i><b>8.7</b> Instituting patterns</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="9-regularizing-and-materializing-objects.html"><a href="9-regularizing-and-materializing-objects.html"><i class="fa fa-check"></i><b>9</b> Regularizing and materializing objects}</a><ul>
<li class="chapter" data-level="9.1" data-path="9-regularizing-and-materializing-objects.html"><a href="9-regularizing-and-materializing-objects.html#genomic-referentiality-and-materiality"><i class="fa fa-check"></i><b>9.1</b> Genomic referentiality and materiality</a></li>
<li class="chapter" data-level="9.2" data-path="9-regularizing-and-materializing-objects.html"><a href="9-regularizing-and-materializing-objects.html#the-genome-as-threshold-object"><i class="fa fa-check"></i><b>9.2</b> The genome as threshold object</a></li>
<li class="chapter" data-level="9.3" data-path="9-regularizing-and-materializing-objects.html"><a href="9-regularizing-and-materializing-objects.html#genomic-knowledges-and-their-datasets"><i class="fa fa-check"></i><b>9.3</b> Genomic knowledges and their datasets</a></li>
<li class="chapter" data-level="9.4" data-path="9-regularizing-and-materializing-objects.html"><a href="9-regularizing-and-materializing-objects.html#the-advent-of-wide-dirty-and-mixed-data"><i class="fa fa-check"></i><b>9.4</b> The advent of ‘wide, dirty and mixed’ data</a></li>
<li class="chapter" data-level="9.5" data-path="9-regularizing-and-materializing-objects.html"><a href="9-regularizing-and-materializing-objects.html#cross-validating-machine-learning-in-genomics"><i class="fa fa-check"></i><b>9.5</b> Cross-validating machine learning in genomics</a></li>
<li class="chapter" data-level="9.6" data-path="9-regularizing-and-materializing-objects.html"><a href="9-regularizing-and-materializing-objects.html#proliferation-of-discoveries"><i class="fa fa-check"></i><b>9.6</b> Proliferation of discoveries</a></li>
<li class="chapter" data-level="9.7" data-path="9-regularizing-and-materializing-objects.html"><a href="9-regularizing-and-materializing-objects.html#variations-in-the-object-or-in-the-machine-learner"><i class="fa fa-check"></i><b>9.7</b> Variations in the object or in the machine learner?</a></li>
<li class="chapter" data-level="9.8" data-path="9-regularizing-and-materializing-objects.html"><a href="9-regularizing-and-materializing-objects.html#whole-genome-functions"><i class="fa fa-check"></i><b>9.8</b> Whole genome functions</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="10-propagating-subject-positions.html"><a href="10-propagating-subject-positions.html"><i class="fa fa-check"></i><b>10</b> Propagating subject positions}</a><ul>
<li class="chapter" data-level="10.1" data-path="10-propagating-subject-positions.html"><a href="10-propagating-subject-positions.html#propagation-across-human-machine-boundaries"><i class="fa fa-check"></i><b>10.1</b> Propagation across human-machine boundaries</a></li>
<li class="chapter" data-level="10.2" data-path="10-propagating-subject-positions.html"><a href="10-propagating-subject-positions.html#competitive-positioning"><i class="fa fa-check"></i><b>10.2</b> Competitive positioning</a></li>
<li class="chapter" data-level="10.3" data-path="10-propagating-subject-positions.html"><a href="10-propagating-subject-positions.html#a-privileged-machine-and-its-diagrammatic-forms"><i class="fa fa-check"></i><b>10.3</b> A privileged machine and its diagrammatic forms</a></li>
<li class="chapter" data-level="10.4" data-path="10-propagating-subject-positions.html"><a href="10-propagating-subject-positions.html#varying-subject-positions-in-code"><i class="fa fa-check"></i><b>10.4</b> Varying subject positions in code</a></li>
<li class="chapter" data-level="10.5" data-path="10-propagating-subject-positions.html"><a href="10-propagating-subject-positions.html#the-subjects-of-a-hidden-operation"><i class="fa fa-check"></i><b>10.5</b> The subjects of a hidden operation</a></li>
<li class="chapter" data-level="10.6" data-path="10-propagating-subject-positions.html"><a href="10-propagating-subject-positions.html#algorithms-that-propagate-errors"><i class="fa fa-check"></i><b>10.6</b> Algorithms that propagate errors</a></li>
<li class="chapter" data-level="10.7" data-path="10-propagating-subject-positions.html"><a href="10-propagating-subject-positions.html#competitions-as-examination"><i class="fa fa-check"></i><b>10.7</b> Competitions as examination</a></li>
<li class="chapter" data-level="10.8" data-path="10-propagating-subject-positions.html"><a href="10-propagating-subject-positions.html#superimposing-power-and-knowledge"><i class="fa fa-check"></i><b>10.8</b> Superimposing power and knowledge</a></li>
<li class="chapter" data-level="10.9" data-path="10-propagating-subject-positions.html"><a href="10-propagating-subject-positions.html#ranked-subject-positions"><i class="fa fa-check"></i><b>10.9</b> Ranked subject positions</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="11-conclusion-out-of-the-data.html"><a href="11-conclusion-out-of-the-data.html"><i class="fa fa-check"></i><b>11</b> Conclusion: Out of the Data}</a><ul>
<li class="chapter" data-level="11.1" data-path="11-conclusion-out-of-the-data.html"><a href="11-conclusion-out-of-the-data.html#machine-learners"><i class="fa fa-check"></i><b>11.1</b> 250,000 machine learners</a></li>
<li class="chapter" data-level="11.2" data-path="11-conclusion-out-of-the-data.html"><a href="11-conclusion-out-of-the-data.html#a-summary-of-the-argument"><i class="fa fa-check"></i><b>11.2</b> A summary of the argument</a></li>
<li class="chapter" data-level="11.3" data-path="11-conclusion-out-of-the-data.html"><a href="11-conclusion-out-of-the-data.html#in-situ-hybridization"><i class="fa fa-check"></i><b>11.3</b> In-situ hybridization</a></li>
<li class="chapter" data-level="11.4" data-path="11-conclusion-out-of-the-data.html"><a href="11-conclusion-out-of-the-data.html#critical-operational-practice"><i class="fa fa-check"></i><b>11.4</b> Critical operational practice?</a></li>
<li class="chapter" data-level="11.5" data-path="11-conclusion-out-of-the-data.html"><a href="11-conclusion-out-of-the-data.html#obstacles-to-the-work-of-freeing-machine-learning"><i class="fa fa-check"></i><b>11.5</b> Obstacles to the work of freeing machine learning</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="12-references.html"><a href="12-references.html"><i class="fa fa-check"></i><b>12</b> References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning: Archaeology of a Data Practice</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="introduction-into-the-data" class="section level1">
<h1><span class="header-section-number">3</span> Introduction: Into the Data</h1>
<p></p>
<blockquote>
<p><em>Definition</em>: A computer program is said to <strong>learn</strong> from experience <span class="math inline">\(E\)</span> with respect to some class of tasks <span class="math inline">\(T\)</span> and performance measure <span class="math inline">\(P\)</span>, if its performance at tasks in <span class="math inline">\(T\)</span>, improves with experience <span class="math inline">\(E\)</span> <span class="citation">(Mitchell <a href="#ref-Mitchell_1997">1997</a>, 2)</span>.  </p>
</blockquote>
<blockquote>
<p>In the past fifteen years, the growth in algorithmic modeling applications and methodology has been rapid. It has occurred largely outside statistics in a new community—often called machine learning—that is mostly young computer scientists. The advances, particularly over the last five years, have been startling <span class="citation">(Breiman <a href="#ref-Breiman_2001">2001</a><a href="#ref-Breiman_2001">b</a>, 200)</span> </p>
</blockquote>
<blockquote>
<p>The key question isn’t ‘How much will be automated?’ It’s how we’ll conceive of whatever <em>can’t</em> be automated at a given time. <span class="citation">(Lanier <a href="#ref-Lanier_2013">2013</a>, 77)</span></p>
</blockquote>
<p>A relatively new field of scientific-engineering devices said to ‘learn from experience’  has become operational in the last three decades. Known by various names – machine learning, pattern recognition, knowledge discovery, data mining – the field and its devices, which all take shape as computer programs or code,  seem to have quickly spread across scientific disciplines, business and commercial settings, industry, engineering, media, entertainment and government. Heavily dependent on computation, they are found in breast cancer research, in autonomous vehicles, in insurance risk modelling, in credit transaction processing, in computer gaming, in face and handwriting recognition systems, in astronomy, advanced prosthetics, ornithology, finance, surveillance (see the U.S. Government’s <code>SkyNet</code> for one example of a machine learning surveillance system <span class="citation">(Agency <a href="#ref-NationalSecurityAgency_2012">2012</a>)</span>) or robots( see a Google robotic arm farm learning to sort drawers of office equipment such as staplers, pens, erasers and paper clips <span class="citation">(Levine et al. <a href="#ref-Levine_2016">2016</a>)</span>. </p>
<p>Sometimes machine learning devices are understood as <em>scientific models</em>, and sometimes they are understood as <em>operational algorithms</em>. In very many scientific fields, publications mention or describe these techniques as part of their analysis of some experimental or observational data (as in the logistic regression classification models found in many biomedical papers).  They anchor the field of ‘data science’ <span class="citation">(Schutt and O’Neil <a href="#ref-Schutt_2013">2013</a>)</span>, as institutionalised in several hundred data science institutes scattered worldwide.  Not so recently, they also became mundane mechanisms deeply embedded in other systems or gadgets (as in the decision tree models used in some computer game consoles to recognise gestures, the neural networks used to recognise voice commands by search engine services such as <code>Google Search</code>  and <code>Apple Siri</code>  <span class="citation">(McMillan <a href="#ref-McMillan_2013">2013</a>)</span> or Google’s <code>TensorFlow</code> software packages that puts deep convolutional neural nets on Android devices <span class="citation">(Google <a href="#ref-Google_2015a">2015</a>)</span>). In platform settings, they operate behind the scenes as part of the everyday functioning of services ranging from player ranking in online games to border control face recognition, from credit scores to news feeds on Facebook .</p>
<p>In all of these settings, applications and fields, machine learning is said to transform the nature of knowledge. Might it transform the practice of critical thought? This book is an experiment in such practice. </p>
<div id="three-accumulations-settings-data-and-devices" class="section level2">
<h2><span class="header-section-number">3.1</span> Three accumulations: settings, data and devices</h2>
<p>Three different accumulations cross-stratify in machine learning: settings, data and devices. The volume and geography of searches on Google Search provides some evidence of the diverse settings or sites doing machine learning. If we search for terms such as <code>artificial intelligence, machine learning</code> and <code>data mining</code> on the <a href="http://www.google.com/trends">Google Trends</a> service , the results for the last decade or so suggest shifting interest in these topics.</p>
<div class="figure"><span id="fig:google-trends"></span>
<img src="_main_files/figure-html/google-trends-1.png" alt="Google Trends search volume for machine learning` and related query terms in English, globally 2004-2016" width="1152" />
<p class="caption">
Figure 3.1: Google Trends search volume for machine learning` and related query terms in English, globally 2004-2016
</p>
</div>
<p>In Figure <a href="3-introduction-into-the-data.html#fig:google-trends">3.1</a>, two general search terms that had a very high search volume in 2004 – ‘artificial intelligence’ and ‘data mining’   – slowly decline over the years before starting to increase again in the last few years. By contrast, <code>machine learning</code> loses volume until around 2008, and then gradually rises again so that by mid-2016 it exceeds the long-standing interests in data-mining and artificial intelligence. Whatever the difficulties in understanding GoogleTrends results, these curves suggest an accumulation of sites and settings turning towards machine learning.<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a>  What does it mean that machine learning surfaces in so many different places, from fMRIs to Facebook’s <code>AI-Flow</code> <span class="citation">(<span class="citeproc-not-found" data-reference-id="Facebook_2016"><strong>???</strong></span>)</span>, , from fisheries management to Al Queda courier network monitoring by <code>SkyNet</code>?<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a> </p>
<div class="figure"><span id="fig:cat"></span>
<img src="figure/cat_hog.png" alt="Cat as histogram of gradients:Close up of cat. The image on the left is already signal-processed as a JPEG format file. The image on the right is further processed using histogram of oriented gradients (HOG) edge detection.  extit{kittydar} models HOG features.  Cat photo courtesy: photos-public-domain.com" width="2400" />
<p class="caption">
Figure 3.2: Cat as histogram of gradients:Close up of cat. The image on the left is already signal-processed as a JPEG format file. The image on the right is further processed using histogram of oriented gradients (HOG) edge detection. extit{kittydar} models HOG features. Cat photo courtesy: photos-public-domain.com
</p>
</div>
<p>A second accumulation concerns the plenitude of things in the world as data.  If we wanted to describe the general horizon of machine learning as a data practice in its specificity, we might turn to cats.  Cat images accumulate on websites and social media platforms as de-centred, highly repetitive data forms. Like the billions of search engine queries, email messages, tweets, or much contemporary scientific data (e.g. the DNA sequence data discussed in chapter <a href="#ch:genome"><strong>??</strong></a>), images accumulate in archives of communication. Take the case of <code>kittydar,</code> a machine learner in the area of image recognition (see <a href="http://harthur.github.io/kittydar">kittydar</a>): ‘Kittydar is short for kitty radar. Kittydar takes an image (canvas) and tells you the locations of all the cats in the image’ <span class="citation">(Arthur <a href="#ref-Arthur_2012">2012</a>)</span>.  This playful piece of code demonstrates how machine learning can be amidst mundane accumulation. Heather Arthur, who developed <code>kittydar</code> writes: </p>
<blockquote>
<p>Kittydar first chops the image up into many “windows” to test for the presence of a cat head. For each window, kittydar first extracts more tractable data from the image’s data. Namely, it computes the Histogram of Orient Gradients descriptor of the image … This data describes the directions of the edges in the image (where the image changes from light to dark and vice versa) and what strength they are. This data is a vector of numbers that is then fed into a neural network … which gives a number from 0 to 1 on how likely the histogram data represents a cat. The neural network … has been pre-trained with thousands of photos of cat heads and their histograms, as well as thousands of non-cats. See the repo for the node training scripts <span class="citation">(Arthur <a href="#ref-Arthur_2012">2012</a>)</span>.  </p>
</blockquote>
<p>This toy device finds cat heads in digital photographs, but also exemplifies many key traits of machine learning. Large accumulations of things become  in a dataset.  A dataset is used to train a typical machine learning device, a neural net,  and the neural net <em>classifies</em> subsequent images probabilistically. The code for all this is ‘in the repo.’  Based on how it locates cats, we can begin to imagine similar pattern recognition techniques in use in self-driving cars <span class="citation">(Thrun et al. <a href="#ref-Thrun_2006">2006</a>)</span>, border control facial recognition systems, military robots or wherever something seen implies something to do.</p>
<p>Faced with the immense accumulation of cat images on the internet, <code>kittydar</code> can do very little. It only detects the presence of cats that face forward. And it sometimes classifies people as cats. As Arthur’s description suggests, the software finds cats by cutting the images into smaller windows. For each window, it measures a set of gradients – a spatial order of great significance in machine learning -  running from light and dark, and then compares these measurements to the gradients of known cat images (the so-called ‘training data’). The work of classification according to the simple categories of ‘cat’ or ‘not cat’  is given either to a neural network (as discussed in chapter <a href="#ch:subjects"><strong>??</strong></a>, a typical machine learning technique and one that has recently been heavily developed by researchers at Google <span class="citation">(Le et al. <a href="#ref-Le_2011">2011</a>)</span>, themselves working on images of cats among other things taken from Youtube videos <span class="citation">(BBC <a href="#ref-BBC_2012">2012</a>)</span>, or to a support vector machine (a technique first developed in the 1990s by researchers working at IBM; see chapter ). </p>
<p>A final accumulation comprises machine learning techniques and devices. Machine learning range from the mundane to the esoteric, from code miniatures such as <code>kittydar</code> to the infrastructural sublime of computational clouds and clusters twirling in internet data-streams. Like the images that <code>kittydar</code> classifies, the names of machine learning techniques and devices proliferate and accumulate in textbooks, instructional courses, website tutorials, software libraries and code listings: linear regression, logistic regression,  neural networks, linear discriminant analysis, support vector machines,  k-means clustering,  decision trees, <em>k</em> nearest neighbours,  random forests,  principal component analysis, or naive Bayes   to name just some of the most commonly used. Sometimes they have proper names: <code>RF-ACE</code>, <code>Le-Net5</code>, or <code>C4.5</code>. These names refer to predictive models and to computational algorithms of various ilk and provenance. Intricate data practices – normalization, regularization, cross-validation, feature engineering, feature selection, optimisation  – embroider datasets into shapes they can recognise. The techniques, algorithms and models are not necessarily startling new or novel. They take shape against a background of more than a century of work in mathematics, statistics, computer science as well as disparate scientific fields ranging from anthropology to zoology. Mathematical constructs drawn from linear algebra, differential calculus, numerical optimization and probability theory pervade practice in the field.  Machine learning itself is an accumulation rather than a radical transformation.</p>
</div>
<div id="who-or-what-is-a-machine-learner" class="section level2">
<h2><span class="header-section-number">3.2</span> Who or what is a machine learner?</h2>
<p>I am focusing on machine learners  – a term that refers to both humans and machines or human-machine relations  throughout this book – situated amidst these three accumulations of settings, data and devices. While it is not always possible to disentangle machine learners from the databases, infrastructures, platforms or interfaces they work through, I will argue that data practices associated with machine learning delimit a  of knowing.  The term ‘positivity’ comes from Michel Foucault’s <em>The Archaeology of Knowledge</em> <span class="citation">(Foucault <a href="#ref-Foucault_1972">1972</a>)</span>, and refers to specific forms of accumulation of  grouped in a discursive practice. Analyzed archaeologically, a positivity can be investigated and inhabited to some degree by critical thought.</p>
<p>Foucault attributes a lift-off effect to positivity:</p>
<blockquote>
<p>The moment at which a discursive practice achieves individuality and autonomy, the moment therefore at which a single system for the formation of statements is put into operation, or the moment at which this system is transformed, might be called the threshold of positivity. <span class="citation">(Foucault <a href="#ref-Foucault_1972">1972</a>, 186)</span></p>
</blockquote>
<p>Machine learners today circulate into domains that lie far afield of the eugenic and psychology laboratories, industrial research institutes or specialized engineering settings in which they first took shape (in some cases, such as the linear regression model or principal component analysis, more than a century ago; in others such as support vector machines or random forests, in the last two decades). If they are not exactly new and have diverse genealogies, the question is: what happen as machine learners shift from localized mathematical or engineering techniques to an everyday device that can be generalized to locate cats in digital images, the Higgs boson in particle physics experiments or fraudulent credit card transactions? Does the somewhat unruly generalization of machine learning  across different epistemic, economic, institutional settings – the pronounced uptick shown in Figure <a href="3-introduction-into-the-data.html#fig:google-trends">3.1</a> – attest to a re-definition of knowledge, decision and control, a new   in which ‘a system is transformed’?</p>
</div>
<div id="algorithmic-control-to-the-machine-learners" class="section level2">
<h2><span class="header-section-number">3.3</span> Algorithmic control to the machine learners?</h2>
<p>Written in code, machine learners operate as programs or computational processes to produce statements that may take the form of numbers, graphs, propositions (see for instance the propositions produced by a recurrent neural net on the text of this book in the concluding chapter <a href="#ch:conclusion"><strong>??</strong></a>). Machine learning can also be viewed as a change in how programs or the code that controls computer operations are developed and operate (see chapter  for more detailed discussion of this).  The term ‘learning’ in machine learning points to this change and many machine learners emphasize it. Pedro Domingos, for instance, a computer scientist at the University of Washington, writes:</p>
<blockquote>
<p>Learning algorithms – also known as learners – are algorithms that make other algorithms. With machine learning, computers write their own programs, so we don’t have to.<span class="citation">(Domingos <a href="#ref-Domingos_2015a">2015</a>, 6)</span></p>
</blockquote>
<p>Viewed from the perspective of control, and how control is practiced, digital computer programs stem from and epitomise the ‘control revolution’ <span class="citation">(Beniger <a href="#ref-Beniger_1986">1986</a>)</span> that arguably has, since the late nineteenth century, programmatically re-configured production, distribution, consumption, and bureaucracy by tabulating, calculating and increasingly communicating events and operations. With the growth of digital communication networks in the form of the internet, the late 20th century entered a new crisis of control, no longer centred on logistic acceleration but on communication and knowledge.  Almost all accounts of the operational power of machine learning emphasise its power to inflect the control of processes of communication – border flows, credit fraud, spam email, financial market prices, cancer diagnosis, targeted online adverts – processes whose unruly or transient multiplicity otherwise evades or overwhelm us – with knowledge (classifications and predictions in particular) derived from algorithms that make other algorithms. On this view, <code>kittydar</code> can isolate cats amidst the excessive accumulation of images on the internet because neural net learning algorithms (back-propagation, gradient descent) have written a program – ‘a pre-trained’ neural net – during its training phase. </p>
<p>If a newly programmatic field of knowledge-control takes shape around machine learning, how would we distinguish it from computation more generally? Recent critical research on algorithms offers one lead. In a study of border control systems, which often use machine learners to do profiling and facial recognition , Louise Amoore advocates attention to calculation and algorithms:</p>
<blockquote>
<p>Surely this must be a primary task for critical enquiry – to uncover and probe the moments that come together in the making of a calculation that will automate all future decisions. To be clear, I am not proposing some form of humanist project of proper ethical judgement, but rather calling for attention to be paid to the specific temporalities and norms of algorithmic techniques that rule out, render invisible, other potential futures <span class="citation">(Amoore <a href="#ref-Amoore_2011">2011</a>)</span>. </p>
</blockquote>
<p>As Amoore writes, some potential futures are being ‘ruled out’ as calculations automate decisions. Anna Munster  puts the challenge more bluntly: ‘prediction takes down potential’ <span class="citation">(Munster <a href="#ref-Munster_2013">2013</a>)</span>. I find much to agree with here. Machine learning is a convoluted but nevertheless concrete and historically specific form of calculation  (as we will see in exploring algebraic operations in chapter <a href="#ch:vector"><strong>??</strong></a>, in finding and optimising certain mathematical functions in chapter  or in characterising and shaping probability distributions in chapter ). It works to mediate future-oriented decisions (although all too often, very near-future decisions such as ad-click prediction).</p>
<p>I am less certain about treating machine learning as automation. Learning from data, as we will see, does often sidestep and substitute for existing ways of acting, and practices of control, and it thereby re-configures human-machine differences.  Yet the notion of automation does not capture well how this comes about.  The programs that machine learner ‘write’ are formulated as probabilistic models, as learned rules or association, and they generate predictive and classificatory statements (‘this is a cat’). They render calculable some things which hitherto appeared intractable to calculation (for instance, the argument of a legal case). Such calculation, with all the investment it attracts (in the form of professional lives, in the form of infrastructures , in reorganisation of institutions, corporations and governments, etc.) does rule out some and reinforce other futures.<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a> If this transformed calculability is automation, we need to understand the specific contemporary reality of automation as it takes shape in machine learning. We cannot conduct critical enquiry into the calculation that will automate future decisions without opening the very notions of calculation and automation into question. </p>
<p>Does the concept of algorithm help us identify the moments that come together in machine learning without resorting to a-historical concepts of automation or calculation? In various scholarly and political debates around changes business, media, education, health, government or science, quasi-omnipotent agency has been imputed to algorithms  <span class="citation">(Barocas, Hood, and Ziewitz <a href="#ref-Barocas_2013">2013</a>; Beer and Burrows <a href="#ref-Beer_2013">2013</a>; Cheney-Lippold <a href="#ref-Cheney-Lippold_2011">2011</a>; Fuller and Goffey <a href="#ref-Fuller_2012">2012</a>; Galloway <a href="#ref-Galloway_2004">2004</a>)</span>; <span class="citation">Gillespie (<a href="#ref-Gillespie_2014">2014</a>)</span>; <span class="citation">Neyland (<a href="#ref-Neyland_2015">2015</a>)</span>; <span class="citation">Pasquinelli (<a href="#ref-Pasquinelli_2014">2014</a>)</span>; <span class="citation">Smith (<a href="#ref-Smith_2013">2013</a>)</span>; <span class="citation">Totaro and Ninno (<a href="#ref-Totaro_2014">2014</a>)</span>; <span class="citation">Wilf (<a href="#ref-Wilf_2013">2013</a>)</span>] or sometimes just ‘the algorithm.’ This growing body of work understands the power of algorithms in the social science and humanities literature in different ways, sometimes in terms of rules, sometimes as functions or mathematical abstractions, and increasingly as a located practice. There is general agreement that algorithms are powerful, or at least, can bear down heavily on people’s lives and conduct, re-configuring, for instance, culture as algorithmic <span class="citation">(Hallinan and Striphas <a href="#ref-Hallinan_2014">2014</a>)</span>.</p>
<p>Some of the critical literature on algorithms identifies abstractions as the source of their power. For instance, in his discussion of the ‘metadata society,’ Paolo Pasquinelli proposes that</p>
<blockquote>
<p>a progressive political agenda for the present is about moving at the same level of abstraction as the algorithm in order to make the patterns of new social compositions and subjectivities emerge. We have to produce new revolutionary institutions out of data and algorithms. If the abnormal returns into politics as a mathematical object, it will have to find its strategy of resistance and organisation, in the upcoming century, in a mathematical way <span class="citation">(Pasquinelli <a href="#ref-Pasquinelli_2015">2015</a>)</span>. </p>
</blockquote>
<p>‘Moving at the same level of abstraction as the algorithm’ offers some purchase as a formulation for critical practice, and for experiments in such practice. Since in mathematics let alone critical thought, abstraction can be understood in many different ways, any direct identification of algorithms with abstraction will, however, be difficult to practice. Which algorithm, what kind of abstraction and which ‘mathematical way’ should we focus on? Like automation and calculation, abstraction and mathematics have mutable historicities.  We cannot ‘move at the same level’ without taking that into account. Furthermore, given the accumulations of settings, data and devices, there might not be any single level of abstraction to move at, only a torque and flux of different moments of abstraction at work in generalizing, classifying, circulating and stratifying in the midst of transient and plural multiplicities. </p>
</div>
<div id="the-archaeology-of-operations" class="section level2">
<h2><span class="header-section-number">3.4</span> The archaeology of operations</h2>
<p>Given mathematics and algorithms do loom large in machine learning, how do we address their the workings without pre-emptively ascribing potency to mathematics, or to algorithms? In the chapters that follow, I do explore specific learning algorithms (gradient descent in chapter <a href="#ch:function"><strong>??</strong></a> or recursive partitioning ) and mathematical techniques (the sigmoid function in chapter  or inner products in chapter ) in greater empirical and conceptual depth. Following much scholarship in science and technology studies, I maintain that attention to specificity of practices is an elementary prerequisite to understanding human-machine relations,  and their transformations. The  of operations that I will develop combines an interest in machine learning as a form of knowledge production and a strategy of power. Like Foucault, I see no exteriority between techniques of knowledge and strategies of power (‘between techniques of knowledge and strategies of power, there is no exteriority, even if they have specific roles and are linked together on the basis of their difference’ <span class="citation">(Foucault <a href="#ref-Foucault_1998">1998</a>, 1:98)</span>). </p>
<p>If we understand machine learning as a data practice that re-configures local centres of power-knowledge through a re-drawing of human-machine relations, then the specific roles and differences associated with machine learners in the production of knowledge should be a focus of attention. Differences are a key concern here since many machine learners classify things. They are often simply called ‘.’  Some of the practice of difference works in terms of categories. <code>Kittydar</code> classifies images as <code>cat</code> with some probability, but categorisation and classification in machine learning occurs much more widely.<a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a> We might understood the importance of categories sociologically. For instance, in his account of media power, Nick Couldry highlights the importance of categories and categorisation:</p>
<blockquote>
<p><em>Category</em> is a key mechanism whereby certain types of ordered (often ‘ritualized’) practice produce power by enacting and embodying categories that serve to mark and divide up the world in certain ways. Without <em>some</em> ordering feature of practice, such as ‘categories’, it is difficult to connect the multiplicity of practice to the workings of power, whether in the media or in any other sphere. By understanding the work of categories, we get a crucial insight into why the social world, in spite of its massive complexity still appears to us as a <em>common</em> world <span class="citation">(Couldry <a href="#ref-Couldry_2012">2012</a>, 62)</span>   </p>
</blockquote>
<p>Orderings of categorical differences undergo a great deal of intensification via machine learning. Categories are often simply an existing set of classifications derived from institutionalised or accepted knowledges (for instance, the categories of customers according to gender or age). Machine learners also generate new categorical workings or mechanisms of differentiation. As we will see (for instance in chapter <a href="#ch:genome"><strong>??</strong></a> in relation to scientific data from genomes), machine learners invent or find new sets of categories for a particular purpose (such as cancer diagnosis or prognosis). These differentiations may or may not bring social good. The person who finds themselves paying a higher price for an air ticket by virtue of some unknown combination of factors including age, credit score, home address, previous travel, or educational qualifications experiences something of the classificatory power.</p>
</div>
<div id="asymmetries-in-common-knowledge" class="section level2">
<h2><span class="header-section-number">3.5</span> Asymmetries in common knowledge</h2>
<p>What can critical thought, the kind of enquiry that seeks to identify the conditions that concretely constitute what anyone can say or think or do, learn from machine learning? If we see a massive regularization of order occurring in machine learning, what is at stake in trying to think through those practices? They display moments of formalisation (especially mathematical and statistical), circulation (pedagogically and operationally), generalization (propagating and proliferating in many domains and settings) and stratification (the socially, epistemically, economically and sometimes politically or ontologically loaded re-iterative enactment of categories). I am not sure that understanding how a support vector machine or a random forest orders differences would change how would we relate to what we see, feel, sense, hear or think in the face of a contemporary platform such as Amazon’s  that uses Association Rule Mining , an app, a passport control system that matches faces of arriving passengers with images in a database, a computer game, or a genetic test (all settings in which machine learning is likely to be operating).</p>
<p>Machine learners themselves sometimes complain of the monolithic and homogeneous success of machine learning. Some expert practitioners complain of a uniformity in its applications. Jeff Hammerbacher , previously chief research scientist at Facebook, co-founder of a successful data analytics company called Cloudera, and currently working also on cancer research at Mount Sinai hospital, complained about the spread of machine learning in 2011: ‘the best of my generation are thinking about how to make people click ads’ <span class="citation">(Vance <a href="#ref-Vance_2011">2011</a>)</span> . Leaving aside debates about the ranking of ‘best minds’ (a highly competitive and exhaustively tested set of subject positions; see chapter <a href="#ch:subjects"><strong>??</strong></a>), Hammerbacher was lamenting the flourishing use of predictive analytics techniques in online platforms such as Twitter, Google and Facebook, and on websites more generally, whether they be websites that sell things or advertising space. The mathematical skills of many PhDs from MIT, Stanford or Cambridge were wrangling data in the interests of micro-targeted advertising. As Hammerbacher observers, they were ‘thinking about how to make people click ads,’ and this ‘thinking’ mainly took and does take the form of building predictive models that tailored the ads shown on websites to clusters of individual preferences and desires.</p>
<p>Hammerbacher’s unhappiness with ad-click prediction resonates with critical responses to the use of machine learning in the digital humanities.  Some versions of the digital humanities make extensive use of machine learning. To cite one example, in <em>Macroanalysis: Digital Methods and Literary History</em>, Matthew Jockers describes how he relates to one currently popular machine learning or statistical modelling technique, the topic model  (itself the topic of discussion in Chapter <a href="#ch:probability"><strong>??</strong></a>; see also <span class="citation">(Mohr and Bogdanov <a href="#ref-Mohr_2013">2013</a>)</span>):</p>
<blockquote>
<p>If the statistics are rather too complex to summarize here, I think it is fair to skip the mathematics and focus on the end results. We needn’t know how long and hard Joyce sweated over <em>Ulysses</em> to appreciate his genius, and a clear understanding of the LDA machine is not required in order to see the beauty of the result. <span class="citation">(Jockers <a href="#ref-Jockers_2013">2013</a>, 124)</span> </p>
</blockquote>
<p>The widely used Latent Dirichlet Allocation or models provide a litmus test of how relations to machine learning is taking shape in the digital humanities. On the one hand, these models promise to make sense of large accumulations of documents (scientific publications, news, literature, online communications, etc.) in terms of underlying themes or latent ‘topics.’ As we will, large document collections have long attracted the interest of machine learners (see chapter <a href="#ch:probability"><strong>??</strong></a>).  On the other hand, Jockers signals the practical difficulties of relating to machine learning when he suggests that ‘it is fair to skip the mathematics’ for the sake of ‘the beauty of the result’. While some parts of the humanities and critical social research exhorts closer attention to algorithms and mathematical abstractions, other parts elides its complexity in the name of ‘the beauty of the results.’</p>
<p>Critical thought has not always endorsed the use of machine learning in digital humanities.  Alex Galloway makes two observations about the circulation of these methods in humanities scholarship. The first points to its marginal status in increasingly machine-learned media cultures:</p>
<blockquote>
<p>When using quantitative methodologies in the academy (spidering, sampling, surveying, parsing, and processing), one must compete broadly with the sorts of media enterprises at work in the contemporary technology sector. A cultural worker who deploys such methods is little more than a lesser Amazon or a lesser Equifax <span class="citation">(Galloway <a href="#ref-Galloway_2014">2014</a>, 110)</span></p>
</blockquote>
<p>Galloway highlights the asymmetry between humanities scholars and media enterprises or credit score agencies (Equifax). The ‘quantitative methodologies’ that he refers to as spidering, sampling, processing and so forth are more or less all epitomised in machine learning techniques (for instance, the Association Rule Mining techniques used by Amazon to recommend purchases, or perhaps the decision tree techniques  used by the credit-rating systems at Equifax and FICO <span class="citation">(Fico <a href="#ref-Fico_2015">2015</a>)</span>).  Galloway’s argument is that the infrastructural scale of these enterprises along with the sometime very large technical workforces they employ to continually develop new predictive techniques dwarfs any gain in efficacy that might accrue to humanities research in its recourse to such methods.</p>
<p>Galloway also observes that even if ‘cultural workers’ do manage to learn to machine learn, and become adept at re-purposing the techniques in the interests of analyzing culture rather than selling things or generating credit scores,they might actually reinforce power asymmetries and exacerbate the ethical and political challenges posed by machine learning:</p>
<blockquote>
<p>Is it appropriate to deploy positivistic techniques against those self-same positivistic techniques? In a former time, such criticism would not have been valid or even necessary. Marx was writing against a system that laid no specific claims to the apparatus of knowledge production itself—even if it was fueled by a persistent and pernicious form of ideological misrecognition. Yet, today the state of affairs is entirely reversed. The new spirit of capitalism is found in brainwork, self-measurement and self-fashioning, perpetual critique and innovation, data creation and extraction. In short, doing capitalist work and doing intellectual work—of any variety, bourgeois or progressive—are more aligned today than they have ever been <span class="citation">(Galloway <a href="#ref-Galloway_2014">2014</a>, 110)</span>.  </p>
</blockquote>
<p>This perhaps is a more serious charge concerning the nature of any knowledge produced by machine learning.  The ‘techniques’ of machine learning may or may not be positivist, and indeed, given the claims that machine learning transforms the production of knowledge, positivism may not be any more stable than other conceptual abstractions.  Hence, it might not be so strongly at odds with critical thought, even if remains complicit – ‘aligned’ – with capitalist work. Intellectual work of the kind associated with machine learning is definitely at the centre of many governmental, media, business and scientific fields of operation and increasingly they anchor the operations of these fields. Yet neither observation – asymmetries in scale, alignment with a ‘positivist’ capitalist knowledge economy – exhaust the potentials of machine learning, particularly if, as many people claim, it transforms the nature of knowledge production and hence ‘brainwork.’</p>
</div>
<div id="what-cannot-be-automated" class="section level2">
<h2><span class="header-section-number">3.6</span> What cannot be automated?</h2>
<p>Jaron Lanier’s question – how will we conceive at a given time what cannot be automated? – suggests an alternative angle of approach.   Like Galloway, I’m wary of certain deployments of machine learning, particularly the platform-based media empires and their efforts to capture sociality <span class="citation">(Gillespie <a href="#ref-Gillespie_2010">2010</a>; Van Dijck <a href="#ref-VanDijck_2012">2012</a>)</span>. Machine learners do seem to be ‘laying claim to the apparatus of knowledge production.’ Yet even amidst the jarring ephemera of targeted online advertising or the more elevated analytics of literary history, the transformations in knowledge and knowing do not automatically appropriate intellectual work to capitalist production. Empirical work to describe differences, negotiations, modifications and contestation of knowledge would be needed to show the unevenness, variability and deep contingency of that appropriation.  As I have already suggested, machine learning practice is not simply automating existing economic relations or even data practices. While Hammerbacher and Galloway are understandably pessimistic about the existential gratifications and critical efficacy of building targeted advertising systems or document classifiers, the ‘deployment’ of machine learning is not a finished process, but very much in train, constantly subject to revision, re-configuration and alteration.</p>
<p>Importantly, the familiar concerns of critical social thought to analyse differences, power, materiality, subject positions, agency, etc. somewhat overlap with the claims that machine learning produces knowledge of differences, of nature, cultural processes, communication and conduct. Unlike other objects of critical thought, machine learners (understood always as human-machine relations) are themselves closely interested in producing knowledge, albeit scientific, governmental or operational.  This coincidence of knowledge projects suggests the possibility of some different articulation, of modification of the practice of critical thought in its empirical and theoretical registers. The altered human-machine relations we see as machine learners might shift and be re-drawn through experiments in empiricism and theory.  </p>
<p>Where in the algorithms, calculations, abstractions and regularizing practices of machine learning would differences be re-drawn? Machine learning in journalism, in specific scientific fields, in the humanities, in social sciences, in art, media, government or civil society sometimes overflows the platform-based deployments and their trenchantly positivist usages. A fairly explicit awareness of the operation of machine learning-driven processes is taking shape in some quarters. And this awareness supports a situationally aware calculative knowledge-practice.</p>
<p>For instance, the campaign to re-elect Barack Obama as U.S. President in 2011-12 relied heavily on micro-targeting of voters in the lead up to the election polls <span class="citation">(Issenberg <a href="#ref-Issenberg_2012">2012</a>; Mackenzie et al. <a href="#ref-Mackenzie_2016a">2016</a>)</span>. In response to the data analytics-driven election campaign run by the US Democrats, data journalists at the non-profit news organisation <em>ProPublica</em> reverse engineered the machine learning models that the Obama re-election team used to target individual votes with campaign messages <span class="citation">(Larsen <a href="#ref-Larsen_2012">2012</a>)</span>.  They built their own machine learning model - the ‘Message Machine’ - using emails sent in by voters to explore the workings of the Obama campaign team’s micro-targeting models.  While the algorithmic complexity and data infrastructures used in the Message Machine hardly match those at the disposal of the Obama team, it combines natural language processing (NLP) techniques such as measures of document similarity and machine learning models such as decision trees to disaggregate and map the micro-targeting processes .</p>
<p>This reverse engineering work focused on the constitution of subject positions (the position of the ‘voter’) can be found in other quarters. In response to the personalised recommendations generated by streaming media service Netflix, journalists at <em>The Atlantic</em> working with Ian Bogost, a media theorist and programmer,  reverse engineered the algorithmic production of around 80,000 micro-genres of cinema used by Netflix.<span class="citation">(Madrigal <a href="#ref-Madrigal_2014">2014</a>)</span>  While Netflix’s system to categorise films relies on much manual classification and tagging with meta-data, the inordinate number of categories they use is typical of the classificatory regimes that are developing in machine learning-based settings.</p>
<p>Both cases explore the constitutive contemporary conditions of doing, saying, and thinking of subjects, not only to recognise how subject positions are assigned or made, but to grasp the possibility of change. While these cases may be exceptional achievements, and indeed highlight the dead weight of ad-tech application of machine learning, knowledge production more generally is not easily reducible to contemporary forms of capitalism labour.</p>
</div>
<div id="different-fields-in-machine-learning" class="section level2">
<h2><span class="header-section-number">3.7</span> Different fields in machine learning?</h2>

<p>The proliferation of scientific machine learners suggests that the generalization of machine learning cannot be reduced to personalized advertising or other highly extractive uses.  Table @ref(tab:difference_ml) presents a small sample of scientific literature at the intersection of ‘difference’ and machine learning. This sample, while no doubt dwarfed by the flood of computer science publications on recommendation systems, targeted advertising or handwriting recognition, is typical of the positivity or specific forms of accumulation associated with machine learners in science.  (I return to this topic in Chapter <a href="#ch:genome"><strong>??</strong></a> in discussing how the leveraging of scientific data via predictive models and classifiers deeply affects the fabric and composition of objects of scientific knowledge.) The longevity and plurality of experiments, variants, alternative techniques, implementations and understandings associated with machine learning makes it difficult to immediately reduce them to capitalist captures of knowledge production.</p>
<div class="figure"><span id="fig:scientific-lit"></span>
<img src="_main_files/figure-html/scientific-lit-1.png" alt="Machine learners in scientific literature: machine learners in scientific literature. The lines in the graph suggest something of the changing fortunes of machine learners over time. The publication data comes from Thomson Reuters     extit{Web of Science}. Separate searches were run for each machine learner. In these plots, as in the GoogleTrends data, the actual counts of publications have been normalised. In contrast to the GoogleTrend plots, these plots do not show the relative counts of the publications, only their distribution in time." width="1152" />
<p class="caption">
Figure 3.3: Machine learners in scientific literature: machine learners in scientific literature. The lines in the graph suggest something of the changing fortunes of machine learners over time. The publication data comes from Thomson Reuters extit{Web of Science}. Separate searches were run for each machine learner. In these plots, as in the GoogleTrends data, the actual counts of publications have been normalised. In contrast to the GoogleTrend plots, these plots do not show the relative counts of the publications, only their distribution in time.
</p>
</div>
<p>Similarly, if we attend to the flow of machine learning practices, devices and techniques in scientific fields, a diversification rather than a simple scaling-up to industrial-strength infrastructures begins to appear. Figure @ref( fig:scientific-lit ) derives from counts of scientific publications that mention particular machine learners such as <code>decision tree</code> or <code>Naive Bayes</code> in their title, their abstract or keywords. The curves, which are probability density plots,  suggest a time-varying distribution of statements and operations for different techniques. This crude plot outlines the duration and the ebbs and flows of work on specific techniques, platforms, knowledges and power relations. Like the Google Trends searches for <code>machine learning</code>, the lines shown in Figure @ref( fig:scientific-lit ) have been normalised in order to adjust for an overall increase in the volume of scientific publications over the last five decades. Unlike the Google Trends search patterns, the scientific literature displays polymorphous temporalities in which different techniques and operations diverge widely from each other over the last half century. Crucially for my purposes, machine learning in the sciences constitutes an a-totality, a heterogeneous volume and de-centred production of statements.  </p>
</div>
<div id="the-diagram-in-critical-thought" class="section level2">
<h2><span class="header-section-number">3.8</span> The diagram in critical thought</h2>
<p>As an experiment in the practice of critical thought amidst the accumulations of data, settings and devices, this book attempts to  the data practices of machine learning in respect to knowledge production. Despite their many operational deployments, the coming together of algorithm, calculation and technique in machine learning is not fully coherent or complete. In order to qualify or specify how machine learners exist in their generality, we would need to specify their operations at a level of abstraction that neither attributes a mathematical or algorithmic essence to them nor frames them as means of production of relative surplus value. Finding ways of accommodating their diversity, loose couplings and mutability would mean grasping their operational power and their potential to create new forms of difference.<a href="#fn6" class="footnoteRef" id="fnref6"><sup>6</sup></a></p>
<p>Diagrams – a form of drawing that smooths away many frictions and variations –  practically abstract. (Gilles Deleuze, in his account of Michel Foucault’s philosophy presents diagrams as centres of power-knowledge: ‘What is a diagram? It is a display of the relations between forces which constitute power in the above conditions … the diagram or abstract machine is the map of relations between forces, a map of destiny, or intensity’ <span class="citation">(Deleuze <a href="#ref-Deleuze_1988">1988</a><a href="#ref-Deleuze_1988">b</a>, 36)</span>). Diagrams retain a connection to forms of doing, such as ‘learning from experience,’ that idea-centric accounts of abstraction sometimes struggle with. Perceptually and operationally, they span and indeed criss-cross between human and machine machine learners. As we will see, they form an axis of human-machine relations in machine learning, and a significant site of invention and emergence. They accommodate compositional substitutions, variations and superimpositions, as well as a play or movement amongst their often heterogeneous elements. Occasionally, perhaps rarely (Foucault as we will see characterises statements by their rarity amidst accumulation), by virtue of its composition, diagrams bring something new into the world.</p>
<p>Both the commonality and specificity (indeed the specific commonality) of machine learners would be hard to grasp without being able to trace their diagrammatic composition. Similarly, in order to understand the operational formation  associated with machine learning, the connections between data structures, infrastructures, processors, databases and lives need to be mapped. One path to follow in doing this - certainly not the one for everyone – is to inhabit and recognise oneself as a machine learner by occupying associated subject positions (the programmer or software developer, the statistics or computer science student, the modeller, the researcher, the data scientist, etc.) In moving between some of these subject positions, the densely operational indexes of mathematical formalisms begin to unravel.</p>
<p>Diagrams can be drawn in multiple ways, using various materials and inscriptive practices. Perhaps naively interpellated by the claim of machine learning to know differently, I use code and software implementations, graphical plots and mathematical expressions absorbed or copied from textbooks, blogs, online videos and the heavy accumulation of scientific publications from many disciplines (for instance, as seen in figure @ref(fig:scientific_lit)), together with the theoretical resources of a media-focused  of knowledge and a science studies-informed ethnographic sensibility towards always situated and configured infrastructural and calculative practices.</p>
<p>From my own learning to machine learn,  I draw six major machine learning operations diagrammatically: vectorisation, optimisation, probabilisation, pattern recognition, regularization and propagation. These generic operations intersect in a diagram of machine learning spanning hardware and software architectures, organizations of data and datasets, practices of designing and testing models, intersections between scientific and engineering disciplines, and professional and popular pedagogies. With varying degrees of formalization and consistency, these operations might also occasion or provoke some creative, resistive or re-distributive moves for critical thought in relation to differences, materiality, experience, agency or power.  They are the topics of chapters <a href="#ch:vector"><strong>??</strong></a>-. A summary of the operations can also be found near the beginning of the concluding chapter.</p>
<p>My somewhat risky a-critical immersion in technical practice seeks to support an alternative account of machine learning, an account in which some feeling of agency movement can take root. Mundane technical practices, sometimes at a quite low level (for instance, vectorisation) and other times at a high level of formalization (for instance, in discussing mathematical functions) are elements to be drawn – sometimes literally, sometimes operationally – on a diagram. The  of the operational formation of machine learning does not unearth the footprint of a strategic monolith, but highlights the local relations of force that feed into the generalization and plurality of the field in both its monumental accumulations and peripheral variations.</p>

</div>
</div>
<h3> References</h3>
<div id="refs" class="references">
<div id="ref-Mitchell_1997">
<p>Mitchell, Tom M. 1997. <em>Machine Learning</em>. New York, NY [u.a.: McGraw-Hill.</p>
</div>
<div id="ref-Breiman_2001">
<p>Breiman, Leo. 2001b. “Statistical Modeling: The Two Cultures (with Comments and a Rejoinder by the Author).” <em>Statistical Science</em> 16 (3): 199–231. <a href="http://projecteuclid.org/euclid.ss/1009213726" class="uri">http://projecteuclid.org/euclid.ss/1009213726</a>.</p>
</div>
<div id="ref-Lanier_2013">
<p>Lanier, Jaron. 2013. <em>Who Owns the Future?</em> London: Allen Lane.</p>
</div>
<div id="ref-NationalSecurityAgency_2012">
<p>Agency, National Security. 2012. “SKYNET: Courier Detection via Machine Learning.” <em>The Intercept</em>. <a href="https://theintercept.com/document/2015/05/08/skynet-courier/" class="uri">https://theintercept.com/document/2015/05/08/skynet-courier/</a>.</p>
</div>
<div id="ref-Levine_2016">
<p>Levine, Sergey, Peter Pastor, Alex Krizhevsky, and Deirdre Quillen. 2016. “Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection,” March. <a href="http://arxiv.org/abs/1603.02199" class="uri">http://arxiv.org/abs/1603.02199</a>.</p>
</div>
<div id="ref-Schutt_2013">
<p>Schutt, Rachel, and Cathy O’Neil. 2013. <em>Doing Data Science</em>. Sebastopol, Calif.: O’Reilly &amp; Associates Inc.</p>
</div>
<div id="ref-McMillan_2013">
<p>McMillan, Robert. 2013. “How Google Retooled Android With Help From Your Brain.” <em>WIRED</em>. February 18. <a href="http://www.wired.com/2013/02/android-neural-network/" class="uri">http://www.wired.com/2013/02/android-neural-network/</a>.</p>
</div>
<div id="ref-Google_2015a">
<p>Google. 2015. “TensorFlow – an Open Source Software Library for Machine Intelligence.” <a href="https://www.tensorflow.org/" class="uri">https://www.tensorflow.org/</a>.</p>
</div>
<div id="ref-Arthur_2012">
<p>Arthur, Heather. 2012. “Harthur/Kittydar.” <em>GitHub</em>. <a href="https://github.com/harthur/kittydar" class="uri">https://github.com/harthur/kittydar</a>.</p>
</div>
<div id="ref-Thrun_2006">
<p>Thrun, Sebastian, Mike Montemerlo, Hendrik Dahlkamp, David Stavens, Andrei Aron, James Diebel, Philip Fong, John Gale, Morgan Halpenny, and Gabriel Hoffmann. 2006. “Stanley: The Robot That Won the DARPA Grand Challenge.” <em>Journal of Field Robotics</em> 23 (9): 661–92. <a href="http://onlinelibrary.wiley.com/doi/10.1002/rob.20147/abstract" class="uri">http://onlinelibrary.wiley.com/doi/10.1002/rob.20147/abstract</a>.</p>
</div>
<div id="ref-Le_2011">
<p>Le, Quoc V., Marc’Aurelio Ranzato, Rajat Monga, Matthieu Devin, Kai Chen, Greg S. Corrado, Jeff Dean, and Andrew Y. Ng. 2011. “Building High-Level Features Using Large Scale Unsupervised Learning,” December. <a href="http://arxiv.org/abs/1112.6209" class="uri">http://arxiv.org/abs/1112.6209</a>.</p>
</div>
<div id="ref-BBC_2012">
<p>BBC. 2012. “Google ’Brain’ Machine Spots Cats.” <em>BBC News: Technology</em>, June 26. <a href="http://www.bbc.co.uk/news/technology-18595351" class="uri">http://www.bbc.co.uk/news/technology-18595351</a>.</p>
</div>
<div id="ref-Foucault_1972">
<p>Foucault, Michel. 1972. <em>The Archaeology of Knowledge and the Discourse on Language</em>. Translated by Allan Sheridan-Smith. New York: Pantheon Books.</p>
</div>
<div id="ref-Domingos_2015a">
<p>Domingos, Pedro. 2015. <em>The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World</em>. New York: Basic Civitas Books.</p>
</div>
<div id="ref-Beniger_1986">
<p>Beniger, James R. 1986. <em>The Control Revolution: Technological and Economic Origins of the Information Society</em>. Harvard University Press.</p>
</div>
<div id="ref-Amoore_2011">
<p>Amoore, Louise. 2011. “Data Derivatives On the Emergence of a Security Risk Calculus for Our Times.” <em>Theory, Culture &amp; Society</em> 28 (6): 24–43. <a href="http://tcs.sagepub.com/content/28/6/24.short" class="uri">http://tcs.sagepub.com/content/28/6/24.short</a>.</p>
</div>
<div id="ref-Munster_2013">
<p>Munster, Anna. 2013. <em>An Aesthesia of Networks: Conjunctive Experience in Art and Technology</em>. MIT Press. <a href="http://mitpress.mit.edu/books/aesthesia-networks/" class="uri">http://mitpress.mit.edu/books/aesthesia-networks/</a>.</p>
</div>
<div id="ref-Barocas_2013">
<p>Barocas, Solon, Sophie Hood, and Malte Ziewitz. 2013. “Governing Algorithms: A Provocation Piece.” SSRN Scholarly Paper ID 2245322. Rochester, NY: Social Science Research Network. <a href="http://papers.ssrn.com/abstract=2245322" class="uri">http://papers.ssrn.com/abstract=2245322</a>.</p>
</div>
<div id="ref-Beer_2013">
<p>Beer, David, and Roger Burrows. 2013. “Popular Culture, Digital Archives and the New Social Life of Data.” <em>Theory, Culture &amp; Society</em>. <a href="http://tcs.sagepub.com/content/early/2013/04/15/0263276413476542.abstract" class="uri">http://tcs.sagepub.com/content/early/2013/04/15/0263276413476542.abstract</a>.</p>
</div>
<div id="ref-Cheney-Lippold_2011">
<p>Cheney-Lippold, John. 2011. “A New Algorithmic Identity Soft Biopolitics and the Modulation of Control.” <em>Theory, Culture &amp; Society</em> 28 (6): 164–81. <a href="http://tcs.sagepub.com/content/28/6/164.short" class="uri">http://tcs.sagepub.com/content/28/6/164.short</a>.</p>
</div>
<div id="ref-Fuller_2012">
<p>Fuller, Matthew, and Andrew Goffey. 2012. <em>Evil Media</em>. Cambridge, Mass: MIT Press.</p>
</div>
<div id="ref-Galloway_2004">
<p>Galloway, Alexander R. 2004. <em>Protocol: How Control Exists After Decentralization</em>. Leonardo (Series) (Cambridge, Mass.). Cambridge, Mass.: MIT Press.</p>
</div>
<div id="ref-Gillespie_2014">
<p>Gillespie, Tarleton. 2014. “The Relevance of Algorithms.” In <em>Media Technologies: Essays on Communication, Materiality, and Society</em>, edited by Tarleton Gillespie, Pablo Boczkowski, and Kirsten A. Foot, 167–94. Cambridge, MA: MIT Press. <a href="https://books.google.co.uk/books?hl=en&amp;lr=&amp;id=zeK2AgAAQBAJ&amp;oi=fnd&amp;pg=PA167&amp;dq=gillespie+tarleton&amp;ots=GmjJQZ-2ue&amp;sig=Omo2OeadgqNvz17IupCqELPagdg" class="uri">https://books.google.co.uk/books?hl=en&amp;lr=&amp;id=zeK2AgAAQBAJ&amp;oi=fnd&amp;pg=PA167&amp;dq=gillespie+tarleton&amp;ots=GmjJQZ-2ue&amp;sig=Omo2OeadgqNvz17IupCqELPagdg</a>.</p>
</div>
<div id="ref-Neyland_2015">
<p>Neyland, Daniel. 2015. “On Organizing Algorithms.” <em>Theory, Culture &amp; Society</em> 32 (1): 119–32. doi:<a href="https://doi.org/10.1177/0263276414530477">10.1177/0263276414530477</a>.</p>
</div>
<div id="ref-Pasquinelli_2014">
<p>Pasquinelli, Matteo. 2014. “Italian Operaismo and the Information Machine.” <em>Theory, Culture &amp; Society</em>, February, 1–20. doi:<a href="https://doi.org/10.1177/0263276413514117">10.1177/0263276413514117</a>.</p>
</div>
<div id="ref-Smith_2013">
<p>Smith, Marquard. 2013. “Theses on the Philosophy of History: The Work of Research in the Age of Digital Searchability and Distributability.” <em>Journal of Visual Culture</em> 12 (3): 375–403. doi:<a href="https://doi.org/10.1177/1470412913507505">10.1177/1470412913507505</a>.</p>
</div>
<div id="ref-Totaro_2014">
<p>Totaro, Paolo, and Domenico Ninno. 2014. “The Concept of Algorithm as an Interpretative Key of Modern Rationality.” <em>Theory, Culture &amp; Society</em>, 29–49. <a href="http://tcs.sagepub.com/content/early/2014/03/17/0263276413510051.abstract" class="uri">http://tcs.sagepub.com/content/early/2014/03/17/0263276413510051.abstract</a>.</p>
</div>
<div id="ref-Wilf_2013">
<p>Wilf, Eitan. 2013. “Toward an Anthropology of Computer-Mediated, Algorithmic Forms of Sociality.” <em>Current Anthropology</em> 54 (6): 716–39. doi:<a href="https://doi.org/10.1086/673321">10.1086/673321</a>.</p>
</div>
<div id="ref-Hallinan_2014">
<p>Hallinan, Blake, and Ted Striphas. 2014. “Recommended for You: The Netflix Prize and the Production of Algorithmic Culture.” <em>New Media &amp; Society</em>, June, 1–21. doi:<a href="https://doi.org/10.1177/1461444814538646">10.1177/1461444814538646</a>.</p>
</div>
<div id="ref-Pasquinelli_2015">
<p>Pasquinelli, Matteo. 2015. “Anomaly Detection: The Mathematization of the Abnormal in the Metadata Society.” In. Berlin. <a href="https://www.academia.edu/10369819/Anomaly_Detection_The_Mathematization_of_the_Abnormal_in_the_Metadata_Society" class="uri">https://www.academia.edu/10369819/Anomaly_Detection_The_Mathematization_of_the_Abnormal_in_the_Metadata_Society</a>.</p>
</div>
<div id="ref-Foucault_1998">
<p>Foucault, Michel. 1998. <em>The Will to Knowledge: The History of Sexuality</em>. Translated by Robert Hurley. Vol. 1. London: Penguin. <a href="http://philpapers.org/rec/FOUTWT" class="uri">http://philpapers.org/rec/FOUTWT</a>.</p>
</div>
<div id="ref-Couldry_2012">
<p>Couldry, Nick. 2012. <em>Media, Society, World: Social Theory and Digital Media Practice</em>. Cambridge ; Malden, MA: Polity.</p>
</div>
<div id="ref-Vance_2011">
<p>Vance, Ashlee. 2011. “This Tech Bubble Is Different.” <em>BusinessWeek: Magazine</em>, April 14. <a href="http://www.businessweek.com/magazine/content/11_17/b4225060960537.htm" class="uri">http://www.businessweek.com/magazine/content/11_17/b4225060960537.htm</a>.</p>
</div>
<div id="ref-Mohr_2013">
<p>Mohr, John W., and Petko Bogdanov. 2013. “Introduction Models: What They Are and Why They Matter.” <em>Poetics</em> 41 (6): 545–69. doi:<a href="https://doi.org/10.1016/j.poetic.2013.10.001">10.1016/j.poetic.2013.10.001</a>.</p>
</div>
<div id="ref-Jockers_2013">
<p>Jockers, Matthew L. 2013. <em>Macroanalysis: Digital Methods and Literary History</em>. Urbana: University of Illinois Press.</p>
</div>
<div id="ref-Galloway_2014">
<p>Galloway, Alexander. 2014. “The Cybernetic Hypothesis.” <em>Differences</em> 25 (1): 107–31. doi:<a href="https://doi.org/10.1215/10407391-2420021">10.1215/10407391-2420021</a>.</p>
</div>
<div id="ref-Fico_2015">
<p>Fico. 2015. “FICO | FICO.” <em>FICO | FICO Decisions</em>. <a href="http://www.fico.com/en/products/fico-analytic-modeler-decision-tree-professional" class="uri">http://www.fico.com/en/products/fico-analytic-modeler-decision-tree-professional</a>.</p>
</div>
<div id="ref-Gillespie_2010">
<p>Gillespie, Tarleton. 2010. “The Politics of ’Platforms’.” <em>New Media &amp; Society</em> 12 (3): 347–64. doi:<a href="https://doi.org/10.1177/1461444809342738">10.1177/1461444809342738</a>.</p>
</div>
<div id="ref-VanDijck_2012">
<p>Van Dijck, José. 2012. “Facebook and the Engineering of Connectivity: A Multi-Layered Approach to Social Media Platforms.” <em>Convergence: The International Journal of Research into New Media Technologies</em>, 1354856512457548. <a href="http://con.sagepub.com/content/early/2012/09/17/1354856512457548.abstract" class="uri">http://con.sagepub.com/content/early/2012/09/17/1354856512457548.abstract</a>.</p>
</div>
<div id="ref-Issenberg_2012">
<p>Issenberg, Sasha. 2012. “The Definitive Story of How President Obama Mined Voter Data to Win A Second Term | MIT Technology Review.” <em>MIT Technology Review</em>. <a href="http://www.technologyreview.com/featuredstory/509026/how-obamas-team-used-big-data-to-rally-voters/" class="uri">http://www.technologyreview.com/featuredstory/509026/how-obamas-team-used-big-data-to-rally-voters/</a>.</p>
</div>
<div id="ref-Mackenzie_2016a">
<p>Mackenzie, Adrian, Matthew Fuller, Andrew Goffey, Richard Mills, and Stuart Sharples. 2016. “Code Repositories as Expressions of Urban Life.” In <em>Code and the City</em>, edited by Rob Kitchin. London: Routledge.</p>
</div>
<div id="ref-Larsen_2012">
<p>Larsen, Jeff. 2012. “How ProPublica’s Message Machine Reverse Engineers Political Microtargeting.” <em>ProPublica</em>. <a href="http://www.propublica.org/nerds/item/how-propublicas-message-machine-reverse-engineers-political-microtargeting" class="uri">http://www.propublica.org/nerds/item/how-propublicas-message-machine-reverse-engineers-political-microtargeting</a>.</p>
</div>
<div id="ref-Madrigal_2014">
<p>Madrigal, Alexis C. 2014. “How Netflix Reverse Engineered Hollywood.” <em>The Atlantic</em>. <a href="http://www.theatlantic.com/technology/archive/2014/01/how-netflix-reverse-engineered-hollywood/282679/" class="uri">http://www.theatlantic.com/technology/archive/2014/01/how-netflix-reverse-engineered-hollywood/282679/</a>.</p>
</div>
<div id="ref-Deleuze_1988">
<p>Deleuze, Gilles. 1988b. <em>Foucault</em>. Translated by Seân Hand. Minneapolis: University of Minnesota Press.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="2">
<li id="fn2"><p>In the plot (Figure @ref(fig:google_trends), the weekly variations in search volume on Google give rise to many spikes in the data. These spikes can be linked to specific events such as significant press releases, public debates, media attention and film releases. It is hard to know who is doing these searches. The data provided by Google Trends includes geography, and it would be interesting to compare the geographies of interest in the different terms over time.<a href="3-introduction-into-the-data.html#fnref2">↩</a></p></li>
<li id="fn3"><p>The diagram shown in Figure @ref(fig:google_trends) actually draws two lines for each trend. The ‘raw’ weekly GoogleTrends data – definitely not raw data, as it has been normalized to a percentage <span class="citation">(Gitelman <a href="#ref-Gitelman_2013">2013</a>)</span> – appears in the very spiky lines, but a much smoother line shows the general trend. This smoothing line is the work of a statistical model – a local regression or loess model <span class="citation">(Cleveland, Grosse, and Shyu <a href="#ref-Cleveland_1992">1992</a>)</span>  developed in the late 1970s.  The line depends on intensive computation and models (linear regression, <em>k</em> nearest neighbours,  ). The smoother lines make the spiky weekly search counts supplied by Google much easier to see. They construct alignments in the data by replacing the irregular variations with a curve that unequivocally runs through time with greater regularity. The smoothed lines shade the diagram with a predictive pattern. The lineaments of machine learning already appear in such lines. How have things been arranged so that smooth lines run through an accumulated archive of search data?<a href="3-introduction-into-the-data.html#fnref3">↩</a></p></li>
<li id="fn4"><p>As for consequences, we need only consider some of the many forms of work that have already been affected by or soon could be affected by machine learning. Postal service clerks no longer sort the mail because neural net-based handwriting recognition reads addresses on envelopes . Locomotives, cars and trucks are already driven by machine learners, and soon driving may not be same occupational cultural it was. Hundreds of occupational categories have to some degree or other machine learners in their near future. Carl Benedikt Frey and Michael Osborne model the chances of occupational change for 700 occupations using, aptly enough, the machine learning technique of Gaussian Processes <span class="citation">(Frey and Osborne <a href="#ref-Frey_2013">2013</a>)</span>.<a href="3-introduction-into-the-data.html#fnref4">↩</a></p></li>
<li id="fn5"><p>John Cheney-Lippold offers a quite general overview of categorization work. He writes: ‘algorithm ultimately exercises control over us by harnessing these forces through the creation of relationships between real-world surveillance data and machines capable of making statistically relevant inferences about what that data can mean’ <span class="citation">(Cheney-Lippold <a href="#ref-Cheney-Lippold_2011">2011</a>, 178)</span>. . Much of my discussion here seeks to explore the space of ‘statistical inference of what that data can mean’ as an operational field of knowledge production.<a href="3-introduction-into-the-data.html#fnref5">↩</a></p></li>
<li id="fn6"><p>Certain strands of social and cultural theory have taken a strong interest in algorithmic processes as operational forms of power. For instance, the sociologist Scott Lash distinguishes the operational rules found in algorithms from the regulative and constitutive rules studied by many social scientists:</p><blockquote><p>in a society of pervasive media and ubiquitous coding, at stake is a third type of rule, algorithmic, generative rules. ‘Generative’ rules are, as it were, virtuals that generate a whole variety of actuals. They are compressed and hidden and we do not encounter them in the way that we encounter constitutive and regulative rules. Yet this third type of generative rules is more and more pervasive in our social and cultural life of the post-hegemonic order. They do not merely open up opportunity for invention, however. They are also pathways through which capitalist power works, in, for example, biotechnology companies and software giants more generally <span class="citation">(Lash <a href="#ref-Lash_2007a">2007</a>, 71)</span>.  </p></blockquote><p>The term ‘generative’ is somewhat resonant in the field of machine learning as generative models, models that treat modelling as a problem of specifying the operations or dynamics that could have given rise to the observed data, are extremely important. <a href="3-introduction-into-the-data.html#fnref6">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="2-preface.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="4-diagramming-machines.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
