
# 6. The biological testing of machine learning 

> Because of a gradient that no doubt characterizes our cultures, discursive formations are constantly becoming epistemologized [@Foucault_1972, 195]

In the opening pages of _Elements of Statistical Learning_, a number of vignettes appear. They include document classification, image recognition, risk of heart attack, stock price prediction and risk factors of prostate cancer, and glucose estimates for diabetics. Four examples are discussed in more detail: spam email classification, handwritten digit recognition, prostate cancer risk and lastly DNA Expression Microarrays [@Hastie_2009, 23]. The latter vignette attracts a whole page colour figure -- a heatmap -- of microarray data [@Hastie_2009, 24].  DNA, genes, genomics and proteomics then more or less disappear from view for 500 hundred pages of the book (aside from a brief mention in the context of cross-validation), only to reappear somewhat suddenly in a discussion of unsupervised machine learning techniques (k-means, agglomerative and hierarchical clustering; Chapter 14), and then again, and much more resoundingly, in a final chapter, new to the second edition of the book, on 'High Dimensional Problems.' This chapter highlights biological processes and genomic applications as key challenges for machine learning since they  generate datasets that are lavishly furnished with variables but often quite meagrely supplied with cases. In the shorthand of typical machine learning terminology, $p$ is larger than $N$: 'the number of features $p$ is much larger than the number of observations $N$ , often written $p>>N$' [@Hastie_2009, 649].

Hastie and co_author's invocation of DNA-related data is no arbitrarily chosen example amidst the general proliferation of illustrations typically found in machine learning pedagogy. If we turn to the research done on machine learning during 1990-2010, biology, and particularly molecular and then genomic biology, has a very high profile. After computer science and statistics, molecular biology, genomics and bioinformatics are by far the most heavily represented disciplines visible in academic journal publications associated with machine learning. In terms of contemporary biological knowledge production, the transformation of biology into a data intensive science is tightly entangled with machine learning. Genomics, I would suggest, is an antigen that provokes a multiplicity of  machine learners to act on it. At the same time, and reciprocally, the incursion of  machine learners profoundly re-configures the highly economically charged fields of biomedicine and biological sciences more generally, and generates new modes of action, accounting, testing, examining and sorting biological processes.  Wide-ranging infrastructural, institutional, professional and financial arrangements unfold along new paths constructed by the modes of operation of machine learning, with its epistopological transformations, its function-finding, its regimes of probabilistic, decisionistic and geometrical engagement with data. In multiple dimensions and directions, genomics -- the project of operating on the whole DNA complement of organisms -- is a nearest neighbour, a tightly adjacent territory to machine learning. This relatively long-established proximity (at least 25 years, and perhaps more) means that genomics and DNA-based data  is strategically important in the generalization of  machine learning, in the processes whereby the diagrammatic forms configured around these techniques, with their specific forms of articulation, statement and making-visible, propagate into multiple, once-disparate settings.  

In the generalization of machine learning, genomics appears with  multiple valencies. The Google Compute Engine, a globally-distributed ensemble of computers,  was briefly turned over to exploration of cancer genomics during 2012, and publicly demonstrated during the annual Google I/O conference. Midway through the demonstration, in which a human genome is visualized as a ring in 'Circos' form [@Krzywinski_2009,]  the speaker, Urs Hölzle, Senior Vice President of Infrastructure at Google 'then went even further and scaled the application to run on 600,000 cores across Google’s global data centers' [@GoogleInc._2012]. The audience clapped as the annular diagram of a human genome was decorated with a rapidly increasing number of cross-links, accompanied by a snapping sound. The world's '3rd largest supercomputer', as it was called by TechCrunch, a prominent technology blog,  'learns associations between genomic features' [@Anthony_2012]. Note the language of machine learning appearing here: it 'learns ... associations between features.' We are in the midst of many such demonstrations of 'scaling applications' of data in the pursuit of associations between 'features.'[^1]

[^1]: A second significant example, equally prestigious, might be IBM Corporation's 'cognitive computing platform,'  Watson. Watson, a distributed computing platform centred on machine learning, is hard to delineate or easily describe since it exists in a seemingly highly variable form. Its uses in genomics, pharmaceutical discovery and clinical trials are heavily promoted by IBM [@IBM_2014]

The I/O conference audience, largely comprising software developers, could hardly be expected to have a detailed interest in what was being shown on the screen. Their interest was steered toward the immediate availability of huge computing power: from 10,000 to 600,000 cores in a few seconds. The principal chain of associations validated by the genomics demonstration was, presumably, something like: genome=>complexity=>cancer/disease=>life/death=>important. Yet the Google Compute demonstration is, I would suggest, typical of how genomes, genes, proteins and biological sciences more generally, have important enunciative functions in machine learning. This transformation is only  hinted at in the Google I/O keynote address in Hölzle's talk of genomic features, gene expression and patient attributes.  The only concrete indication of how what was happening in the demonstration related to machine learning  was one mention of the RF-ACE (Random Forest- Artificial Contrasts with Ensembles) algorithm.  Google's press release emphasises epistemic values:

>it allows researchers to visually explore associations between factors such as gene expression, patient attributes, and mutations - a tool that will ultimately help find better ways to cure cancer. The primary computation that Google Compute Engine cluster performs is the RF-ACE code, a sophisticated machine learning algorithm which learns associations between genomic features, using an input matrix provided by ISB (Institute for Systems Biology). When running on the 10,000 cores on Google Compute Engine, a single set of association can be computed in seconds rather than ten minutes, the time it takes when running on ISB’s own cluster of nearly 1000 cores. The entire computation can be completed in an hour, as opposed to 15 hours [@GoogleInc._2012].

Amidst this mire of fairly technical computing jargon, we might observe that Google applies here an algorithm developed by engineers at Intel Corporation and Amazon to genomic datasets provided by the Institute of Systems Biology, Seattle, a doyen of big-data genomics. The RF-ACE literally re-draws a diagram of the genome, and re-draws it increasingly rapidly as the demonstration scales up to 10,000 cores (or CPUs).  A diagram that normally appears statically on-screen or on the page is now animated by an algorithic process. This confluence of commerce (Amazon), industry (Intel), media (Google) and genomic science (ISB) is, I suggest, symptomatic of the generalization of machine learning. In none of these demonstrations and examples, whether they come from _Elements of Statistical Learning_ or from Google Compute Engine, does the object of the knowledge -- genomes, genes, proteins -- actually appear in the terms of their own disciplines or scientific field (typically cancer biology). The 'positivity,' to use Michel Foucault's term from _The Archaeology of Knowledge_, as the matrix from which propositions are developed. As Foucault writes:

>To analyse positivities is to show in accordance with which rules a discursive practice may form groups of objects, enunciations, concepts, or theoretical choices. The elements thus formed do not constitute a science, with a defined structure of ideality; their system of relations is certainly less strict; but neither are they items of knowledge piled up one on top of another, derived from heterogeneous experiments, traditions, or discoveries, and linked only by the identity of the subject that possesses them. They are that on the basis of which coherent (or incoherent) propositions are built up, more or less exact descriptions developed, verifications carried out, theories deployed. They form the precondition of what is later revealed and which later functions as an item of knowledge or an illusion, an accepted truth or an exposed error, a definitive acquisition or an obstacle surmounted [@Foucault_1972, 181-2]

In the case, the positivity of RF-ACE and its scaled-up demonstration on Google Compute consists in the system of relations it permits to develop. Similarly, the treatment of DNA microarray data in the slightly earlier examples found in _Elements of Statistical Learning_ does not principally concern the subject of cancer biology, but much more the way a group of elements are assembled so as to permit the production of propositions that may cross the threshold of scientificity if configured in relation to experimental practices and scientific literatures, but may just as well constitute different forms of governmental, market-focused, organisational or managerial knowledges.  

## The advent of 'wide, dirty and mixed' data

The plurality of applications can sometimes make it seem that machine learning docks in the different domains, and then proceeds to administer learning algorithms to existing knowledges practices wherever it finds them. The mode of generalization here would seem to be an epistemic _terra nullius_ doctrine, in which existing knowledge titles are rapidly extinguished by their algorithmic treatment. The case of genomics suggest that matters are more complicated. The research literature published on machine learning since the early 1990s clusters around several main problems -- image recognition, document classification, and segmenting market behaviour (as in, working out what advertisement to show, or whether someone is likely to a buy a particular product, etc.). These problems position machine learning amidst regimes of visibility, the regularities of statements, and the production of economic value. Where, amidst these major problems, does molecular biology or genomics (arguably the successor of molecular biology) fit? For almost any of the major  machine learning techniques, albeit supervised or unsupervised, discriminative or generative, parametric or non-parametric, substantial research activity during the last two or so decades bridges across to genomics. (The difference between the first and second editions of  _Elements of Statistical Learning_ is largely attributable the style of problems associated with genomic data.)

Both the RF-ACE cancer biology demonstration and  the DNA microarray data extensively modelled in the final chapter of  _Elements of Statistical Learning_ address some elementary problems associated with genomic data. If we turn back to the `iris` dataset [@Fisher_1936], perhaps the most heavily used pedagogical dataset in the literature, it is strikingly obvious that the data does not provoke the infrastructural contortions associated with Google Compute, or for that matter, the highly sophisticated and quite subtle treatment of gene expression we find in genomics-related machine learning. 

```{r iris, echo=FALSE, message=FALSE, cache=TRUE,  fig.cap ="Fisher's iris dataset, 1936",comment=NA, size='smallsize'} 
    library(xtable)
	data(iris)
    tab = xtable(iris[1:5,], label='iris_sample', caption="First 5 rows of Fisher's `iris` dataset")
	print(tab)

```
It is usual, in working with `iris,` to construct machine learners that use the variables from the first four columns shown in \ref{tab:iris_sample} to infer the value of the `Species` variable (as seen in Chapter 5, where a decision tree was constructed using this same dataset). The measurements of petals and sepals of the irises of the Gaspé Peninsula in Novia Scotia, and their classification into different species is perhaps a typical mid-twentieth century biological procedure. The technique of linear discriminant analysis that Fisher demonstrated as a way of classifying the species of iris continues in use, but the shape and texture of contemporary datasets differs greatly from what we see in this table. Even in the excerpt shown in Table \ref{tab:iris_sample}, we can see that it is quite narrow as it has only a few columns, the data is nearly all of one type (measurements of lengths and widths), and the data is clean (there are no missing values). The data is tightly contained in the table. `Iris` is typical of classic statistics, and much  biological data prior to genomics in its relatively homogeneity and distinct partitioning.
[HERE]
Third and finally, there is much data missing from the table because it cannot be shown or easily displayed. In this example, the actual dataset is _wider_ than it is longer  (that is, 103 samples, each characterised by 300 different 'features' or variables), and this breadth of data simply cannot be displayed on the page. _Wide_ datasets are quite common in machine learning settings generally, but particularly common in genomics where they might only be a relatively small number of biological samples but a huge amount of sequence data for each sample. We will soon see how machine learning seeks to corral and contain their tendencies to sprawl.  In these features -- predictivity, heterogeneity, expansiveness -- we glimpse some problems that are not limited to,  but certain writ particularly large in,  genomics.

