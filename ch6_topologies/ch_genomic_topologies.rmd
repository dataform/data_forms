
# 6. Re-scaling and shifting objects with machine learning

## todo

- conclusion is still not right -- shorten it
- knn discussion is not properly integrated -- how to do that? Does the technique itself suggest a manner of moving? also, it's not of khan, golub, etc -- redo with Khan; add hclust dendrogram
- clustering, etc. not represented, but important -- use images for this
- quotes from fix are not here properly -- integrate them -- especially the one about non-parametric, blah, blah
- circo diagram of the disciplines and techniques?

> Because of a gradient that no doubt characterizes our cultures, discursive formations are constantly becoming epistemologized [@Foucault_1972, 195]

And, we might add, ontologized as things in the world. In the opening pages of _Elements of Statistical Learning_, a number of vignettes appear. They include document classification, image recognition, risk of heart attack, stock price prediction, risk factors for prostate cancer, and glucose estimates for diabetics. Four examples spanning communication, administration, medicine and scientific research are discussed in more detail: spam email classification, handwritten digit recognition, prostate cancer risk and lastly 'DNA Expression Microarrays' [@Hastie_2009, 23].[^6.22]  The latter vignette attracts a whole page colour figure -- a heatmap -- of microarray data [@Hastie_2009, 24].  DNA, genes, genomics and proteomics then more or less disappear from view for the next 503 hundred pages of the book (aside from a brief mention in the context of cross-validation), only to reappear somewhat suddenly in a discussion of unsupervised machine learning techniques (k-means, agglomerative and hierarchical clustering; Chapter 14, where the DNA microarray data is re-analysed using hierarchical clustering\index{mlrs}{hierarchical clustering}), and then again, and much more resoundingly, in a final chapter (Chapter 18) new to the second edition of the book, on 'High Dimensional Problems.' This chapter highlights  genomic research into biological processes as key challenges for machine learning. Genomics  generates datasets that are lavishly furnished with 'features' but often quite meagrely supplied with clinical cases or 'observations'. In the shorthand typical of machine learning terminology, $p$ is larger than $N$: 'the number of features $p$ is much larger than the number of observations $N$, often written $p\gg N$' [@Hastie_2009, 649]. Apart from one example where the Hastie and co-authors develop a document classifier for their journal articles, every example in the added Chapter 18 comes from genomic science, a scientific field that largely begins to take a recognisable shape in the late 1990s as both sequence data and high-throughput DNA-analysis devices, particularly microarrays, become widely available. 

[^6.22]: I have discussed the history of heatmaps and their place in contemporary science in other work [@Mackenzie_2013c].

Hastie and co-author's invocation of DNA-related data is no arbitrarily chosen example amidst the general proliferation of settings, domains, cases and examples typically found in machine learning pedagogy.  In multiple dimensions and directions, genomics -- the scientific project of operating on the whole DNA complement of organisms -- is a nearest neighbour, a tightly adjacent territory of machine learning. This relatively long-established proximity (at least 25 years, and perhaps more) means that genomics and DNA-based data  is strategically important in the generalization of  machine learning, in the processes whereby the diagrammatic forms configured around these techniques, with their specific forms of articulation, statement and making-visible, propagate into multiple, once-disparate settings.[^6.21]

[^6.21]: Signal processing is another such domain. Many of the techniques now prominent in machine learning developed in parallel in signal processing, where the encoding and decoding of signals has long been seen as a problem of pattern recognition amenable to statistical calculation. In some specific cases, such as Hidden Markov Models, the same techniques seem to appear almost simultaneously in very disparate domains. Hidden Markov Models appear in genomics (as part of the problem of sequence alignment) at the same time as the begin to appear in digital signal processing for wireless communication and video image compression [@Mackenzie_2010a] and above all, in speech recognition [@Rabiner_1989].

If we turn to the research done on machine learning during 1990-2010, biology, and particularly molecular and then genomic biology, has a very high profile in the research publications. After computer science and statistics, molecular biology, genomics and bioinformatics are by far the disciplines attracting most academic journal publications associated with machine learning. This may be because genomes, and human genomes in particular, are premiere scientific objects like the human brain, dark matter, global climate or fundamental particles in contemporary sciences. So they attracted vast research activity. But it might also be the case -- and I follow this line of argument here -- that the re-shaping and re-scaling of scientific fields is tied up with machine learning. In terms of contemporary biological knowledge production, the transformation of biology into a data-intensive science is tightly entangled with machine learning. 

## The positivity of the genome

```{r disciplines,  engine='python', results='asis', echo=FALSE, warning=FALSE, message=FALSE}



```

Genomes, I would suggest, are antigens that provoke a multiplicity of  machine learners to bind to them. Reciprocally, the incursion of  machine learners profoundly re-configures the highly economically charged fields of biomedicine and biological sciences more generally, and generates new modes of action, accounting, testing, examining and sorting biological processes.  Wide-ranging infrastructural, institutional, professional and financial arrangements follow along new paths constructed by the modes of operation of machine learning, with its epistopological transformations, its function-finding, its regimes of probabilistic, decisionistic and geometrical engagement with data.

In the generalization of machine learning, genomics appears with  multiple valencies. Genomes are both a challenge to the capacity of machine learning to produce scientific (as distinct from say the merely commercial knowledge of a credit risk model), and a way of validating machine learning  as a life-death relevant biomedical knowledge practice. Genomes also provide the pretext for many infrastructural re-configurations. The Google Compute Engine, a globally-distributed ensemble of computers typical of recent distributed commercial computing architectures,  was briefly turned over to exploration of cancer genomics during 2012, and publicly demonstrated at the annual Google I/O conference. Midway through the demonstration, in which a human genome is visualized as a ring in 'Circos' form [@Krzywinski_2009]  the speaker, Urs Hölzle, Senior Vice President of Infrastructure at Google 'then went even further and scaled the application to run on 600,000 cores across Google’s global data centers' [@GoogleInc.2012]. The audience clapped as the annular diagram of a human genome was decorated with a rapidly increasing number of cross-links, accompanied by a snapping sound as it appeared. The world's '3rd largest supercomputer', as it was called by TechCrunch, a prominent technology blog,  'learns associations between genomic features' [@Anthony_2012]. Note the language of machine learning appearing here: it 'learns ... associations between features.' We are in the midst of many such demonstrations of 'scaling applications' of data in the pursuit of associations between 'features.'[^6.0]

\begin{figure}
  \centering
      \includegraphics[width=0.9\textwidth]{figure/circos.pdf}
        \caption{A human genome diagrammed using the Circos form. The many tracks of this diagram support a range of graphic forms including scatterplots, heatmaps and histograms all anchored to the ideogram of the 23 chromosomes of the human genome. }
  \label{fig:circos}
\end{figure}

[^6.0]: A second significant and equally prestigious example of this infrastructural re-scaling might be IBM Corporation's 'cognitive computing platform,'  Watson. Watson, a distributed computing platform centred on machine learning, is hard to delineate or easily describe since it exists in a seemingly highly variable form. Its uses in genomics, pharmaceutical discovery,  clinical trials and cooking are heavily promoted by IBM [@IBM_2014]. Another would be Amazon Web Services various cloud computing services, some of which have been heavily used by genomic scientists. 

The I/O conference audience, largely comprising software developers, could hardly be expected to have a detailed interest in what was being shown on the screen. Their interest was steered toward the immediate availability of huge computing power: from 10,000 to 600,000 cores in a few seconds. The principal chain of associations validated by the genomics demonstration was, presumably, something like: genome=>complexity=>cancer/disease=>life/death=>important. Yet the Google Compute demonstration is, I would suggest, typical of how genomes, genes, proteins and biological sciences more generally, have important enunciative functions in machine learning. This transformation is only  hinted at in the Google I/O keynote address in Hölzle's talk of genomic features, gene expression and patient attributes.  The only concrete indication of how what was happening in the demonstration related to machine learning  was one mention of the RF-ACE (Random Forest- Artificial Contrasts with Ensembles) algorithm.  Google's press release emphasises epistemic values:

>it allows researchers to visually explore associations between factors such as gene expression, patient attributes, and mutations - a tool that will ultimately help find better ways to cure cancer. The primary computation that Google Compute Engine cluster performs is the RF-ACE code, a sophisticated machine learning algorithm which learns associations between genomic features, using an input matrix provided by ISB (Institute for Systems Biology). When running on the 10,000 cores on Google Compute Engine, a single set of association can be computed in seconds rather than ten minutes, the time it takes when running on ISB’s own cluster of nearly 1000 cores. The entire computation can be completed in an hour, as opposed to 15 hours [@GoogleInc.2012].

Amidst this mire of fairly technical computing jargon, we might observe that Google applies here an algorithm developed by engineers at Intel Corporation and Amazon to genomic datasets provided by the Institute of Systems Biology, Seattle, a doyen of big-data genomics. The RF-ACE algorithm (a further development of Breiman's random forests discussed in Chapter 5) literally re-draws a diagram of the genome, and re-draws it increasingly rapidly as the demonstration scales up to 10,000 cores (or CPUs).  A diagram that normally appears statically on-screen or on the printed page is now animated by an algorithmic process. This confluence of commerce (Amazon), industry (Intel), media (Google) and genomic science (ISB) is, I suggest, symptomatic of the generalization of machine learning. In none of these demonstrations and examples, whether they come from _Elements of Statistical Learning_ or from Google Compute Engine, does the object of the knowledge -- genomes, genes, proteins -- actually appear in the terms of their own disciplines or scientific field (typically cancer biology). The 'positivity,' to use Michel Foucault's term from _The Archaeology of Knowledge_, as the matrix from which propositions are developed. As Foucault writes:

>To analyse positivities is to show in accordance with which rules a discursive practice may form groups of objects, enunciations, concepts, or theoretical choices. The elements thus formed do not constitute a science, with a defined structure of ideality; their system of relations is certainly less strict; but neither are they items of knowledge piled up one on top of another, derived from heterogeneous experiments, traditions, or discoveries, and linked only by the identity of the subject that possesses them. They are that on the basis of which coherent (or incoherent) propositions are built up, more or less exact descriptions developed, verifications carried out, theories deployed. They form the precondition of what is later revealed and which later functions as an item of knowledge or an illusion, an accepted truth or an exposed error, a definitive acquisition or an obstacle surmounted [@Foucault_1972, 181-2]

In the case, the positivity of RF-ACE and its scaled-up demonstration on Google Compute consists in the system of relations it permits to develop. Similarly, the treatment of DNA microarray data in the slightly earlier examples found in _Elements of Statistical Learning_ does not principally concern cancer biology as such, but much more the way a group of elements are assembled so as to permit the production of propositions that may cross the threshold of scientificity if configured in relation to experimental practices and scientific literatures, but may just as well constitute different forms of governmental, market-focused, organisational or managerial knowledges.[^6.9]

[^6.9]: In their account of the surprisingly slow shift of microarrays towards clinical practice, Paul Keating and Alberto Cambrosio identify statistics as a kind of bottleneck:

    >The handling and processing of the massive data generated by microarrays has made bioinformatics a must, but has not exempted the domain from becoming answerable to statistical requirements. The centrality of statistical analysis emerged diachronically, as the field moved into the clinical domain, and is re-specified synchronically depending on the kind of experiments one carries out [@Keating_2012,49].

    What Keating and Cambrosio describe as 'becoming answer to statistical requirements' I would suggest also entails a transformation of statistical requirements in a new operational diagram that reduces some of the frictions associated with existing statistical practice. This operational diagram is machine learning. 


## The advent of 'wide, dirty and mixed' data

The plurality of applications can sometimes make it seem that machine learning docks in the different domains, and then proceeds to administer learning algorithms to existing knowledges practices wherever it finds them. The mode of generalization here would seem to be an epistemic _terra nullius_ doctrine, in which existing knowledge titles are rapidly extinguished by their algorithmic treatment. The case of genomics suggest that matters are more complicated. The research literature published on machine learning since the early 1990s clusters around several main problems -- image recognition, document classification, and segmenting market behaviour (as in, working out what advertisement to show, or whether someone is likely to a buy a particular product, etc.). These problems position machine learning amidst regimes of visibility, the regularities of statements, and the production of economic value. Where, amidst these major problems, does molecular biology or genomics (arguably the successor of molecular biology) fit? For almost any of the major  machine learning techniques, albeit supervised or unsupervised, discriminative or generative, parametric or non-parametric, substantial research activity during the last two or so decades bridges across to genomics. (The difference between the first and second editions of  _Elements of Statistical Learning_ is largely attributable the style of problems associated with genomic data.) The entanglements between genomics and machine learning display two aspects. The first of these is a new version of the now familiar problem of data dimensionality, data whose abundance, diffusion, heterogeneity or impaction thwarts its examination, tabulation, and regulated circulation. Genomic data epitomises these problems. The second of these is the problem of time, genesis, duration, activity and process that generates transient events, events in which sequence and above all becoming matter. This latter problem animates the frequent invocation of cancer datasets in the machine learning literature (particularly [@Golub_1999; @Khan_2001]). 

As to the former problem of dimensionality, both the RF-ACE cancer biology demonstration and  the DNA microarray data extensively modelled in the final chapter of  _Elements of Statistical Learning_ address some elementary problems associated with genomic data. If we turn back to the `iris` dataset [@Fisher_1936], perhaps the most heavily used pedagogical dataset in the literature, it is strikingly obvious that the data does not provoke the infrastructural contortions associated with Google Compute, or for that matter, the highly sophisticated and quite subtle treatment of gene expression we find in genomics-related machine learning. 

```{r iris, echo=FALSE, message=FALSE, cache=TRUE,  fig.cap ="Fisher's iris dataset, 1936",comment=NA, size='smallsize'} 
    library(xtable)
	data(iris)
    tab = xtable(iris[1:5,], label='iris_sample', caption="First 5 rows of Fisher's `iris` dataset")
	print(tab)

```

It is usual, in working with `iris,` to construct machine learners that use the variables from the first four columns shown in \ref{tab:iris_sample} to infer the value of the `Species` variable (as seen in Chapter 5, where a decision tree was constructed using this same dataset). The measurements of petals and sepals of the irises of the Gaspé Peninsula in Novia Scotia, and their classification into different species is perhaps a typical mid-twentieth century biological procedure. The technique of linear discriminant analysis that Fisher demonstrated as a way of classifying the species of iris continues in use, but the shape and texture of contemporary datasets differs greatly from what we see in this table. Even in the excerpt shown in Table \ref{tab:iris_sample}, we can see that it is quite narrow as it has only a few columns, the data is nearly all of one type (measurements of lengths and widths), and the data is clean (there are no missing values). The data is tightly contained in the table. `Iris` is typical of classic statistics, and much  biological data prior to genomics in its relatively homogeneity and distinct partitioning.

If `iris` is the conventional form, how does a genomic dataset differ? One clue comes from descriptions of the RF-ACE algorithm, first published in 2009. RF_ACE is attempts to  deal with 'modern data sets' that are 'wide, dirty, mixed with both numerical and categorical predictors, and may contain interactive effects that require complex models' [@Tuv_2009, 1341]. Such algorithms and the datasets they work on have a distinctive texture, which, I would suggest,  we should try to grasp if we want to understand how genomic data has become a lightning rod for machine learning. More clues come from the various treatments of DNA microarray data in _Elements of Statistical Learning_.  Hastie and co-authors introduce one microarray dataset they use in this way:

>The data in our next example form a matrix of 2308 genes (columns) and 63 samples (rows), from a set of microarray experiments. Each expression value is a log-ratio log(R/G). R is the amount of gene-specific RNA in the target sample that hybridizes to a particular (gene-specific) spot on the microarray, and G is the corresponding amount of RNA from a reference sample. The samples arose from small, round blue-cell tumors (SRBCT) found in children, and are classified into four major types: BL (Burkitt lymphoma), EWS (Ewing’s sarcoma), NB (neuroblastoma), and RMS (rhabdomyosarcoma). There is an additional test data set of 20 observations. We will not go into the scientific background here [@Hastie_2009, 651]

Note that while the number of samples (~80) in the small round blue-cell tumors (`SRBT`) [@Khan_2001]  dataset is less than the number of flowers measured in `iris,` the number of variables presented by the columns in the table (2308) is much greater. Hastie and co-authors, like the Google I/O demonstration do not 'go into the scientific background.' This again suggests that scientific knowledge _per se_ is not the central concern in machine learning. We could note too that the original publication of this dataset in 2001 [@Khan_2001] also made use of machine learning techniques (neural networks), but precisely in order to address the diagnostic difficulties of distinguishing different types of such tumors without resort to new experiments or biological knowledge.[^6.1]

[^6.1]: Khan and co-authors write: 

    > Gene-expression profiling using cDNA microarrays permits a simultaneous analysis of multiple markers, and has been used to categorize cancers into subgroups 5–8 . However, despite the many statistical techniques to analyze gene-expression data, none so far has been rigorously tested for their ability to accurately distinguish cancers belonging to several diagnostic categories [@Khan_2001, 673] 

```{r microarray, results='asis'}
library(xtable)
library(datamicroarray)
tab1 = xtable(describe_data(), label='microarray_data', caption ='Microarray datasets used in machine learning')
data('khan')
tab = xtable(head(khan[[1]], 5), label='srbct', caption='Small round blue-cell tumour data sample (Khan, 2001)')
print(tab)
```

The sample of the `SRBCT` data shown in Table \ref{tab:srbct} does not readily accommodate the wide table of this dataset. Unlike `iris`, the thousands of variables simply cannot be displayed on a page or screen. _Wide_ datasets are quite common in machine learning settings generally, but particularly common in genomics where in a given study there might only be a relatively small number of biological samples but a huge amount of sequencer or microarray data for each sample. This dimensionality of the data is common. Note too that while _Elements of Statistical Learning_ picks up the `SRBCT` dataset from biomedical research ([@Khan_2001] appears in the journal _Nature Medicine_), many other similar datasets appear in the machine learning literature. Table \ref{tab:microarray_data} shows some of the more commonly used cancer microarray data. The dimensions of the genomic data share this generic feature of width. Importantly, as discussed in Chapter 1 (in terms of the diagonalization running between different elements of code, data, mathematical functions and indexical signs) and in Chapter 2 (in terms of the auratic power of datasets), the fact that these datasets can be so readily loaded and accessed via bioinformatic infrastructures using code written in `R` or `Python` is also a notable feature of their advent in the machine learning literature.  The accessibility of genomic datasets is such that even a social science researcher can quickly write programs to retrieve this data. It attests to  several decades, if not longer, work on databases, web and network infrastructures, and analytical software, all, almost without exception, driven by the desire for aggregation, integration, archiving and annotation of sequence data that first became highly visible in the Human Genome Project of the 1990s.  The brevity of these lines of code -- half a dozen statements in `R`, no more -- suggests we are dealing with a high-sedimented set of practices, not something that has to be laboriously articulated, configured or artificed. Code brevity almost always signposts  highly-trafficked routes in contemporary network cultures. Without describing in any great detail the topography of databases, protocols and standards woven by and weaving through bioinformatics, the ready invocation of genomic datasets suggests that the mixed, dirty, wide datasets fed to algorithms such as RF-ACE or analysed in [@Hastie_2009] derives from the layered couplings and interweaving of scientific publications and scientific databases developed by biological science over the last three decades. As the code shows, sequence and other genomic data (and we will see some other types of contemporary genomic data below)  are available to scientists not only as users searching for something in particular and  retrieving specific data, but to scientists as programmers developing  ways of connecting up, gathering and integrating many different data points into to produce the wide ( many-columned), mixed (different types of data), and dirty (missing data, data that is 'noisy') datasets, datasets whose heterogeneous and often awkward topography then elicits and invites algorithmic treatment.

## Machine learning in genomics

Unlike the `iris` data, the `SRBCT` data is highly diagrammatic (see Chapter 1). That is, the columns refer to genes whose levels of expression in different samples are measured by comparison to their levels in a reference sample. The very identification of the several thousand genes whose levels of expression are measured by the microarray experiments already presupposes much preceding work on DNA sequences and their identification as protein-coding DNA amidst the highly repetitive vector of the genome sequence. Highly leveraged infrastructures for access to biological data pervade such  datasets. Considered more diagrammatically, genomes in many  ways becomes less linear or flat than the base sequences might suggest. (Even these sequences themselves harbour, as we will see, probabilistic models). The linear sequences of DNA data becomes more mixed and wide partly through the accessibility we have just seen that allows them to be superimposed, annotated and layered. But their shape also changes for a different reason. A recent review in the journal *Genomics* highlights the increasing importance of machine learning techniques:

> High-throughput genomic technologies, including gene expression microarray, single Nucleotideide polymorphism (SNP) array, microRNA array, RNA-seq, ChIP-seq, and whole genome sequencing, are powerful tools that have dramatically changed the landscape of biological research. At the same time, large-scale genomic data present significant challenges for statistical and bioinformatic data analysis as the high dimensionality of genomic features makes the classical regression framework no longer feasible. As well, the highly correlated structure of genomic data violates the independent assumption required by standard statistical models[@Chen_2012, 323].

This kind of commentary on the changing shape, not just the volume, of genomic data is quite common. Such contrasts typically highlight the incompatibility between a surging multiplicity of data forms and the constraints of existing statistical modelling techniques ('standard statistical models'). First of all,  newer instruments or tools such as microarrays and faster sequencers (so-called 'next generation sequencers') loom large [@Mackenzie_2015c]. The tropes of waves, deluges, floods and waves of DNA sequence and microarray data being somewhat washed out, this account instead highlights the 'high dimensionality of genomic features' and the 'highly correlated structures of genomic data.'

The new ways of working with sequence data typically highlight changes in statistical or modelling approaches. So for instance, Chen and co-authors recommend the use of the random forest algorithm because it:

>is highly data adaptive, applies to “large p, small n” problems, and is able to account for correlation as well as interactions among features. This makes RF particularly appealing for high-dimensional genomic data analysis. In this article, we systematically review the applications and recent progresses of RF for genomic data, including prediction and classification, variable selection, pathway analysis, genetic association and epistasis detection, and unsupervised learning [@Chen_2012, 323]

The key terms on the machine-learning-side of this formulation are 'large p, small n', 'high dimensional', 'prediction', 'classification,' 'variable selection' and 'unsupervised learning.' While these terms are widely used in machine-learning research since the early 1990s,  they are becoming increasingly visible in genomics. The key terms on the genomics side of this formulation would perhaps be 'pathway analysis', 'genetic association,' and 'epistasis.' These biological terms point to forms of  relationality typically associated with biologically interesting processes. Epistasis for instance broadly refers to linked gene action, a process that has been difficult to study before high-throughput methods of functional genomics were developed. In contemporary genomic science, these biological processes are increasingly understood in terms of eliciting and modelling the relations between *features* of genomic datasets in order to classify and predict biological outcomes. In between the machine learning and the genomic references appear several statistical terms: 'correlation' and 'interaction.' How does machine learning differ from the statistical practice that has underpinned much of modern biology?

## Making the genome into a model 

As data forms, genomes have a problematic mode of existence. In this respect, they are rather like cat pictures on the internet (see Introduction for a discussion of these), and possibly are rather worse. Just as the proliferation and circulation of cat images depends on signal compression and its models, genomes come into being as products of statistical modelling.  DNA sequences exist in great abundance (in databases, and increasingly, from the cheaper and more compact sequencing technologies), yet determining how they can be put in sequence, how they make sense in some biological assemblage, which they undoubtedly must (since we live on), is much harder.  They are assembled via statistical models. 'Genome assembly continues to be one of the central problems of bioinformatics' write the authors of a recent scientific review of the techniques of constructing whole genomes from DNA sequencer data [@Henson_2012]. The primary 'object' in genomics is a genome, the full complement of DNA in an organism. Genomes vary in size from the 2000 DNA base pairs of a virus, the  3.2 Gb (gigabase pairs) of humans through to the 130 Gb of the lung fish. The founding presupposition of genomic science is that knowing the complete sequence of DNA potentially yields insight into biological processes of many different kinds, ranging from evolution (phylogeny), development (ontogeny), metabolism, structure and pathology. In all of these respects, the DNA sequences have since at least the 1980s served as the common substrate for many different scientific experiments, technical developments, cyber-infrastructures and needless to say, biological imaginaries.[^6.3] This presupposition has an ineluctably promissory aspect since prior to whole genome sequencing projects initiated in the 1990s, biologists had never worked with genomes only with selected DNA sequences, especially those associate with genes and the proteins that they code. 

[^6.3]: A large and very diverse social science and humanities literature now exists around genomics. I draw on some of that literature as general background here, especially [@SunderRajan_2006; @Thacker_2005a; @Stevens_2011; @Leonelli_2014] and [@Haraway_1997], but largely do not address it directly.

While many earlier tabulations of variation, difference, groups, types and relations are woven through the life sciences, genomes have for the last several decades mesmerised biological sciences as a way of analysing and re-distributing the confused multiplicities associated with living things. As a data form, genomes are remarkably homogeneous. They are one-dimensional strings of letters corresponding to the well-known four nucleic acids (`g`, `a`, `t`, `c`). The raw data for genomes comes from the sequencing of DNA obtained from various organisms - viruses, bacteria, plants, fish, animals and humans. The sequencing of DNA, especially DNA that encodes the proteins that pervade biological processes, that structure tissues or assemble in complicated metabolic pathways, has been the concern first of molecular biology (mainly in the 1970s-1990s) and more recently  genomics (post-1990). In molecular biology, DNA sequences were carefully elicited (using the experimental techniques for instance of Sanger sequencing) and then compared with already known sequences of DNA to identify similarities that might have biology significance (for instance, evolution from a common ancestor). In genomics, DNA sequences generally originate from increasingly high-throughput sequencers that output massive datasets (see [@Stevens_2013; @Mackenzie_2015b]. Given both the accumulated store of already sequenced DNA and the increasingly viable practices of sequencing all of the DNA in a given organism together, genomics has  promised a much wider and more detailed understanding of biological complexity than any previous life science had been able to obtain. With genomes in hand, biologists for the first time would be in a position to build models of entire domains of biology, domains that previously could only be explored through painstaking experiments targeting specific cells, molecules, biochemical reactions and networks. The vast yet somewhat dispersed knowledges of the life sciences might be re-ordered and aligned on a new very extensive yet quite homogeneous  backbone of the genome read out as billions of DNA base pairs.  

Yet even the elementary data form of the genome as DNA base pairs is a highly algorithmic construct. The very existence of genomes as datasets  that can be downloaded in various forms presupposes their assembly from the fragments of DNA that sequencing techniques actually process. No existing sequencing technology produces a genome as a single form, as a vector (in the sense described in Chapter 2).  Instead, sequencing produces randomly sets of sequence fragments of various lengths that have to be assembled into a complete genome algorithmically. Whole genome assembly as reported for the initial draft of the human genome in 2001 [@Venter_2001;@Lander_2001] or for the model biological organism, _Drosophila_ [@Myers_2000] were not at the time understood as machine learning tasks. But the problem of whole genome assembly from DNA fragments was seen as  probabilistic in the sense that the aim is to assemble the often millions of short sequence fragments in an order that is most likely to occur. Even prior to the first full human genome assembly, genomic science had made heavy use of probabilistic models in aligning DNA (and protein amino acid) sequences.[^6.5] The practical problem here is that genomes contain swathes of duplicated regions that make assembling sequences in good  order a severe challenge. While sequence alignment algorithms have long used algorithmic approaches (known as dynamic programming) to score the similarity between two given DNA sequences, assembling the millions of DNA sequences produced by contemporary sequencers has necessitated entirely new techniques (shifting, for instance, from the overlap-layout-consensus model to the de Bruijn graph-based path models [@Pevnzer_2001].

[^6.5]: Richard Durbin, Sean Eddy, Anders Krogh and Graeme Mitchison's highly cited _Biological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids_ [@Durbin_1998] was based almost entirely on Hidden Markov Models, a way of modelling a sequence of states that _Elements of Statistical Learning_ treats at chapter length (see [@Hastie_2009, Chapter 17]) in the context of machine learning. While sequence alignment was regarded as a deeply algorithmic and statistical problem in the former volume, it is not at all formulated in the language of machine learning. There is little discussion of cost functions, vector spaces, optimisation, problems of generalization, supervised or unsupervised learning. On the other, David Haussler, a key bioinformatician in the first draft of the human genome in his work explicitly sought to bring  machine learning methods to bear on biology, and continues to do so.  

Despite the intensive work on genome assembly [see [@Henson_2012] for review), all genome assemblers produce errors. Put differently, whatever else might be known on the basis of the genome (genes, mutations, evolutionary relationships, variations associated with disease, heredity or individual identity), the genomic data and hence the genome itself as a scientific object is already deeply structured as a probabilistic model. The assemblers - the algorithmic process producing whole genomes from many fragments -- order sequences according to the same statistical criteria of maximum likelihood used in machine learning models. The rely heavily, moreover, on existing biological databases for reference or anchor points. No matter what assembly algorithm is used, the dependencies on other datasets and other knowledges, the perseverance of errors, and the deeply probabilistic texture of what counts as sequence data remain as key features of genomes. The indelible errors,  the entangled reliances on accumulated biological knowledges and above all this deeply probabilistic rendering of DNA as biological bedrock make genomes a particularly challenging site of machine learning activity.


## The expression of genomes in time

The broader point here is that genomes, as privileged and indeed somewhat singular contemporary scientific objects, were already the product not just of algorithmic or computational rendering, but of algorithms focused on the probabilistic  modelling of a form of linear order -- the sequence -- from which a plurality of processes (evolutionary diversity, ontogeny, metabolism, pathology, etc.) could be derived, typed, grouped, clustered or classified.  The inherently ordered fabric of genomes both bears the trace of something like machine learning (at least in the Hidden Markov models used in sequence alignment), and yet occludes that constitution in its ubiquity as sequence data.

There are important differences between the construction of the genome as object of genomic sciences and say, the iris as presented in the `iris` dataset. But the differences and gaps between the genome as biological data and machine learning practice was developing during the 1990s are neither so great as to amount to profound separation nor so small as not to matter. Rather, the genome and machine learning as a knowledge practice began to articulate themselves on each other. 

The analysis of gene expression is symptomatic of this mutual articulation. On the one hand, the genome promises a read out of all the genes in a given organism (~20,000 for humans). On the hand, the activity of these genes in time, or any particular point in the development of an organism, cannot be read from the genome. The overt arrival of machine learning techniques in genomic research was initially largely concerned with this problem of the genes in time, and in fact, nearly all of the analysis of genomic data in _Elements of Statistical Learning_ explicitly deals with various domains of gene expression. 

How does the positivity of machine learning, its specific forms of accumulation and generalization impinge on something like gene expression?  Compared to the algorithmic craft seen in whole genome assembly [@Venter_2001; @Myers_2000; @Pevzner_2001], the handling of DNA-based data in machine learning settings can seem rather crude in their lack of biological referentiality and specificity. Hastie and co-authors say, 'we will not go into the scientific background here.' But in that case, what do they go into?

Like the authors of the original scientific study [@Khan_2001], machine learners treat gene expression profiling largely as a problem of classification. The many gene expression studies seek to discriminate between different conditions, diseases, or pathologies on the basis of differing levels of gene expression. For machine learners, each gene is a variable whose levels of expression in a given sample may help identify what type that sample belongs to. In the case of the `SRBCT` data, the types include lymphomas, sarcomas and neuroblastomas. The immediate problem is with the shape of the data, a shape that owes much to the great accumulation of knowledge associated with molecular biology, and that, as well will see, only becomes more problematic in the later proliferation of sequence data associated with whole genome sequencing.

'Since $p\gg N$' write Hastie and co-authors, 'we cannot fit a full linear discriminant analysis (LDA) to the data; some sort of regularization is needed' [@Hastie_2009, 651]. Why cannot standard machine learners such as linear discriminant analysis fit here? What happens when they encounter the wide genomic datasets? 'Some sort of regularization' is needed they add. What is this regularization?  Michel Foucault describes the advent of disciplinary power partly in terms of enclosure or individualizing observation, but also in terms of techniques of supervising, examining and above all, _regularizing_ conduct. He writes:  

>Shift the object and change the scale. Define new tactics in order to reach a target that is now more subtle but also more widely spread in the social body. Find new techniques for adjusting punishment to it and for adapting its effects. Lay down new principles for regularizing, refining, universalizing the art of punishing [@Foucault_1977, 89] 

Foucault's description of the generalization of the techniques of disciplinary power, the formation that emerged in the late 18th century as a way of ordering 'massive or transient pluralities' (143) in Western European societies, resonates with contemporary treatments of data such as microarray gene expression. The 'target,' a term often used in machine learning to describe the type, group or response being modelled, in genomics is often subtle variations (in phylogeny, in pathogenesis), and these variations spread through and indeed define populations (in the sense of species, but also in the sense of the distributions of attributes that constitute the poles of abnormal and normal). Foucault's account of supervision (_surveiller_) and  penalisation as responses to 'popular illegality' [@Foucault_1977, 130] stresses the capillary network of observations, examining, ranking, test and gradation that adapt to the surging multiplicities by ordering them in tables. While the tables of data (see Table \ref{tab:srbct} in the microarray gene expression datasets suggest the persistence of the same technique of ordering multiplicities through partitioned observations, the _cells_ no longer target contain individuals under observation but focus on the  attributes of a multiplicity in movement, the human genome for instance in its many states and functions.  

'Shift the object and change the scale,' Foucault writes. 'Regularize in a way that automatically drops out features that are not contributing to the class predictions,' Hastie and co-authors write [@Hastie_2009, 652]. The new principles of regularizing in machine learning involve (as we have seen in Chapter 3) 'cost functions' or techniques of penalization that suppress model complexity, and that train models to eliminate features that yield only low predictive weight. In the many different techniques that _Elements of Statistical Learning_ brings to bear on the problem of gene expression -- diagonal linear discriminant analysis, nearest shrunken centroids, linear classifiers with quadratic regularization, regularized discriminant analysis, regularized multinomial logistic regression, support vector classifier -- essentially the same movement occurs as a model is fit. The object, the patterns of expression of genes associated with different types of tumours, is rescaled by the techniques of regularization that learn shrinking or dropping the weights of parameters of each gene in the model and examining the effect on the predictions that result. The coefficients or weights of parameters in the model, the $\beta_p$ values, are either reduced ($L_2$ regularization) or eliminated  ($L_1$ regularization) if they contribute little to the predictive accuracy of the machine learner. 

The somewhat daunting model *optimization function* for  one commonly used machine learning technique  called 'lasso regression' is frequently used in genomics. It displays features that might help us grasp what is how genomic data is handled in statistical machine learning. Remember that the linear regression model provides the framework for many machine learning techniques. We have seen the function in Equation \ref{eq:lineary_model} several times already (in Chapter 2,3,4) in different variations, including logistic regression used for classification of types or groupings.

\begin {equation}
\label {eq:linear_model}
\hat{Y} = \hat{\beta_0}  + \sum^p_{j=1} X_j \hat{\beta_j}
\end{equation}

In Equation \ref{eq:linear_model}, the values of $\beta$ map on to the different levels of expression of the many genes measured in the microarray experiments. The assumption here is that different tumour types will be strongly associated with different patterns of gene expression. As we have already seen, the problem here is that the number of combinations of genes associated with different tumour types vastly outweighs the number of samples. Regularization is the process of removing or eliminating many of the possible combinations by limiting the values of $\beta$ that contribute little to the prediction. This is a kind of 'shifting' or 're-scaling' of the object through a training technique. How does this training take place?

The regularized version of the linear regression framework known as 'lasso' -- Least Absolute Shrinkage and Selection Operator --  hinges on the lasso penalty shown in equation \ref{eq:lasso}[^6.7]

[^6.7]: The original publication of the lasso technique in a paper entitled 'Shrinkage and Selection via the Lasso' [@Tibshirani_1996] has been heavily cited in subsequent literature. [Google Scholar](http://scholar.google.co.uk/scholar?hl=en&q=tibshirani+1996+lasso) counts around 13,000 citations. For a paper published in the _Journal of the Royal Statistical Society_, this is surprisingly high, but attests, I would suggest, to the intense interest in renovating linear models for new problems such as image recognition or tumour classification. Somewhat surprisingly, given its heavy usage in other scientists, Andrew Ng's CS229 machine learning  course at Stanford University doesn't mention the lasso. 

\begin {equation}
\label {eq:lasso}
\widehat{\beta}^{lasso} = argmin_\beta\left\{ \frac{1}{2}\sum_{i}^{N}{(y_i - \beta_0 - \sum_{j=1}^{p}{x_{ij}\beta_j})^2}+ \lambda \sum_{j=1}^{p} \vert \beta_j \vert \right\}.
\end{equation}
>>	[@Hastie_2009, 68]

Equation \ref{eq:lasso} is notable for the way that it subjects the familiar 'residual sum of squares' way of calculating the coefficients to the 'penalty' carried by the last part of the equation $\sum\limits_i^p\vert\beta_j\vert$. As Hastie and co-authors write, 'the lasso does a kind of continuous subset [feature] selection' [@Hastie_2009, 69]. As always $argmin_\beta$ suggests that the algorithm should find the set of values for $\beta$ that minimize the overall value of the function. This is a balancing act in this case between reducing the sum of residuals shown in the first half of the equation, and minimizing the sum of the absolute values of $\beta_j$ in the second part of the function. The precise mechanics of this feature selection are not difficult to understand, but the point is that the re-shaping of the object proceeds, as is nearly always the case in machine learning, by gradually introducing and scaling all of the features in the common vector space $\mathbf{X}$, but only allowing those variables or features to remain in the set that help minimize the difference between the predicted response and the actual response. Figure \ref{fig:lasso} makes something of this scaling diagrammatically. In this diagram, the various diagonal lines show how values of coefficients grow and sometimes diminish as the `lasso` process runs. Vertical lines show steps as new variables are included in the model with different values of the control parameter $\lambda$.[^6.8] 

[^6.8]: [TBA -- not sure about this lambda reading]

\marginpar{}

```{r lasso, fig.show='hide', echo=FALSE, message=FALSE, warning=FALSE, fig.cap='', dev='pdf', cache=TRUE}
library(lars)
library(datamicroarray)
data(golub)
y = ifelse(golub$y=='ALL', TRUE, FALSE)
l1 = lars(x = golub$x, y, use.Gram=FALSE)
plot(l1, main='')
```

\begin{figure}
  \centering
      \includegraphics[width=1.0\textwidth]{figure/lasso-1.png}
        \caption{Shrinkage path of coefficients for Lasso regression on (Golub,1999) leukemia data}
  \label{fig:lasso}
\end{figure}

This intensive testing and selection of features results sometimes radically changes the object. In Figure \ref{fig:lasso},  these changes become a matter of diagrammatic observation. Comparing eight different methods for analyzing the microarray cancer data from [@Ramaswamy_2001], Hastie reports that 'lasso regression (one versus all)' selects 1,429 of the 16,063 genes in the dataset. The shifted-rescaled object, a set of 1400 genes, or in the case of the 'elastic-net penalized multinomial' model that uses only 384 genes, highlights a drastically reduced subset of the original object. 

## GWAS and its proliferating interactions

Machine learning does not stabilise genomes as data objects. In many ways, it gives rise to further difficulties, new sources of error and sometimes new transformations of objects. One important difficult is the increasingly visible presence of variations in genomes. These variations first become visible after the assembly of whole genome sequences. Genomes of individuals of the same species vary in having slightly different versions of the genes (alleles), many of which differ only by single nucleotide base pairs. Whole genome sequencing made these differences, known as single nucleotide polymorphisms (SNPs), much more apparent. They occur in their tens of millions in the human genome (some 100 million are reported in the NCBI dbSNP database). In coding regions, SNPs may occasion changes in protein structure; in non-coding regions that can affect how gene expression occurs or is regulated. Like genes, SNPs can be detected using microarrays. SNP microarrays are commonly used in genome-wide association studies (GWAS) that explore complex genetic traits and conditions. SNP-based DNA microarrays measure the occurrence of millions of SNPs in a given biological sample.

In GWAS, the term 'wide' is especially resonant. SNPs are widely distributed across the genome, and occur in their millions, so finding a pattern of association between the presence and distribution of these small variations and a complex trait or disease (in humans, animals, plants, etc.) is statistically challenging. Even the largest genomic studies only have a few thousand individual biological samples. For instance,    While a large study may have thousands of rows corresponding to tissue samples -- for example, the Wellcome Case Control Consortium studied 14,000 individuals [@Burton_2007] -- it will certainly  have many more columns (features) than rows (individual observed cases)  since each SNP tested on the microarray will have its own column in the dataset (up to a million or so). Usually a relatively small number of the columns of a typical GWAS dataset record the incidence of some other biomarker (levels of blood lipids, etc.) or physiological variable (for instance, blood pressure, sex or height). The vast bulk of the columns  in a GWAS dataset, however, will contain data from the microarray that points to the presence or absence of single mutations -- the SNPs -- in the base sequence.

Finding the association between a _single_ SNP and the disease or some phenotypical trait is quite straightforwardly done using standard statistical tests. But in GWAS, scientists do not know in advance which of the often several hundred thousand SNPs are associated with the trait. Furthermore, it is very unlikely that a single SNP is associated with a disease or given biological trait (for example, faster growth of a particular strain  of an agricultural plant, or a particular type of tumour) in any case. That would suggest that we are dealing with the relatively uncommon case of a single-gene trait or single-gene disorder. Much more likely, a set of SNP’s distributed across the genome will be associated with the trait under study. 

Even if statistically significant association between a set of SNPs and the occurrence of a complex trait can be detected, a challenge still remains. At a genomic level, the main challenge lies not in the association of sets of SNPs with traits as such, but in working out which combination of the hundreds of thousands of associations between SNPs and trait are significant.  Take for instance the task of analyzing epistasis or gene interaction in a GWA study. In epistasis, the level of expression of one gene is affected by its interactions with other genes. This would an instance of a 'highly correlated structure' in genomic data. The expectation is that analysing how combinations of SNPs are associated with traits or disorders will yield insights into epistatic processes associated disease or development.  As a review of statistical reasoning in GWAS points out:

> SNPs that combine to make larger genetic effects can statistically reflect an epistatic interaction, where the alleles of one gene influence the effects of alleles of another on a trait value or risk of disease. These interactions are by definition nonlinear and thus can dramatically increase the trait or risk [@Cantor_2010, 10]

Interaction between genes affects the expression of specific genes. This is seen to play an important biological role in complex disorders and also in development. Yet statistically modelling these interactions is, even in the eyes of statistically minded genomic researchers, extremely difficult. As Cantor and co-authors point out, ‘in GWAS, even for main effects, the number of predictors far exceeds the number of observations. Exhaustive examination of all pairwise interactions is possible, but for multiway interactions the task is totally impractical’ [@Cantor_2010, 11]. The number of predictors or 'features' is given by the number of SNPs analysed in the study. This is currently around one million, suggesting an astronomical number of potential interactions. The network of possible interactions, and the combinatorial explosion of possible pathways between parts of genomes involved in these interactions, dwarfs readily available raw computational power. Hence the turn to learning techniques:  'penalized regression methods extend standard regression techniques so that a large number of possibly correlated variables may be analyzed' [@Szymczak_2009, S51]. If interactions between SNPs can be addressed using machine learning techniques such as a lasso (a leading example of a penalized regression method), then the statistically unruly interactions between different genomic loci (regions) might be tamed. 

GWAS proliferate because of this promise of dealing with a subtly-varying plurality of interactions.  Since around 2004, these studies have proliferated almost like SNPs themselves. Lists of these studies such as (the GWAS Catalogue)[http://www.ebi.ac.uk/gwas/api/search/downloads/full] now include tens of thousands of attempts to locate and identify patterns of SNPs associated with complex traits and diseases. Tracking the role of machine learning through the studies would indicate some of ways in which interactions between variations have been generalized across a vast variety of settings, mostly associated with human disease, but not restricted to that.  [@Szymczak_2009] describes the importance of machine learning techniques to identify SNPs jointly involved in complex traits. 

## The supervision of machine learners in practice

Some of the difficulties in statistical practice can be illustrated by GWAS (Genome Wide Association Studies; note the resonance again of the term 'wide'), studies that compare selected variations in the genomes of individuals from a given population. Note that GWAS studies rely heavily on the accumulated, augmented genomic datasets described previously. And GWAS in turn provide, for instance, the knowledge base on which 23andMe, the consumer genetic profiling service, draws.  Typically, GWAS uses microarrays to look for statistically significant associations between the presence of SNPs in individuals and the occurrence of diseases. While microarrays in many different forms are heavily used in research,  they have been surprisingly slow to affect clinical practice. This proliferation in turns generates new efforts to observe and supervised the application of machine learning methods. If on the one hand, machine learners offer to regularized transient multiplicities (such as epistasis in complex genetic disorders), the observation and supervision of machine learners themselves has become necessary in order to validate and normalize their power in clinical practice, in diagnostic settings or in drug research. 


Within genomics  itself,  machine learning figures as a way of correcting for the fact that genomic instruments often produces different results, and that genomic knowledge claims shift unpredictably. That is, machine learning has increasingly been directed to the reorganisation of genomics itself. For instance, since microarray data itself suffers from many such problems of variation, the US Food and Drug Administration has since 2003 conducted a study of data analysis techniques for microarrays:  

>The US Food and Drug Administration MicroArray Quality Control (MAQC) project is a community-wide effort to analyze the technical performance and practical use of emerging biomarker technologies (such as DNA microarrays, genome-wide association studies and next generation sequencing) for clinical application and risk/safety assessment [@Parry_2010, 292].

Phase I of the US Federal Drug Administration-led MACQ addressed many issues of data analysis in the context of the clinical applications of gene expression analysis using microarrays. The primary statistical issue there was minimizing the ‘false discovery rate’ [@Slikker_2010,S1], a typical biostatistical problem. In MACQ-II however the focus rested on the construction of predictive models for ‘toxicological and clinical endpoints … and the impact of different methods for analyzing GWAS data’ [@Slikker_2010, 2]. On both the clinical and GWAS fronts, the 36 participating research teams tried out many predictive classifier models. Different machine learning techniques generate different kinds of models, genomic and biomedical researchers are compelled to engage with the many variations in prediction. 

In the shift from MACQ-I to MACQ-II, the problem of variations in the predictions produced by the machine learning models moved to center-stage.  The problem of variation arises not because any of the different modelling strategies used in machine learning gene expression datasets are wrong or erroneous, but because every model moves through the ‘feature space’ [@Parry_2010,292] in a different way (as we saw in Chapter 5 in discussions of different treatments of dimensionality). In the MACQ-II consortium, the teams were tasked to build 'classifiers' to predict whether a given sample or case belongs to a 'normal' or 'disease' group.  The most popular classifier in the MACQ consortium was the _k_ nearest neighbours model: '[a]mong the 19 779 classification models submitted by 36 teams, 9742 were k-nearest neighbor-based (KNN-based) models (that is, 49.3% of the total) [@Parry_2010, 293]. But, these models varied greatly in their predictions: 'there have been large variations in prediction performance among KNN models submitted by different teams' (293). Here we can begin to see that not only the genome itself varies, but the models themselves vary. In seeking to track and predict genomic variation, post-genomic science has constructed a new zone of variations at the intersection of database infrastructures and sequencing or gene expression technologies. 

What accounts for this variation? First of all, the 33 teams did not build single models. As is the norm in machine learning, they built thousands. For instance, in their attempt normalise the variations of  their models, one of the research groups in MACQ-II write that ‘for clinical end points and controls from breast cancer, neuroblastoma and multiple myeloma, we systematically generated 463,320 *k-nn* [*k*-nearest neighbour] models by varying feature ranking method, number of features, distance metric, number of neighbors, vote weighting and decision threshold’ [@Parry_2010,292].  The striking feature here is the proliferation of models in an effort to  tame the variations of predictive models. The number of predictive models constructed here rivals the number of SNPs typically assayed by the microarrays. 

Why do these models, in this case of exactly the same kind of data we have been discussed above, multiply so greatly and at the same time vary so much? All data analysis faces the so-called ‘curse of dimensionality’\index{general}{curse of dimensionality} [@Hastie_2009,22], but genomic data is particularly 'cursed' by its high dimensionality. Every distinct feature (e.g. a SNP) in a dataset effectively adds a new dimension to the data space. If every SNP, or every RNA transcript, adds a new dimension in the feature space, efforts to model this feature encounter  high dimensional spaces. Now nearly all classifier models seek some kind of regularity or boundary that cuts the space into two or more regions (e.g. 'diseased' and 'normal'). The line or surface that separates data points in a classifier can be linear (as in the lasso models) or non-linear (as in models). Since the 1950s, problems of classification and prediction in high-dimensional dataspaces have been the object of mathematical interest. The mathematician Richard Bellman coined the term ‘the curse of dimensionality’ to describe how partitioning becomes more unstable  as the dimensions of the dataspace increase  [@Bellman_1961]. The problem is that while the volume of a space increases exponentially with dimensions,  the volume of data usually does not usually increase at the same rate. In high dimensional spaces, the data becomes more thinly spread out. This makes it hard to construct good partitions. Sparsely populated spaces accommodate many different boundaries. 

## _k_ nearest neighbours models

\begin{figure}
  \centering
      \includegraphics[width=0.99\textwidth]{figure/fix_hodges_excerpt.pdf}
        \caption{The earliest formulation of the _k_ nearest neighbours model from Eveln Fix and Joseph Hodges work [@Fix_1951]}
  \label{fig:fix_excerpt}
\end{figure}

'The method of k-nearest neighbors makes very mild structural assumptions: its predictions are often accurate but can be unstable' write Hastie and co-authors [@Hastie_2009, 23]. The algorithm, first described by Evelyn Fix\index{mlrs}{Fix, Evelyn} and Joseph Hodges working at Berkeley in the early 1950s [@Fix_1951], is extremely simple in mathematical terms.[6.10] Equation \ref{eq:knn} shows almost the entire algorithm:

[^6.10]:Fix and Hodges frame their suggestion of the _k_ nearest neighbour model in this way:  'there seems to be a need for discriminative procedures whose validity does not require the amount of knowledge implied by by the normality assumption, the homoscedastic assumption, or any assumption of parametric form. The present paper is, as far as the authors are aware, the first one to attack subproblem (iii): can reasonable discrimination procedures be found which will work even if no parametric form can be assumed?' [@Fix_1951,7]. Subproblem (iii) in this quote refers to the challenge of deciding which of two populations an observed case belongs to if we know nothing about the parameters describing the two populations. 

\begin {equation}
\label {eq:knn}
\hat{Y(x)} = \frac{1}{k}\sum_{x_i \in N_k(x)}y_i
\end {equation}

'where $N_k(x)$ is the neighbourhood of $x$ defined by the $k$ closest points $x_i$ in the training sample'[@Hastie_2009, 14]. The algorithm effectively takes the average values of points in the neighbourhood, and uses that value to predict the result for a given set of features. As Hastie and co-authors put it, the neighbourhood is just those $k$ points near the case under consideration. The assumption here, as in nearly all machine learners traversing the common vector-space, is that proximity implies similarity. Neighbouring points in the vector-space are more similar than those at a distance.  As equation \ref{eq:knn} shows,  _k_ nearest neighbours seems to have only one parameter, the value $k$, the number of neighbours that a given model includes in its definition of a neighbourhood. In contrast to the linear forms of the models (formulated in equations \ref{eq:lasso} or \ref{eq:linear_model}), equation \ref{eq:knn} seems to require little training, supervision or regularization to work as a classifier. While nearly all of the models I have discussed in this and earlier chapters still work with the smooth form of the line or curve as their basic way of dividing, predicting or classifying, _k_ nearest neighbours models easily generate highly non-linear boundaries wending their way through the data. 

```{r knn, fig.cap='', echo=FALSE, cache=TRUE, message=FALSE, warning=FALSE, comment=NA, fig.show='hide'} 

	library(ElemStatLearn)
	require(class)
    library(datamicroarray)
    data('khan')
    k = c(5,15)
	x <- mixture.example$x
	g <- mixture.example$y
	xnew <- mixture.example$xnew
    
    train = sample(1:length(g), 40)
    xtest = khan$x[-train,]
    ytest = khan$y[-train]
    xtrain = x[train,]
    ytrain = g[train]

	mod <- class::knn(train=xtrain, test=xtest, cl=ytrain, k[1], prob=TRUE)
	mod <- class::knn(train=x, test=xnew, cl=g, k[1], prob=TRUE)
	prob <- attr(mod, "prob")
	prob <- ifelse(mod=="1", prob, 1-prob)
	px1 <- mixture.example$px1
	px2 <- mixture.example$px2
	prob <- matrix(prob, length(px1), length(px2))

	par(mar=rep(2,4), mfrow=c(1,1))
	contour(px1, px2, prob, levels=0.5, labels="", xlab="", ylab="", main=paste(k[1],"- nearest neighbour"), axes=FALSE)
	points(x, col=ifelse(g==1, "coral", "cornflowerblue"))
	gd <- expand.grid(x=px1, y=px2)
	points(gd, pch=".", cex=1.2, col=ifelse(prob>0.5, "coral", "cornflowerblue"))
	box()

```

```{r cluster, echo=FALSE, message=FALSE, fig.cap=''}
    pr.out = prcomp(khan$x, scale=TRUE)
    sd.data = scale(khan$x)
    data.dist = dist(sd.data)
    plot(hclust(data.dist, method='complete'), labels=khan$y, main='complete')
    plot(hclust(data.dist, method='average'), labels=NCI60$labs, main='average')
    plot(hclust(data.dist, method='single'), labels=NCI60$labs, main='single', xlab='')
    #cutting the dendrograms
    hc.out  =hclust(data.dist)
    hc.clusters =cutree(hc.out, 4)
    table(hc.clusters, khan$y)
    par(mfrow=c(1,1))
    plot(hc.out, labels = khan$y)
    plot(hc.clusters, labels = khan$y)
    plot(hc.clusters)
    par(mfrow=c(1,1))
    plot(hc.out, labels = khan$y)
    abline(h=139, col='red')
    km.out = kmeans(sd.data, 4, nstart=20)
    km.clusters = km.out$cluster
    table(km.clusters)
    hc.out2 = hclust(dist(pr.out$x[, 1:5]))
    plot(hc.out2, labels = khan$y)
    plot(hc.out, labels = khan$y)
```

\begin{figure}
  \centering
      \includegraphics[width=0.9\textwidth]{figure/knn-1.png}
        \caption{KNN models for simulated data in two classes. The decision boundary that separates the two classes is non-linear for both versions of the KNN model. For $k=5$, the decision boundary is more non-linear than for $k=15$ }
  \label{fig:knn}
\end{figure}

Most machine learning techniques embody some way of managing the dimensionality of the feature space, either by effectively reducing that dimensionality (as we saw in the lasso technique, which corrals the numbers of predictive features in the model), or concentrating on localised regions of the feature space, as does *k-nn*. For instance, the *k-nn* models — ‘k-nearest neighbour models’ — analyzed by Parry as part of MACQ-II are widely used because they are the simplest way of the dealing with interactions in an ‘exceedingly large feature space’ [@Parry_2010,292]. In *k-nn*, a training set of observations whose outcomes are known (e.g. clinical endpoints in cancer might be benign or malignant) are used to help classify (and hence predict) the test data.

As we can see from Figure \ref{fig:knn} above, in which data belongs to two classes (normal vs. not-normal), the decision boundaries produced by the algorithm can be unstable. The example shows two models, one for $k=5$ and the other for $k=15$.  Each model examines the relations between `r k` points in deciding whether a particular case belongs to one class or another.  While *k-nn*  finds local clusters and classifies on the basis of an irregular decision boundary, this classificatory power comes at the cost of instability. The more dimensions or features in the dataset, the larger the local neighbourhood needed to capture a fraction of the volume of the data,  and the more likely that most sample points will lie close to the boundary of the sample space, where they will be affected by the neighbouring space. The result is that ‘in high dimensions all feasible training samples sparsely populate the input space’ [@Hastie_2009, 23]. Because *k-nn* allows for non-linear interactions between features, for instance, small differences in the number of points in particular neighbourhoods can drastically affect the boundaries (as we see in comparing the right and left hand plots). These kinds of topological instability account for the propensity of machine learning treatments of feature-rich genomic data to produce accurate but unstable predictions.  We can also see why the MACQ-II team reported in [@Parry_2010] might have ended up  producing 463,000 _k_ nearest neighbour models in an effort to normalise and regulate predictive predictions.  The price of accurate predictivity in genomics is variation in prediction.

## Whole genome states

how is the whole repeating mass a functional process?
Genomes, as is quite well known, typically contain very little coding DNA and instead mostly contain duplicated stretches of the DNA that have moved around and been copied during the evolution of the genome. 

## Conclusion

Numbers around 600,000 have appeared several times in this chapter. Google Compute devoted 600,000 cores to the RF-ACE attack on the cancer genome; typical DNA microarrays for GWAS typically have around that number of cells; and the MACQ-II attempt to standardise the shape of the _k_ nearest neighbours models used to predict clinical outcomes developed around 600,000 models. Cores, SNPS, and models; infrastructural scaling, biological variation and the training of machine learners are entwined here.  What part does machine learning play in the transformation of science?

>Science is concentrated in an area of knowledge it does not absorb and in a formation which is in itself the object of knowledge and not of science. Knowledge (savoir) in general is not science or a even a particular corpus of knowledge [@Deleuze_1988, 19]

I have been suggesting that the genomic data -- beginning with DNA sequences, then levels of gene expression, followed by genome wide association studies of small mutations, and finally by whole genome analyses -- has been a constant $p \gg N$ antigen in machine learning. It is no accident that the techniques of regularization -- the lasso -- of linear models discussed in this chapter came to light, and were first demonstrated on the genomic data coming to light in the mid-1990s. Throughout the ongoing development and enrichment of DNA and protein sequencing techniques, replete with a vast and quite dynamic bioinformatic infrastructure, machine learning and genomics have been in a reciprocal embrace. Scientists and statisticians traffic between genomics and machine learning at almost every level, ranging from the sequence assembly to testing and analysis of DNA data in clinical settings.  For genomics, elementary practices of aligning and assembling sequences into whole genomes were re-configured probabilistically through machine learning models. Almost every subsequent development in genomics and related fields such as proteomics follows this pattern: an entity whose constitution is thoroughly dependent on prediction or algorithmic classification displays variations and grouping (such as gene expression, the linkage disequilibrium of SNPs, the seeming abundance of junk DNA that is actually functioning, etc.) that attract further efforts to differentiate and classify ever more subtly distributed differences.  It is hard to imagine contemporary biomedicine, biotechnology, pharmaceutical and environmental science research without this pattern. Some of the most elementary practices in contemporary genomics -- sequence alignment -- were already explicitly formulated as generative models to be constructed using algorithms such as expectation maximization. Cancer, in all its uncontrolled, diffuse and dangerously tumid concentrations, is the target of the most concerted ranking and typing treatments. As we see in the vignettes from  _Elements of Statistical Learning_, the demonstration of Google Compute cloud computing, or for that matter in the myriad publications in both machine learning  and life science journals that make use of support vector machines, neural networks, linear discriminant analysis or random forests, the positivity of machine learning establishes a new set of conditions for the exercise of scientific research, and configures new kinds of statements, new fields of objects (genomes in particular are difficult to conceive without their probabilistic modelling) and, as will be discussed in the next chapter,  subject positions (bioinformaticians, computational biologists, data scientists and others). 

The machine-learned models flow back through genomic science, reorganising and refocusing research, experiments, databases and funding in subtle but important ways. Between pre-genomic and post-genomic science, the status of significant differences in genomes shifted. Pre-HGP biology understood the significant differences between individual organisms largely in terms of gene alleles responsible for variations in phenotypes. Biological differences, and disease in particular, stemmed from different forms of genes. Understanding disease meant finding the disease genes. Even prominent proponents of genomics, such as Leroy Hood, writing of 'Biology and Medicine in the Twenty-First Century' in 1991, envisaged genomics as a way of simplifying 'the task of finding disease genes' [@Hood_1992,138]. Across the life sciences, genes were the object of much way of annotation, labelling and description. Two decades after the inception of whole genome sequencing, genomes present a different image of variation. According to Nikolas Rose, writing more recently, ‘there is no normal human genome; variation is the norm’ [@Rose_2009, 75]. ‘In this new configuration’, he writes, ‘what is required is not a binary judgment of normality and pathology, but a constant modulation of the relations between biology and forms of life, in the light of genomic knowledge.’ The emphasis in Rose’s formulation falls on ‘constant modulation’ of the relations between biology and forms of life. If post-genomic science departs from the understanding that there is no single genome but many genomes (see also Dupre, this volume), then according to Rose, variation itself becomes of primary interest. Here I want to argue that pursuit of variation is re-making the genome into ‘a form whose only object is the inseparability of distinct variations’ [@Deleuze_1994, 21]. These variations are, as we will see, located within genomics understood as a technical enterprise, between genomes and within the genome. In all three loci of variation, machine learning comes into play.

There are different ways of making sense of the implication of machine learning  in genomic and post-genomic science. We might attend to the long-standing and still operative metaphors of DNA as code, program [@Kay_2000], communication system or network, to the rich and diverse development of information retrieval and database systems dedicated to retrieving and annotating biological data of many different kinds (the annual database issue of the journal *Nucleic Acids Research*  lists over 400), to the role of various kinds of system models in systems biology ([@Szallasi_2006]) and dynamic models in biology more generally, to the crucial role played by certain kinds of matching and alignment techniques in  sequencing [@Durbin_1998;@Stevens_2011], to the rise of bioinformatics as a bundle of techniques focused on  cross-linking sequences and annotations, or, stretching further afield, to the attempts to reconfigure DNA and RNA sequences as components that function like digital logic in synthetic biology. In the predictive models of epistasis in GWAS, in standardisation of the predictions of clinical outcomes using *k-nn* models in MACQ-II, and in the self-organizing map of genomic states in ENCODE, we glimpse a set of transformations that are changing the shape of genomic data,  and thereby how genomes matter and what they mean. These examples, while by no means trivial, are inevitably quite limited in their scope. Yet they are not isolated. There are many other such examples. We could look at the large body of work that seeks to elicit sub-types in relation to  diseases, disorder or traits from populations (for example, [@Watson_2007,3]). We could examine the processes of imputation of genotypes that play a key role in firming up the statistical power of GWAS to identify associations (for example, ([@Burton_2007])). 

All of these planes and sites of tabulation and ranking are linked by the positivity of machine learning. Machine learning widely affects the forms of accumulation, the enunciative modalities, and the ordered multiplicities that constitute scientific knowledges. In the biosciences of the last two decades,  it seeks to disaggregate, compartmentalise and rank those aspects of genomes — their confused variations, their manifold spatial and temporal relationality in biological processes — that seem most distant and difficult to derive from the putatively fundamental, monolithic  and hence  tractable order of DNA  sequence data.  As we have seen in the various data excerpts above, that order is largely linear. DNA can be laid down in tracks or grids, aligned and annotated in multiple overlapping database records, but the problem is how this thickened linearity maps onto the  subtle, pervasive and transient forms of temporal and spatial re-shaping in life-forms.  In the feature-rich spaces countenanced by machine learning, we see attempts to embed manifolds in local regions, local linearities. Sometimes these local regions are regions of annotated DNA, as in the ENCODE examples, or non-linear interactions between sets of genes, as in the GWAS analysis of epistasis. At other times, these local regions are forms of life in a more general sense — clinical outcomes or diagnostic tests — as in MACQ-II.

I have emphasised on several occasions the common thread of what could be the termed the ‘genomic curse of dimensionality.’ The genomic sciences are based around the extraction, sorting and ordering of  DNA sequences. Machine learning practices, I have suggested,  begin to construct different dimensional spaces amidst the sequences, lines and tracks of genomes. What perhaps most vexes and animates post-genomic science is the desire to separate out from the DNA sequences variations that matter to both life-forms and forms of life. This vexation generates strenuous infrastructural, technical and conceptual attempts to reorganise and re-conceptualise what it is to know a genome. The machine learning techniques I have discussed  have begun to transform  bioinformatics and biostatistics. To understand machine learning as either as intensification of statistics, as hybridization of bioinformatics and statistics, or as the product of a reciprocal interactive convergence of biology and computing misses some of the important features of the reorganisation of genomics associated with these practices. It misses what machine learning brings from elsewhere to genomics, and it overlooks how the genome itself serves as a showcase, as we saw in the Google I/O demonstration,  of certain much wider transformations in the practices of anticipation and prediction. 

What is at stake in machine learning for the sciences or practices of knowing and predicting more generally? An analytical and an ethico-epistemic stake appear here. Foucault writes that 'we should distinguish carefully between _scientific domains_ and _archaeological territories_' [@Foucault_1972, 183]. If knowledge is what we can say within a given discursive formation, and knowledge has the status of knowledge by virtue of the practices that connects objects, field, subjects, statements, and institutions, then sciences are always localized within a field of knowledge that may exceed, and that may mutate in ways that alter resident sciences. Machine learning is just such a formation, I would suggest. Could we pose or address any normative questions by becoming aware of and articulating machine learning in practice  with greater clarity? Genomic science, in what it borrows from and in how it is affected from machine learning, displays some of the tendencies to reduce divergences and to corral differences typical of knowledge economies more generally. The philosopher of science, Isabelle Stengers writes:

>with the knowledge economy, we may have scientists at work everywhere, producing facts with the speed that new sophisticated instruments make possible, but that the way those facts are interpreted will now mostly follow the landscape of settled interests. … We will more and more deal with instrumental knowledge. (Stengers 2011:377)

As we see in the 600,000 cores of Google Compute applied to exploration of associations in cancer genomics using random forests, or the lasso applied to microarray SNP data,  machine learning rapidly produces facts.  Stengers suggests that the risk here is that divergence and unexpected forms of experimental result are somewhat diminished as a result. Machine-learning in genomics might produce a ‘self-organising map’ that poses questions following the 'landscape of settled interests' or *status quo*.  

The very justification for using such techniques is the inordinate difficulty of exploring the many dimensions of genomes otherwise. I have already hinted what I think is one core stake in these shifts. Genomes and genomics are touchstones for wider transformations in many sectors of science, industry, commerce, media and government susceptible to the imperatives of the knowledge economy. In contrast to some of these domains, where much that happens is obscured from view, the great virtue or genomic science is the relative openness of its workings and its dogged insistence on DNA as the generating set. The fact that data practices are relatively generic and accessible means that critical research into transformations associated with data and knowledge economies can accompany nearly every aspect of genomic practice. This is a forensic good. 

