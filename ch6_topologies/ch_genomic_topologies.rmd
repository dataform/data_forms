
# 6. The biological testing of machine learning 

> Because of a gradient that no doubt characterizes our cultures, discursive formations are constantly becoming epistemologized [@Foucault_1972, 195]

In the opening pages of _Elements of Statistical Learning_, a number of vignettes appear. They include document classification, image recognition, risk of heart attack, stock price prediction and risk factors of prostate cancer, and glucose estimates for diabetics. Four examples are discussed in more detail: spam email classification, handwritten digit recognition, prostate cancer risk and lastly DNA Expression Microarrays [@Hastie_2009, 23]. The latter vignette attracts a whole page colour figure -- a heatmap -- of microarray data [@Hastie_2009, 24].  DNA, genes, genomics and proteomics then more or less disappear from view for 500 hundred pages of the book (aside from a brief mention in the context of cross-validation), only to reappear somewhat suddenly in a discussion of unsupervised machine learning techniques (k-means, agglomerative and hierarchical clustering; Chapter 14), and then again, and much more resoundingly, in a final chapter, new to the second edition of the book, on 'High Dimensional Problems.' This chapter highlights biological processes and genomic applications as key challenges for machine learning since they  generate datasets that are lavishly furnished with variables but often quite meagrely supplied with cases. In the shorthand of typical machine learning terminology, $p$ is larger than $N$: 'the number of features $p$ is much larger than the number of observations $N$ , often written $p>>N$' [@Hastie_2009, 649].

Hastie and co_author's invocation of DNA-related data is no arbitrarily chosen example amidst the general proliferation of illustrations typically found in machine learning pedagogy. If we turn to the research done on machine learning during 1990-2010, biology, and particularly molecular and then genomic biology, has a very high profile. After computer science and statistics, molecular biology, genomics and bioinformatics are by far the most heavily represented disciplines visible in academic journal publications associated with machine learning. In terms of contemporary biological knowledge production, the transformation of biology into a data intensive science is tightly entangled with machine learning. Genomics, I would suggest, is an antigen that provokes a multiplicity of  machine learners to act on it. At the same time, and reciprocally, the incursion of  machine learners profoundly re-configures the highly economically charged fields of biomedicine and biological sciences more generally, and generates new modes of action, accounting, testing, examining and sorting biological processes.  Wide-ranging infrastructural, institutional, professional and financial arrangements unfold along new paths constructed by the modes of operation of machine learning, with its epistopological transformations, its function-finding, its regimes of probabilistic, decisionistic and geometrical engagement with data. In multiple dimensions and directions, genomics -- the project of operating on the whole DNA complement of organisms -- is a nearest neighbour, a tightly adjacent territory to machine learning. This relatively long-established proximity (at least 25 years, and perhaps more) means that genomics and DNA-based data  is strategically important in the generalization of  machine learning, in the processes whereby the diagrammatic forms configured around these techniques, with their specific forms of articulation, statement and making-visible, propagate into multiple, once-disparate settings.  

In the generalization of machine learning, genomics appears with  multiple valencies. The Google Compute Engine, a globally-distributed ensemble of computers,  was briefly turned over to exploration of cancer genomics during 2012, and publicly demonstrated during the annual Google I/O conference. Midway through the demonstration, in which a human genome is visualized as a ring in 'Circos' form [@Krzywinski_2009,]  the speaker, Urs Hölzle, Senior Vice President of Infrastructure at Google 'then went even further and scaled the application to run on 600,000 cores across Google’s global data centers' [@GoogleInc._2012]. The audience clapped as the annular diagram of a human genome was decorated with a rapidly increasing number of cross-links, accompanied by a snapping sound. The world's '3rd largest supercomputer', as it was called by TechCrunch, a prominent technology blog,  'learns associations between genomic features' [@Anthony_2012]. Note the language of machine learning appearing here: it 'learns ... associations between features.' We are in the midst of many such demonstrations of 'scaling applications' of data in the pursuit of associations between 'features.'[^1]

[^1]: A second significant example, equally prestigious, might be IBM Corporation's 'cognitive computing platform,'  Watson. Watson, a distributed computing platform centred on machine learning, is hard to delineate or easily describe since it exists in a seemingly highly variable form. Its uses in genomics, pharmaceutical discovery and clinical trials are heavily promoted by IBM [@IBM_2014]

The I/O conference audience, largely comprising software developers, could hardly be expected to have a detailed interest in what was being shown on the screen. Their interest was steered toward the immediate availability of huge computing power: from 10,000 to 600,000 cores in a few seconds. The principal chain of associations validated by the genomics demonstration was, presumably, something like: genome=>complexity=>cancer/disease=>life/death=>important. Yet the Google Compute demonstration is, I would suggest, typical of how genomes, genes, proteins and biological sciences more generally, have important enunciative functions in machine learning. This transformation is only  hinted at in the Google I/O keynote address in Hölzle's talk of genomic features, gene expression and patient attributes.  The only concrete indication of how what was happening in the demonstration related to machine learning  was one mention of the RF-ACE (Random Forest- Artificial Contrasts with Ensembles) algorithm.  Google's press release emphasises epistemic values:

>it allows researchers to visually explore associations between factors such as gene expression, patient attributes, and mutations - a tool that will ultimately help find better ways to cure cancer. The primary computation that Google Compute Engine cluster performs is the RF-ACE code, a sophisticated machine learning algorithm which learns associations between genomic features, using an input matrix provided by ISB (Institute for Systems Biology). When running on the 10,000 cores on Google Compute Engine, a single set of association can be computed in seconds rather than ten minutes, the time it takes when running on ISB’s own cluster of nearly 1000 cores. The entire computation can be completed in an hour, as opposed to 15 hours [@GoogleInc._2012].

Amidst this mire of fairly technical computing jargon, we might observe that Google applies here an algorithm developed by engineers at Intel Corporation and Amazon to genomic datasets provided by the Institute of Systems Biology, Seattle, a doyen of big-data genomics. The RF-ACE algorithm (a further development of Breiman's random forests discussed in Chapter 5) literally re-draws a diagram of the genome, and re-draws it increasingly rapidly as the demonstration scales up to 10,000 cores (or CPUs).  A diagram that normally appears statically on-screen or on the printed page is now animated by an algorithmic process. This confluence of commerce (Amazon), industry (Intel), media (Google) and genomic science (ISB) is, I suggest, symptomatic of the generalization of machine learning. In none of these demonstrations and examples, whether they come from _Elements of Statistical Learning_ or from Google Compute Engine, does the object of the knowledge -- genomes, genes, proteins -- actually appear in the terms of their own disciplines or scientific field (typically cancer biology). The 'positivity,' to use Michel Foucault's term from _The Archaeology of Knowledge_, as the matrix from which propositions are developed. As Foucault writes:

>To analyse positivities is to show in accordance with which rules a discursive practice may form groups of objects, enunciations, concepts, or theoretical choices. The elements thus formed do not constitute a science, with a defined structure of ideality; their system of relations is certainly less strict; but neither are they items of knowledge piled up one on top of another, derived from heterogeneous experiments, traditions, or discoveries, and linked only by the identity of the subject that possesses them. They are that on the basis of which coherent (or incoherent) propositions are built up, more or less exact descriptions developed, verifications carried out, theories deployed. They form the precondition of what is later revealed and which later functions as an item of knowledge or an illusion, an accepted truth or an exposed error, a definitive acquisition or an obstacle surmounted [@Foucault_1972, 181-2]

In the case, the positivity of RF-ACE and its scaled-up demonstration on Google Compute consists in the system of relations it permits to develop. Similarly, the treatment of DNA microarray data in the slightly earlier examples found in _Elements of Statistical Learning_ does not principally concern the subject of cancer biology, but much more the way a group of elements are assembled so as to permit the production of propositions that may cross the threshold of scientificity if configured in relation to experimental practices and scientific literatures, but may just as well constitute different forms of governmental, market-focused, organisational or managerial knowledges.  

## The advent of 'wide, dirty and mixed' data

The plurality of applications can sometimes make it seem that machine learning docks in the different domains, and then proceeds to administer learning algorithms to existing knowledges practices wherever it finds them. The mode of generalization here would seem to be an epistemic _terra nullius_ doctrine, in which existing knowledge titles are rapidly extinguished by their algorithmic treatment. The case of genomics suggest that matters are more complicated. The research literature published on machine learning since the early 1990s clusters around several main problems -- image recognition, document classification, and segmenting market behaviour (as in, working out what advertisement to show, or whether someone is likely to a buy a particular product, etc.). These problems position machine learning amidst regimes of visibility, the regularities of statements, and the production of economic value. Where, amidst these major problems, does molecular biology or genomics (arguably the successor of molecular biology) fit? For almost any of the major  machine learning techniques, albeit supervised or unsupervised, discriminative or generative, parametric or non-parametric, substantial research activity during the last two or so decades bridges across to genomics. (The difference between the first and second editions of  _Elements of Statistical Learning_ is largely attributable the style of problems associated with genomic data.)

Both the RF-ACE cancer biology demonstration and  the DNA microarray data extensively modelled in the final chapter of  _Elements of Statistical Learning_ address some elementary problems associated with genomic data. If we turn back to the `iris` dataset [@Fisher_1936], perhaps the most heavily used pedagogical dataset in the literature, it is strikingly obvious that the data does not provoke the infrastructural contortions associated with Google Compute, or for that matter, the highly sophisticated and quite subtle treatment of gene expression we find in genomics-related machine learning. 

```{r iris, echo=FALSE, message=FALSE, cache=TRUE,  fig.cap ="Fisher's iris dataset, 1936",comment=NA, size='smallsize'} 
    library(xtable)
	data(iris)
    tab = xtable(iris[1:5,], label='iris_sample', caption="First 5 rows of Fisher's `iris` dataset")
	print(tab)

```
It is usual, in working with `iris,` to construct machine learners that use the variables from the first four columns shown in \ref{tab:iris_sample} to infer the value of the `Species` variable (as seen in Chapter 5, where a decision tree was constructed using this same dataset). The measurements of petals and sepals of the irises of the Gaspé Peninsula in Novia Scotia, and their classification into different species is perhaps a typical mid-twentieth century biological procedure. The technique of linear discriminant analysis that Fisher demonstrated as a way of classifying the species of iris continues in use, but the shape and texture of contemporary datasets differs greatly from what we see in this table. Even in the excerpt shown in Table \ref{tab:iris_sample}, we can see that it is quite narrow as it has only a few columns, the data is nearly all of one type (measurements of lengths and widths), and the data is clean (there are no missing values). The data is tightly contained in the table. `Iris` is typical of classic statistics, and much  biological data prior to genomics in its relatively homogeneity and distinct partitioning.

If `iris` is the conventional form, how does a genomic dataset differ? One clue comes from descriptions of the RF-ACE algorithm, first published in 2009. RF_ACE is attempts to  deal with 'modern data sets' that are 'wide, dirty, mixed with both numerical and categorical predictors, and may contain interactive effects that require complex models' [@Tuv_2009, 1341]. Such algorithms and the datasets they work on have a distinctive texture, which, I would suggest,  we should try to grasp if we want to understand how genomic data has become a lightning rod for machine learning. More clues come from the treatment of DNA microarray data in _Elements of Statistical Learning_.  Hastie and co-authors introduce one microarray dataset they use in this way:

>The data in our next example form a matrix of 2308 genes (columns) and 63 samples (rows), from a set of microarray experiments. Each expression value is a log-ratio log(R/G). R is the amount of gene-specific RNA in the target sample that hybridizes to a particular (gene-specific) spot on the microarray, and G is the corresponding amount of RNA from a reference sample. The samples arose from small, round blue-cell tumors (SRBCT) found in children, and are classified into four major types: BL (Burkitt lymphoma), EWS (Ewing’s sarcoma), NB (neuroblastoma), and RMS (rhabdomyosarcoma). There is an additional test data set of 20 observations. We will not go into the scientific background here [@Hastie_2009, 651]

Note that while the number of samples (~80) in the small round blue-cell tumors (`SRBT`) [@Khan_2001]  dataset is less than the number of flowers measured in `iris,` the number of variables presented by the columns in the table (2308) is much greater. Hastie and co-authors, like the Google I/O demonstration do 'go into the scientific background.' This again suggests that scientific knowledge _per se_ is not the central concern in machine learning. The original publication of this dataset in 2001 [@Kkan_2001] also made use of machine learning techniques (neural networks), but precisely in order to address the diagnostic difficulties of distinguishing different types of such tumors without resort to new experiments or biological knowledge. 

```{r microarray, results='asis'}
library(xtable)
library(devtools)
install_github('ramhiser/datamicroarray')
library(datamicroarray)
tab1 = xtable(describe_data(), label='microarray_data', caption ='Microarray datasets used in machine learning')
data('khan')
tab = xtable(head(khan[[1]], 5), label='srbct', caption='Small round blue-cell tumour data sample (Khan, 2001)')
print(tab)
```

The sample of the `SRBCT` data shown in Table \ref{tab:srbct} does not readily accommodate the wide table of this dataset. Unlike `iris`, the thousands of variables simply cannot be displayed on a page or screen. _Wide_ datasets are quite common in machine learning settings generally, but particularly common in genomics where in a given study there might only be a relatively small number of biological samples but a huge amount of sequencer or microarray data for each sample. This dimensionality of the data is common. Note too that while _Elements of Statistical Learning_ picks up the `SRBCT` dataset from biomedical research ([@Khan_2001] appears in the journal _Nature Medicine_), many other similar datasets appear in the machine learning literature. Table \ref{tab:microarray_data} shows some of the more commonly used cancer microarray data. The dimensions of the genomic data share this generic feature of width. Importantly, as discussed in Chapter 1 (in terms of the diagonalization running between different elements of code, data, mathematical functions and indexical signs) and in Chapter 2 (in terms of the auratic power of datasets), the fact that these datasets can be so readily loaded and accessed via bioinformatic infrastructures using code written in `R` or `Python` is also a notable feature of their advent in the machine learning literature.  The accessibility of genomic datasets is such that even a social science researcher can quickly write programs to retrieve this data. It attests to  several decades, if not longer, work on databases, web and network infrastructures, and analytical software, all, almost without exception, driven by the desire for aggregation, integration, archiving and annotation of sequence data that first became highly visible in the Human Genome Project of the 1990s.  The brevity of these lines of code -- half a dozen statements in `R`, no more -- suggests we are dealing with a high-sedimented set of practices, not something that has to be laboriously articulated, configured or artificed. Code brevity almost always signposts  highly-trafficked routes in contemporary network cultures. Without describing in any great detail the topography of databases, protocols and standards woven by and weaving through bioinformatics, the ready invocation of genomic datasets suggests that the mixed, dirty, wide datasets fed to algorithms such as RF-ACE or analysed in [@Hastie_2009] derives from the layered couplings and interweaving of scientific publications and scientific databases developed by biological science over the last three decades. As the code shows, sequence and other genomic data (and we will see some other types of contemporary genomic data below)  are available to scientists not only as users searching for something in particular and  retrieving specific data, but to scientists as programmers developing  ways of connecting up, gathering and integrating many different data points into to produce the wide ( many-columned), mixed (different types of data), and dirty (missing data, data that is 'noisy') datasets, datasets whose heterogeneous and often awkward topography then elicits and invites algorithmic treatment.

## Forests and lassos in genomics

Highly leveraged infrastructures for access to biological data reshape what counts as datasets. As a data form, genomes in many  ways becomes less linear or flat than the base sequences. The linear sequences of DNA data becomes more mixed and wide partly through the accessibility we have just seen that allows them to be superimposed, annotated and layered. But their shape also changes for a different reason. A recent review in the journal *Genomics* highlights the increasing importance of machine learning techniques:

> High-throughput genomic technologies, including gene expression microarray, single Nucleotideide polymorphism (SNP) array, microRNA array, RNA-seq, ChIP-seq, and whole genome sequencing, are powerful tools that have dramatically changed the landscape of biological research. At the same time, large-scale genomic data present significant challenges for statistical and bioinformatic data analysis as the high dimensionality of genomic features makes the classical regression framework no longer feasible. As well, the highly correlated structure of genomic data violates the independent assumption required by standard statistical models[@Chen_2012, 323].

This kind of commentary on the changing shape, not just the volume, of genomic data is quite common. Such contrasts typically highlight the incompatibility between a surging multiplicity of data forms and the constraints of existing statistical modelling techniques ('standard statistical models'). First of all,  newer instruments or tools such as microarrays and faster sequencers (so-called 'next generation sequencers') loom large [@Mackenzie_2015c]. The tropes of waves, deluges, floods and waves of DNA sequence and microarray data being somewhat washed out, this account instead highlights the 'high dimensionality of genomic features' and the 'highly correlated structures of genomic data.'

[HERE]

The new ways of working with sequence data typically highlight changes in statistical or modelling approaches. So for instance, Chen and co-authors recommend the use of the random forest algorithm because it:

>is highly data adaptive, applies to “large p, small n” problems, and is able to account for correlation as well as interactions among features. This makes RF particularly appealing for high-dimensional genomic data analysis. In this article, we systematically review the applications and recent progresses of RF for genomic data, including prediction and classification, variable selection, pathway analysis, genetic association and epistasis detection, and unsupervised learning [@Chen_2012, 323]

The key terms on the machine-learning-side of this formulation are 'large p, small n', 'high dimensional', 'prediction', 'classification,' 'variable selection' and 'unsupervised learning.' While these terms are widely used in machine-learning research since the early 1990s,  they are becoming increasingly visible in genomics. The key terms on the genomics side of this formulation would perhaps be 'pathway analysis', 'genetic association,' and 'epistasis.' These biological terms point to forms of  relationality typically associated with biologically interesting processes. Epistasis for instance broadly refers to linked gene action, a process that has been difficult to study before high-throughput methods of functional genomics were developed. In contemporary genomic science, these biological processes are increasingly understood in terms of eliciting and modelling the relations between *features* of genomic datasets in order to classify and predict biological outcomes. In between the machine learning and the genomic references appear several statistical terms: 'correlation' and 'interaction.' How does machine learning differ from the statistical practice that has underpinned much of modern biology?


## Conclusion


