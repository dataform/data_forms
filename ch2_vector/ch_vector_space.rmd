\chapter{Vectorisation and its consequences}
\label{ch:vector}


```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=12, tidy=TRUE, fig.height=8, 
                      echo=FALSE,  warning=FALSE, message=FALSE, dev='pdf')

library(RSQLite)
con <- dbConnect(RSQLite::SQLite(),'../ml_lit/all_refs.sqlite3')
res = dbGetQuery(con, statement ="select * from basic_refs limit 10;")
library(ggplot2)
library(stringr)
library(xtable)
options(xtable.comment = FALSE)
```

>All things are vectors [@Whitehead_1960]

>We call _any_ set that satisfies these properties (or axioms) a *vector space*, and the objects in the set are called *vectors*. [@Larson_1996, 166] \index{vector} \index{vector space|see also {mathematics!linear algebra}}

The epistemic mobility of  machine learning is not only due to the generic mathematical, statistical or computational advances. It attests to the advent of a data practice, which opens an expanding epistemic space. The space arises, I will suggest, from a  specific operational diagram of data associated with linear algebra. This diagrammatic operation generates an expanding volume, a vector space. It  \gls{\textit{vectorises}} data according to axes, coordinates, and scales. Machine learning, in turn, inhabits a vectorised space, and it operates vectorally.  \index{data!vectorisation|see {vector space!vectorisation}}

Often data is  represented as if it is an homogeneous mass or a continuous flowing stream. My aim here, however, is to examine the transformations that allow different shapes and densities of data to become learnable. Data in its local complexes spaces out in many different density shapes, depending on how the data has been generated or instanced.[^2.104] \index{data!density of} Whatever the starting point -- a measuring instrument, people clicking and typing on websites, a device like a camera, a random number generator, etc. --  machine learners only every encounter data in specific *vectorised* shapes (vectors,  matrices, arrays, etc), scaled within a geometrically coordinate volume.  The mapping and forming, when mentioned at all,  is sometimes referred to as 'data cleaning' but that term covers over important but largely taken for granted transformations. \index{data!cleaning} 

If, as I have been proposing, we engage with the data practices  then the reshaping and reflowing of data densities into vectors matters greatly.   This forming and reforming of data is evidence of  implicated relations. These practices attest to what the philosopher A.N. Whitehead called 'strain':

>a feeling in which the forms exemplified in the datum concern geometrical, straight, and flat loci will be called a "strain." In a strain, qualitative elements, other than the geometrical forms, express themselves as qualities implicated in those forms [@Whitehead_1960, 310]. \index{Whitehead, A. North}  \index{data!strain}

In many machine learning models the exemplified forms are straight or flat loci (as we see in chapter \ref{ch:function}).  Yet  different practices also elicit relations that strain the linear shaping of data and these divergent relations sometimes combine in generative and provocative ways. 

#  Vector space and geometry

Statistical modelling, data-mining, pattern recognition, recommendation systems, network modelling and machine learning rely very much on the operation called 'fitting a model.' \index{model!fitting} The fitting of a model  as a spatial practice has some elements that resemble the phenomenologist Edmund Husserl's account of the origin of geometry. Husserl writes:

>First to be singled out from the thing-shapes are surfaces -- more or less "smooth," more or less perfect surfaces; edges, more or less rough or fairly "even"; in other words, more or less pure lines, angles, more or less perfect points; then again, among the lines, for examples, straight lines are especially preferred, and among surfaces, the even surfaces. ... Thus the production of even surfaces and their perfection (polishing) always plays its role in praxis [@Derrida_1989, 178] \index{Husserl, Edmund!on thing-shapes in geometry}

Husserl here refers is attempting to describe something of the way in which forms such as planes, lines, circles, triangles, squares, and points became objects of geometrical practice. A similar polishing and smoothing of surfaces is certainly taking place today in the thing-shapes we call data. The basic machine learning work of 'fitting a model' (or many models)  to data is often literally implemented, as we will see, by  constraining data within a coordinate, discretised space, which I term the \gls{\textit{vector space}} \index{vector space}  Critical thought from phenomenology to social theory has a long-standing  nervousness about the power of geometry and its gradual movement away from shapes and things towards mathematical operations.   The philosopher Hannah Arendt, for instance, observes: 

>decisive is the entirely un-Platonic subjection of geometry to algebraic treatment, which discloses the modern idea of reducing terrestrial sense data and movements to mathematical symbols [@Arendt_1990, 265] \index{Arendt, Hannah!on geometry and algebra}

The crux of the problem rests on the 'treatment' or operations that 'reduce terrestrial sense data and movements' to symbols. One challenge for contemporary thought is how to orient itself to such operations, particularly in their data-intensive forms,  without assuming that the familiar story of scientific and technical reduction of sense and movement to the lines and planes of modern geometry is repeated in contemporary data practice. \index{critical thought!relation to geometry}

They also, as we will see, reach down into the practices of programming, infrastructure and hardware production in ways that differ somewhat from increases in computational power or speed. Familiar narratives of Moore's Law increases in  speed or efficiency of computation do not account for transformations of   data in  `R` and other computing environments (for instance, the popular `Map-Reduce` architecture invented at Google Corporation to speed up its search engine services [@Mackenzie_2011] \index{data!architecture!map-reduce}).  Vectorisation transforms data along more diagrammatic lines.[^2.1]  

# Mixing places

```{r elemstatlearn_data_2, echo=FALSE, results = "asis"}
library(ElemStatLearn)
library(xtable)
datasets_results <-data(package='ElemStatLearn')$results[,3:4]
datasets = as.data.frame(datasets_results)
dataset_count <- nrow(datasets)
dataset_table = xtable(datasets, caption='Datasets in \\textit{Elements of Statistical Learning}', label = 'tab:datasets_elemstatlearn', table.placement = '!ht')
print.xtable(dataset_table, type='latex')
```

Date appears in _Elements of Statistical Learning_ in multiple forms. \index{_Elements of Statistical Learning_!datasets in} Maps of New Zealand fishing patterns lie next to plots of factors in South African heart disease. The `r dataset_count` datasets  shown in table \ref{tab:datasets_elemstatlearn} typify the variety found in [@Hastie_2009]. They span scientific, clinical, commercial and media fields. Note that they include many patches and pathways of everyday life -- speaking, seeing, writing and reading -- as well as specific scientific objects of knowledge such as galaxies, cancer, climate and national economies. This mixture is not unusual for machine learners.  To give another example, the statistician Leo Breiman's 2001 article on random forests [@Breiman_2001], \index{machine learner!random forest} perhaps the most cited journal paper in the machine learning literature, displays a similarly diagonal line across human and non-human worlds: \index{Breiman, Leo}

>Glass,  Breast cancer,  Diabetes,  Sonar,  Vowel,  Ionosphere,  Vehicle,  Soybean,  German credit,  Image,  Ecoli,  Votes,  Liver,  Letters,  Sat-images,  Zip-code,  Waveform,  Twonorm,  Threenorm,  Ringnorm,  [@Breiman_2001,12].

In some ways, the improbable conjunction of spam email and cancer detection in machine learning continues what statistics as a field has always done: rove  across scattered fields ranging from astronomy to statecraft, from zoology to epidemiology, gleaning data as it goes (see [@Stigler_1986; @Hacking_1990] for samples of itineraries). \index{statistics} 

In _Elements of Statistical Learning_ and the field of machine learning more generally, something more is moving through and coordinating these datasets. The coordination might be rhetorical: the colligation of datasets -- vowels, ozone, bone density, marketing, prostate cancer, and spam -- in all their diversity  suggests the mobility of machine learners. The combination of datasets deriving from network media, from medicine, from business administration and from cutting-edge life science (c.2000) suggests a tremendous, indeed almost spectacular miscibility, one that in principle could surprise us because there is otherwise little mixing between the settings and knowledge domains these datasets come from. How is this mixing, conformation and homogenisation being done? The repetition of data sets, the juxtaposition of discontinuous domains, and forms of movement construct and order continuities in the service of various forms of predictive and inferential knowledge. The miscible juxtapositions we encounter in machine learning enter into, it seems, a _regularity_ or a common space, a space that displays strong tendencies to expand, accumulate and archive relations. Rather than peripatetic learning, the accumulation of diverse datasets attests to a prior ordering of data to afford its traversal. 


The practices of naming and ordering, the sorting of different data types, the addressing and expansion of data  exemplified in these diverse datasets can tell us a lot about how machine learning  learns from data. \index{learning!from data}  The shapes, compositions and loci formed  from the datasets enable the functioning of machine learners as they operate to generate statements, classifications and decisions.  If machine learning can be understood as a constantly evolving diagram of practical abstractions,  the way it draws on and relates to different forms of data matter.  As we will see, machine learners transpose data in an increasingly extensive, heavily coordinated space, the vector space.  


# Truth is no longer in the table?

'This book is about learning from data' write Hastie, Tibshirani and Friedman on the first page of _Elements of Statistical Learning_, as they rapidly begin to iterate through some datasets. \index{_Elements of Statistical Learning_!on learning from data} On the second page of the book, a table of spam email word frequencies appears (and the problem of spam classification is canonical in the machine learning literature - we return to in chapter \ref{ch:probability}). \index{dataset!spam} They come from the dataset `spam` [@Cranor_1998]. On the third page, a complicated data graphic appears (Figure 1.1, [@Hastie_2009, 3]. It is a scatterplot matrix of the `prostate` dataset included in the `R` package `ElemStatLearn,` the companion `R` package for the book. In a third example, a set of scanned handwritten numbers appears. These scans are images of zipcode or postcode numbers written on postal envelopes taken from the dataset `zip` [@LeCun_2012], and they differ from both the `spam` table and `prostate` plots because they directly resemble something in the world, which, however, happens to be numbers, and is, therefore, probably already recruited into data-making and data-circulating processes (a dataset we return to in chapter \ref{ch:subjects}).\index{dataset|zip}  The final example in the introduction, 'Example 4: DNA Expression Microarrays,' draws from biology, and particularly, high-throughput genomic biology, a science that produces large amounts of data about biological processes  by running many tests, or by constructing devices that generate many measurements, in this case, a DNA microarray.[^2.10]


The table or the row-column addressable grid is common to all of these datasets. And yet, as we are about to see, machine learning in many ways deals with the collapse or liquidation of tabular datasets. 'Things, in their fundamental truth,' writes Foucault in _The Order of Things_ 'have now escaped from the space of the table' [@Foucault_1992, 239]. \index{Foucault, Michel!on table} \index{table!history |(} Foucault  writes in these pages about the fabled emergence of life, labour and language as the anchoring vertexes of a new triangle of knowledge and power structuring the figure of the 'human' in the 19th century.

Before the emergence of the characteristic sciences of the human -- political economy, linguistics and biology -- knowledges such as natural history, the general grammars, and philosophies of wealth (such as Adam Smith's work)  had ordered empirical materials of diverse provenance in tables or grids. While the history of tables as data forms reaches a long way back (see [@Marchese_2013] for a broad historical overview that reaches back to Mesopotamia),  Foucault argues that the Classical age first developed the system of grids that permitted ranking, sorting and ordering in tables. These grids replace the Renaissance tabulations based on 'buried similitudes' and 'invisible analogies' [@Foucault_1992, 26]. In pre-Classical tables, an image or figure from myth might lie alongside a measurement or a count of occurrences, and this proximity was ordered by systems of analogical association that spanned what we might today, in the wake of the 19th century, might see as incongruous (for instance, associations between medicine and Biblical prophecy). 

The Classical Age grid or table, by contrast, brought plural and diverse resemblances into exhaustive systematic visible enumeration. As Foucault puts it:

> The space of order, which served as a _common place_ for representation and for things, for empirical visibility and for the essential rules, which united the regularities of nature and the resemblances of imagination in the grid of identities and differences, which displayed the empirical sequence of representations in a simultaneous table, and made it possible to scan step by step, in accordance with a logical sequence, the totality of nature's elements thus rendered contemporaneous with one another [@Foucault_1972, 239]

The table as space of order did not stand in isolation. It served a localized epistemic function in conjunction with other of knowledge such as experiment and mathematical proof. Since algebra or experimental _mathesis_ only applied to 'simple natures' (planets in movement, dynamics of falling bodies, etc.), \index{mathematics!application to nature} table-based knowledges such as taxonomy dealt with more complex natures. In the tables, systems of signs -- for instance, the groupings established by the eighteenth century taxonomist Carl Linnaeus -- sought to reduce complex natures (plants, animals, etc.) to simpler forms as columns and rows in a table based on resemblances and similarities. \index{Linnaeus, Carl} \index{differences!taxonomy of}  Importantly, the table as space of order was a space of imagination in that one could begin to see continuities and differences between things (organisms, words, nations)  by carefully ordering and scanning the table. 'Hedged in by calculus and genesis,' Foucault suggest, 'we have the area of the _table_' [@Foucault_1992, 73]. \index{calculus} Note in passing that calculus  and calculations bound the table only in relation to 'simple natures' whose identity and difference can be understood in the form of movements, rates and change in position. This is an important limitation since it is precisely the complex natures in genesis that machine learning tries to engage with using algebra, calculus, statistics and computation.

Finally (at least for our purposes), in the nineteenth century, a different form of ordering shattered tabulation based on enumerated similarity and ordered resemblance. Foucault figures this change as shattering the table into shards of order:

>this space of order is from now on shattered: there will be things, with their own organic structures, their hidden veins, the spaces that articulates them, the time that produces them; and then representation, a purely temporal succession, in which those things address themselves (always partially) to a subjectivity [@Foucault_1992, 239-240].

Life, labour and language -- Foucault's famous historically emergent triadic figure of the human  -- replace the enumerative, synoptic classificatory tables of the Classical age. Tables still abound in newer temporal, genetic orderings (almost any episode from the history of nineteenth and twentieth century statistics will confirm that; see [@Stigler_1986; @Stigler_2002])\index{statistics!history}, but from now on tables are localized, relating to a place and changing in time, and functioning as shards of representational order addressed to (and constitutive) of a subjectivity. \index{subject position!human!histrical constitution of} A double interiority takes over. Things such as a language, a species or an economic system have their own genesis. Our knowledges and indeed experience of them become finite, historical, with their own dynamics and internal life. In this change, the table itself is no longer the foundation or distillation of knowledge. It is one knowledge apparatus amongst many. \index{table!history|)} We might say, tables become data, inscriptions pendant on hidden structures and their genesis. 

This brief résumé of a thread of argument in _The Order of Things_ might help us reassess how machine learning  goes into the data. The tables of `spam`, `prostate`, `image` and `microarray` data in contemporary machine learning do not operate in the way Linnaeus would have tabulated a table of living things based on similarities and resemblances or a Renaissance medical textbook might assemble analogical resemblances between disease and astronomy. But, as I argue, measures of similarity and resemblance operate strongly in machine learning as it moves through tables. In this respect, the Classical table and perhaps even the pre-Classical table returns with extended relevance.  The repeated juxtaposition of tables of diverse provenance alongside each other, and the  operational superimposition of different tables suggests machine learners scan for alignments and resemblances between whole tables as well as between particular cells, rows, columns or margins of a table.  As we have seen, the tables in the opening pages of _Elements of Statistical Learning_ concern work, life, language and economy and  map very readily onto the anchor points, the new 'empiricities' (Foucault's term for the empirical problems),  of labour, life and language or biopower \index{biopower} that took root at this time [@Foucault_1991]. \index{empiricities} 

Things may no longer be hedged in by the table, and their fundamental truth may have escaped from the space of similarities found in the Classical table. Instead,  we are now confronted by transmuted tables whose expansion and open margins derives from  calculations of similarity and difference. In many practical respects, we are hedged in by tables, and many different mechanisms animate and multiply tables around us.[^2.22] In certain respects the tables we see in the first pages of [@Hastie_2009] appear familiar in their scale and in their relatively immutability, yet even in the handwritten digits and the microarray data, scale and dimensions block display of the data.   In the settings such as social media platforms or genomics in which machine learning operates, tables change rapidly in scale and sometimes in organisation. Despite differences between the tables of data used to teach  machine learning techniques and data tables used in contemporary operational environments, the multiplication and juxtaposition of tables suggests that we might be seeing the advent of a post-order space for regularities and resemblances, for nature and imagination, or science and media. 

# The epistopic fault line in tables

```{r ElemStatLearn_data, echo=TRUE}
library('ElemStatLearn')
datasets = data(package='ElemStatLearn')
datasets[['results']][,3]
```

The `ElemStatLearn` `R` package brings, as we have seen in the previous chapter, with it around 20 different datasets, including the four found in the introduction to _Elements of Statistical Learning_.  On the one hand, every data table indexes a localised complex of activities (clinical, social media platform, financial transactions, etc.) with possible referential importance.\index{referentiality}  On the other hand, for machine learners, the table is a space of potential similarities and differences concerning both the table itself (e.g. how much does row number 1000 differ from row 1,000,000) and other tables (e.g. how much does this table of clinical test relate to that table of microarray data?). \index{differences!between datasets}  These internal and external differences entwine with each other in ways that create a fault line, an unstable yet generative line of diagonal movement. It is this fault or fold line, I propose, that diagrammatically distributes data tables into the expansive and moving substrata of the  vector space. \index{diagrammatic movement} 

The table has been vectorised. The vectorisation of the table is epistopic.   The term 'epistopic' \index{epistopic} comes from the work of the science studies scholar Mike Lynch \index{Lynch, Mike!epistopics} whose account of scientific practice is particularly focused on ordinariness\index{practice!scientific}. Akin to what   Foucault in the _Archaeology of Knowledge_  terms a threshold of epistemologization [@Foucault_1972, 195] \index{epistemologization!threshold of} Lynch characterizes the 'epistopic' as a way of connecting  localized practices ('topics')  with 'familiar themes from epistemology and general methodology' in the local achievement of coherence in knowledge [@Lynch_1993, 280]. \index{knowledge!local coherence of} In other words, as the term itself suggests, an epistopic connects general epistemic themes such as validity, precision, specificity, error, confidence, expectation, likelihood, uncertainty, or approximation with a place, a 'local complex of activities' (281). \index{machine learning!statistical aspects}  This emphasis on epistemic location frames the problem of what happens when the 'local complex' of a specific dataset encounters a generalizing epistemic practice such as machine learning.   \index{epistemology}

# Surface and depths: the problem of volume in data 

The local complex of activities for mixing of dataset shatters tables from a different direction than that described by Foucault in _The Order of Things_. Algebra,  linear algebra in particular, organizes and distributes differences in vector space. _Mathesis_ in the form of algebraic operations of addition and multiplication of  collections of tabular elements, now re-defined as vectors, re-structure tabular data as a vector space, as a 'set' whose membership is only limited by the applicability of the constructing relations. These operations absorb and subtend differences in quality, type, kind and quantity. While vector space is operational, it meets frictions in both the data and in computational infrastructures by ramifying the data in different diagmrammatic moves.     \index{vector space!ramification in}

```{r prostate_plot, echo=TRUE, message=FALSE, results = 'hide', fig.show='hide'}
library('ElemStatLearn')
data('prostate')
pairs(prostate[, -10],  cex=0.2)
```

```{r prostate_data, results='asis'}

library(xtable)
print(xtable(head(prostate, 3), label= 'tab:prostate', caption='First rows of the \`prostate\` dataset'))
```

\begin{figure}
  \centering
      \includegraphics[width=0.9\textwidth]{figure/prostate_plot-1.pdf}
        \caption{Scatter plot matrix of prostate data}
  \label{fig:prostate_plot_matrix}
\end{figure}

Of the three example datasets  (`prostate`, `spam` and `zip`), _Elements of Statistical Learning_  returns most frequently to `prostate.` This dataset derives from the work of urologists working at Stanford [@Stamey_1989], and concerns various clinical measurements performed on men who were about to undergo radical prostatectomy. The measurements range across the volume and weight of the prostate, as well as levels of various prostate-related biomarkers such as PSA -- prostate specific antigen. Several rows from the dataset are shown in Table \ref{tab:prostate}. The first pages of the book had already exhaustively plotted all the variables in the dataset against each other using the table-related form of a scatter plot matrix [@Hastie_2009,3] (shown in figure \ref{fig:prostate_plot_matrix}, and they return to the same data on almost a dozen occasions in the course of the book, subjecting it to repeated vectorization. \index{dataset!prostate} \index{graphics!scatterplot matrix}

The contrast between the Table \ref{tab:prostate} and the Figure \ref{fig:prostate_plot_matrix} already depends on a transformation intrinsic to vectorisation.  On the one hand, the table arrays all different data types in rows and columns. In the table, the relation between the different data types (the log of the weight of prostate - `lwp` and `age` for instance) is quite hard to see. Moreover, different kinds of variables stand side by side. `svi`, short for 'seminal vesicle invasion' is a categorical variable. \index{data!type!categorical} It takes the values 'true' or 'false,' shown here as `1` or `0`, but the other variables either measure or count things (years, sizes, or levels of antigens). 

On the other hand, the scatter plot matrix also takes the form of a grid-like figure, but the cells of the grid are not occupied by numbers but by `x-y` plots of different pairs of variables in the `prostate` dataset. The 'matrix' of figures shows of 72 plots is mirrored across the diagonal that runs from the top-life to bottom right in figure \ref{fig:prostate_plot_matrix}. Taking this folding into account, we see 36 unique plots with different data in each one. Each sub-plot displays the relation between two variables in the dataset as a scatter plot. Some variables such as `svi` are not very amenable to plotting in this way. More importantly perhaps, certain combinations of variables appear as flat loci that can be read as signs of relations between different variables. The scatter plot matrix constructs a tabular space in which relational contrasts between pairs of variables start to appear. In the light of these contrasts (and I use 'light' here in an almost literal sense to refer to the way in which the architecture of the figure creates a space in which light scatters in varying patterns), the `prostate` dataset begins to expose relations that might be worth knowing about. We have moved on from the bare table of the dataset to a transformed tabulation, from a textual-numerical grid to a geometrical-numerical grid. Everything remains on the surface of a grid here, but the grid permits differences in relationality to begin to appear. \index{differences!visibility in data}

HERE
All of this somewhat precedes the operational formation of machine learning. Similar tables and plots are part and parcel of statistical data exploration more generally (see [@Beniger_1978] for an historical account of quantitative graphics in statistics \index{statistics!graphics|seealso {graphics}). But the point of the scatterplot matrix is not to exhaust the dataset, but rather to highlight the need to constantly revisit it (12 times in the _Elements of Statistical Learning_) in order to tease out the multiple relations or influences that remain opaque to even the most exhaustive matrices of plots. We cannot clearly see in the scatterplot matrix beyond pairs of variables in relation. If the crucial diagnostic measure in this case is the level of the PSA (prostate specific antigen), how do we know what combinations of other measurements might be associated with its elevation? What is multiple variables affect the level of PSA?[^2.31] This question can be pursued by scanning the matrix of plots but not very stably since different data analysts might see different associations combining with each other there. Different statements could be supported by the same figure. Perhaps worse, the very question of relation between multiple variables and the predicted levels of PSA suggests the existence of a hidden volume, an occluded or internal space that cannot be seen in a data table, or that cannot be brought to light in the geometry of a plot. This volume is not the measured volume of the prostate but the virtual volume suggested by both the dataset table and the scatterplot matrix, a nine dimensional space subtended by treating each of the nine variables in the dataset as occupying a dimension. \index{common vector space!dimension}  When Foucault wrote of truth escaping the table, he might well have pointed towards the higher-dimensional volumes of the _vector space_ into which lines were already beginning to regress even in the late 18th century.[^2.33]. A different basis of order -- the common vector space -- \index{common vector space|(}  begins to take shape when the variables (usually columns) become dimensions.  Strenuous efforts are made in machine learning to ensure that as much as possible goes into and comes of that common space. 

# Vector spaces expand

To show this space in its epistopic making, we might bfollow what happens to just one or two columns of the `prostate` data in the common vector space as it is prepared for machine learning. In the `prostate` dataset, some variables are continuous quantitative values, and some are categorical (they represent membership in a group or category) or ordinal variables (they represent a ranking or order). But a variable such as `svi` has `True` or `False` values. How can such values be positioned in the vector space, and thus be subject to calculation in the same way as a measure of prostate gland volume? In order to put classifications or categories into vector space, they need to be translated into the same _basis_ as the quantitative variables with their rather more obvious geometrical and linear coordinate values. \index{classification} How does one geometrically or indeed algebraically render a category so that it can be mobilised in the way that equation \ref{eq:linear_regression} (see chapter \ref{ch:diagram}), the vector form of the linear regression model, suggests it might be? \index{linear regression model} The problem is solved via a form of binary coding:

>Qualitative variables are typically represented numerically by codes. The easiest case is when there are only two classes or categories, such as “success” or “failure,” “survived” or “died.” These are often represented by a single binary digit or bit as 0 or 1, or else by 1 and 1 ... When there are more than two categories, several alternatives are available. The most useful and commonly used coding is via dummy variables. Here a K-level qualitative variable is represented by a vector of K binary variables or bits, only one of which is “on” at a time [@Hastie_2009, 12] 

Again, the details are not so important here as the transformations that the common space permits once things inhabit it. Note that a single qualitative or categorical variable expands into 'a vector of K binary variables or bits.' \index{data!variable!qualitative|seealso{data!variable!categorical}} The dimensions of the vector space expand accordingly, and the machine learners  treat these added dimensions as variables to be included in the model. Qualitative data, once coded in this way, can be multiplied, added, and in short, handled algebraically using the same aggregate operations we saw in discussing linear algebra more generally. Note also that not only has the vector space expanded here, this expansion smooths over important gaps or differences that figure large in the dataset. Complex natures become simple natures. The different kinds of variables -- qualitative and quantitative, discrete and continuous, nominal and ordinal -- can be accommodated by adding dimensions to the vector space. \index{common vector space|)}

# Traversing behind the light 

Adding dimensions to the common vector space makes seeing the volumes and densities of data distributed in this space more challenging.   The character of the transformations in `prostate` that ensue in _Elements of Statistical Learning_ are difficult to summarise. They are the locus of machine learning as an epistopic form of movement. \index{machine learning!epistopic}  In terms of the historical transformations of the table, they can be seen as subtending differences in a common vector space of simple natures understood as things with direction and displacement, and hence susceptible to calculation as vectors. \index{common vector space!dimension}  Once this hidden virtual volume in the data is glimpsed, strenuous efforts will be made to bring it to light, even sometimes blatantly disregarding the local complex of activities that originally produced the data. These efforts will proceed along different lines. Sometimes this space is treated as one filled with constantly varying proximities and similarities, and machine learning techniques gather and order these differences (for instance, as in the _k_ nearest neighbours model \index{\texit{k}-nearest neighbours model}) or in unsupervised methods such as k-means clustering [@Hastie_2009, 513]\index{\testit{k}-means clustering}.  More commonly, machine learning draws lines or flat surfaces that con-strain through the volume using some version of linear regression, perhaps the most important modelling technique in modern statistics\index{linear regression}. \index{common vector space|strain}

\index{linear model} The importance of the linear modelling, finding lines of best fit,  should not be under-estimated amidst the plethora of machine learners that attract more recent attention (neural nets, support vector machines, random forests). Linear regression lays down the basis of common vector space that renders all differences as distances and directions of movement. Drawing lines or flat surfaces at various angles and directions is perhaps the main way in which the volume of data is traversed, and a relation between input and output, between predictors and prediction, consolidated.[^2.351] While it is more or less obvious to the eye from the scatterplot matrix that lines could be drawn through the clouds of points as a way of defining directions of movement, the way to draw  these lines in higher dimensions is not obvious. Yet the line of best fit has a readily graspable geometrical intuition to it, even in higher dimensions, and that line can be diagrammed by making use of the equations of linear algebra, the field of mathematics that operates on lines in spaces of arbitrary dimensions. \index{linear algebra|(}  It is no accident that linear algebra is such a taken-for-granted part of  machine learning that its techniques for finding intersections between lines and planes, of manipulating collections of lines and surfaces through mappings and transformations (rotations, displacements or translations, skewing, and scaling), and above all, treating systems of equations using aggregate forms such as matrices and vectors. It brings with it a set of formalisations -- vector space, dimension, matrix, determinant, coordinate system, linear independence, eigenvectors and eigenvalues, inner-product space, etc. -- that machine learners resort to constantly.[^2.36]

Many of these formalisations quickly become difficult to geometrically figure. We will have reason to examine some important ways in which linear algebra structures machine learning practices in later chapters, but for the moment, it might be understood as offering a way to draw lines through spaces that can only be expressed diagrammatically in the form of equations, not in the geometrical form of figures.[^2.41b] Let us return to the equations for linear regression models a third time (remembering that both C.S. Pierce and Andrew Ng advocate returning often to simple expressions). The 'mainstay of statistics,' the linear regression model, usually appears diagrammatically in the form of linear algebra: 

\begin {equation}
\label {eq:linear_model}
\hat{Y}=\hat{\beta_0}  + \sum^p_{j=1}X_j\hat{\beta_j}
\end {equation}


\begin {equation}
\label {eq:linear_model_vector}
\hat{Y} = X_T\hat{\beta}
\end {equation}

The concision of this way of diagramming the drawing of line through a high dimensional space derives largely from linear algebra. Reading Equation \ref{eq:linear_model} from left to right, the expression $\hat{Y}$ already points to a set of calculated, predicted values, or a vector of $y$ values, such as all the `lpsa` or PSA readings included in the `prostate` dataset. Similarly, the term $X_j$ points to the table of all the other variables in the `prostate` dataset. \index{dataset!prostate} Since there are 8 other variables, and close to 100 rows, $X$ is a *matrix*  -- a higher dimensional table -- of values, addressable by coordinates. Finally $\beta_j$ are the pivotal coefficients or multiplying quantities that determine the slope or direction of the lines drawn. \index{deep learning} The second expression Equation  \ref{eq:linear_model_vector} relies more fully on linear algebra. This is the linear model written in 'vector form' [@Hastie_2009, 11]. The right hand side comprises two operations $X^T$, the transpose or rotation of the data, and implicitly -- multiplication is hardly ever shown, but diagrammed by putting terms alongside each other -- an _inner product_ of the $X$ matrix and the $\beta$ parameters (to use model talk) or coefficients (to use linear algebra talk). \index{parameters|seealso{model!coefficient}}

So, in the expression for Equation \ref{linear_model} we can begin to grasp the diagramming of a line that cannot be fully drawn in any figure, only projected onto the dimensions of a plot. In spite of these limitations, it can be readily imagined and conceptualised within the expandable dimensions of vector spaces. While that line can never fully come to light, the diagrammatics of equations \ref{eq:linear_model} and \ref{eq:linear_model_vector}  expresses a way of constructing it calculating the coefficients $\hat\beta$. Much effort, many techniques, and sub-fields of science attach themselves to various ways of constructing and making statements about such lines. The problem of calculating optimal values of the coefficients $\beta$ \index{model!coefficient} has attracted and continues to attract the attention of statisticians for a long time (at least two centuries) and today lies at the heart of cutting machine learners such as deep learning neural nets.  Such expressions are epistopic in that they connect the local complex of activities indexed as tabulated data together through the diagonal diagrammatic element of a line or plane angling through common vector space. This diagrammatic element is not necessarily reductive, although it does render differences as distances in vector space.\index{common vector space|distance} Between the statement of the data and the system of coefficients defining a fitted surface, the diagrammatic expression of the linear model creates a new kind of anamorphic or diagonal space, 'constituting hundreds of points of emergence or creativity, unexpected conjunctions or improbable continuums,' as Deleuze puts it in describing such diagrams [@Deleuze_1988,35].\index{Deleuze, Gilles!diagrams} 

# Drawing lines in a common space of transformation

The common vector space generated by the epistopic operation of machine learners has several facets. First of all, it invites mathematical analysis in terms of matrix operations and linear algebra \index{common vector space!linear algebra|seealso{linear algebra}}. In high school mathematics, at least since WWII, students have been taught how to solve systems of linear equations, first using algebra, and then using matrix operations. Solving a system of linear equations means finding those values of the variables that satisfy all the equations in the system. If such values can be found, we know that the lines expressed by equations of the form $y = ax_1 + bx_2 + c$ intersect at a point, along a line, in some sub-space. In all of these cases, diligent mathematics students solve to find the common space (shown in figure \ref{fig:common_space}), even if it consists in a single point, inhabited by the elements of the system.

\begin{figure}
  \centering
      \includegraphics[width=0.5\textwidth]{figure/intersection.pdf}
        \caption{Common space in linear equations}
  \label{fig:common_space}
\end{figure}

Similarly, although the mathematics is slightly more complicated than high school level (although not by much), drawing the line of best fit through a set of data points can be seen as solving a system of linear equations. \index{linear algebra|)} Viewed in terms of linear algebra, the 'solution' -- or 'closed form solution' -- to the linear model is given in equation \ref{eq:linear_model_solution}:

\begin {equation}
\label {eq:linear_model_solution}
\hat{\beta} = (\mathbf{X}^T\mathbf{X})^-1\mathbf{X}^T\mathbf{y}
\end {equation}

In this expression, the data shown as $\mathbf{X}$ supports the calculation of the coefficients $\hat{\beta}$ that define a flat surface or plane cutting across the common vector space.[^2.38a] The derivation of the analytical 'ordinary least squares' solution \index{ordinary least squares} relies on some differential calculus \index{calculus!differential} as well as a range of linear algebra operations such as matrix transpose, inner product and matrix inversion, the details of which need not trouble us here. (As usual, and in keeping the diagrammatic reading of these forms, these operations all consist of ways of either moving or combining numbers in practice.) The relevant point is that equation \ref{eq:linear_model_solution} calculates some values for the linear regression model coefficients $\hat\beta$ and therefore assigns a slope and intercept, a direction and displacement to a line or flat surface traversing the density-shape of a dataset in its full dimensional vector space (nine dimensions in the case of `prostate`).[^2.38] 

# `-ply`: vectorisation and the transformation of common space 

The common vector space \index{common vector space!vectorisation} appears and operates just as directly in code, especially in `R` and other programming languages designed for data practice (Octave, Matlab, Python's NumPy, or C++ Armadillo). _Vectorised_ transformations of data lie are the moving substrate of machine learning as it expands.\index{programming languages}  As usual, `R` coding practice instantiates this in miniature. As a programming language, `R` is striking for its many  *vectorised* constructs and operations. In vectorised languages such as `R`, transformations of a data structure  expressed in one line of code  simultaneously affect all the elements of the data structure. As the widely used _R Cookbook_ puts it, 'many functions [in `R`] operate on entire vectors ...  and return a vector result' [@Teetor_2011, 38]. Or as _The Art of `R` Programming: A Tour of Statistical Software Design_ by Norman Matloff puts it, 'the fundamental data type in `R` is the _vector_' [@Matloff_2011, 24], and indeed in `R`, all data is vector. There are no individual data types,  only varieties of vectors in `R`. Or, as _R in a Nutshell_  puts the point: 'in `R`, any number that you enter ... is interpreted as a vector' or as 'an ordered collection of numbers' [@Adler_2010, 17]. In these operations, all data is a vector. That is, all data operations treat data as a movement with direction and magnitude. This implicitly links data to a spatial dimension, even if it has no obvious spatial index  or reference.\index{common vector space!vector}   There are many vectorised operations in the `R` core language and many to be found in packages (the popular 'plyr' package; versions of 'ply' can also be found in recent Python data analysis libraries such as `numpy` or `pandas` [@McKinney_2012]).  In many cases, these vectorised operations occur implicitly. `R` sometimes presents difficulties for programmers trained to code using so-called procedural programming languages because it so thoroughly embraces the notion of the _vector_ -- and hence, regards all data as inhabiting vector space. In many mainstream programming languages, transformations of data rely on loops and array constructs in which some operation is successively repeated on each element of a data structure. 


```{r vectorisation, echo=TRUE, cache=TRUE, message=FALSE, warning=FALSE, comment=NA, size='smallsize', results='markup' } 
 
     vector1 <- c(0,1,2,3,4,5,6,8,9,10)
     vector2 <- c(0,1,2,3,4,5,6,8,9,10)

     #procedural programming-style looped addition
     result_looped = vector()
     for (i in vector1) {
        result_looped[i] = vector1[i] + vector2[i]
     }
     result_looped

     #vectorised addition
     result_vectorised  <- vector1 + vector2
     result_vectorised
```

The practical difference between the two approaches to moving through data is illustrated in the code listing \ref{vectorisation} above in which two 'vectors' of numbers are added together, in the first case using a classic `for`-loop construct, and in the second case using an implicitly vectorised arithmetic operation `+`. The difference between adding $1 ... 10$ using a loop or vector arithmetic is completely trivial here, but when millions of numbers are handled, these implementation differences significantly affect the work of both programmers and computing machinery.  This simultaneity is only apparent, since somehow the underlying code has to deal with all the individual elements, but vectorised programming languages take advantage of hardware optimisations or carefully-crafted low-level linear algebra libraries.[^2.41] More importantly, this is a different mode of movement. Operations now longer step through a series of coordinates that address data elements, but wield plans, surfaces, cross-sections or stratifications of the vector space. 

A further level of vectorisation appears in the vectorisation of functions in `R`. Specific `R` constructs such as `apply`, `sapply`, `tapply`, `lapply`, `mapply` exemplify these vectorised functions. All of the `-ply` constructs have a common feature: they take some collection of things (it may be ordered in many different ways – as a list, as a table, as an array, etc.), do something to it, and return something else. This hardly sounds startling. But while most programming languages in common use offer constructs to help deal with collections of things sequentially (for instance, by accessing each element of a list in turn and doing something with it), `R` offers ways of expressing a simultaneous operation  on  them all. The `-ply` constructs ultimately derive from the functional logic developed by the mathematician Alonzo Church in the 1930s [@Church_1936;@Church_1996]. \index{Church, Alonzo}  The functional programming style of applying functions to functions seems strangely abstract. \index{programming!functional} These -ply constructs  and the implicit vectorisation of many operations on data structures can be written concisely and tersely.  They  transform data in ways that readily adapt to and indeed motivate  increasingly parallel contemporary chip architectures, clusters of computers, reallocation of computation to GPU (Graphic Processing Unit), FPGA (Field Programmable Gate Arrays) and, perhaps most significantly,  the increasingly Cyclopean infrastructures of cloud computing and associated data centres. \index{cloud computing} Many of these condensing and expanding movements of data are diagrammed in miniature in the `R` constructs as operators in vector space. The `-ply` operations reduce both data and computational frictions. The real stake in -plying data, is not speed but transformation. -plying makes working with data less like iteration (number, text, table, list, etc.), and more like folding a pliable material. Such shifts in feeling for data are very mundane yet crucial to the epistopic movements in data. While the examples of vectorised computation that I have shown are relatively trivial --  adding sequences of numbers -- these vectorised operations exert  both infrastructural and epistemic strains for the common vector space. \index{common vector space!vectorisation!function}  

The infrastructural implication we have just seen:  the implicit and explicit vectorisation of data and operations feeds directly into the scaling up of computational  work on data. \index{common vector space|scaling} \index{common vector space|infrastructure} Not only vectorised computation, but the various styles of programming and infrastructure that lend themselves to simultaneous transformations of large matrices or tables of data lie at the kernel of not only machine learning, but of the intensive processing of data in many different places. The rendering of moving images in  digital video and in computer game animation depends implicitly on vectorised  transformations of matrices of numbers [@Mackenzie_2010a]. The more abstract implications of vectorisation and the forms of movement it encourages and  proliferates bring us back to the problem of what it means for machine learning to learn. \index{machine learning!learning}. In many respects, it entails drawing a line through the data distributed in the common vector space. In short, vectorising computation makes the common vector space, which we might understand as a resurgent form of the table, operationally concrete and machinically abstract. It is no longer a formal diagram, but a machinic process that multiplies and propagates into the world along many diagonal lines.  


# 'We fit a linear model'

'We fit a linear model' write Hastie and co-authors, referring to the `prostate` data. Models constrain movement in the fluxing dimensionality of common vector space. \index{common vector space!dimensionality} In `R` this might look like the code excerpt shown below:

```{r prostate_model, echo=TRUE, results='asis'}
library(ElemStatLearn)
library(xtable)
data(prostate)
columns_to_standardize = c(1,2,3,4,6,8,9)
prostate_standard = as.matrix(prostate[, columns_to_standardize]) 
prostate_standard = as.data.frame(scale(prostate_standard))
prostate_standard = cbind(prostate_standard, gleason=prostate$gleason, svi = prostate$svi, train = prostate$train)
train = prostate$train ==TRUE
prostate_model = lm(lpsa~., prostate_standard[train,-10])
tab1 = xtable(summary(prostate_model), caption = 'Fitting a linear model', label='tab:prostate_linear_regression', type='latex')
print(tab1, include.rownames = FALSE)
```

The epistopic character of Table \ref{tab:prostate_linear_regression} surfaces around the coefficients  $\hat{\beta}$  that define the direction of a flat surface running through the common vector space of the `prostate` data.[^2.101] \index{dataset!prostate} This vector is a product of operations in that space. As Hastie and co-authors write, the 'unique solution' to the problem of fitting a linear model to a given dataset using the most popular method of 'least squares' [@Hastie_2009, 12] is given by the operations we have seen in equation \ref{eq:linear_model_solution}. This tightly coiled expression calculates the parameters $\hat{\beta}$ for a linear model. The two matrices involved are the $X$ input variables (all of the `prostate` variables apart from the variable chosen as the response variable, in this case `lpsa`, the log of the PSA level) and a 1-dimensional matrix $y$, the `lpsa` values. These matrices are multiplied, transposed (a form of rotation that swaps rows for columns) and inverted (a more complex operation that finds another matrix). Epitomising the vectorised code often used in machine learning, calculating $\hat{\beta}$ for  the `prostate` data only requires one line of `R` code:

```{r beta_hat, eval = FALSE}
beta_hat = ginv(t(X) %*% X) %*% t(X) %*% y
```

The implicit vectorisation of the `R` code in the code listing \ref{beta_hat}, the fact that it already concretely operates in the common vector space, operationalizes the concise diagrammatizing of equation \ref{eq:linear_model_solution} as a machine process. \index{common vector space!vectorisation} More importantly, the vector of values that result from the vectorised multiplication, transposition and inversion of matrices creates the new vector defined by $\hat{\beta}$ whose estimated values will be subjected to the statistical tests of significance, variation, and error that we see in Table \ref{tab:prostate_linear_regression}. We will have occasion to return to these statistical estimates (in chapter \ref{ch:probability}), since the play of values that starts to appear even in just fitting one model will become much more significant when fitting hundreds or thousands of models, as some machine learners do.[^2.300] 


# The vectorised table?

>The table has the function of treating multiplicity itself, distributing it and deriving from it as many effects as possible [@Foucault_1997, 149] \index{table} \index{Foucault, Michel}

In  *Surveiller et Punir* or *Discipline and Punish* [@Foucault_1977], Foucault returned to the question of the table in a slightly different context: an account of the operation of power in disciplinary institutions and knowledges. In these knowledges, the table becomes generative of 'as many effects as possible.' A version of this generative function appears in the machine learning shown by equations \ref{eq:linear_model} and  \ref{eq:linear_model_solution} and in the code listing \ref{beta_hat}.  In the construction of the common vector space in linear algebra, and then its practice in code rather than solely in the analytical space of equations, machine learning vectorises new volumes or dimensionalities that could not be seen, or rendered in either tables of data or graphic figures of data. Every   machine learner inhabits and moves through the common vector space. Sometimes their operations flatten the vector space down into lower dimensions, sometimes they expand the vector space into a great many new dimensions (as we saw with 'dummy variables' that embody categories, and as we will see with support vector machine classifiers in a chapter \ref{ch:pattern}). 

The generalized application of machine learning relies on and reinforces a broad, powerful yet quite subtle transformation of data into vectors and a vector space. It remaps the grid of the table into the expanding dimensions of the common vector space. This is not the first such expansion of the table. We need only think of the relational database systems of the late 1960s, and their multiplication of tables [@Mackenzie_2013b]. But perhaps in the vectorised and matrix-form practices of the  common vector space, machine learners produce for the first time a quasi-tabular or meta(s)table volume that cannot be surfaced on a page or screen, yet is heavily  diagrammatically traversed through vectorised operations.  \index{table} \index{common vector space!metatable}

Does machine learning learn from the data? It certainly puts learning into the data. The epistopic transformation of the table pulls and re-aligns communication and infrastructures. It acts as a powerful tensor on knowledges and operations of many different kinds since it transposes, inverts, and re-maps local complexes of activity. In following what happens to vectors, lists, matrices, arrays, dictionaries, sets, dataframes, or series or tuples in data, we might get a sense of how the epistemic operations of predictive models, the supervised and unsupervised learners, the classifiers, the decision trees and the neural networks fold and refold matrices and vectors in a common vector space. What is at stake in the common vector space? The broad stake, at the risk of over-emphasising it, is the production of new kinds of realities occasioned by the mobility of machine learners. \index{machine learner!mobility of}  The machine learner recognising zip codes might be found in the navigation system of an autonomous vehicle or the methods section of a paper published in *Nature.* The datasets that _Elements of Statistical Learning_ ranges sides by side evince this improbable diagonal movement. 

The  challenge lies in attending to this expansive data vectorisation in ways that maintain and indeed augment its value-relevance, its concreteness, and attachments to lives and places. We lack good intuitions of how to do that because of the ways in which data as vector space has been constructed and described. \index{machine learning!epistopic} Does the act of unwinding some of these operations, for instance, by seeing how  matrix multiplication ripples through different treatments of data in a linear regression model (even such an small one as the `prostate` data), bring us closer to the vitality of the multiple/multiplying concrete value-situations that connect calculation?  My suggestion is that the act of diagramming how machine learners  vectorise data densities  begins to locate and unravel the processes of knowing, predicting and deciding on which many aspects of the turn to data rely.  As we will see, the vectoral operations we have just been viewing are themselves organised by other lines of diagrammatic movement \index{diagrammatic movement} that shape surfaces in more convoluted forms.    



[^2.38]: As we will see in the following chapter (chapter \ref{ch:function}), it is not always possible to calculate the parameters of a model analytically. Especially in relation to contemporary datasets that have very many variables and many instances (rows in the table), linear algebra approaches become unwieldy in their attempt to produce exact results, and machine learning steps in with a variety of computational optimisation techniques. \index{coefficients}

[^2.38a]: Perhaps more importantly, the linear algebraic expression of these operations presupposes that all the data, both the values used to build the model and the predicted values the model may generate as it is refined or put into operation somewhere, are contained in a common space, the vector space, a space whose formation and transformation can be progressively ramified and reiterated by various lines that either separate volumes in the space, or head in a direction that brings along most of the data. Not all of these lines are bound to be straight, and much of the variety and dispersion visible in machine learning techniques comes from efforts to construct different kinds of lines or different kinds of 'decision boundaries' (in the case of classification problems) in vector space (for instance, the k-nearest neighbors method does not construct straight lines, but somewhat meandering curves that weave between nearby vectors in the vector space; see [@Hastie_2009, 14-16]). Whether they are straight or not, the epistopic aspect of these lines remains prominent. Typically, many different statistical tests (Z-scores or standard errors, F-tests, confidence intervals, and then prediction errors) will be applied to any estimate of the coefficients of even the basic linear regression model, well before most advanced or sophisticated models and techniques (cross-validation, bootstrap testing, subset and shrinkage selection) begin to re-configure the model in more radical ways. \index{statistics!tests}


[^2.10]: Some genomic data will be the focus of a later chapter \ref{ch:genome}. Machine learning during the 1990s and 2000s was in some ways boosted heavily by the advent of genomic biology with its large, enterprise style knowledge endeavours such as the Human Genome Project.

[^2.11]: The data derives from a publication in _nature genetics_ [@Ross_1916_2000] analysing gene expression in the cancer cell lines maintained as experimental models by the us national cancer institute.

[^2.41]: Learning machine learning, and learning to implement machine learning techniques, is largely a matter of implementing series of matrix multiplications. As Andrew Ng advises his students,

    > Almost any programming language you use will have great linear algebra libraries. And they will be high optimised to do that matrix-matrix multiplication very efficiently including taking advantage of any parallelism your computer. So that you can very efficiently make lots of predictions of lots of hypotheses [@ng_2008a, 10:50]\index{Ng, Andrew}

    In other parts of his teaching, and indeed throughout the practice exercises and assignments, Ng stresses the value of implementing machine learning techniques for both understanding them and using them properly. But this is one case where implementation does not facilitate learning. Ng advises his learners against implementing their own matrix handling code.    They should instead  use the 'great linear algebra libraries' found in 'almost any programming language.' 'linear algebra libraries' multiply, transpose, decompose, invert and generally transform matrices and vectors. \index{linear algebra} they will be 'highly optimised' not because every programming language has been prepared for the advent of machine learning on a large scale, but rather more likely because matrix operations are just so widely used in image and audio processing.  Happily, Ng observes, that means that 'you can make lots of predictions' [@Ng_2008e].   It seems that generating predictions and hypotheses outweighs the value of understanding how things work on this point. \index{prediction}


[^2.351]: One sign of the centrality of the line in machine learning can be seen, for instance, from the contents page of the book [@hastie_2009, xiii-xxii]. After the introduction of the linear model in the first chapter and its initial exposition in chapter 2 ('overview of supervised learning'), it forms the central topics of chapter 3 ('linear methods for regression'), chapter 4 ('linear methods for classification'), chapter 5 ('basis expansions and regularization'), chapter 6 ('kernel smoothing methods'), much of chapter 7 ('model assessment and selection'), chapter 8 ('model inference and averaging'), major parts of chapter 9 ('additive models, trees and related methods'), important parts of chapter 11 ('neural networks' -- neural networks can be understood as a kind of regression model), the anchoring point of chapter 12 ('support vector machines and flexible discriminants') and the main focus in the final chapter ('high dimensional problems'). A similar topic distribution can be found in Andrew Ng's Cs229 lectures on machine learning. More than half of the 20 lectures concern linear models and their variants. See [@Ng_2008a; @Ng_2008b;@Ng_2008c;@Ng_2008d].


[^2.36]: Along with statistics and probability, linear algebra is a such an important part of machine learning that many books and courses recommend students complete a linear algebra course before they study machine learning. Cathy O'Neill and Rachel Schutt advise:
    When you’re developing your skill set as a data scientist, certain foundational pieces need to be in place first—statistics, linear algebra, some programming [@Schutt_2013, 17]

[^2.31]: Despite the intensive work that Hastie and co-authors conduct on the `prostate` data, all with a view to better predicting PSA levels using volumes and weights of prostates, etc., Stamey and other urologists more than a decade or so concluded that PSA is not a good biomarker for prostate cancer. \index{dataset!prostate} Stamey writes in 2004:
    What is urgently needed is a serum marker for prostate cancer that is truly proportional to the volume and grade of this ubiquitous cancer, and solid observations on who should and should not be treated which will surely require randomized trials once such a marker is available. Since there is no such marker for any other organ confined cancer, little is likely to change the current state of overdiagnosis (and over-treatment) of prostate cancer, a cancer we all get if we live long enough unbound points in the matrix [@Stamey_2004, 1301]
\index{dataset!prostate}
[^2.33]: Carl Friedrich Gauss \index{Gauss, Carl Friedirch} and Adrien-Marie Legendre's work on linear regression at this time is well-known. The first independent use of linear regression was Gauss' prediction of the location of an 'occluded volume,' the position of the asteroid Ceres after it reappeared in its orbit behind the sun. [@Stigler_2002] -- TBA page ref


[^2.1]:  Later chapters will discuss various ways in which the vectoral dimensionality of data or its rendering as _vector space_ scales up and scales down in machine learning. In terms of multiplying matrices, dimensionality both constrains and enables many aspects of the prediction. Perhaps on the grounds of data dimensionality alone, we should attend to dimensionality practices in  `R`. A fuller discussion of dimensionality of data is the topic of chapter \ref{ch:pattern}. There I discuss how machine learning re-dimensions data in various ways, sometimes reducing dimensions and at other times, multiplying dimensions.

[^2.22]: By and large, I am not discussing networking and database infrastructures here. In other work, I have attempted to account for the multiplication of tables in databases but also in mundane practices of ordering. See [@Mackenzie_2011; @Mackenzie_2012]. My analysis there pivots mainly on a set-driven account of data derived from the work of Alain Badiou. \index{Badiou, Alain} Although I've already been using some set terminology here -- as in 'dataset' -- I'm persisting with a more geometrical-algebraic account of datasets in order to better deal with some of the spatial oeprations common in machine learning. \index{Badiou, Alain!set-based account of data}

[^2.41b]: We might add also approach the epistopic fault line in machine learning topologically \index{topology}. Over a decade ago, the cultural theorist Brian Massumi wrote that 'the space of experience is really, literally, physically a topological hyperspace of transformation' [@Massumi_2002, 184] \index{Massumi, Brian}. Much earlier, Gilles Deleuze had conceptualised Michel Foucault's philosophy as a topology, or 'thought of the outside' [@Deleuze_1988], as a set of movements that sought to map the diagrams that generated a 'kind of reality, a new model of truth' [@Deleuze_1988, 35]. More recently, this topological thinking has been extended and developed by Celia Lury amongst others. In 'The Becoming Topological of Culture,' Lury, Luciana Parisi and Tiziana Terranova suggests that 'a  new rationality is emerging: the moving ratio of a topological culture' [@Lury_2012, 4] \index{{Lury, Celia}. \index{Parisi, Luciana} \index{Terranova, Tiziana} In this new rationality, practices of ordering, modelling, networking and mapping co-constitute culture, technology and science [@Lury_2012, 5].  At the core of this new rationality, however, lies a new ordering of continuity. The 'ordering of continuity,' Lury, Parisi and Terranova propose, takes shape 'in practices of sorting, naming, numbering, comparing, listing, and calculating' (4). The phrase 'ordering of continuity' is interesting, since we don't normally think of continuities as subject to ordering. In many ways, that which is continuous bears within it its own ordering, its own immanent seriation or lamination. But in the becoming topological of culture, movement itself undergoes a transformation according to these authors. Rather than movement as something moving from place to place relatively unchanged (as in geometrical translation), movement should be understood as more like an animation, a set of shape-changing operations. These transformations, I would suggest, should be legible in the way that machine learning, almost the epitome of the processes of modelling and calculation that Lury, Parisi and Terranova point to, itself moves through the data. And indeed, the juxtaposition of spam, biomedical data, gene expression data and handwritten digits already suggests that topological equivalences, and a 'radical expansion' of comparison might be occurring. Bringing epistopics and topologies together might, I suggest, help trace, map and importantly diagram some of the movements into the data occurring today.

[^2.101]: From the epistopic viewpoint, the most obvious result of fitting a linear model is the production not of a line on a diagram or in a graphic. As we have seen, such lines cannot be easily rendered visible. Instead, the model generates a new table of coefficients (see Table \ref{tab:prostate_linear_regression}) and some new numbers, _statistics_.  This table is not as extensive as the original data, the $\mathbf{X}$ and $Y$ vectors. But the names of the variables in the dataset appear as rows in the new table, a table that describes something of how a line has been fitted by the linear model to the data. The columns of the table now bear abbreviated and much more statistical names such as `estimate` (the estimated values of $\hat{\beta}$, the key parameters in any linear model), `Std. Error`, `t value`, and the all important _p_ values written as `Pr(|t|)`.  The numerical values ranging along the rows mostly range from -1 to 1, but the final column includes values that are incredibly small: `1.47e-06` is a few millionths. Other statistics range around the outside the table: the `F-statistic`, the `R-squared` statistic, and the `Residual standard error`. I don't propose discussing these in any great detail here. No matter how we understand these statistics, they constitute a transformation of the `prostate` dataset. The numbers of the \ref{tab:prostate} become epistopic here, since they now appear as a set of standard errors, estimates, t-statistics, and _p_ values, that together indicate how likely the estimated values of $\beta$ are, and therefore how well the diagonal line expresses the relations between different dimensions of the dataset in the common vector space. How have these statistics, which all act as qualifications and qualifications on the line whose direction and point of entry is given the $\hat{\beta}$ coefficents, arisen? We seem to have crossed some threshold between drawing a line through an occluded data volume and generating propositions about the way that line passes through the data.


[^2.300]: This is an important differentiation: it is not typical machine learning practice to construct one model, characterised by a single set of statistics (F scores, R^2 scores, _t_ values, etc.). In practice, most machine learning techniques construct many models, and the efficacy of some predictive techniques derives often from the multiplication or indeed proliferation of models. Techniques such as neural networks, cross-validation, bagging, shrinkage and subset selection, and random forests, to name a few, generate many statistics, and navigating the multiple or highly variable models that result becomes a major concern. An epistopic abundance will appear here -- bias, variance, precision, recall, training error, test error, expectation, Bayesian Information Criteria, etc. as well as graphisms such as ROC (Receiver-Operator-Characteristics) curves. Put simply, the proliferation of models start to drive the dimensional expansion of the common vector space. At the same time, the multiplicity of models multiplied by the machine learners  becomes the topic of statistical analysis. 


[^2.104]:   I loosely borrow the term 'density' from statistics, where *probability density functions* are often used to describe the hardly ever uniform distribution of probabilities of different values of a variable. Sensing density as a form of variation matters greatly both in machine learning itself, where algorithms seek purchase on unevenly distributed data, and in any broader diagram of data. Probability densities are discussed in much more detail in Chapters \ref{ch:probability}. The collection _"Raw Data" is an Oxymoron_ [@Gitelman_2013] evinces some of these different densities and distributions of data.  


# to use

TO USE IN SECTION ON IDEALISATION

Every distinct column  in a table practically adds a new dimension to the common vector space. Since the 1950s, problems of classification and prediction in high-dimensional dataspaces have been the object of mathematical interest. The mathematician Richard Bellman coined the term ‘the curse of dimensionality’ to describe how partitioning becomes more unstable  as the dimensions of the dataspace increase  [@Bellman_1961]. \index{Bellman, Richard!curse of dimensionality} The problem is that while the volume of a space increases exponentially with dimensions,  the number of data points (actual measurements or observations) usually does not usually increase at the same rate. In high dimensional spaces, the data becomes more thinly spread out. It is hard to partitions sparsely populated spaces because they accommodate many different boundaries. \index{common vector space!dimensionality!curse of}

OTHER STUFF TAKEN OUT


In the epistopic mode of diagramming, machine learners  find edges in data tables that are capable of combining or conjugating with each other, even if they are not reducible to each other. \index{machine learner!epistopic mode}  Let us see how Hastie, Tibshirani and Friedman begin to move through the datasets. First of all, they highlight in the first three (`spam`, `prostate` and `zip`, leaving aside the cancer genes dataset (`nci`)),  what they have in common:

>The first three examples described in Chapter 1 have several components in common. For each there is a set of variables that might be denoted as inputs, which are measured or preset. These have some influence on one or more outputs. For each example the goal is to use the inputs to predict the values of the outputs. This exercise is called supervised learning. We have used the more modern language of machine learning.[@Hastie_2009, 9].

In the ensuing discussion,  they begin to dissect the tables and point to various discontinuities traversing the tables. The major discontinuity is already remarked in the common cut between 'inputs which are measured' and values that might called 'outputs' that inputs 'influence.' Not all tables are cut by machine learners in this way, but this cut is epistopic since the table now shows that some things are measured and some might not be (the values of the outputs). In addition to this vertical discontinuity dividing tables, more diverse horizontal discontinuities can be seen (and Hastie et. al. go on to describe it). Some of the variables in the table are continuous numerical values, others are discrete (they can only take on certain values such as a whole number), and indeed may be categorical or nominal (for instance, when predicting the actual digit from the handwritten digits, the predicted digit itself is a categorical variable, since the prediction must choose one of the digits 0,1,2,3,4,5,6,7,8 or 9). \index{data!variable!types} Discrete qualitative data appears frequently in machine learning, or wherever classification problems can be found. \index{classification} (I will have much more to say about classification and machine learning classifiers in later chapters. For the moment, we need only note that predicting the class to which things belong is a major topic of interest.)
