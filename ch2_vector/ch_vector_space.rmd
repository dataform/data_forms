# Vectorising data

## todo
- go straight into the data -- talk about how hastie and others do this;
- the thing about looking at patterns -- someone elses turkish carpet
- Beniger article on history of stats graphics; Marchese on tables;
- bring in `iris` discussion
- do library dependencies with datasets added; create map of datasets; add datasets from other sources; 
- pull apart some diagrams using inkscape -- use this to show what happens to the abstract spaces
- mention how things that don't seem to be vector spaces are put into vector spaces (e.g. market basket analysis)
- use the metaphor of the house architecture and real estate as a figure to help explain the diagramming
- reference history of tables -- e.g. downloaded chapter -- need to get from samsung laptop
- discuss problem of tables -- how they diagrammaticaly generate high D spaces that we then wrestle with, mightly and again diagrammatically -- fallacy of misplaced concreteness?
- see history of statistics in terms of plurality of attempts to deal with the diagramming of the table
- add Foucault on the appearance of deep order in life/language/labour -- this is a really important thread to draw through - in this case through the reconstruction of the _table_. 
- add quotes from Foucault on the table of space of order, and ask how he thought about quantity, etc; and what is happening to tables today.
- quotes from Wark on the vector

## quotes to use

'things, in their fundamental truth, have now escaped from the space of the table' [@Foucault_1992, 239]

Strata could never organize themselves if they did not harness diagrammatic matters or functions and formalize them from the standpoint of both expression and content. 144 [@Guattari_1988, 144]

The space of order, which served as a _common place_ for representation and for things, for empirical visibility and for the essential rules, which united the regularities of nature and the resemblances of imagination in the grid of identities and differences, which displayed the empirical sequence of representations in a simultaneous table, and made it possible to scan step by step, in accordance with a logical sequence, the totality of nature's elements thus rendered contemporaneous with one another -- this space of order is from now on shattered: there will be things, with their own organic structures, their hidden veins, the spaces that articulates them, the time that produces them; and then representation, a purely temporal succession, in which those things address themselves (always partially) to a subjectivity ... 239-240 [@Foucault_1992, 239-240]


## Four first steps into the data: media, medicine, business and biology
[W]e can see that references to things act simultaneously as reference to (and within) activities. [@Lynch_1993, 193]

'This book is about learning from data' write Hastie, Tibshirani and Friedman on the first page of _Elements of Statistical Learning_, and they rapidly do indeed begin to iterate through some data. On the second page of the book [@Hastie_2009, 2], a table of spam email word frequencies appears (and the problem of spam classification is canonical in the machine learning literature - we return to in Chapter 5). They come from the dataset `spam` [REF TBA]. On the third page, a complicated data graphic appears (Figure 1.1, [@Hastie_2009, 3]. It is a scatter plot matrix of the `prostate` dataset included in the `R` package `ElemStatLearn,` the companion `R` package for the book. In a third example, a set of scanned handwritten numbers appears. These scans are images of zipcode or postcode numbers written on postal envelopes taken from the dataset `zip` [REF TBA], and they differ from both the table and plots because they directly resemble something in the world, which, however, happens to be numbers, and is, therefore, probably already recruited into data-making and data-circulating processes.  The final example in the introduction, 'Example 4: DNA Expression Microarrays,' draws this time from biology, and particularly, high-throughput genomic biology, the kind of science that produces large amounts of data about something in the world by running many tests, or by constructing devices that generate many measurements, in this case, a DNA microarray.[^10] The image shown here is perhaps most striking. No numbers are shown, only a colorful heatmap with brighter colours standing for higher levels of gene expression, and darker colours for lower levels. For all its dense color, the data shown here is a sample of 100 of the approximately 7000 genes in the dataset `nic` [REF TBA] [@Hastie_2009, 6]. In comparison to the medical data from the `prostate` dataset with its 97 rows of 10 columns, the microarray dataset has 64 columns of data.[^11] Both are cancer datasets, but the `nci` dataset cannot be shown in its entirety because it refers to 60 different cell lines ranging across colon, breast, prostate, ovarian and renal cancers, as well as the thousands of different genes.

The combined effect of these four example datasets deriving from network media, from medicine, from business administration and from cutting-edge life science (c.2000) is to suggest a tremendous, indeed almost spectacular coherence, one that in principle would surprise us because there is otherwise little contact between the places these datasets come from. In passing, we should note that the _Elements of Statistical Learning_ is not alone in this juxapositioning opening. Very similar example sets can be seen in most machine learning publications. To give just a few examples: Andrew Ng's CS229 lectures, for instance, treat spams  in Lecture 5 [@Ng_2008h]; Rachel Schutt and Cathy O'Neil's _Doing Data Science_ discusses spam in Chapter 4 [@Schutt_2013], and Peter Flach's _Machine Learning The Art and Science of Algorithms that Make Sense of Data_ [@Flach_2012] introduces spam in the first few pages. Similar parallels exist around the image recognition problem (exemplified by handwritten digits), around the measurements dataset (`prostate` in the case of [@Hastie_2009], and in relation to the larger, impossible-to-see-the-patterns datasets of the cancer microarray data). How is this conformation and coherence being done? The repetition of data sets, the juxtaposition of discontinuous domains, and forms of movement construct and order continuities in the service of various forms of predictive and inferential knowledge. In these miscible juxtapositions we are dealing with, it seems, a _regularity_ that invites archaeological work.

[^10]: This kind of data will be the focus of a later chapter (Chapter 6). Machine learning during the 1990s and 2000s was in some ways boosted heavily by the advent of genomic biology with its large, enterprise style knowledge endeavours such as the Human Genome Project.

[^11]: The data derives from a publication in _Nature Genetics_ [@Ross_2000] analysing gene expression in the cancer cell lines maintained as experimental models by the US National Cancer Institute.


## Truth is no longer in the table

'Things, in their fundamental truth,' writes Foucault in _The Order of Things_ 'have now escaped from the space of the table' [@Foucault_1992, 239]. Foucault of course was writing in these pages about the fabled emergence of life, labour and language as the anchoring vertexes of a new triangle of knowledge and power structuring the figure of the 'human' in the 19th century. The table as a space was abandoned because it could no longer serve as the 'common place for representation and things, for empirical visibility and for the essential rules' (239). Before the emergence of the sciences of the human -- political economy, linguistics and biology -- knowledges such as natural history had ordered empirical materials of diverse provenance in a shared grid. An image or figure from myth might lie alongside a measurement or a count of occurrences, and this proximity was ordered by systems of classification that spanned what we might today, in the light of the 19th century, still want to separate.  While the history of tables as data forms reaches a long way back (see [@Marchese_2013] for a broad historical overview), and tables still definitely abounded and indeed multiplied in the 19th and then 20th centuries, Foucault argues that the system of grids that permitted ranking, sorting and ordering in tables had between the end of the Renaissance and the early nineteenth been based on 'buried similitudes' and 'invisible analogies' [@Foucault_1992, 26]. The grid or table was a way of bringing the otherwise scattered and diverse resemblances into exhaustive enumeration where they could be subjected to analysis by counting and comparison. The table as space of order did not stand alone. Algebra or _mathesis_ more generally applied to 'simple nature' (planets in movement, dynamics of falling bodies, etc), but taxonomy dealt with more complex natures. Even here, a system of signs (the groupings established by the eighteenth century taxonomist Carl Linnaeus), sought to reduce complex natures (plants, animals, etc.) to simpler forms as columns and rows in a table. Importantly, the table as space of order was a space of imagination in that one could begin to see continuities and the genesis of things (including grammar, nature and wealth) by carefully looking at the table. Conversely, the table encompassed all order.   'Hedged in by calculus and genesis,' Foucault suggest, 'we have the area of the _table_' [@Foucault_1992, 73]. Note in passing that calculus and algebra help bound the table, not because everything has been mathematized, but because it laid down a general possibility of ordering a field of identity and differences.

In the nineteenth century, such tabulations were replaced by a different form of ordering. Foucault figures this change as shattering:

>The space of order, which served as a _common place_ for representation and for things, for empirical visibility and for the essential rules, which united the regularities of nature and the resemblances of imagination in the grid of identities and differences, which displayed the empirical sequence of representations in a simultaneous table, and made it possible to scan step by step, in accordance with a logical sequence, the totality of nature's elements thus rendered contemporaneous with one another -- this space of order is from now on shattered: there will be things, with their own organic structures, their hidden veins, the spaces that articulates them, the time that produces them; and then representation, a purely temporal succession, in which those things address themselves (always partially) to a subjectivity [@Foucault_1992, 239-240].

Life, labour and language -- the figure of the human  -- famously replace the aggregative tables of the Classical age. From now on there are things, organically structured, relating to a place and changing in time, and there are representations addressed to (and constitute) of a subjectivity. Double interiority takes over. Things such as a language, a species or an economic system have their own genesis, and our knowledges and indeed experience too become finite, historical, with their own dynamics and internal life. Knowledge is, for instance,  epistemologized in various ways so as to become scientific. Writing the 1960s, Foucault postulates that 'our thought still belongs to the same dynasty' (243).

This brief resumé of one of the main arguments in _The Order of Things_ might help us see what is being diagrammed in the data tables we find in machine learning, and in particular, in the series of tables of data with which _Elements of Statistical Learning_ begins. The question here is how we make sense of these tables in terms of the broader diagram they make up. Presumably the spam, prostate, image and genetic data are not put together in the way Linnaeus would have put together a table of living things. Yet their regular juxtaposition does suggest we should be looking for alignments and analogies between them. At the same time, they topics of these tables -- they concern work, life, language and economy in various ways -- maps very readily onto the anchor points, the new empiricities of labour, life and language.  The paradox here is that things may longer be hedged in by the table, their fundamental truth may have escaped from the space of the table, and yet we are confronted by many more tables. In many senses, we are hedged in by tables, and many different mechanisms animate and multiply tables around us.[^22] In certain respects the tables we see in the first pages of [@Hastie_2009] are much more classical in their scale and in their relatively immutability. In many settings in which machine learning operates, tables change rapidly in scale and sometimes in organisation. 

[^22]: By and large, I am not discussing networking and database infrastructures here. In other work, I have attempted to account for the multiplication of tables in databases but also in mundane practices of ordering. See [@Mackenzie_2001; @Mackenzie_2012]. My analysis there pivots mainly on a set-driven account of data derived from the work of Alain Badiou. Although I've already been using some set terminology here -- as in 'dataset' -- I'm persisting with a more geometrical and algebraic account of datasets in order to better deal with some of the graphisms common in machine learning. 



## Epistopological transformations

```[r ElemStatLearn_data, echo=TRUE}
library('ElemStatLearn')
datasets = data(package='ElemStatLearn')
datasets$results[,3]
```

The collocation of the four introductory examples of spam, handwriting, cancer and biology is not unusual. As we have already seen, it is relatively common in machine learning literature to juxtapose datasets from science, business, government and media, and then show how one can move between them. The `ElemStatLearn` `R` package brings, as we have seen in the previous chapter, brings with it around 20 different datasets, including the four we see in the opening examples. At this point, I'm not so much interested mapping the domains these datasets relate to, but in exploring how we might look at and speak about such tables.   Building on the diagrammatic account of the _Elements of Statistical Learning_ developed in the previous chapter, this chapter attends to both the regularity and the instability associated with the data forms in machine learning. It does this by following a variegated faultline that runs to a less or greater degree across all machine learning. The faultline runs between what is said and what is shown, between statements and visibilities. Put prosaically, in the informal diagram of machine learning, the graphics and the text cannot quite hold together. They are divided in ways that can be seen by, I propose, by following some transformations of data tables into predictions and inferences.

In moving along this fault, we need to be proceed epistopologically. The word 'epistopological' is a neologism combining 'epistopic' and 'topological.'  The 'epistopic' term comes from the work of the science studies scholar Mike Lynch, whose account of scientific practice is usefully focused on ordinariness. He proposed the 'epistopic' as a way of connecting 'familiar themes from epistemology and general methodology' [@Lynch_1993, 280] with localized practices, or with the local achievement of coherence. In other words, as the term itself suggests, an epistopic connects a general epistemic theme with a place, a 'local complex of activities' (281).  This emphasis on epistemic-placeness is very useful in several ways, especially if it can be brought to bear on the problem of how the 'local complex' of a specific dataset can be transformed by a general epistemic practice such as machine learning.  It's interest in the couplings between 'graphism' and practice should help make sense of some of the graphic forms we see in _Elements of Statistical Learning_ as forms of movement that epistemologize data, or that engender epistemic talk about data. It's concern with the connections between seemingly generic scientific epistemological statements about  error, bias, variation, confidence, expectation and likelihood and graphically visible and down-to-earth practices such as drawing a line, labelling a point or sorting a list of values is a good antidote to the sometimes ballistic epistemological trajectories fuelled by statistical machine learning (e.g. the infamous and heavily cited comments by the former _Wired_ magazine editor, Chris Anderson, on the 'end of theory').  But what kind of transformations occur here? Over a decade ago, the cultural theorist Brian Massumi wrote that 'the space of experience is really, literally, physically a topological hyperspace of transformation' [@Massumi_2002, 184]. Much earlier, Gilles Deleuze had conceptualised Michel Foucault's philosophy as a topology, or 'thought of the outside' [@Deleuze_1988], as a set of movements that sought to map the diagrams that generated a 'kind of reality, a new model of truth' [@Deleuze_1988, 35]. More recently, this topological thinking has been extended and developed by Celia Lury amongst others. In 'The Becoming Topological of Culture,' Lury, Luciana Parisi and Tiziana Terranova suggests that 'a  new rationality is emerging: the moving ratio of a topological culture' [@Lury_2012, 4]. In this new rationality, practices of ordering, modelling, networking and mapping co-constitute culture, technology and science [@Lury_2012, 5].  At the core of this new rationality, however, lies a new ordering of continuity. The 'ordering of continuity,' Lury, Parisi and Terranova propose, takes shape 'in practices of sorting, naming, numbering, comparing, listing, and calculating' (4). The phrase 'ordering of continuity' is slightly odd, since we don't normally think of continuities as subject to ordering. In many ways, that which is continuous bears within it its own ordering, its own immanent seriation or lamination. But in the becoming topological of culture movement is itself undergoing a transformation according to these authors. Rather than movement as something moving from place to place relatively unchanged (as in geometrical translation), movement should be understood as more like an animation, a set of shape-changing operations. These transformations, I would suggest, should be legible in the way that machine learning, almost the epitome of the processes of modelling and calculation that Lury, Parisi and Terranova point to, itself moves through the data. And indeed, the juxtaposition of spam, biomedical data, gene expression data and handwritten digits already suggests that topological equivalences, and a 'radical expansion' of comparison might be occurring. Bringing epistopics and topologies together might, I suggest, help trace, map and importantly diagram some of the movements into the data occurring today.
If the epistopological is a mode of diagramming, of finding those traits in the data that are capable of combining or conjugating with each other, but are not reducible to each other, how do do it in practice?

[^12]: The _locus classicus_ account of such transformation is perhaps Michel Foucault's _The Order of Things: An Archaeology of the Human Sciences_ [@Foucault_1992], and his account of what happened to tables in the 18th century will be of some use in the pages to follow. 

##  `iris`: 'a superficial glitter above any abyss' 

Let us see how Hastie, Tibshirani and Friedman begin to move through the datasets. First of all, they select the first three (`spam`, `prostate` and `zip`), leaving aside the cancer genes (`nci`). They say:

>The first three examples described in Chapter 1 have several components in common. For each there is a set of variables that might be denoted as inputs, which are measured or preset. These have some influence on one or more outputs. For each example the goal is to use the inputs to predict the values of the outputs. This exercise is called supervised learning. We have used the more modern language of machine learning.[@Hastie_2009, 9].

[HERE] . they begin to dissect the tables and point to their discontinuities; they they then introduce iris; add iris in. Mention the omission of nci and why (Clustering!). Then track the many invocations of iris; and particularly its blurring of the line between continuity and discontinuity (numbers vs classes). 
## Introduction


```{r prostrate, echo=TRUE, results = 'asis'}
library('ElemStatLearn')
library(xtable)
data('prostate')
pairs(prostate[, -10])
xtable(head(prostate))

```
The character of the transformations that might follow is hard to summarise. Nevertheless, at several specific points in the lines of code above  -- the  `sapply` function and  -- it demonstrates another form of compression or superimposition commonly found in software such as  `R` and other programming languages designed for data practice (Octave, Matlab, Python's NumPy, or C++ Armadillo): _vectorised_ transformations of data.  As a programming language, `R` is striking for its many  `-ply` constructs.  There are several in the core language and many to be found in packages (the popular 'plyr' package; versions of 'ply' can also be found in recent Python data analysis libraries such as Pandas).  These include `apply`, `sapply`, `tapply`, `lapply`, `mapply`. All of the `-ply` constructs have a common feature: they take some collection of things (it may be ordered in many different ways – as a list, as a table, as an array, etc), do something to it, and return something else. This hardly sounds startling. But while most programming languages in common use offer constructs to help deal with collections of things sequentially (for instance, by accessing each element of data set in turn and doing something), `R` offers ways of dealing with  them all at once. The `-ply` constructs ultimately derive from the functional logic developed by the mathematician Alonzo Church in the 1930s [@Church_1936; @Church_1996]. By virtue of these `-ply` constructs,  `R` sometimes presents difficulties for programmers trained to code using so-called procedural programming languages. In many mainstream programming languages, transformations of data are often done using loops and array constructs in which some operation is successively repeated on each element of a data structure. Moreover,  in vectorised languages such as `R`, transformations of a data structure  expressed in one line of code  simultaneously affect all the elements of the data structure. As the widely used _R Cookbook_ puts it, 'many functions [iN `R`] operate on entire vectors, too, and return a vector result' [@Teetor_2011, 38]. Or as _The Art of `R` Programming: A Tour of Statistical Software Design_ by Norman Matloff puts it, 'the fundamental data type in `R` is the _vector_' [@Matloff_2011, 24], and indeed in `R`, all data is vector. There are no individual data types,  only varieties of vectors in `R`. Again, as _R in a Nutshell_  puts the point directly: 'in `R`, any number that you enter ... is interpreted as a vector' or as 'an ordered collection of numbers' [@Adler_2010, 17]. The practical difference is illustrated in the code vignette above in which two 'vectors' of numbers are added together, in the first case using a classic `for`-loop construct, and in the second case using an implicitly vectorised arithmetic operation `+`. The difference in speed between adding $1 ... 10$ using a loop or vector arithmetic is completely trivial here, but when millions of numbers are handled arithmetically, these implementation differences significantly affect the work of both programmers and computing machinery.  This simultaneity is only apparent, since somehow the underlying code has to deal with all the individual elements, but vectorised programming languages take advantage of hardware optimisations or carefully-crafted low-level libraries. 

```{r vectorisation, echo=TRUE, cache=TRUE, message=FALSE, warning=FALSE, comment=NA, size='smallsize', results='markup' } 
 
     vector1 <- c(0,1,2,3,4,5,6,8,9,10)
     vector2 <- c(0,1,2,3,4,5,6,8,9,10)

     # procedural programming-style looped addition
     result_looped = vector()
     for (i in vector1) {
        result_looped[i] = vector1[i] + vector2[i]
     }
     result_looped

     # vectorised addition
     result_vectorised  <- vector1 + vector2
     result_vectorised

```
The functional programming style of applying functions to functions seems strangely abstract. Once you get a feel for them, these -ply constructs  and the implicit vectorisation of many operations on data structures are very convenient to write, and the code runs much faster. They  transform data in ways that utilise  increasingly parallel contemporary chip architectures, and adapt readily to the increasingly Cyclopean infrastructures of cloud computing. The `-ply` operations reduce both data and computational frictions. The real stake in -plying data, however, is not speed but transformation. -Plying makes working with data less like iteration (number, text, table, list, etc), and more like folding a pliable material. Such shifts in feeling for data are very mundane yet crucial to the flow of data.  Plying, especially with added connotation of trade and exchnage as in the 'plying goods', seems much closer to the kinds of work done on data than the figures of data flow or data deluge. 

While the examples of vectorised computation that I have shown are relatively trivial -- turning a nested list into a table, multiplying sequences of numbers -- these vectorised operations have  both concrete and abstract implications.  The concrete implication we have just seen:  the implicit and explicit use of vectorised processes feeds directly into the scaling up of computational  work on data. Not only vectorised computation, but the various styles of programming and infrastructure that lend themselves to simultaneous transformations of large matrixes or tables of data lie at the heart of not only machine learning, but in the intensive processing of data in many different places. The rendering of moving images in  computer graphics depends implicitly on vectorised  transformations of matrices of numbers. 

## Composing data forms  in vector space

The more abstract implication of vectorisation and the forms of multiplication it encourages and demands will take us longer to unravel. Statistical modelling, data-mining, pattern recognition, recommendation systems, network modelling and machine learning rely on  such transformations. 'Fitting a model' to data is often literally implemented, as we will see, by  multiplying data matrices together.  Hence,  describing vectorised transformations of   data in  `R` and other computing environments (for instance, the popular `Map-Reduce` architecture) is not only a matter of pointing to the now familiar increases in  speed or efficiency of computation. The vectoral treatment of data taps into and is interwoven with the transformation of data more generally. Later chapters will discuss various ways in which the vectoral dimensionality of data or its rendering as _vector space_ scales up and scales down in machine learning.[^1]  In fact, a major goal is to disentangle some different forms of vectorisation, or different ways of inhabiting vector space, associated with data today. But to emphasise this abstract mode of existence of data as a vector space  is not to say that the practical transformations of data in various forms of code and computing infrastructure can be ignored. Just the opposite is the case: the  abstract understanding of data as vector space generates many scaling potentials to which particular implementation in code and computing hardware respond. This concretising dynamism arising from abstraction is not new. We need only think of the way in which Marx describes the replacement of highly distributed artisanal manufacture by interconnected systems of machines in factories driven by prime-movers (Watt's steam engine above all) to see a similar process of reorganisation driven by  a numerate abstraction, in that case the cost of labour (see Chapter XV of [@Marx_1986]).  

The praxiographic challenge lies in attending to this highly abstracted vector space in ways that maintain and indeed augment its value-relevance, its concreteness, and attachments to lives and places. We lack good intuitions of how to do that because of the ways in which data as vector space has been constructed and described. While we now often hear about algorithms, predictions and smart devices affecting our lives, the plural actuality of what they compose largely still eludes us. Often data is  represented as if it is an homogeneous mass or a continuous flow, but  it takes many different shapes and has many different _densities_.  I loosely borrow the term 'density' from statistics, where *probability density functions* are often used to describe the hardly ever uniform distribution of probabilities of different values of a variable. Sensing density as a form of variation matters greatly both in machine learning itself, where algorithms seek purchase on unevenly distributed data, and in any broader praxiography of data.  By thinking of data density, we perhaps also gain a better sense of the heft and weave of data. Data  spaces out in many different density shapes, depending on how the data has been generated or instanced. Whatever the starting point (a measuring instrument, people clicking and typing on websites, a device like a camera, a random number generator, etc.), it is inevitable that later transformations will remap  it  to different shapes and forms. A given machine learning algorithm, data visualisation or database query will need the data to be in specific vectoral shapes (vectors,  matrices, arrays, etc), and fit within a certain volume or scale.    The process of composing data for statistical, visual, predictive or even storage purposes, maps a concrete situation, some state of affairs, onto forms imbued with various geometrical, probabilistic, decisionist abstractions often expressed in terms of functions or mathematical models. If, as I have been proposing, we seek contact with the practices, or the concrete value-situation, then the reshaping and reflowing of densities matters greatly.   This forming and reforming of data is evidence of  implicated relations. These practices attest to what Whitehead called 'strain': 'a' feeling in which the forms exemplified in the datum concern geometrical, straight, and flat loci will be called a 'strain.' In a strain, qualitative elements, other than the geometrical forms, express themselves as qualities implicated in those forms' [@Whitehead_1960, 310]. In many machine learning models, for instance, the exemplified forms are straight or flat loci (see the discussion of line fitting, hyperplanes, decision boundaries in the next three chapters).  Yet  different practices also seek to elicit relations that strain the linear or geometrical shaping of data, that show how it does not fit.  Some  practices working in the name of linear forms effectively trace strain feelings. These feelings are the affective connectors between the abstractions and the concrete-value situations in which data arises. They are vital to the psychic life of data, and the ongoing reality-multiples it figures. 

In the machine learning literature, the composition of data is sometimes expressed quite formally. For instance, Peter Flach's _Machine Learning: The Art and Science of Algorithms that Make Sense of Data_ [@Flach_2012] formalises the relation between data and shaped data density like this:

> Features, also call attributes, are defined as mappings $f_i: \mathcal{I} \rightarrow \mathcal{F}_{i}$ from the instance space $\mathcal{i}$ to the feature domain $\mathcal{F}_{i}$. We can distinguish features by their domain. [@Flach_2012, 298]

'Features' or 'attributes' often appear as  columns in a table. They are sometimes already present in the data (for instance, the house price dataset features are pre-defined), but they are sometimes constructed or engineered from the data. As the prominent machine learning Pedro Domingos writes in a recent overview of the difficulties of doing machine learning: 'feature engineering is more difficult because it is domain-specific, while learners can be largely general purpose' [@Domingos_2012, 84]. Note that Domingos uses 'domain' here to refer to a concrete situation, whereas Flach's domain refers to a set of values that can be input into a function or a machine learning algorithm. This coincidence in choice of words is actually symptomatic. People reshape data in different ways and for different purposes, sometimes in the name of a concrete situation, sometimes in view of the form of a particular mathematical function or a computational infrastructure (for instance, the amount of RAM heavily affects  modelling). At times, data is  folded together in order to contain it or reduce its dimensionality so that it fits somewhere. At other times, much work goes into expanding the dimensionality of data in order to find ways of eliciting relations that remain somewhat latent or hidden in it, that strain or contort its form in ways that are not easily seen. 

One of the stakes in following what happens to vectors, lists, matrixes, arrays, dictionaries, sets, dataframes, or series or tuples in data, is to get a sense of how such practices map a concrete situation into something more abstract. These are the practices of abstraction, and any experience of abstraction in this domain must process along these lines. More concretely, as we will see in later chapters, the predictive models, the supervised and unsupervised learners, the classifiers, the decision trees and the neural networks in machine learning, to name a few techniques, will be largely implemented as transformations that fold and refold matrices and vectors.  Amidst the many different operations and transformations associated with machine learning and in algorithmic treatments of data, perhaps the single most important practice is matrix multiplication.  It looms large in writing about machine learning, and in many articles and textbooks, matrix manipulations are taken for granted.  For instance, the first full mathematical expression in _The Elements of Statistical Learning_ is shown below along with the surrounding text:

>The linear model has been a mainstay of statistics for the past 30 years and remains one of our most important tools. Given a vector of inputs
$X^T = (X_1 , X_2, . . . , X_p)$, we predict the output $Y$ via the model
>$$\hat{Y} = \hat{\beta_0}  + \sum^p_(j=1) X_j \hat{\beta_j)}$$
>The term $\hat{\beta_0}$ is the intercept, also known as the _bias_ in machine learning [@Hastie_2009, 11].

Introducing the linear model, the authors of the textbook immediately resort to matrix notation. There is nothing particular mathematically elusive here, only a highly aggregated set of sums and multiplications. While such expressions are not easy to read with knowing what all the symbols mean, they offer a very direct opening to thinking about machine learning more generally.  This model is the 'simple but powerful linear model' (11) in which $Y$, the so-called 'response variable' is modelled by adding together  (the $\sum$ operator means adding them) a combination of the input variables or 'features' in  $X_j$ and a value of the intercept $\beta_0$.  Even if this does not make much sense without reading more, we can begin to see that the linear model, a mainstay of machine learning, concretely takes the form of vectors and matrices added and multiplied.  The subscripts $j=1$ refer to individual input variables.   Importantly, as Hastie and co-authors go on to say, 

> Often it is convenient to include the constant variable 1 in $X$, include $\hat{\beta_0)}$ in the vector of coefficients $\hat{\beta}$, and then write the linear model in the vector form as an inner product:
$\hat{Y} = X^T \hat{\beta}$
> where $X^T$ denotes vector or matrix transpose [@Hastie_2009, 12-13].

We will need further explanation to make more sense of these dense expressions, but for the moment, the only thing that needs to come across is the heavy reliance on vectors and matrices, which means that entire models can be expressed highly concisely as matrix multiplication operations. In this case, with the convenient convention of making the intercept into a constant input variable, linear models can be written simply as an 'inner product', that is,  as a matrix-matrix multiplication. Learning machine learning, and learning to implement machine learning techniques, is largely a matter of implementing series of matrix multiplications. As Andrew Ng advises his students,

> Almost any programming language you use will have great linear algebra libraries. And they will be high optimised to do that matrix-matrix multiplication very efficiently including taking advantage of any parallelism your computer. So that you can very efficiently make lots of predictions of lots of hypotheses [@Ng_2008a, 10:50

In other parts of his teaching, and indeed throughout the practice exercises and assignments, Ng stresses the value of implementing machine learning techniques for both understanding them and using them properly. But this is one case where implementation does not facilitate learning. Ng advises his learners against implementing their own matrix handling code.    They should instead  use the 'great linear algebra libraries' found in 'almost any programming language.' 'Linear algebra libraries' multiply, transpose, decompose, invert and generally transform matrices and vectors. They will be 'highly optimised' not because every programming language has been prepared for the advent of machine learning on a large scale, but rather more likely because matrix operations are just so widely used in image and audio processing.  Happily, Ng observes, that means that 'you can make lots of predictions.'  It seems that generating predictions and hypotheses outweighs the value of understanding how things work on this point. 

How do we 'efficiently make lots of predictions' using matrices? Let's return to the example of house prices mentioned by Norman Nie. In this example, house price is generally treated as the response variable ($Y$), and the number of bedrooms and overall size in square feet are the input variables ($X_1$, $X_2$).  A linear model seeks to find a line (or a plane, or as we will see, a hyperplane) that best fits the points. While I will discuss in the next chapter how such a line can be found, here I want to only show one solution to this problem because it takes the form of a matrix multiplication. As Hastie and co-authors write, the 'unique solution' to the problem of fitting a linear model to a given dataset using the most popular method of 'least squares' [@Hastie_2009, 12] is given by:

>$\hat{\beta} = (\mathbf{X}^T\mathbf{X})^-1\mathbf{X}^T\mathbf{y}$

This is quite a tightly coiled expression of how to calculate the parameters $\hat{\beta}$ for a linear model. The two matrices involved are the $X$ input variables (house size and number of bedrooms) and a 1-dimensional matrix $y$, the house price. These matrices are multiplied, transposed (a form of rotation that swaps rows for columns) and inverted (a more complex operation that finds another matrix) Implementing it for  the San Francisco house price data only requires a few extra lines of code:

```{r house_price_predict, echo=TRUE, cache=TRUE, message=FALSE, warning=FALSE, comment=NA, size='smallsize', results='markup' } 
 
    library(MASS)
    houses = read.csv('data/ex1data2.txt', header=FALSE)
    names(houses) = c('size', 'bedrooms', 'price')
    X = as.matrix(cbind(1, houses[,1:2]))
    y = houses$price

    # calculate the coefficients of the model using the least squares algorithm
    beta_hat = ginv(t(X) %*% X) %*% t(X) %*% y
    
    #making a prediction
    rooms =3
    size = 2000
    predicted_price = beta_hat[1] + beta_hat[2]*size + beta_hat[3]*rooms
    plot(houses$size, houses$price)
    abline(beta_hat[1], beta_hat[2])
```

Having calculated the  values of $\hat{\beta}$,  the coefficients of the linear model, we can make predictions of house prices for any given number of bedrooms and house area. For instance, a house with 4 bedrooms and area of 1000 sq. feet should cost around `r predicted_price`. As the plot of the house price data shows, albeit without showing all the variables, the line that has been fitted passes through many of the points. Finding the geometric form of the line or surface is the predictive element of the function. 

We don't need to grasp all the details of the matrix operations working to fit this prediction surface. As with the terseness of  the code that fetches data from the World Bank databases, I want to point mainly here to the tightly coiled matrices  that produce predictions.  One line of fairly dense code that pivots on matrix multiplication operations (`%*%`) effectively makes the model

`beta_hat = ginv(t(X) %*% X) %*% t(X) %*% y`.

The critical question is whether by unwinding some of these operations, for instance, by seeing how  matrix multiplication ripples through different treatments of data, we get closer to the vitality of the multiple/multiplying concrete value-situations that connect calculation and feeling.  My suggestion is that the praxiographic description of how machine learning techniques vectorise and multiply data densities  as abstractions provides a fairly direct way begin to name and unravel the processes of knowing, predicting and deciding on which many aspects of the turn to data rely.  The following three chapters enlarge on this general point about multiplication in different ways. As we will see, the matrix operations we have just been viewing are themselves organised by other layers of intuition that explore shape, movement and surfaces in much more convoluted forms.  (Without mentioning it, the same multiplication operation, the so-called _dot product_ lay at the core of the perceptron algorithm code I quoted from Wikipedia earlier.)  Experience in so many settings -- media, consumption, science, security, etc -- is embedded in the hyperspaces of matrix transformation, and matrix multiplications. 


[^1]:  In terms of multiplying matrices, dimensionality both constrains and enables many aspects of the prediction. Perhaps on the grounds of data dimensionality alone, we should attend to dimensionality practices in  `R`. A fuller discussion of dimensionality of data is the topic of a later chapter. There I discuss how machine learning re-dimensions data in various ways, sometimes reducing dimensions and at other times, multiplying dimensions.

[^2]: Machine learning in genomics is also the topic of a later chapter. Recent life sciences are not alone in their resort to computational techniques, but in contrast to commercial, industry or government data practices, it is easier to track how data is transformed, reshaped and re-dimensioned in producing biological knowledges. The scientific publications, the public databases and open source software work together to allow this. 

[^3]:In combining Markdown and LaTex, I also make use of the [Pandoc utility](http://johnmacfarlane.net/pandoc/README.html), a command line tool that converts between different document formats.

[^4]: I discuss the competitions and forms of machine learning subjectification in a later chapter. 

[^5]: Isabelle Stenger's  book on Whitehead offers a deeply philosophical introduction to his work [@Stengers_2002]. [TBA: English version]
    

## Houses and the architecture of datasets
Let us stick with Nie's example of house prices in order to concretise what descriptive practices (of the kind that admit reality as multiple, abstractions as concretely relevant, and computation as affectively expansive not restrictive) might look like in `R` work with data. House price data is possibly a worst case example, since house prices seem so heavily webbed into employment, salaries, class position, and business as usual. Is it possible to describe  predictive modelling of house prices in a way that responds to what Mol, Whitehead and Wilson propose? Could house price data engage us in something more than working out what to buy or when to sell? House price datasets appear quite often in machine learning research and tutorials. (The most well-known is the Boston House Price dataset dating from the 1970s [Boston Housing Data](http://archive.ics.uci.edu/ml/datasets/Housing)) The sample shown below comes from San Francisco, prior to the global financial crisis 2007 and its associated sub-prime mortgages. As discussed  below, the dataset comes from an online machine learning course. The dataset is fairly small and simple and can be loaded, summarised and plotted using a few lines of `R` code. 

```{r house_price, echo=TRUE, cache=TRUE, message=FALSE, warning=FALSE, comment=NA, size='smallsize', results='markup' }

    house_prices = read.csv('data/ex1data2.txt', header=FALSE)
    names(house_prices) = c('size', 'bedrooms', 'price')
    head(house_prices)
    summary(house_prices)
    attach(house_prices)
    plot(size, price)
```

Two architectures come together here. On the one hand, around fifty houses  in a city, San Francisco, are described in terms of the number of bedrooms, their total floor area and the price they sold for. This is a very sparse description of domestic urban life, and it reduces the encounter with the psychically rich form of houses/apartments to a cognitive minimum. (The Boston House Price dataset, by comparison, has many more variables and offers a somewhat richer picture.) On the other hand, the description of this encounter has its own architectural forms -- the  pervasive form of the table, grid or, crucially for machine learning,  _matrix_ and the plot that displays values of variables in a geometric sapce. (Whatever else happens to the data in machine learning,  matrix operations and graphic plots remain ever-present. As we will see below, in modelling this data, matrix manipulations will be essential.) So Nie's recommendation of `R` as a way of learning house prices is not unusual. His mention of house price prediction as something that might engage us has many precedents. House price prediction is a surprisingly common teaching or demonstration example in machine learning, alongside other iconic datasets including R.A. Fisher's 'iris', the MNIST handwritten digits dataset [@LeCun_2012]  (derived from U.S. National Institute of Standards and Technology (NIST)), or the cat photo dataset discussed in the previous chapter. 

