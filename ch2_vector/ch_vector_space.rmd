\chapter{The vector space and its products}
\label{ch:vector}


```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=12, tidy=TRUE, fig.height=8, 
                      echo=FALSE,  warning=FALSE, message=FALSE, dev='pdf')

library(RSQLite)
con <- dbConnect(RSQLite::SQLite(),'../ml_lit/all_refs.sqlite3')
res = dbGetQuery(con, statement ="select * from basic_refs limit 10;")
library(ggplot2)
library(stringr)
library(xtable)
options(xtable.comment = FALSE)
```

>In the form of the disciplinary distribution ... the table has the function of treating multiplicity itself, distributing it and deriving from it as many effects as possible [@Foucault_1997, 149] \index{table} \index{Foucault, Michel}

I sometimes have the feeling that machine learners  want to go everywhere in the world, that they aim to take on any problem, that they see no data as too difficult to learn on. This formulation from Matthew Kirk's _Thoughtful Machine Learning_ is typical:

> Machine learning is an amazing application of computation because it tackles problems that are straight out of science fiction. These algorithms can solve voice recognition, mapping, recommendations, and disease detection. The applications are endless, which is what makes machine learning so fascinating [@Kirk_2014, 11] \index{Kirk, Matthew}

It is harder but not impossible to find dissenting or sceptical voices.  Computer scientists such as Anand Rajaraman and David Ullman in their _Mining Massive Datasets_ voice doubts about the power of machine learning algorithms and contrast these algorithms with specialized search and retrieval techniques [@Rajaraman_2012]. Similarly, the mathematician Cathy O'Neil has trenchantly criticised the rapid extension of machine learning approaches in finance. Regardless of these reservations and criticisms, the appearance of 'endless' applications is not misleading. Machine learning has, once you start looking for it, become astonishingly generalized. The epistemic imperium is not only due to the generic mathematical or computational techniques but I will suggest from the advent of a new model of truth crossing a threshold and creating a new epistemic continuum. The continuum arises from a very specific diagrammatization of data that has already begun to appear in the NAND truth table and the perceptron in chapter \ref{ch:diagram}. On this continuum, all data is associative. In _Elements of Statistical Learning_,  plots of New Zealand fishing patterns lie next to plots of factors in South African heart disease. \index{\textit{Elements of Statistical Learning}} This seemingly diasporic variety of data as well as the variety of domains and situates the datasets index are salient components of the practical assemblages in machine learning.

```{r elemstatlearn_data_2, echo=FALSE, results = "asis"}
library(ElemStatLearn)
library(xtable)
datasets_results <-data(package='ElemStatLearn')$results[,3:4]
datasets = as.data.frame(datasets_results)
dataset_count <- nrow(datasets)
dataset_table = xtable(datasets, caption='Datasets in _Elements of Statistical Learning_', label = 'tab:datasets_elemstatlearn')
print.xtable(dataset_table, type='latex')
```

The `r dataset_count` datasets  shown in \ref{tab:datasets_elemstatlearn'} typify the diaspora of domains found in [@Hastie_2009; @Hastie_2001]. As mentioned above,  they span scientific, clinical, commercial and media fields. Note that they include many patches and pathways of everyday life -- speaking, seeing, writing and reading -- as well as specific scientific objects of knowledge such as galaxies, cancer, climate and national economies. This span of interests is not unusual for machine learners, which is notable for the unexpected conjunctions it creates.  To give a another example, the statistician Leo Breiman's 2001 article on random forests [@Breiman_2001] \index{random forest} is one of the most cited journal papers in the machine learning literature and displays a similarly diagonal line across human and non-human worlds: \index{Breiman, Leo}

>Glass,  Breast cancer,  Diabetes,  Sonar,  Vowel,  Ionosphere,  Vehicle,  Soybean,  German credit,  Image,  Ecoli,  Votes,  Liver,  Letters,  Sat-images,  Zip-code,  Waveform,  Twonorm,  Threenorm,  Ringnorm,  [@Breiman_2001, 12].

In some ways, the improbable conjunction of spam email and cancer detection in machine learning  simply continues what statistics as a field has always done: rove  across scattered fields ranging from astronomy to statecraft, from zoology to epidemiology, gleaning data as it goes (see [@Stigler_1986; @Hacking_1990] for samples of this trekking movement). \index{statistics} But in _Elements of Statistical Learning_ and the field of machine learning more generally, something more is moving through these datasets. Rather than attending to any particular dataset, crossing between different data elements seems most important.  While the datasets span many domains -- vowels, ozone, bone density, marketing, prostate cancer, and spam -- their diversity  demonstrates the mobility of the techniques. There are many different ways to encounter data. The diversity of the data could be almost aleatory, as if the datasets were the fruit of random derivÃ¨ in the world.  Rather than random drifting, I think it more likely the field of machine learning performs something like one of its own statistical procedures in 'bagging' (bootstrap aggregating [@Hastie_2009, 300]) datasets in order to average its predictions out across different domains. \index{bagging} The repeated sampling of the world bootstraps machine learning as a field into its operational mode of generalization.\index{generalization}  

These datasets have visible forms of alignments or architectures that we should pay close attention to. If machine learning can be understood as a constantly evolving diagram or an abstracting multiplicity, the way it moves through data and between different forms of data matter. This movement is guided by the shape of the data in some ways, as we will see in this chapter. It also shapes data in increasingly extensive ways.  The tabular form, the practices of naming and labelling, and the sorting of different data types exemplified in these diverse datasets can tell us a lot about how machine learning organises and energises its movement through worlds.  The different shapes and composition of the datasets provide indications of the functioning of machine learners as they make knowledge or operationalise power relation. Already in table \ref{tab:datasets_elemsstatlearn} differences between datasets labelled 'training' and 'test,' appear quite often. We might also attend to differences between data called 'simulated' and other kinds of data. 

## Mixing media, medicine, business and biology

>References to things act simultaneously as reference to (and within) activities. [@Lynch_1993, 193]

'This book is about learning from data' write Hastie, Tibshirani and Friedman on the first page of _Elements of Statistical Learning_, and they rapidly do indeed begin to iterate through some data. On the second page of the book [@Hastie_2009, 2], a table of spam email word frequencies appears (and the problem of spam classification is canonical in the machine learning literature - we return to in Chapter 5). They come from the dataset `spam` [@Cranor_1998]. On the third page, a complicated data graphic appears (Figure 1.1, [@Hastie_2009, 3]. It is a scatterplot matrix of the `prostate` dataset included in the `R` package `ElemStatLearn,` the companion `R` package for the book. In a third example, a set of scanned handwritten numbers appears. These scans are images of zipcode or postcode numbers written on postal envelopes taken from the dataset `zip` [REF TBA], and they differ from both the `spam` table and `prostate` plots because they directly resemble something in the world, which, however, happens to be numbers, and is, therefore, probably already recruited into data-making and data-circulating processes.  The final example in the introduction, 'Example 4: DNA Expression Microarrays,' draws this time from biology, and particularly, high-throughput genomic biology, the kind of science that produces large amounts of data about something in the world by running many tests, or by constructing devices that generate many measurements, in this case, a DNA microarray.[^2.10] The image shown here is perhaps most striking. No numbers are shown, only a colorful heatmap with brighter colours standing for higher levels of gene expression, and darker colours for lower levels. For all its dense color, the data shown here is a sample of 100 of the approximately 7000 genes in the dataset `nci`  [@Hastie_2009, 6]. In comparison to the medical data from the `prostate` dataset with its 97 rows of 10 columns, the microarray dataset has 64 columns of data.[^2.11] Both are cancer datasets, but the `nci` dataset cannot be shown in its entirety because it refers to 60 different cell lines ranging across colon, breast, prostate, ovarian and renal cancers, as well as the thousands of different genes.

The combined effect of these four example datasets deriving from network media, from medicine, from business administration and from cutting-edge life science (c.2000) is to suggest a tremendous, indeed almost spectacular miscibility, one that in principle could surprise us because there is otherwise little mixing between the places these datasets come from. In passing, we should note that the _Elements of Statistical Learning_ is not alone in this juxapositioning opening. Very similar example sets can be seen arrayed in most machine learning publications. To give just a few examples: Andrew Ng's CS229 lectures, for instance, treat spams  in Lecture 5 [@Ng_2008h]; Rachel Schutt and Cathy O'Neil's _Doing Data Science_ discusses spam in Chapter 4 [@Schutt_2013], and Peter Flach's _Machine Learning The Art and Science of Algorithms that Make Sense of Data_ [@Flach_2012] introduces spam in the first few pages. Similar parallels exist around the image recognition problem (exemplified by handwritten digits), around the measurements dataset (`prostate` in the case of [@Hastie_2009], and in relation to the larger, impossible-to-see-the-patterns datasets of the cancer microarray data). How is this conformation and coherence being done? The repetition of data sets, the juxtaposition of discontinuous domains, and forms of movement construct and order continuities in the service of various forms of predictive and inferential knowledge. The miscible juxtapositions we are dealing with here produce, it seems, a _regularity_ or a continuous common space.

[^2.10]: This kind of data will be the focus of a later chapter \ref{ch:genome}. Machine learning during the 1990s and 2000s was in some ways boosted heavily by the advent of genomic biology with its large, enterprise style knowledge endeavours such as the Human Genome Project.

[^2.11]: The data derives from a publication in _Nature Genetics_ [@Ross_2000] analysing gene expression in the cancer cell lines maintained as experimental models by the US National Cancer Institute.
