# Vectorising data

##todo
- reference history of tables -- e.g. downloaded chapter
- discuss problem of tables -- how they diagrammaticaly generate high D spaces that we then wrestle with, mightly and again diagrammatically -- fallacy of misplaced concreteness?
- add Foucault on the appearance of deep order in life/language/labour -- this is a really important thread to draw through - in this case through the reconstruction of the _table_. 
- add quotes from Foucault on the table of space of order, and ask how he thought about quantity, etc; and what is happening to tables today.
- quotes from Wark on the vector

## Introduction
The character of the transformations that might follow is hard to summarise. Nevertheless, at several specific points in the lines of code above  -- the  `sapply` function and  -- it demonstrates another form of compression or superimposition commonly found in software such as  `R` and other programming languages designed for data practice (Octave, Matlab, Python's NumPy, or C++ Armadillo): _vectorised_ transformations of data.  As a programming language, `R` is striking for its many  `-ply` constructs.  There are several in the core language and many to be found in packages (the popular 'plyr' package; versions of 'ply' can also be found in recent Python data analysis libraries such as Pandas).  These include `apply`, `sapply`, `tapply`, `lapply`, `mapply`. All of the `-ply` constructs have a common feature: they take some collection of things (it may be ordered in many different ways â€“ as a list, as a table, as an array, etc), do something to it, and return something else. This hardly sounds startling. But while most programming languages in common use offer constructs to help deal with collections of things sequentially (for instance, by accessing each element of data set in turn and doing something), `R` offers ways of dealing with  them all at once. The `-ply` constructs ultimately derive from the functional logic developed by the mathematician Alonzo Church in the 1930s [@Church_1936; @Church_1996]. By virtue of these `-ply` constructs,  `R` sometimes presents difficulties for programmers trained to code using so-called procedural programming languages. In many mainstream programming languages, transformations of data are often done using loops and array constructs in which some operation is successively repeated on each element of a data structure. Moreover,  in vectorised languages such as `R`, transformations of a data structure  expressed in one line of code  simultaneously affect all the elements of the data structure. As the widely used _R Cookbook_ puts it, 'many functions [iN `R`] operate on entire vectors, too, and return a vector result' [@Teetor_2011, 38]. Or as _The Art of `R` Programming: A Tour of Statistical Software Design_ by Norman Matloff puts it, 'the fundamental data type in `R` is the _vector_' [@Matloff_2011, 24], and indeed in `R`, all data is vector. There are no individual data types,  only varieties of vectors in `R`. Again, as _R in a Nutshell_  puts the point directly: 'in `R`, any number that you enter ... is interpreted as a vector' or as 'an ordered collection of numbers' [@Adler_2010, 17]. The practical difference is illustrated in the code vignette above in which two 'vectors' of numbers are added together, in the first case using a classic `for`-loop construct, and in the second case using an implicitly vectorised arithmetic operation `+`. The difference in speed between adding $1 ... 10$ using a loop or vector arithmetic is completely trivial here, but when millions of numbers are handled arithmetically, these implementation differences significantly affect the work of both programmers and computing machinery.  This simultaneity is only apparent, since somehow the underlying code has to deal with all the individual elements, but vectorised programming languages take advantage of hardware optimisations or carefully-crafted low-level libraries. 

```{r vectorisation, echo=TRUE, cache=TRUE, message=FALSE, warning=FALSE, comment=NA, size='smallsize', results='markup' } 
 
     vector1 <- c(0,1,2,3,4,5,6,8,9,10)
     vector2 <- c(0,1,2,3,4,5,6,8,9,10)

     # procedural programming-style looped addition
     result_looped = vector()
     for (i in vector1) {
        result_looped[i] = vector1[i] + vector2[i]
     }
     result_looped

     # vectorised addition
     result_vectorised  <- vector1 + vector2
     result_vectorised

```
The functional programming style of applying functions to functions seems strangely abstract. Once you get a feel for them, these -ply constructs  and the implicit vectorisation of many operations on data structures are very convenient to write, and the code runs much faster. They  transform data in ways that utilise  increasingly parallel contemporary chip architectures, and adapt readily to the increasingly Cyclopean infrastructures of cloud computing. The `-ply` operations reduce both data and computational frictions. The real stake in -plying data, however, is not speed but transformation. -Plying makes working with data less like iteration (number, text, table, list, etc), and more like folding a pliable material. Such shifts in feeling for data are very mundane yet crucial to the flow of data.  Plying, especially with added connotation of trade and exchnage as in the 'plying goods', seems much closer to the kinds of work done on data than the figures of data flow or data deluge. 

While the examples of vectorised computation that I have shown are relatively trivial -- turning a nested list into a table, multiplying sequences of numbers -- these vectorised operations have  both concrete and abstract implications.  The concrete implication we have just seen:  the implicit and explicit use of vectorised processes feeds directly into the scaling up of computational  work on data. Not only vectorised computation, but the various styles of programming and infrastructure that lend themselves to simultaneous transformations of large matrixes or tables of data lie at the heart of not only machine learning, but in the intensive processing of data in many different places. The rendering of moving images in  computer graphics depends implicitly on vectorised  transformations of matrices of numbers. 

## Composing data forms  in vector space

The more abstract implication of vectorisation and the forms of multiplication it encourages and demands will take us longer to unravel. Statistical modelling, data-mining, pattern recognition, recommendation systems, network modelling and machine learning rely on  such transformations. 'Fitting a model' to data is often literally implemented, as we will see, by  multiplying data matrices together.  Hence,  describing vectorised transformations of   data in  `R` and other computing environments (for instance, the popular `Map-Reduce` architecture) is not only a matter of pointing to the now familiar increases in  speed or efficiency of computation. The vectoral treatment of data taps into and is interwoven with the transformation of data more generally. Later chapters will discuss various ways in which the vectoral dimensionality of data or its rendering as _vector space_ scales up and scales down in machine learning.[^1]  In fact, a major goal is to disentangle some different forms of vectorisation, or different ways of inhabiting vector space, associated with data today. But to emphasise this abstract mode of existence of data as a vector space  is not to say that the practical transformations of data in various forms of code and computing infrastructure can be ignored. Just the opposite is the case: the  abstract understanding of data as vector space generates many scaling potentials to which particular implementation in code and computing hardware respond. This concretising dynamism arising from abstraction is not new. We need only think of the way in which Marx describes the replacement of highly distributed artisanal manufacture by interconnected systems of machines in factories driven by prime-movers (Watt's steam engine above all) to see a similar process of reorganisation driven by  a numerate abstraction, in that case the cost of labour (see Chapter XV of [@Marx_1986]).  

The praxiographic challenge lies in attending to this highly abstracted vector space in ways that maintain and indeed augment its value-relevance, its concreteness, and attachments to lives and places. We lack good intuitions of how to do that because of the ways in which data as vector space has been constructed and described. While we now often hear about algorithms, predictions and smart devices affecting our lives, the plural actuality of what they compose largely still eludes us. Often data is  represented as if it is an homogeneous mass or a continuous flow, but  it takes many different shapes and has many different _densities_.  I loosely borrow the term 'density' from statistics, where *probability density functions* are often used to describe the hardly ever uniform distribution of probabilities of different values of a variable. Sensing density as a form of variation matters greatly both in machine learning itself, where algorithms seek purchase on unevenly distributed data, and in any broader praxiography of data.  By thinking of data density, we perhaps also gain a better sense of the heft and weave of data. Data  spaces out in many different density shapes, depending on how the data has been generated or instanced. Whatever the starting point (a measuring instrument, people clicking and typing on websites, a device like a camera, a random number generator, etc.), it is inevitable that later transformations will remap  it  to different shapes and forms. A given machine learning algorithm, data visualisation or database query will need the data to be in specific vectoral shapes (vectors,  matrices, arrays, etc), and fit within a certain volume or scale.    The process of composing data for statistical, visual, predictive or even storage purposes, maps a concrete situation, some state of affairs, onto forms imbued with various geometrical, probabilistic, decisionist abstractions often expressed in terms of functions or mathematical models. If, as I have been proposing, we seek contact with the practices, or the concrete value-situation, then the reshaping and reflowing of densities matters greatly.   This forming and reforming of data is evidence of  implicated relations. These practices attest to what Whitehead called 'strain': 'a' feeling in which the forms exemplified in the datum concern geometrical, straight, and flat loci will be called a 'strain.' In a strain, qualitative elements, other than the geometrical forms, express themselves as qualities implicated in those forms' [@Whitehead_1960, 310]. In many machine learning models, for instance, the exemplified forms are straight or flat loci (see the discussion of line fitting, hyperplanes, decision boundaries in the next three chapters).  Yet  different practices also seek to elicit relations that strain the linear or geometrical shaping of data, that show how it does not fit.  Some  practices working in the name of linear forms effectively trace strain feelings. These feelings are the affective connectors between the abstractions and the concrete-value situations in which data arises. They are vital to the psychic life of data, and the ongoing reality-multiples it figures. 

In the machine learning literature, the composition of data is sometimes expressed quite formally. For instance, Peter Flach's _Machine Learning: The Art and Science of Algorithms that Make Sense of Data_ [@Flach_2012] formalises the relation between data and shaped data density like this:

> Features, also call attributes, are defined as mappings $f_i: \mathcal{I} \rightarrow \mathcal{F}_i$ from the instance space $\mathcal{i}$ to the feature domain $\mathcal{F}_i$. We can distinguish features by their domain. [@Flach_2012, 298]

'Features' or 'attributes' often appear as  columns in a table. They are sometimes already present in the data (for instance, the house price dataset features are pre-defined), but they are sometimes constructed or engineered from the data. As the prominent machine learning Pedro Domingos writes in a recent overview of the difficulties of doing machine learning: 'feature engineering is more difficult because it is domain-specific, while learners can be largely general purpose' [@Domingos_2012, 84]. Note that Domingos uses 'domain' here to refer to a concrete situation, whereas Flach's domain refers to a set of values that can be input into a function or a machine learning algorithm. This coincidence in choice of words is actually symptomatic. People reshape data in different ways and for different purposes, sometimes in the name of a concrete situation, sometimes in view of the form of a particular mathematical function or a computational infrastructure (for instance, the amount of RAM heavily affects  modelling). At times, data is  folded together in order to contain it or reduce its dimensionality so that it fits somewhere. At other times, much work goes into expanding the dimensionality of data in order to find ways of eliciting relations that remain somewhat latent or hidden in it, that strain or contort its form in ways that are not easily seen. 

One of the stakes in following what happens to vectors, lists, matrixes, arrays, dictionaries, sets, dataframes, or series or tuples in data, is to get a sense of how such practices map a concrete situation into something more abstract. These are the practices of abstraction, and any experience of abstraction in this domain must process along these lines. More concretely, as we will see in later chapters, the predictive models, the supervised and unsupervised learners, the classifiers, the decision trees and the neural networks in machine learning, to name a few techniques, will be largely implemented as transformations that fold and refold matrices and vectors.  Amidst the many different operations and transformations associated with machine learning and in algorithmic treatments of data, perhaps the single most important practice is matrix multiplication.  It looms large in writing about machine learning, and in many articles and textbooks, matrix manipulations are taken for granted.  For instance, the first full mathematical expression in _The Elements of Statistical Learning_ is shown below along with the surrounding text:

>The linear model has been a mainstay of statistics for the past 30 years and remains one of our most important tools. Given a vector of inputs
$X^T = (X_1 , X_2, . . . , X_p)$, we predict the output $Y$ via the model
>$$\hat{Y} = \hat{\beta_0}  + \sum^p_(j=1) X_j \hat{\beta_j)}$$
>The term $\hat{\beta_0}$ is the intercept, also known as the _bias_ in machine learning [@Hastie_2009, 11].

Introducing the linear model, the authors of the textbook immediately resort to matrix notation. There is nothing particular mathematically elusive here, only a highly aggregated set of sums and multiplications. While such expressions are not easy to read with knowing what all the symbols mean, they offer a very direct opening to thinking about machine learning more generally.  This model is the 'simple but powerful linear model' (11) in which $Y$, the so-called 'response variable' is modelled by adding together  (the $\sum$ operator means adding them) a combination of the input variables or 'features' in  $X_j$ and a value of the intercept $\beta_0$.  Even if this does not make much sense without reading more, we can begin to see that the linear model, a mainstay of machine learning, concretely takes the form of vectors and matrices added and multiplied.  The subscripts $j=1$ refer to individual input variables.   Importantly, as Hastie and co-authors go on to say, 

> Often it is convenient to include the constant variable 1 in $X$, include $\hat{\beta_0)}$ in the vector of coefficients $\hat{\beta}$, and then write the linear model in the vector form as an inner product:
$\hat{Y} = X^T \hat{\beta}$
> where $X^T$ denotes vector or matrix transpose [@Hastie_2009, 12-13].

We will need further explanation to make more sense of these dense expressions, but for the moment, the only thing that needs to come across is the heavy reliance on vectors and matrices, which means that entire models can be expressed highly concisely as matrix multiplication operations. In this case, with the convenient convention of making the intercept into a constant input variable, linear models can be written simply as an 'inner product', that is,  as a matrix-matrix multiplication. Learning machine learning, and learning to implement machine learning techniques, is largely a matter of implementing series of matrix multiplications. As Andrew Ng advises his students,

> Almost any programming language you use will have great linear algebra libraries. And they will be high optimised to do that matrix-matrix multiplication very efficiently including taking advantage of any parallelism your computer. So that you can very efficiently make lots of predictions of lots of hypotheses [@Ng_2008a, 10:50

In other parts of his teaching, and indeed throughout the practice exercises and assignments, Ng stresses the value of implementing machine learning techniques for both understanding them and using them properly. But this is one case where implementation does not facilitate learning. Ng advises his learners against implementing their own matrix handling code.    They should instead  use the 'great linear algebra libraries' found in 'almost any programming language.' 'Linear algebra libraries' multiply, transpose, decompose, invert and generally transform matrices and vectors. They will be 'highly optimised' not because every programming language has been prepared for the advent of machine learning on a large scale, but rather more likely because matrix operations are just so widely used in image and audio processing.  Happily, Ng observes, that means that 'you can make lots of predictions.'  It seems that generating predictions and hypotheses outweighs the value of understanding how things work on this point. 

How do we 'efficiently make lots of predictions' using matrices? Let's return to the example of house prices mentioned by Norman Nie. In this example, house price is generally treated as the response variable ($Y$), and the number of bedrooms and overall size in square feet are the input variables ($X_1$, $X_2$).  A linear model seeks to find a line (or a plane, or as we will see, a hyperplane) that best fits the points. While I will discuss in the next chapter how such a line can be found, here I want to only show one solution to this problem because it takes the form of a matrix multiplication. As Hastie and co-authors write, the 'unique solution' to the problem of fitting a linear model to a given dataset using the most popular method of 'least squares' [@Hastie_2009, 12] is given by:

>$\hat{\beta} = (\mathbf{X}^T\mathbf{X})^-1\mathbf{X}^T\mathbf{y}$

This is quite a tightly coiled expression of how to calculate the parameters $\hat{\beta}$ for a linear model. The two matrices involved are the $X$ input variables (house size and number of bedrooms) and a 1-dimensional matrix $y$, the house price. These matrices are multiplied, transposed (a form of rotation that swaps rows for columns) and inverted (a more complex operation that finds another matrix) Implementing it for  the San Francisco house price data only requires a few extra lines of code:

```{r house_price_predict, echo=TRUE, cache=TRUE, message=FALSE, warning=FALSE, comment=NA, size='smallsize', results='markup' } 
 
    library(MASS)
    houses = read.csv('data/ex1data2.txt', header=FALSE)
    names(houses) = c('size', 'bedrooms', 'price')
    X = as.matrix(cbind(1, houses[,1:2]))
    y = houses$price

    # calculate the coefficients of the model using the least squares algorithm
    beta_hat = ginv(t(X) %*% X) %*% t(X) %*% y
    
    #making a prediction
    rooms =3
    size = 2000
    predicted_price = beta_hat[1] + beta_hat[2]*size + beta_hat[3]*rooms
    plot(houses$size, houses$price)
    abline(beta_hat[1], beta_hat[2])
```

Having calculated the  values of $\hat{\beta}$,  the coefficients of the linear model, we can make predictions of house prices for any given number of bedrooms and house area. For instance, a house with 4 bedrooms and area of 1000 sq. feet should cost around `r predicted_price`. As the plot of the house price data shows, albeit without showing all the variables, the line that has been fitted passes through many of the points. Finding the geometric form of the line or surface is the predictive element of the function. 

We don't need to grasp all the details of the matrix operations working to fit this prediction surface. As with the terseness of  the code that fetches data from the World Bank databases, I want to point mainly here to the tightly coiled matrices  that produce predictions.  One line of fairly dense code that pivots on matrix multiplication operations (`%*%`) effectively makes the model

`beta_hat = ginv(t(X) %*% X) %*% t(X) %*% y`.

The critical question is whether by unwinding some of these operations, for instance, by seeing how  matrix multiplication ripples through different treatments of data, we get closer to the vitality of the multiple/multiplying concrete value-situations that connect calculation and feeling.  My suggestion is that the praxiographic description of how machine learning techniques vectorise and multiply data densities  as abstractions provides a fairly direct way begin to name and unravel the processes of knowing, predicting and deciding on which many aspects of the turn to data rely.  The following three chapters enlarge on this general point about multiplication in different ways. As we will see, the matrix operations we have just been viewing are themselves organised by other layers of intuition that explore shape, movement and surfaces in much more convoluted forms.  (Without mentioning it, the same multiplication operation, the so-called _dot product_ lay at the core of the perceptron algorithm code I quoted from Wikipedia earlier.)  Over a decade ago, the cultural theorist Brian Massumi wrote in an almost mathematical vein that 'the space of experience is really, literally, physically a topological hyperspace of transformation' [@Massumi_2002, 184]. He may have been describing the heavily vectorised operations on which much machine learning, statistical modelling and prediction depend. Experience in so many settings -- media, consumption, science, security, etc -- is embedded in the hyperspaces of matrix transformation, and matrix multiplications. 


[^1]:  In terms of multiplying matrices, dimensionality both constrains and enables many aspects of the prediction. Perhaps on the grounds of data dimensionality alone, we should attend to dimensionality practices in  `R`. A fuller discussion of dimensionality of data is the topic of a later chapter. There I discuss how machine learning re-dimensions data in various ways, sometimes reducing dimensions and at other times, multiplying dimensions.

[^2]: Machine learning in genomics is also the topic of a later chapter. Recent life sciences are not alone in their resort to computational techniques, but in contrast to commercial, industry or government data practices, it is easier to track how data is transformed, reshaped and re-dimensioned in producing biological knowledges. The scientific publications, the public databases and open source software work together to allow this. 

[^3]:In combining Markdown and LaTex, I also make use of the [Pandoc utility](http://johnmacfarlane.net/pandoc/README.html), a command line tool that converts between different document formats.

[^4]: I discuss the competitions and forms of machine learning subjectification in a later chapter. 

[^5]: Isabelle Stenger's  book on Whitehead offers a deeply philosophical introduction to his work [@Stengers_2002]. [TBA: English version]
    


