\chapter{ Optimising machine learners to learn on their own}
\label{ch:subjects}


```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=12, tidy=TRUE, fig.height=8, 
                      echo=FALSE,  warning=FALSE, message=FALSE, dev='pdf')
library(RSQLite)
con <- dbConnect(RSQLite::SQLite(),'../ml_lit/all_refs.sqlite3')
res = dbGetQuery(con, statement ="select * from basic_refs limit 10;")
library(ggplot2)
library(stringr)
library(xtable)
options(xtable.comment = FALSE)
```

> If a proposition, a sentence, a group of signs can be called 'statement' , it is not therefore because, one day, someone happened to speak them or put them into some concrete form of writing; it is because the position of the subject can be assigned [@Foucault_1972,  95] \index{statement!position of subject}

> 'generalization error is what we care about' [@Ng_2008f]

> *Predict if an online bid is made by a machine or a human*, 'Facebook Recruiting IV: Human or Robot?' [@Kaggle_2015e]

Hilary Mason, who was Chief Scientist at bitly.com (an online service that shortens URLs), outlined a machine learning subject position at a London conference in 2012 called 'Bacon: Things Developers Love':

>You have all of these things that are different – engineering, infrastructure, mathematics and computer science, curiosity and an understanding of human behaviour – that is something that usually falls under the social sciences. At the intersection of all these things are wonderful people. But we're starting to develop something new, and that is - not that all of these things have not been done for a very long time - but we are only just now building systems where people, individual people, have all of these tools in one package, in one mind. And there are lots of reasons this is happening now. But its a pretty exciting time to be in any of these things. (Hilary Mason, Chief Scientist, bitly.com) [@Bacon_2012] 

These 'things that are different,'  if they amount to a _statement_ in Michel Foucault's sense of that term, together assign subject positions \index{statement} \index{Foucault, Michel}. In what ways? In front of an audience of several hundred software developers, Mason describes shifts in the work of programming associated with the growth of large amounts of data associated with 'human behaviour.' At the centre of this shift stand 'wonderful people' who combine practices and knowledges of communication infrastructure, technology, statistics, and 'human behaviour' through curiosity and technical skills.  Mason was, in effect, telling her audience of software developers who they should become and at the same time pointing to the some of the expansive changes occurring around them. The title of her talk was 'machine learning for hackers', and her audience were those hackers or programmers whose coding and programming attention may have been previously trained on web interfaces or database queries, but was now drawn towards machine learning. A change in programming practice and a shift towards machine learning was, she implied,  the key to programmers becoming the wonderful people, agents of their own time, capable of doing what is only now just possible because it is all together in 'one package, one mind.' Note that concatenation of 'one package, one mind' does not definitively allocate agency to people or things. Here, I would tentatively suggest, Mason adumbrates the outline of an agent of anticipation, someone or something at the intersection of network infrastructure, mathematics and human behaviour.[^7.1] Mason, one of _Fortune_ magazines 'Top 40 under 40' business leaders to watch [@CNN_2011] and also featured in _Glamour_, a teenage fashion magazine [@Mason_2012], might personify such a wonderful person.\index{Mason, Hilary} She is not a lone example.[^7.2] Equally so, she might _a-personify_ the intersection. 

'It is the privileged machine in this context that creates its marginalized human others' writes Lucy Suchman in her account of  the encounters that 'effect "persons" and "machines" as distinct entities' [@Suchman_2006, 269] \index{Suchman, Lucy}. While Mason and other relatively well-known human machine learners are not exactly marginalized (just the opposite, they achieve minor celebrity status in some cases), Suchman recommends 'recognition of the particularities of bodies and artifacts, of the cultural-historical practices through which human-machine differences are (re-)iteratively drawn, and of the possibilities for and politics of redistribution across the human machine boundary' (285). The intersections that machine learners currently occupy are heavily re-distributional. In almost every instance, machine learners claim to do something that humans alone, no matter how expert, could not. (We need only think of the demonstration of the radio-controlled helicopter flying upside down that Andrew Ng shows in his introduction of machine learning lecture [@Ng_2008] \index{Ng, Andrew}.) Could the 'wonderful people' that Mason describes also be seen as marginalized human others? Does the re-distribution of engineering, mathematics, curiosity, infrastructure and 'something that usually falls under the social sciences' (but perhaps no longer does so?) both energise subjects ('its a pretty exciting time to be in any of these things') and assign them a marginal albeit still pivotal position?

Machine learner subject positions are the topic of this chapter. I draw on artificial neural networks, or neural nets, in their various forms ranging from the perceptron, through the multilayer perceptron (MLP), the convolutional neural nets (CNN), recurrent neural nets (RNN) and deep belief networks or deep neural networks of many recent deep learning projects (particularly in machine learning  competitions, as discussed below [@Dahl_2013]). \index{neural net|see{machine learner!neural net}} in exploring the re-drawing of subject-machine positions.  \index{diagram!machine learner as} Neural nets propagate between infrastructures, engineering and human behaviour (as Mason puts it), re-drawing human-machine differences. A straightforward example of this re-drawing appears in neural net publications. Geoffrey Hinton, Simon Osindero and Yee-Why Teh \index{Hinton, Geoffrey} writing in _Neural Computation_ in 2006 described a 'fast learning algorithm for deep belief nets' [@Hinton_2006a]. Their description, whilst mostly couched in terms of conditional distributions, parameters, and generative models \index{model!generative} also contains a section entitled 'Looking into the Mind of a Neural Network' (1545-1546). In that section, they describe how they used their deep belief network to _generate_ rather than classify images.[^7.01] In the process they were able to see what the 'associative memory has in mind' (1545). The term 'mind' it turns out 'is not intended to be metaphorical' (1546) because the neural net in question has a distributed memory of the digits it has seen. Put slightly more formally, 'the network has a full generative model, so it is easy to look into its mind - we simply generate an image from its high-level representations' (1529).   The conjunction or intersection of 'mind' and generative model draws a new diagram of artificial intelligence (which has typically relied on rule-based or symbolic reasoning), but the appearance of 'mind' in the form of a generative model (see chapter \ref{ch:probability}) covers over other layers of positioning and subjectification. These layers might be more interesting to explore archaeologically (for instance, in the work done on specific artifacts such as images) than in terms of the general existential threat of artificial intelligence. Neural nets re-iterate human machine boundaries through a combination of feeding-forward of potentials and propagating backwards of differences specifically concerned with images. Similarly, the practice of machine learning assigns subject positions in a backward and forward movement that propagates potentialising optimism even as it undercuts the very differences that give rise to that optimism. \index{back-propagation!see{algorithm!back-propagation}}

## The recurrence of neural nets in machine learning 

```{r nn_de, cache=TRUE}

library(stringr)
library(RSQLite)
library(ggplot2)
con <- dbConnect(RSQLite::SQLite(),'../ml_lit/all_refs.sqlite3')
res = dbGetQuery(con, statement = "select DE, WC, SC, TI, PY from basic_refs")
res$WC = tolower(res$WC)
res$DE = tolower(res$DE)
stat =  grep(res$WC, pattern = 'statistic', value=FALSE)
compsci =  grep(res$WC, pattern = 'computer', value=FALSE)
eng =  grep(res$WC, pattern = 'engineer', value=FALSE)
interdisc = intersect(stat, compsci)

norm_terms <- function(x){
    y = str_replace(x, pattern='artificial ', '')
    z = str_replace(y, pattern='svm', 'support vector machine')
    return(z)
}

de_plural <- function(x){
    z = str_replace(x, pattern = '(.+)s$', replacement = '\\1')
    z = norm_terms(z)
    return(z)
}

de_split <- function(x){
    y = unlist(str_split(x, '; '))
    return(y)
}

tablify <- function(x) {
    z  = sort(table(de_plural(de_split(x))), decreasing=TRUE)
    df = data.frame(count = z, discipline = names(z))
    return(df)
} 

stat_df = (head(tablify(res$DE[stat]), 100))
inter_df = (head(tablify(res$DE[interdisc]), 100))
compsci_df = (head(tablify(res$DE[compsci]), 100))
eng_df = (head(tablify(res$DE[eng]), 100))

df = cbind(stat_df, inter_df, compsci_df, eng_df)
dft = xtable(df[1:20,],  label='tab:disc_tech', caption='Techniques and concepts most frequently mentioned in machine learning publication keywords, 1955-2015')
print(dft, type='latex', include.rownames = FALSE)
```

Almost every machine learning class, textbook, demonstration, and in recent years, in machine learning competitions at some point turns to neural nets. They display, however, some instability in the research literature. Table \ref{tab:disc_tech} shows the most frequent keywords for technical publications across the three main disciplinary domains inhabited by machine learners. While neural nets rank very high in computer science and engineering disciplines (appearing just after support vector machines), they do not appear in the statistics literature until `r which(stat_df$discipline == 'neural network')` in the rankings. The prominence of neural nets on the engineering side of machine learning suggests they perform specific enunciative functions. \index{function!enunciative!of neural net}   Neural nets are often described from a deeply split perspective that in some points turns towards human subjects, or at least, the brains of human subjects, and in other ways towards the ongoing expansion of computational platforms. In some ways, they renew long-standing cybernetic hopes of brains and cognition as models of computational intelligence and agency.  Although they stem from a biological inspiration (dating at least back to the work on McCulloch and Pitts in the 1940s [@Halpern_2015; @Wilson_2010]), they gain traction first in the 1980s and then again from mid-2000s onwards, as ways of dealing with changing computational infrastructures, and the difficulties of capitalising on infrastructure that is powerful yet difficult to manage. Neural nets almost constantly oscillate between brain and information infrastructure. In the course of fifty years, their triple re-invention -- from perceptron via neural net to deep belief net -- re-distributes subject positions amidst infrastructural re-configurations and vectorisation. \index{infrastructure!reconfiguration of} \index{vectorisation!of infrastructure} For instance, David Ackley, Geoffrey Hinton (an important figure in the inception of neural nets during the 1980s and in the revival of neural net in the form of deep learning in the last decade), and Terrence Sejnowski wrote in the early 1980s: \index{Hinton, Geoffrey}

> Evidence about the architecture of the brain and the potential of the new VLSI technology have led to a resurgence of interest in “connectionist” systems ...  that store their long-term knowledge as the strengths of the connections between simple neuron-like processing elements. These networks are clearly suited to tasks like vision that can be performed efficiently in parallel networks which have physical connections in just the places where processes need to communicate. ... The more difficult problem is to discover parallel organizations that do not require so much problem-dependent information to be built into the architecture of the network. Ideally, such a system would adapt a given structure of processors and communication paths to whatever problem it was faced with [@Ackley_1985, 147-148].

Alignments and diagrammatic overlaps between brain and 'new VSLI [Very Large Scale Integrated] technology' -- semiconductor chips -- architectures sought to map the plasticity of neuronal networks onto the parallel distributed processing enabled by very densely packed semiconductor circuits. \index{diagram!overlay} The problem here was how to organize these connections without having to hardwire domain specificity into 'the architecture of the network.' How could the architectures adapt to the problem in hand?

We saw in chapter \ref{ch:diagram} that the psychologist Frank Rosenblatt's perceptron [@Rosenblatt_1958] first implemented McCulloch and Pitts' cybernetic vision of neurones as models of computation [@Edwards_1996] \index{machine learner!perceptron}. While the perceptron did not weather the criticism of artificial intelligence experts such as Marvin Minsky (Minsky famously showed that a perceptron cannot learn the logical exclusive OR or `XOR` function; [@Minsky_1969]), cognitive psychologists such as David Rumelhart, Geoffrey Hinton and Ronald Williams persisted with perceptrons, seeking to generalize their operations by connecting them together in networks (also known as multilayer perceptrons). In the mid-1980s, they developed the back-propagation algorithm \index{algorithm!back-propagation} [@Rumelhart_1985; @Hinton_1989], a way of adjusting the connections between nodes (neurones) in the network in response to features in the data (see Figure \ref{fig:rumelhart1985}). The back-propagation algorithm directly addressed the problem of modifying parallel network organizations without reliance on problem-dependent architectures, and in without having to program them in. \index{programmability!problem of!neural net as solution to} Effectively, an architecture of generalization was implemented. While cognition, and the idea that machines would be cognitive (rather than say, mechanical, calculative, or even algorithmic) constantly organised research work in artificial intelligence for several decades, the development of the back-propagation algorithm as a way for a set of connected computational nodes to learn also had strong infrastructural resonances. These resonances became much more visible from around 2006 when 'deep belief nets' appeared as a way of training many-layered neural nets [@Hinton_2006b]. These resonances continue to echo today and indeed attract much attention.[^7.005] Like the advent of VSLI in the early 1980s, the vast concentrations of processing units in contemporary data centres (hundreds of thousands of cores as we saw in the case of Google Compute in the previous chapter \ref{ch:genomic}) and even in the graphics cards developed for high-end gaming  and video rendering (GPUs for PC gaming now typically have a thousand and sometimes several thousand cores) pose the problem of organizing infrastructure so that processes can communicate with each other.Machine learners may well become more important as loose or mutable infrastructural arrangements than as epistemic instruments. 



\begin{figure}
  \centering
      \includegraphics[width=0.9\textwidth]{figure/hinton.pdf}
        \caption{An early publication of the back-propagation algorithm: Rumelhart, Hinton and William's 1985 paper [@Rumelhart\_1985]}
  \label{fig:rumelhart1985}
\end{figure}

The back-propagation or 'backprop' algorithm will return below, but for the moment, this oscillation between cognition and infrastructures, between people and machines, itself suggests another way of thinking about how 'long-term knowledge' takes shape today. At the same time as infrastructural reorganization takes place around learning, and around the production of statements by machine learners, both human and non-human machine learners are assigned new positions. These positions are dispersed, hierarchical and distributed. The subject position in machine learning is a highly relational one rather than a single concentrated form of expertise (as we might find in a clinical oncologist, biostatistician or geologist). Because machine learners construct vectorized epistopics, partial observers traversing vector spaces, inverse probabilisations and pattern dispersions, what counts as agency, skill, action, experience and learning shifts constantly. \index{common vector space!epistopics} It is intimately bound and connected to transforms in infrastructure, variations in referentiality (such as we have seen in the construction of the common vector space), and competing forms of authority (as we have seen in the accumulations of different techniques). As Suchman suggests, examining privileged machines is a way to pay attention to variously marginalized human others.

HERE

The ranking of keywords in table \ref{tab:tech_disc} suggests that some machine learners attribute a more privileged and constitutive function to neural nets. Neural nets synchronically spread into many difference disciplines: cognitive science, computer science, linguistics, adaptive control engineering, psychology, finance, operations research, etc., and particularly statistics and computer science during the 1980-1990s. This  dendritic growth did not just popularise machine learning. It brought engineering and statistics together more strongly. As Ethem Alpaydin writes:

>Perhaps the most important contribution of research on neural networks is this synergy that bridged various disciplines, especially statistics and engineering. It is thanks to this that the field of machine learning is now well established [@Alpaydin_2010, 274]. 

The forms of this field-making bridging are various. We saw some use of neural nets in genomics in the previous chapter (\ref{ch:genome}). But the primary space of coexistence of different disciplines around machine learning has perhaps been competitions during the 1990s that pitted neural nets against other machine learners in classifying handwritten numerals such as zipcodes on envelopes, and in particular one set of handwritten digits that remain in constant use, the MNIST (Modified National Institute of Standards) dataset [@LeCun_2012]. \index{dataset!MNIST} The many competitions focused on the MNIST dataset are, I suggest, a form of demonstration and testing of machines and people that matter greatly to re-iterative drawing of machine-human differences in machine learning. It is no accident that Hastie and co-authors in _Elements of Statistical Learning_ devote a lengthy section to the analysis of these handwritten digit recognition competitions that began in the early 1990s and still today. Like Alpaydin, they affirm the coordinating effect of these competitions on the development of machine learning:

>This problem captured the attention of the machine learning and neural network community for many years, and has remained a benchmark problem in the field [@Hastie_2009, 404].

The handwritten digits used in these competitions, particularly the Neural Information Processing System workshops and KDD Cup (Knowledge Discovery and Data Mining) [@KDD_2013], all come from the MNIST dataset  and during the 1990s, much effort focused on crafting neural nets to recognise these 60,000 or so handwritten digits. As Hastie and co-authors observe, 'at this point the digit recognition datasets became test beds for every new learning procedure, and researchers worked hard to drive down the error rates' [@Hastie_2009, 408-409].

Despite this binding function, neural networks have had a somewhat problematic position in to machine learning. Even in relation to the paradigmatic handwritten digit recognition problem, neural nets struggled to gain purchase.  On the one hand, their analogies and figurations as sophisticated neuronal-style models suggested cognitive capacities surpassing the more geometrical, algebraic and statistically grounded machine learners such as linear discriminant analysis, logistic regression, or decision trees. On the other hand, the density and complexity of their architecture made them difficult to train. Neural nets could easily overfit \index{overfitting} the data. As _Elements of Statistical Learning_ puts it, it required 'pioneering efforts to handcraft the neural network to overcome some these deficiencies..., which ultimately led to the state of the art in neural network performance' [@Hastie_2009, 404]. It is rare to find the word 'handcraft' in machine learning literature. The operational premise of most machine learners is that machine learning works without handcrafting. Somewhat loopily, the competition to automate recognition of handwritten digits entailed much handcrafting and recognition of variations in performances of the digital. 


Neural nets also receive uneven attention in the machine learning literature. In Andrew Ng's Stanford CS229 lectures from 2007, they receive somewhat short shrift: around 30 minutes of discussion in Lecture 6, in between Naive Bayes classifiers and several weeks of lectures on support vector machines [@Ng_2008b]. As he introduces a video of an autonomous vehicle steered by a neural net after a 20 minute training session with a human driver, Ng comments that 'neural nets were the best for many years.' \index{Ng, Andrew} The lectures quickly moves on to the successor, support vector machines.  In _Elements of Statistical Learning_, a whole chapter appears on the topic, but prefaced by a discussion of the antecedent statistical method of 'projection pursuit regression.' The inception of 'projection pursuit' is dated to 1974, and thus precedes the 1980s work on neural nets that was to receive so much attention. In _An Introduction to Statistical Learning with Applications in R_, a book whose authors include Hastie and Tibshirani, neural nets are not discussed and indeed not mentioned [@James_2013]. Textbooks written by computer scientists such as Ethem Alpaydin's _Introduction to Machine Learning_ do usually include at least a chapter on them, sometimes under different titles such as 'multi-layer perceptrons' [@Alpaydin_2010].  Willi Richert and Luis Pedro Coelho's _Building Machine Learning Systems with Python_ likewise does not mention them [@Richert_2013]. Cathy O'Neil and Rachel Schutt's _Doing Data Science_ mentions them but does not discuss them [@Schutt_2013], whereas both Brett Lantz's _Machine Learning with R_ [@Lantz_2013] and Matthew Kirk's _Thoughtful Machine Learning_ [@Kirk_2014] devote chapters to them. In the broader cannon of machine learning texts, the computer scientist Christopher Bishop's heavily cited books on pattern recognition dwell extensively on neural nets [@Bishop_1995; @Bishop_2006]. Amongst statisticians, Brian Ripley's _Pattern Recognition and Neural Networks_ [@Ripley_1996], also highly cited, placed a great deal of emphasis on them. But these specific documents against a pointillistic background of hundreds of thousands of scientific publications mentioning or making use of neural nets since the late 1980s in the usual litany of fields -- atmospheric sciences, biosensors, botany, power systems, water resource management, internal medicine, etc. This swollen publication tide attests to some kind of formation or configuration of knowledge invested in these particular techniques, perhaps more so than other I have discussed so far ( logistic regression, support vector machine, decision trees, random forests, linear discriminant analysis, etc.). 

The shifting fortunes of neural nets are frequently discussed in contrasting terms by machine learners themselves, but in recent years they share an awareness of some kind of transformation:

>Neural networks went out of fashion for a while in the 90s - 2005 because they are hard to train and other techniques like SVMs beat them on some problems. Now people have figured out better methods for training deep neural networks, requiring far fewer problem-specific tweaks. You can use the same pretraining whether you want a neural network to identify whose handwriting it is or if you want to decipher the handwriting, and the same pretraining methods work on very different problems. Neural networks are back in fashion and have been outperforming other methods, and not just in contests [@Zare_2012].

The somewhat vacillating presence of neural nets in the machine learning literature itself finds parallels in the fortunes of individual machine learners. Yann Le Cun's work on optical character recognition during 1980-1990s is said to have discovered the back-propagation algorithm at the same time as Rumelhart, Hinton and Williams [@Rumelhart_1986]. His implementations in `LeNet` led many academic machine learning  competitions during the 1990s. In 2007, Andrew Ng could casually observe that neural nets _were_ the best, but in  2014, Le Cun find himself working on machine learning at Facebook [@Gomes_2014]. \index{Le Cun, Yann} Similarly, the cognitive psychologist Geoffrey Hinton's involvement in the early 1980s work on connectionist learning procedures in neural nets and subsequently on 'deep learning nets' [@Hinton_2006] delivers him to Google in 2013. These trajectories between academic research and industry are not unusual. Many of the techniques in machine learning have been incorporated into companies later acquired by other larger companies. Even if there is no spin-off company to be acquired, machine learners themselves have been assigned key positions in many industry settings. Corinna Cortes, co-inventor with Vladimir Vapnik of the  support vector machine, heads research at Google New York \index{Cortes, Corinna}. In 2011, Ng led a neural net-based project at Google that had, among other things, detected cats  in millions of hours of Youtube videos.[^7.3] Ng himself in 2014 began work as chief scientist for the Chinese search engine, Baidu leading a team of AI researchers specializing in 'deep learning,' the contemporary incarnation of neural nets [@Hof_2014] \index{deep learning}. \index{Ng, Andrew} In recent years, (2012-2015), work on neural nets has again intensified, most prominently in association with social media platforms, but also in the increasingly common speech and face recognition systems found in everyday services and devices. Many of these neural nets are like `kittydar` \index{kittydar}, but implemented on a much larger and more distributed scale (for instance, in classifying videos on Youtube). In contemporary machine learning competitions, as we will see, neural nets again surface as intersectional machines, re-distributing differences between humans and machines. 

[^7.3]: Unlike the cats detected by `kittydar,` the software discussed in the introduction to this book, the Google experiment did not use supervised learning. The deep learning approach was unsupervised [@Markoff_2012]. That is, the neural nets were not trained using labelled images of cats.


[^7.01]: In the case of this paper, and many others related to neural nets, the images are of hand-written digits. These digits have an almost constitutive role, as I discuss in this chapter. 


[^7.1]: In earlier work on machine learning [@Mackenzie_2013], I presented programmers as agents of anticipation, suggesting that the turn to machine learning amongst programmers could be useful in understanding how predictivity was being done amidst broader shift to the regime of anticipation described by Vincenne Adams, Michelle Murphy and Adele Clarke [@Adams_2009]. Subsequently developments in machine learning, even just in the last three years, confirm that view, but in this chapter and in this book more generally, I focus less on transformations in programming practice and software development, and more on the asymmetries of different machine learner  subjects in relation to infrastructures and knowledge.

[^7.2]: Other figures we might follow include Claudia Perlich, Andrew Ng, Geoffrey Hinton, Corinna Cortez,  Daphne Koller, Christopher Bishop, Yann LeCun, or Jeff Hammerbacher.  Although some women's names appear here, in any such list, men's names are much more likely to appear. This is no accident.  \index{machine learner!gender of}


[^7.005]: Although mainstream media accounts of machine learning are not the focus of my interest here, it is hard to ignore the extraordinary level of interest that deep learning projects and techniques have attracted in the last few years. Articles have appeared in all the usual places -- _The New York Times_ [@Markoff_2012], _Wired_[@Garling_2015], or _The Guardian_ [@Arthur_2015]. In many of these accounts, machine learning and neural nets in particular appear both in the guise of the existential threat of artificial intelligence and as a mundane device (for instance, speech recognition on a mobile phone). The spectacular character of deep learning could be analysed in terms like that of the genomes discussed in chapter \ref{ch:genome}. In both cases, the advent and transformation of these machine learners is closely linked to networked platforms (such as Google, Facebook, Yahoo and Microsoft) and their efforts to encompass within their services as many elements of experience, exchange, communication and power as possible. Deep learning  machine learners currently focus mostly on images (photographs and video) and sounds (speech and music), and usually attempt to locate and label objects, words or genres.  \index{machine learner!deep learning!existential threat of}
