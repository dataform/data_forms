
@misc{stanforduniversity_lecture_2008,
	title = {Lecture 2 {\textbar} Machine Learning (Stanford)},
	lccn = {0000},
	url = {http://www.youtube.com/watch?v=5u4G23_OohI&feature=youtube_gdata_player},
	abstract = {Lecture by Professor Andrew Ng for Machine Learning ({CS} 229) in the Stanford Computer Science department.  Professor Ng lectures on linear regression, gradient descent, and normal equations and discusses how they relate to machine learning. 

This course provides a broad introduction to machine learning and statistical pattern recognition. Topics include supervised learning, unsupervised learning, learning theory, reinforcement learning and adaptive control.   Recent applications of machine learning, such as to robotic control, data mining, autonomous navigation, bioinformatics, speech recognition, and text and web data processing are also discussed.

Complete Playlist for the Course:
{http://www.youtube.com/view\_play\_list?p=A89DCFA6ADACE599}

{CCS} 229 Course Website:
http://www.stanford.edu/class/cs229/

Stanford University:
http://www.stanford.edu/

Stanford University Channel on {YouTube:}
http://www.youtube.com/stanford},
	urldate = {2013-02-11},
	collaborator = {{{StanfordUniversity}}},
	month = jul,
	year = {2008}
},

@article{_grill:_2010,
	title = {The Grill: Tom Mitchell},
	lccn = {0000},
	shorttitle = {The Grill},
	url = {http://www.computerworld.com/s/article/346917/The_Grill_Tom_Mitchell},
	urldate = {2012-03-09},
	journal = {Computerworld},
	month = feb,
	year = {2010},
	file = {Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/NKP5QXI2/The_Grill_Tom_Mitchell.html:text/html}
},

@misc{_company_????,
	title = {The Company for Apache Lucene Solr Open Source Search {\textbar} Lucid Imagination},
	url = {http://www.lucidimagination.com/},
	urldate = {2012-03-15}
},

@book{clarke_information_2010,
	title = {Information Retrieval: Implementing and Evaluating Search Engines},
	isbn = {0262026511},
	lccn = {0000},
	shorttitle = {Information Retrieval},
	publisher = {The {MIT} Press},
	author = {Clarke, Charles L. A. and Buettcher, Stefan and Cormack, Gordon V.},
	month = jul,
	year = {2010}
},

@book{manning_introduction_2008,
	edition = {1},
	title = {Introduction to Information Retrieval},
	isbn = {0521865719},
	lccn = {0036},
	publisher = {Cambridge University Press},
	author = {Manning, Christopher D. and Raghavan, Prabhakar and Schütze, Hinrich},
	month = jul,
	year = {2008}
},

@article{breiman_random_2001,
	title = {Random forests},
	volume = {45},
	lccn = {6381},
	number = {1},
	journal = {Machine learning},
	author = {Breiman, L.},
	year = {2001},
	pages = {5–32}
},

@article{ensmenger_is_2012,
	title = {Is Chess the Drosophila of Artificial Intelligence? A Social History of an Algorithm},
	volume = {42},
	issn = {0306-3127, 1460-3659},
	lccn = {0000},
	shorttitle = {Is chess the drosophila of artificial intelligence?},
	url = {http://sss.sagepub.com.ezproxy.lancs.ac.uk/content/42/1/5},
	doi = {10.1177/0306312711424596},
	abstract = {Since the mid 1960s, researchers in computer science have famously referred to chess as the ‘drosophila’ of artificial intelligence ({AI).} What they seem to mean by this is that chess, like the common fruit fly, is an accessible, familiar, and relatively simple experimental technology that nonetheless can be used productively to produce valid knowledge about other, more complex systems. But for historians of science and technology, the analogy between chess and drosophila assumes a larger significance. As Robert Kohler has ably described, the decision to adopt drosophila as the organism of choice for genetics research had far-reaching implications for the development of 20th century biology. In a similar manner, the decision to focus on chess as the measure of both human and computer intelligence had important and unintended consequences for {AI} research. This paper explores the emergence of chess as an experimental technology, its significance in the developing research practices of the {AI} community, and the unique ways in which the decision to focus on chess shaped the program of {AI} research in the decade of the 1970s. More broadly, it attempts to open up the virtual black box of computer software – and of computer games in particular – to the scrutiny of historical and sociological analysis.},
	language = {en},
	number = {1},
	urldate = {2012-05-28},
	journal = {Social Studies of Science},
	author = {Ensmenger, Nathan},
	month = feb,
	year = {2012},
	keywords = {artificial intelligence, computing, drosophila, experimental technology},
	pages = {5--30},
	file = {Is chess the drosophila of artificial intelligence? A social history of an algorithm:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/V3KGKEKN/5.html:text/html;Is chess the drosophila of artificial intelligence? A social history of an algorithm:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/SXFNK8TD/5.html:text/html}
},

@misc{bacon_hilary_2012,
	title = {Hilary Mason - Machine Learning for Hackers},
	url = {http://vimeo.com/43547079},
	abstract = {Vimeo is the home for high-quality videos and the people who love them.},
	urldate = {2012-07-06},
	author = {Bacon},
	month = jun,
	year = {2012},
	file = {Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/XXH9QQJS/43547079.html:text/html}
},

@book{koren_matrix_2009,
	title = {Matrix Factorization Techniques for Recommender Systems},
	abstract = {As the Netflix Prize competition has demonstrated, matrix factorization models are superior to classic nearest-neighbor techniques for producing product recommendations, allowing the incorporation of additional information such as implicit feedback, temporal effects, and confidence levels. Modern consumers are inundated with choices. Electronic retailers and content providers offer a huge selection of products, with unprecedented opportunities to meet a variety of special needs and tastes. Matching consumers with the most appropriate products is key to enhancing user satisfaction and loyalty. Therefore, more retailers have become interested in recommender systems, which analyze patterns of user interest in products to provide personalized recommendations that suit a user’s taste. Because good personalized recommendations can add another dimension to the user experience, e-commerce leaders like Amazon.com and Netflix have made recommender systems a salient part of their websites. Such systems are particularly useful for entertainment products such as movies, music, and {TV} shows. Many customers will view the same movie, and each customer is likely to view numerous different movies. Customers have proven willing to indicate their level of satisfaction with particular movies, so a huge volume of data is available about which movies appeal to which customers. Companies can analyze this data to recommend movies to particular customers. Recommender system strategies Broadly speaking, recommender systems are based on one of two strategies. The content filtering approach creates a profile for each user or product to characterize its nature. For example, a movie profile could include attributes regarding its genre, the participating actors, its box office popularity, and so forth. User profiles might include demographic information or answers provided on a suitable questionnaire. The profiles allow programs to associate users with matching products. Of course, content-based strategies require gathering external information that might not be available or easy to collect. A known successful realization of content filtering is the Music Genome Project, which is used for the Internet radio service Pandora.com. A trained music analyst scores},
	author = {Koren, Yehuda and Bell, Robert and Volinsky, Chris},
	year = {2009},
	file = {Citeseer - Full Text PDF:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/KSM4IF25/Koren et al. - 2009 - Matrix Factorization Techniques for Recommender Sy.pdf:application/pdf;Citeseer - Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/RHNCXHWT/summary.html:text/html}
},

@misc{_dataists_????,
	title = {dataists » A Taxonomy of Data Science},
	url = {http://www.dataists.com/2010/09/a-taxonomy-of-data-science/},
	urldate = {2012-07-06},
	file = {dataists » A Taxonomy of Data Science:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/6VAB8TUH/a-taxonomy-of-data-science.html:text/html}
},

@article{wagstaff_machine_2012,
	title = {Machine Learning that Matters},
	url = {http://arxiv.org/abs/1206.4656},
	abstract = {Much of current machine learning ({ML)} research has lost its connection to problems of import to the larger world of science and society. From this perspective, there exist glaring limitations in the data sets we investigate, the metrics we employ for evaluation, and the degree to which results are communicated back to their originating domains. What changes are needed to how we conduct research to increase the impact that {ML} has? We present six Impact Challenges to explicitly focus the field?s energy and attention, and we discuss existing obstacles that must be addressed. We aim to inspire ongoing discussion and focus on {ML} that matters.},
	urldate = {2012-07-16},
	journal = {{arXiv:1206.4656}},
	author = {Wagstaff, Kiri},
	month = jun,
	year = {2012},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Learning, Statistics - Machine Learning},
	file = {1206.4656 PDF:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/UZRQSNZ9/Wagstaff - 2012 - Machine Learning that Matters.pdf:application/pdf;arXiv.org Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/XGPZ3HCN/1206.html:text/html}
},

@article{langley_changing_2011,
	title = {The changing science of machine learning},
	volume = {82},
	lccn = {0003},
	url = {http://www.springerlink.com/index/J067H855N8223338.pdf},
	number = {3},
	urldate = {2012-06-21},
	journal = {Machine Learning},
	author = {Langley, P.},
	year = {2011},
	pages = {275–279}
},

@article{carstens_sentiment_2011,
	title = {Sentiment Analysis},
	lccn = {0000},
	url = {http://www.doc.ic.ac.uk/teaching/distinguished-projects/2011/l.carstens.pdf},
	urldate = {2012-06-21},
	author = {Carstens, L. and Intelligence, S. A},
	year = {2011}
},

@article{wagstaff_machine_2012-1,
	title = {Machine Learning that Matters},
	lccn = {0000},
	url = {http://ml.jpl.nasa.gov/papers/wagstaff/wagstaff-MLmatters-icml12.pdf},
	urldate = {2012-06-21},
	author = {Wagstaff, K. L},
	year = {2012}
},

@book{vapnik_nature_1999,
	edition = {2nd ed. 2000},
	title = {The Nature of Statistical Learning Theory},
	isbn = {0387987800},
	lccn = {0267},
	publisher = {Springer},
	author = {Vapnik, Vladimir},
	month = dec,
	year = {1999}
},

@inproceedings{ma_identifying_2009,
	title = {Identifying suspicious {URLs:} an application of large-scale online learning},
	lccn = {0108},
	shorttitle = {Identifying suspicious {URLs}},
	url = {http://dl.acm.org/citation.cfm?id=1553462},
	urldate = {2013-03-18},
	booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
	author = {Ma, Justin and Saul, Lawrence K. and Savage, Stefan and Voelker, Geoffrey M.},
	year = {2009},
	pages = {681–688},
	file = {Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/BS4JEB3N/citation.html:text/html}
},

@inproceedings{ma_beyond_2009,
	title = {Beyond blacklists: learning to detect malicious web sites from suspicious {URLs}},
	lccn = {0121},
	shorttitle = {Beyond blacklists},
	url = {http://dl.acm.org/citation.cfm?id=1557153},
	urldate = {2013-03-18},
	booktitle = {Proceedings of the 15th {ACM} {SIGKDD} international conference on Knowledge discovery and data mining},
	author = {Ma, Justin and Saul, Lawrence K. and Savage, Stefan and Voelker, Geoffrey M.},
	year = {2009},
	pages = {1245–1254},
	file = {[PDF] from sinica.edu.tw:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/V3934UNU/Ma et al. - 2009 - Beyond blacklists learning to detect malicious we.pdf:application/pdf;Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/3GI8RQQN/citation.html:text/html}
},

@article{ma_learning_2011,
	title = {Learning to detect malicious {URLs}},
	volume = {2},
	lccn = {0009},
	url = {http://dl.acm.org/citation.cfm?id=1961202},
	number = {3},
	urldate = {2013-03-18},
	journal = {{ACM} Transactions on Intelligent Systems and Technology ({TIST)}},
	author = {Ma, Justin and Saul, Lawrence K. and Savage, Stefan and Voelker, Geoffrey M.},
	year = {2011},
	pages = {30},
	file = {[PDF] from berkeley.edu:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/BJ2GZCKP/Ma et al. - 2011 - Learning to detect malicious URLs.pdf:application/pdf;Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/VS6CUR7Z/citation.html:text/html}
},

@article{le_building_2011,
	title = {Building high-level features using large scale unsupervised learning},
	lccn = {0046},
	url = {http://arxiv.org/abs/1112.6209},
	abstract = {We consider the problem of building high- level, class-specific feature detectors from only unlabeled data. For example, is it possible to learn a face detector using only unlabeled images? To answer this, we train a 9-layered locally connected sparse autoencoder with pooling and local contrast normalization on a large dataset of images (the model has 1 bil- lion connections, the dataset has 10 million 200x200 pixel images downloaded from the Internet). We train this network using model parallelism and asynchronous {SGD} on a clus- ter with 1,000 machines (16,000 cores) for three days. Contrary to what appears to be a widely-held intuition, our experimental re- sults reveal that it is possible to train a face detector without having to label images as containing a face or not. Control experiments show that this feature detector is robust not only to translation but also to scaling and out-of-plane rotation. We also find that the same network is sensitive to other high-level concepts such as cat faces and human bod- ies. Starting with these learned features, we trained our network to obtain 15.8\% accu- racy in recognizing 20,000 object categories from {ImageNet}, a leap of 70\% relative im- provement over the previous state-of-the-art.},
	urldate = {2013-04-21},
	journal = {{arXiv:1112.6209}},
	author = {Le, Quoc V. and Ranzato, {Marc'Aurelio} and Monga, Rajat and Devin, Matthieu and Chen, Kai and Corrado, Greg S. and Dean, Jeff and Ng, Andrew Y.},
	month = dec,
	year = {2011},
	keywords = {Computer Science - Learning},
	file = {1112.6209 PDF:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/QUF2DCRS/Le et al. - 2011 - Building high-level features using large scale uns.pdf:application/pdf;arXiv.org Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/TU8JVCMI/1112.html:text/html}
},

@book{bishop_pattern_2006,
	title = {Pattern recognition and machine learning},
	volume = {1},
	url = {http://www.library.wisc.edu/selectedtocs/bg0137.pdf},
	urldate = {2013-07-12},
	publisher = {springer New York},
	author = {Bishop, Christopher M. and Nasrabadi, Nasser M.},
	year = {2006},
	file = {[PDF] from wisc.edu:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/DWIDSVQD/Bishop and Nasrabadi - 2006 - Pattern recognition and machine learning.pdf:application/pdf}
},

@book{murphy_machine_2012,
	address = {Cambridge, {MA}},
	title = {Machine learning: a probabilistic perspective},
	isbn = {9780262018029  0262018020},
	shorttitle = {Machine learning},
	abstract = {{"This} textbook offers a comprehensive and self-contained introduction to the field of machine learning, based on a unified, probabilistic approach. The coverage combines breadth and depth, offering necessary background material on such topics as probability, optimization, and linear algebra as well as discussion of recent developments in the field, including conditional random fields, L1 regularization, and deep learning. The book is written in an informal, accessible style, complete with pseudo-code for the most important algorithms. All topics are copiously illustrated with color images and worked examples drawn from such application domains as biology, text processing, computer vision, and robotics. Rather than providing a cookbook of different heuristic methods, the book stresses a principled model-based approach, often using the language of graphical models to specify models in a concise and intuitive way. Almost all the models described have been implemented in a {MATLAB} software package--{PMTK} (probabilistic modeling toolkit)--that is freely available online"--Back cover.},
	language = {English},
	publisher = {{MIT} Press},
	author = {Murphy, Kevin P},
	year = {2012}
},

@article{bbc_google_2012,
	chapter = {Technology},
	title = {Google 'brain' machine spots cats},
	url = {http://www.bbc.co.uk/news/technology-18595351},
	abstract = {A Google research team has trained a network of 1,000 computers wired up like the human brain to recognise cats.},
	urldate = {2013-06-06},
	journal = {{BBC} News},
	author = {{BBC}},
	month = jun,
	year = {2012},
	file = {BBC News Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/GX9SSXDE/technology-18595351.html:text/html}
},

@article{breiman_statistical_2001,
	title = {Statistical modeling: The two cultures (with comments and a rejoinder by the author)},
	volume = {16},
	shorttitle = {Statistical modeling},
	url = {http://projecteuclid.org/euclid.ss/1009213726},
	number = {3},
	urldate = {2013-06-10},
	journal = {Statistical Science},
	author = {Breiman, Leo},
	year = {2001},
	pages = {199–231},
	file = {[PDF] from recognition.su:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/U23JA82S/Breiman - 2001 - Statistical modeling The two cultures (with comme.pdf:application/pdf;Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/XNW8S5Q5/DPubS.html:text/html}
},

@misc{stanforduniversity_lecture_2008-1,
	title = {Lecture 1 {\textbar} Machine Learning (Stanford)},
	url = {http://www.youtube.com/watch?v=UzxYlbK2c7E&feature=youtube_gdata_player},
	abstract = {Lecture by Professor Andrew Ng for Machine Learning ({CS} 229) in the Stanford Computer Science department.  Professor Ng provides an overview of the course in this introductory meeting. 

This course provides a broad introduction to machine learning and statistical pattern recognition. Topics include supervised learning, unsupervised learning, learning theory, reinforcement learning and adaptive control.   Recent applications of machine learning, such as to robotic control, data mining, autonomous navigation, bioinformatics, speech recognition, and text and web data processing are also discussed.

Complete Playlist for the Course:
{http://www.youtube.com/view\_play\_list?p=A89DCFA6ADACE599}

{CS} 229 Course Website:
http://www.stanford.edu/class/cs229/

Stanford University:
http://www.stanford.edu/

Stanford University Channel on {YouTube:}
http://www.youtube.com/stanford},
	urldate = {2013-06-10},
	collaborator = {{{StanfordUniversity}}},
	month = jul,
	year = {2008}
},

@article{wacquant_13._2010,
	title = {13. Participant {Observation/Observant} Participation},
	url = {http://books.google.co.uk/books?hl=en&lr=&id=pLSAay_xwjEC&oi=fnd&pg=PA69&dq=wacquant+observant+&ots=LUgGkfHYRD&sig=WwrRprm32d0QB4H8LScW8zlINHc},
	urldate = {2013-06-11},
	journal = {Sociology: Introductory Readings},
	author = {Wacquant, Loïc},
	year = {2010},
	pages = {69},
	file = {Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/JJ9N4N2X/books.html:text/html}
},

@book{barber_bayesian_2011,
	address = {Cambridge; New York},
	title = {Bayesian reasoning and machine learning},
	isbn = {9780521518147 0521518148},
	abstract = {{"Machine} learning methods extract value from vast data sets quickly and with modest resources. They are established tools in a wide range of industrial applications, including search engines, {DNA} sequencing, stock market analysis, and robot locomotion, and their use is spreading rapidly. People who know the methods have their choice of rewarding jobs. This hands-on text opens these opportunities to computer science students with modest mathematical backgrounds. It is designed for final-year undergraduates and master's students with limited background in linear algebra and calculus. Comprehensive and coherent, it develops everything from basic reasoning to advanced techniques within the framework of graphical models. Students learn more than a menu of techniques, they develop analytical and problem-solving skills that equip them for the real world. Numerous examples and exercises, both computer based and theoretical, are included in every chapter. Resources for students and instructors, including a {MATLAB} toolbox, are available online"-- {"Vast} amounts of data present amajor challenge to all thoseworking in computer science, and its many related fields, who need to process and extract value from such data. Machine learning technology is already used to help with this task in a wide range of industrial applications, including search engines, {DNA} sequencing, stock market analysis and robot locomotion. As its usage becomes more widespread, no student should be without the skills taught in this book. Designed for final-year undergraduate and graduate students, this gentle introduction is ideally suited to readers without a solid background in linear algebra and calculus. It covers everything from basic reasoning to advanced techniques in machine learning, and rucially enables students to construct their own models for real-world problems by teaching them what lies behind the methods. Numerous examples and exercises are included in the text. Comprehensive resources for students and instructors are available online"--},
	language = {English},
	publisher = {Cambridge University Press},
	author = {Barber, David},
	year = {2011}
},

@book{wacquant_body_2004,
	address = {Oxford},
	title = {Body and Soul. Notebooks of an apprentice boxer},
	publisher = {Oxford University Press},
	author = {Wacquant, Loic},
	year = {2004}
},

@article{olazaran_sociological_1996,
	title = {A Sociological Study of the Official History of the Perceptrons Controversy},
	volume = {26},
	issn = {0306-3127, 1460-3659},
	lccn = {0023},
	url = {http://sss.sagepub.com/content/26/3/611},
	doi = {10.1177/030631296026003005},
	abstract = {In this paper, I analyze the controversy within Artificial Intelligence ({AI)} which surrounded the `perceptron' project (and neural nets in general) in the late 1950s and early 1960s. I devote particular attention to the proofs and arguments of Minsky and Papert, which were interpreted as showing that further progress in neural nets was not possible, and that this approach to {AI} had to be abandoned. I maintain that this official interpretation of the debate was a result of the emergence, institutionalization and (importantly) legitimation of the symbolic {AI} approach (with its resource allocation system and authority structure). At the `research-area' level, there was considerable interpretative flexibility. This interpretative flexibility was further demonstrated by the revival of neural nets in the late 1980s, and subsequent rewriting of the official history of the debate.},
	language = {en},
	number = {3},
	urldate = {2013-06-17},
	journal = {Social Studies of Science},
	author = {Olazaran, Mikel},
	month = jan,
	year = {1996},
	pages = {611--659},
	file = {Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/U5C4XRIM/611.html:text/html}
},

@misc{ng_2013,
	title = {{\textbar} Machine Learning {III:} Linear Algebra Review},
	url = {https://class.coursera.org/ml-003/lecture/},
	abstract = {Video Lecture:  in Machine Learning on Coursera.},
	urldate = {2013-06-14},
	journal = {Coursera},
	author = {Ng, Andrew},
	file = {Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/N72CWJG4/index.html:text/html}
},

@article{rosenblatt_perceptron:_1958,
	title = {The perceptron: A probabilistic model for information storage and organization in the brain},
	volume = {65},
	copyright = {(c) 2012 {APA}, all rights reserved},
	issn = {1939-1471(Electronic);0033-{295X(Print)}},
	lccn = {0004},
	shorttitle = {The perceptron},
	doi = {10.1037/h0042519},
	abstract = {To answer the questions of how information about the physical world is sensed, in what form is information remembered, and how does information retained in memory influence recognition and behavior, a theory is developed for a hypothetical nervous system called a perceptron. The theory serves as a bridge between biophysics and psychology. It is possible to predict learning curves from neurological variables and vice versa. The quantitative statistical approach is fruitful in the understanding of the organization of cognitive systems. 18 references.},
	number = {6},
	journal = {Psychological Review},
	author = {Rosenblatt, F.},
	year = {1958},
	keywords = {brain, information storage, probabilistic model},
	pages = {386--408}
},

@article{minsky_perceptron:_1969,
	title = {Perceptron: an introduction to computational geometry},
	volume = {19},
	lccn = {0299},
	shorttitle = {Perceptron},
	journal = {The {MIT} Press, Cambridge, expanded edition},
	author = {Minsky, Marvin and Papert, Seymour},
	year = {1969},
	pages = {88}
},

@misc{dahl_deep_????,
	title = {Deep Learning How I Did It: Merck 1st place interview},
	shorttitle = {Deep Learning How I Did It},
	url = {http://blog.kaggle.com/2012/11/01/deep-learning-how-i-did-it-merck-1st-place-interview/},
	abstract = {What was your background prior to entering this challenge? We are a team of computer science and statistics academics. Ruslan Salakhutdinov and Geoff Hinton are professors at the University of Toro...},
	urldate = {2013-06-17},
	journal = {no free hunch},
	author = {Dahl, George},
	keywords = {code, geoff hinton, learning, machine, merck, model, neural, professor hinton},
	file = {Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/2M52PT7J/deep-learning-how-i-did-it-merck-1st-place-interview.html:text/html}
},

@article{hinton_reducing_2006,
	title = {Reducing the dimensionality of data with neural networks},
	volume = {313},
	lccn = {0928},
	url = {http://www.sciencemag.org/content/313/5786/504.short},
	number = {5786},
	urldate = {2013-06-17},
	journal = {Science},
	author = {Hinton, Geoffrey E. and Salakhutdinov, Ruslan R.},
	year = {2006},
	pages = {504–507},
	file = {[PDF] from uni-saarland.de:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/KGM75BT4/Hinton and Salakhutdinov - 2006 - Reducing the dimensionality of data with neural ne.pdf:application/pdf;Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/WRD6BG4A/504.html:text/html}
},

@article{ackley_learning_1985,
	title = {A learning algorithm for Boltzmann machines},
	volume = {9},
	lccn = {2119},
	url = {http://www.sciencedirect.com/science/article/pii/S0364021385800124},
	number = {1},
	urldate = {2013-06-17},
	journal = {Cognitive science},
	author = {Ackley, David H. and Hinton, Geoffrey E. and Sejnowski, Terrence J.},
	year = {1985},
	pages = {147–169},
	file = {Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/G6CJZC4J/S0364021385800124.html:text/html}
},

@misc{_perceptron_2013,
	title = {Perceptron},
	copyright = {Creative Commons Attribution-{ShareAlike} License},
	lccn = {0001},
	url = {http://en.wikipedia.org/w/index.php?title=Perceptron&oldid=557301943},
	abstract = {In computational geometry, the perceptron is an algorithm for supervised classification of an input into one of several possible non-binary outputs. It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector describing a given input using the delta rule. The learning algorithm for perceptrons is an online algorithm, in that it processes elements in the training set one at a time.},
	language = {en},
	urldate = {2013-06-17},
	journal = {Wikipedia, the free encyclopedia},
	month = may,
	year = {2013},
	note = {Page Version {ID:} 557301943},
	file = {Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/P3MWEXNI/index.html:text/html}
},

@article{patil_pymc:_2010,
	title = {{PyMC:} Bayesian stochastic modelling in Python},
	volume = {35},
	lccn = {0071},
	shorttitle = {{PyMC}},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3097064/},
	number = {4},
	urldate = {2013-06-17},
	journal = {Journal of statistical software},
	author = {Patil, Anand and Huard, David and Fonnesbeck, Christopher J.},
	year = {2010},
	pages = {1},
	file = {[HTML] from nih.gov:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/3NWB7TGN/PMC3097064.html:text/html}
},

@misc{koller_daphne_2012,
	title = {Daphne Koller: What we're learning from online education {\textbar} Video on {TED.com}},
	shorttitle = {Daphne Koller},
	url = {http://www.ted.com/talks/daphne_koller_what_we_re_learning_from_online_education.html},
	abstract = {Daphne Koller is enticing top universities to put their most intriguing courses online for free -- not just as a service, but as a way to research how people learn. With Coursera (cofounded by Andrew Ng), each keystroke, quiz, peer-to-peer discussion and self-graded assignment builds an unprecedented pool of data on how knowledge is processed.},
	urldate = {2013-06-24},
	collaborator = {Koller, Daphne},
	month = aug,
	year = {2012},
	keywords = {Computers, education, global issues, Internet, Talks, {TED}},
	file = {Snapshot:/home/mackenza/.mozilla/firefox/w5dcspnj.default/zotero/storage/83HZTXGW/daphne_koller_what_we_re_learning_from_online_education.html:text/html}
},

@book{alpaydin_introduction_2010,
	address = {Cambridge, Massachusetts; London},
	title = {Introduction to machine learning},
	isbn = {9780262012430  {026201243X}},
	language = {English},
	publisher = {The {MIT} Press},
	author = {Alpaydin, E},
	year = {2010}
},

@book{rasmussen_gaussian_2006,
	address = {Cambridge, Mass.},
	title = {Gaussian processes for machine learning},
	isbn = {{026218253X} 9780262182539},
	abstract = {{"Gaussian} processes ({GPs)} provide a principled, practical, probabilistic approach to learning in kernel machines. {GPs} have received increased attention in the machine-learning community over the past decade, and this book provides a long-needed systematic and unified treatment of theoretical and practical aspects of {GPs} in machine learning. The treatment is comprehensive and self-contained, targeted at researchers and students in machine learning and applied statistics."--Jacket.},
	language = {English},
	publisher = {{MIT} Press},
	author = {Rasmussen, Carl Edward and Williams, Christopher K. I},
	year = {2006}
},

@book{marsland_machine_2009,
	address = {Boca Raton, Mass. [u.a.},
	title = {Machine learning: an algorithmic perspective},
	isbn = {9781420067187  1420067184},
	shorttitle = {Machine learning},
	language = {English},
	publisher = {{CRC} {Press/Taylor} \& Francis},
	author = {Marsland, Stephen},
	year = {2009}
},

@book{mitchell_machine_1997,
	address = {New York, {NY} [u.a.},
	title = {Machine learning},
	isbn = {0071154671 9780071154673},
	language = {English},
	publisher = {{McGraw-Hill}},
	author = {Mitchell, Tom M},
	year = {1997}
}