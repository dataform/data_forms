<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta property="og:title" content="" />
<meta property="og:type" content="book" />







<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="">

<title></title>

<script src="libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="libs/navigation-1.1/tabsets.js"></script>





<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
</style>
</head>

<body>

<div class="container-fluid main-container">


<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="1-acknowledgments.html#acknowledgments"><span class="toc-section-number">1</span> Acknowledgments</a></li>
<li><a href="2-machine-learners.html#machine-learners"><span class="toc-section-number">2</span> 250,000 machine learners</a></li>
<li><a href="3-a-summary-of-the-argument.html#a-summary-of-the-argument"><span class="toc-section-number">3</span> A summary of the argument</a></li>
<li><a href="4-in-situ-hybridization.html#in-situ-hybridization"><span class="toc-section-number">4</span> In-situ hybridization</a></li>
<li><a href="5-critical-operational-practice.html#critical-operational-practice"><span class="toc-section-number">5</span> Critical operational practice?</a></li>
<li><a href="6-obstacles-to-the-work-of-freeing-machine-learning.html#obstacles-to-the-work-of-freeing-machine-learning"><span class="toc-section-number">6</span> Obstacles to the work of freeing machine learning</a></li>
<li><a href="7-preface.html#preface"><span class="toc-section-number">7</span> Preface</a></li>
<li><a href="8-introduction-into-the-data.html#introduction-into-the-data"><span class="toc-section-number">8</span> Introduction: Into the Data</a></li>
<li><a href="9-three-accumulations-settings-data-and-devices.html#three-accumulations-settings-data-and-devices"><span class="toc-section-number">9</span> Three accumulations: settings, data and devices</a></li>
<li><a href="10-who-or-what-is-a-machine-learner.html#who-or-what-is-a-machine-learner"><span class="toc-section-number">10</span> Who or what is a machine learner?</a></li>
<li><a href="11-algorithmic-control-to-the-machine-learners.html#algorithmic-control-to-the-machine-learners"><span class="toc-section-number">11</span> Algorithmic control to the machine learners?</a></li>
<li><a href="12-the-archaeology-of-operations.html#the-archaeology-of-operations"><span class="toc-section-number">12</span> The archaeology of operations</a></li>
<li><a href="13-asymmetries-in-common-knowledge.html#asymmetries-in-common-knowledge"><span class="toc-section-number">13</span> Asymmetries in common knowledge</a></li>
<li><a href="14-what-cannot-be-automated.html#what-cannot-be-automated"><span class="toc-section-number">14</span> What cannot be automated?</a></li>
<li><a href="15-different-fields-in-machine-learning.html#different-fields-in-machine-learning"><span class="toc-section-number">15</span> Different fields in machine learning?</a></li>
<li><a href="16-the-diagram-in-critical-thought.html#the-diagram-in-critical-thought"><span class="toc-section-number">16</span> The diagram in critical thought</a></li>
<li><a href="17-we-dont-have-to-write-programs.html#we-dont-have-to-write-programs"><span class="toc-section-number">17</span> ‘We don’t have to write programs’?</a></li>
<li><a href="18-the-elements-of-machine-learning.html#the-elements-of-machine-learning"><span class="toc-section-number">18</span> The elements of machine learning</a></li>
<li><a href="19-who-reads-machine-learning-textbooks.html#who-reads-machine-learning-textbooks"><span class="toc-section-number">19</span> Who reads machine learning textbooks?</a></li>
<li><a href="20-r-a-matrix-of-transformations.html#r-a-matrix-of-transformations"><span class="toc-section-number">20</span> <code>R</code>: a matrix of transformations</a></li>
<li><a href="21-the-obdurate-mathematical-glint-of-machine-learning.html#the-obdurate-mathematical-glint-of-machine-learning"><span class="toc-section-number">21</span> The obdurate mathematical glint of machine learning</a></li>
<li><a href="22-cs229-2007-returning-again-and-again-to-certain-features.html#cs229-2007-returning-again-and-again-to-certain-features"><span class="toc-section-number">22</span> CS229, 2007: returning again and again to certain features</a></li>
<li><a href="23-the-visible-learning-of-machine-learning.html#the-visible-learning-of-machine-learning"><span class="toc-section-number">23</span> The visible learning of machine learning</a></li>
<li><a href="24-the-diagram-of-an-operational-formation.html#the-diagram-of-an-operational-formation"><span class="toc-section-number">24</span> The diagram of an operational formation</a></li>
<li><a href="25-vector-space-and-geometry.html#vector-space-and-geometry"><span class="toc-section-number">25</span> Vector space and geometry</a></li>
<li><a href="26-mixing-places.html#mixing-places"><span class="toc-section-number">26</span> Mixing places</a></li>
<li><a href="27-truth-is-no-longer-in-the-table.html#truth-is-no-longer-in-the-table"><span class="toc-section-number">27</span> Truth is no longer in the table?</a></li>
<li><a href="28-the-epistopic-fault-line-in-tables.html#the-epistopic-fault-line-in-tables"><span class="toc-section-number">28</span> The epistopic fault line in tables</a></li>
<li><a href="29-surface-and-depths-the-problem-of-volume-in-data.html#surface-and-depths-the-problem-of-volume-in-data"><span class="toc-section-number">29</span> Surface and depths: the problem of volume in data</a></li>
<li><a href="30-vector-space-expansion.html#vector-space-expansion"><span class="toc-section-number">30</span> Vector space expansion</a></li>
<li><a href="31-drawing-lines-in-a-common-space-of-transformation.html#drawing-lines-in-a-common-space-of-transformation"><span class="toc-section-number">31</span> Drawing lines in a common space of transformation</a></li>
<li><a href="32-implicit-vectorization-in-code-and-infrastructures.html#implicit-vectorization-in-code-and-infrastructures"><span class="toc-section-number">32</span> Implicit vectorization in code and infrastructures</a></li>
<li><a href="33-lines-traversing-behind-the-light.html#lines-traversing-behind-the-light"><span class="toc-section-number">33</span> Lines traversing behind the light</a></li>
<li><a href="34-the-vectorised-table.html#the-vectorised-table"><span class="toc-section-number">34</span> The vectorised table?</a></li>
<li><a href="35-learning-functions.html#learning-functions"><span class="toc-section-number">35</span> Learning functions</a></li>
<li><a href="36-supervised-unsupervised-reinforcement-learning-and-functions.html#supervised-unsupervised-reinforcement-learning-and-functions"><span class="toc-section-number">36</span> Supervised, unsupervised, reinforcement learning and functions</a></li>
<li><a href="37-which-function-operates.html#which-function-operates"><span class="toc-section-number">37</span> Which function operates?</a></li>
<li><a href="38-what-does-a-function-learn.html#what-does-a-function-learn"><span class="toc-section-number">38</span> What does a function learn?</a></li>
<li><a href="39-observing-with-curves-the-logistic-function.html#observing-with-curves-the-logistic-function"><span class="toc-section-number">39</span> Observing with curves: the logistic function</a></li>
<li><a href="40-the-cost-of-curves-in-machine-learning.html#the-cost-of-curves-in-machine-learning"><span class="toc-section-number">40</span> The cost of curves in machine learning</a></li>
<li><a href="41-curves-and-the-variation-in-models.html#curves-and-the-variation-in-models"><span class="toc-section-number">41</span> Curves and the variation in models</a></li>
<li><a href="42-observing-costs-losses-and-objectives-through-optimisation.html#observing-costs-losses-and-objectives-through-optimisation"><span class="toc-section-number">42</span> Observing costs, losses and objectives through optimisation</a></li>
<li><a href="43-gradients-as-partial-observers.html#gradients-as-partial-observers"><span class="toc-section-number">43</span> Gradients as partial observers</a></li>
<li><a href="44-the-power-to-learn.html#the-power-to-learn"><span class="toc-section-number">44</span> The power to learn</a></li>
<li><a href="45-data-reduces-uncertainty.html#data-reduces-uncertainty"><span class="toc-section-number">45</span> Data reduces uncertainty?</a></li>
<li><a href="46-machine-learning-as-statistics-inside-out.html#machine-learning-as-statistics-inside-out"><span class="toc-section-number">46</span> Machine learning as statistics inside out</a></li>
<li><a href="47-distributed-probabilities.html#distributed-probabilities"><span class="toc-section-number">47</span> Distributed probabilities</a></li>
<li><a href="48-naive-bayes-and-the-distribution-of-probabilities.html#naive-bayes-and-the-distribution-of-probabilities"><span class="toc-section-number">48</span> Naive Bayes and the distribution of probabilities</a></li>
<li><a href="49-spam-when-foralln-is-too-much.html#spam-when-foralln-is-too-much"><span class="toc-section-number">49</span> Spam: when <span class="math inline">\(\forall{N}\)</span> is too much?</a></li>
<li><a href="50-the-improbable-success-of-the-naive-bayes-classifier.html#the-improbable-success-of-the-naive-bayes-classifier"><span class="toc-section-number">50</span> The improbable success of the Naive Bayes classifier</a></li>
<li><a href="51-ancestral-probabilities-in-documents-inference-and-prediction.html#ancestral-probabilities-in-documents-inference-and-prediction"><span class="toc-section-number">51</span> Ancestral probabilities in documents: inference and prediction</a></li>
<li><a href="52-statistical-decompositions-bias-variance-and-observed-errors.html#statistical-decompositions-bias-variance-and-observed-errors"><span class="toc-section-number">52</span> Statistical decompositions: bias, variance and observed errors</a></li>
<li><a href="53-does-machine-learning-construct-a-new-statistical-reality.html#does-machine-learning-construct-a-new-statistical-reality"><span class="toc-section-number">53</span> Does machine learning construct a new statistical reality?</a></li>
<li><a href="54-splitting-and-the-growth-of-trees.html#splitting-and-the-growth-of-trees"><span class="toc-section-number">54</span> Splitting and the growth of trees</a></li>
<li><a href="55-differences-in-recursive-partitioning.html#differences-in-recursive-partitioning"><span class="toc-section-number">55</span> 1984: Differences in recursive partitioning</a></li>
<li><a href="56-limiting-differences.html#limiting-differences"><span class="toc-section-number">56</span> Limiting differences</a></li>
<li><a href="57-the-successful-dispersion-of-the-support-vector-machine.html#the-successful-dispersion-of-the-support-vector-machine"><span class="toc-section-number">57</span> The successful dispersion of the support vector machine</a></li>
<li><a href="58-differences-blur.html#differences-blur"><span class="toc-section-number">58</span> Differences blur?</a></li>
<li><a href="59-bending-the-decision-boundary.html#bending-the-decision-boundary"><span class="toc-section-number">59</span> Bending the decision boundary</a></li>
<li><a href="60-instituting-patterns.html#instituting-patterns"><span class="toc-section-number">60</span> Instituting patterns</a></li>
<li><a href="61-genomic-referentiality-and-materiality.html#genomic-referentiality-and-materiality"><span class="toc-section-number">61</span> Genomic referentiality and materiality</a></li>
<li><a href="62-the-genome-as-threshold-object.html#the-genome-as-threshold-object"><span class="toc-section-number">62</span> The genome as threshold object</a></li>
<li><a href="63-genomic-knowledges-and-their-datasets.html#genomic-knowledges-and-their-datasets"><span class="toc-section-number">63</span> Genomic knowledges and their datasets</a></li>
<li><a href="64-the-advent-of-wide-dirty-and-mixed-data.html#the-advent-of-wide-dirty-and-mixed-data"><span class="toc-section-number">64</span> The advent of ‘wide, dirty and mixed’ data</a></li>
<li><a href="65-cross-validating-machine-learning-in-genomics.html#cross-validating-machine-learning-in-genomics"><span class="toc-section-number">65</span> Cross-validating machine learning in genomics</a></li>
<li><a href="66-proliferation-of-discoveries.html#proliferation-of-discoveries"><span class="toc-section-number">66</span> Proliferation of discoveries</a></li>
<li><a href="67-variations-in-the-object-or-in-the-machine-learner.html#variations-in-the-object-or-in-the-machine-learner"><span class="toc-section-number">67</span> Variations in the object or in the machine learner?</a></li>
<li><a href="68-whole-genome-functions.html#whole-genome-functions"><span class="toc-section-number">68</span> Whole genome functions</a></li>
<li><a href="69-propagation-across-human-machine-boundaries.html#propagation-across-human-machine-boundaries"><span class="toc-section-number">69</span> Propagation across human-machine boundaries</a></li>
<li><a href="70-competitive-positioning.html#competitive-positioning"><span class="toc-section-number">70</span> Competitive positioning</a></li>
<li><a href="71-a-privileged-machine-and-its-diagrammatic-forms.html#a-privileged-machine-and-its-diagrammatic-forms"><span class="toc-section-number">71</span> A privileged machine and its diagrammatic forms</a></li>
<li><a href="72-varying-subject-positions-in-code.html#varying-subject-positions-in-code"><span class="toc-section-number">72</span> Varying subject positions in code</a></li>
<li><a href="73-the-subjects-of-a-hidden-operation.html#the-subjects-of-a-hidden-operation"><span class="toc-section-number">73</span> The subjects of a hidden operation</a></li>
<li><a href="74-algorithms-that-propagate-errors.html#algorithms-that-propagate-errors"><span class="toc-section-number">74</span> Algorithms that propagate errors</a></li>
<li><a href="75-competitions-as-examination.html#competitions-as-examination"><span class="toc-section-number">75</span> Competitions as examination</a></li>
<li><a href="76-superimposing-power-and-knowledge.html#superimposing-power-and-knowledge"><span class="toc-section-number">76</span> Superimposing power and knowledge</a></li>
<li><a href="77-ranked-subject-positions.html#ranked-subject-positions"><span class="toc-section-number">77</span> Ranked subject positions</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="does-machine-learning-construct-a-new-statistical-reality" class="section level1">
<h1><span class="header-section-number">53</span> Does machine learning construct a new statistical reality?</h1>
<p> Following a broadly Foucaultean line of argument, Hacking proposes that statistical thinking and practice in the nineteenth and early twentieth century ontologically re-configured things in terms of probability distributions (and the Gaussian distribution in particular). What happens in worlds where the statistical treatment of error – the bias-variance decomposition is a shorthand term for this – distributes probability throughout an operational formation?  I have suggested that an ancestral probabilisation of domains and the statistical decomposition of error come together in statistical machine learning. The bias-variance decomposition includes both tightly bound points and certainly relatively free or unbound points, as we saw in the case of the Naive Bayes classifier in its encounter with data. It generates highly erroneous probability estimates but performs well as a classifier.</p>
<p>Viewed diagrammatically, unbound points matter greatly to the relations of force at work in a knowledge-power conjunction. Probabilisation gives machine learning a relation to its own plurality, to the tendencies of its models to proliferate and vary.  Every attempt to construct a machine learner in a given setting draws on both the re-iteration of ancestral probabilities (that is, prior structuring of settings in conformity with some probability distribution) or on the many interactive adjustments, re-distributions and re-samplings of the data <em>and</em> transformations of the models associated with the bias-variance decomposition.</p>
<p>Mayer-Schönberger and Cukier argue that having much data or all data (<span class="math inline">\(N=\forall{\boldsymbol{X}}\)</span>) re-bases knowledge.  Versions of this claim can be found running through various scientific and business settings throughout the 20th century.<a href="#fn62" class="footnoteRef" id="fnref62"><sup>62</sup></a> In certain settings, <span class="math inline">\(N=all\)</span> has been around for quite a while (as for instance, in many document classification settings where the whole archive or corpus of documents have been electronically curated for decades). Mayer-Schönberger and Cukier rightly emphasize that the huge quantities of data sluicing through some contemporary infrastructures support wider inferences (11). Their discounting of statistical sampling as a concept ‘developed to solve a particular problem at a particular moment in time under specific technological constraints’ <span class="citation">[@Mayer-Schonberger_2013, 31]</span> does not, however, accommodate the operational practices of sampling that pervade machine learning, particularly in the forms of probabilisation.</p>
<p>Whether or not someone uses Naive Bayes, a topic model, neural networks or logistic regression, does not greatly alter the processes of probabilisation. Random variables, probability distributions, errors and model selection practices crowd in around and re-configure machine learners as members of a population generating statements.  In many ways, the Mayer-Schönberger and Cukier account bobs in the wake of the enterprise-wide accumulations of data. They pay so much attention to the capital potentials of data accumulation that they cannot easily attend to the question of how machine learners probabilise that data. Sampling, estimation, likelihoods, and a whole gamut of dynamic relationships between random variables in joint probability distributions reassert themselves amidst a population of models. The data may not be sampled, but models moving through the high-dimensional vector spaces opened up by having ‘all’ the data transform it probabilistically. While not all machine learners are strictly speaking probabilistic models,<a href="#fn63" class="footnoteRef" id="fnref63"><sup>63</sup></a> machine learners relate to themselves and the data as populations defined by probability distributions.</p>
<p>Machine learning inhabits a reality that had already introjected statistical realities at least a century earlier, whether through the social physics of Quetelet, the biopolitical norms of Francis Galton and his regression to the mean (the linear model of regression is probably the basic machine learning model) or later, in the probability functions of quantum mechanics in early twentieth century physics.    Assembling an aggregate reality of many devices, machine learning inverts probability distributions. In this inversion, probability distributions, which had become the operational statement and model of truth for many different kinds of populations, fold back or re-distribute themselves into devices such as machine learners whose variations and uncertainties become populations. Populations of models are sampled, measured, and aggregated in the ongoing production of statistical realities whose object is no longer a property of individual members of a population (their height, their life-expectancy, their chance of HIV/AIDS), but a population of models of populations.</p>

<p>\chapter{Patterns and differences </p>
<blockquote>
<p>The notion of pattern involves the concept of different modes of togetherness <span class="citation">[@Whitehead_1956, 195-6]</span>.  </p>
</blockquote>
<blockquote>
<p>Algorithms for pattern recognition were therefore from the very beginning associated with the construction of linear decision surfaces <span class="citation">[@Cortes_1995, 273-4]</span> </p>
</blockquote>
<p>Do machine learners generate new patterns of difference? Should we hold machine learners accountable for their claims to recognise patterns in data in the same way we hold experimental scientists accountable for their factual claims?<a href="#fn64" class="footnoteRef" id="fnref64"><sup>64</sup></a>  This chapter explores two major machine learning treatments of pattern dating from the last decades of the twentieth century from the standpoint of differences. I suggest that what counts as pattern changes in machine learning over time. While much machine learning strains to identify differences in terms of differences of degree, the practice of pattern-finding itself harbours differences of kind.  For critical thought, the connection between pattern and differences is particularly important, since if machine learning changes what counts as pattern, this will also affect the recognition or articulation of differences.  We have seen the emergence of the vector space and its vectorised transformations, the multiplication of operational functions and their associated partial observers, and then the probabilisation that distributes machine learners into populations of error-sensitive learners. What in this diagram and in the forest-like growth of techniques, projects, applications and proponents, allows us to make sense of what happens to differences in machine learning?</p>
<p>Across vectors, functions and populations, the diagram of machine learning weaves and knots many points of emergence, continuity and conjunction. I view the formidable accumulations of infrastructure, devices and expertise accrediting around machine learning as multi-faceted abstractions, where abstraction is understood diagrammatically as a concretising entanglement of references.  Three highly developed and heavily used machine learners – decision trees, support vector machines and neural nets – more or less mesmerised machine learning between 1980-2000. They initiated relatively novel and somewhat heterogeneous diagrammatic movements into data.    These diagrammatic movements, which we might characterise as <em>splitting</em>, and <em>marginalising</em> not only animate subsequent machine learners in producing newer techniques, they re-configure what counts as pattern.  Since machine learning has no fixed idea of pattern (a term lacks much operational definition), then claims that machine learners uncover hidden patterns in data might be better grounded in the operational practices of working with differences.<a href="#fn65" class="footnoteRef" id="fnref65"><sup>65</sup></a></p>
<p>As machine learners of recent decades, the decision tree and support vector machine embody a new , a way of describing, locating and perceiving differences in which differences of degree and differences of kind are re-mapped.  Every machine learner generates statements, but from different places, by somewhat different individuals, and from the different situations they ‘occupy in relation to the various domains or groups of objects’ <span class="citation">[@Foucault_1972, 52]</span>.<a href="#fn66" class="footnoteRef" id="fnref66"><sup>66</sup></a> Practically, decision trees and support vector machines loom large in various contemporary accounts of machine learning as a way of knowing (for instance, in popular machine learning books such as <em>Machine Learning for Hackers</em> <span class="citation">[@Conway_2012]</span> or <em>Doing Data Science</em> <span class="citation">[@Schutt_2013]</span>). The machine learning research published in statistics, computer science, mathematics, artificial intelligence and a swathe of related scientific fields during 1980-2010 bristles with references to decision trees and support vector machine, as well as neural networks.<a href="#fn67" class="footnoteRef" id="fnref67"><sup>67</sup></a></p>
<p>Rather than seeing pattern as something discovered in data, the notion of enunciative modality suggests we should examine the diagrammatic operations that configure differences in the practice of machine learning, giving rise to a field of patterns attributed to objects or subject positions. The two machine learners that anchor this chapter are perhaps the most distinctive data mining, pattern recognition and predictive modelling achievements of the late twentieth century (at least judging by the citations and usage they attract). They differ greatly in how they move through data. At certain times, they come together (for instance, in machine learning competitions discussed in chapter ; or in certain formalizations such as machine learning theory or in graphs of the bias-variance decomposition discussed in chapter ; or in the pedagogy of machine learning discussed in chapter ).</p>
</div>
<div class="footnotes">
<hr />
<ol start="62">
<li id="fn62"><p>Later chapters of this book will track several instances of having all the data in the sciences, in government and in business in order to show what having all the data entails in different settings. <a href="53-does-machine-learning-construct-a-new-statistical-reality.html#fnref62">↩</a></p></li>
<li id="fn63"><p>Machine learning textbooks written by computer scientists tend to define probabilistic models more narrowly. As Peter Flach suggests:</p><blockquote><p>Probabilistic models view learning as a process of reducing uncertainty using data. For instance, a Bayesian classifier models the posterior distribution <span class="math inline">\(P(Y|X)\)</span> (or its counterpart, the likelihood function <span class="math inline">\(P(X|Y)\)</span>) which tells me the class distribution <span class="math inline">\(Y\)</span> after observing the features values <span class="math inline">\(X\)</span> <span class="citation">[@Flach_2012, 47]</span></p></blockquote><p>But whether they are probabilistic in this sense or not, the evaluation and configuring of machine learners irreducibly depends on a statistical treatment of errors and their trade-offs. <a href="53-does-machine-learning-construct-a-new-statistical-reality.html#fnref63">↩</a></p></li>
<li id="fn64"><p>Many authors have suggested that algorithms should be the focus of more attention. The sociologist Mike Savage, in his account of the growth of ‘descriptive assemblages’ based around large scale data mining of transactions, administrative records and social media practice concludes:</p><p>It follows that a core concern might be to scrutinize how pattern is derived and produced in social inscription devices, as a means of considering the robustness of such derivations, what may be left out or made invisible from them, and so forth. We need to develop an account which seeks to criticize notions of the descriptive insofar as this involves the simple generation of categories and groups, and instead focus on the fluid and intensive generation of potential <span class="citation">[@Savage_2009, 171]</span> <a href="53-does-machine-learning-construct-a-new-statistical-reality.html#fnref64">↩</a></p></li>
<li id="fn65"><p><em>Elements of Statistical Learning</em> uses the term ‘pattern’ only occasionally. The term appears 33 times there, and mainly in the bibliography. Apart from Brian Ripley’s <em>Pattern Recognition and Neural Networks</em> <span class="citation">[@Ripley_1996]</span>, statisticians largely eschew the term. Computer scientists like it more, and particularly in work on the classification of images (see Christopher Bishop <em>Pattern Recognition and Machine Learning</em> <span class="citation">[@Bishop_2006]</span>. Hastie, Tibshirani and Friedman, as statistical machine learners, confine their use of pattern to the term ‘pattern recognition’. <a href="53-does-machine-learning-construct-a-new-statistical-reality.html#fnref65">↩</a></p></li>
<li id="fn66"><p>While Foucault tends to retain a decoupled subject-object relation in the production of statements, I tend to see these enunciative modalities as distributed across people and things. As always, machine learner is a composite term for this distribution.<a href="53-does-machine-learning-construct-a-new-statistical-reality.html#fnref66">↩</a></p></li>
<li id="fn67"><p>The top 20 most cited publications in the field include Ross Quinlan and Leo Breiman’s papers on decision trees <span class="citation">[@Quinlan_1986; @Breiman_1984]</span>, Vladimir Vapnik and Corinna Cortes’ support vector machines papers <span class="citation">[@Vapnik_1999; @Cortes_1995]</span>, an early textbook written by a computer scientist on machine learning <span class="citation">[@Mitchell_1997]</span>, a textbook and software package on data mining using Java <span class="citation">[@Witten_2005]</span>; a textbook on pattern recognition dating from the 1970s <span class="citation">[@Duda_2012]</span>, a tutorial on an error control technique (ROC - Receiver Operating Characteristics, first developed by the US military during WWII) and somewhat lower, another well-known textbook, this time on neural networks and pattern recognition <span class="citation">[@Bishop_2006]</span>. <a href="53-does-machine-learning-construct-a-new-statistical-reality.html#fnref67">↩</a></p></li>
</ol>
</div>
<p style="text-align: center;">
<a href="52-statistical-decompositions-bias-variance-and-observed-errors.html"><button class="btn btn-default">Previous</button></a>
<a href="54-splitting-and-the-growth-of-trees.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
