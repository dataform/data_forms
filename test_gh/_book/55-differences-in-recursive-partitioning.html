<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta property="og:title" content="" />
<meta property="og:type" content="book" />







<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="">

<title></title>

<script src="libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="libs/navigation-1.1/tabsets.js"></script>





<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
</style>
</head>

<body>

<div class="container-fluid main-container">


<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="1-acknowledgments.html#acknowledgments"><span class="toc-section-number">1</span> Acknowledgments</a></li>
<li><a href="2-machine-learners.html#machine-learners"><span class="toc-section-number">2</span> 250,000 machine learners</a></li>
<li><a href="3-a-summary-of-the-argument.html#a-summary-of-the-argument"><span class="toc-section-number">3</span> A summary of the argument</a></li>
<li><a href="4-in-situ-hybridization.html#in-situ-hybridization"><span class="toc-section-number">4</span> In-situ hybridization</a></li>
<li><a href="5-critical-operational-practice.html#critical-operational-practice"><span class="toc-section-number">5</span> Critical operational practice?</a></li>
<li><a href="6-obstacles-to-the-work-of-freeing-machine-learning.html#obstacles-to-the-work-of-freeing-machine-learning"><span class="toc-section-number">6</span> Obstacles to the work of freeing machine learning</a></li>
<li><a href="7-preface.html#preface"><span class="toc-section-number">7</span> Preface</a></li>
<li><a href="8-introduction-into-the-data.html#introduction-into-the-data"><span class="toc-section-number">8</span> Introduction: Into the Data</a></li>
<li><a href="9-three-accumulations-settings-data-and-devices.html#three-accumulations-settings-data-and-devices"><span class="toc-section-number">9</span> Three accumulations: settings, data and devices</a></li>
<li><a href="10-who-or-what-is-a-machine-learner.html#who-or-what-is-a-machine-learner"><span class="toc-section-number">10</span> Who or what is a machine learner?</a></li>
<li><a href="11-algorithmic-control-to-the-machine-learners.html#algorithmic-control-to-the-machine-learners"><span class="toc-section-number">11</span> Algorithmic control to the machine learners?</a></li>
<li><a href="12-the-archaeology-of-operations.html#the-archaeology-of-operations"><span class="toc-section-number">12</span> The archaeology of operations</a></li>
<li><a href="13-asymmetries-in-common-knowledge.html#asymmetries-in-common-knowledge"><span class="toc-section-number">13</span> Asymmetries in common knowledge</a></li>
<li><a href="14-what-cannot-be-automated.html#what-cannot-be-automated"><span class="toc-section-number">14</span> What cannot be automated?</a></li>
<li><a href="15-different-fields-in-machine-learning.html#different-fields-in-machine-learning"><span class="toc-section-number">15</span> Different fields in machine learning?</a></li>
<li><a href="16-the-diagram-in-critical-thought.html#the-diagram-in-critical-thought"><span class="toc-section-number">16</span> The diagram in critical thought</a></li>
<li><a href="17-we-dont-have-to-write-programs.html#we-dont-have-to-write-programs"><span class="toc-section-number">17</span> ‘We don’t have to write programs’?</a></li>
<li><a href="18-the-elements-of-machine-learning.html#the-elements-of-machine-learning"><span class="toc-section-number">18</span> The elements of machine learning</a></li>
<li><a href="19-who-reads-machine-learning-textbooks.html#who-reads-machine-learning-textbooks"><span class="toc-section-number">19</span> Who reads machine learning textbooks?</a></li>
<li><a href="20-r-a-matrix-of-transformations.html#r-a-matrix-of-transformations"><span class="toc-section-number">20</span> <code>R</code>: a matrix of transformations</a></li>
<li><a href="21-the-obdurate-mathematical-glint-of-machine-learning.html#the-obdurate-mathematical-glint-of-machine-learning"><span class="toc-section-number">21</span> The obdurate mathematical glint of machine learning</a></li>
<li><a href="22-cs229-2007-returning-again-and-again-to-certain-features.html#cs229-2007-returning-again-and-again-to-certain-features"><span class="toc-section-number">22</span> CS229, 2007: returning again and again to certain features</a></li>
<li><a href="23-the-visible-learning-of-machine-learning.html#the-visible-learning-of-machine-learning"><span class="toc-section-number">23</span> The visible learning of machine learning</a></li>
<li><a href="24-the-diagram-of-an-operational-formation.html#the-diagram-of-an-operational-formation"><span class="toc-section-number">24</span> The diagram of an operational formation</a></li>
<li><a href="25-vector-space-and-geometry.html#vector-space-and-geometry"><span class="toc-section-number">25</span> Vector space and geometry</a></li>
<li><a href="26-mixing-places.html#mixing-places"><span class="toc-section-number">26</span> Mixing places</a></li>
<li><a href="27-truth-is-no-longer-in-the-table.html#truth-is-no-longer-in-the-table"><span class="toc-section-number">27</span> Truth is no longer in the table?</a></li>
<li><a href="28-the-epistopic-fault-line-in-tables.html#the-epistopic-fault-line-in-tables"><span class="toc-section-number">28</span> The epistopic fault line in tables</a></li>
<li><a href="29-surface-and-depths-the-problem-of-volume-in-data.html#surface-and-depths-the-problem-of-volume-in-data"><span class="toc-section-number">29</span> Surface and depths: the problem of volume in data</a></li>
<li><a href="30-vector-space-expansion.html#vector-space-expansion"><span class="toc-section-number">30</span> Vector space expansion</a></li>
<li><a href="31-drawing-lines-in-a-common-space-of-transformation.html#drawing-lines-in-a-common-space-of-transformation"><span class="toc-section-number">31</span> Drawing lines in a common space of transformation</a></li>
<li><a href="32-implicit-vectorization-in-code-and-infrastructures.html#implicit-vectorization-in-code-and-infrastructures"><span class="toc-section-number">32</span> Implicit vectorization in code and infrastructures</a></li>
<li><a href="33-lines-traversing-behind-the-light.html#lines-traversing-behind-the-light"><span class="toc-section-number">33</span> Lines traversing behind the light</a></li>
<li><a href="34-the-vectorised-table.html#the-vectorised-table"><span class="toc-section-number">34</span> The vectorised table?</a></li>
<li><a href="35-learning-functions.html#learning-functions"><span class="toc-section-number">35</span> Learning functions</a></li>
<li><a href="36-supervised-unsupervised-reinforcement-learning-and-functions.html#supervised-unsupervised-reinforcement-learning-and-functions"><span class="toc-section-number">36</span> Supervised, unsupervised, reinforcement learning and functions</a></li>
<li><a href="37-which-function-operates.html#which-function-operates"><span class="toc-section-number">37</span> Which function operates?</a></li>
<li><a href="38-what-does-a-function-learn.html#what-does-a-function-learn"><span class="toc-section-number">38</span> What does a function learn?</a></li>
<li><a href="39-observing-with-curves-the-logistic-function.html#observing-with-curves-the-logistic-function"><span class="toc-section-number">39</span> Observing with curves: the logistic function</a></li>
<li><a href="40-the-cost-of-curves-in-machine-learning.html#the-cost-of-curves-in-machine-learning"><span class="toc-section-number">40</span> The cost of curves in machine learning</a></li>
<li><a href="41-curves-and-the-variation-in-models.html#curves-and-the-variation-in-models"><span class="toc-section-number">41</span> Curves and the variation in models</a></li>
<li><a href="42-observing-costs-losses-and-objectives-through-optimisation.html#observing-costs-losses-and-objectives-through-optimisation"><span class="toc-section-number">42</span> Observing costs, losses and objectives through optimisation</a></li>
<li><a href="43-gradients-as-partial-observers.html#gradients-as-partial-observers"><span class="toc-section-number">43</span> Gradients as partial observers</a></li>
<li><a href="44-the-power-to-learn.html#the-power-to-learn"><span class="toc-section-number">44</span> The power to learn</a></li>
<li><a href="45-data-reduces-uncertainty.html#data-reduces-uncertainty"><span class="toc-section-number">45</span> Data reduces uncertainty?</a></li>
<li><a href="46-machine-learning-as-statistics-inside-out.html#machine-learning-as-statistics-inside-out"><span class="toc-section-number">46</span> Machine learning as statistics inside out</a></li>
<li><a href="47-distributed-probabilities.html#distributed-probabilities"><span class="toc-section-number">47</span> Distributed probabilities</a></li>
<li><a href="48-naive-bayes-and-the-distribution-of-probabilities.html#naive-bayes-and-the-distribution-of-probabilities"><span class="toc-section-number">48</span> Naive Bayes and the distribution of probabilities</a></li>
<li><a href="49-spam-when-foralln-is-too-much.html#spam-when-foralln-is-too-much"><span class="toc-section-number">49</span> Spam: when <span class="math inline">\(\forall{N}\)</span> is too much?</a></li>
<li><a href="50-the-improbable-success-of-the-naive-bayes-classifier.html#the-improbable-success-of-the-naive-bayes-classifier"><span class="toc-section-number">50</span> The improbable success of the Naive Bayes classifier</a></li>
<li><a href="51-ancestral-probabilities-in-documents-inference-and-prediction.html#ancestral-probabilities-in-documents-inference-and-prediction"><span class="toc-section-number">51</span> Ancestral probabilities in documents: inference and prediction</a></li>
<li><a href="52-statistical-decompositions-bias-variance-and-observed-errors.html#statistical-decompositions-bias-variance-and-observed-errors"><span class="toc-section-number">52</span> Statistical decompositions: bias, variance and observed errors</a></li>
<li><a href="53-does-machine-learning-construct-a-new-statistical-reality.html#does-machine-learning-construct-a-new-statistical-reality"><span class="toc-section-number">53</span> Does machine learning construct a new statistical reality?</a></li>
<li><a href="54-splitting-and-the-growth-of-trees.html#splitting-and-the-growth-of-trees"><span class="toc-section-number">54</span> Splitting and the growth of trees</a></li>
<li><a href="55-differences-in-recursive-partitioning.html#differences-in-recursive-partitioning"><span class="toc-section-number">55</span> 1984: Differences in recursive partitioning</a></li>
<li><a href="56-limiting-differences.html#limiting-differences"><span class="toc-section-number">56</span> Limiting differences</a></li>
<li><a href="57-the-successful-dispersion-of-the-support-vector-machine.html#the-successful-dispersion-of-the-support-vector-machine"><span class="toc-section-number">57</span> The successful dispersion of the support vector machine</a></li>
<li><a href="58-differences-blur.html#differences-blur"><span class="toc-section-number">58</span> Differences blur?</a></li>
<li><a href="59-bending-the-decision-boundary.html#bending-the-decision-boundary"><span class="toc-section-number">59</span> Bending the decision boundary</a></li>
<li><a href="60-instituting-patterns.html#instituting-patterns"><span class="toc-section-number">60</span> Instituting patterns</a></li>
<li><a href="61-genomic-referentiality-and-materiality.html#genomic-referentiality-and-materiality"><span class="toc-section-number">61</span> Genomic referentiality and materiality</a></li>
<li><a href="62-the-genome-as-threshold-object.html#the-genome-as-threshold-object"><span class="toc-section-number">62</span> The genome as threshold object</a></li>
<li><a href="63-genomic-knowledges-and-their-datasets.html#genomic-knowledges-and-their-datasets"><span class="toc-section-number">63</span> Genomic knowledges and their datasets</a></li>
<li><a href="64-the-advent-of-wide-dirty-and-mixed-data.html#the-advent-of-wide-dirty-and-mixed-data"><span class="toc-section-number">64</span> The advent of ‘wide, dirty and mixed’ data</a></li>
<li><a href="65-cross-validating-machine-learning-in-genomics.html#cross-validating-machine-learning-in-genomics"><span class="toc-section-number">65</span> Cross-validating machine learning in genomics</a></li>
<li><a href="66-proliferation-of-discoveries.html#proliferation-of-discoveries"><span class="toc-section-number">66</span> Proliferation of discoveries</a></li>
<li><a href="67-variations-in-the-object-or-in-the-machine-learner.html#variations-in-the-object-or-in-the-machine-learner"><span class="toc-section-number">67</span> Variations in the object or in the machine learner?</a></li>
<li><a href="68-whole-genome-functions.html#whole-genome-functions"><span class="toc-section-number">68</span> Whole genome functions</a></li>
<li><a href="69-propagation-across-human-machine-boundaries.html#propagation-across-human-machine-boundaries"><span class="toc-section-number">69</span> Propagation across human-machine boundaries</a></li>
<li><a href="70-competitive-positioning.html#competitive-positioning"><span class="toc-section-number">70</span> Competitive positioning</a></li>
<li><a href="71-a-privileged-machine-and-its-diagrammatic-forms.html#a-privileged-machine-and-its-diagrammatic-forms"><span class="toc-section-number">71</span> A privileged machine and its diagrammatic forms</a></li>
<li><a href="72-varying-subject-positions-in-code.html#varying-subject-positions-in-code"><span class="toc-section-number">72</span> Varying subject positions in code</a></li>
<li><a href="73-the-subjects-of-a-hidden-operation.html#the-subjects-of-a-hidden-operation"><span class="toc-section-number">73</span> The subjects of a hidden operation</a></li>
<li><a href="74-algorithms-that-propagate-errors.html#algorithms-that-propagate-errors"><span class="toc-section-number">74</span> Algorithms that propagate errors</a></li>
<li><a href="75-competitions-as-examination.html#competitions-as-examination"><span class="toc-section-number">75</span> Competitions as examination</a></li>
<li><a href="76-superimposing-power-and-knowledge.html#superimposing-power-and-knowledge"><span class="toc-section-number">76</span> Superimposing power and knowledge</a></li>
<li><a href="77-ranked-subject-positions.html#ranked-subject-positions"><span class="toc-section-number">77</span> Ranked subject positions</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="differences-in-recursive-partitioning" class="section level1">
<h1><span class="header-section-number">55</span> 1984: Differences in recursive partitioning</h1>
<p>As Einhorn expected, it was too much to hope that all researchers would take time to investigate how a particular program works. Some researchers however did take time in the following decade to investigate how decision trees work. Writing around 2000, Hastie, Tibshirani and Friedman, who could hardly by accused of not understanding decision trees, happily recommend decision trees as the best ‘off-the-shelf’ classifier: ‘of all the well-known learning methods, decision trees comes closest to meeting the requirements for serving as an off-the-shelf procedure for data-mining’ <span class="citation">[@Hastie_2009, 352]</span>. We might wonder here, however, whether they damn with faint praise, since ‘off-the-shelf’ suggests pre-packaged, and commodified, and the term ‘data-mining’ itself is not without negative connotations. As for its commercial realities, in 2013, Salford Systems, the purveyors of the leading contemporary commercial decision tree software, <code>CART</code>, could claim:</p>
<blockquote>
<p>CART is the ultimate classification tree that has revolutionized the entire field of advanced analytics and inaugurated the current era of data mining. CART, which is continually being improved, is one of the most important tools in modern data mining. Others have tried to copy CART but no one has succeeded as evidenced by unmatched accuracy, performance, feature set, built-in automation and ease of use. <a href="http://www.salford-systems.com/products/cart">Salford Systems</a> </p>
</blockquote>
<p>What happened between 1973 and 2013? Decision trees somehow stepped out of the statistically murky waters of social science departments and business schools in the early 1970s to inaugurate the ‘current era of data mining’ (which the scientific literature indicates starts in the early 1990s).  This was not only a commercial innovation. As the earlier citation from U.S. National Institutes of Health biostatisticians Malley, Malley and Pajevic indicates, decision trees enjoy high regard even in biomedical research, a setting where statistical rigour is highly valued for life and death reasons. The happy situation of decision trees four decades on suggests some kind of threshold was crossed in which the epistemological, statistical, or algorithmic (‘built-in automation’) power of the technique altered substantially. </p>
<p>The third author of <em>Elements of Statistical Learning</em>, Jerome Friedman, worked at the U.S. Department of Energy’s Stanford Linear Accelerator during the late 1970s. Friedman was instrumental in rescuing decision trees from the ignominy of profligate ease of use and pure empiricism they had endured since the late 1960s.  The reorganisation and statistical retrofitting of the decision tree was not a single or focused effort. During the 1980s, statisticians such as Friedman and Leo Breiman renovated the decision tree as a statistical tool <span class="citation">[@Breiman_1984]</span>. At the same time, computer scientists such as Ross Quinlan in Sydney were re-implementing decision trees guided by an artificial intelligence-based formalisation as rule-based induction technique <span class="citation">[@Quinlan_1986]</span>.<a href="#fn69" class="footnoteRef" id="fnref69"><sup>69</sup></a>   This uneasy parallel effort between computer science and statistics still somewhat strains relations in machine learning today. Statisticians and computer scientists do and use the same techniques, but often with the computer scientists focusing on optimisation and algorithmic scale and the statisticians inventing novel statistical formalizations and abstractions. The fateful embrace of statistics and computer science, the disciplinary binary that vectorizes machine learning, has been generative in the retrieval of the decision tree. </p>
<p>An initial symptom of the transformation of the technique appears in a name change. The term ‘decision tree,’ although still widely used in the research literature and machine learner parlance was supplanted by ‘classification and regression tree’ during the late 1970s and 1980s. The terms ‘classification and regression tree’ is sometimes contracted to ‘CART,’ and that term strictly speaking refers to a computer program described in <span class="citation">[@Breiman_1984]</span> as well as the title of that highly-cited monograph, <em>Classification and Regression Trees</em>.   As we have seen in previous chapters, classification and regression (predictive modelling using estimates of relations between variables)  stage the two main sides of machine learning practice. Their concatenation with ‘tree’ attests to a renovation of existing machine learning approaches behind a single facade.</p>
<p>The implementation of machine learning techniques in <code>R</code> accentuates the statistical side of decision tree practice, but that has certain forensic virtues not offered by commercial or closed-source software often produced by computer scientists. The name of one long-standing and widely-used <code>R</code> package itself attests to something: <code>rpart</code> is a contraction of ‘recursive partitioning’ and this term generally describes how the decision tree algorithm works to partition the vector space into the form shown in Figure  <span class="citation">[@Therneau_2015]</span>.  ‘CART,’ on the other hand, is a registered trademark of Salford Systems, the software company mentioned above, who sell the leading commercial implementation of classification and regression trees. Hence, the <code>R</code> package <code>rpart</code> cannot call itself the more obvious name <code>cart,</code> and instead invokes the underlying algorithmic process: recursive partitioning.<a href="#fn70" class="footnoteRef" id="fnref70"><sup>70</sup></a>  </p>

<p>R.A. Fisher’s <code>iris</code> dataset, which contains 150 measurements made in the 1930s of petal and sepal lengths of <em>iris virginica, iris setosa</em> and <em>iris versicolor</em> is a standard instructional example for decision trees <span class="citation">[@Fisher_1938]</span>.<a href="#fn71" class="footnoteRef" id="fnref71"><sup>71</sup></a>  The code shown here loads the <code>iris</code> data (the dataset is routinely installed with many data analysis tools), loads the <code>rpart</code> decision tree library, and builds a decision to classify the irises by species. What has happened to the iris data in this decision tree? The <code>R</code> code that invokes the recursive partitioning algorithm is so brief <code>iris_tree =rpart(Species ~ ., iris)</code> that we can’t tell much about how the data has been ‘recursively partitioned.’ We know that the <em>iris</em> has 150 rows, and that there are equal numbers of the three iris varieties.</p>
<p>Code brevity indicates a great deal of formalization of practice has accrued around decision trees.  Some of this formalization was described in the landmark <em>Classification and Regression Trees</em> monograph <span class="citation">[@Breiman_1984]</span>. Classification in decision trees operates by splitting each of the dimensions of vector space into two parts (as we saw in figure ). These splits institute branches along which differences are hierarchically ordered in a tree structure. The recursive splitting algorithm draws a diagram of hierarchical differences. The problem here is that many splits are possible. What is a good split or ordering of differences? </p>
<blockquote>
<p>The first problem in tree construction is how to use <span class="math inline">\(\mathcal{L}\)</span> to determine the binary splits of <span class="math inline">\(\mathcal{X}\)</span> into smaller pieces. The fundamental idea is to select each split of a subset so that the data in each of the descendant subsets are “purer” than the data in the parent subset <span class="citation">[@Breiman_1984, 23]</span>.</p>
</blockquote>
<p>Tree construction hinges on the notion of purity or more precisely ‘node impurity’, a function that measures to what extent data labelled as belonging to different classes are mixed together at a given branch or node in a decision tree: ‘that is, the node impurity is largest when all classes are equally mixed together in it, and smallest when the node contains only one class’ <span class="citation">[@Breiman_1984, 24]</span>. As Malley and co-authors note, ‘the collection of purity measures is still a subject of research’ <span class="citation">[@Malley_2011, 123]</span>, but Breiman, Friedman, Olshen and Stone promoted a particular form of impurity measure for classification trees known as ‘Gini index of diversity’ <span class="citation">[@Breiman_1984, 38]</span>.  Like the planar decision surface used in classifiers such as the perceptron or linear regression model, recursive partitioning combined with measures of node impurity transforms data by cuts or divides. Whereas in linear model-based machine learners, the intuition motivating the function-finding or learning was ‘find the line that best expresses the distribution of the data,  here the intuition is more like: ’find the cuts that minimize mixing’. Good splits decrease the level of impurity in the tree. In a tree with maximum purity, each terminal node – the nodes at the base of the tree – would contain a single class.</p>

<p>In Figure , the plot on the left shows the decision tree and the plot on the right shows just <em>setosa</em> and <em>versicolor</em> plotted by petal and sepal widths and lengths. Decision trees are read from the top down, left to right. The top level of this tree can be read, for instance, as saying, if the length of petal is less 2.45, then the iris is <em>setosa</em>.  As the plot on the right shows, most of the measurements are well clustered. Only the <em>setosa</em> petal lengths and widths seem to vary widely. All the other measurements are tightly bunched. A decision tree has little trouble ordering differences between species of iris.</p>
<p>Like logistic regression models, neural networks, support vector machines or any other machine learning technique, decision trees order differences in terms of specific qualities and logics. Recursive partitioning split and sub-divide the vector space to capture every minor difference between cases, and thereby achieve a ever-closer fit to the individual or sub-individual variations.  Although the partitioning or splitting rules have strong statistical justifications, they do not at all eliminate the problem of instability or variance in trees.  For instance, they easily end up ‘overfitting’ the data. Overfitting is a problem for all machine learning techniques. Algorithms sometimes find it hard to know when to stop identifying differences. During construction of a decision tree, recursive partitioning splits features in the data into smaller and smaller groups. ‘The goodness of the split’, wrote Breiman and co-authors, ‘is defined to be the decrease in impurity’ <span class="citation">[@Breiman_1984, 25]</span>. Under this definition of goodness, the terminal nodes or leaves of the tree can end up containing a single case, or a single class of cases. </p>
<p>The decision tree targets the differences of the individual case to such a degree that it could end up seeing categorical differences everywhere. Operating to maximise the purity of the partitions it creates, it leans too heavily on data it has been trained on to see relevant similarities when fresh data appears. Trees that branch too much are sensitive to differences and generalize poorly (that is, they suffer from generalization error \index{error!generalization}). Such a model will almost always <em>overfit</em>, since slight variations in the values of variables in a fresh case are likely to yield widely differing predictions. In the terminology of machine learning, such a decision tree may have low bias but high variance.  </p>
</div>
<div class="footnotes">
<hr />
<ol start="69">
<li id="fn69"><p>Quinlan’s papers and book on versions of the decision tree (<code>ID3</code> and <code>c4.5</code>) are both amongst the top ten the most highly cited references in the machine literature itself. Google Scholar reports over 20,00 citations of the Quinlan’s book <em>C4.5: Programs for Machine Learning</em> <span class="citation">[@Quinlan_1993]</span> (although far fewer appear in Thomson Reuters Web of Science). Several years ago, <code>C4.5</code> was voted the top data mining algorithm <span class="citation">[@Wu_2008]</span>. While I don’t discuss Quinlan’s work in much detail here, we should note as a computer scientist, Quinlan takes a much more rule-based approach to decision tree that Breiman and co-authors.  <a href="55-differences-in-recursive-partitioning.html#fnref69">↩</a></p></li>
<li id="fn70"><p>Other <code>R</code> packages such as <code>party</code> <span class="citation">[@Hothorn_2006]</span> and <code>tree</code> <span class="citation">[@Ripley_2014]</span> also use recursive partitioning, but with various tweaks and optimisations that I leave aside here.  <a href="55-differences-in-recursive-partitioning.html#fnref70">↩</a></p></li>
<li id="fn71"><p><code>iris</code> is a very small dataset, a pre-computational miniature.  That diminutive character makes it diagrammatically mobile. It supports a rhizomatic ecosystem of examples scattered across the machine learning literature. The usual framing of the classification problem is how to decide whether a given iris blossom is of the species <em>virginica</em>, <em>setosa</em> or _versicolor. These irises don’t grow in forests – they are more often found in riverbanks and meadows – but they do offer a variety of illustrations of how machine learning classifiers are brought to bear on classification problems. Here the classification problem is taxonomic - the <em>iris</em> genus has various sub-genera, and sections within the sub-genera.Setosa, <em>virginica</em> and <em>versicolor</em> all belong to the sub-genus <em>Limniris</em>. This botanical context is routinely ignored in machine learning applications. In machine learning textbooks and tutorials, <code>iris</code> typically would be used to demonstrate how cleanly a classifier can separate the different kinds of irises. <a href="55-differences-in-recursive-partitioning.html#fnref71">↩</a></p></li>
</ol>
</div>
<p style="text-align: center;">
<a href="54-splitting-and-the-growth-of-trees.html"><button class="btn btn-default">Previous</button></a>
<a href="56-limiting-differences.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
