<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta property="og:title" content="" />
<meta property="og:type" content="book" />







<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="">

<title></title>

<script src="libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="libs/navigation-1.1/tabsets.js"></script>





<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
</style>
</head>

<body>

<div class="container-fluid main-container">


<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="1-acknowledgments.html#acknowledgments"><span class="toc-section-number">1</span> Acknowledgments</a></li>
<li><a href="2-machine-learners.html#machine-learners"><span class="toc-section-number">2</span> 250,000 machine learners</a></li>
<li><a href="3-a-summary-of-the-argument.html#a-summary-of-the-argument"><span class="toc-section-number">3</span> A summary of the argument</a></li>
<li><a href="4-in-situ-hybridization.html#in-situ-hybridization"><span class="toc-section-number">4</span> In-situ hybridization</a></li>
<li><a href="5-critical-operational-practice.html#critical-operational-practice"><span class="toc-section-number">5</span> Critical operational practice?</a></li>
<li><a href="6-obstacles-to-the-work-of-freeing-machine-learning.html#obstacles-to-the-work-of-freeing-machine-learning"><span class="toc-section-number">6</span> Obstacles to the work of freeing machine learning</a></li>
<li><a href="7-preface.html#preface"><span class="toc-section-number">7</span> Preface</a></li>
<li><a href="8-introduction-into-the-data.html#introduction-into-the-data"><span class="toc-section-number">8</span> Introduction: Into the Data</a></li>
<li><a href="9-three-accumulations-settings-data-and-devices.html#three-accumulations-settings-data-and-devices"><span class="toc-section-number">9</span> Three accumulations: settings, data and devices</a></li>
<li><a href="10-who-or-what-is-a-machine-learner.html#who-or-what-is-a-machine-learner"><span class="toc-section-number">10</span> Who or what is a machine learner?</a></li>
<li><a href="11-algorithmic-control-to-the-machine-learners.html#algorithmic-control-to-the-machine-learners"><span class="toc-section-number">11</span> Algorithmic control to the machine learners?</a></li>
<li><a href="12-the-archaeology-of-operations.html#the-archaeology-of-operations"><span class="toc-section-number">12</span> The archaeology of operations</a></li>
<li><a href="13-asymmetries-in-common-knowledge.html#asymmetries-in-common-knowledge"><span class="toc-section-number">13</span> Asymmetries in common knowledge</a></li>
<li><a href="14-what-cannot-be-automated.html#what-cannot-be-automated"><span class="toc-section-number">14</span> What cannot be automated?</a></li>
<li><a href="15-different-fields-in-machine-learning.html#different-fields-in-machine-learning"><span class="toc-section-number">15</span> Different fields in machine learning?</a></li>
<li><a href="16-the-diagram-in-critical-thought.html#the-diagram-in-critical-thought"><span class="toc-section-number">16</span> The diagram in critical thought</a></li>
<li><a href="17-we-dont-have-to-write-programs.html#we-dont-have-to-write-programs"><span class="toc-section-number">17</span> ‘We don’t have to write programs’?</a></li>
<li><a href="18-the-elements-of-machine-learning.html#the-elements-of-machine-learning"><span class="toc-section-number">18</span> The elements of machine learning</a></li>
<li><a href="19-who-reads-machine-learning-textbooks.html#who-reads-machine-learning-textbooks"><span class="toc-section-number">19</span> Who reads machine learning textbooks?</a></li>
<li><a href="20-r-a-matrix-of-transformations.html#r-a-matrix-of-transformations"><span class="toc-section-number">20</span> <code>R</code>: a matrix of transformations</a></li>
<li><a href="21-the-obdurate-mathematical-glint-of-machine-learning.html#the-obdurate-mathematical-glint-of-machine-learning"><span class="toc-section-number">21</span> The obdurate mathematical glint of machine learning</a></li>
<li><a href="22-cs229-2007-returning-again-and-again-to-certain-features.html#cs229-2007-returning-again-and-again-to-certain-features"><span class="toc-section-number">22</span> CS229, 2007: returning again and again to certain features</a></li>
<li><a href="23-the-visible-learning-of-machine-learning.html#the-visible-learning-of-machine-learning"><span class="toc-section-number">23</span> The visible learning of machine learning</a></li>
<li><a href="24-the-diagram-of-an-operational-formation.html#the-diagram-of-an-operational-formation"><span class="toc-section-number">24</span> The diagram of an operational formation</a></li>
<li><a href="25-vector-space-and-geometry.html#vector-space-and-geometry"><span class="toc-section-number">25</span> Vector space and geometry</a></li>
<li><a href="26-mixing-places.html#mixing-places"><span class="toc-section-number">26</span> Mixing places</a></li>
<li><a href="27-truth-is-no-longer-in-the-table.html#truth-is-no-longer-in-the-table"><span class="toc-section-number">27</span> Truth is no longer in the table?</a></li>
<li><a href="28-the-epistopic-fault-line-in-tables.html#the-epistopic-fault-line-in-tables"><span class="toc-section-number">28</span> The epistopic fault line in tables</a></li>
<li><a href="29-surface-and-depths-the-problem-of-volume-in-data.html#surface-and-depths-the-problem-of-volume-in-data"><span class="toc-section-number">29</span> Surface and depths: the problem of volume in data</a></li>
<li><a href="30-vector-space-expansion.html#vector-space-expansion"><span class="toc-section-number">30</span> Vector space expansion</a></li>
<li><a href="31-drawing-lines-in-a-common-space-of-transformation.html#drawing-lines-in-a-common-space-of-transformation"><span class="toc-section-number">31</span> Drawing lines in a common space of transformation</a></li>
<li><a href="32-implicit-vectorization-in-code-and-infrastructures.html#implicit-vectorization-in-code-and-infrastructures"><span class="toc-section-number">32</span> Implicit vectorization in code and infrastructures</a></li>
<li><a href="33-lines-traversing-behind-the-light.html#lines-traversing-behind-the-light"><span class="toc-section-number">33</span> Lines traversing behind the light</a></li>
<li><a href="34-the-vectorised-table.html#the-vectorised-table"><span class="toc-section-number">34</span> The vectorised table?</a></li>
<li><a href="35-learning-functions.html#learning-functions"><span class="toc-section-number">35</span> Learning functions</a></li>
<li><a href="36-supervised-unsupervised-reinforcement-learning-and-functions.html#supervised-unsupervised-reinforcement-learning-and-functions"><span class="toc-section-number">36</span> Supervised, unsupervised, reinforcement learning and functions</a></li>
<li><a href="37-which-function-operates.html#which-function-operates"><span class="toc-section-number">37</span> Which function operates?</a></li>
<li><a href="38-what-does-a-function-learn.html#what-does-a-function-learn"><span class="toc-section-number">38</span> What does a function learn?</a></li>
<li><a href="39-observing-with-curves-the-logistic-function.html#observing-with-curves-the-logistic-function"><span class="toc-section-number">39</span> Observing with curves: the logistic function</a></li>
<li><a href="40-the-cost-of-curves-in-machine-learning.html#the-cost-of-curves-in-machine-learning"><span class="toc-section-number">40</span> The cost of curves in machine learning</a></li>
<li><a href="41-curves-and-the-variation-in-models.html#curves-and-the-variation-in-models"><span class="toc-section-number">41</span> Curves and the variation in models</a></li>
<li><a href="42-observing-costs-losses-and-objectives-through-optimisation.html#observing-costs-losses-and-objectives-through-optimisation"><span class="toc-section-number">42</span> Observing costs, losses and objectives through optimisation</a></li>
<li><a href="43-gradients-as-partial-observers.html#gradients-as-partial-observers"><span class="toc-section-number">43</span> Gradients as partial observers</a></li>
<li><a href="44-the-power-to-learn.html#the-power-to-learn"><span class="toc-section-number">44</span> The power to learn</a></li>
<li><a href="45-data-reduces-uncertainty.html#data-reduces-uncertainty"><span class="toc-section-number">45</span> Data reduces uncertainty?</a></li>
<li><a href="46-machine-learning-as-statistics-inside-out.html#machine-learning-as-statistics-inside-out"><span class="toc-section-number">46</span> Machine learning as statistics inside out</a></li>
<li><a href="47-distributed-probabilities.html#distributed-probabilities"><span class="toc-section-number">47</span> Distributed probabilities</a></li>
<li><a href="48-naive-bayes-and-the-distribution-of-probabilities.html#naive-bayes-and-the-distribution-of-probabilities"><span class="toc-section-number">48</span> Naive Bayes and the distribution of probabilities</a></li>
<li><a href="49-spam-when-foralln-is-too-much.html#spam-when-foralln-is-too-much"><span class="toc-section-number">49</span> Spam: when <span class="math inline">\(\forall{N}\)</span> is too much?</a></li>
<li><a href="50-the-improbable-success-of-the-naive-bayes-classifier.html#the-improbable-success-of-the-naive-bayes-classifier"><span class="toc-section-number">50</span> The improbable success of the Naive Bayes classifier</a></li>
<li><a href="51-ancestral-probabilities-in-documents-inference-and-prediction.html#ancestral-probabilities-in-documents-inference-and-prediction"><span class="toc-section-number">51</span> Ancestral probabilities in documents: inference and prediction</a></li>
<li><a href="52-statistical-decompositions-bias-variance-and-observed-errors.html#statistical-decompositions-bias-variance-and-observed-errors"><span class="toc-section-number">52</span> Statistical decompositions: bias, variance and observed errors</a></li>
<li><a href="53-does-machine-learning-construct-a-new-statistical-reality.html#does-machine-learning-construct-a-new-statistical-reality"><span class="toc-section-number">53</span> Does machine learning construct a new statistical reality?</a></li>
<li><a href="54-splitting-and-the-growth-of-trees.html#splitting-and-the-growth-of-trees"><span class="toc-section-number">54</span> Splitting and the growth of trees</a></li>
<li><a href="55-differences-in-recursive-partitioning.html#differences-in-recursive-partitioning"><span class="toc-section-number">55</span> 1984: Differences in recursive partitioning</a></li>
<li><a href="56-limiting-differences.html#limiting-differences"><span class="toc-section-number">56</span> Limiting differences</a></li>
<li><a href="57-the-successful-dispersion-of-the-support-vector-machine.html#the-successful-dispersion-of-the-support-vector-machine"><span class="toc-section-number">57</span> The successful dispersion of the support vector machine</a></li>
<li><a href="58-differences-blur.html#differences-blur"><span class="toc-section-number">58</span> Differences blur?</a></li>
<li><a href="59-bending-the-decision-boundary.html#bending-the-decision-boundary"><span class="toc-section-number">59</span> Bending the decision boundary</a></li>
<li><a href="60-instituting-patterns.html#instituting-patterns"><span class="toc-section-number">60</span> Instituting patterns</a></li>
<li><a href="61-genomic-referentiality-and-materiality.html#genomic-referentiality-and-materiality"><span class="toc-section-number">61</span> Genomic referentiality and materiality</a></li>
<li><a href="62-the-genome-as-threshold-object.html#the-genome-as-threshold-object"><span class="toc-section-number">62</span> The genome as threshold object</a></li>
<li><a href="63-genomic-knowledges-and-their-datasets.html#genomic-knowledges-and-their-datasets"><span class="toc-section-number">63</span> Genomic knowledges and their datasets</a></li>
<li><a href="64-the-advent-of-wide-dirty-and-mixed-data.html#the-advent-of-wide-dirty-and-mixed-data"><span class="toc-section-number">64</span> The advent of ‘wide, dirty and mixed’ data</a></li>
<li><a href="65-cross-validating-machine-learning-in-genomics.html#cross-validating-machine-learning-in-genomics"><span class="toc-section-number">65</span> Cross-validating machine learning in genomics</a></li>
<li><a href="66-proliferation-of-discoveries.html#proliferation-of-discoveries"><span class="toc-section-number">66</span> Proliferation of discoveries</a></li>
<li><a href="67-variations-in-the-object-or-in-the-machine-learner.html#variations-in-the-object-or-in-the-machine-learner"><span class="toc-section-number">67</span> Variations in the object or in the machine learner?</a></li>
<li><a href="68-whole-genome-functions.html#whole-genome-functions"><span class="toc-section-number">68</span> Whole genome functions</a></li>
<li><a href="69-propagation-across-human-machine-boundaries.html#propagation-across-human-machine-boundaries"><span class="toc-section-number">69</span> Propagation across human-machine boundaries</a></li>
<li><a href="70-competitive-positioning.html#competitive-positioning"><span class="toc-section-number">70</span> Competitive positioning</a></li>
<li><a href="71-a-privileged-machine-and-its-diagrammatic-forms.html#a-privileged-machine-and-its-diagrammatic-forms"><span class="toc-section-number">71</span> A privileged machine and its diagrammatic forms</a></li>
<li><a href="72-varying-subject-positions-in-code.html#varying-subject-positions-in-code"><span class="toc-section-number">72</span> Varying subject positions in code</a></li>
<li><a href="73-the-subjects-of-a-hidden-operation.html#the-subjects-of-a-hidden-operation"><span class="toc-section-number">73</span> The subjects of a hidden operation</a></li>
<li><a href="74-algorithms-that-propagate-errors.html#algorithms-that-propagate-errors"><span class="toc-section-number">74</span> Algorithms that propagate errors</a></li>
<li><a href="75-competitions-as-examination.html#competitions-as-examination"><span class="toc-section-number">75</span> Competitions as examination</a></li>
<li><a href="76-superimposing-power-and-knowledge.html#superimposing-power-and-knowledge"><span class="toc-section-number">76</span> Superimposing power and knowledge</a></li>
<li><a href="77-ranked-subject-positions.html#ranked-subject-positions"><span class="toc-section-number">77</span> Ranked subject positions</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="a-summary-of-the-argument" class="section level1">
<h1><span class="header-section-number">3</span> A summary of the argument</h1>
<p>I have been experimenting with abstraction in midst of data practices of machine learning. Let me resume the argument of the book, an archaeological argument that excavates seven major facets or intersecting planes that belong to the machine learning as an operational formation.  Chapter  addressed the problem of where amidst the mire of data, mathematics, code, infrastructures, scientific and other knowledge fields, a critical engagement with machine learning might situate itself. I suggested that we should consider the formal, mathematical abstraction and certain transformations in the production of software associated with machine learning as diagrammatic processes that organise and assemble human-machine relations.  Amidst a great accumulation of statements, figures, techniques, constructs, datasets and code implementations derived from many settings, the task is to map the intersecting references, the diagonal connections, and the transformatinos and substitutions that weave through machine learning. The positivity  of machine learning, its specific forms of accumulation, regularity and rarety do not attest to the power of algorithms but rather lend liveliness to the field by concentrating expressions from many regions.</p>
<p>Chapter  examined the practices of vectorising data, situating machine learners themselves in an organised, dimensioned space accommodating an increasing repertoire of transformations operating on vectors.  Viewed as another mutation of the tabular grid, vector space invites transformations of data. Machine learning is a practice of working with data to accommodate all differences within an expanding dimensional space, a space in which data is under the strain of smooth surfaces, straight lines, regular curves and hyper-planes.  Both in terms of infrastructure and epistemic cultures, the vector space abstracts and concretises  spaces inside data.</p>
<p>What is learning in machine learning? If information and computation can be understood as responding to a crisis in control, what do machine learners do? Chapter  examined how learning institutes experimental relays between operation and observation in optimising functions that predict and classify. The proliferation of methods and devices in machine learning and the attempts to unify them as ‘learners’ was understood as a result of this entwining of operations and observations. The interplay between operational transformations and observational functions in optimisation accounts for much of the ‘learning’ effect in machine learning. </p>
<p>An important and wide-reaching critical strand of work in humanities and social sciences over the last few decades has focused on knowledge in its entanglements with apparatuses of governmentalised power. Populations and other large aggregates have been central objects of concern. They remain so in contemporary operational formations, although under somewhat altered conditions. Having all the data, chapter  suggested, is not the principal stake in contemporary data cultures. Instead, the probabilisation  of both data and machine learners as populations, as distributed probabilities, indicates a different axis along which power-knowledge develops in machine learning.</p>
<p>What happens to differences amidst vectorisation, learning as optimisation, probabilisation and the generalized diagrammatic abstraction of machine learning? . Are all differences reduced to quantitative comparisons? Treated as pattern, chapter  explored different treatments of difference in machine learning. Differences bifurcate between infinitesimal graduation and rigid decision boundaries, sometimes blurring or overlapping, and sometimes distributed into inaccessibly high-dimensional inner data spaces. The archaeological task amidst the dispersed patterns is to locate differences in kind.</p>
<p>Rather than any new materiality, I have pointed to transformations in referentiality associated with machine learning. From the standpoint of operational , the materiality of machine learning refers to the practices of re-use that stabilise references. Science, by virtue of its experimental inventiveness and truth-authority, cross-validates the referentiality of machine learning.  The topic of chapter  was a particularly data-intensive contemporary scientific hyperobject, the genome. As a data form, genomic sequence data provokes re-use, transcription and transmission of classifications and predictions. This incites both infrastructural transformations but also new concretisations of the hyperobject (as for instance in genome wide association studies).</p>
<p>Finally, chapter  explored the subject position of machine learners. Within operational formations, subject positions arise in gaps between operations and statements concerning operations. The argument here concerned human-machine differences and the dispersion of subject positions through operations that alter those differences. Even amongst machine learners themselves, subject positions are not fixed or unified. The deep neural networks that beat Go champions in 2015 and 2016 <span class="citation">[@Silver_2016]</span> or developed hitherto unseen tactics in playing Atari computer games <span class="citation">[@Mnih_2015]</span> evidence the deeply competitive or test-based administration of this gap. </p>
</div>
<p style="text-align: center;">
<a href="2-machine-learners.html"><button class="btn btn-default">Previous</button></a>
<a href="4-in-situ-hybridization.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
