<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title></title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="">
  <meta name="generator" content="bookdown 0.3 and GitBook 2.6.7">

  <meta property="og:title" content="" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="r-a-matrix-of-transformations.html">
<link rel="next" href="cs229-2007-returning-again-and-again-to-certain-features.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i><b>1</b> Acknowledgments</a></li>
<li class="chapter" data-level="2" data-path="conclusion-out-of-the-data.html"><a href="conclusion-out-of-the-data.html"><i class="fa fa-check"></i><b>2</b> Conclusion: Out of the Data}</a></li>
<li class="chapter" data-level="3" data-path="machine-learners.html"><a href="machine-learners.html"><i class="fa fa-check"></i><b>3</b> 250,000 machine learners</a></li>
<li class="chapter" data-level="4" data-path="a-summary-of-the-argument.html"><a href="a-summary-of-the-argument.html"><i class="fa fa-check"></i><b>4</b> A summary of the argument</a></li>
<li class="chapter" data-level="5" data-path="in-situ-hybridization.html"><a href="in-situ-hybridization.html"><i class="fa fa-check"></i><b>5</b> In-situ hybridization</a></li>
<li class="chapter" data-level="6" data-path="critical-operational-practice.html"><a href="critical-operational-practice.html"><i class="fa fa-check"></i><b>6</b> Critical operational practice?</a></li>
<li class="chapter" data-level="7" data-path="obstacles-to-the-work-of-freeing-machine-learning.html"><a href="obstacles-to-the-work-of-freeing-machine-learning.html"><i class="fa fa-check"></i><b>7</b> Obstacles to the work of freeing machine learning</a></li>
<li class="chapter" data-level="8" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i><b>8</b> Preface</a></li>
<li class="chapter" data-level="9" data-path="introduction-into-the-data.html"><a href="introduction-into-the-data.html"><i class="fa fa-check"></i><b>9</b> Introduction: Into the Data</a></li>
<li class="chapter" data-level="10" data-path="three-accumulations-settings-data-and-devices.html"><a href="three-accumulations-settings-data-and-devices.html"><i class="fa fa-check"></i><b>10</b> Three accumulations: settings, data and devices</a></li>
<li class="chapter" data-level="11" data-path="who-or-what-is-a-machine-learner.html"><a href="who-or-what-is-a-machine-learner.html"><i class="fa fa-check"></i><b>11</b> Who or what is a machine learner?</a></li>
<li class="chapter" data-level="12" data-path="algorithmic-control-to-the-machine-learners.html"><a href="algorithmic-control-to-the-machine-learners.html"><i class="fa fa-check"></i><b>12</b> Algorithmic control to the machine learners?</a></li>
<li class="chapter" data-level="13" data-path="the-archaeology-of-operations.html"><a href="the-archaeology-of-operations.html"><i class="fa fa-check"></i><b>13</b> The archaeology of operations</a></li>
<li class="chapter" data-level="14" data-path="asymmetries-in-common-knowledge.html"><a href="asymmetries-in-common-knowledge.html"><i class="fa fa-check"></i><b>14</b> Asymmetries in common knowledge</a></li>
<li class="chapter" data-level="15" data-path="what-cannot-be-automated.html"><a href="what-cannot-be-automated.html"><i class="fa fa-check"></i><b>15</b> What cannot be automated?</a></li>
<li class="chapter" data-level="16" data-path="different-fields-in-machine-learning.html"><a href="different-fields-in-machine-learning.html"><i class="fa fa-check"></i><b>16</b> Different fields in machine learning?</a></li>
<li class="chapter" data-level="17" data-path="the-diagram-in-critical-thought.html"><a href="the-diagram-in-critical-thought.html"><i class="fa fa-check"></i><b>17</b> The diagram in critical thought</a></li>
<li class="chapter" data-level="18" data-path="diagramming-machines.html"><a href="diagramming-machines.html"><i class="fa fa-check"></i><b>18</b> Diagramming machines}</a></li>
<li class="chapter" data-level="19" data-path="we-dont-have-to-write-programs.html"><a href="we-dont-have-to-write-programs.html"><i class="fa fa-check"></i><b>19</b> ‘We don’t have to write programs’?</a></li>
<li class="chapter" data-level="20" data-path="the-elements-of-machine-learning.html"><a href="the-elements-of-machine-learning.html"><i class="fa fa-check"></i><b>20</b> The elements of machine learning</a></li>
<li class="chapter" data-level="21" data-path="who-reads-machine-learning-textbooks.html"><a href="who-reads-machine-learning-textbooks.html"><i class="fa fa-check"></i><b>21</b> Who reads machine learning textbooks?</a></li>
<li class="chapter" data-level="22" data-path="r-a-matrix-of-transformations.html"><a href="r-a-matrix-of-transformations.html"><i class="fa fa-check"></i><b>22</b> <code>R</code>: a matrix of transformations</a></li>
<li class="chapter" data-level="23" data-path="the-obdurate-mathematical-glint-of-machine-learning.html"><a href="the-obdurate-mathematical-glint-of-machine-learning.html"><i class="fa fa-check"></i><b>23</b> The obdurate mathematical glint of machine learning</a></li>
<li class="chapter" data-level="24" data-path="cs229-2007-returning-again-and-again-to-certain-features.html"><a href="cs229-2007-returning-again-and-again-to-certain-features.html"><i class="fa fa-check"></i><b>24</b> CS229, 2007: returning again and again to certain features</a></li>
<li class="chapter" data-level="25" data-path="the-visible-learning-of-machine-learning.html"><a href="the-visible-learning-of-machine-learning.html"><i class="fa fa-check"></i><b>25</b> The visible learning of machine learning</a></li>
<li class="chapter" data-level="26" data-path="the-diagram-of-an-operational-formation.html"><a href="the-diagram-of-an-operational-formation.html"><i class="fa fa-check"></i><b>26</b> The diagram of an operational formation</a></li>
<li class="chapter" data-level="27" data-path="vectorisation-and-its-consequences.html"><a href="vectorisation-and-its-consequences.html"><i class="fa fa-check"></i><b>27</b> Vectorisation and its consequences}</a></li>
<li class="chapter" data-level="28" data-path="vector-space-and-geometry.html"><a href="vector-space-and-geometry.html"><i class="fa fa-check"></i><b>28</b> Vector space and geometry</a></li>
<li class="chapter" data-level="29" data-path="mixing-places.html"><a href="mixing-places.html"><i class="fa fa-check"></i><b>29</b> Mixing places</a></li>
<li class="chapter" data-level="30" data-path="truth-is-no-longer-in-the-table.html"><a href="truth-is-no-longer-in-the-table.html"><i class="fa fa-check"></i><b>30</b> Truth is no longer in the table?</a></li>
<li class="chapter" data-level="31" data-path="the-epistopic-fault-line-in-tables.html"><a href="the-epistopic-fault-line-in-tables.html"><i class="fa fa-check"></i><b>31</b> The epistopic fault line in tables</a></li>
<li class="chapter" data-level="32" data-path="surface-and-depths-the-problem-of-volume-in-data.html"><a href="surface-and-depths-the-problem-of-volume-in-data.html"><i class="fa fa-check"></i><b>32</b> Surface and depths: the problem of volume in data</a></li>
<li class="chapter" data-level="33" data-path="vector-space-expansion.html"><a href="vector-space-expansion.html"><i class="fa fa-check"></i><b>33</b> Vector space expansion</a></li>
<li class="chapter" data-level="34" data-path="drawing-lines-in-a-common-space-of-transformation.html"><a href="drawing-lines-in-a-common-space-of-transformation.html"><i class="fa fa-check"></i><b>34</b> Drawing lines in a common space of transformation</a></li>
<li class="chapter" data-level="35" data-path="implicit-vectorization-in-code-and-infrastructures.html"><a href="implicit-vectorization-in-code-and-infrastructures.html"><i class="fa fa-check"></i><b>35</b> Implicit vectorization in code and infrastructures</a></li>
<li class="chapter" data-level="36" data-path="lines-traversing-behind-the-light.html"><a href="lines-traversing-behind-the-light.html"><i class="fa fa-check"></i><b>36</b> Lines traversing behind the light</a></li>
<li class="chapter" data-level="37" data-path="the-vectorised-table.html"><a href="the-vectorised-table.html"><i class="fa fa-check"></i><b>37</b> The vectorised table?</a></li>
<li class="chapter" data-level="38" data-path="machines-finding-functions.html"><a href="machines-finding-functions.html"><i class="fa fa-check"></i><b>38</b> Machines finding functions}</a></li>
<li class="chapter" data-level="39" data-path="learning-functions.html"><a href="learning-functions.html"><i class="fa fa-check"></i><b>39</b> Learning functions</a></li>
<li class="chapter" data-level="40" data-path="supervised-unsupervised-reinforcement-learning-and-functions.html"><a href="supervised-unsupervised-reinforcement-learning-and-functions.html"><i class="fa fa-check"></i><b>40</b> Supervised, unsupervised, reinforcement learning and functions</a></li>
<li class="chapter" data-level="41" data-path="which-function-operates.html"><a href="which-function-operates.html"><i class="fa fa-check"></i><b>41</b> Which function operates?</a></li>
<li class="chapter" data-level="42" data-path="what-does-a-function-learn.html"><a href="what-does-a-function-learn.html"><i class="fa fa-check"></i><b>42</b> What does a function learn?</a></li>
<li class="chapter" data-level="43" data-path="observing-with-curves-the-logistic-function.html"><a href="observing-with-curves-the-logistic-function.html"><i class="fa fa-check"></i><b>43</b> Observing with curves: the logistic function</a></li>
<li class="chapter" data-level="44" data-path="the-cost-of-curves-in-machine-learning.html"><a href="the-cost-of-curves-in-machine-learning.html"><i class="fa fa-check"></i><b>44</b> The cost of curves in machine learning</a></li>
<li class="chapter" data-level="45" data-path="curves-and-the-variation-in-models.html"><a href="curves-and-the-variation-in-models.html"><i class="fa fa-check"></i><b>45</b> Curves and the variation in models</a></li>
<li class="chapter" data-level="46" data-path="observing-costs-losses-and-objectives-through-optimisation.html"><a href="observing-costs-losses-and-objectives-through-optimisation.html"><i class="fa fa-check"></i><b>46</b> Observing costs, losses and objectives through optimisation</a></li>
<li class="chapter" data-level="47" data-path="gradients-as-partial-observers.html"><a href="gradients-as-partial-observers.html"><i class="fa fa-check"></i><b>47</b> Gradients as partial observers</a></li>
<li class="chapter" data-level="48" data-path="the-power-to-learn.html"><a href="the-power-to-learn.html"><i class="fa fa-check"></i><b>48</b> The power to learn</a></li>
<li class="chapter" data-level="49" data-path="n-forallboldsymbolx-probabilisation-and-the-taming-of-machines.html"><a href="n-forallboldsymbolx-probabilisation-and-the-taming-of-machines.html"><i class="fa fa-check"></i><b>49</b> <span class="math inline">\(N = \forall\boldsymbol{X}\)</span> Probabilisation and the Taming of Machines}</a></li>
<li class="chapter" data-level="50" data-path="data-reduces-uncertainty.html"><a href="data-reduces-uncertainty.html"><i class="fa fa-check"></i><b>50</b> Data reduces uncertainty?</a></li>
<li class="chapter" data-level="51" data-path="machine-learning-as-statistics-inside-out.html"><a href="machine-learning-as-statistics-inside-out.html"><i class="fa fa-check"></i><b>51</b> Machine learning as statistics inside out</a></li>
<li class="chapter" data-level="52" data-path="distributed-probabilities.html"><a href="distributed-probabilities.html"><i class="fa fa-check"></i><b>52</b> Distributed probabilities</a></li>
<li class="chapter" data-level="53" data-path="naive-bayes-and-the-distribution-of-probabilities.html"><a href="naive-bayes-and-the-distribution-of-probabilities.html"><i class="fa fa-check"></i><b>53</b> Naive Bayes and the distribution of probabilities</a></li>
<li class="chapter" data-level="54" data-path="spam-when-foralln-is-too-much.html"><a href="spam-when-foralln-is-too-much.html"><i class="fa fa-check"></i><b>54</b> Spam: when <span class="math inline">\(\forall{N}\)</span> is too much?</a></li>
<li class="chapter" data-level="55" data-path="the-improbable-success-of-the-naive-bayes-classifier.html"><a href="the-improbable-success-of-the-naive-bayes-classifier.html"><i class="fa fa-check"></i><b>55</b> The improbable success of the Naive Bayes classifier</a></li>
<li class="chapter" data-level="56" data-path="ancestral-probabilities-in-documents-inference-and-prediction.html"><a href="ancestral-probabilities-in-documents-inference-and-prediction.html"><i class="fa fa-check"></i><b>56</b> Ancestral probabilities in documents: inference and prediction</a></li>
<li class="chapter" data-level="57" data-path="statistical-decompositions-bias-variance-and-observed-errors.html"><a href="statistical-decompositions-bias-variance-and-observed-errors.html"><i class="fa fa-check"></i><b>57</b> Statistical decompositions: bias, variance and observed errors</a></li>
<li class="chapter" data-level="58" data-path="does-machine-learning-construct-a-new-statistical-reality.html"><a href="does-machine-learning-construct-a-new-statistical-reality.html"><i class="fa fa-check"></i><b>58</b> Does machine learning construct a new statistical reality?</a></li>
<li class="chapter" data-level="59" data-path="patterns-and-differences.html"><a href="patterns-and-differences.html"><i class="fa fa-check"></i><b>59</b> Patterns and differences</a></li>
<li class="chapter" data-level="60" data-path="splitting-and-the-growth-of-trees.html"><a href="splitting-and-the-growth-of-trees.html"><i class="fa fa-check"></i><b>60</b> Splitting and the growth of trees</a></li>
<li class="chapter" data-level="61" data-path="differences-in-recursive-partitioning.html"><a href="differences-in-recursive-partitioning.html"><i class="fa fa-check"></i><b>61</b> 1984: Differences in recursive partitioning</a></li>
<li class="chapter" data-level="62" data-path="limiting-differences.html"><a href="limiting-differences.html"><i class="fa fa-check"></i><b>62</b> Limiting differences</a></li>
<li class="chapter" data-level="63" data-path="the-successful-dispersion-of-the-support-vector-machine.html"><a href="the-successful-dispersion-of-the-support-vector-machine.html"><i class="fa fa-check"></i><b>63</b> The successful dispersion of the support vector machine</a></li>
<li class="chapter" data-level="64" data-path="differences-blur.html"><a href="differences-blur.html"><i class="fa fa-check"></i><b>64</b> Differences blur?</a></li>
<li class="chapter" data-level="65" data-path="bending-the-decision-boundary.html"><a href="bending-the-decision-boundary.html"><i class="fa fa-check"></i><b>65</b> Bending the decision boundary</a></li>
<li class="chapter" data-level="66" data-path="instituting-patterns.html"><a href="instituting-patterns.html"><i class="fa fa-check"></i><b>66</b> Instituting patterns</a></li>
<li class="chapter" data-level="67" data-path="regularizing-and-materializing-objects.html"><a href="regularizing-and-materializing-objects.html"><i class="fa fa-check"></i><b>67</b> Regularizing and materializing objects}</a></li>
<li class="chapter" data-level="68" data-path="genomic-referentiality-and-materiality.html"><a href="genomic-referentiality-and-materiality.html"><i class="fa fa-check"></i><b>68</b> Genomic referentiality and materiality</a></li>
<li class="chapter" data-level="69" data-path="the-genome-as-threshold-object.html"><a href="the-genome-as-threshold-object.html"><i class="fa fa-check"></i><b>69</b> The genome as threshold object</a></li>
<li class="chapter" data-level="70" data-path="genomic-knowledges-and-their-datasets.html"><a href="genomic-knowledges-and-their-datasets.html"><i class="fa fa-check"></i><b>70</b> Genomic knowledges and their datasets</a></li>
<li class="chapter" data-level="71" data-path="the-advent-of-wide-dirty-and-mixed-data.html"><a href="the-advent-of-wide-dirty-and-mixed-data.html"><i class="fa fa-check"></i><b>71</b> The advent of ‘wide, dirty and mixed’ data</a></li>
<li class="chapter" data-level="72" data-path="cross-validating-machine-learning-in-genomics.html"><a href="cross-validating-machine-learning-in-genomics.html"><i class="fa fa-check"></i><b>72</b> Cross-validating machine learning in genomics</a></li>
<li class="chapter" data-level="73" data-path="proliferation-of-discoveries.html"><a href="proliferation-of-discoveries.html"><i class="fa fa-check"></i><b>73</b> Proliferation of discoveries</a></li>
<li class="chapter" data-level="74" data-path="variations-in-the-object-or-in-the-machine-learner.html"><a href="variations-in-the-object-or-in-the-machine-learner.html"><i class="fa fa-check"></i><b>74</b> Variations in the object or in the machine learner?</a></li>
<li class="chapter" data-level="75" data-path="whole-genome-functions.html"><a href="whole-genome-functions.html"><i class="fa fa-check"></i><b>75</b> Whole genome functions</a></li>
<li class="chapter" data-level="76" data-path="propagating-subject-positions.html"><a href="propagating-subject-positions.html"><i class="fa fa-check"></i><b>76</b> Propagating subject positions}</a></li>
<li class="chapter" data-level="77" data-path="propagation-across-human-machine-boundaries.html"><a href="propagation-across-human-machine-boundaries.html"><i class="fa fa-check"></i><b>77</b> Propagation across human-machine boundaries</a></li>
<li class="chapter" data-level="78" data-path="competitive-positioning.html"><a href="competitive-positioning.html"><i class="fa fa-check"></i><b>78</b> Competitive positioning</a></li>
<li class="chapter" data-level="79" data-path="a-privileged-machine-and-its-diagrammatic-forms.html"><a href="a-privileged-machine-and-its-diagrammatic-forms.html"><i class="fa fa-check"></i><b>79</b> A privileged machine and its diagrammatic forms</a></li>
<li class="chapter" data-level="80" data-path="varying-subject-positions-in-code.html"><a href="varying-subject-positions-in-code.html"><i class="fa fa-check"></i><b>80</b> Varying subject positions in code</a></li>
<li class="chapter" data-level="81" data-path="the-subjects-of-a-hidden-operation.html"><a href="the-subjects-of-a-hidden-operation.html"><i class="fa fa-check"></i><b>81</b> The subjects of a hidden operation</a></li>
<li class="chapter" data-level="82" data-path="algorithms-that-propagate-errors.html"><a href="algorithms-that-propagate-errors.html"><i class="fa fa-check"></i><b>82</b> Algorithms that propagate errors</a></li>
<li class="chapter" data-level="83" data-path="competitions-as-examination.html"><a href="competitions-as-examination.html"><i class="fa fa-check"></i><b>83</b> Competitions as examination</a></li>
<li class="chapter" data-level="84" data-path="superimposing-power-and-knowledge.html"><a href="superimposing-power-and-knowledge.html"><i class="fa fa-check"></i><b>84</b> Superimposing power and knowledge</a></li>
<li class="chapter" data-level="85" data-path="ranked-subject-positions.html"><a href="ranked-subject-positions.html"><i class="fa fa-check"></i><b>85</b> Ranked subject positions</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="the-obdurate-mathematical-glint-of-machine-learning" class="section level1">
<h1><span class="header-section-number">23</span> The obdurate mathematical glint of machine learning</h1>
<p>If scientific research literature and operational <code>R</code> code constitute the elements of an operational formation presented in <em>Elements of Statistical Learning</em>, what of the mathematics? While references from many different places flow in and out of <em>Elements of Statistical Learning</em>, they are nearly all articulated in mathematical form. Machine learning as a grouping of statements relies heavily on mathematics. Given that mathematics is itself diverse and multi-stranded, what kind of mathematics matters here? While later chapters will explore some of the main mathematical practices (linear algebra, statistical inference, etc.), machine learners in <em>Elements of Statistical Learning</em> coalesce around a single exemplary technique: linear regression models or fitting a line to points.  The linear regression model is pivotal, not just in <em>Elements of Statistical Learning</em> but in much of the scientific and engineering literature. The linear regression model pushes up some of the citational peaks in Figure . Even though it is an old technique dating back to Francis Galton in the 1890s (see <span class="citation">[@Stigler_1986, chapter 8]</span>),  it remains perhaps the central working element of machine learning. </p>
<p><em>Elements of Statistical Learning</em> acknowledges the statistical legacy and inheritance in machine learning:</p>
<blockquote>
<p>The linear model has been a mainstay of statistics for the past 30 years and remains one of our most important tools. Given a vector of inputs <span class="math inline">\(X^T = (X_1 , X_2, . . . , X_p)\)</span>, we predict the output <span class="math inline">\(Y\)</span> via the model <span class="math display">\[\hat{Y} = \hat{\beta_0}  + \sum^p_(j=1)X_j\hat{\beta_j)}\]</span> The term <span class="math inline">\(\hat{\beta_0}\)</span> is the intercept, also known as the <em>bias</em> in machine learning <span class="citation">[@Hastie_2009, 11]</span>.</p>
</blockquote>
<p>In the course of the book, this mathematical expression appears in many variations, iterations, expansions and modifications (‘ridge regression’; ‘least angle regression’; ‘project pursuit,’ etc.). But this introduction of the ‘mainstay of statistics,’ the linear model, already introduces a diagrammatic element – the mathematical equation – that is perhaps the most prominent feature in the text.</p>
<p>Any reading of the book has to work out a way to traverse the forms show in equation .</p>

<p>In its relatively compressed typographic weave, expressions such as Equation  operationalize movements through data or ‘a vector of inputs <span class="math inline">\(X^T\)</span>.’ These expressions, which are not comfortable reading for non-technical readers, are however worth looking at carefully if we want to move ‘at the same level of abstraction as the algorithm’ <span class="citation">[@Pasquinelli_2015]</span>.  They can be found in hundreds in <span class="citation">[@Hastie_2009]</span>, but also in many other places. While their presence distinguishes machine learning from parts of computer science where mathematical equations are less common, mathematical formalizations also allow the book to suture a panoply of scientific publications and datasets in fields of statistics, artificial intelligence and computer science. Along with the citations, the graphical plots, and the code relationality, these equations are integral connective tissue in machine learning.</p>
<p>In dealing with the obscuring dazzle of mathematical formalisation, I find it useful here to follow Charles Sanders Peirce account of mathematics.  ‘Mathematical reasoning,’ he writes, ‘is diagrammatic’ <span class="citation">[@Peirce_1998, 206]</span>.   For Peirce, we should see mathematics, whether it takes an algebraic or geometrical form, whether it appears in symbols, letters, lines or curves as diagrams. For Peirce, a diagram is a kind of ‘icon.’  The icon is a sign that resembles the object it refers to: it has a relation of likeness. What likeness appears in equation ? Peirce says: ‘many diagrams resemble their objects not all in their looks; it is only in respect to the relations of their parts that their likeness consists’ <span class="citation">[@Peirce_1998, 13]</span>. As we have seen in the perceptron code, machine learners can be expressed in statements in a programming language like <code>Python</code> or <code>R.</code> In code, however, the relations between parts cannot be observed in the same way as they can in the algebraic form. The ‘very idea of the art,’ as Peirce puts it <span class="citation">[@Peirce_1992, 228]</span>, of algebraic expressions is that the formulae can be manipulated, or that component elements can be moved without much effort through substitutions, transformations and variations. The graphic form of the expression include the various classical Greek operator symbols such as  or , as well as the letters <span class="math inline">\(x, y, z\)</span> and the indices (indexical signs) such as <span class="math inline">\(j\)</span>  that appear in subscript or superscript, as well as the spatial arrangement of all these elements in lines and sometimes arrays. A variety of relations run between these different symbols and spatial arrangements. For instance, in all such expressions, the relation between the left hand side of the ‘=’ and the right hand side is very important. By convention, the left hand side of the expression is the value that is predicted or calculated (the ‘response’ variable) and the right hand side are the input variables or ‘features’ that contribute data to the model or algorithm.  This spatial arrangement fundamentally affects the design of algorithms. In the case of equations , the ‘^’ over <span class="math inline">\(\hat{Y}\)</span> symbolises a predicted value rather than a value that can be known completely through deduction, derivation or calculation. This distinction between predicted and actual values organizes a panoply of different practices and imperatives (for instance, to investigate the disparities between the predicted and actual values – machine learning practitioners spend a lot of time on that problem).</p>
<p>The broad point is that the whole formulae is a diagram, or an icon that ‘<em>exhibits</em>, by means of the algebraical signs (which are not themselves icons), the relations of the quantities concerned’ <span class="citation">[@Peirce_1998, 13]</span>. Because diagrams suppress so many details, they allow one to focus on a selected range of relations between parts. This affordance of diagrammatic mathematical forms is extremely important in the operational formation of machine learning. Diagrams can diagram other diagrams. Operations can themselves become the subject of operations. The nesting of diagram is highly generative since it allows what Peirce calls ‘transformations’ <span class="citation">[@Peirce_1998, 212]</span> or the construction of ‘a new general predicate’<span class="citation">[@Peirce_1992, 303]</span>.<a href="#fn13" class="footnoteRef" id="fnref13"><sup>13</sup></a> </p>
<p>While I seek to relate to the equations as diagrams, and will present a selection of them (nowhere near as many as found in <em>Elements of Statistical Learning</em>) in the following pages, I am not assuming their operation is transparent or fully legible. Just as much as the analysis of a photograph, a literary work or an ethnographic observation, their diagrammatic composition calls for repeated consideration. Peirce advises not to begin with examples that are too simple: ‘in simple cases, the essential features are often so nearly obliterated that they can only be discerned when one knows what to look for’ <span class="citation">[@Peirce_1998, 206]</span>. He also suggests ‘it is of great importance to return again and again to certain features’ <span class="citation">[@Peirce_1998, 206]</span>. Looking at these diagrammatic expressions repeatedly might allows us to map something of how transformations, generalisations or intensification flow across disciplinary boundaries, across social stratifications, and sometimes, generate potentially different ways of thinking about collectives, inclusion and belonging.</p>
</div>
<div class="footnotes">
<hr />
<ol start="13">
<li id="fn13"><p>Félix Guattari makes direct use of Peirce’s account of diagrams as icons of relation in his account of ‘abstract machines’ <span class="citation">[@Guattari_1984]</span>. He writes that ‘diagrammaticism brings into play more or less de-territorialized trans-semiotic forces, systems of signs, of code, of catalysts and so on, that make it possible in various specific ways to cut across stratifications of every kind’ <span class="citation">[@Guattari_1984, 145]</span>.   Here the ‘trans-semiotic forces’ include mathematical formulae and operations (such as the banking system of Renaissance Venice, Pisa and Genoa). They are trans-semiotic because they are not tethered by the signifying processes that code experience or speaking positions according to given stratifications such as class, gender, nation and so forth. While Guattari (and Deleuze in turn in their co-written works <span class="citation">[@Guattari_1988]</span>) is strongly critical of the way which signification territorializes (we might think of cats patrolling, marking and displaying in order to maintain their territories), he is much more affirmative of diagrammatic processes. He calls them ‘a-signifying’ to highlight their difference from the signifying processes that order social strata. He suggests that diagrams become the foundation for ‘abstract machines’ and the ‘simulation of physical machinic processes.’ Writing in the 1960s, Guattari powerfully anticipates the abstract machines and their associated diagrams that have taken shape and physical form in the succeeding decades. <a href="the-obdurate-mathematical-glint-of-machine-learning.html#fnref13">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="r-a-matrix-of-transformations.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="cs229-2007-returning-again-and-again-to-certain-features.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
