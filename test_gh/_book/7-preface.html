<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta property="og:title" content="" />
<meta property="og:type" content="book" />







<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="">

<title></title>

<script src="libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="libs/navigation-1.1/tabsets.js"></script>





<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
</style>
</head>

<body>

<div class="container-fluid main-container">


<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="1-acknowledgments.html#acknowledgments"><span class="toc-section-number">1</span> Acknowledgments</a></li>
<li><a href="2-machine-learners.html#machine-learners"><span class="toc-section-number">2</span> 250,000 machine learners</a></li>
<li><a href="3-a-summary-of-the-argument.html#a-summary-of-the-argument"><span class="toc-section-number">3</span> A summary of the argument</a></li>
<li><a href="4-in-situ-hybridization.html#in-situ-hybridization"><span class="toc-section-number">4</span> In-situ hybridization</a></li>
<li><a href="5-critical-operational-practice.html#critical-operational-practice"><span class="toc-section-number">5</span> Critical operational practice?</a></li>
<li><a href="6-obstacles-to-the-work-of-freeing-machine-learning.html#obstacles-to-the-work-of-freeing-machine-learning"><span class="toc-section-number">6</span> Obstacles to the work of freeing machine learning</a></li>
<li><a href="7-preface.html#preface"><span class="toc-section-number">7</span> Preface</a></li>
<li><a href="8-introduction-into-the-data.html#introduction-into-the-data"><span class="toc-section-number">8</span> Introduction: Into the Data</a></li>
<li><a href="9-three-accumulations-settings-data-and-devices.html#three-accumulations-settings-data-and-devices"><span class="toc-section-number">9</span> Three accumulations: settings, data and devices</a></li>
<li><a href="10-who-or-what-is-a-machine-learner.html#who-or-what-is-a-machine-learner"><span class="toc-section-number">10</span> Who or what is a machine learner?</a></li>
<li><a href="11-algorithmic-control-to-the-machine-learners.html#algorithmic-control-to-the-machine-learners"><span class="toc-section-number">11</span> Algorithmic control to the machine learners?</a></li>
<li><a href="12-the-archaeology-of-operations.html#the-archaeology-of-operations"><span class="toc-section-number">12</span> The archaeology of operations</a></li>
<li><a href="13-asymmetries-in-common-knowledge.html#asymmetries-in-common-knowledge"><span class="toc-section-number">13</span> Asymmetries in common knowledge</a></li>
<li><a href="14-what-cannot-be-automated.html#what-cannot-be-automated"><span class="toc-section-number">14</span> What cannot be automated?</a></li>
<li><a href="15-different-fields-in-machine-learning.html#different-fields-in-machine-learning"><span class="toc-section-number">15</span> Different fields in machine learning?</a></li>
<li><a href="16-the-diagram-in-critical-thought.html#the-diagram-in-critical-thought"><span class="toc-section-number">16</span> The diagram in critical thought</a></li>
<li><a href="17-we-dont-have-to-write-programs.html#we-dont-have-to-write-programs"><span class="toc-section-number">17</span> ‘We don’t have to write programs’?</a></li>
<li><a href="18-the-elements-of-machine-learning.html#the-elements-of-machine-learning"><span class="toc-section-number">18</span> The elements of machine learning</a></li>
<li><a href="19-who-reads-machine-learning-textbooks.html#who-reads-machine-learning-textbooks"><span class="toc-section-number">19</span> Who reads machine learning textbooks?</a></li>
<li><a href="20-r-a-matrix-of-transformations.html#r-a-matrix-of-transformations"><span class="toc-section-number">20</span> <code>R</code>: a matrix of transformations</a></li>
<li><a href="21-the-obdurate-mathematical-glint-of-machine-learning.html#the-obdurate-mathematical-glint-of-machine-learning"><span class="toc-section-number">21</span> The obdurate mathematical glint of machine learning</a></li>
<li><a href="22-cs229-2007-returning-again-and-again-to-certain-features.html#cs229-2007-returning-again-and-again-to-certain-features"><span class="toc-section-number">22</span> CS229, 2007: returning again and again to certain features</a></li>
<li><a href="23-the-visible-learning-of-machine-learning.html#the-visible-learning-of-machine-learning"><span class="toc-section-number">23</span> The visible learning of machine learning</a></li>
<li><a href="24-the-diagram-of-an-operational-formation.html#the-diagram-of-an-operational-formation"><span class="toc-section-number">24</span> The diagram of an operational formation</a></li>
<li><a href="25-vector-space-and-geometry.html#vector-space-and-geometry"><span class="toc-section-number">25</span> Vector space and geometry</a></li>
<li><a href="26-mixing-places.html#mixing-places"><span class="toc-section-number">26</span> Mixing places</a></li>
<li><a href="27-truth-is-no-longer-in-the-table.html#truth-is-no-longer-in-the-table"><span class="toc-section-number">27</span> Truth is no longer in the table?</a></li>
<li><a href="28-the-epistopic-fault-line-in-tables.html#the-epistopic-fault-line-in-tables"><span class="toc-section-number">28</span> The epistopic fault line in tables</a></li>
<li><a href="29-surface-and-depths-the-problem-of-volume-in-data.html#surface-and-depths-the-problem-of-volume-in-data"><span class="toc-section-number">29</span> Surface and depths: the problem of volume in data</a></li>
<li><a href="30-vector-space-expansion.html#vector-space-expansion"><span class="toc-section-number">30</span> Vector space expansion</a></li>
<li><a href="31-drawing-lines-in-a-common-space-of-transformation.html#drawing-lines-in-a-common-space-of-transformation"><span class="toc-section-number">31</span> Drawing lines in a common space of transformation</a></li>
<li><a href="32-implicit-vectorization-in-code-and-infrastructures.html#implicit-vectorization-in-code-and-infrastructures"><span class="toc-section-number">32</span> Implicit vectorization in code and infrastructures</a></li>
<li><a href="33-lines-traversing-behind-the-light.html#lines-traversing-behind-the-light"><span class="toc-section-number">33</span> Lines traversing behind the light</a></li>
<li><a href="34-the-vectorised-table.html#the-vectorised-table"><span class="toc-section-number">34</span> The vectorised table?</a></li>
<li><a href="35-learning-functions.html#learning-functions"><span class="toc-section-number">35</span> Learning functions</a></li>
<li><a href="36-supervised-unsupervised-reinforcement-learning-and-functions.html#supervised-unsupervised-reinforcement-learning-and-functions"><span class="toc-section-number">36</span> Supervised, unsupervised, reinforcement learning and functions</a></li>
<li><a href="37-which-function-operates.html#which-function-operates"><span class="toc-section-number">37</span> Which function operates?</a></li>
<li><a href="38-what-does-a-function-learn.html#what-does-a-function-learn"><span class="toc-section-number">38</span> What does a function learn?</a></li>
<li><a href="39-observing-with-curves-the-logistic-function.html#observing-with-curves-the-logistic-function"><span class="toc-section-number">39</span> Observing with curves: the logistic function</a></li>
<li><a href="40-the-cost-of-curves-in-machine-learning.html#the-cost-of-curves-in-machine-learning"><span class="toc-section-number">40</span> The cost of curves in machine learning</a></li>
<li><a href="41-curves-and-the-variation-in-models.html#curves-and-the-variation-in-models"><span class="toc-section-number">41</span> Curves and the variation in models</a></li>
<li><a href="42-observing-costs-losses-and-objectives-through-optimisation.html#observing-costs-losses-and-objectives-through-optimisation"><span class="toc-section-number">42</span> Observing costs, losses and objectives through optimisation</a></li>
<li><a href="43-gradients-as-partial-observers.html#gradients-as-partial-observers"><span class="toc-section-number">43</span> Gradients as partial observers</a></li>
<li><a href="44-the-power-to-learn.html#the-power-to-learn"><span class="toc-section-number">44</span> The power to learn</a></li>
<li><a href="45-data-reduces-uncertainty.html#data-reduces-uncertainty"><span class="toc-section-number">45</span> Data reduces uncertainty?</a></li>
<li><a href="46-machine-learning-as-statistics-inside-out.html#machine-learning-as-statistics-inside-out"><span class="toc-section-number">46</span> Machine learning as statistics inside out</a></li>
<li><a href="47-distributed-probabilities.html#distributed-probabilities"><span class="toc-section-number">47</span> Distributed probabilities</a></li>
<li><a href="48-naive-bayes-and-the-distribution-of-probabilities.html#naive-bayes-and-the-distribution-of-probabilities"><span class="toc-section-number">48</span> Naive Bayes and the distribution of probabilities</a></li>
<li><a href="49-spam-when-foralln-is-too-much.html#spam-when-foralln-is-too-much"><span class="toc-section-number">49</span> Spam: when <span class="math inline">\(\forall{N}\)</span> is too much?</a></li>
<li><a href="50-the-improbable-success-of-the-naive-bayes-classifier.html#the-improbable-success-of-the-naive-bayes-classifier"><span class="toc-section-number">50</span> The improbable success of the Naive Bayes classifier</a></li>
<li><a href="51-ancestral-probabilities-in-documents-inference-and-prediction.html#ancestral-probabilities-in-documents-inference-and-prediction"><span class="toc-section-number">51</span> Ancestral probabilities in documents: inference and prediction</a></li>
<li><a href="52-statistical-decompositions-bias-variance-and-observed-errors.html#statistical-decompositions-bias-variance-and-observed-errors"><span class="toc-section-number">52</span> Statistical decompositions: bias, variance and observed errors</a></li>
<li><a href="53-does-machine-learning-construct-a-new-statistical-reality.html#does-machine-learning-construct-a-new-statistical-reality"><span class="toc-section-number">53</span> Does machine learning construct a new statistical reality?</a></li>
<li><a href="54-splitting-and-the-growth-of-trees.html#splitting-and-the-growth-of-trees"><span class="toc-section-number">54</span> Splitting and the growth of trees</a></li>
<li><a href="55-differences-in-recursive-partitioning.html#differences-in-recursive-partitioning"><span class="toc-section-number">55</span> 1984: Differences in recursive partitioning</a></li>
<li><a href="56-limiting-differences.html#limiting-differences"><span class="toc-section-number">56</span> Limiting differences</a></li>
<li><a href="57-the-successful-dispersion-of-the-support-vector-machine.html#the-successful-dispersion-of-the-support-vector-machine"><span class="toc-section-number">57</span> The successful dispersion of the support vector machine</a></li>
<li><a href="58-differences-blur.html#differences-blur"><span class="toc-section-number">58</span> Differences blur?</a></li>
<li><a href="59-bending-the-decision-boundary.html#bending-the-decision-boundary"><span class="toc-section-number">59</span> Bending the decision boundary</a></li>
<li><a href="60-instituting-patterns.html#instituting-patterns"><span class="toc-section-number">60</span> Instituting patterns</a></li>
<li><a href="61-genomic-referentiality-and-materiality.html#genomic-referentiality-and-materiality"><span class="toc-section-number">61</span> Genomic referentiality and materiality</a></li>
<li><a href="62-the-genome-as-threshold-object.html#the-genome-as-threshold-object"><span class="toc-section-number">62</span> The genome as threshold object</a></li>
<li><a href="63-genomic-knowledges-and-their-datasets.html#genomic-knowledges-and-their-datasets"><span class="toc-section-number">63</span> Genomic knowledges and their datasets</a></li>
<li><a href="64-the-advent-of-wide-dirty-and-mixed-data.html#the-advent-of-wide-dirty-and-mixed-data"><span class="toc-section-number">64</span> The advent of ‘wide, dirty and mixed’ data</a></li>
<li><a href="65-cross-validating-machine-learning-in-genomics.html#cross-validating-machine-learning-in-genomics"><span class="toc-section-number">65</span> Cross-validating machine learning in genomics</a></li>
<li><a href="66-proliferation-of-discoveries.html#proliferation-of-discoveries"><span class="toc-section-number">66</span> Proliferation of discoveries</a></li>
<li><a href="67-variations-in-the-object-or-in-the-machine-learner.html#variations-in-the-object-or-in-the-machine-learner"><span class="toc-section-number">67</span> Variations in the object or in the machine learner?</a></li>
<li><a href="68-whole-genome-functions.html#whole-genome-functions"><span class="toc-section-number">68</span> Whole genome functions</a></li>
<li><a href="69-propagation-across-human-machine-boundaries.html#propagation-across-human-machine-boundaries"><span class="toc-section-number">69</span> Propagation across human-machine boundaries</a></li>
<li><a href="70-competitive-positioning.html#competitive-positioning"><span class="toc-section-number">70</span> Competitive positioning</a></li>
<li><a href="71-a-privileged-machine-and-its-diagrammatic-forms.html#a-privileged-machine-and-its-diagrammatic-forms"><span class="toc-section-number">71</span> A privileged machine and its diagrammatic forms</a></li>
<li><a href="72-varying-subject-positions-in-code.html#varying-subject-positions-in-code"><span class="toc-section-number">72</span> Varying subject positions in code</a></li>
<li><a href="73-the-subjects-of-a-hidden-operation.html#the-subjects-of-a-hidden-operation"><span class="toc-section-number">73</span> The subjects of a hidden operation</a></li>
<li><a href="74-algorithms-that-propagate-errors.html#algorithms-that-propagate-errors"><span class="toc-section-number">74</span> Algorithms that propagate errors</a></li>
<li><a href="75-competitions-as-examination.html#competitions-as-examination"><span class="toc-section-number">75</span> Competitions as examination</a></li>
<li><a href="76-superimposing-power-and-knowledge.html#superimposing-power-and-knowledge"><span class="toc-section-number">76</span> Superimposing power and knowledge</a></li>
<li><a href="77-ranked-subject-positions.html#ranked-subject-positions"><span class="toc-section-number">77</span> Ranked subject positions</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="preface" class="section level1">
<h1><span class="header-section-number">7</span> Preface</h1>
<p>Although book is not an ethnography, it has an ethnographic situation. If it has a field site, it lies close to the places where the writing was done – in universities, on campuses, in classrooms and online training courses (including MOOCs), and then amidst the books, documents, websites, software manuals and documentation, and a rather vast accumulation of scientific publications. It’s a case of ‘dig where you stand,’ or ‘auto-archaeology.’ </p>
<p>Readers familiar with textbooks in computer science and statistics can detect the traces of this setting in various typographic conventions drawn from the fields I write about. Important conventions include:</p>
<ol style="list-style-type: decimal">
<li>Typesetting the name of any code or devices that do machine learning and datasets on which machine learners operate in a <code>monospace</code> or terminal font: <code>machine learner</code> or <code>iris</code>;</li>
<li>Presenting formulae, functions, and equations using the bristling indexicality of mathematical typography: <span class="math inline">\(\hat{\beta}\)</span></li>
</ol>
<p>I emulate the apparatus of science and engineering publication as an experiment in <em>in-situ</em> hybridization. Social science and humanities researchers, even when they are observant participants in their field sites, rarely experience a coincidence between their own writing practices and that of the participants in the research site they study. The object of study in this book, however, is a knowledge practice that documents itself in code, equations, diagrams and statements circulated in articles, books and various online formats (blogs, wikis, software repositories). It is possible for a social researcher to also adopt some of these practices.</p>
<p>I’ve been writing code for years <span class="citation">[@Mackenzie_2006]</span>. Writing code was nearly always something distant from writing about code since coding was about software projects and writing was about thinking and knowledge. I was slow to realise they are much entangled. Recent developments in ways of analysing and publishing scientific data bring coding and writing closer together. Implementing code can be done almost in the same space, in the same screen or pane, as writing about code. The mingling of coding and writing about code brings about sometimes generative, sometimes frustrating, encounters with various scientific knowledge (mathematics, statistics, computer science), with infrastructures and devices on many scales (ranging across networks, text editors, databases here and there, hardware and platforms of various kinds, as well as interfaces) and many domains.</p>
<p>At many points in researching the book, I digressed a long way into quite technical domains of statistical inference, probability theory, linear algebra, dynamic models as well as database design and data standards. In the interests of maintaining a strong feedback signal running through the many propositions, formulations, diagrams, equations, citations and images in this book, much of the code I’ve written in implementing machine learning models or in reconstructing certain data practices does not appear in this text, just as not all of the words I’ve written in trying to construct arguments or think about data practices has been included. Much has been cut away and left on the ground (although the <code>git</code> repository of the book preserves many traces of the writing and code; see <a href="https://github.com/datapractice/machinelearning" class="uri">https://github.com/datapractice/machinelearning</a>). As in the many machine learning textbooks, recipe books, cookbooks, how-tos, tutorials and manuals I have read, code, graphics and prose have been tidied here. Many exploratory forays are lost and almost forgotten.</p>
<p>The several years I have spent doing and writing about data practice has felt substantially different to any other project by virtue of the hybridization between code in text, and text in code. Practically, this is made possible by working on code and text within the same file, in the same text editor. Switching between writing <code>R</code> and Python code (about which I say more below) to retrieve data, to transform it, to produce graphics, to construct models or some kind of graphic image, and within the same file be writing academic prose, might be one way to write about machine learning as a data practice.</p>
<p>The capacity to mingle text, code and images depends on an ensemble of open source, often command-line software tools that differ somewhat from the typical social scientist or humanities researchers’ software toolkit of word processor, bibliographic software, image editor and web browser. In particular, I have relied on software packages in the <code>R</code> programming language such as the ‘<code>knitr</code>’ <span class="citation">[@Xie_2013; @Xie_2012]</span> and in <code>python,</code> the <code>ipython</code> notebook environment <span class="citation">[@Perez_2007]</span>. Both have been developed by scientists and statisticians in the name of ‘reproducible research.’  Many examples of this form of writing can be found on the web: see <a href="http://nbviewer.ipython.org/">IPython Notebook Viewer</a> for a sample of these. These packages are designed to allow a combination of code written in <code>R</code>, python or other programming languages, scientific writing (including mathematical formula) and images to be included, and importantly, executed together to produce a document.<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a> </p>
<p>In making use of the equipment created by the people I study, I’ve attempted to bring the writing of code and writing about code-like operations into critical proximity. Does proximity or mixing of writing code and writing words make a practical difference to an account of practice? If recent theories of code and software as forms of speech, expression or performative utterance <span class="citation">[@Cox_2012;@Coleman_2012]</span>, or more generally praxiography as a reality-making descriptive practice <span class="citation">[@Mol_2003]</span> are right, it should. Weaving code through writing in one domain of contemporary technical practice, machine learning, might be one way of keeping multiple practices present, developing a concrete sense of abstraction and allowing an affective expansion in relation to machines.</p>

</div>
<div class="footnotes">
<hr />
<ol start="2">
<li id="fn2"><p>In order to do this, they typically combine some form of text formatting or ‘markup,’ that ranges from very simple formatting conventions (for instance, the ‘Markdown’ format used in this book is much less complicated than HTML, and uses markup conventions readable as plain text and modelled on email <span class="citation">[@Gruber_2004]</span>;) to the highly technical (LaTeX, the de-facto scientific publishing format or ‘document preparation system’ <span class="citation">[@Lamport_1986]</span> elements of which are also used here to convey mathematical expressions). They add to that blocks of code and inline code fragments that are executed as the text is formatted in order to produce results that are shown in the text or inserted as figures in the text.</p><p>There are a few different ways of weaving together text, computation and images together. Each suffers from different limitations. In <code>ipython</code>, a scientific computing platform dating from 2005 <span class="citation">[@Perez_2007]</span> and used across a range of scientific settings, interactive visualization and plotting, as well as access to operating system functions are brought together in a <code>Python</code> programming environment. Especially in using the <code>ipython</code> notebook, where editing text and editing code is all done in the same window, and the results of changes to code can be seen immediately, practices of working with data can be directly woven together with writing about practice. By contrast, <code>knitr</code> generates documents by combining text passages and the results (graphs, calculations, tabulations of data) of code interleaved between the text into one output document. When <code>knitr</code> runs, it executes the code and inserts the results (calculations, text, images) in the flow of text. </p><p>Practically, this means that the text editor used to write code and text, remains somewhat separate from the software that executes the code. By contrast, <code>ipython</code> combines text and <code>Python</code> code more continuously, but at the cost of editing and writing code and text in a browser window. Most of the conveniences and affordances of text editing software is lost. While <code>ipython</code> focuses on interactive computation, <code>knitr</code> focuses on bringing together scientific document formatting and computation. Given that both can include code written in other languages (that is, <code>python</code> code can be processed by <code>knitr</code>, and <code>R</code> code executed in <code>ipython</code>), the differences are not crucially important. This whole book could have been written using just Python, since Python is a popular general purpose programming language, and many statistical, machine learning and data analysis libraries have been written for Python. I have used both, sometimes to highlight tensions between the somewhat more research-oriented <code>R</code> and the more practical applications typical of Python, and sometimes because code in one language is more easily understood than the other.  <a href="7-preface.html#fnref2">↩</a></p></li>
</ol>
</div>
<p style="text-align: center;">
<a href="6-obstacles-to-the-work-of-freeing-machine-learning.html"><button class="btn btn-default">Previous</button></a>
<a href="8-introduction-into-the-data.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
