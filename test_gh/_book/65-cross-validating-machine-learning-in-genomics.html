<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta property="og:title" content="" />
<meta property="og:type" content="book" />







<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="">

<title></title>

<script src="libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="libs/navigation-1.1/tabsets.js"></script>





<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
</style>
</head>

<body>

<div class="container-fluid main-container">


<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="1-acknowledgments.html#acknowledgments"><span class="toc-section-number">1</span> Acknowledgments</a></li>
<li><a href="2-machine-learners.html#machine-learners"><span class="toc-section-number">2</span> 250,000 machine learners</a></li>
<li><a href="3-a-summary-of-the-argument.html#a-summary-of-the-argument"><span class="toc-section-number">3</span> A summary of the argument</a></li>
<li><a href="4-in-situ-hybridization.html#in-situ-hybridization"><span class="toc-section-number">4</span> In-situ hybridization</a></li>
<li><a href="5-critical-operational-practice.html#critical-operational-practice"><span class="toc-section-number">5</span> Critical operational practice?</a></li>
<li><a href="6-obstacles-to-the-work-of-freeing-machine-learning.html#obstacles-to-the-work-of-freeing-machine-learning"><span class="toc-section-number">6</span> Obstacles to the work of freeing machine learning</a></li>
<li><a href="7-preface.html#preface"><span class="toc-section-number">7</span> Preface</a></li>
<li><a href="8-introduction-into-the-data.html#introduction-into-the-data"><span class="toc-section-number">8</span> Introduction: Into the Data</a></li>
<li><a href="9-three-accumulations-settings-data-and-devices.html#three-accumulations-settings-data-and-devices"><span class="toc-section-number">9</span> Three accumulations: settings, data and devices</a></li>
<li><a href="10-who-or-what-is-a-machine-learner.html#who-or-what-is-a-machine-learner"><span class="toc-section-number">10</span> Who or what is a machine learner?</a></li>
<li><a href="11-algorithmic-control-to-the-machine-learners.html#algorithmic-control-to-the-machine-learners"><span class="toc-section-number">11</span> Algorithmic control to the machine learners?</a></li>
<li><a href="12-the-archaeology-of-operations.html#the-archaeology-of-operations"><span class="toc-section-number">12</span> The archaeology of operations</a></li>
<li><a href="13-asymmetries-in-common-knowledge.html#asymmetries-in-common-knowledge"><span class="toc-section-number">13</span> Asymmetries in common knowledge</a></li>
<li><a href="14-what-cannot-be-automated.html#what-cannot-be-automated"><span class="toc-section-number">14</span> What cannot be automated?</a></li>
<li><a href="15-different-fields-in-machine-learning.html#different-fields-in-machine-learning"><span class="toc-section-number">15</span> Different fields in machine learning?</a></li>
<li><a href="16-the-diagram-in-critical-thought.html#the-diagram-in-critical-thought"><span class="toc-section-number">16</span> The diagram in critical thought</a></li>
<li><a href="17-we-dont-have-to-write-programs.html#we-dont-have-to-write-programs"><span class="toc-section-number">17</span> ‘We don’t have to write programs’?</a></li>
<li><a href="18-the-elements-of-machine-learning.html#the-elements-of-machine-learning"><span class="toc-section-number">18</span> The elements of machine learning</a></li>
<li><a href="19-who-reads-machine-learning-textbooks.html#who-reads-machine-learning-textbooks"><span class="toc-section-number">19</span> Who reads machine learning textbooks?</a></li>
<li><a href="20-r-a-matrix-of-transformations.html#r-a-matrix-of-transformations"><span class="toc-section-number">20</span> <code>R</code>: a matrix of transformations</a></li>
<li><a href="21-the-obdurate-mathematical-glint-of-machine-learning.html#the-obdurate-mathematical-glint-of-machine-learning"><span class="toc-section-number">21</span> The obdurate mathematical glint of machine learning</a></li>
<li><a href="22-cs229-2007-returning-again-and-again-to-certain-features.html#cs229-2007-returning-again-and-again-to-certain-features"><span class="toc-section-number">22</span> CS229, 2007: returning again and again to certain features</a></li>
<li><a href="23-the-visible-learning-of-machine-learning.html#the-visible-learning-of-machine-learning"><span class="toc-section-number">23</span> The visible learning of machine learning</a></li>
<li><a href="24-the-diagram-of-an-operational-formation.html#the-diagram-of-an-operational-formation"><span class="toc-section-number">24</span> The diagram of an operational formation</a></li>
<li><a href="25-vector-space-and-geometry.html#vector-space-and-geometry"><span class="toc-section-number">25</span> Vector space and geometry</a></li>
<li><a href="26-mixing-places.html#mixing-places"><span class="toc-section-number">26</span> Mixing places</a></li>
<li><a href="27-truth-is-no-longer-in-the-table.html#truth-is-no-longer-in-the-table"><span class="toc-section-number">27</span> Truth is no longer in the table?</a></li>
<li><a href="28-the-epistopic-fault-line-in-tables.html#the-epistopic-fault-line-in-tables"><span class="toc-section-number">28</span> The epistopic fault line in tables</a></li>
<li><a href="29-surface-and-depths-the-problem-of-volume-in-data.html#surface-and-depths-the-problem-of-volume-in-data"><span class="toc-section-number">29</span> Surface and depths: the problem of volume in data</a></li>
<li><a href="30-vector-space-expansion.html#vector-space-expansion"><span class="toc-section-number">30</span> Vector space expansion</a></li>
<li><a href="31-drawing-lines-in-a-common-space-of-transformation.html#drawing-lines-in-a-common-space-of-transformation"><span class="toc-section-number">31</span> Drawing lines in a common space of transformation</a></li>
<li><a href="32-implicit-vectorization-in-code-and-infrastructures.html#implicit-vectorization-in-code-and-infrastructures"><span class="toc-section-number">32</span> Implicit vectorization in code and infrastructures</a></li>
<li><a href="33-lines-traversing-behind-the-light.html#lines-traversing-behind-the-light"><span class="toc-section-number">33</span> Lines traversing behind the light</a></li>
<li><a href="34-the-vectorised-table.html#the-vectorised-table"><span class="toc-section-number">34</span> The vectorised table?</a></li>
<li><a href="35-learning-functions.html#learning-functions"><span class="toc-section-number">35</span> Learning functions</a></li>
<li><a href="36-supervised-unsupervised-reinforcement-learning-and-functions.html#supervised-unsupervised-reinforcement-learning-and-functions"><span class="toc-section-number">36</span> Supervised, unsupervised, reinforcement learning and functions</a></li>
<li><a href="37-which-function-operates.html#which-function-operates"><span class="toc-section-number">37</span> Which function operates?</a></li>
<li><a href="38-what-does-a-function-learn.html#what-does-a-function-learn"><span class="toc-section-number">38</span> What does a function learn?</a></li>
<li><a href="39-observing-with-curves-the-logistic-function.html#observing-with-curves-the-logistic-function"><span class="toc-section-number">39</span> Observing with curves: the logistic function</a></li>
<li><a href="40-the-cost-of-curves-in-machine-learning.html#the-cost-of-curves-in-machine-learning"><span class="toc-section-number">40</span> The cost of curves in machine learning</a></li>
<li><a href="41-curves-and-the-variation-in-models.html#curves-and-the-variation-in-models"><span class="toc-section-number">41</span> Curves and the variation in models</a></li>
<li><a href="42-observing-costs-losses-and-objectives-through-optimisation.html#observing-costs-losses-and-objectives-through-optimisation"><span class="toc-section-number">42</span> Observing costs, losses and objectives through optimisation</a></li>
<li><a href="43-gradients-as-partial-observers.html#gradients-as-partial-observers"><span class="toc-section-number">43</span> Gradients as partial observers</a></li>
<li><a href="44-the-power-to-learn.html#the-power-to-learn"><span class="toc-section-number">44</span> The power to learn</a></li>
<li><a href="45-data-reduces-uncertainty.html#data-reduces-uncertainty"><span class="toc-section-number">45</span> Data reduces uncertainty?</a></li>
<li><a href="46-machine-learning-as-statistics-inside-out.html#machine-learning-as-statistics-inside-out"><span class="toc-section-number">46</span> Machine learning as statistics inside out</a></li>
<li><a href="47-distributed-probabilities.html#distributed-probabilities"><span class="toc-section-number">47</span> Distributed probabilities</a></li>
<li><a href="48-naive-bayes-and-the-distribution-of-probabilities.html#naive-bayes-and-the-distribution-of-probabilities"><span class="toc-section-number">48</span> Naive Bayes and the distribution of probabilities</a></li>
<li><a href="49-spam-when-foralln-is-too-much.html#spam-when-foralln-is-too-much"><span class="toc-section-number">49</span> Spam: when <span class="math inline">\(\forall{N}\)</span> is too much?</a></li>
<li><a href="50-the-improbable-success-of-the-naive-bayes-classifier.html#the-improbable-success-of-the-naive-bayes-classifier"><span class="toc-section-number">50</span> The improbable success of the Naive Bayes classifier</a></li>
<li><a href="51-ancestral-probabilities-in-documents-inference-and-prediction.html#ancestral-probabilities-in-documents-inference-and-prediction"><span class="toc-section-number">51</span> Ancestral probabilities in documents: inference and prediction</a></li>
<li><a href="52-statistical-decompositions-bias-variance-and-observed-errors.html#statistical-decompositions-bias-variance-and-observed-errors"><span class="toc-section-number">52</span> Statistical decompositions: bias, variance and observed errors</a></li>
<li><a href="53-does-machine-learning-construct-a-new-statistical-reality.html#does-machine-learning-construct-a-new-statistical-reality"><span class="toc-section-number">53</span> Does machine learning construct a new statistical reality?</a></li>
<li><a href="54-splitting-and-the-growth-of-trees.html#splitting-and-the-growth-of-trees"><span class="toc-section-number">54</span> Splitting and the growth of trees</a></li>
<li><a href="55-differences-in-recursive-partitioning.html#differences-in-recursive-partitioning"><span class="toc-section-number">55</span> 1984: Differences in recursive partitioning</a></li>
<li><a href="56-limiting-differences.html#limiting-differences"><span class="toc-section-number">56</span> Limiting differences</a></li>
<li><a href="57-the-successful-dispersion-of-the-support-vector-machine.html#the-successful-dispersion-of-the-support-vector-machine"><span class="toc-section-number">57</span> The successful dispersion of the support vector machine</a></li>
<li><a href="58-differences-blur.html#differences-blur"><span class="toc-section-number">58</span> Differences blur?</a></li>
<li><a href="59-bending-the-decision-boundary.html#bending-the-decision-boundary"><span class="toc-section-number">59</span> Bending the decision boundary</a></li>
<li><a href="60-instituting-patterns.html#instituting-patterns"><span class="toc-section-number">60</span> Instituting patterns</a></li>
<li><a href="61-genomic-referentiality-and-materiality.html#genomic-referentiality-and-materiality"><span class="toc-section-number">61</span> Genomic referentiality and materiality</a></li>
<li><a href="62-the-genome-as-threshold-object.html#the-genome-as-threshold-object"><span class="toc-section-number">62</span> The genome as threshold object</a></li>
<li><a href="63-genomic-knowledges-and-their-datasets.html#genomic-knowledges-and-their-datasets"><span class="toc-section-number">63</span> Genomic knowledges and their datasets</a></li>
<li><a href="64-the-advent-of-wide-dirty-and-mixed-data.html#the-advent-of-wide-dirty-and-mixed-data"><span class="toc-section-number">64</span> The advent of ‘wide, dirty and mixed’ data</a></li>
<li><a href="65-cross-validating-machine-learning-in-genomics.html#cross-validating-machine-learning-in-genomics"><span class="toc-section-number">65</span> Cross-validating machine learning in genomics</a></li>
<li><a href="66-proliferation-of-discoveries.html#proliferation-of-discoveries"><span class="toc-section-number">66</span> Proliferation of discoveries</a></li>
<li><a href="67-variations-in-the-object-or-in-the-machine-learner.html#variations-in-the-object-or-in-the-machine-learner"><span class="toc-section-number">67</span> Variations in the object or in the machine learner?</a></li>
<li><a href="68-whole-genome-functions.html#whole-genome-functions"><span class="toc-section-number">68</span> Whole genome functions</a></li>
<li><a href="69-propagation-across-human-machine-boundaries.html#propagation-across-human-machine-boundaries"><span class="toc-section-number">69</span> Propagation across human-machine boundaries</a></li>
<li><a href="70-competitive-positioning.html#competitive-positioning"><span class="toc-section-number">70</span> Competitive positioning</a></li>
<li><a href="71-a-privileged-machine-and-its-diagrammatic-forms.html#a-privileged-machine-and-its-diagrammatic-forms"><span class="toc-section-number">71</span> A privileged machine and its diagrammatic forms</a></li>
<li><a href="72-varying-subject-positions-in-code.html#varying-subject-positions-in-code"><span class="toc-section-number">72</span> Varying subject positions in code</a></li>
<li><a href="73-the-subjects-of-a-hidden-operation.html#the-subjects-of-a-hidden-operation"><span class="toc-section-number">73</span> The subjects of a hidden operation</a></li>
<li><a href="74-algorithms-that-propagate-errors.html#algorithms-that-propagate-errors"><span class="toc-section-number">74</span> Algorithms that propagate errors</a></li>
<li><a href="75-competitions-as-examination.html#competitions-as-examination"><span class="toc-section-number">75</span> Competitions as examination</a></li>
<li><a href="76-superimposing-power-and-knowledge.html#superimposing-power-and-knowledge"><span class="toc-section-number">76</span> Superimposing power and knowledge</a></li>
<li><a href="77-ranked-subject-positions.html#ranked-subject-positions"><span class="toc-section-number">77</span> Ranked subject positions</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="cross-validating-machine-learning-in-genomics" class="section level1">
<h1><span class="header-section-number">65</span> Cross-validating machine learning in genomics</h1>
<p>The linear sequences of DNA data mix and diffuse partly through the archival accumulation that allows them to be superimposed, annotated and layered, but also through the many efforts to traverse their expanded volume using classifiers and predictive models. A recent review in the journal <em>Genomics</em> highlights the increasing bearing of machine learning techniques on genomic science:</p>
<blockquote>
<p>High-throughput genomic technologies, including gene expression microarray, single Nucleotideide polymorphism (SNP) array, microRNA array, RNA-seq, ChIP-seq, and whole genome sequencing, are powerful tools that have dramatically changed the landscape of biological research. At the same time, large-scale genomic data present significant challenges for statistical and bioinformatic data analysis as the high dimensionality of genomic features makes the classical regression framework no longer feasible. As well, the highly correlated structure of genomic data violates the independent assumption required by standard statistical models<span class="citation">[@Chen_2012, 323]</span>.  </p>
</blockquote>
<p>Commentary on the ‘highly correlated structure,’ not just the volume, of genomic data, points to another referential operation concerning the differentiation of things.  Many such statements highlight the incompatibility between a surging multiplicity of data forms and the constraints of existing statistical modelling techniques (‘standard statistical models’). So for instance, Chen and co-authors recommend the use of the random forest (RF) because it:</p>
<blockquote>
<p>is highly data adaptive, applies to “large p, small n” problems, and is able to account for correlation as well as interactions among features. This makes RF particularly appealing for high-dimensional genomic data analysis, … including prediction and classification, variable selection, pathway analysis, genetic association and epistasis detection, and unsupervised learning <span class="citation">[@Chen_2012, 323]</span> </p>
</blockquote>
<p>Familiar machine learning vectorisation keywords such as ‘large p, small n’ and ‘high dimensional’ pepper their recommendations. But the key terms on the genomics side of this formulation would perhaps be ‘pathway analysis’, ‘genetic association,’ and ‘epistasis.’ Such biological terms point to forms of relationality associated with biologically interesting processes. Epistasis for instance broadly refers to linked gene action, a process that has been difficult to study before high-throughput methods of functional genomics brought shifting patterns of gene expression to light.  In contemporary genomic science, these biological processes are increasingly understood in terms of eliciting and modelling the relations between <em>features</em> of genomic datasets in order to classify and predict biological outcomes.</p>
<p>How does machine learning differ from the statistical practice that has underpinned much of modern biology? The analysis of <code>SRBCT</code> gene expression in <em>Elements of Statistical Learning</em> is symptomatic of a mutual articulation, a cross-validation that entangles genomics and machine learning.  The overt arrival of machine learning techniques in genomic research was initially largely concerned with the problem of variations in gene expression ( and in fact, nearly all of the analysis of genomic data in <em>Elements of Statistical Learning</em> explicitly deals with various cases of gene expression).  On the one hand, the genomics data promises legibility of all the genes in a given organism (~20,000 for humans). On the hand, the pattern of activity of these genes in time, or any particular point in the life of an organism, cannot be read from the genome but only in time-varying expression, the changes in state and the variations in closely similar genomes. </p>
<p>Compared to the refined algorithmic craft of whole genome assembly <span class="citation">[@Venter_2001; @Myers_2000; @Pevzner_2001]</span>, the handling of the problem of gene expression in machine learning settings can seem rather crudely lacking in biological specificity. Hastie and co-authors almost deprecate scientific knowledge: ‘we will not go into the scientific background here.’ Like the authors of the original scientific study <span class="citation">[@Khan_2001]</span>, <em>Elements of Statistical Learning</em> treats gene expression profiling largely as a problem of learning to classify differences in disease or other health-related conditions. The many gene expression studies seek to discriminate between different conditions, diseases, or pathologies on the basis of differing levels of gene expression. For machine learners, each gene is a variable whose levels of expression in a given sample may help identify what type that sample belongs to. In the case of the <code>SRBCT</code> data, the types include lymphomas, sarcomas and neuroblastomas.</p>
<p>Like Chen, <em>Elements of Statistical Learning</em> begins by addressing the problem of the shape of the data. ‘Since <span class="math inline">\(p\gg N\)</span>’ write Hastie and co-authors, ‘we cannot fit a full linear discriminant analysis (LDA) to the data; some sort of regularization is needed’ <span class="citation">[@Hastie_2009, 651]</span>.   What is this ? Like the re-distribution of classification into a randomised population of machine learners (see chapter , regularization governs an potentially unruly plurality through a form of training and observation. Michel Foucault describes the advent of disciplinary power partly in terms of enclosure or individualizing observation, but also in terms of techniques of supervising, examining and above all, <em>regularizing</em> conduct. He writes:</p>
<blockquote>
<p>Shift the object and change the scale. Define new tactics in order to reach a target that is now more subtle but also more widely spread in the social body. Find new techniques for adjusting punishment to it and for adapting its effects. Lay down new principles for regularizing, refining, universalizing the art of punishing <span class="citation">[@Foucault_1977, 89]</span> </p>
</blockquote>
<p>Foucault’s description of regularization as a technique of disciplinary power – the formation that emerged in the late 18th century as a way of ordering ‘massive or transient pluralities’ (143) in Western European societies – seems a long way from microarray gene expression data.   Yet the data in genomic and other referentials (transactions, social media, etc.) displays some of the traits – massiveness, transience, plurality – that Foucault identifies as key targets of regulation for the operations of disciplinary power focused on the social body or populations.  The ‘target,’ a term often used in machine learning to describe the type, group or response being modelled, in genomics is often subtle variations (in gene expression, in phylogeny, in pathogenesis), and these variations are widely dispersed in genomic sequence data and in the populations it stems from.  Foucault’s account of supervision (<em>surveiller</em>) and penalisation as disciplinary techniques responding to ‘popular illegality’ <span class="citation">[@Foucault_1977, 130]</span> dwells on the capillary network of observations, examining, ranking, test and gradation that adapt to the surging multiplicities by ordering them in tables. While the tables of data (see Table  in the microarray gene expression datasets suggest the persistence of the same technique of ordering multiplicities through partitioned observations, the <em>cells</em> no longer target contain individuals under observation but focus on the attributes of a multiplicity in movement, the human genome for instance in its many functional states. </p>
<p>‘Shift the object and change the scale,’ Foucault writes, in describing how partitions, segmentations, forms of enclosure, and above all, ranked classifications target a more subtly distributed nexus of relations in disciplinary power. Often understood in terms of enclosure and surveillance, disciplinary power, according to Foucault, operates through ranking: ‘discipline is an art of rank, a technique for the transformation of arrangements’ <span class="citation">[@Foucault_1977, 145]</span>.  ‘Regularize in a way that automatically drops out features that are not contributing to the class predictions,’ Hastie and co-authors write <span class="citation">[@Hastie_2009, 652]</span> in describing how regularization deals with the problem of too many variables in the microarray datasets. In the many different techniques that <em>Elements of Statistical Learning</em> brings to bear on the problem of gene expression – diagonal linear discriminant analysis, nearest shrunken centroids, linear classifiers with quadratic regularization, regularized discriminant analysis, regularized multinomial logistic regression, support vector classifier – essentially the same ordering movement occurs. Regularization re-scales the excessive potential relations of the hyperobject – the patterns of expression of genes associated with different types of tumours – by shrinking or dropping the weights of parameters of each gene in the model and examining the effect on the predictions that result. The coefficients or weights of parameters in the model, the <span class="math inline">\(\beta_p\)</span> values, are ranked by importance, and then either reduced (<span class="math inline">\(L_2\)</span> regularization) or eliminated (<span class="math inline">\(L_1\)</span> regularization) if they contribute little to the predictive accuracy of the machine learner. Learning here takes the form of regularization.  </p>
<p>A technique called ‘lasso regression’ displays features that might help us grasp how machine learners regularize genomic data.  Remember that the linear regression model with its diagonal line or plane running through vector space provides the underlying intuition for many machine learners. We have seen the function in Equation  several times already in different variations, including logistic regression used for classification of types or groupings.</p>

<p>In gene expression models, the values of <span class="math inline">\(\beta\)</span> shown in equation  map on to the different levels of expression of the many genes indexed by the <span class="math inline">\(p\)</span> columns of the microarray dataset. The model tests how different patterns of gene expression associate with different tumour types. As we have already seen, the number of combinations of genes associated with different tumour types vastly outweighs the number of samples.</p>
<p>The regularized version of the linear regression framework known as ‘lasso’ – Least Absolute Shrinkage and Selection Operator – introduces a different form of training and observation of model construction. This train hinges on the lasso penalty shown in equation <a href="#fn87" class="footnoteRef" id="fnref87"><sup>87</sup></a> </p>

<blockquote>
<blockquote>
<p><span class="citation">[@Hastie_2009, 68]</span></p>
</blockquote>
</blockquote>
<p>Equation  is notable for the way that it subjects the familiar ‘residual sum of squares’ way of calculating the coefficients to the ‘penalty’ carried by the last part of the equation <span class="math inline">\(\sum\limits_i^p\vert\beta_j\vert\)</span>.  As Hastie and co-authors write, ‘the lasso does a kind of continuous subset [feature] selection’ <span class="citation">[@Hastie_2009, 69]</span>. As always <span class="math inline">\(argmin_\beta\)</span> suggests that the algorithm should optimise the set of values for <span class="math inline">\(\beta\)</span> that minimize the overall value of the function. It balances the costs of reducing the sum of residual errors shown in the first half of the equation, and minimizing the sum of the absolute values of the model parameters <span class="math inline">\(\beta_j\)</span> in the second part of the function. The optimizing double movement re-shapes its expression of the data along a diagonal line drawn as the algorithm gradually introduces and scales all of the features in the vector space <span class="math inline">\(\mathbf{X}\)</span>, only allowing those variables or features to remain in the set that help minimize the difference between the predicted response and the actual response.  (Figure  makes something of this scaling diagrammatically visible. In this diagram, the various diagonal lines show how values of coefficients grow and sometimes diminish as the <code>lasso</code> process runs. Vertical lines show steps as new variables are included in the model with different values of the control parameter <span class="math inline">\(\lambda\)</span>. )</p>

<p>Regularization sometimes radically changes the object. In Figure , these changes become a matter of diagrammatic observation. Comparing eight different methods for analyzing the microarray cancer data from <span class="citation">[@Ramaswamy_2001]</span>, Hastie reports that ‘lasso regression (one versus all)’ selects 1,429 of the 16,063 genes in the dataset. The shifted-rescaled object, a set of 1400 genes, or in the case of the ‘elastic-net penalized multinomial’ model that uses only 384 genes, highlights a drastically reduced subset of the original object. A regularized genome of 384 genes suggest a much more targeted set of interventions than 16,000.</p>
</div>
<div class="footnotes">
<hr />
<ol start="87">
<li id="fn87"><p>The original publication of the lasso technique in a paper entitled ‘Shrinkage and Selection via the Lasso’ <span class="citation">[@Tibshirani_1996]</span> has been heavily cited in subsequent literature. <a href="http://scholar.google.co.uk/scholar?hl=en&amp;q=tibshirani+1996+lasso">Google Scholar</a> counts around 13,000 citations. For a paper published in the <em>Journal of the Royal Statistical Society</em>, this is surprisingly high, but attests, I would suggest, to the intense interest in renovating linear models for new problems such as image recognition or tumour classification. Somewhat surprisingly, given its heavy usage in other scientists, Andrew Ng’s CS229 machine learning course at Stanford University doesn’t mention the lasso.<a href="65-cross-validating-machine-learning-in-genomics.html#fnref87">↩</a></p></li>
</ol>
</div>
<p style="text-align: center;">
<a href="64-the-advent-of-wide-dirty-and-mixed-data.html"><button class="btn btn-default">Previous</button></a>
<a href="66-proliferation-of-discoveries.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
