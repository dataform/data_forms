<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta property="og:title" content="" />
<meta property="og:type" content="book" />







<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="">

<title></title>

<script src="libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="libs/navigation-1.1/tabsets.js"></script>





<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
</style>
</head>

<body>

<div class="container-fluid main-container">


<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="1-acknowledgments.html#acknowledgments"><span class="toc-section-number">1</span> Acknowledgments</a></li>
<li><a href="2-machine-learners.html#machine-learners"><span class="toc-section-number">2</span> 250,000 machine learners</a></li>
<li><a href="3-a-summary-of-the-argument.html#a-summary-of-the-argument"><span class="toc-section-number">3</span> A summary of the argument</a></li>
<li><a href="4-in-situ-hybridization.html#in-situ-hybridization"><span class="toc-section-number">4</span> In-situ hybridization</a></li>
<li><a href="5-critical-operational-practice.html#critical-operational-practice"><span class="toc-section-number">5</span> Critical operational practice?</a></li>
<li><a href="6-obstacles-to-the-work-of-freeing-machine-learning.html#obstacles-to-the-work-of-freeing-machine-learning"><span class="toc-section-number">6</span> Obstacles to the work of freeing machine learning</a></li>
<li><a href="7-preface.html#preface"><span class="toc-section-number">7</span> Preface</a></li>
<li><a href="8-introduction-into-the-data.html#introduction-into-the-data"><span class="toc-section-number">8</span> Introduction: Into the Data</a></li>
<li><a href="9-three-accumulations-settings-data-and-devices.html#three-accumulations-settings-data-and-devices"><span class="toc-section-number">9</span> Three accumulations: settings, data and devices</a></li>
<li><a href="10-who-or-what-is-a-machine-learner.html#who-or-what-is-a-machine-learner"><span class="toc-section-number">10</span> Who or what is a machine learner?</a></li>
<li><a href="11-algorithmic-control-to-the-machine-learners.html#algorithmic-control-to-the-machine-learners"><span class="toc-section-number">11</span> Algorithmic control to the machine learners?</a></li>
<li><a href="12-the-archaeology-of-operations.html#the-archaeology-of-operations"><span class="toc-section-number">12</span> The archaeology of operations</a></li>
<li><a href="13-asymmetries-in-common-knowledge.html#asymmetries-in-common-knowledge"><span class="toc-section-number">13</span> Asymmetries in common knowledge</a></li>
<li><a href="14-what-cannot-be-automated.html#what-cannot-be-automated"><span class="toc-section-number">14</span> What cannot be automated?</a></li>
<li><a href="15-different-fields-in-machine-learning.html#different-fields-in-machine-learning"><span class="toc-section-number">15</span> Different fields in machine learning?</a></li>
<li><a href="16-the-diagram-in-critical-thought.html#the-diagram-in-critical-thought"><span class="toc-section-number">16</span> The diagram in critical thought</a></li>
<li><a href="17-we-dont-have-to-write-programs.html#we-dont-have-to-write-programs"><span class="toc-section-number">17</span> ‘We don’t have to write programs’?</a></li>
<li><a href="18-the-elements-of-machine-learning.html#the-elements-of-machine-learning"><span class="toc-section-number">18</span> The elements of machine learning</a></li>
<li><a href="19-who-reads-machine-learning-textbooks.html#who-reads-machine-learning-textbooks"><span class="toc-section-number">19</span> Who reads machine learning textbooks?</a></li>
<li><a href="20-r-a-matrix-of-transformations.html#r-a-matrix-of-transformations"><span class="toc-section-number">20</span> <code>R</code>: a matrix of transformations</a></li>
<li><a href="21-the-obdurate-mathematical-glint-of-machine-learning.html#the-obdurate-mathematical-glint-of-machine-learning"><span class="toc-section-number">21</span> The obdurate mathematical glint of machine learning</a></li>
<li><a href="22-cs229-2007-returning-again-and-again-to-certain-features.html#cs229-2007-returning-again-and-again-to-certain-features"><span class="toc-section-number">22</span> CS229, 2007: returning again and again to certain features</a></li>
<li><a href="23-the-visible-learning-of-machine-learning.html#the-visible-learning-of-machine-learning"><span class="toc-section-number">23</span> The visible learning of machine learning</a></li>
<li><a href="24-the-diagram-of-an-operational-formation.html#the-diagram-of-an-operational-formation"><span class="toc-section-number">24</span> The diagram of an operational formation</a></li>
<li><a href="25-vector-space-and-geometry.html#vector-space-and-geometry"><span class="toc-section-number">25</span> Vector space and geometry</a></li>
<li><a href="26-mixing-places.html#mixing-places"><span class="toc-section-number">26</span> Mixing places</a></li>
<li><a href="27-truth-is-no-longer-in-the-table.html#truth-is-no-longer-in-the-table"><span class="toc-section-number">27</span> Truth is no longer in the table?</a></li>
<li><a href="28-the-epistopic-fault-line-in-tables.html#the-epistopic-fault-line-in-tables"><span class="toc-section-number">28</span> The epistopic fault line in tables</a></li>
<li><a href="29-surface-and-depths-the-problem-of-volume-in-data.html#surface-and-depths-the-problem-of-volume-in-data"><span class="toc-section-number">29</span> Surface and depths: the problem of volume in data</a></li>
<li><a href="30-vector-space-expansion.html#vector-space-expansion"><span class="toc-section-number">30</span> Vector space expansion</a></li>
<li><a href="31-drawing-lines-in-a-common-space-of-transformation.html#drawing-lines-in-a-common-space-of-transformation"><span class="toc-section-number">31</span> Drawing lines in a common space of transformation</a></li>
<li><a href="32-implicit-vectorization-in-code-and-infrastructures.html#implicit-vectorization-in-code-and-infrastructures"><span class="toc-section-number">32</span> Implicit vectorization in code and infrastructures</a></li>
<li><a href="33-lines-traversing-behind-the-light.html#lines-traversing-behind-the-light"><span class="toc-section-number">33</span> Lines traversing behind the light</a></li>
<li><a href="34-the-vectorised-table.html#the-vectorised-table"><span class="toc-section-number">34</span> The vectorised table?</a></li>
<li><a href="35-learning-functions.html#learning-functions"><span class="toc-section-number">35</span> Learning functions</a></li>
<li><a href="36-supervised-unsupervised-reinforcement-learning-and-functions.html#supervised-unsupervised-reinforcement-learning-and-functions"><span class="toc-section-number">36</span> Supervised, unsupervised, reinforcement learning and functions</a></li>
<li><a href="37-which-function-operates.html#which-function-operates"><span class="toc-section-number">37</span> Which function operates?</a></li>
<li><a href="38-what-does-a-function-learn.html#what-does-a-function-learn"><span class="toc-section-number">38</span> What does a function learn?</a></li>
<li><a href="39-observing-with-curves-the-logistic-function.html#observing-with-curves-the-logistic-function"><span class="toc-section-number">39</span> Observing with curves: the logistic function</a></li>
<li><a href="40-the-cost-of-curves-in-machine-learning.html#the-cost-of-curves-in-machine-learning"><span class="toc-section-number">40</span> The cost of curves in machine learning</a></li>
<li><a href="41-curves-and-the-variation-in-models.html#curves-and-the-variation-in-models"><span class="toc-section-number">41</span> Curves and the variation in models</a></li>
<li><a href="42-observing-costs-losses-and-objectives-through-optimisation.html#observing-costs-losses-and-objectives-through-optimisation"><span class="toc-section-number">42</span> Observing costs, losses and objectives through optimisation</a></li>
<li><a href="43-gradients-as-partial-observers.html#gradients-as-partial-observers"><span class="toc-section-number">43</span> Gradients as partial observers</a></li>
<li><a href="44-the-power-to-learn.html#the-power-to-learn"><span class="toc-section-number">44</span> The power to learn</a></li>
<li><a href="45-data-reduces-uncertainty.html#data-reduces-uncertainty"><span class="toc-section-number">45</span> Data reduces uncertainty?</a></li>
<li><a href="46-machine-learning-as-statistics-inside-out.html#machine-learning-as-statistics-inside-out"><span class="toc-section-number">46</span> Machine learning as statistics inside out</a></li>
<li><a href="47-distributed-probabilities.html#distributed-probabilities"><span class="toc-section-number">47</span> Distributed probabilities</a></li>
<li><a href="48-naive-bayes-and-the-distribution-of-probabilities.html#naive-bayes-and-the-distribution-of-probabilities"><span class="toc-section-number">48</span> Naive Bayes and the distribution of probabilities</a></li>
<li><a href="49-spam-when-foralln-is-too-much.html#spam-when-foralln-is-too-much"><span class="toc-section-number">49</span> Spam: when <span class="math inline">\(\forall{N}\)</span> is too much?</a></li>
<li><a href="50-the-improbable-success-of-the-naive-bayes-classifier.html#the-improbable-success-of-the-naive-bayes-classifier"><span class="toc-section-number">50</span> The improbable success of the Naive Bayes classifier</a></li>
<li><a href="51-ancestral-probabilities-in-documents-inference-and-prediction.html#ancestral-probabilities-in-documents-inference-and-prediction"><span class="toc-section-number">51</span> Ancestral probabilities in documents: inference and prediction</a></li>
<li><a href="52-statistical-decompositions-bias-variance-and-observed-errors.html#statistical-decompositions-bias-variance-and-observed-errors"><span class="toc-section-number">52</span> Statistical decompositions: bias, variance and observed errors</a></li>
<li><a href="53-does-machine-learning-construct-a-new-statistical-reality.html#does-machine-learning-construct-a-new-statistical-reality"><span class="toc-section-number">53</span> Does machine learning construct a new statistical reality?</a></li>
<li><a href="54-splitting-and-the-growth-of-trees.html#splitting-and-the-growth-of-trees"><span class="toc-section-number">54</span> Splitting and the growth of trees</a></li>
<li><a href="55-differences-in-recursive-partitioning.html#differences-in-recursive-partitioning"><span class="toc-section-number">55</span> 1984: Differences in recursive partitioning</a></li>
<li><a href="56-limiting-differences.html#limiting-differences"><span class="toc-section-number">56</span> Limiting differences</a></li>
<li><a href="57-the-successful-dispersion-of-the-support-vector-machine.html#the-successful-dispersion-of-the-support-vector-machine"><span class="toc-section-number">57</span> The successful dispersion of the support vector machine</a></li>
<li><a href="58-differences-blur.html#differences-blur"><span class="toc-section-number">58</span> Differences blur?</a></li>
<li><a href="59-bending-the-decision-boundary.html#bending-the-decision-boundary"><span class="toc-section-number">59</span> Bending the decision boundary</a></li>
<li><a href="60-instituting-patterns.html#instituting-patterns"><span class="toc-section-number">60</span> Instituting patterns</a></li>
<li><a href="61-genomic-referentiality-and-materiality.html#genomic-referentiality-and-materiality"><span class="toc-section-number">61</span> Genomic referentiality and materiality</a></li>
<li><a href="62-the-genome-as-threshold-object.html#the-genome-as-threshold-object"><span class="toc-section-number">62</span> The genome as threshold object</a></li>
<li><a href="63-genomic-knowledges-and-their-datasets.html#genomic-knowledges-and-their-datasets"><span class="toc-section-number">63</span> Genomic knowledges and their datasets</a></li>
<li><a href="64-the-advent-of-wide-dirty-and-mixed-data.html#the-advent-of-wide-dirty-and-mixed-data"><span class="toc-section-number">64</span> The advent of ‘wide, dirty and mixed’ data</a></li>
<li><a href="65-cross-validating-machine-learning-in-genomics.html#cross-validating-machine-learning-in-genomics"><span class="toc-section-number">65</span> Cross-validating machine learning in genomics</a></li>
<li><a href="66-proliferation-of-discoveries.html#proliferation-of-discoveries"><span class="toc-section-number">66</span> Proliferation of discoveries</a></li>
<li><a href="67-variations-in-the-object-or-in-the-machine-learner.html#variations-in-the-object-or-in-the-machine-learner"><span class="toc-section-number">67</span> Variations in the object or in the machine learner?</a></li>
<li><a href="68-whole-genome-functions.html#whole-genome-functions"><span class="toc-section-number">68</span> Whole genome functions</a></li>
<li><a href="69-propagation-across-human-machine-boundaries.html#propagation-across-human-machine-boundaries"><span class="toc-section-number">69</span> Propagation across human-machine boundaries</a></li>
<li><a href="70-competitive-positioning.html#competitive-positioning"><span class="toc-section-number">70</span> Competitive positioning</a></li>
<li><a href="71-a-privileged-machine-and-its-diagrammatic-forms.html#a-privileged-machine-and-its-diagrammatic-forms"><span class="toc-section-number">71</span> A privileged machine and its diagrammatic forms</a></li>
<li><a href="72-varying-subject-positions-in-code.html#varying-subject-positions-in-code"><span class="toc-section-number">72</span> Varying subject positions in code</a></li>
<li><a href="73-the-subjects-of-a-hidden-operation.html#the-subjects-of-a-hidden-operation"><span class="toc-section-number">73</span> The subjects of a hidden operation</a></li>
<li><a href="74-algorithms-that-propagate-errors.html#algorithms-that-propagate-errors"><span class="toc-section-number">74</span> Algorithms that propagate errors</a></li>
<li><a href="75-competitions-as-examination.html#competitions-as-examination"><span class="toc-section-number">75</span> Competitions as examination</a></li>
<li><a href="76-superimposing-power-and-knowledge.html#superimposing-power-and-knowledge"><span class="toc-section-number">76</span> Superimposing power and knowledge</a></li>
<li><a href="77-ranked-subject-positions.html#ranked-subject-positions"><span class="toc-section-number">77</span> Ranked subject positions</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="distributed-probabilities" class="section level1">
<h1><span class="header-section-number">47</span> Distributed probabilities</h1>
<p>While these structuring differences deeply shape practice in machine learning, the underlying operator that allows swapping between knowledge and the world, between events and devices, is probability, and in particular, functions that describe variations in populations, probability distributions.  Probability distributions both map population variations and, as we will see, multiply the number of things that count as populations.  </p>
<p>The normal distribution pervades nineteenth century statistical thinking as it affects populations across law, medicine, agriculture, finance and not least, sociology as a domain of knowledge. Normal distributions appear in countless variations in scientific, government and institutional settings as functions that map events, measurements, observations and records to evidential probability quantities.<a href="#fn50" class="footnoteRef" id="fnref50"><sup>50</sup></a></p>

<p>The function shown in equation () expresses the probability of a given value of the variable <span class="math inline">\(x\)</span> given a population whose variations (with respect to <span class="math inline">\(x\)</span>) can be expressed in terms of two parameters, <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>, the mean and variance. This is the so-called normal or Gaussian distribution.<a href="#fn51" class="footnoteRef" id="fnref51"><sup>51</sup></a> Its mathematics were intensively worked over during the late eighteenth and early nineteenth centuries in what has been termed ‘one of the major success stories in the history of science’ <span class="citation">[@Stigler_1986, 158]</span>. It has a power-laden biopolitical history closely tied with knowledges and governing of populations in terms of morality, mortality, health, and wealth (see <span class="citation">[@Hacking_1975, 113-124]</span>.  The key parameters here include <span class="math inline">\(\mu\)</span>, the mean and <span class="math inline">\(\sigma\)</span>, the variance, a number that describes the dispersion of values of the variable, <span class="math inline">\(x\)</span> are. These two parameters together describe the shape of the curve. Given knowledge of <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>, the normal or Gaussian probability distribution maps all outcomes to probabilities (or numbers in the range <span class="math inline">\(0\)</span> to <span class="math inline">\(1\)</span>). Put statistically, functions such as the Gaussian distribution probabilise events as random variables. Every variable potentially becomes a function: ‘a random variable is a mapping that assigns a real number to each outcome’ <span class="citation">[@Wasserman_2003,19]</span>.  </p>
<p>The possibility of treating population variations as random variables, that is, as probability distributions, was a significant historical achievement, one that continues to develop and ramify.<a href="#fn52" class="footnoteRef" id="fnref52"><sup>52</sup></a> Random variables distribute probability in the world. When conceptualised as real quantities in the world rather than epiphenomenal by-products of inaccuracies in our observations or measuring devices, probability distributions weave directly into the productive operations of power.  Distribution in the sense of locating, positioning, partitioning, sectioning, serialising or queuing operations has received much more attention in critical thought (particularly in the many uses of Foucault’s concept of disciplinary power <span class="citation">[@Foucault_1977]</span>), but in almost every setting, distribution in the sense of counting, apportioning and weighting of different outcomes also operates. This constant interweaving of spatial, architectural, logistical and functional processes has energised statistical thought for several centuries.<a href="#fn53" class="footnoteRef" id="fnref53"><sup>53</sup></a>  For instance, given the normal distribution, it is possible, under certain circumstances, to effectively subjectify someone on the spot. If an educational psychologist indicates to someone that their intelligence lies towards the left-hand side of the normal curve peak (and hence less than the population mean), they quickly assign them to a potentially institutionally and economically consequential trajectory. Since its inception in the social physics of Adolphe Quetelet as a way of referring to a property of populations, the normal curve has not only described but modulated and re-shaped populations (in terms of health, morality and wealth). </p>
<p>If functions such as equation () have persisted for so long as elements of population governmentality or biopolitics,  what happen to them in machine learning? The pages of a book such as <em>Elements of Statistical Learning</em> show many signs of an ongoing invocation of probability distributions. We could simply observe their abundance. Hastie and co-authors invoke probability distributions. They speak of ‘Gaussian mixtures,’ ‘bivariate Gaussian distributions,’ standard Gaussian,‘’Gaussian kernels,’ ‘Gaussian assumptions,’ ‘Gaussian errors,’ ‘Gaussian noise,’ ‘Gaussian radial basis function,’ ‘Gaussian variables,’ ‘Gaussian densities,’ ‘Gaussian process,’ and so forth. (The term ‘normal’ appears in an even wider spectrum of similar guises.) Events, things, properties, operations, functions, and attributes all associate with probability distributions. </p>
<p>The multiple invocations of probability distributions attests to the variety of events (occurrence of cancer, occurrence of the word ‘Viagra’ in an email, a click on a hyperlink, etc.) map to real numbers. Despite the sometimes dense mathematical diagrammaticism, the term <em>distribution</em> emphasises a tangible and practically resonant way of thinking about how events or possible outcomes shift about as the parameters of a function vary.<a href="#fn54" class="footnoteRef" id="fnref54"><sup>54</sup></a> Whatever inferences and predictions become possible, probability distributions are a crucial control surface for machine learning understood as a form of movement through data.  In contrast to the endowment of living aggregates such as populations with probability that we see in the biopolitical history of statistics (and later in natural sciences such as physics and biology), statistical machine learning increasingly constitutes devices as populations via probability distributions. </p>
</div>
<div class="footnotes">
<hr />
<ol start="50">
<li id="fn50"><p>Statistical graphics have a rich history and semiology that I do not discuss here (see <span class="citation">[@Bertin_1983]</span>).<a href="47-distributed-probabilities.html#fnref50">↩</a></p></li>
<li id="fn51"><p>Dozens of differently shaped probability distributions map continuous and discrete variations to real numbers. Other probability distributions — normal (Gaussian), uniform, Cauchy exponential, gamma, beta, hypergeometric, binomial, Poisson, chi-squared, Dirichlet, Boltzmann-Gibbs distributions, etc. (see <span class="citation">[@NIST_2012]</span> for a gallery of distributions) — functionally express widely differing patterns.  The queuing times at airport check-ins do not, for instance, easily fit a normal distribution. Statisticians model queues using a Poisson distribution, in which, unfortunately for travellers, distributes the number of events in a given time interval quite broadly. Similarly, it might be better to think of the probability of rain today in north-west England in terms of a Poisson distribution that models clouds in the Atlantic queuing to rain on the northwest coast of England. (Rather than addressing the question of whether it will rain or not, a Poisson-based model might address the question of how many times it will rain today.)<a href="47-distributed-probabilities.html#fnref51">↩</a></p></li>
<li id="fn52"><p>The mapping that assigns numbers to outcomes (heads v. tails; cancer v. benign; spam v. not-spam) is a probability distribution. As I have argued in <span class="citation">[@Mackenzie_2015d]</span>, random variables have become much more widespread in statistical practice due to changes in computational techniques. <a href="47-distributed-probabilities.html#fnref52">↩</a></p></li>
<li id="fn53"><p>‘Distribution’ pervades Foucault’s account of power and knowledge from <em>The Order of Things</em> <span class="citation">[@Foucault_1992]</span> onwards.  Foucault treats distributions in several different ways: as spatial or logistical techniques, as mathematical orderings of large numbers of people or things, and as a methodological and theoretical framing device. In <em>Discipline and Punish</em> <span class="citation">[@Foucault_1977]</span>, the spatial sense prevails, but in later works, the population or demographic sense of distribution takes precedence <span class="citation">[@Foucault_1998]</span>. Distribution certainly has theoretical primacy in his account of power: ‘relations of power-knowledge are not static forms of distribution, they are “matrices of transformations”’ <span class="citation">[@Foucault_1998, 99]</span>.<a href="47-distributed-probabilities.html#fnref53">↩</a></p></li>
<li id="fn54"><p>Machine learners adjust these parameters in different ways. For instance, parametric and non-parametric models (see table ) differ in that the former have a limited number of parameters and the latter an undefined number of parameters (for instance, Naive Bayes, <em>k</em> nearest neighbours or support vector machine models). But both kinds assume that an underlying probability distribution – a function, ‘unobservable’ or not – operates, even if it changes with new data. A probability distribution under these assumptions becomes the closest reality we have to whatever process generated all the variations in data gathered through experiments and observations. From a probabilistic perspective, the task of machine learning is to estimate the parameters (the mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma\)</span> in the case of Gaussian curve) that shape of the curve of the probability distribution.<a href="47-distributed-probabilities.html#fnref54">↩</a></p></li>
</ol>
</div>
<p style="text-align: center;">
<a href="46-machine-learning-as-statistics-inside-out.html"><button class="btn btn-default">Previous</button></a>
<a href="48-naive-bayes-and-the-distribution-of-probabilities.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
