<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title></title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="">
  <meta name="generator" content="bookdown 0.3 and GitBook 2.6.7">

  <meta property="og:title" content="" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="machine-learning-as-statistics-inside-out.html">
<link rel="next" href="naive-bayes-and-the-distribution-of-probabilities.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i><b>1</b> Acknowledgments</a></li>
<li class="chapter" data-level="2" data-path="conclusion-out-of-the-data.html"><a href="conclusion-out-of-the-data.html"><i class="fa fa-check"></i><b>2</b> Conclusion: Out of the Data}</a></li>
<li class="chapter" data-level="3" data-path="machine-learners.html"><a href="machine-learners.html"><i class="fa fa-check"></i><b>3</b> 250,000 machine learners</a></li>
<li class="chapter" data-level="4" data-path="a-summary-of-the-argument.html"><a href="a-summary-of-the-argument.html"><i class="fa fa-check"></i><b>4</b> A summary of the argument</a></li>
<li class="chapter" data-level="5" data-path="in-situ-hybridization.html"><a href="in-situ-hybridization.html"><i class="fa fa-check"></i><b>5</b> In-situ hybridization</a></li>
<li class="chapter" data-level="6" data-path="critical-operational-practice.html"><a href="critical-operational-practice.html"><i class="fa fa-check"></i><b>6</b> Critical operational practice?</a></li>
<li class="chapter" data-level="7" data-path="obstacles-to-the-work-of-freeing-machine-learning.html"><a href="obstacles-to-the-work-of-freeing-machine-learning.html"><i class="fa fa-check"></i><b>7</b> Obstacles to the work of freeing machine learning</a></li>
<li class="chapter" data-level="8" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i><b>8</b> Preface</a></li>
<li class="chapter" data-level="9" data-path="introduction-into-the-data.html"><a href="introduction-into-the-data.html"><i class="fa fa-check"></i><b>9</b> Introduction: Into the Data</a></li>
<li class="chapter" data-level="10" data-path="three-accumulations-settings-data-and-devices.html"><a href="three-accumulations-settings-data-and-devices.html"><i class="fa fa-check"></i><b>10</b> Three accumulations: settings, data and devices</a></li>
<li class="chapter" data-level="11" data-path="who-or-what-is-a-machine-learner.html"><a href="who-or-what-is-a-machine-learner.html"><i class="fa fa-check"></i><b>11</b> Who or what is a machine learner?</a></li>
<li class="chapter" data-level="12" data-path="algorithmic-control-to-the-machine-learners.html"><a href="algorithmic-control-to-the-machine-learners.html"><i class="fa fa-check"></i><b>12</b> Algorithmic control to the machine learners?</a></li>
<li class="chapter" data-level="13" data-path="the-archaeology-of-operations.html"><a href="the-archaeology-of-operations.html"><i class="fa fa-check"></i><b>13</b> The archaeology of operations</a></li>
<li class="chapter" data-level="14" data-path="asymmetries-in-common-knowledge.html"><a href="asymmetries-in-common-knowledge.html"><i class="fa fa-check"></i><b>14</b> Asymmetries in common knowledge</a></li>
<li class="chapter" data-level="15" data-path="what-cannot-be-automated.html"><a href="what-cannot-be-automated.html"><i class="fa fa-check"></i><b>15</b> What cannot be automated?</a></li>
<li class="chapter" data-level="16" data-path="different-fields-in-machine-learning.html"><a href="different-fields-in-machine-learning.html"><i class="fa fa-check"></i><b>16</b> Different fields in machine learning?</a></li>
<li class="chapter" data-level="17" data-path="the-diagram-in-critical-thought.html"><a href="the-diagram-in-critical-thought.html"><i class="fa fa-check"></i><b>17</b> The diagram in critical thought</a></li>
<li class="chapter" data-level="18" data-path="diagramming-machines.html"><a href="diagramming-machines.html"><i class="fa fa-check"></i><b>18</b> Diagramming machines}</a></li>
<li class="chapter" data-level="19" data-path="we-dont-have-to-write-programs.html"><a href="we-dont-have-to-write-programs.html"><i class="fa fa-check"></i><b>19</b> ‘We don’t have to write programs’?</a></li>
<li class="chapter" data-level="20" data-path="the-elements-of-machine-learning.html"><a href="the-elements-of-machine-learning.html"><i class="fa fa-check"></i><b>20</b> The elements of machine learning</a></li>
<li class="chapter" data-level="21" data-path="who-reads-machine-learning-textbooks.html"><a href="who-reads-machine-learning-textbooks.html"><i class="fa fa-check"></i><b>21</b> Who reads machine learning textbooks?</a></li>
<li class="chapter" data-level="22" data-path="r-a-matrix-of-transformations.html"><a href="r-a-matrix-of-transformations.html"><i class="fa fa-check"></i><b>22</b> <code>R</code>: a matrix of transformations</a></li>
<li class="chapter" data-level="23" data-path="the-obdurate-mathematical-glint-of-machine-learning.html"><a href="the-obdurate-mathematical-glint-of-machine-learning.html"><i class="fa fa-check"></i><b>23</b> The obdurate mathematical glint of machine learning</a></li>
<li class="chapter" data-level="24" data-path="cs229-2007-returning-again-and-again-to-certain-features.html"><a href="cs229-2007-returning-again-and-again-to-certain-features.html"><i class="fa fa-check"></i><b>24</b> CS229, 2007: returning again and again to certain features</a></li>
<li class="chapter" data-level="25" data-path="the-visible-learning-of-machine-learning.html"><a href="the-visible-learning-of-machine-learning.html"><i class="fa fa-check"></i><b>25</b> The visible learning of machine learning</a></li>
<li class="chapter" data-level="26" data-path="the-diagram-of-an-operational-formation.html"><a href="the-diagram-of-an-operational-formation.html"><i class="fa fa-check"></i><b>26</b> The diagram of an operational formation</a></li>
<li class="chapter" data-level="27" data-path="vectorisation-and-its-consequences.html"><a href="vectorisation-and-its-consequences.html"><i class="fa fa-check"></i><b>27</b> Vectorisation and its consequences}</a></li>
<li class="chapter" data-level="28" data-path="vector-space-and-geometry.html"><a href="vector-space-and-geometry.html"><i class="fa fa-check"></i><b>28</b> Vector space and geometry</a></li>
<li class="chapter" data-level="29" data-path="mixing-places.html"><a href="mixing-places.html"><i class="fa fa-check"></i><b>29</b> Mixing places</a></li>
<li class="chapter" data-level="30" data-path="truth-is-no-longer-in-the-table.html"><a href="truth-is-no-longer-in-the-table.html"><i class="fa fa-check"></i><b>30</b> Truth is no longer in the table?</a></li>
<li class="chapter" data-level="31" data-path="the-epistopic-fault-line-in-tables.html"><a href="the-epistopic-fault-line-in-tables.html"><i class="fa fa-check"></i><b>31</b> The epistopic fault line in tables</a></li>
<li class="chapter" data-level="32" data-path="surface-and-depths-the-problem-of-volume-in-data.html"><a href="surface-and-depths-the-problem-of-volume-in-data.html"><i class="fa fa-check"></i><b>32</b> Surface and depths: the problem of volume in data</a></li>
<li class="chapter" data-level="33" data-path="vector-space-expansion.html"><a href="vector-space-expansion.html"><i class="fa fa-check"></i><b>33</b> Vector space expansion</a></li>
<li class="chapter" data-level="34" data-path="drawing-lines-in-a-common-space-of-transformation.html"><a href="drawing-lines-in-a-common-space-of-transformation.html"><i class="fa fa-check"></i><b>34</b> Drawing lines in a common space of transformation</a></li>
<li class="chapter" data-level="35" data-path="implicit-vectorization-in-code-and-infrastructures.html"><a href="implicit-vectorization-in-code-and-infrastructures.html"><i class="fa fa-check"></i><b>35</b> Implicit vectorization in code and infrastructures</a></li>
<li class="chapter" data-level="36" data-path="lines-traversing-behind-the-light.html"><a href="lines-traversing-behind-the-light.html"><i class="fa fa-check"></i><b>36</b> Lines traversing behind the light</a></li>
<li class="chapter" data-level="37" data-path="the-vectorised-table.html"><a href="the-vectorised-table.html"><i class="fa fa-check"></i><b>37</b> The vectorised table?</a></li>
<li class="chapter" data-level="38" data-path="machines-finding-functions.html"><a href="machines-finding-functions.html"><i class="fa fa-check"></i><b>38</b> Machines finding functions}</a></li>
<li class="chapter" data-level="39" data-path="learning-functions.html"><a href="learning-functions.html"><i class="fa fa-check"></i><b>39</b> Learning functions</a></li>
<li class="chapter" data-level="40" data-path="supervised-unsupervised-reinforcement-learning-and-functions.html"><a href="supervised-unsupervised-reinforcement-learning-and-functions.html"><i class="fa fa-check"></i><b>40</b> Supervised, unsupervised, reinforcement learning and functions</a></li>
<li class="chapter" data-level="41" data-path="which-function-operates.html"><a href="which-function-operates.html"><i class="fa fa-check"></i><b>41</b> Which function operates?</a></li>
<li class="chapter" data-level="42" data-path="what-does-a-function-learn.html"><a href="what-does-a-function-learn.html"><i class="fa fa-check"></i><b>42</b> What does a function learn?</a></li>
<li class="chapter" data-level="43" data-path="observing-with-curves-the-logistic-function.html"><a href="observing-with-curves-the-logistic-function.html"><i class="fa fa-check"></i><b>43</b> Observing with curves: the logistic function</a></li>
<li class="chapter" data-level="44" data-path="the-cost-of-curves-in-machine-learning.html"><a href="the-cost-of-curves-in-machine-learning.html"><i class="fa fa-check"></i><b>44</b> The cost of curves in machine learning</a></li>
<li class="chapter" data-level="45" data-path="curves-and-the-variation-in-models.html"><a href="curves-and-the-variation-in-models.html"><i class="fa fa-check"></i><b>45</b> Curves and the variation in models</a></li>
<li class="chapter" data-level="46" data-path="observing-costs-losses-and-objectives-through-optimisation.html"><a href="observing-costs-losses-and-objectives-through-optimisation.html"><i class="fa fa-check"></i><b>46</b> Observing costs, losses and objectives through optimisation</a></li>
<li class="chapter" data-level="47" data-path="gradients-as-partial-observers.html"><a href="gradients-as-partial-observers.html"><i class="fa fa-check"></i><b>47</b> Gradients as partial observers</a></li>
<li class="chapter" data-level="48" data-path="the-power-to-learn.html"><a href="the-power-to-learn.html"><i class="fa fa-check"></i><b>48</b> The power to learn</a></li>
<li class="chapter" data-level="49" data-path="n-forallboldsymbolx-probabilisation-and-the-taming-of-machines.html"><a href="n-forallboldsymbolx-probabilisation-and-the-taming-of-machines.html"><i class="fa fa-check"></i><b>49</b> <span class="math inline">\(N = \forall\boldsymbol{X}\)</span> Probabilisation and the Taming of Machines}</a></li>
<li class="chapter" data-level="50" data-path="data-reduces-uncertainty.html"><a href="data-reduces-uncertainty.html"><i class="fa fa-check"></i><b>50</b> Data reduces uncertainty?</a></li>
<li class="chapter" data-level="51" data-path="machine-learning-as-statistics-inside-out.html"><a href="machine-learning-as-statistics-inside-out.html"><i class="fa fa-check"></i><b>51</b> Machine learning as statistics inside out</a></li>
<li class="chapter" data-level="52" data-path="distributed-probabilities.html"><a href="distributed-probabilities.html"><i class="fa fa-check"></i><b>52</b> Distributed probabilities</a></li>
<li class="chapter" data-level="53" data-path="naive-bayes-and-the-distribution-of-probabilities.html"><a href="naive-bayes-and-the-distribution-of-probabilities.html"><i class="fa fa-check"></i><b>53</b> Naive Bayes and the distribution of probabilities</a></li>
<li class="chapter" data-level="54" data-path="spam-when-foralln-is-too-much.html"><a href="spam-when-foralln-is-too-much.html"><i class="fa fa-check"></i><b>54</b> Spam: when <span class="math inline">\(\forall{N}\)</span> is too much?</a></li>
<li class="chapter" data-level="55" data-path="the-improbable-success-of-the-naive-bayes-classifier.html"><a href="the-improbable-success-of-the-naive-bayes-classifier.html"><i class="fa fa-check"></i><b>55</b> The improbable success of the Naive Bayes classifier</a></li>
<li class="chapter" data-level="56" data-path="ancestral-probabilities-in-documents-inference-and-prediction.html"><a href="ancestral-probabilities-in-documents-inference-and-prediction.html"><i class="fa fa-check"></i><b>56</b> Ancestral probabilities in documents: inference and prediction</a></li>
<li class="chapter" data-level="57" data-path="statistical-decompositions-bias-variance-and-observed-errors.html"><a href="statistical-decompositions-bias-variance-and-observed-errors.html"><i class="fa fa-check"></i><b>57</b> Statistical decompositions: bias, variance and observed errors</a></li>
<li class="chapter" data-level="58" data-path="does-machine-learning-construct-a-new-statistical-reality.html"><a href="does-machine-learning-construct-a-new-statistical-reality.html"><i class="fa fa-check"></i><b>58</b> Does machine learning construct a new statistical reality?</a></li>
<li class="chapter" data-level="59" data-path="patterns-and-differences.html"><a href="patterns-and-differences.html"><i class="fa fa-check"></i><b>59</b> Patterns and differences</a></li>
<li class="chapter" data-level="60" data-path="splitting-and-the-growth-of-trees.html"><a href="splitting-and-the-growth-of-trees.html"><i class="fa fa-check"></i><b>60</b> Splitting and the growth of trees</a></li>
<li class="chapter" data-level="61" data-path="differences-in-recursive-partitioning.html"><a href="differences-in-recursive-partitioning.html"><i class="fa fa-check"></i><b>61</b> 1984: Differences in recursive partitioning</a></li>
<li class="chapter" data-level="62" data-path="limiting-differences.html"><a href="limiting-differences.html"><i class="fa fa-check"></i><b>62</b> Limiting differences</a></li>
<li class="chapter" data-level="63" data-path="the-successful-dispersion-of-the-support-vector-machine.html"><a href="the-successful-dispersion-of-the-support-vector-machine.html"><i class="fa fa-check"></i><b>63</b> The successful dispersion of the support vector machine</a></li>
<li class="chapter" data-level="64" data-path="differences-blur.html"><a href="differences-blur.html"><i class="fa fa-check"></i><b>64</b> Differences blur?</a></li>
<li class="chapter" data-level="65" data-path="bending-the-decision-boundary.html"><a href="bending-the-decision-boundary.html"><i class="fa fa-check"></i><b>65</b> Bending the decision boundary</a></li>
<li class="chapter" data-level="66" data-path="instituting-patterns.html"><a href="instituting-patterns.html"><i class="fa fa-check"></i><b>66</b> Instituting patterns</a></li>
<li class="chapter" data-level="67" data-path="regularizing-and-materializing-objects.html"><a href="regularizing-and-materializing-objects.html"><i class="fa fa-check"></i><b>67</b> Regularizing and materializing objects}</a></li>
<li class="chapter" data-level="68" data-path="genomic-referentiality-and-materiality.html"><a href="genomic-referentiality-and-materiality.html"><i class="fa fa-check"></i><b>68</b> Genomic referentiality and materiality</a></li>
<li class="chapter" data-level="69" data-path="the-genome-as-threshold-object.html"><a href="the-genome-as-threshold-object.html"><i class="fa fa-check"></i><b>69</b> The genome as threshold object</a></li>
<li class="chapter" data-level="70" data-path="genomic-knowledges-and-their-datasets.html"><a href="genomic-knowledges-and-their-datasets.html"><i class="fa fa-check"></i><b>70</b> Genomic knowledges and their datasets</a></li>
<li class="chapter" data-level="71" data-path="the-advent-of-wide-dirty-and-mixed-data.html"><a href="the-advent-of-wide-dirty-and-mixed-data.html"><i class="fa fa-check"></i><b>71</b> The advent of ‘wide, dirty and mixed’ data</a></li>
<li class="chapter" data-level="72" data-path="cross-validating-machine-learning-in-genomics.html"><a href="cross-validating-machine-learning-in-genomics.html"><i class="fa fa-check"></i><b>72</b> Cross-validating machine learning in genomics</a></li>
<li class="chapter" data-level="73" data-path="proliferation-of-discoveries.html"><a href="proliferation-of-discoveries.html"><i class="fa fa-check"></i><b>73</b> Proliferation of discoveries</a></li>
<li class="chapter" data-level="74" data-path="variations-in-the-object-or-in-the-machine-learner.html"><a href="variations-in-the-object-or-in-the-machine-learner.html"><i class="fa fa-check"></i><b>74</b> Variations in the object or in the machine learner?</a></li>
<li class="chapter" data-level="75" data-path="whole-genome-functions.html"><a href="whole-genome-functions.html"><i class="fa fa-check"></i><b>75</b> Whole genome functions</a></li>
<li class="chapter" data-level="76" data-path="propagating-subject-positions.html"><a href="propagating-subject-positions.html"><i class="fa fa-check"></i><b>76</b> Propagating subject positions}</a></li>
<li class="chapter" data-level="77" data-path="propagation-across-human-machine-boundaries.html"><a href="propagation-across-human-machine-boundaries.html"><i class="fa fa-check"></i><b>77</b> Propagation across human-machine boundaries</a></li>
<li class="chapter" data-level="78" data-path="competitive-positioning.html"><a href="competitive-positioning.html"><i class="fa fa-check"></i><b>78</b> Competitive positioning</a></li>
<li class="chapter" data-level="79" data-path="a-privileged-machine-and-its-diagrammatic-forms.html"><a href="a-privileged-machine-and-its-diagrammatic-forms.html"><i class="fa fa-check"></i><b>79</b> A privileged machine and its diagrammatic forms</a></li>
<li class="chapter" data-level="80" data-path="varying-subject-positions-in-code.html"><a href="varying-subject-positions-in-code.html"><i class="fa fa-check"></i><b>80</b> Varying subject positions in code</a></li>
<li class="chapter" data-level="81" data-path="the-subjects-of-a-hidden-operation.html"><a href="the-subjects-of-a-hidden-operation.html"><i class="fa fa-check"></i><b>81</b> The subjects of a hidden operation</a></li>
<li class="chapter" data-level="82" data-path="algorithms-that-propagate-errors.html"><a href="algorithms-that-propagate-errors.html"><i class="fa fa-check"></i><b>82</b> Algorithms that propagate errors</a></li>
<li class="chapter" data-level="83" data-path="competitions-as-examination.html"><a href="competitions-as-examination.html"><i class="fa fa-check"></i><b>83</b> Competitions as examination</a></li>
<li class="chapter" data-level="84" data-path="superimposing-power-and-knowledge.html"><a href="superimposing-power-and-knowledge.html"><i class="fa fa-check"></i><b>84</b> Superimposing power and knowledge</a></li>
<li class="chapter" data-level="85" data-path="ranked-subject-positions.html"><a href="ranked-subject-positions.html"><i class="fa fa-check"></i><b>85</b> Ranked subject positions</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="distributed-probabilities" class="section level1">
<h1><span class="header-section-number">52</span> Distributed probabilities</h1>
<p>While these structuring differences deeply shape practice in machine learning, the underlying operator that allows swapping between knowledge and the world, between events and devices, is probability, and in particular, functions that describe variations in populations, probability distributions.  Probability distributions both map population variations and, as we will see, multiply the number of things that count as populations.  </p>
<p>The normal distribution pervades nineteenth century statistical thinking as it affects populations across law, medicine, agriculture, finance and not least, sociology as a domain of knowledge. Normal distributions appear in countless variations in scientific, government and institutional settings as functions that map events, measurements, observations and records to evidential probability quantities.<a href="#fn50" class="footnoteRef" id="fnref50"><sup>50</sup></a></p>

<p>The function shown in equation () expresses the probability of a given value of the variable <span class="math inline">\(x\)</span> given a population whose variations (with respect to <span class="math inline">\(x\)</span>) can be expressed in terms of two parameters, <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>, the mean and variance. This is the so-called normal or Gaussian distribution.<a href="#fn51" class="footnoteRef" id="fnref51"><sup>51</sup></a> Its mathematics were intensively worked over during the late eighteenth and early nineteenth centuries in what has been termed ‘one of the major success stories in the history of science’ <span class="citation">[@Stigler_1986, 158]</span>. It has a power-laden biopolitical history closely tied with knowledges and governing of populations in terms of morality, mortality, health, and wealth (see <span class="citation">[@Hacking_1975, 113-124]</span>.  The key parameters here include <span class="math inline">\(\mu\)</span>, the mean and <span class="math inline">\(\sigma\)</span>, the variance, a number that describes the dispersion of values of the variable, <span class="math inline">\(x\)</span> are. These two parameters together describe the shape of the curve. Given knowledge of <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>, the normal or Gaussian probability distribution maps all outcomes to probabilities (or numbers in the range <span class="math inline">\(0\)</span> to <span class="math inline">\(1\)</span>). Put statistically, functions such as the Gaussian distribution probabilise events as random variables. Every variable potentially becomes a function: ‘a random variable is a mapping that assigns a real number to each outcome’ <span class="citation">[@Wasserman_2003,19]</span>.  </p>
<p>The possibility of treating population variations as random variables, that is, as probability distributions, was a significant historical achievement, one that continues to develop and ramify.<a href="#fn52" class="footnoteRef" id="fnref52"><sup>52</sup></a> Random variables distribute probability in the world. When conceptualised as real quantities in the world rather than epiphenomenal by-products of inaccuracies in our observations or measuring devices, probability distributions weave directly into the productive operations of power.  Distribution in the sense of locating, positioning, partitioning, sectioning, serialising or queuing operations has received much more attention in critical thought (particularly in the many uses of Foucault’s concept of disciplinary power <span class="citation">[@Foucault_1977]</span>), but in almost every setting, distribution in the sense of counting, apportioning and weighting of different outcomes also operates. This constant interweaving of spatial, architectural, logistical and functional processes has energised statistical thought for several centuries.<a href="#fn53" class="footnoteRef" id="fnref53"><sup>53</sup></a>  For instance, given the normal distribution, it is possible, under certain circumstances, to effectively subjectify someone on the spot. If an educational psychologist indicates to someone that their intelligence lies towards the left-hand side of the normal curve peak (and hence less than the population mean), they quickly assign them to a potentially institutionally and economically consequential trajectory. Since its inception in the social physics of Adolphe Quetelet as a way of referring to a property of populations, the normal curve has not only described but modulated and re-shaped populations (in terms of health, morality and wealth). </p>
<p>If functions such as equation () have persisted for so long as elements of population governmentality or biopolitics,  what happen to them in machine learning? The pages of a book such as <em>Elements of Statistical Learning</em> show many signs of an ongoing invocation of probability distributions. We could simply observe their abundance. Hastie and co-authors invoke probability distributions. They speak of ‘Gaussian mixtures,’ ‘bivariate Gaussian distributions,’ standard Gaussian,‘’Gaussian kernels,’ ‘Gaussian assumptions,’ ‘Gaussian errors,’ ‘Gaussian noise,’ ‘Gaussian radial basis function,’ ‘Gaussian variables,’ ‘Gaussian densities,’ ‘Gaussian process,’ and so forth. (The term ‘normal’ appears in an even wider spectrum of similar guises.) Events, things, properties, operations, functions, and attributes all associate with probability distributions. </p>
<p>The multiple invocations of probability distributions attests to the variety of events (occurrence of cancer, occurrence of the word ‘Viagra’ in an email, a click on a hyperlink, etc.) map to real numbers. Despite the sometimes dense mathematical diagrammaticism, the term <em>distribution</em> emphasises a tangible and practically resonant way of thinking about how events or possible outcomes shift about as the parameters of a function vary.<a href="#fn54" class="footnoteRef" id="fnref54"><sup>54</sup></a> Whatever inferences and predictions become possible, probability distributions are a crucial control surface for machine learning understood as a form of movement through data.  In contrast to the endowment of living aggregates such as populations with probability that we see in the biopolitical history of statistics (and later in natural sciences such as physics and biology), statistical machine learning increasingly constitutes devices as populations via probability distributions. </p>
</div>
<div class="footnotes">
<hr />
<ol start="50">
<li id="fn50"><p>Statistical graphics have a rich history and semiology that I do not discuss here (see <span class="citation">[@Bertin_1983]</span>).<a href="distributed-probabilities.html#fnref50">↩</a></p></li>
<li id="fn51"><p>Dozens of differently shaped probability distributions map continuous and discrete variations to real numbers. Other probability distributions — normal (Gaussian), uniform, Cauchy exponential, gamma, beta, hypergeometric, binomial, Poisson, chi-squared, Dirichlet, Boltzmann-Gibbs distributions, etc. (see <span class="citation">[@NIST_2012]</span> for a gallery of distributions) — functionally express widely differing patterns.  The queuing times at airport check-ins do not, for instance, easily fit a normal distribution. Statisticians model queues using a Poisson distribution, in which, unfortunately for travellers, distributes the number of events in a given time interval quite broadly. Similarly, it might be better to think of the probability of rain today in north-west England in terms of a Poisson distribution that models clouds in the Atlantic queuing to rain on the northwest coast of England. (Rather than addressing the question of whether it will rain or not, a Poisson-based model might address the question of how many times it will rain today.)<a href="distributed-probabilities.html#fnref51">↩</a></p></li>
<li id="fn52"><p>The mapping that assigns numbers to outcomes (heads v. tails; cancer v. benign; spam v. not-spam) is a probability distribution. As I have argued in <span class="citation">[@Mackenzie_2015d]</span>, random variables have become much more widespread in statistical practice due to changes in computational techniques. <a href="distributed-probabilities.html#fnref52">↩</a></p></li>
<li id="fn53"><p>‘Distribution’ pervades Foucault’s account of power and knowledge from <em>The Order of Things</em> <span class="citation">[@Foucault_1992]</span> onwards.  Foucault treats distributions in several different ways: as spatial or logistical techniques, as mathematical orderings of large numbers of people or things, and as a methodological and theoretical framing device. In <em>Discipline and Punish</em> <span class="citation">[@Foucault_1977]</span>, the spatial sense prevails, but in later works, the population or demographic sense of distribution takes precedence <span class="citation">[@Foucault_1998]</span>. Distribution certainly has theoretical primacy in his account of power: ‘relations of power-knowledge are not static forms of distribution, they are “matrices of transformations”’ <span class="citation">[@Foucault_1998, 99]</span>.<a href="distributed-probabilities.html#fnref53">↩</a></p></li>
<li id="fn54"><p>Machine learners adjust these parameters in different ways. For instance, parametric and non-parametric models (see table ) differ in that the former have a limited number of parameters and the latter an undefined number of parameters (for instance, Naive Bayes, <em>k</em> nearest neighbours or support vector machine models). But both kinds assume that an underlying probability distribution – a function, ‘unobservable’ or not – operates, even if it changes with new data. A probability distribution under these assumptions becomes the closest reality we have to whatever process generated all the variations in data gathered through experiments and observations. From a probabilistic perspective, the task of machine learning is to estimate the parameters (the mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma\)</span> in the case of Gaussian curve) that shape of the curve of the probability distribution.<a href="distributed-probabilities.html#fnref54">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="machine-learning-as-statistics-inside-out.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="naive-bayes-and-the-distribution-of-probabilities.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
