<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta property="og:title" content="" />
<meta property="og:type" content="book" />







<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="">

<title></title>

<script src="libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="libs/navigation-1.1/tabsets.js"></script>





<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
</style>
</head>

<body>

<div class="container-fluid main-container">


<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="1-acknowledgments.html#acknowledgments"><span class="toc-section-number">1</span> Acknowledgments</a></li>
<li><a href="2-machine-learners.html#machine-learners"><span class="toc-section-number">2</span> 250,000 machine learners</a></li>
<li><a href="3-a-summary-of-the-argument.html#a-summary-of-the-argument"><span class="toc-section-number">3</span> A summary of the argument</a></li>
<li><a href="4-in-situ-hybridization.html#in-situ-hybridization"><span class="toc-section-number">4</span> In-situ hybridization</a></li>
<li><a href="5-critical-operational-practice.html#critical-operational-practice"><span class="toc-section-number">5</span> Critical operational practice?</a></li>
<li><a href="6-obstacles-to-the-work-of-freeing-machine-learning.html#obstacles-to-the-work-of-freeing-machine-learning"><span class="toc-section-number">6</span> Obstacles to the work of freeing machine learning</a></li>
<li><a href="7-preface.html#preface"><span class="toc-section-number">7</span> Preface</a></li>
<li><a href="8-introduction-into-the-data.html#introduction-into-the-data"><span class="toc-section-number">8</span> Introduction: Into the Data</a></li>
<li><a href="9-three-accumulations-settings-data-and-devices.html#three-accumulations-settings-data-and-devices"><span class="toc-section-number">9</span> Three accumulations: settings, data and devices</a></li>
<li><a href="10-who-or-what-is-a-machine-learner.html#who-or-what-is-a-machine-learner"><span class="toc-section-number">10</span> Who or what is a machine learner?</a></li>
<li><a href="11-algorithmic-control-to-the-machine-learners.html#algorithmic-control-to-the-machine-learners"><span class="toc-section-number">11</span> Algorithmic control to the machine learners?</a></li>
<li><a href="12-the-archaeology-of-operations.html#the-archaeology-of-operations"><span class="toc-section-number">12</span> The archaeology of operations</a></li>
<li><a href="13-asymmetries-in-common-knowledge.html#asymmetries-in-common-knowledge"><span class="toc-section-number">13</span> Asymmetries in common knowledge</a></li>
<li><a href="14-what-cannot-be-automated.html#what-cannot-be-automated"><span class="toc-section-number">14</span> What cannot be automated?</a></li>
<li><a href="15-different-fields-in-machine-learning.html#different-fields-in-machine-learning"><span class="toc-section-number">15</span> Different fields in machine learning?</a></li>
<li><a href="16-the-diagram-in-critical-thought.html#the-diagram-in-critical-thought"><span class="toc-section-number">16</span> The diagram in critical thought</a></li>
<li><a href="17-we-dont-have-to-write-programs.html#we-dont-have-to-write-programs"><span class="toc-section-number">17</span> ‘We don’t have to write programs’?</a></li>
<li><a href="18-the-elements-of-machine-learning.html#the-elements-of-machine-learning"><span class="toc-section-number">18</span> The elements of machine learning</a></li>
<li><a href="19-who-reads-machine-learning-textbooks.html#who-reads-machine-learning-textbooks"><span class="toc-section-number">19</span> Who reads machine learning textbooks?</a></li>
<li><a href="20-r-a-matrix-of-transformations.html#r-a-matrix-of-transformations"><span class="toc-section-number">20</span> <code>R</code>: a matrix of transformations</a></li>
<li><a href="21-the-obdurate-mathematical-glint-of-machine-learning.html#the-obdurate-mathematical-glint-of-machine-learning"><span class="toc-section-number">21</span> The obdurate mathematical glint of machine learning</a></li>
<li><a href="22-cs229-2007-returning-again-and-again-to-certain-features.html#cs229-2007-returning-again-and-again-to-certain-features"><span class="toc-section-number">22</span> CS229, 2007: returning again and again to certain features</a></li>
<li><a href="23-the-visible-learning-of-machine-learning.html#the-visible-learning-of-machine-learning"><span class="toc-section-number">23</span> The visible learning of machine learning</a></li>
<li><a href="24-the-diagram-of-an-operational-formation.html#the-diagram-of-an-operational-formation"><span class="toc-section-number">24</span> The diagram of an operational formation</a></li>
<li><a href="25-vector-space-and-geometry.html#vector-space-and-geometry"><span class="toc-section-number">25</span> Vector space and geometry</a></li>
<li><a href="26-mixing-places.html#mixing-places"><span class="toc-section-number">26</span> Mixing places</a></li>
<li><a href="27-truth-is-no-longer-in-the-table.html#truth-is-no-longer-in-the-table"><span class="toc-section-number">27</span> Truth is no longer in the table?</a></li>
<li><a href="28-the-epistopic-fault-line-in-tables.html#the-epistopic-fault-line-in-tables"><span class="toc-section-number">28</span> The epistopic fault line in tables</a></li>
<li><a href="29-surface-and-depths-the-problem-of-volume-in-data.html#surface-and-depths-the-problem-of-volume-in-data"><span class="toc-section-number">29</span> Surface and depths: the problem of volume in data</a></li>
<li><a href="30-vector-space-expansion.html#vector-space-expansion"><span class="toc-section-number">30</span> Vector space expansion</a></li>
<li><a href="31-drawing-lines-in-a-common-space-of-transformation.html#drawing-lines-in-a-common-space-of-transformation"><span class="toc-section-number">31</span> Drawing lines in a common space of transformation</a></li>
<li><a href="32-implicit-vectorization-in-code-and-infrastructures.html#implicit-vectorization-in-code-and-infrastructures"><span class="toc-section-number">32</span> Implicit vectorization in code and infrastructures</a></li>
<li><a href="33-lines-traversing-behind-the-light.html#lines-traversing-behind-the-light"><span class="toc-section-number">33</span> Lines traversing behind the light</a></li>
<li><a href="34-the-vectorised-table.html#the-vectorised-table"><span class="toc-section-number">34</span> The vectorised table?</a></li>
<li><a href="35-learning-functions.html#learning-functions"><span class="toc-section-number">35</span> Learning functions</a></li>
<li><a href="36-supervised-unsupervised-reinforcement-learning-and-functions.html#supervised-unsupervised-reinforcement-learning-and-functions"><span class="toc-section-number">36</span> Supervised, unsupervised, reinforcement learning and functions</a></li>
<li><a href="37-which-function-operates.html#which-function-operates"><span class="toc-section-number">37</span> Which function operates?</a></li>
<li><a href="38-what-does-a-function-learn.html#what-does-a-function-learn"><span class="toc-section-number">38</span> What does a function learn?</a></li>
<li><a href="39-observing-with-curves-the-logistic-function.html#observing-with-curves-the-logistic-function"><span class="toc-section-number">39</span> Observing with curves: the logistic function</a></li>
<li><a href="40-the-cost-of-curves-in-machine-learning.html#the-cost-of-curves-in-machine-learning"><span class="toc-section-number">40</span> The cost of curves in machine learning</a></li>
<li><a href="41-curves-and-the-variation-in-models.html#curves-and-the-variation-in-models"><span class="toc-section-number">41</span> Curves and the variation in models</a></li>
<li><a href="42-observing-costs-losses-and-objectives-through-optimisation.html#observing-costs-losses-and-objectives-through-optimisation"><span class="toc-section-number">42</span> Observing costs, losses and objectives through optimisation</a></li>
<li><a href="43-gradients-as-partial-observers.html#gradients-as-partial-observers"><span class="toc-section-number">43</span> Gradients as partial observers</a></li>
<li><a href="44-the-power-to-learn.html#the-power-to-learn"><span class="toc-section-number">44</span> The power to learn</a></li>
<li><a href="45-data-reduces-uncertainty.html#data-reduces-uncertainty"><span class="toc-section-number">45</span> Data reduces uncertainty?</a></li>
<li><a href="46-machine-learning-as-statistics-inside-out.html#machine-learning-as-statistics-inside-out"><span class="toc-section-number">46</span> Machine learning as statistics inside out</a></li>
<li><a href="47-distributed-probabilities.html#distributed-probabilities"><span class="toc-section-number">47</span> Distributed probabilities</a></li>
<li><a href="48-naive-bayes-and-the-distribution-of-probabilities.html#naive-bayes-and-the-distribution-of-probabilities"><span class="toc-section-number">48</span> Naive Bayes and the distribution of probabilities</a></li>
<li><a href="49-spam-when-foralln-is-too-much.html#spam-when-foralln-is-too-much"><span class="toc-section-number">49</span> Spam: when <span class="math inline">\(\forall{N}\)</span> is too much?</a></li>
<li><a href="50-the-improbable-success-of-the-naive-bayes-classifier.html#the-improbable-success-of-the-naive-bayes-classifier"><span class="toc-section-number">50</span> The improbable success of the Naive Bayes classifier</a></li>
<li><a href="51-ancestral-probabilities-in-documents-inference-and-prediction.html#ancestral-probabilities-in-documents-inference-and-prediction"><span class="toc-section-number">51</span> Ancestral probabilities in documents: inference and prediction</a></li>
<li><a href="52-statistical-decompositions-bias-variance-and-observed-errors.html#statistical-decompositions-bias-variance-and-observed-errors"><span class="toc-section-number">52</span> Statistical decompositions: bias, variance and observed errors</a></li>
<li><a href="53-does-machine-learning-construct-a-new-statistical-reality.html#does-machine-learning-construct-a-new-statistical-reality"><span class="toc-section-number">53</span> Does machine learning construct a new statistical reality?</a></li>
<li><a href="54-splitting-and-the-growth-of-trees.html#splitting-and-the-growth-of-trees"><span class="toc-section-number">54</span> Splitting and the growth of trees</a></li>
<li><a href="55-differences-in-recursive-partitioning.html#differences-in-recursive-partitioning"><span class="toc-section-number">55</span> 1984: Differences in recursive partitioning</a></li>
<li><a href="56-limiting-differences.html#limiting-differences"><span class="toc-section-number">56</span> Limiting differences</a></li>
<li><a href="57-the-successful-dispersion-of-the-support-vector-machine.html#the-successful-dispersion-of-the-support-vector-machine"><span class="toc-section-number">57</span> The successful dispersion of the support vector machine</a></li>
<li><a href="58-differences-blur.html#differences-blur"><span class="toc-section-number">58</span> Differences blur?</a></li>
<li><a href="59-bending-the-decision-boundary.html#bending-the-decision-boundary"><span class="toc-section-number">59</span> Bending the decision boundary</a></li>
<li><a href="60-instituting-patterns.html#instituting-patterns"><span class="toc-section-number">60</span> Instituting patterns</a></li>
<li><a href="61-genomic-referentiality-and-materiality.html#genomic-referentiality-and-materiality"><span class="toc-section-number">61</span> Genomic referentiality and materiality</a></li>
<li><a href="62-the-genome-as-threshold-object.html#the-genome-as-threshold-object"><span class="toc-section-number">62</span> The genome as threshold object</a></li>
<li><a href="63-genomic-knowledges-and-their-datasets.html#genomic-knowledges-and-their-datasets"><span class="toc-section-number">63</span> Genomic knowledges and their datasets</a></li>
<li><a href="64-the-advent-of-wide-dirty-and-mixed-data.html#the-advent-of-wide-dirty-and-mixed-data"><span class="toc-section-number">64</span> The advent of ‘wide, dirty and mixed’ data</a></li>
<li><a href="65-cross-validating-machine-learning-in-genomics.html#cross-validating-machine-learning-in-genomics"><span class="toc-section-number">65</span> Cross-validating machine learning in genomics</a></li>
<li><a href="66-proliferation-of-discoveries.html#proliferation-of-discoveries"><span class="toc-section-number">66</span> Proliferation of discoveries</a></li>
<li><a href="67-variations-in-the-object-or-in-the-machine-learner.html#variations-in-the-object-or-in-the-machine-learner"><span class="toc-section-number">67</span> Variations in the object or in the machine learner?</a></li>
<li><a href="68-whole-genome-functions.html#whole-genome-functions"><span class="toc-section-number">68</span> Whole genome functions</a></li>
<li><a href="69-propagation-across-human-machine-boundaries.html#propagation-across-human-machine-boundaries"><span class="toc-section-number">69</span> Propagation across human-machine boundaries</a></li>
<li><a href="70-competitive-positioning.html#competitive-positioning"><span class="toc-section-number">70</span> Competitive positioning</a></li>
<li><a href="71-a-privileged-machine-and-its-diagrammatic-forms.html#a-privileged-machine-and-its-diagrammatic-forms"><span class="toc-section-number">71</span> A privileged machine and its diagrammatic forms</a></li>
<li><a href="72-varying-subject-positions-in-code.html#varying-subject-positions-in-code"><span class="toc-section-number">72</span> Varying subject positions in code</a></li>
<li><a href="73-the-subjects-of-a-hidden-operation.html#the-subjects-of-a-hidden-operation"><span class="toc-section-number">73</span> The subjects of a hidden operation</a></li>
<li><a href="74-algorithms-that-propagate-errors.html#algorithms-that-propagate-errors"><span class="toc-section-number">74</span> Algorithms that propagate errors</a></li>
<li><a href="75-competitions-as-examination.html#competitions-as-examination"><span class="toc-section-number">75</span> Competitions as examination</a></li>
<li><a href="76-superimposing-power-and-knowledge.html#superimposing-power-and-knowledge"><span class="toc-section-number">76</span> Superimposing power and knowledge</a></li>
<li><a href="77-ranked-subject-positions.html#ranked-subject-positions"><span class="toc-section-number">77</span> Ranked subject positions</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="r-a-matrix-of-transformations" class="section level1">
<h1><span class="header-section-number">20</span> <code>R</code>: a matrix of transformations</h1>
<p> Although barely a single line of code appears in <em>Elements of Statistical Learning</em>, all of the learners presented there are implemented in a single programming language, <code>R</code>. Coding is the operational practice that links the different planes and elements of machine learning in an operational formation.   The authors say ‘we used the R and S-PLUS programming languages in our courses’ <span class="citation">[@Hastie_2009,9]</span> but many elements of the book derive from <code>R</code> code.<a href="#fn11" class="footnoteRef" id="fnref11"><sup>11</sup></a> The proliferation of programming languages such as <code>FORTRAN</code> (dating from the 1950s), <code>C</code> (1970s), <code>C++</code> (1980s), then <code>Perl</code> (1990s), <code>Java</code> (1990s), <code>Python</code> (1990s) and <code>R</code> (1990s), and computational scripting environments such as Matlab, multiplied the paths along which machine learners move through operational formations. It would be difficult to comprehend the propagation of machine learners across domains of science, business and government without paying attention to coding practices. Even if textbooks and research articles are not read, software packages and libraries for machine learning are used. Code has a mobility that extends the diagrammatic practices of machine learning into a variety of settings and places where the scientific reading apparatuses of equations, statistical plots, and citations of research articles would not be operative. </p>

<p>\begin{table}[!htb] \caption[<code>ElemStatLearn</code> depends on R packages]{<code>R</code> packages depended on by the <code>ElemStatLearn</code> package}  \end{table}</p>
<p>How should we think of the <code>R</code> code in <em>Elements of Statistical Learning</em> in its operational specificity?  Its growth is perhaps just as important as its operation <span class="citation">[@Mackenzie_2014]</span>. An open source programming language, according to surveys of business and scientific users, at the time of writing, <code>R</code> has replaced popular statistical software packages such as SPSS, SAS and Stata as the statistical and data analysis tool of choice for many people in business, government, and sciences ranging from political science to genomics, from quantitative finance to climatology <span class="citation">[@RexerAnalytics_2015]</span>. Developed in New Zealand in the mid-1990s, and like many open source software projects, emulating <code>S</code>, a commercialised predecessor developed at AT &amp; T Bell Labs during the 1980s, <code>R</code> is now extremely widely used across life and physical sciences, as well as quantitative social sciences. John Chambers, the designer of <code>S</code>, was awarded the Association for Computing Machinery (ACM) ‘Software System Award’ in 1998 for ‘the S system, which has forever altered how people analyze, visualize, and manipulate data’ <span class="citation">[@ACM_2013]</span>.  Many undergraduate and graduate students today earn <code>R</code> as a basic tool for statistics. Skills in <code>R</code> are often seen as essential pre-requisite for scientific researchers, especially in the life sciences. (In engineering, <code>Matlab</code> is widely used.) Research articles and textbooks in statistics commonly both use <code>R</code> to demonstrate methods and techniques, and create <code>R</code> packages to distribute the techniques and sample data. Nearly all of these publication-related software packages, including quite a few from the authors of <em>Elements of Statistical Learning</em> are soon or later available from the ‘Comprehensive <code>R</code> Archive Network (CRAN)’ <span class="citation">[@CRAN_2010]</span>. Estimates of its number of users range between 250000 and 2 million. Increasingly, <code>R</code> is integrated into commercial services and products (for instance, SAS, a widely used business data analysis system now has an <code>R</code> interface; Norman Nie, one of the original developers of the SPSS package heavily used in social sciences, now leads a business, Revolution, devoted to commercialising <code>R</code>; <code>R</code> is heavily used at Google, at FaceBook, and by quantitative traders in hedge funds; in 2013 <a href="http://www.r-bloggers.com/r-usage-skyrocketing-rexer-poll/">‘R usage is sky-rocketing’</a>; etc.). In general terms, <code>R</code> has a kind of disciplinary polyglot currency as a form of expression, and exhibits a fine-grained relationality with many different epistemic and operational situations associated with machine learning.</p>
<p>Two early proponents of <code>R</code> and <code>S</code> describe the motivation for the language:</p>
<blockquote>
<p>The goal of the S language … is “to turn ideas into software, quickly and faithfully” … it is the duty of the responsible data analysts to engage in this process … the exercise of drafting an algorithm to the level of precision that programming requires can in itself clarify ideas and promote rigorous intellectual scrutiny. … Turning ideas into software in this way need not be an unpleasant duty. <span class="citation">[@Venables_2002, 2]</span>  </p>
</blockquote>
<p>Bill Venables and Brian Ripley, statisticians working on developing <code>S</code>, the almost identical commercial predecessor to <code>R</code>, wrote in the early 1990s of the responsibility of data analysts to write not just use software.   They write ‘software’ here not in the sense of a product, but in the sense that today would more likely be called ‘code.’ Their sense of coding and programming as clarifying and concretising ideas with precision – as abstractions – has thoroughly taken hold in contemporary data analysis. If code, as they suggest, entails a threshold of idealisation, it differs from mathematical formalization in that it changes the positions and relations of knowledge to include machines, devices and infrastructures.</p>
<p>While the view that code is a precise expression of ideas makes sense, it does not capture the relational complexity of <code>R</code> code as it operates in a setting such as <em>Elements of Statistical Learning</em>. In machine learning, code, along with mathematics, is a primary operational form that ideas take as they become machine learners. But code as an expressive operation by which ‘an individual formulates an idea’ or as ‘a rational activity that may operate in a system of inference’ <span class="citation">[@Foucault_1972, 117]</span> does not exhaust and should not be conflated with operational practice in machine learning.  Similarly, the intersection of <code>R</code> with machine learning also lies somewhat at odds with Pedro Domingos’ characterisation of machine learning as a shift away from people building to learners growing programs.</p>

<p>What in <code>R</code> (let alone other programming languages) overflows both the ideas of code as expression of ideas and code as automation? Alongside expression and automation, much <code>R</code> code furnishes a matrix of practice crossing network infrastructures, display screens, statistical techniques, software engineering architectures as well as publication and documentation standards. For instance, the line of <code>R</code> code shown in the listing  when executed opens another way of reading <em>Elements of Statistical Learning</em> and getting a feel for the dragnet of practical relations and infrastructural configurations running through it. Take the part of the line <code>dependencies = 'Suggests'</code>. When the line of code executes, the stipulation of <code>dependencies</code> leads to a quite wide-ranging installation event that installs many <code>R</code> packages. If the installation works (and that assumes quite a lot of configuration work has already taken place; for instance, installing a recent version of the <code>R</code> platform), then <em>Elements of Statistical Learning</em> is now augmented by various pieces of code, and by various datasets that in some ways echo or mimic the book but in other ways extend it operationally (see tables  and ).<a href="#fn12" class="footnoteRef" id="fnref12"><sup>12</sup></a>  These code elements are often stunningly specialised. As Karl Marx wrote of the 500 different hammers made in Birmingham, ‘not only is each adapted to one particular process, but several varieties often serve exclusively for the different operations in one and the same process’ <span class="citation">[@Marx_1986, 375]</span> . Something similar holds in <code>R</code>: thousands of software packages in online repositories suggest that a highly specialised division of labour and possibly refined co-operative labour processes operate around data. </p>
<p>Because of this almost incoherent plurality, and its labile status as both a programming environment and a statistical analysis package, <code>R</code> is an evocative object, to use the psychoanalyst Christopher Bollas’ term <span class="citation">[@Bollas_2008]</span> , an object through which many different ways of thinking circulate. Standing somewhere at the intersection of statistics and computing, modelling and programming, many different disciplines, techniques, domains and actors intersect in <code>R</code>. It engages immediately, practically and widely with words, numbers, images, symbols, signals, sensors, forms, instruments and above all abstract forms such as mathematical functions like probability distributions and many different architectural forms (vectors, matrices, arrays, etc.), as it employs data.  If, as Bollas suggests, ‘our encounter, engagement with, and sometimes our employment of, actual things is a <em>way</em> of thinking’ <span class="citation">[@Bollas_2008, 92]</span>,  then it plausible that <code>R</code> not only gathers a plurality of data practices – working with measurements, numbers, text, images, models and equations, with techniques for sampling and sorting, with probability distributions and random numbers – but that it embodies the kernel of a mode of thought relevant to contemporary realities. </p>
</div>
<div class="footnotes">
<hr />
<ol start="11">
<li id="fn11"><p>In a later book by some of the same authors with the very similar sounding title <em>An Introduction to Statistical Machine Learning with Applications in <code>R</code></em> <span class="citation">[@James_2013]</span>, <code>R</code> does appear in abundance. This book, however, is much shorter and less inclusive in various ways. There are in any case many online manuals, guides and tutorials relating to <code>R</code> <span class="citation">[@Wikibooks_2013]</span>.  For present purposes, I draw mainly on semi-popular books such as <em>R in a Nutshell</em> <span class="citation">[@Adler_2010]</span>, <em>The Art of <code>R</code> Programming</em> <span class="citation">[@Matloff_2011]</span>, <em>R Cookbook</em> <span class="citation">[@Teetor_2011]</span>, <em>Machine Learning with R</em> <span class="citation">[@Lantz_2013]</span> or <em>An Introduction to Statistical Learning with Applications in R</em> <span class="citation">[@James_2013]</span>. These books are not written for academic audiences, although academics often write them and use them in their work.  They are largely made up of illustrations and examples of how to do different things with different types of data, and their examples are typically oriented towards business or commercial settings, where, presumably, the bulk of the readers work, or aspire to work. Given a certain predisposition (that is, geekiness), these books and other learning materials can make for enjoyable reading. While they vary highly in quality, it is sometimes pleasing to see the economical way in which they solve common data problems. This genre of writing about programming specialises in posing a problem and solving it directly. In these settings, learning takes place largely through following or emulating what someone else has done with either a well known dataset (Fisher’s <code>iris</code>) or a toy dataset generated for demonstration purposes. The many frictions, blockages and compromises that often affect data practice are largely occluded here in the interests of demonstrating the neat application of specific techniques. Yet are not those frictions, neat compromises and strains around data and machine learning precisely what we need to track diagrammatically? In order to demonstrate both the costs and benefits of approaching <code>R</code> through such materials, rather than through ethnographic observation of people using <code>R</code>, it will be necessary to stage encounters here with data that has not been completely transformed or cleaned in the interests of neatly demonstrating the power of a technique. One way to do this is by writing about code while coding.<a href="20-r-a-matrix-of-transformations.html#fnref11">↩</a></p></li>
<li id="fn12"><p>Most of the packages associated with the <code>ElemStatLearn</code>  implement methods or techniques developed by Hastie, Tibshirani or Friedman, but some are much more generic. <code>MASS</code> for instance is highly cited <code>R</code> package. (Of the 9192 packages in the R CRAN system, 0 depend on the library <code>MASS</code>,  itself an adjunct to the influential and highly cited <em>Modern Applied Statistics with S</em> <span class="citation">[@Venables_2002]</span>, a textbook that presents many machine learning techniques using <code>S</code> , AT &amp; T Bell Labs commercial precursor to the open sourced <code>R</code>. ) For our purposes, this hardly accidental mixing of academic or research work with a programming languages and its associated infrastructures is fortuitous. It allows us to transit between different strata of the social fields of science, engineering, health, medicine, business media and government more easily.<a href="20-r-a-matrix-of-transformations.html#fnref12">↩</a></p></li>
</ol>
</div>
<p style="text-align: center;">
<a href="19-who-reads-machine-learning-textbooks.html"><button class="btn btn-default">Previous</button></a>
<a href="21-the-obdurate-mathematical-glint-of-machine-learning.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
