<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title></title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="">
  <meta name="generator" content="bookdown 0.3 and GitBook 2.6.7">

  <meta property="og:title" content="" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="observing-costs-losses-and-objectives-through-optimisation.html">
<link rel="next" href="the-power-to-learn.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i><b>1</b> Acknowledgments</a></li>
<li class="chapter" data-level="2" data-path="conclusion-out-of-the-data.html"><a href="conclusion-out-of-the-data.html"><i class="fa fa-check"></i><b>2</b> Conclusion: Out of the Data}</a></li>
<li class="chapter" data-level="3" data-path="machine-learners.html"><a href="machine-learners.html"><i class="fa fa-check"></i><b>3</b> 250,000 machine learners</a></li>
<li class="chapter" data-level="4" data-path="a-summary-of-the-argument.html"><a href="a-summary-of-the-argument.html"><i class="fa fa-check"></i><b>4</b> A summary of the argument</a></li>
<li class="chapter" data-level="5" data-path="in-situ-hybridization.html"><a href="in-situ-hybridization.html"><i class="fa fa-check"></i><b>5</b> In-situ hybridization</a></li>
<li class="chapter" data-level="6" data-path="critical-operational-practice.html"><a href="critical-operational-practice.html"><i class="fa fa-check"></i><b>6</b> Critical operational practice?</a></li>
<li class="chapter" data-level="7" data-path="obstacles-to-the-work-of-freeing-machine-learning.html"><a href="obstacles-to-the-work-of-freeing-machine-learning.html"><i class="fa fa-check"></i><b>7</b> Obstacles to the work of freeing machine learning</a></li>
<li class="chapter" data-level="8" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i><b>8</b> Preface</a></li>
<li class="chapter" data-level="9" data-path="introduction-into-the-data.html"><a href="introduction-into-the-data.html"><i class="fa fa-check"></i><b>9</b> Introduction: Into the Data</a></li>
<li class="chapter" data-level="10" data-path="three-accumulations-settings-data-and-devices.html"><a href="three-accumulations-settings-data-and-devices.html"><i class="fa fa-check"></i><b>10</b> Three accumulations: settings, data and devices</a></li>
<li class="chapter" data-level="11" data-path="who-or-what-is-a-machine-learner.html"><a href="who-or-what-is-a-machine-learner.html"><i class="fa fa-check"></i><b>11</b> Who or what is a machine learner?</a></li>
<li class="chapter" data-level="12" data-path="algorithmic-control-to-the-machine-learners.html"><a href="algorithmic-control-to-the-machine-learners.html"><i class="fa fa-check"></i><b>12</b> Algorithmic control to the machine learners?</a></li>
<li class="chapter" data-level="13" data-path="the-archaeology-of-operations.html"><a href="the-archaeology-of-operations.html"><i class="fa fa-check"></i><b>13</b> The archaeology of operations</a></li>
<li class="chapter" data-level="14" data-path="asymmetries-in-common-knowledge.html"><a href="asymmetries-in-common-knowledge.html"><i class="fa fa-check"></i><b>14</b> Asymmetries in common knowledge</a></li>
<li class="chapter" data-level="15" data-path="what-cannot-be-automated.html"><a href="what-cannot-be-automated.html"><i class="fa fa-check"></i><b>15</b> What cannot be automated?</a></li>
<li class="chapter" data-level="16" data-path="different-fields-in-machine-learning.html"><a href="different-fields-in-machine-learning.html"><i class="fa fa-check"></i><b>16</b> Different fields in machine learning?</a></li>
<li class="chapter" data-level="17" data-path="the-diagram-in-critical-thought.html"><a href="the-diagram-in-critical-thought.html"><i class="fa fa-check"></i><b>17</b> The diagram in critical thought</a></li>
<li class="chapter" data-level="18" data-path="diagramming-machines.html"><a href="diagramming-machines.html"><i class="fa fa-check"></i><b>18</b> Diagramming machines}</a></li>
<li class="chapter" data-level="19" data-path="we-dont-have-to-write-programs.html"><a href="we-dont-have-to-write-programs.html"><i class="fa fa-check"></i><b>19</b> ‘We don’t have to write programs’?</a></li>
<li class="chapter" data-level="20" data-path="the-elements-of-machine-learning.html"><a href="the-elements-of-machine-learning.html"><i class="fa fa-check"></i><b>20</b> The elements of machine learning</a></li>
<li class="chapter" data-level="21" data-path="who-reads-machine-learning-textbooks.html"><a href="who-reads-machine-learning-textbooks.html"><i class="fa fa-check"></i><b>21</b> Who reads machine learning textbooks?</a></li>
<li class="chapter" data-level="22" data-path="r-a-matrix-of-transformations.html"><a href="r-a-matrix-of-transformations.html"><i class="fa fa-check"></i><b>22</b> <code>R</code>: a matrix of transformations</a></li>
<li class="chapter" data-level="23" data-path="the-obdurate-mathematical-glint-of-machine-learning.html"><a href="the-obdurate-mathematical-glint-of-machine-learning.html"><i class="fa fa-check"></i><b>23</b> The obdurate mathematical glint of machine learning</a></li>
<li class="chapter" data-level="24" data-path="cs229-2007-returning-again-and-again-to-certain-features.html"><a href="cs229-2007-returning-again-and-again-to-certain-features.html"><i class="fa fa-check"></i><b>24</b> CS229, 2007: returning again and again to certain features</a></li>
<li class="chapter" data-level="25" data-path="the-visible-learning-of-machine-learning.html"><a href="the-visible-learning-of-machine-learning.html"><i class="fa fa-check"></i><b>25</b> The visible learning of machine learning</a></li>
<li class="chapter" data-level="26" data-path="the-diagram-of-an-operational-formation.html"><a href="the-diagram-of-an-operational-formation.html"><i class="fa fa-check"></i><b>26</b> The diagram of an operational formation</a></li>
<li class="chapter" data-level="27" data-path="vectorisation-and-its-consequences.html"><a href="vectorisation-and-its-consequences.html"><i class="fa fa-check"></i><b>27</b> Vectorisation and its consequences}</a></li>
<li class="chapter" data-level="28" data-path="vector-space-and-geometry.html"><a href="vector-space-and-geometry.html"><i class="fa fa-check"></i><b>28</b> Vector space and geometry</a></li>
<li class="chapter" data-level="29" data-path="mixing-places.html"><a href="mixing-places.html"><i class="fa fa-check"></i><b>29</b> Mixing places</a></li>
<li class="chapter" data-level="30" data-path="truth-is-no-longer-in-the-table.html"><a href="truth-is-no-longer-in-the-table.html"><i class="fa fa-check"></i><b>30</b> Truth is no longer in the table?</a></li>
<li class="chapter" data-level="31" data-path="the-epistopic-fault-line-in-tables.html"><a href="the-epistopic-fault-line-in-tables.html"><i class="fa fa-check"></i><b>31</b> The epistopic fault line in tables</a></li>
<li class="chapter" data-level="32" data-path="surface-and-depths-the-problem-of-volume-in-data.html"><a href="surface-and-depths-the-problem-of-volume-in-data.html"><i class="fa fa-check"></i><b>32</b> Surface and depths: the problem of volume in data</a></li>
<li class="chapter" data-level="33" data-path="vector-space-expansion.html"><a href="vector-space-expansion.html"><i class="fa fa-check"></i><b>33</b> Vector space expansion</a></li>
<li class="chapter" data-level="34" data-path="drawing-lines-in-a-common-space-of-transformation.html"><a href="drawing-lines-in-a-common-space-of-transformation.html"><i class="fa fa-check"></i><b>34</b> Drawing lines in a common space of transformation</a></li>
<li class="chapter" data-level="35" data-path="implicit-vectorization-in-code-and-infrastructures.html"><a href="implicit-vectorization-in-code-and-infrastructures.html"><i class="fa fa-check"></i><b>35</b> Implicit vectorization in code and infrastructures</a></li>
<li class="chapter" data-level="36" data-path="lines-traversing-behind-the-light.html"><a href="lines-traversing-behind-the-light.html"><i class="fa fa-check"></i><b>36</b> Lines traversing behind the light</a></li>
<li class="chapter" data-level="37" data-path="the-vectorised-table.html"><a href="the-vectorised-table.html"><i class="fa fa-check"></i><b>37</b> The vectorised table?</a></li>
<li class="chapter" data-level="38" data-path="machines-finding-functions.html"><a href="machines-finding-functions.html"><i class="fa fa-check"></i><b>38</b> Machines finding functions}</a></li>
<li class="chapter" data-level="39" data-path="learning-functions.html"><a href="learning-functions.html"><i class="fa fa-check"></i><b>39</b> Learning functions</a></li>
<li class="chapter" data-level="40" data-path="supervised-unsupervised-reinforcement-learning-and-functions.html"><a href="supervised-unsupervised-reinforcement-learning-and-functions.html"><i class="fa fa-check"></i><b>40</b> Supervised, unsupervised, reinforcement learning and functions</a></li>
<li class="chapter" data-level="41" data-path="which-function-operates.html"><a href="which-function-operates.html"><i class="fa fa-check"></i><b>41</b> Which function operates?</a></li>
<li class="chapter" data-level="42" data-path="what-does-a-function-learn.html"><a href="what-does-a-function-learn.html"><i class="fa fa-check"></i><b>42</b> What does a function learn?</a></li>
<li class="chapter" data-level="43" data-path="observing-with-curves-the-logistic-function.html"><a href="observing-with-curves-the-logistic-function.html"><i class="fa fa-check"></i><b>43</b> Observing with curves: the logistic function</a></li>
<li class="chapter" data-level="44" data-path="the-cost-of-curves-in-machine-learning.html"><a href="the-cost-of-curves-in-machine-learning.html"><i class="fa fa-check"></i><b>44</b> The cost of curves in machine learning</a></li>
<li class="chapter" data-level="45" data-path="curves-and-the-variation-in-models.html"><a href="curves-and-the-variation-in-models.html"><i class="fa fa-check"></i><b>45</b> Curves and the variation in models</a></li>
<li class="chapter" data-level="46" data-path="observing-costs-losses-and-objectives-through-optimisation.html"><a href="observing-costs-losses-and-objectives-through-optimisation.html"><i class="fa fa-check"></i><b>46</b> Observing costs, losses and objectives through optimisation</a></li>
<li class="chapter" data-level="47" data-path="gradients-as-partial-observers.html"><a href="gradients-as-partial-observers.html"><i class="fa fa-check"></i><b>47</b> Gradients as partial observers</a></li>
<li class="chapter" data-level="48" data-path="the-power-to-learn.html"><a href="the-power-to-learn.html"><i class="fa fa-check"></i><b>48</b> The power to learn</a></li>
<li class="chapter" data-level="49" data-path="n-forallboldsymbolx-probabilisation-and-the-taming-of-machines.html"><a href="n-forallboldsymbolx-probabilisation-and-the-taming-of-machines.html"><i class="fa fa-check"></i><b>49</b> <span class="math inline">\(N = \forall\boldsymbol{X}\)</span> Probabilisation and the Taming of Machines}</a></li>
<li class="chapter" data-level="50" data-path="data-reduces-uncertainty.html"><a href="data-reduces-uncertainty.html"><i class="fa fa-check"></i><b>50</b> Data reduces uncertainty?</a></li>
<li class="chapter" data-level="51" data-path="machine-learning-as-statistics-inside-out.html"><a href="machine-learning-as-statistics-inside-out.html"><i class="fa fa-check"></i><b>51</b> Machine learning as statistics inside out</a></li>
<li class="chapter" data-level="52" data-path="distributed-probabilities.html"><a href="distributed-probabilities.html"><i class="fa fa-check"></i><b>52</b> Distributed probabilities</a></li>
<li class="chapter" data-level="53" data-path="naive-bayes-and-the-distribution-of-probabilities.html"><a href="naive-bayes-and-the-distribution-of-probabilities.html"><i class="fa fa-check"></i><b>53</b> Naive Bayes and the distribution of probabilities</a></li>
<li class="chapter" data-level="54" data-path="spam-when-foralln-is-too-much.html"><a href="spam-when-foralln-is-too-much.html"><i class="fa fa-check"></i><b>54</b> Spam: when <span class="math inline">\(\forall{N}\)</span> is too much?</a></li>
<li class="chapter" data-level="55" data-path="the-improbable-success-of-the-naive-bayes-classifier.html"><a href="the-improbable-success-of-the-naive-bayes-classifier.html"><i class="fa fa-check"></i><b>55</b> The improbable success of the Naive Bayes classifier</a></li>
<li class="chapter" data-level="56" data-path="ancestral-probabilities-in-documents-inference-and-prediction.html"><a href="ancestral-probabilities-in-documents-inference-and-prediction.html"><i class="fa fa-check"></i><b>56</b> Ancestral probabilities in documents: inference and prediction</a></li>
<li class="chapter" data-level="57" data-path="statistical-decompositions-bias-variance-and-observed-errors.html"><a href="statistical-decompositions-bias-variance-and-observed-errors.html"><i class="fa fa-check"></i><b>57</b> Statistical decompositions: bias, variance and observed errors</a></li>
<li class="chapter" data-level="58" data-path="does-machine-learning-construct-a-new-statistical-reality.html"><a href="does-machine-learning-construct-a-new-statistical-reality.html"><i class="fa fa-check"></i><b>58</b> Does machine learning construct a new statistical reality?</a></li>
<li class="chapter" data-level="59" data-path="patterns-and-differences.html"><a href="patterns-and-differences.html"><i class="fa fa-check"></i><b>59</b> Patterns and differences</a></li>
<li class="chapter" data-level="60" data-path="splitting-and-the-growth-of-trees.html"><a href="splitting-and-the-growth-of-trees.html"><i class="fa fa-check"></i><b>60</b> Splitting and the growth of trees</a></li>
<li class="chapter" data-level="61" data-path="differences-in-recursive-partitioning.html"><a href="differences-in-recursive-partitioning.html"><i class="fa fa-check"></i><b>61</b> 1984: Differences in recursive partitioning</a></li>
<li class="chapter" data-level="62" data-path="limiting-differences.html"><a href="limiting-differences.html"><i class="fa fa-check"></i><b>62</b> Limiting differences</a></li>
<li class="chapter" data-level="63" data-path="the-successful-dispersion-of-the-support-vector-machine.html"><a href="the-successful-dispersion-of-the-support-vector-machine.html"><i class="fa fa-check"></i><b>63</b> The successful dispersion of the support vector machine</a></li>
<li class="chapter" data-level="64" data-path="differences-blur.html"><a href="differences-blur.html"><i class="fa fa-check"></i><b>64</b> Differences blur?</a></li>
<li class="chapter" data-level="65" data-path="bending-the-decision-boundary.html"><a href="bending-the-decision-boundary.html"><i class="fa fa-check"></i><b>65</b> Bending the decision boundary</a></li>
<li class="chapter" data-level="66" data-path="instituting-patterns.html"><a href="instituting-patterns.html"><i class="fa fa-check"></i><b>66</b> Instituting patterns</a></li>
<li class="chapter" data-level="67" data-path="regularizing-and-materializing-objects.html"><a href="regularizing-and-materializing-objects.html"><i class="fa fa-check"></i><b>67</b> Regularizing and materializing objects}</a></li>
<li class="chapter" data-level="68" data-path="genomic-referentiality-and-materiality.html"><a href="genomic-referentiality-and-materiality.html"><i class="fa fa-check"></i><b>68</b> Genomic referentiality and materiality</a></li>
<li class="chapter" data-level="69" data-path="the-genome-as-threshold-object.html"><a href="the-genome-as-threshold-object.html"><i class="fa fa-check"></i><b>69</b> The genome as threshold object</a></li>
<li class="chapter" data-level="70" data-path="genomic-knowledges-and-their-datasets.html"><a href="genomic-knowledges-and-their-datasets.html"><i class="fa fa-check"></i><b>70</b> Genomic knowledges and their datasets</a></li>
<li class="chapter" data-level="71" data-path="the-advent-of-wide-dirty-and-mixed-data.html"><a href="the-advent-of-wide-dirty-and-mixed-data.html"><i class="fa fa-check"></i><b>71</b> The advent of ‘wide, dirty and mixed’ data</a></li>
<li class="chapter" data-level="72" data-path="cross-validating-machine-learning-in-genomics.html"><a href="cross-validating-machine-learning-in-genomics.html"><i class="fa fa-check"></i><b>72</b> Cross-validating machine learning in genomics</a></li>
<li class="chapter" data-level="73" data-path="proliferation-of-discoveries.html"><a href="proliferation-of-discoveries.html"><i class="fa fa-check"></i><b>73</b> Proliferation of discoveries</a></li>
<li class="chapter" data-level="74" data-path="variations-in-the-object-or-in-the-machine-learner.html"><a href="variations-in-the-object-or-in-the-machine-learner.html"><i class="fa fa-check"></i><b>74</b> Variations in the object or in the machine learner?</a></li>
<li class="chapter" data-level="75" data-path="whole-genome-functions.html"><a href="whole-genome-functions.html"><i class="fa fa-check"></i><b>75</b> Whole genome functions</a></li>
<li class="chapter" data-level="76" data-path="propagating-subject-positions.html"><a href="propagating-subject-positions.html"><i class="fa fa-check"></i><b>76</b> Propagating subject positions}</a></li>
<li class="chapter" data-level="77" data-path="propagation-across-human-machine-boundaries.html"><a href="propagation-across-human-machine-boundaries.html"><i class="fa fa-check"></i><b>77</b> Propagation across human-machine boundaries</a></li>
<li class="chapter" data-level="78" data-path="competitive-positioning.html"><a href="competitive-positioning.html"><i class="fa fa-check"></i><b>78</b> Competitive positioning</a></li>
<li class="chapter" data-level="79" data-path="a-privileged-machine-and-its-diagrammatic-forms.html"><a href="a-privileged-machine-and-its-diagrammatic-forms.html"><i class="fa fa-check"></i><b>79</b> A privileged machine and its diagrammatic forms</a></li>
<li class="chapter" data-level="80" data-path="varying-subject-positions-in-code.html"><a href="varying-subject-positions-in-code.html"><i class="fa fa-check"></i><b>80</b> Varying subject positions in code</a></li>
<li class="chapter" data-level="81" data-path="the-subjects-of-a-hidden-operation.html"><a href="the-subjects-of-a-hidden-operation.html"><i class="fa fa-check"></i><b>81</b> The subjects of a hidden operation</a></li>
<li class="chapter" data-level="82" data-path="algorithms-that-propagate-errors.html"><a href="algorithms-that-propagate-errors.html"><i class="fa fa-check"></i><b>82</b> Algorithms that propagate errors</a></li>
<li class="chapter" data-level="83" data-path="competitions-as-examination.html"><a href="competitions-as-examination.html"><i class="fa fa-check"></i><b>83</b> Competitions as examination</a></li>
<li class="chapter" data-level="84" data-path="superimposing-power-and-knowledge.html"><a href="superimposing-power-and-knowledge.html"><i class="fa fa-check"></i><b>84</b> Superimposing power and knowledge</a></li>
<li class="chapter" data-level="85" data-path="ranked-subject-positions.html"><a href="ranked-subject-positions.html"><i class="fa fa-check"></i><b>85</b> Ranked subject positions</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="gradients-as-partial-observers" class="section level1">
<h1><span class="header-section-number">47</span> Gradients as partial observers</h1>
<p>We have some sense of how a function can be configured as an observer, but little sense of how they manage variations. Many optimization techniques rely on differential calculus and particularly the calculus of variations to maximise or minimise the value of a cost function. In fact, loss functions are often chosen on the basis of their differentiability.  One widely used optimisation algorithm called ‘gradient descent’ is quite easy to grasp intuitively. In neural nets and deep learning, gradient descent (or ascent) occurs on an increasingly vast scale.  As in many formulations of machine learning techniques, the framing of the problem is finding the parameters of the model/function that best approximates to the function that generated the data. It optimises the parameters of a model by searching for the maximum or minimum values of the objective function. The algorithm can be written using calculus style notation as:</p>
<span class="math display">\[\begin{equation}
\label{eq:stochastic_gradient_ascent}
Repeat until convergence:
\beta_{j} := \beta_j + \alpha (y_i - h_\beta(x_i))x_{\beta j}
\end{equation}\]</span>
<p>The version of the algorithm shown in algorithm  is called ‘stochastic gradient descent.’ Archaeologically, in presenting such formula, the point is not to read and understand them directly but to characterise the enunciative function that regulates them. Practical understanding would be the point in a machine learning course. Actually reading these formal expressions, and being able to follow the chain of references, and indexical signs that lead away from them in various directions depends very much on the diagrammatic processes described in chapter . Many people who directly use machine learning techniques in industry and science would not often if ever need to make use of such expressions as they build models. They would mostly take them for granted, and simply execute via functions supplied by software libraries (e.g. <code>GradientDescentOptimizer</code> in the <code>TensorFlow</code> library or <code>StochasticGradient</code> in <code>torch</code>).</p>

<p>Given that equation  encapsulates the heart of a major optimisation technique, we might first of all be struck by its operational brevity. This is not an elaborate or convoluted algorithm. As Malley, Mally and Pajevic observe, ‘most of the [machine learning] procedures … are (often) nearly trivial to implement’ <span class="citation">[@Malley_2011, 6]</span>. Note that this expression of the algorithm, taken from the class notes for [Lecture 3] of Andrew Ng’s ‘Machine Learning’ CS229 course at Stanford (<span class="citation">[@Ng_2008j]</span>; see figure ), mixes an algorithmic set of operations with function notation. We see this in several respects: the formulation includes the imperative ‘repeat until convergence’; it also uses the so-called ‘assignment operator’ <span class="math inline">\(:=\)</span> rather than the equality operator <span class="math inline">\(=\)</span>. The latter specifies that two values or expressions are equal, whereas the former specifies that the values on the right hand side of the expression should be assigned to the left. Both algorithmic forms – repeat until convergence, and assign/update values – owe more to techniques of computation than to mathematical abstraction.</p>

<p>In gradient descent, we see functions acting as partial observers. The specification for the gradient descent algorithm brings us to the scene where ongoing transformation of data from irregular volume to plane can be observed. At the heart of this reshaping lies a different mathematical formalism: the , <span class="math inline">\(\frac {\partial }{\partial \beta_j}J(\beta_j)\)</span>. Like all derivatives in calculus, this expression can be interpreted as the rate at which one variable changes in relation to another; that is, as the rate at which the cost function <span class="math inline">\(J(\beta)\)</span> changes with respect to the different values of <span class="math inline">\(\beta\)</span> <a href="#fn47" class="footnoteRef" id="fnref47"><sup>47</sup></a>. Much learning in machine learning pivots on the observation of rates of change of a cost function in relation to its ‘arguments,’ the <span class="math inline">\(j\)</span>-dimensional vector space defined by <span class="math inline">\(\beta\)</span>, the model parameters. The partial derivatives in the gradient descent algorithm observe the direction in which the value of the cost function reduces. Each iteration of the algorithm reduces or increases the parameters <span class="math inline">\(\beta\)</span> of the model in the direction of reduced cost, and perhaps less error.  Importantly, the derivative of a sigmoid (or logistic function) <span class="math inline">\(\sigma(x)\)</span> is given by <span class="math inline">\(\sigma(x)(1-\sigma(x))\)</span>, which means that the partial derivatives of a cost function will be easy to compute. </p>
</div>
<div class="footnotes">
<hr />
<ol start="47">
<li id="fn47"><p>The derivative <span class="math inline">\(\frac {\partial }{\partial \beta_j}J(\beta_j)\)</span> is <em>partial</em> because <span class="math inline">\(\beta\)</span> is a vector <span class="math inline">\(\beta_0, \beta_1 ... \beta_j\)</span>.<a href="gradients-as-partial-observers.html#fnref47">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="observing-costs-losses-and-objectives-through-optimisation.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="the-power-to-learn.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
