{
	"auto_complete":
	{
		"selected_items":
		[
			[
				"Y_",
				"Y_sd"
			],
			[
				"Y",
				"Y_mean"
			],
			[
				"alp",
				"alpha_v"
			],
			[
				"X_",
				"X_norm"
			],
			[
				"grad",
				"gradientDescent"
			],
			[
				"thea",
				"theta_temp"
			],
			[
				"thet",
				"theta_temp"
			],
			[
				"missing",
				"missing_proportion"
			],
			[
				"msing",
				"missing_proportion"
			],
			[
				"missing_prop",
				"missing_proportion"
			],
			[
				"missing_",
				"missing_all_df"
			],
			[
				"study_summ",
				"study_summary"
			],
			[
				"rai",
				"ratio_x"
			],
			[
				"ratio_",
				"ratio_y"
			],
			[
				"uni",
				"unicode"
			],
			[
				"try",
				"try	Try/Except/Finally"
			],
			[
				"sub",
				"sub_dict"
			],
			[
				"cen",
				"center_name"
			],
			[
				"tr",
				"try	Try/Except"
			],
			[
				"sub_",
				"sub_dict"
			],
			[
				"study_su",
				"study_summary_df"
			],
			[
				"query",
				"query_runs"
			],
			[
				"study",
				"study_summary_df"
			],
			[
				"sra",
				"SRAdb"
			],
			[
				"sr",
				"SRA"
			],
			[
				"study_",
				"study_names_clean"
			],
			[
				"go",
				"goldman_towards_2013"
			],
			[
				"top",
				"top_machines"
			],
			[
				"insu",
				"instrument_name"
			],
			[
				"fi",
				"fields_to_use"
			],
			[
				"graph",
				"graph_sra_term"
			],
			[
				"sra_",
				"sra_xml"
			],
			[
				"re",
				"retmode"
			],
			[
				"tab",
				"table_count"
			],
			[
				"data",
				"data-economy"
			],
			[
				"n",
				"ngs_paper"
			],
			[
				"da",
				"data_economy"
			],
			[
				"plot",
				"plot_basic_users"
			]
		]
	},
	"buffers":
	[
		{
			"file": "proposal.rmd",
			"settings":
			{
				"buffer_size": 38818,
				"line_ending": "Unix"
			}
		},
		{
			"file": "/home/mackenza/R/coursera/ml-ng/ex1/computeCost.R",
			"settings":
			{
				"buffer_size": 0,
				"line_ending": "Unix",
				"name": "computeCost <- function(X,Y, theta) {"
			}
		},
		{
			"file": "/home/mackenza/R/coursera/ml-ng/ex1/gradientDescent.R",
			"settings":
			{
				"buffer_size": 0,
				"line_ending": "Unix"
			}
		},
		{
			"file": "/home/mackenza/R/coursera/ml-ng/ex1/ex1.R",
			"settings":
			{
				"buffer_size": 0,
				"line_ending": "Unix",
				"name": "#ex1"
			}
		},
		{
			"file": "/home/mackenza/Documents/notes/rabinow_anthropos_2003.md",
			"settings":
			{
				"buffer_size": 2239,
				"line_ending": "Unix",
				"name": "Rabinow, P. 2003. Anthropos Today. Reflections on"
			}
		},
		{
			"file": "/home/mackenza/Documents/notes/Wilson,_Affect and Artificial Intelligence2009.md",
			"settings":
			{
				"buffer_size": 678,
				"line_ending": "Unix"
			}
		},
		{
			"file": "technique_demos.rmd",
			"settings":
			{
				"buffer_size": 4842,
				"line_ending": "Unix"
			}
		},
		{
			"file": "/home/mackenza/Documents/notes/Whitehead_PR.mdown",
			"settings":
			{
				"buffer_size": 614,
				"line_ending": "Unix",
				"name": "# Whitehead, Process and Reality"
			}
		},
		{
			"file": "/home/mackenza/Documents/notes/WhiteheadModes.mdown",
			"settings":
			{
				"buffer_size": 5354,
				"line_ending": "Unix"
			}
		},
		{
			"file": "techniques.md",
			"settings":
			{
				"buffer_size": 4134,
				"line_ending": "Unix"
			}
		},
		{
			"file": "functions.mdown",
			"settings":
			{
				"buffer_size": 10729,
				"line_ending": "Unix"
			}
		},
		{
			"file": "/home/mackenza/Documents/data_intensive/transformation_soc_sci_oxford_2013/mackenzie_slides_oxford_feb_2013.Rmd",
			"settings":
			{
				"buffer_size": 2789,
				"line_ending": "Unix"
			}
		},
		{
			"file": "build.sh",
			"settings":
			{
				"buffer_size": 57,
				"line_ending": "Unix"
			}
		},
		{
			"file": "pandoc.sh",
			"settings":
			{
				"buffer_size": 247,
				"line_ending": "Unix"
			}
		},
		{
			"file": "animations/grad_desc.R",
			"settings":
			{
				"buffer_size": 1986,
				"line_ending": "Unix"
			}
		},
		{
			"file": "references/refs.bib",
			"settings":
			{
				"buffer_size": 41626,
				"line_ending": "Unix"
			}
		},
		{
			"file": "references/Exported Items.bib",
			"settings":
			{
				"buffer_size": 0,
				"line_ending": "Unix"
			}
		},
		{
			"contents": "# A praxiography of machine learning\n\n## Overview\n\n- reconstruction vs problematization -- Rabinow\n- data: its giveness, abstraction and actuality\n    - the case of iris (science), digits (transactions), spam (media) and kittens -- in that order\n    - the first function from Hastie has to go in\n- practice and writing about practice - developing Mol\n    - observant participation -- developing Wacquant; participating by observing\n    - writing recursively -- developing Kelty w.r.t subjects\n    - implementation as a practice -- the executable paper \n    - James on feeling of transition - and practicing radical empiricism - -see Massumi\n- Implementing machine learning\n    - the case of R -- the scientific\n    - the case of Python -- the business/industry\n    - the case of javascript -- popular culture\n- computation -- psychic engagement - Wilson\n- convergence and learning: for fitting a line; but then for choosing which features to use to learn\n- learning about learning -- coursera, youtube, textbooks, the pile of books (across the gamut)\n    - the Literature - landeker and kelty on treating literature as the informant\n- things to include:\n    - my R books, libraries, papers, competitions, blogs, talking, courses\n        - The number of packages\n        - The history of the language\n        - The increase in popularity\n    - the architecture of CRAN - DONE\n    - knitr/ipython notebooks - executable - some examples from python notebooks\n    - the notion of repl -- read-evaluate-print-loop\n    - folding/plying\n    - Add in Animation and Automation – The Liveliness and Labours of Bodies and Machines Body & Society March 2012 18: 1-46, \n    - machine learning textbooks -- hastie, williams, manning, see document archive\n    - the github version of the book\n\n## Quotes to use\n\n    >The main point of this description is the concept of actuality as something that matters, by reason of its own self-enjoyment, which includes enjoyment of others and transitions towards the future. Whitehead, Modes, 161\n\n    >But this enhancement of energy presupposes that the abstraction is preserved with its adequate relevance to the concrete sense of value-attainment from which it is derived. In this way, the effect of the abstraction stimulates the vividness and depth of the whole of experience. 169\n\n    >Matter-of-fact is an abstraction, arrived at by confining thought to purely formal relations which then masquerade as the final reality. 25\n\n    >A feeling in which the forms exemplified in the datum concern geometrical, straight, and flat loci will be called a 'strain.' In a strain, qualitative elements, other than the geometrical forms, express themselves as qualities implicated in those forms Whitehead,  PR, 310\n\n\n## Introduction -- the data-driven mode\n\nThis chapter stands at the confluence of two broad tendencies shaping what is happening to data today. On the one hand, ways of working with scientific and other data have shifted substantially over the last decade or so due to the growth in open source programming languages and networked platforms. Many scientists use programming languages such as Python and R to do their work, and scientific research, which has long relied on counting and calculation, increasingly organises and processes data more intensively. (Scientific computing languages such as FORTRAN have long underpinned scientific research in various more mathematical fields.) On the other hand, several decades of research and application of statistical and machine learning algorithms in science, government, industry and commerce have gradually developed fairly powerful ways of automating the construction of models that classify and predict events and associations between things, people, processes, etc. Programming and software development has long handled data and algorithms, and conversely, machine learning has always relied on programming. But they are powerfully coalescing in data-driven research and in various domains of data practice in ways. Can we locate anything in the proliferation machine learning that is more than plying data into a tightly woven matrix of commercial-scientific-governmental knowing/surveillance? I don't answer that question directly here, but my basic answer is affirmative. I think we can and we should  explore some of the shifts in practices of working with data that can be observed around data. These practices are both the object of analysis, and in  certain cases, provide support for a shift in the way in which we might carry out research on changes in data practices. \n\n\n## Reading machine learning\n\nThe way that I approach data has been heavily influenced  by a single highly technical and compendious textbook, _Elements of Statistical Learning: Data Mining, Inference, and Prediction_ [@hastie_elements_2009], now in its third edition. Like the online machine learning courses and programming books I discuss below, _Elements of Statistical Learning_  combines statistical techniques with various algorithms to 'learn from data' [@hastie_elements_2009, 1]. The 18 chapters of the book range across various kinds of data (spam email, Californian forest fires, blood pressure measurements, handwritten digit recognition), and various machine learning techniques, methods and algorithms (linear regression, k-nearest neighbours, neural networks, support vector machines, the Google Page Rank algorithm, etc). While these techniques come from different places, in _Elements of Statistical Learning_, they are all framed by a notion of learning based on predictive models. The predictive models in statistical machine learning are  underpinned by a single prediction technique, linear regression, fitting a line to some points, that has been subjected to countless variations, iterations and modifications. The authors of the book, Jeff Hastie, Rob Tibshirani and Jerome Friedman are statisticians working at Stanford and Columbia University. (Statisticians and computer scientists from Stanford University loom large in the world of machine learning, perhaps due to their promixity to Silicon Valley.)\n\nWhile 'learning' is briefly discussed on the first page of _Elements of Statistical Learning_, the book hardly ever returns to the topic explicitly. We can read on page two that a 'learner' in machine learning is a model that predicts outcomes.  The notion of learning in machine learning derives from the field of artificial intelligence. The broad project of artificial intelligence, at least as envisaged in its 1960s-1970s heydey, is today largely regarded as a failure. There is no general, well-rounded artificial intelligence in existence. But in the course of its failure, many interesting problems were generated. [TBA - references on the history of AI] The field of machine learning might be seen as one such offshoot. The so-called 'learning problem' and the theory of learning machines was developed largely by researchers in the 1960-1970s, based on work already done in the 1950s on learning machines such as the perceptron, the  neural network model developed by the psychologist Frank Rosenblatt in the 1950s [@rosenblatt_perceptron:_1958]. Drawing on McCulloch-Pitts model of the neurone, Rosenblatt implemented the perceptron, which today would be called a single-layer neural network on a computer at the Cornell University Aeronautical Laboratory in 1957. As Vladimir Vapnik, a leading machine learning theorist, observes: 'the perceptron was constructed to solve pattern recognition problems; in the simplest case this is the problem of constructing a rule for separating data of two different categories using given examples' [@vapnik_nature_1999, 2]. While computer scientists in artificial intelligence of the time, such as Marvin Minsky and Seymour Papert, were sceptical about the capacity of the perceptron model to distinguish  or 'learn' different patterns [@minsky_perceptron:_1969], later work showed that perceptrons could 'learn universally.' For present purposes, the key point is not that neural networks have turned out several decades later to be extremely powerful algorithms in learning to distinguish patterns, and that intense research in neural networks has led to their ongoing development and increasing sophistication in many 'real world' applications (see for instance, for their use in sciences [@hinton_reducing_2006], or in commercial applications such as drug prediction [@dahl_deep_2012]). Rather, the important point is that it began to introduce learning machines as an ongoing project in which trying to understand what machines can learn, and to predict how they will classify or predict became central concerns. At the same time, and less visibly,  implementations and applications of techniques derived from artificial intelligence and adjacent scientific disciplines became more statistically sophisticated. That is, the theory of machine learning alongside decision theory was interwoven with a set of concepts, techniques and language drawn from statistics. Just as humans, crops, habitats, particles and economies had been previously, learning machines became entwined with statistical methods. Not only in their reliance on the linear model as a common starting point, but in theories of machine learning, statistical terms such as bias, error, likelihood and indeed an increasingly thorough-going probabilistic framing of learning machines emerged. \n\nIn describing the entwined elements of machine learning techniques, I'm not attempting to provide any detailed history of their development. For the most part, I leave controversies about the techniques to one side. Also, in describing these developments, I focus mainly on what happens from the 1980s onwards. Rather than history or controversies, I focus on key methods, and the many configurational shifts associated with their implementations. While many of the machine learning techniques I discuss have much longer lineages (in some cases, runnning back to the 1930s), machine learning techniques begin to circulate much more widely in the 1980s as a result of personal computers, and then in the mid-1990s, the internet. The proliferation of programming languages such as FORTRAN, C, C++, Pascal, then Perl, Java, Python and R, and computational scripting environments such as Matlab, multiplied the paths along which  implementation of machine learning techniques could proceed. It would be impossible for anyone to read the vast machine learning research literature, or follow the the propagation of techniques across domains of science, business and goverment. For instance, a  perceptron that 'learns' the binary logical operation NAND (Not-AND) is  expressed in twenty lines of python code on the Wikipedia 'Perceptron' page [@perceptron_2013]. \n\n```{r perceptron, echo=TRUE, cache=TRUE, message=FALSE, warning=FALSE, comment=NA, size='smallsize', results='markup', engine='python' }\n    \n    threshold = 0.5\n    learning_rate = 0.1\n    weights = [0, 0, 0]\n    training_set = [((1, 0, 0), 1), ((1, 0, 1), 1), ((1, 1, 0), 1), ((1, 1, 1), 0)]\n     \n    def dot_product(values):\n        return sum(value * weight for value, weight in zip(values, weights))\n     \n    while True:\n        print '-' * 60\n        error_count = 0\n        for input_vector, desired_output in training_set:\n            print weights\n            result = dot_product(input_vector) > threshold\n            error = desired_output - result\n            if error != 0:\n                error_count += 1\n                for index, value in enumerate(input_vector):\n                    weights[index] += learning_rate * error * value\n        if error_count == 0:\n            break\n ```\n\nWhile perceptrons and neural networks are the topic of a later chapter, the typical features of this code as the implementation of a machine learning algorithm are the presence of the 'learning rate', a 'training_set,' 'weights', an 'error count', a loop function that  multiplies values ('dot_product'). Some of the terms present in the code bear the marks of the theory of learning machines that we will discuss. But much of the code here is much more familiar, generic programming. It defines variables, sets up data structures (lists of numerical values), checks conditions, loops through statements or prints results. Executing this code (by pasting it into a python terminal) produces several dozen lines of numbers that are initially different to each other, but that gradually converge on the same values (see printout). These numbers are the 'weights' of the nodes of the perceptron as it iteratively learns to recognise patterns in the input values. None of the workings of the perceptron need concern us at the moment. It is a typical machine learning algorithm, almost always included in ML textbooks and usually taught in introductory ML classes.  Perhaps more strikingly than its persistence as an algorithm over half a century, the implementation of the whole algorithm in twenty lines of code on a Wikipedia page suggests the mobility of these methods. What required the resources of a major research engineering laboratory in the 1950s can be re-implemented by a cut and paste operation between wikipedia pages and a terminal window on a laptop in 2013. This is now a familiar observation, and perhaps not very striking at all. But the question of who implements perceptrons or neural networks, and where they implement them today is rather more interesting. \n\n\n## Notion of praxiography: where practices are the features\n\nWhat would we learn by studying the proliferation of implementations of artificial intelligence or machine learning algorithms rather than their history or the controversies associated with them? The question that guides much of this book is how to live with machine learning.  This is a largely affirmative question, rather than a critical one. I am looking for ways of thinking with machine learning, and this has meant learning to do machine learning in some measure. Several general  possibilities present themselves here. \n\nScience studies scholars such as Anne-Marie Mol and John Law have long urged the need to keep practice together with ontology. Towards the beginning of _The Body Multiple: Ontology in Medical Practice_[@mol_body_2003], Mol writes:\n\n    >If it is not removed from the practices that sustain it, reality is multiple. This may be read as a description that beautifully fits the facts. But attending to the multiplicity of reality is also an act. It is something that may be done – or left undone [@mol_body_2003, 6]\n\nMol's work offers a cogent case for developing accounts of what is real steeped in the practices that make it real. Similar affirmations of the sustaining role of practice can be found in many parts of social sciences and humanities. So for the first part, looking at implementations and the flow of implementations is a way of keeping practices in the picture, and therefore, a heuristic for learning reality as multiple. This already suggests that describing machine learning in terms of practices could be an act that attends to their multiplicity. Mol's notion of *praxiography*, a variant on ethnography, as an act of describing practice in the name of preserving their multiple-making value, has particular resonance for work with data, which is itself always heavily entwined in writing and reading practices. Could we praxiographically go into data and machine learning?\n\nA second  and related tack comes from the work of Alfred North Whitehead. Whitehead's work on how abstractions are embodied is highly relevant to thinking about machine learning. While Whitehead is comprehensively critical of certain tendencies in modern science (the fallacy of misplaced concreteness; the reduction of space-time to discrete locations, etc), he is very enthusiastic about the potential for better abstractions. While voiced in terminology that takes some getting used to, the broad affirmative point can be grasped:\n\n    > But this enhancement of energy presupposes that the abstraction is preserved with its adequate relevance to the concrete sense of value-attainment from which it is derived. In this way, the effect of the abstraction stimulates the vividness and depth of the whole of experience. [@whitehead_modes_1958,169]\n\nThe enhancement of energy Whitehead talks about here is concerned with the 'fortunate use of abstractions' (169). Machine learning and  much work with data today can be seen as a form of abstraction (as can much of science). The crucial question for Whitehead, however, is how to abstract well. Like Mol, he advocates keeping abstractions together ('adequate relevance') with something like practice ('the concrete sense of value-attainment from which it is derived'). Like Mol too, the virtue of this bundling together of abstractions with their concrete sense affects the 'whole of the experience.' Abstractions can make experience more vivid or deep if they are aligned and assembled connectively. Here I pin some methodological hope on Whitehead's work as a way to experiment with abstraction as stimulant and enrichment, not as reduction or pale imitation.\n\nA final and again different kind of approach here comes from the Elizabeth Wilson's study, _Affect and Artificial Intelligence_ [@wilson_affect_2010]. Drawing on a combination of psychoanalytic, psychological and archival materials, Wilson discusses the work of key figures in the early history of artificial intelligence such as Alan Turing, Warren McCulloch and Walter Pitts on neural nets, and recent examples of affective computing and robots such as Kismet. The framing of her project is interesting:\n    \n    > Sometimes machines are the very means by which we can stay alive psychically, and they can  just as readily be a means for affective expansion and amplification as for affective attenuation. This is especially the case of computational machines. 30 \n\nUnder what conditions do machines and for present purposes, computational machines, become 'the very means we can stay alive psychically'? She  addresses this question by positing 'some kind of intrinsic affinity, some kind of intuitive alliance between the machinic and the affective, between calculation and feeling' (31), and suggesting that the 'one of the most important challenges will be to operationalize affectivity in ways that facilitate pathways of introjection between humans and machines' (31). Introjection, the process of bringing the world within self is, according to psychoanalytic accounts of subjectivity, crucial to the formation of 'a stable subject position' (25). Wilson envisages introjection of machine processes as a good, not as a failure or attenuation of relation to the world. \n\nThese three takes on how to think about practices, abstraction and machines work on somewhat different levels, but they broadly share an interest or even a commitment to multiple, plural or surprising conjunctions between what people do, what the world is, and the many adjustments, alignments and slippages that connect them through models, abstractions, computation and data. I regard them as  offering guidance or methodological steers on how to work with data, with models and with machine learning somewhat differently. The question is how would one practically explore such insights.\n\n## Mobility of statistical methods in the 1990s: the case of R\n\nNearly all of the examples in _The Elements of Statistical Learning_ are implemented in a single programming language, R. R is a well-known and widely used statistical programming language and environment  [@r_development_core_team_r_2010]. An open source programming language, according to surveys of business and scientific users, R is replacing popular software packages such as SPSS, SAS and Stata as the statistical and data analysis tool of choice for many people in business, government, and sciences ranging from political science to genomics, from quantitative finance to climatology [@rexer_analytics_rexer_2010]. Developed in New Zealand in the mid-1990s, and like many open source software projects, emulating S, a commercialised predecessor developed at AT&T Bell Labs during the 1980s, R is now extremely widely used across life and physical sciences, as well as quantitative social sciences. Many undergraduate and graduate students learn R as a basic tool for statistics. Skills in R are often seen as essential pre-requisite for scientific researchers, especially in the life sciences. Estimates of its number of users range between 250000 and 2 million. Increasingly, R is integrated into commercial services and products (for instance, SAS, a widely used business data analysis system now has an R interface; Norman Nie, one of the original developers of the SPSS package heavily used in social sciences, now leads a business, Revolution, devoted to commercialising R; R is heavily used at Google, at FaceBook, and by quantitative traders in hedge funds, etc.).  R is an interestingly diffuse entity. Some ways of working with data are way more clearly focused on equations and calculation. For instance, in order to pursue number practices in physical sciences and engineering, maybe MATLAB or Mathematica would be better. In business or government, many ways of working with data are more focused on ordering and searching. For instance, in order to the look at the organisation of large aggregates of data, relational databases, query languages, data-centre architectures, and perhaps the techniques of aggregating and disaggregating data en-masse would be worth studying. \n\nWhy then choose R, a statistical programming language, a language developed largely by academic statisticians and research scientists rather than computer scientists, software engineers or hackers? The primary ground is that the development, demonstration, proliferation and adoption of machine learning has occurred in R. The R packages such as 'ElemStatLearn' [@halvorsen_elemstatlearn:_2012], for instance, encapsulates some of the main datasets and methods discussed in _The Elements of Statistical Learning_.  Research articles and texbooks in statistics commonly both use R to demonstrate methods and techniques, and create R packages to distribute the techniques and sample data. For instance, an article published in 2007 by the Princeton computer scientist David Blei on 'correlated topic models' for classification of documents [@blei_correlated_2007] leads to a package 'topicmodels' available from the 'Comprehensive R Archive Network (CRAN)' a few years later [@cran_comprehensive_2010]. CRAN itself is an important infrastructure in the global life of R. \n\n```{r mirrors, echo=TRUE, cache=TRUE, message=FALSE, warning=FALSE, comment=NA, size='smallsize', results='markup' } \n    library(utils)\n    mirrors = getCRANmirrors(all = FALSE, local.only = FALSE)\n    head(mirrors)\n\n```\nA set of `r nrow(mirrors)` mirror sites in `r length(unique(mirrors$Country))` countries distributes the latest R platform and R packages to many different users. These mirror sites are largely run by universities and public institutions, alongside a few commercial users (such as Revolution, a company I discuss in a later chapter). So, in terms of machine learning practice, R is important because it is a staple programming language in statistics research and teaching. It is practically circulated in ways that make it globally available. \n\nR has also attracted  mainstream media attention.  An article in the _New York Times_ in 2009 highlighted its practical importance in data analysis [@vance_data_2009], but at that time the _New York Times_ was heavily promoting its idea of data journalism.  More broadly, R is an evocative object, to use the psychoanalyst Christopher Bollas' term [@bollas_evocative_2009], an object through which run many different ways of thinking. Standing somewhere at the intersection of statistics and computing, modelling and programming, many different disciplines, techniques, domains and actors intersect in R. Bollas suggests that 'our encounter, engagement with, and sometimes our employment of, actual things is a _way_ of thinking' [@bollas_evocative_2009, 92].  R embodies something of plurality of practices of working with measurements, numbers, text, images, models and equations, with techniques for sampling and sorting, with probability distributions and random numbers. It engages immediately, practically and widely with  words, numbers, images, symbols, signals, sensors, forms, instruments and above all abstract forms such as mathematical functions like probability distributions and many different architectural forms (vectors, matrices, arrays, etc), as it employs data. By virtue of thousands of packages that flow across boundaries between nature and culture, between aesthetic, epistemic and pragmatic domain, R embodies a wide-with data economies, cultures, sciences, politics and technologies. Somewhere between calculation and searching, R channels counting and sorting, in the estimation of likelihoods.\n\nHow would we describe the R-based practices in ways that deepen our perception of their plurality, their relevance, and potential amplification of affect? Note that Bollas distinguishes encounter, engagement and employment of things. Employment, in his terms, is less likely to lead become a way of thinking than encounter and engagement. On balance, I'd have to admit that the employment dimension of R dominates.  It is easy today to find an employment-centric view of data practice. The 'chief economist' at Google, Hal Varian in 2009 made a widely quoted prediction about data:\n\n    > The ability to take data — to be able to understand it, to process it, to extract value from it, to visualize it, to communicate it — that’s going to be a hugely important skill in the next decades, not only at the professional level but even at the educational level for elementary school kids, for high school kids, for college kids. Because now we really do have essentially free and ubiquitous data. So the complimentary [sic] scarce factor is the ability to understand that data and extract value from it.” Hal Varian, chief economist at Google: [@mckinsey_&_company_hal_2009]\n\nWhile cited here in a report prepared by the global business consultancy, McKinsey & Co, I have seen this quote or selections from it in many different contexts, ranging from government reports on higher education to the student noticeboard in the statistics department at the university. What Varian presents as an 'important skill'  also has a 'scarce factor':  the 'ability to understand that data.' While Varian presents understanding data as the prelude to 'extract[ing] value from it,' understanding might take different forms, depending on the kind of encounter or engagement that precedes it. \n\nEncounters with R are by no means limited to employment. A somewhat broader framing of the value of data practice can be found associated with the language. Some people – admittedly a small group – explicitly promote R in association with the wider growth of data democracy or open data. I say somewhat, because the promotion of R as a valuable software object is linked to individual entrepreneurship. Norman Nie, the original developer of the widely-used SPSS social science statistics software (see [uprichard_spss_2008]), is a proponent of R. Here is Norman Nie interviewed in _Forbes_, a leading business news magazine, evangelising for R:\n\n    > Everyone can, with open-source R, afford to know exactly the value of their house, their automobile, their spouse and their children--negative and positive  … It's a great power equalizer, a Magna Carta for the devolution of analytic rights [@hardy_power_2010].\n\nWhile strictly speaking referring to R for employment purposes, Nie, the founder of Revolution Analytics, a company that provides software and support services to R users, promotes R as a way of protect individual liberties to know what they own. In the context of the global financial crisis of 2007, it may be that the value of houses became much more problematic, and understandably, harder to know. And the reference to 'spouse and their children' must be flippant. Yet the 'devolution of analytic rights' that Nie speaks of here, even as he frames it in terms of entrepreneurial individualism, is important. While Nie's company  Revolution Analytics promotes the incorporation of  R into  powerful and pervasive business and government prediction systems (such as SAS and IBM's Pure Data [@ibm_ibm_2013]), Nie here points to the problem of 'manipulation and control' of customers associated with just that incorporation. The 'analytic rights' he advocates in the form of R are meant to help individuals counterbalance the power of large scale data analysis conducted by corporations and governments. \n\nNie's advocacy of R points towards a surfeit or overflowing sense of possibility associated with R. It would be possible to cite many other instances of this belief and desire in the potential of R. A good indication of this overflow is the aggregate blog, [R-bloggers](R-bloggers.com). R-bloggers brings together several hundred R-related blogs in one place. The range of topics discussed there on any one day – Merrill-Lynch Bond Return indices, how to run R on an iPhone, using US Department of Agriculture commodity prices, analysing human genetic variation using PLINK/SEQ via R, using the 'Rhipe' (sic!) package to program big data applications on Map-Reduce cloud architectures, doing meta-analysis of phylogenetic trees, etc., etc (R-bloggers 2011) – suggest how widely desires/beliefs in R range. To take just one fairly recent example, 'R Analysis Shows How UK Health System could save £200 million' claims a recent post on the site. While many of the applications, experiments, or techniques reported there can be reduced to employment or to individual uses of R, they also, as we will see, suggest the potential for encounters and engagements. \n    \n\nIf we were to try to describe these encounters and engagements in which R as an actual thing is also a way of thinking, how would we go about it? The act of describing the practices of working with R might, at least this is my hope, be a way of staging an encounter with those aspects of data practice that exceed employment, and that instead do something like vivify experience. But one problem is that there are so many different ways to do things with data. In many cases, data practice begins not from data, but from books, articles and websites. This a fairly banal observation, but acquiring, generating, finding or, to use the term often used by database architects, 'discovering' data is not all that common in practice. It is perhaps far more typical to encounter data in the context of learning how to manipulate, sort, model or display it. These settings are multiple. We could turn to R-blogger's aggregation or the programmer Q&A site [Stackoverflow](http://www.stackoverflow.com), which has many questions tagged with R. There are also many online manuals, guides and tutorials relating to R [@wikibooks_r_2013]. For present purposes, I draw mainly on semi-popular books such as _R in a Nutshell_ [@adler_r_2010], _The Art of R Programming_ [@matloff_art_2011], or _R Cookbook_ [@teetor_r_2011]. These books are not written for academic audiences, although academics often write them and use them in their work. They are largely made up of illustrations and examples of how to do different things with different types of data, and their examples are typically oriented towards business or commercial settings, where, presumably, the bulk of the readers work, or aspire to work. Given a certain predisposition (that is, geekiness), these books and other learning materials can make for enjoyable reading. While they vary highly in quality, it is sometimes pleasing to see  the economical way in which they solve common data problems. This genre of writing about programming specialises in posing a problem and solving it directly. In these settings, learning takes place largely through following or emulating what someone else has done with either a well known dataset (Fisher's 'iris') or a toy dataset generated for demonstration purposes.  The many frictions, blockages and compromises that often affect data practice are largely occluded here in the interests of demonstrating the neat application of specific techniques. Yet are not those frictions, neat compromises and strains around data and machine learning precisely what we need to cleave to praxiographically? In order to demonstrate both the costs and benefits of approaching R through such materials, rather than through ethnographic observation of people using R, it will be necessary to stage encounters here with data that has not been completely transformed or cleaned in the interests of neatly demonstrating the power of a technique. One way to do this is by writing about code while coding. \n\n## Writing code about code: scientific software and executable papers\n\nI've been writing code for decades, mostly sporadically. But writing code was always something that lay at a distance from my academic work.  Only recently, owing mainly to developments in ways of analysing and publishing scientific data, have I found ways to write about code while coding. This looping between writing code and writing about code is the main way that this book has been written. Not all of the code I've written in implementing machine learning models or in reconstructing certain data practices is included in this text, just as not all of the prose text I've written in trying to construct arguments or think about data practices has been included. Much has been cut away and left on the ground. So, like the recipe books, cookbooks, how-tos, tutorials and other documentations I have read, things are heavily tidied up here. At many points in researching the book, I digressed a long way into quite technical domains of statistical inference, probability theory, linear algebra, dynamic models as well as database design and data standards. Much of this exploration is lost on me. And since this is neither a textbook or a how-to cookbook, I don't need to retrace my path through all that. Nevertheless, the several years I have spent  writing about data practice has felt substantially different to any other project by virtue of a strongly recursive invocation of code in text, and text in code, made possible by working on code and text within the the same file, in the same text editor. Switching between writing  R and  Python code (about which I say more below) to retrieve data, to transform it, to produce graphics, to construct models or some kind of graphic image, and within the same file be writing academic prose, is one way to write more praxiographically.\n\nThe mixture of text, code and images depends on an ensemble of software tools that differ somewhat from the usual collection of word processor, bibliographic software, image editor and web browser. In particular, it relies on software packages in R such as the 'knitr' [@xie_knitr_2013; @xie_new_2012,] and in python, the 'ipython' notebook environment [@perez_ipython:_2007]. Both have been developed by scientists and statisticians in the name of 'reproducible research.' These packages are designed to allow a combination of code written in R, python or other programming languages, scientific text (including mathematical formula) and images to be included, and importantly, executed together. In order to do this, they typically combine some form of text formatting or 'markup,' that ranges from very simple formatting conventions (for instance, the 'Markdown' format used in this book is much less complicated than HTML, and uses markup conventions readable as plain text and modelled on email [@gruber_markdown:_2004];) to the highly technical (LaTeX, the de-facto scientific publishing format or 'document preparation system' [@lamport_document_1986] elements of which are also used here to convey mathematical expressions). They add to that blocks of code and inline code fragments that are executed as the text is formatted in order to produce results that are shown in the text or inserted as figures in the text. \n\nThere are different ways of weaving together text, computation and images together. In ipython, a scientific computing project dating from 2005 [@perez_ipython:_2007], and used across a range of scientific settings, interactive visualization and plotting, as well as access to operating system functions are brought together. Especially in using the ipython notebook, where editing text and editing code is all done in the same window, and the results of changes to code can be seen immediately, practices of working with data can be directly woven together with writing about practice. By contrast, knitr composes documents that have been written by interleaving chunks of code and text. When knitr runs, it executes the code and inserts the results (calculations, text, images) in the flow of text. Practically, this means that a text editor, the software used to write code and text, remains somewhat separate from the software that executes the code. While ipython focuses on interactive computation, knitr focuses on bringing together scientific document formatting and computation. From the perspective of praxiography, given that both can include code written in other languages(that is, python code can be processed by knitr, and R code executed in ipython), the differences are not crucially important [^3]. This whole book could have been written using just Python, since Python is a popular general purpose programming language, and many statistical, machine learning and data analysis libraries have been written for Python. Modules such as NumPy, SciPy, Scikit-learn, open-cv or Pandas [TBA : references needed here] mean that almost anything that could be done in R could also be done in Python. But I write using both R and Python in order to highlight the tensions between the more research-oriented R and the practical implementations typical of Python. \n \nWriters are always drawn into what they write about to some extent. So while I have read textbooks on machine learning and statistics, how-to books on data analysis, data-mining and machine learning, as well as myriad online documents, research papers, and help files, the praxiographic contact zone for me has been the attempt to bring the writing of code and writing about code into proximity, indeed to maximise the degree of mixture. Already at various points in this text, mixture of written materials has figured. The question is how this mode of writing answers the methodological injunctions of keeping the practices present (Mol), maintaining the 'concrete sense of value-attainment' (Whitehead) or allowing 'affective expansion' (Wilson). It seems as though, unusually, it brings a world of practices directly into a written text. Rather than simply describing a situation, executable texts form part of the situation. They belong to the situation to the extent that the code in the text invokes processes, or navigates architectures and infrastructures. Executable text is differently performant, but in ways that are not always controllable.  \n\n[^3]:In combining Markdown and LaTex, I also make use of the [Pandoc utility](http://johnmacfarlane.net/pandoc/README.html), a command line tool that converts between different document formats.\n\n\n## Vectoral encounters\n\nWhile there are many different ways to encounter data, let us imagine it happens somewhat by chance, as when walking down a city street we notice something about a building for the first time. Some feature, old or new, attracts our attention. What is happening in such situations? From a psychoanalytic perspective, as Christopher Bollas writes, \n\n> the ability to move freely in the object world, to use its thing-ness as a matrix for thinking-by-action, pivots around whether the aleatory object determines - whether we move on quickly due to the dynamic of our last encounter with the object - or whether we select objects because we are unconsciously grazing: finding food for thought that only retrospectively could be seen to have a logic [@bollas_evocative_2009, 93].\n\nSomething similar happens in encountering data. There are many potentially interesting, relevant or important datasets out there. Our encounters with them are necessarily 'aleatory' due to their extent. Huge public scientific databases across the gamut of disciplines, social media datasets, government statistical data, data from various environmental sensors, financial market and other price data (so much of this!), let alone all the media forms -- images, text, video -- that can be rendered into data, congregate on the internet. Even researchers within specific fields struggle to 'discover' the data  in their field.   \n\nFor instance, via the [World Bank Data catalog](http://data.worldbank.org/developers/data-catalog-api), the World Bank publishes much data on global economic activity. Much of this data can be retrieved from its Application Programmer Interface (API) using R code. The World Bank Data is a thing. How can its thing-ness be used a 'matrix for thinking-by-action'? This data could also be downloaded using a web browser, and then imported into a spreadsheet, but in this case, since I'm interested in how a programming language affects encounters with data, the R code helps us slow down the dynamics of the encounter a bit. The lines of code, unlike a series of mouse movements, pointing, clicking and typing into a spreadsheet, make it possible to track the movements and gestures involved much more closely. Ironically, coding by hand slows down practice in some ways. Sometimes code is criticised for its unreadability or inaccessibility. Actually, in many ways, code makes it possible to follow practice much more closely.\n\n```{r worldbank, echo=TRUE, cache=TRUE, message=FALSE, warning=FALSE, comment=NA, size='smallsize', results='markup' } \n\n    library(RCurl)\n    library(rjson)\n    url = 'http://api.worldbank.org/countries?incomeLevel=OEC&format=json'\n    high_income_countries = fromJSON(getURL(url))\n    countries = unlist(sapply(high_income_countries[[2]], '[', 'name'), use.names=FALSE)\n\n```\n\nThere is an element of retrospective logic in looking at these lines. It still surprises me to see how tersely whole infrastructures can be marshalled in code. I have written lines like the first  five lines of R code shown here many times. They superimpose internet protocols (supplied via the RCurl library), data standards (rjson), internet servers (api.worldbank.org), databases (the 'countries?incomeLevel...' is a database query), data structures and algorithms ('unlist'). In this case they request a list of high-income countries (members of the OECD) from the WorldBank databases. The World Bank database returns a nested list describing `r length(countries)` countries. The list is hardly surprising (`r paste(countries[1:7],collapse=', ')` ... ). But the point of this code is to show something of the economy of movement and reshaping of data associated with R.  In this code, the first line loads an R library (the 'RCurl' package) to make requests to internet servers. The second line loads an R library  (the 'rjson' package) that can process data in the JSON (Javascript Standard Object Notation) format, a data format commonly used by servers to transfer data. Lines 3-4 provide the internet address of the World Bank API, request the data, receive the response, and convert the JSON data into a typical R data structure, a list. The final line selects from the list, which has many sub-lists nested within it, just the names of the countries.  Although this trivial code vignette does not do any of the understanding or extracting of value that Hal Varian propounds, it shows something about the state of data practice. Brevity in code suggests very well travelled paths of practice. In terms of urban architecture, the packages and modules that support this brevity are like escalators that quickly move people between levels and floors in a city.  Like the how-to book with their often just-so demonstrations of problem solving, the compactness or briefness code suggests that many obstacles, detours and blockages have already been encountered by others and dealt with by them. This is not to deny any agency on my part.  I did some 'processing'  here in, for instance, choosing a path into the World Bank datasets via the selection of the query terms (`r cat(url)`). It also includes the re-formatting operations that first transform JSON data into an R list, and then traverse the nested list structure, selecting only the items of data labelled 'name.' These kinds of selection operations, and transformations in the format of data are vital in contemporary data. Whether in the form of well-established relational database query language SQL, in the requests made to one of the legion of APIs that render data on various internet platforms such as Youtube, WorldBank or Pubmed, modifications in the shapes and sorts of data are a constant concern. These include transformations in types of data. \n\nFor instance, numerical data might binned into categories such as high, medium, low or some other form of ranking; text might be split into keywords; the presence or absence of some value might be counted; etc. \n\n```{r world_bank_flat, echo=TRUE, cache=TRUE, message=FALSE, warning=FALSE, comment=NA, size='smallsize', results='markup' }\n\n    countries_table = do.call('rbind', high_income_countries[[2]])\n    df=do.call('rbind', apply(countries_table, MARGIN=2, \n        FUN =unlist, use.names=FALSE))\n    countries_table_complete = t(df)\n    head(df)\n ```\n\nSome of these transformations matter more than others. The further lines of code produce a table from the WorldBank data. Tables, arrays and matrixes -- the rectangular form of data that typically puts one example per row, with different columns representing different measurements, attributes or properties of the examples. In R, such matrixes and dataframes are heavily used for reasons that go deep in machine learning algorithms and data practices more generally. These kinds of table-making operations are often highlighted in books, documentations and tutorials on R since they deeply affected how code runs. In this case,   the code vignette turns the list, with its sublists, into a flat table or matrix. This is a kind of topological transformation since the nested list of countries with their economic indicators has a different spatio-data structure than the table with its `r ncol(countries_table_complete)` columns.  \n\nThe code for these transformations is ugly and difficult to read, and perhaps could be written more cleanly. Nevertheless, at three points  -- the two `do.call` statements, and the `apply` function -- it demonstrates a distinctive aspect of R, and other programming languages designed for data practice (Octave, Matlab, Python's NumPy, or C++ Armadillo): vectorised transformations of data.\n\n```{r vectorisation, echo=TRUE, cache=TRUE, message=FALSE, warning=FALSE, comment=NA, size='smallsize', results='markup' } \n \n     vector1 <- c(0,1,2,3,4,5,6,8,9,10)\n     vector2 <- c(0,1,2,3,4,5,6,8,9,10)\n\n     # looped addition\n     result_looped = vector()\n     for (i in vector1) {\n        result_looped[i] = vector1[i] + vector2[i]\n     }\n     result_looped\n\n     # vectorised addition\n     result_vectorised  <- vector1 + vector2\n     result_vectorised\n\n```\nIn mainstream programming languages, transformations of data are often done using loops and array constructs in which some operation is successively repeated on each element of a data structure. In vectorised languages such as R, transformations of a data structure usually expressed in one line of code that seems to simultaneously work on all the elements of the code. As the widely used _R Cookbook_ puts it, 'many functions [in R] operate on entire vectors, too, and return a vector result' [@teetor_r_2011, 38]. Or as _The Art of R Programming: A Tour of Statistical Software Design_ by Normal Matloff puts it, 'the fundamental data type in R is the _vector_' [@matloff_art_2011, 24], and indeed in R, all data is vector. There are no individual data types,  only varieties of vectors in R. Again, as _R in a Nutshell_  puts the point directly: 'in R, any number that you enter ... is interpreted as a vector' or as 'an ordered collection of numbers' [@adler_r_2010, 17]. The practical difference is illustrated in the code vignette above in which two 'vectors' of numbers are added together, in the first case using a classic `for`-loop construct, and in the second case using an implicitly vectorised arithmetic operation `+`. The difference in speed between adding $1 ... 10$ using a loop or vector arithmetic is completely trivial here, but when millions of numbers are handled arithmetically, these implementation differences significantly affect the work of both programmers and computing machinery.  This simultaneity is only apparent, since somehow the underlying code has to deal with all the individual elements, but vectorised programming languages take advantage of hardward optimisations or carefully-crafted low-level libraries. \n\nWhile the examples of vectorised computation that I have shown are relatively trivial -- turning a nested list into a table, multiplying sequences of numbers -- these vectorised operations are carried out at scale in many domains of contemporary computation. They are, for instance, especially important in computer graphics, where constant transformations of matrices of numbers support animation. They are extensively used in statistical modelling and machine learning, where 'fitting a model' to data is often literally implemented through the multiplication of matrices containing data in order to calculate parameters of a model. So when Brian Massumi writes that 'the space of experience is really, literally, physically a topological hyperspace of transformation' [@massumi_parables_2002, 184], he may have been describing the heavily vectorised operations on which much machine learning, statistical modelling and prediction depend. \n\n## Reshaping data densities\n\nI am describing this style of working with data in vectors not only because they are conducive to statistical computation.  R and other numerical computing environments are not only reliant on vectorised operations for the purposes of efficiency or speed of computation. The vectoral treatment of data taps into and is interwoven the dimensionality of data more generally. Later chapters will discuss various ways in the dimensionality of data fluxes in machine learning.   The dimensieonality of data practice is something that we hardly think of when we think of models, algorithms, predictions and smart devices. But in terms of multiplying matrices, dimensionality both constrains and enables many aspects of the prediction [^1]. Perhaps on the grounds of data dimensionality alone, we should attend to dimensionality practices in  R. \n\nThere is another more praxiographic reason to follow them. Often data is  represented as if it is an homogeneous mass, but in reality it takes many different shapes and has many different _densities_.  I borrow the term 'density' from statistics, where *probability density functions* are often used to describe the distribution of probabilities of different values of a variable.   Data values are spaced out in many different density shapes, depending on how the data has been generated or created. Whatever the starting point (a measuring instrument, a device, a random number generator, etc.), it is quite likely that a particular machine learning, data visualisation or statistical test will need the data to be in specific linear shapes (vectors, arrays, tables, etc), and fit within a certain volume.    The process of re-shaping data for statistical, visual, predictive or even storage purposes, links the concrete situation, the praxiographic contact zone, to more abstract forms I will discuss later, such as models. If, as I have been suggesting, we sought to keep in contact with the practices, or the concrete value-situation, then the reshaping and reflowing of densities matters greatly.   This forming and reforming of data is evidence of  implicated relations. These practices attest to what Whitehead called 'strain': 'a' feeling in which the forms exemplified in the datum concern geometrical, straight, and flat loci will be called a 'strain.' In a strain, qualitative elements, other than the geometrical forms, express themselves as qualities implicated in those forms' [@whitehead_process_1960, 310]. In most data practice, the exemplified forms are straight, flat or geometrical.  Yet many different practices seeks to elicit relations that strain  -- in both senses of the term, to apply force and to filter out -- the data.  Put differently, many data practices working in the name of linear forms effectively trace strain feelings. These feelings are the affective connectors between the abstractions and the concrete-value situations in which data arises. They are vital to the psychic life of data, and the ongoing reality-multiples it figures. \n\nFor example, as a programming language, R is striking for its vectorised `-ply` constructs.  There are several in the core language and many to be found in packages (the popular 'plyr' package; versions of 'ply' can also be found in recent Python data analysis libraries such as Pandas).  These include `apply`, `sapply`, `tapply`, `lapply`, `mapply`. All of the `-ply` constructs have a common feature: they take some collection of things (it may be ordered in many different ways – as a list, as a table, as an array, etc), do something to it, and return something else. This hardly sounds startling. But while most programming languages in common use offer constructs to help deal with collections of things sequentially (for instance, by accessing each element of data set in turn and doing something), R offers ways of dealing with  them all at once. The `-ply` constructs ultimately derive from the functional logic developed by the mathematician Alonzo Church in the 1930s [@church_note_1936; @church_introduction_1996], R presents difficulties for programmers used to so-called procedural programming languages. The functional programming style of applying functions to functions seems strangely abstract. Once you get a feel for them, these -ply constructs they are very convenient to write, and the code runs much faster. Like the vectorised operations described above, they implicitly parallelise operations on data in ways that adapts to the increasingly parallel contemporary chip architectures. The `-ply` operations reduce both data and computational frictions. The real stake in -plying data, however, is not speed but transformation. -Plying makes working with data less like iteration (number, text, table, list, etc), and more like folding a pliable material. Such shifts in feeling for data are very mundane yet crucial to the flow of data.  Plying, in the sense of plying trade, seems much closer to the kinds of work done on data than the figures of data flow or data deluge. \n\nPeople reshape data for different ends. As we will see in later chapters, data can be folded together in order to contain it or reduce its dimensionality. At other times, much work goes into expanding the dimensionality of data in order to find ways of eliciting relations that remain somewhat latent or hidden in it. But for the moment, the question for us is what to make of those practices. How does the folding or plying of sheets of data into different shapes and densities matter to us? One of the stakes in following what happens to vectors, lists, matrixes, arrays, dictionaries, sets, dataframes, series or tuples in data, and particular in predictive machine learning, is to get a sense of how these practices transform a concrete situation into something more abstract. These are the practices of abstraction, and any abstract experience in this domain must process along these lines. [HEREo]\n\n> Almost any programming language you use will have great linear algebra libraries. And they will be high optimised to do that matrix-matrix multiplication very efficiently including taking advantage of any parallelism your computer. So that you can very efficiently make lots of predictions of lots of hypotheses [Ng, lecture 10.50]\n\n\n```{r matrix, echo=TRUE, cache=TRUE, message=FALSE, warning=FALSE, comment=NA, size='smallsize', results='markup' } \n    \n    a_vector = c(1,2,3)\n    a_matrix = matrix(c(1,2,3,4,5,6,7,8,9), nrow=3)\n    matrix_product = a_matrix %*% a_vector\n    \n    cat('Matrix:', a_matrix, '\\n')\n    cat('Vector:', a_vector, '\\n')\n    cat('Matrix times vector:', matrix_product, '\\n')\n\n\n[HERE] -- write about matrix manipulations, their architecture and why they are essential to prediction, and house prices ... \n\n- multiplying\n- commutative\n- associative\n- identity matrix\n- inverse - singular/degenerate\n- transpose\n\n    \n## Datasets as evocative objects at what price?\n\nLet's return to the example of house prices mentioned by Norman Nie. This is possibly the worst example for my case, since house prices seem so webbed into employment, salaries, class position, etc. It will be interesting to see if it is possible to write about predictive modelling of house prices in a way that responds to what Mol, Whitehead and Wilson propose. The house price dataset I use comes from around San Francisco, prior to the global financial crisis, but I will say more about how I encountered this dataset below. The dataset is fairly small and simple. Only the first few lines are shown below. \n\n```{r house_price, echo=TRUE, cache=TRUE, message=FALSE, warning=FALSE, comment=NA, size='smallsize', results='markup' }\n\n    house_prices = read.csv('data/ex1data2.txt', header=FALSE)\n    names(house_prices) = c('size', 'bedrooms', 'price')\n    head(house_prices)\n    summary(house_prices)\n ```\nTwo architectures come together here. On the one hand, around fifty houses  in a city, San Francisco, are described in terms of the number of bedrooms, their total floor area and the price they sold for. This is a very sparse description of urban space, and it reduces the encounter with the psychically rich form of houses/apartments to a cognitive minimum. On the other hand, this description has its own architecture -- the  very familiar form of the table, grid or matrix. Whatever else happens to the data in machine learning, the matrix form pervades much data practice. As we will see below, in modelling this data, certain parts of the matrix will be separated out and treated differently. For the moment, only the bare architecture of the matrix frame concerns us.\n\n## Learning to predict online\n\nI'm not choosing the house price example at random. It is a surprisingly common teaching or demonstration example in machine learning, alongside other iconic datasets including R.A. Fisher's 'iris', the MNIST handwritten digits dataset [@lecun_mnist_2012]  (derived from U.S. National Institute of Standards and Technology (NIST)), or the cat photo dataset discussed in the previous chapter. In the course of writing this book, as well as reading the academic textbooks, popular how-to books, software manuals and blog-how-to posts,  I attended graduate courses in Bayesian statistics, genomic data analysis, data mining, and missing data. Significantly, I also participated in the online machine learning courses and some machine learning competitions [^4]. Typicallym, a house price dataset might be used for teaching purposes, as in Professor Andrew Ng's course 'Machine Learning' CS229 at Stanford (http://cs229.stanford.edu/) and also delivered on [Coursera.org](https://class.coursera.org/ml-003/class/index). These two  courses, and especially CS229, are fairly typical recent expositions of machine learning. In these lectures, a dataset derived from house prices in San Francisco. Given the course was running before 2007, it is likely that this dataset does not reflect the financial crisis that began in 2007 and led to drastic reductions in house prices from 2008 onwards. After sitting through several dozen hours of Ng's online lectures, and attempting  some of the review questions and programming exercises, including implementing well-known algorithms using R, I have some sense of how predictive models are constructed, and what kinds of algorithmic architectures and forms of data are preferred in machine learning. Importantly, these courses have been viewed by many other people. Ng's machine learning course on Coursera (of which he is co-founder, along with Daphne Koller, another machine learning scientist from Stanford), has a typical enrolment of 100,000. Youtube videos of his Stanford University lectures have viewing figures of around 500,000. My participation in these courses was a kind of arduous experiment. Like the hundreds of thousands of other students, I was trying to learn about machine learning. At the same time, however, I was trying to keep open a set of potentials that much of the course was trying to minimise or shutdown, even as instructor's such as Ng from Stanford or Pedro Domingo from the University of Washington sought to convey their excitement about the content of their courses. In both the Coursera  and Stanford courses, Ng uses the house price dataset to demonstrate certain techniques of predictive modelling. \n\nHow would someone with a somewhat transdisciplinary background (undergraduate degrees in science and philosophy; postgraduate in philosophy) learn about machine learning online? I first encountered machine learning in reading recent scientific articles on genomics. Scholars interested in genomics have often discussed the growth of DNA sequence data in contemporary life sciences. They have carefully studied sequence and other biological databases, but less often attended to the ways in which data from those databases has been modelled or used predictively [^2]. Understanding predictive modelling in genomic science is difficult, since both the biology and the statistics are technically challenging. The possibility of reconstructing or implementing genomic models are more limited. Although I have attended genomic data analysis courses, even in these classes, where my classmates where either postgraduate research students or scientists, only a narrow selection of techniques for working with genomic data are covered. The online machine courses are, by contrast, quite generic and the examples are broad-ranging.  \n\nBoth the textbooks and the online university courses on machine learning have a similar structure. They nearly always begin with linear regression models (fitting a line to the data), then move to logistic regression (a way of using line fitting to classify outcomes), and afterwards move to some selection of neural networks, decision trees, support vector machine and clustering algorithms. They add in some decision theory, techniques of optimization, and ways of selecting predictive models (especially the bias-variance tradeoff). Whether in the textbook or the online course, examples such as the house price dataset form part of the learning of machine learning. They differ, however, in one crucial regard. Reading the _Elements of Statistical Learning_ textbook or one of machine learning books written for programmers (for example, _Programming Collective Intelligence_ or _Machine Learning for Hackers_ [@segaran_programming_2007; conway_machine_2012]) does directly imbricate the reader in a machine learning process. By contrast, doing a Coursera course on machine learning brings with it an ineluctable sense of being machine-learned. The learners on Coursera as the target of machine learning. Daphne Koller and Andrew Ng are leading researchers in the field of machine learning. They are also co-founded  the online learning site [Coursera](http://coursera.org).  As experts in machine learning, it is hard to imagine how they would not treat courses as learning problems. And indeed, Daphne Koller sees things this way:\n\n    There are some tremendous opportunities to be had from this kind of framework. The first is that it has the potential of giving us a completely unprecedented look into understanding human learning. Because the data that we can collect here is unique. You can collect every click, every homework submission, every forum post from tens of thousands of students. So you can turn the study of human learning from the hypothesis-driven mode to the data-driven mode, a transformation that, for example, has revolutionized biology. You can use these data to understand fundamental questions like, what are good learning strategies that are effective versus ones that are not? And in the context of particular courses, you can ask questions like, what are some of the misconceptions that are more common and how do we help students fix them? [@koller_daphne_2012]\n\nWhether the turn from 'hypothesis-driven mode to the data-driven mode' has 'revolutionized biology' is debatable (I return to this in a later chapter). And whether or not the data generated by my participation in Coursera's courses on machine learning generates data supports understanding of fundamental questions, that seems also improbable. Nevertheless, the loopiness of this description interests and appeals to me. I learn about machine learning, a way for computer models to optimise their predictions on the basis of 'experience'/data, but at the same time, my learning is learned by machine learners. This is not something that could happen very easily with a printed text, although versions of it happen all the time as teachers work with students on reading texts. \n    \n\n## Python: connectivity of practices\n\n- The place of python amongst languages\n- Python as symptom of industry, business and science more broadly\n- The rise of industrial machine learning\n\n\n```\n\n\n\n## Linear models: Irises, digits, spam and kittens, titanic survivors: iterating through data\n\nThe first full mathematical expression in _The Elements of Statistical Learning_ is shown below: \n> The linear model has been a mainstay of statistics for the past 30 years and remains one of our most important tools. Given a vector of inputs\n$X^T = (X_1 , X_2, . . . , X_p)$, we predict the output $Y$ via the model\np\nY\n = β0 ˆ\n +\n Xj βj ˆ\n .\nj=1\n(2.1)\n[@hastie_elements_2009, 12]\n\n\n## Conclusion\n\n- return to the psychic life and lived abstraction issue\n- data is experience -- some James?\n - Reconstruction and problematization\n     - Dewey on the need for reconstruction\n    - Foucault on problematization\n    - 'Problematic implementation'\n\n## Where does this stuff go?\n\nActually, this is a key problem for me – having just said that desires/beliefs in R travel widely, I'm not sure what I mean by 'widely' or 'travel.' Maybe it would be better to say they that they do not travel very far, but undergo some kind of modulation that reduces data and computational friction (Edwards, 2010), and allows them to slip around the corners that separate things. As a preliminary take on this question, I would say that frictions are handled R in several ways: \n1. Habit formation. Because R is a  programming language, it allow scripts to be written and run very on everything from laptops to cloud computing. In this it differs greatly from statistics software applications such as SPSS, SAS, or STATA that, arguably, change less often and under the control of business or scientific needs. Learning R is learning to read, write and run scripts, rather than how to carry out operations using the standard interfaces of menus, buttons and windows found in more conventional software applications. There is also much about its connection to programming and computing cultures (Unix operating system, open source scripting languages such as Perl, scientific programming languages such as FORTRAN) that mobilises R in a different way to statistics software applications. Scripts are much more fragmented, mobile elements than software applications. Their mutability and malleability means they can make and move data across many different interstices. At the same time, learning to use and write R opens pathways to habit-forming investments. \n2. Believing is seeing. Given that seeing something in data, finding patterns, showing connections or regularities, or locating important or relevant parameters animates all work with data, the endless variations in visualization found in R play an important role. The many forms of visualization it allows are very closely connected to debates, trends, fashions concerning how data should be made visible – as table, scatterplot, network diagram, tag-cloud, histogram, heatmap, map, etc. The associated minutiae of visual grammars also connect R directly into the literary technologies of many scientific, government, and business knowledges. It would be hard to over-estimate the role played by 'low science' graphing and diagramming in some of these domains.  \n3. The latest and greatest. The latest statistical and algorithmic techniques and models developed by statisticians, domain scientists and computer scientists are often published as R packages (or libraries), especially on CRAN, the Comprehensive R Archive Network (CRAN 2010).  The deposit of an R package  - a collection of functions, data structures, data sets or interfaces to data sources – into CRAN often accompanies journal or book publication in certain fields. The quick rollout of techniques in the form of packages means that people resort to R to keep up. To understand what precipitates the data deluge, how people handle it, and what kinds of resistances, we might think more about the different things that become a package. Maybe it would be worth thinking about packages as monads, as world-making inscriptive-perspectives. This question is approached below in the discussion of the 'Bayesian revolution.'\n\nAll of this might indicate how R becomes believable, or how people are prepared to believe in R. But it doesn't say how belief actually is actually worked on in R. Here I think my position could move in two directions – either heading towards an analysis of some localised practices of working with data in R or towards an analysis of how the grounds on which data are made and analysed shift. This is returning to my broader position or hunch that a fairly subtle but important change in what counts as  empirically credible is occurring in many domains. This change troubles the negotiated settlement between beliefs and events achieved in the last 200-300 years of statistical reasoning practice. Any such change is inevitably hard to infer, especially as it is taking place. How could we apprehend something whose forms, practices and position are fluxing? \nWhat counts as data is subject to constant reshaping. Taking data and changing its form through various rearrangements has become a matter of central economic, commercial, political and scientific importance. While data has long been gathered and published in tables, in files and in databases, the injunction to re-use and re-analyse generates re-shaping or 'data munging' imperatives. We could approach these imperatives from the perspective of various discourses (transparency, sharing, accountability, democracy, Mertonian science, etc), or from the perspective of the world of analytics, data-mining and machine learning where data is seen as a valuable material whose inherent patterns promise new forms of economic, aesthetic or epistemic value. All of this deserves further analysis and comment. However, between the strategies of openness or pattern-finding and the data sources lies a terrain where data is re-shaped, transformed and plied into forms and patterns. The practices of plying, multiplying and applying data seem to me crucial in understanding belief in data. \n\n## Notes\n\n[^1]: The dimensionality of data is the topic of a later chapter. There I discuss how machine learning re-dimensions data in various ways, sometimes reducing dimensions and at other times, multiplying dimensions.\n\n[^2]: Machine learning in genomics is also the topic of a later chapter. Recent life sciences are not alone in their resort to computational techniques, but in contrast to commercial, industry or government data practices, it is easier to track how data is transformed, reshaped and re-dimensioned in producing biological knowledges. The scientific publications, the public databases and open source software work together to allow this. \n\n[^4]: I discuss the competitions and forms of machine learning subjectification in a later chapter. \n\n## References\n\n    ",
			"file": "ch1-learning/ch1_praxis.rmd",
			"file_size": 72935,
			"file_write_time": 1373446603000000,
			"settings":
			{
				"buffer_size": 72899,
				"line_ending": "Unix"
			}
		},
		{
			"file": "/media/0031-014A/digital contours_ software materiality_review.md",
			"settings":
			{
				"buffer_size": 2924,
				"line_ending": "Unix"
			}
		}
	],
	"build_system": "Packages/R Tools/R.sublime-build",
	"command_palette":
	{
		"height": 117.0,
		"selected_items":
		[
			[
				"set mark",
				"Set Syntax: Markdown"
			],
			[
				"set am",
				"Set Syntax: Markdown"
			],
			[
				"set",
				"Set Syntax: Markdown"
			],
			[
				"set m",
				"Set Syntax: Markdown"
			],
			[
				"set lat",
				"Set Syntax: LaTeX"
			],
			[
				"mark",
				"Set Syntax: Markdown"
			],
			[
				"set pyth",
				"Set Syntax: Python"
			],
			[
				"Snippet: ",
				"Snippet: knit-chunk"
			],
			[
				"pack",
				"Package Control: Disable Package"
			],
			[
				"set syntaxm",
				"Set Syntax: Markdown"
			],
			[
				"pa",
				"Package Control: Install Package"
			],
			[
				"set syntax py",
				"Set Syntax: Python"
			],
			[
				"pac",
				"Package Control: Install Package"
			],
			[
				"set s",
				"Set Syntax: R"
			],
			[
				"p",
				"Package Control: Install Package"
			],
			[
				"",
				"View: Toggle Side Bar"
			],
			[
				"se",
				"Set Syntax: Markdown"
			],
			[
				"s",
				"Set Syntax: BibTeX"
			],
			[
				"S",
				"Set Syntax: R"
			]
		],
		"width": 449.0
	},
	"console":
	{
		"height": 139.0
	},
	"distraction_free":
	{
		"menu_visible": true,
		"show_minimap": false,
		"show_open_files": false,
		"show_tabs": false,
		"side_bar_visible": false,
		"status_bar_visible": false
	},
	"file_history":
	[
		"/home/mackenza/Documents/data_intensive/book/ch1/ch1_praxis.rmd",
		"/tmp/data_forms/ch1-learning/ch1_praxis.rmd",
		"/home/mackenza/Documents/data_intensive/book/ch1_praxis.md",
		"/home/mackenza/Documents/data_intensive/book/ch1_praxis.rmd",
		"/home/mackenza/Documents/data_intensive/book/knit_all.sh",
		"/var/tmp/kdecache-mackenza/krun/19894.0.international_collaboration.shtml",
		"/home/mackenza/R/coursera/ml-ng/ex1/gradientDescent.R",
		"/home/mackenza/R/coursera/ml-ng/ex1/gradientDescent.m",
		"/home/mackenza/R/coursera/ml-ng/ex1/computeCost.m",
		"/home/mackenza/R/coursera/ml-ng/ex1/ex1.m",
		"/home/mackenza/R/coursera/ml-ng/ex1/ex1data1.txt",
		"/home/mackenza/Documents/notes/Whitehead_science_mw.mdown",
		"/home/mackenza/bank.txt",
		"/home/mackenza/.cache/.fr-k5nMam/ghProjectInfo2013-Feb.txt",
		"/home/mackenza/Documents/data_intensive/book/knit_all.R",
		"/home/mackenza/Documents/data_intensive/book/proposal.md",
		"/home/mackenza/Documents/notes/hayles_my_mother_2005.mdown",
		"/home/mackenza/Dropbox/epistle/Data forms.txt",
		"/home/mackenza/Desktop/jss725.Rnw",
		"/home/mackenza/Documents/notes/milleplateux.md",
		"/home/mackenza/Desktop/jss725-tikzDictionary",
		"/home/mackenza/Documents/data_intensive/book/template.latex",
		"/home/mackenza/Documents/data_intensive/book/references/Exported Items.bib",
		"/home/mackenza/Documents/data_intensive/book/references/refs.bib",
		"/home/mackenza/.cache/.fr-BlOcAR/2012-04-08-9.json",
		"/home/mackenza/Documents/google_analytics_spet2012/google.sublime-workspace",
		"/home/mackenza/Documents/notes/dewey_reconstruction_1957.odt",
		"/home/mackenza/Documents/data_intensive/lse_cambridge_markets_algorithms/markets_algorithms.Rmd",
		"/home/mackenza/Documents/data_intensive/book/Sources.md",
		"/home/mackenza/Documents/data_intensive/book/build.sh",
		"/home/mackenza/Documents/data_intensive/book/pandoc.sh",
		"/home/mackenza/Desktop/drawing-1.svg",
		"/home/mackenza/Documents/data_intensive/book/markets_algorithms.Rmd",
		"/home/mackenza/R/igem/igem.sublime-workspace",
		"/home/mackenza/Documents/data_intensive/book/proposal2.mdown",
		"/var/tmp/kdecache-mackenza/krun/8479.0.nbt.2495.html",
		"/home/mackenza/Documents/data_intensive/ngs_data_topography/ngs_paper/test.md",
		"/home/mackenza/Documents/data_intensive/ngs_data_topography/ngs_paper/test.rmd",
		"/home/mackenza/R/tips.R",
		"/home/mackenza/R/ngs/data/xml/study/SRP002001.xml",
		"/home/mackenza/Documents/data_intensive/ngs_data_topography/ngs_paper/latex.template",
		"/home/mackenza/Documents/data_intensive/ngs_data_topography/ngs_paper/references/Exported Items.bib",
		"/home/mackenza/Documents/data_intensive/ngs_data_topography/ngs_paper/ngs_metacommunities.md",
		"/home/mackenza/Documents/data_intensive/ngs_data_topography/ngs_paper/ngs_metacommunities.pdf",
		"/home/mackenza/R/ngs/data/xml/study/SRP006001.xml",
		"/home/mackenza/R/ngs/data/xml/study/ERP002001.xml",
		"/home/mackenza/R/ngs/data/xml/study/ERP001001.xml",
		"/home/mackenza/R/ngs/convert_xml.py",
		"/home/mackenza/R/ngs/download_ebi_study_info.py",
		"/home/mackenza/R/ngs/data/xml/study/SRP003001.xml",
		"/home/mackenza/R/ngs/data/xml/study/ERP000001.xml",
		"/home/mackenza/R/ngs/data/xml/study/SRP020001.xml",
		"/home/mackenza/R/ngs/data/xml/study/DRP002001.xml",
		"/home/mackenza/R/ngs/hiseq_storage.R",
		"/home/mackenza/Documents/data_intensive/ngs_data_topography/ngs_paper/build.sh",
		"/home/mackenza/R/ngs/ensemble_explore.R",
		"/home/mackenza/R/ngs/ncbi_explore.R",
		"/home/mackenza/R/ngs/gold_db.R",
		"/home/mackenza/R/ngs/download_ebi_submission_info.py",
		"/home/mackenza/Documents/data_intensive/ngs_data_topography/ngs_paper/knit.R",
		"/home/mackenza/Documents/data_intensive/ngs_data_topography/ngs_paper/pandoc.sh",
		"/home/mackenza/Desktop/knitr-book-master/09-cache.md",
		"/home/mackenza/R/ngs/dbGap_analysis.R",
		"/home/mackenza/R/ngs/centers.Rmd",
		"/home/mackenza/R/ngs/data/birmingham_stats.html",
		"/home/mackenza/Downloads/sequence.asn1",
		"/home/mackenza/Dropbox/data-economy/ngs_paper/template.latex",
		"/home/mackenza/Dropbox/data-economy/ngs_paper/latex.template",
		"/home/mackenza/Dropbox/data-economy/ngs_paper/pandoc_format_paper.sh",
		"/home/mackenza/.config/sublime-text-2/Packages/User/Default (Linux).sublime-keymap",
		"/home/mackenza/.config/sublime-text-2/Packages/Default/Default (Linux).sublime-keymap",
		"/home/mackenza/.config/sublime-text-2/Packages/User/Preferences.sublime-settings",
		"/home/mackenza/.config/sublime-text-2/Packages/User/Markdown.sublime-settings",
		"/home/mackenza/R/ngs/eagle_analysis.R",
		"/home/mackenza/.config/sublime-text-2/Packages/SublimeREPL/SublimeREPL.sublime-settings",
		"/home/mackenza/R/ngs/sra_run_analysis.Rmd",
		"/home/mackenza/Dropbox/data-economy/ngs_paper/ngs_data_methods.md",
		"/home/mackenza/Dropbox/data-economy/DSR/ebi_visit_1nov2012.txt",
		"/home/mackenza/.config/sublime-text-2/Packages/User/syntheticbiology.sublime-snippet",
		"/home/mackenza/.config/sublime-text-2/Packages/User/sbi.sublime-snippet",
		"/var/tmp/kdecache-mackenza/krun/17104.0.projects",
		"/home/mackenza/.config/sublime-text-2/Packages/User/SideBarEnhancements/Open With/Side Bar.sublime-menu",
		"/home/mackenza/.config/sublime-text-2/Packages/WordCount/WordCount.py",
		"/home/mackenza/.config/sublime-text-2/Packages/SideBarEnhancements/Side Bar.sublime-settings",
		"/home/mackenza/.config/sublime-text-2/Packages/User/SmartMarkdown.sublime-settings",
		"/home/mackenza/.config/sublime-text-2/Packages/IPython Integration/README.md",
		"/home/mackenza/R/ngs/sra_run_analysis.md",
		"/home/mackenza/Dropbox/data-economy/ngs_paper/ngs_data_methods.rmd",
		"/home/mackenza/.config/sublime-text-2/Packages/User/R.sublime-settings",
		"/home/mackenza/.config/sublime-text-2/Packages/R Tools/R.sublime-build",
		"/home/mackenza/Dropbox/data-economy/ngs_paper/ngs_data_methods.txt",
		"/home/mackenza/Dropbox/data-economy/ngs_paper/paper/ngs_data_methods.txt",
		"/home/mackenza/.config/sublime-text-2/Packages/SmartMarkdown/Default.sublime-keymap",
		"/home/mackenza/Dropbox/ngs_paper/paper/ngs_data_methods.rmd",
		"/home/mackenza/Dropbox/ngs_paper/paper/references/ngs.bib",
		"/home/mackenza/Dropbox/ngs_paper/paper/ngs_data_paper.sublime-workspace",
		"/home/mackenza/Dropbox/data-economy/paper/ngs_data_methods.md",
		"/home/mackenza/Dropbox/data-economy/paper/references/ngs.bib",
		"/home/mackenza/Dropbox/data-economy/paper/pandoc_format_paper.sh",
		"/home/mackenza/Dropbox/ngs_paper/build/pandoc.sublime-build",
		"/home/mackenza/.config/sublime-text-2/Packages/User/untitled.sublime-snippet",
		"/home/mackenza/Dropbox/data-economy/paper/4S EASST talk.docx",
		"/home/mackenza/Desktop/build.R",
		"/home/mackenza/Desktop/test3.md",
		"/home/mackenza/Desktop/test4.R",
		"/home/mackenza/Desktop/test2.mdown",
		"/home/mackenza/Desktop/test1.md",
		"/home/mackenza/R/montecarlo/data/refs.bib",
		"/home/mackenza/Desktop/test6.rmd",
		"/home/mackenza/R/ngs/data/geo_schema.sql",
		"/home/mackenza/Desktop/test.sublime-project",
		"/media/0031-014A/Documents/supervision/xaroula/chapter 2 attempt.doc",
		"/home/mackenza/Desktop/scrivener-manual-a4.pdf"
	],
	"find":
	{
		"height": 35.0
	},
	"find_in_files":
	{
		"height": 0.0,
		"where_history":
		[
		]
	},
	"find_state":
	{
		"case_sensitive": true,
		"find_history":
		[
			"library",
			"white",
			"X_3",
			"X_2",
			"colmeans",
			"gradientDescent_two_dim",
			"    >",
			"print",
			"The recent",
			"technique_demos",
			"Whitehead",
			"savage",
			"savag",
			"jss",
			"logo",
			"month",
			"biagioli_situated_1999",
			"munster",
			"latour_drawing_1990",
			"carlsson_topology_2009",
			"carl",
			"lury_introduction_2012",
			"conn",
			"Law",
			"SRA",
			"Law",
			" 'months')",
			"'months'",
			"sra_con",
			"runs",
			"sapply",
			"runs",
			"samples",
			"studies",
			"experiments",
			"Ankeny",
			"especially",
			"ncbi_stat",
			"img/",
			"SRA",
			"')\n",
			"ERA",
			"\n",
			"DRA",
			"subm$",
			"subm",
			"submission",
			"ena",
			"Cochrane",
			"center_name",
			"/>\n",
			"center_name",
			"submission",
			"subsets",
			"# ",
			"sum",
			"stud_acc",
			"samples",
			".df",
			"study_summary",
			"--",
			"study_names",
			"1000",
			"microbiome",
			"')",
			"100",
			"'tumour'",
			"--",
			"quack",
			"Loman",
			"ngs_data_methods",
			"Times",
			"  \n",
			" ",
			"plot_basic_users",
			"Wateron",
			"Waterson",
			"Latour",
			"of",
			"edwards",
			"672",
			"Coles",
			"coles",
			"##",
			"metadata",
			"Metadata",
			"ADRIAN",
			"Adrian",
			"ADRIAN",
			"Adrian",
			"------------------------------------------------------------",
			"Adrian",
			"ADRIAN",
			"Adrian",
			"---------------------------------\n",
			"---------------------------------",
			"Discussion"
		],
		"highlight": true,
		"in_selection": false,
		"preserve_case": false,
		"regex": false,
		"replace_history":
		[
			"—"
		],
		"reverse": false,
		"show_context": true,
		"use_buffer2": true,
		"whole_word": false,
		"wrap": true
	},
	"groups":
	[
		{
			"selected": 18,
			"sheets":
			[
				{
					"buffer": 0,
					"file": "proposal.rmd",
					"settings":
					{
						"buffer_size": 38818,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"syntax": "Packages/Markdown/Markdown.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 114.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 1,
					"file": "/home/mackenza/R/coursera/ml-ng/ex1/computeCost.R",
					"settings":
					{
						"buffer_size": 0,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"auto_name": "computeCost <- function(X,Y, theta) {",
							"syntax": "Packages/R/R.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 2,
					"file": "/home/mackenza/R/coursera/ml-ng/ex1/gradientDescent.R",
					"settings":
					{
						"buffer_size": 0,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"syntax": "Packages/R/R.tmLanguage",
							"tab_size": 4,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 528.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 3,
					"file": "/home/mackenza/R/coursera/ml-ng/ex1/ex1.R",
					"settings":
					{
						"buffer_size": 0,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"auto_name": "#ex1",
							"syntax": "Packages/R/R.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 1004.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 4,
					"file": "/home/mackenza/Documents/notes/rabinow_anthropos_2003.md",
					"settings":
					{
						"buffer_size": 2239,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"auto_name": "Rabinow, P. 2003. Anthropos Today. Reflections on",
							"syntax": "Packages/Markdown/Markdown.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 5,
					"file": "/home/mackenza/Documents/notes/Wilson,_Affect and Artificial Intelligence2009.md",
					"settings":
					{
						"buffer_size": 678,
						"regions":
						{
						},
						"selection":
						[
							[
								53,
								53
							]
						],
						"settings":
						{
							"syntax": "Packages/Markdown/Markdown.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 6,
					"file": "technique_demos.rmd",
					"settings":
					{
						"buffer_size": 4842,
						"regions":
						{
						},
						"selection":
						[
							[
								4189,
								4189
							]
						],
						"settings":
						{
							"syntax": "Packages/Markdown/Markdown.tmLanguage",
							"translate_tabs_to_spaces": false
						},
						"translation.x": 0.0,
						"translation.y": 2346.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 7,
					"file": "/home/mackenza/Documents/notes/Whitehead_PR.mdown",
					"settings":
					{
						"buffer_size": 614,
						"regions":
						{
						},
						"selection":
						[
							[
								34,
								34
							]
						],
						"settings":
						{
							"auto_name": "# Whitehead, Process and Reality",
							"syntax": "Packages/Markdown/Markdown.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 8,
					"file": "/home/mackenza/Documents/notes/WhiteheadModes.mdown",
					"settings":
					{
						"buffer_size": 5354,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"syntax": "Packages/Markdown/Markdown.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 9,
					"file": "techniques.md",
					"settings":
					{
						"buffer_size": 4134,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"syntax": "Packages/Markdown/Markdown.tmLanguage",
							"translate_tabs_to_spaces": false
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 10,
					"file": "functions.mdown",
					"settings":
					{
						"buffer_size": 10729,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"syntax": "Packages/Markdown/Markdown.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 11,
					"file": "/home/mackenza/Documents/data_intensive/transformation_soc_sci_oxford_2013/mackenzie_slides_oxford_feb_2013.Rmd",
					"settings":
					{
						"buffer_size": 2789,
						"regions":
						{
						},
						"selection":
						[
							[
								2162,
								2162
							]
						],
						"settings":
						{
							"syntax": "Packages/Markdown/Markdown.tmLanguage",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 467.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 12,
					"file": "build.sh",
					"settings":
					{
						"buffer_size": 57,
						"regions":
						{
						},
						"selection":
						[
							[
								57,
								57
							]
						],
						"settings":
						{
							"syntax": "Packages/ShellScript/Shell-Unix-Generic.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 13,
					"file": "pandoc.sh",
					"settings":
					{
						"buffer_size": 247,
						"regions":
						{
						},
						"selection":
						[
							[
								12,
								12
							]
						],
						"settings":
						{
							"syntax": "Packages/ShellScript/Shell-Unix-Generic.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 14,
					"file": "animations/grad_desc.R",
					"settings":
					{
						"buffer_size": 1986,
						"regions":
						{
						},
						"selection":
						[
							[
								20,
								20
							]
						],
						"settings":
						{
							"syntax": "Packages/R/R.tmLanguage",
							"tab_size": 4,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 15,
					"file": "references/refs.bib",
					"settings":
					{
						"buffer_size": 41626,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"syntax": "Packages/LaTeX/Bibtex.tmLanguage",
							"translate_tabs_to_spaces": false
						},
						"translation.x": 0.0,
						"translation.y": 10056.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 16,
					"file": "references/Exported Items.bib",
					"settings":
					{
						"buffer_size": 0,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"syntax": "Packages/LaTeX/Bibtex.tmLanguage",
							"translate_tabs_to_spaces": false
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 17,
					"file": "ch1-learning/ch1_praxis.rmd",
					"settings":
					{
						"buffer_size": 72899,
						"regions":
						{
						},
						"selection":
						[
							[
								56253,
								56253
							]
						],
						"settings":
						{
							"syntax": "Packages/Markdown/Markdown.tmLanguage",
							"tab_size": 4,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 14632.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 18,
					"file": "/media/0031-014A/digital contours_ software materiality_review.md",
					"settings":
					{
						"buffer_size": 2924,
						"regions":
						{
						},
						"selection":
						[
							[
								2924,
								2924
							]
						],
						"settings":
						{
							"syntax": "Packages/Markdown/Markdown.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"type": "text"
				}
			]
		}
	],
	"incremental_find":
	{
		"height": 34.0
	},
	"input":
	{
		"height": 38.0
	},
	"layout":
	{
		"cells":
		[
			[
				0,
				0,
				1,
				1
			]
		],
		"cols":
		[
			0.0,
			1.0
		],
		"rows":
		[
			0.0,
			1.0
		]
	},
	"menu_visible": true,
	"output.exec":
	{
		"height": 142.0
	},
	"replace":
	{
		"height": 64.0
	},
	"save_all_on_build": true,
	"select_file":
	{
		"height": 0.0,
		"selected_items":
		[
		],
		"width": 0.0
	},
	"select_project":
	{
		"height": 500.0,
		"selected_items":
		[
			[
				"",
				"/home/mackenza/R/metacommunities/metacommunities.sublime-project"
			]
		],
		"width": 380.0
	},
	"show_minimap": true,
	"show_open_files": true,
	"show_tabs": true,
	"side_bar_visible": true,
	"side_bar_width": 338.0,
	"status_bar_visible": true
}
