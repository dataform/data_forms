{
	"auto_complete":
	{
		"selected_items":
		[
			[
				"tit",
				"titanic_train_r"
			],
			[
				"titanic",
				"titanic_test"
			],
			[
				"cofield",
				"cofield_m	statement"
			],
			[
				"fil",
				"fill_diagonal	function"
			],
			[
				"bet",
				"between_central"
			],
			[
				"cow",
				"coword_net	statement"
			],
			[
				"graph",
				"Graph"
			],
			[
				"netwo",
				"networkx"
			],
			[
				"tita",
				"titanic_rp"
			],
			[
				"iris",
				"iris_tree"
			],
			[
				"ci",
				"citing_refs"
			],
			[
				"ref_",
				"ref_other	statement"
			],
			[
				"topic",
				"topics"
			],
			[
				"s",
				"savefig	function"
			],
			[
				"co",
				"coword_m	statement"
			],
			[
				"start",
				"start_year"
			],
			[
				"t",
				"topics"
			],
			[
				"map",
				"map_sorted	statement"
			],
			[
				"keywo",
				"keyword"
			],
			[
				"c",
				"cum	Cummulative"
			],
			[
				"in",
				"index	None"
			],
			[
				"ar",
				"article_count"
			],
			[
				"ke",
				"key1"
			],
			[
				"key",
				"key_count"
			],
			[
				"dataframe",
				"DataFrame"
			],
			[
				"i",
				"index	param"
			],
			[
				"retur",
				"Returns"
			],
			[
				"Y_",
				"Y_sd"
			],
			[
				"Y",
				"Y_mean"
			],
			[
				"alp",
				"alpha_v"
			],
			[
				"X_",
				"X_norm"
			],
			[
				"grad",
				"gradientDescent"
			],
			[
				"thea",
				"theta_temp"
			],
			[
				"thet",
				"theta_temp"
			],
			[
				"missing",
				"missing_proportion"
			],
			[
				"msing",
				"missing_proportion"
			],
			[
				"missing_prop",
				"missing_proportion"
			],
			[
				"missing_",
				"missing_all_df"
			],
			[
				"study_summ",
				"study_summary"
			],
			[
				"rai",
				"ratio_x"
			],
			[
				"ratio_",
				"ratio_y"
			],
			[
				"uni",
				"unicode"
			],
			[
				"try",
				"try	Try/Except/Finally"
			],
			[
				"sub",
				"sub_dict"
			],
			[
				"cen",
				"center_name"
			],
			[
				"tr",
				"try	Try/Except"
			],
			[
				"sub_",
				"sub_dict"
			],
			[
				"study_su",
				"study_summary_df"
			],
			[
				"query",
				"query_runs"
			],
			[
				"study",
				"study_summary_df"
			],
			[
				"sra",
				"SRAdb"
			],
			[
				"sr",
				"SRA"
			],
			[
				"study_",
				"study_names_clean"
			],
			[
				"go",
				"goldman_towards_2013"
			],
			[
				"top",
				"top_machines"
			],
			[
				"insu",
				"instrument_name"
			],
			[
				"fi",
				"fields_to_use"
			],
			[
				"sra_",
				"sra_xml"
			],
			[
				"re",
				"retmode"
			],
			[
				"tab",
				"table_count"
			],
			[
				"data",
				"data-economy"
			],
			[
				"n",
				"ngs_paper"
			],
			[
				"da",
				"data_economy"
			],
			[
				"plot",
				"plot_basic_users"
			]
		]
	},
	"buffers":
	[
		{
			"file": "proposal.rmd",
			"settings":
			{
				"buffer_size": 38966,
				"line_ending": "Unix"
			}
		},
		{
			"file": "ch0_introduction/ch0_introduction.rmd",
			"settings":
			{
				"buffer_size": 2388,
				"line_ending": "Unix"
			}
		},
		{
			"file": "ch1_learning/ch1_praxis.rmd",
			"settings":
			{
				"buffer_size": 74038,
				"line_ending": "Unix"
			}
		},
		{
			"file": "ch2_curves/ch2_curves_function.rmd",
			"settings":
			{
				"buffer_size": 74379,
				"line_ending": "Unix"
			}
		},
		{
			"contents": "```{r echo=FALSE} \nlibrary(knitr)\nopts_chunk$set(results='asis', message=FALSE, warnings=FALSE, cache=TRUE)\n```\n\n#  Movements in machine learning\nDimensional exuberance and pattern recognition in movement\n\n## overview\n\n### recap and intro\n- recap of vector space, function and the sheer proliferation of algorithms: some basic intuitions - -cut a space; embed the space in a higher dimensional context; \n- return to vector space as the guiding model -- both critical of this -- it linearises but is tricked out in various ways\n- pattern discovery -- pathetic or worthwhile?\n\n### decision trees\n\n- why decision trees matter\n- how decision trees were renovated\n- how decision trees multiplied -- across fields; amongst themselves -- weak learners\n\n### neural networks\n\n- the perceptron reconceived\n- neuralisation as the lure of learning\n\n### svms\n\n\n- recapitulating LDA\n- the high dimensionality\n\n### main arguments\n\n- is there any possibility of an affirmative relationship to machine learning? Under what conditions? -- return to the ones I set out in ch1: Rabinow, Whitehead, Wilson, etc. Now want to ground those in some particular techniques that cut/include the world in different ways\n- so many techniques to choose from: can only choose something whose specificity itself configures the literature, the field of practice, the knowledge economy in its own ways. \n    - choosing 3 techniques that include many other techniques in them somehow\n        - random forests include many decision trees - 60-70s\n        - neural networks include many logistic regressions and perceptrons - 50s\n        - svms include many different dimensions -- basis functions + inner product (avoiding calculating too much); also Fisher's LDA - 30s\n- see that including as countenancing alternatives -- and hence flashes of mentality into algorithms\n- That is, to take the SVM  or NN or the Decision Tree seriously is already to understand the world in a particular way; each of them are monadologically potent; they express a world. \n- What we can do with this grip on the world? The idea of relaying it -- pursuing motion through experiments with forms of movement through scale and subjectivity\n	- implementation as experiment in forms of movement -- the inner product, \n- How do they express/contain/grip the world?\n	- critical argument: incredibly rigid approach in some ways to flux and change; what do we do with that rigidity?\n	- extrinsic vs intrinsic structure: link to Whitehead can also use the shapeofdata stuff; -- key point -- how the vector space is reshaped; or not. Or the extraordinary efforts to get the vector space to accommodate differences\n	- classification as the key problem: linear separability vs non-linear separability; strict splits vs shades of difference\n	- making things linear in high dim: good or bad?  15 dim patterns \n- \n\n### to put in\n- recap on functions, gradient descent, optimization and regression -- what do they do?\n- curse of dimensionality -- discussion from LSE paper, and from Warwick paper;\n- Warwick discussion on trees\n- Friedman's role in trees\n- the local vs global structure discussion - Hastie 19-20 onwards\n- generative vs disciminative models\n- situated all three algorithms by reference to adjacent work in humanities -- the trace/mark in deconstruction for handwriting recognition -- neural nets; the assemblage -- smooth space for svm; decision tree-random forest\n\n### to do \n- implement a neural network by going through Ng lectures or Hinton\n- implement a svm - using Ng?\n\n### quotes to use: theory\n\n> The process that I am currently engaged in and seeking to name  ... concerns the emergence of form as a process, includes in an essential manner claims to say or see something true. The process that concerns me is the one in which such \"knowledge-things\" are being assembled 85 Rabinow\n\n>it follows that if one's object is an anthropological account of a problematization, then one's informants will differ from each other. The challenge lies in finding an experiential and experimental site that would provide a contemporary instance. Rabinow, 87\n\n>USED: I advocate pursuing in our thought and writing something like the motion, through different scales and different subject positions. ... Such movement is easy to initiate and hard to master. Yet I firmly believe that in the actual conjuncture of things, it is a paramount challenge for philosophy and the human sciences to experiment with forms that will be, if not fully adequate to, at least cognizant of, the need for such movement through scale and subjectivity. Rabinow, 135-6.\n>\n>Perhaps our knowledge is distorted unless we can comprehend its essential connection with happenings which involve spatial relationships of fifteen dimensions. W, MoT, 78\n>\n>The sharp-cut scientific classifications are essential for scientific method. But they are dangerous for philosophy. Such classification hides the truth that the different modes of natural existnece shade off into each other. W, MoT, 215\n>\n>\n>notion of pattern involves the concept of different modes of togetherness MoT, 195-6\n>\n>Thus beyond all questions of quantity, there lie questions of pattern, which are essential for the understanding of nature. Apart from a presupposed pattern, quantity determines nothing. Indeed quantity itself is nothing other than analogy of functions within analogous patterns W, MoT, 195\n>\n>My unity -- which is Descartes' \"I am\" --is my process of shaping this welter of material into a consistent pattern of feelings.  W, MoT, 228\n>\n>new function estimation methods have been created where a high dimensionality of the unknown function does not always require a large number of observations in order to obtain a good estimate. The new methods control generalization using capacity factors that do not necessarily depend on the dimensionality of the space vii Vapnik\n>\n>The kernel trick is another commonly used technique to solve linearly inseparably problems. This issue is to define an appropriate kernel function based on the _inner product_ between the given data, as a nonlinear transformation of data from the input space to a feature space with higher (even infinite) dimension in order to make the problems linearly separable. The underlying justification can be found in _Cover's theorem_ on the separability of patterns; that is, a complex pattern classification problem case in a high-dimensional space is _more likely_ to be linearly separable than in a low dimensional space. Wu, 42\n>\n>\n>To repeat the precarious political pragmatic hope I presented, it is only if we become able, as philosophers, to put scientific achievements on the same plane of immanence together with other diverging conventions, each with its demanding definition of what matters, that we can stop poisoning this hope, that we can share, instead, the pragmatic concern for the itinerant process of creation of new ‘‘it works’’ as they mark the process of empowerment of new minorities, with new actively diverging ‘‘habits’’ that must be celebrated each time as something new entering the world and indeed as modifying it. Stengers, 2005, 162\n>\n\n### main examples/illustrations & materials to draw on \n\nsupport vector machine -- the Russian connection; the text classification projects\n\n-- Ng's lectures  -- go through notes on this\n-- Vapknik\n-- the trajectories of neural networks and support vector machines in the literature\n\nneural  network -- the McCulloch-Pitts connection -- heather what's her name on cat recognition; -- the autonomous vehicles;  my camera and its face recognition; kittydar\n\ndecision tree - random forest\n\n## Introduction\n\n> The complexity of a data set increases rapidly with increasing dimensionality [@breiman_cart_1984, 8]\n\n> The first problem facing you is the bewildering variety of learning algorithms available. Which one to use? There are literally thousands available, and hundreds more are published each year [@domingos_few_2012, 1]\n\nUnder what conditions would it be possible to have an affirmative relationship to machine learning? This seems like a difficult learning problem in its own right. First of all,  the problem in answering this question is that there are so many techniques to choose from, and each of these techniques includes and  cuts the world in different ways. We have already seen that the list of techniques and the domain of their application is vast by any standards. \n\n```{r mlit1, echo=FALSE, cache=TRUE, message=FALSE, warning=FALSE, comment=NA, size='smallsize', results='asis' } \n\n    library(xtable)\n    ml_dir = '../ml_lit/data/machine_learning_WOS'\n    files <- dir(ml_dir,full.names=TRUE, pattern='savedrecs.*')\n    recs <-lapply(files, read.csv, sep='\\t', header=TRUE, as.is=TRUE, quote='', row.names=NULL)  \n    ml_df <-do.call( rbind, recs )\n    colnames(ml_df)[1:52] <- colnames(ml_df)[2:53]\n    colnames(ml_df)[1] <-'PT'\n    ml_df = ml_df[order(ml_df$TC, decreasing=TRUE),]\n    print(xtable(head(ml_df[,c('TI', 'TC')])))\n\n```\n\nThe machine learning literature itself has a sprawling, fluxing dimensionality, not easily captured by search terms or disciplines. The academic papers themselves are daunting in their bulk and variety, before we even try to move into the much more crowded and heavily trafficked domains where machine learning is practiced.  This literature itself, viewed in machine learning terms, comprises a high-dimensional vector space, whose features and attributes unstably move around as the techniques find new applications, as computer scientists, engineers, statisticians and others work on reshaping them, sometimes concertedly, sometimes independently and at odds with each other.  Viewed as a bundle of divergent, tangled trajectories in a high-dimensional social-material space, machine learning (or data-mining) can be described, sorted, and classified  in different ways. The immediate question is: what form of movement, from trajectories through the forest of practices associated with machine learning would allow us to inhabit the space of machine learning? What kinds of scales and subject positions would we need to move through in order to do that? A less immediate but no less important question is: in the name of what or whom would we affirm machine learning? This question is not going to be answered easily.\n\nAs I have emphasised previously, the form of movement through machine learning I'm experimenting with here is recursive or loopy. That is, rather than describing machine learning from the outside, or participating in it as an observer (participant observation, or even observant participation), I'm attempting to find ways of moving through machine learning by learning machine learning.  And as the previous chapter discussed, here the term 'learning' is overloaded with multiple meanings and practices. But the motivation is similar to what the anthropologist Paul Rabinow advocates: 'pursuing in our thought and writing something like motion, through different scales and different subject positions. ... Such movement is easy to initiate and hard to master. Yet I firmly believe that in the actual conjuncture of things, it is a paramount challenge for philosophy and the human sciences to experiment with forms that will be, if not fully adequate to, at least cognizant of, the need for such movement through scale and subjectivity' [@rabinow_anthropos_2003, 135-6]. Learning machine learning is a form of movement through textbooks, scientific articles, databases of scientific literature and sample datasets, online tutorials, Wikipedia pages, software documentation, read and writing many lines of `R` or `python` code. As Rabinow mentions, it is easy to start moving through machine learning, but hard to control that movement. I have found myself careening through websites, software manuals, YouTube lectures on machine learning, short intensive courses on data mining, news reports, APIs, code repositories and various code fragments.  Much of this movement has been saccadic (TBC) and aleatory rather than purposive or Dao-like, and only in writing now, in attempting to connect code, literature, people and events, might this movement take shape. In the course of moving backwards and forwards between research articles, textbooks, datasets and code, it is sometimes to possible to find amidst the many contingencies and specificities, some general forms of movement. \n\n## Movement in data\n\nLet us return to basic problem of the data space. Imagine we have a dataset, with many variables and with many components.  Although we may know where the data came from, and what it refers to, we don't have any firm ideas about the patterns or structures in it.  We can view the dataset to a certain extent by plotting it. \n\n```{r data_set, echo=FALSE, cache=TRUE, message=FALSE, warning=FALSE, comment=NA, engine='python' } \n\nimport random\nimport matplotlib.pyplot as plt\nimport numpy as np\nsize = 1000\nx = [random.uniform(0,10) for i in range(0,size)]\ny = x+ np.random.normal(np.mean(x), 5.0, size)\nz = [random.randint(20,80) for i in range(0,size)]\n\nmarkers = ['o', '8']\nshapes_index = np.random.randint(0,2, size)\nclass1 = [random.randint(1,2) for i in range(0,size)]\nclass2 = [random.randint(1,4) for i in range(0,size)]\nclass3 = [markers[i] for i in shapes_index]\nplt.figure(figsize=(10,10))\nplt.scatter(x,y,s=z, c = class1)\nplt.savefig('figure/synthetic_data.png', dpi=400)\nprint(\"![Data shapes](figure/synthetic_data.png)\")\n```\n\nThis plot of 1000 somewhat random points has no obvious structure, but there are functions generating the points. The dataset here has only four dimensions (`x`, `y`, colour and size).   Every point belongs to a class. That is, it can be classified. Classification has long been the main concern of machine learning. The patterns of belonging in  \n\nTBC -- the problem of classification and dimensionality ... \n## The movement of citation\n\n```{r ml_top_cites, echo=TRUE, cache=TRUE, message=FALSE, warning=FALSE, comment=NA , engine='python'} \n\nimport ml_lit_anal as ml\nimport google_scholar_parser as gs\nimport pandas as pd\nimport numpy as np\nimport collections\n\ndf = ml.load_records('../ml_lit/data/machine_learning_WOS/')\ndf = ml.clean_topics(df)\nprint('There are %s records in the dataset'%df.shape[0])t\nall_refs = [ref for refs in df.cited_refs for ref in refs]\nref_collection = collections.Counter(all_refs)\nprint(ref_collection.most_common(n=20))\n```\nThe machine learning literature is enormously but helpful in deciding which direction to move. It bristles with references to papers in statistics, computer science, mathematics, artificial intelligence and a swathe of related scientific fields. In the background stand substantial bodies of knowledge and practice coming from the social sciences, insurance and actuarial practice, and marketing research. A very rought citation analysis of the research literature indicates that  the top 20 most cited publications in the field have focused on decision trees (Quinlan, Breiman), on support vector machines (Vapnik, Christiani, Cortes, Chang),  machine learning pedagogy (Mitchell, an early textbook written by a computer scientist; Witten, a textbook and software package on data mining using Java; Duda, a textbook on pattern recognition), a tutorial on an error control techique (ROC - Receiver Operating Characteristics, first developed by the US military during WWII) and somewhat lower, the popular machine learning technique of neural networks (Bishop). Across these variations, it is possible to discern some forms of movement, or put simply, movements in machine learning. Briefly and perhaps too crudely summarised, they might be termed a _statistical_ movement, an _imitative_ movement, and an _infinitist_ movement. \n\n[ADD HERE: stuff from python notebook about topics and how they are organised ... including graph?]\n\nThe descriptive challenge entailed in tracking these movements was  figured more than a decade ago by Geoffrey Bowker and Susan Leigh Star in their work on classification systems. They wrote then:\n\n> Imagine you are walking through a forest of interarticulated branches. Some are covered with ice or snow, and the suns melts their touching tips to reveal space between. Some are so thickly brambled they seem solid; others are oddly angular in nature, like esplanaded trees.  Some of the trees are wild, some have been cultivated ... Helicopters flying overhead can quickly tell your many types of each, even each leaf, there are in the world, but they cannot yet give you a guidebook for bird-watching or forestry conservation. There is a lot of underbrush and a complex ecology of soil bacteria, flora and fauna.  ... Now imagine that the forest is a huge information space and each of the trees and bushes are classification systems.  ... Your job is to describe this forest. [@bowker_sorting_1999,31-32]\n \nBowker and Star refer here to a variety of standardised classification systems running across sciences, institutions, industries and geographies. Many of these classifications have extended genealogies running back more than a century or so through paper-record based systems. The classification systems exemplified in machine learning are in some ways much less diverse than Bowker and Star's examples in that they do not directly index a plurality of practices. Indeed, machine learning in many cases seeks to reduce the plurality of practices to a set of algorithmic processes that can be monitored and controlled directly.  As we will see, a concern with the _generalization of classification_ pervades the field. Nevertheless, a clear contrast between the Bowker and Star's challenge and the problems of machine learning comes through: while much, perhaps most,  machine learning practice concerns long-standing problems of classification, the classifying done in machine learning has little or no paper-based precedent. Many of the best-known and most widely used contemporary techniques -- logistic regression, neural networks, decision trees, random forests, support vector machines and a variety of clustering techniques -- do focus on classification problems, but classify in ways that lie a long way from the sorting and ordering practices found in preceding classification systems. \n\nAs it turns out, at the very time Bowker and Star were writing about the interarticulated forest of classification systems, the technique of RandomForests (TM) was being developed as a way of navigating huge information spaces. Statisticians such as Leo Breiman at UCLA were assembling decision trees (a technique I discuss below) into forests that could classify [@breiman_random_2001].  In the machine learning literature,  the random forest paper is the most highly cited reference [@breiman_random_2001]. According to Thomson Scientific Web of Science/ISI, this paper has been cited  `r ml_df$TC[1]` times. Like two other key machine learning techniques -- neural networks in the 1980s; support vector machines in the late 1990s --  random forests provided different ways of moving through the thicket of possible connections, associations, features and relations occasioned by digital data, especially as online data sets were becoming available.  This is not to say the machine learning techniques are absolutely unprecedented. On the contrary, each of these techniques of random forests, neural networks and support vector machine, as I will discuss below, took an existing predictive or classificatory technique or mechanism and re-configured it in order to deal with the problems of a disorientating profusion of  possible classifications. With lesser and greater degrees of success, they brought together different scales of movement, and different ways of including the world. \n\nSo in a way, machine learning has been taking on the job that Bowker and Star envisaged: 'imagine you are walking in a forest -- describe this forest.' But the forest we are moving in here is a forest of machine learning classifiers, mainly taking root in the  scientific-technical literature and associated software implementations.  The basic intuitions underlying  machine learning classification are not hard to grasp. We have already seen a few versions of it. Logistic regression, the technique discussed in the previous chapter and pervasively used in biomedical sciences, is a mainstay of classification techniques.  The techniques of decision trees, random forests, neural networks and support vector machines I will discuss here display many commonalities in their treatment of data.  They all  attempt to find ways of moving through data, and dealing with problems of deciding what groups, classes or sets things belong to, without having to know too much about  the world that data indexes. At the same time, or strictly speaking, slightly afterwards, these techniques themselves move through the world. Their movement is never purely abstract, as if they only inhabited a mathematical forest, although it possesses a certain texture or specificity. As with all functions, indelible residues of indexicality entangle machine learning technique with matters of fact. I like the way the biostatisticians  James Malley, Karen Malley and Sinisa Pajevic describe machine learning: 'if the data has any signal at all then most machines will detect it' but 'we freely admit that many machines studied in this text are somewhat mysterious, though powerful engines' [@malley_statistical_2011, 257]. As we will see, the ways in which machine learning methods move through data detecting signals are intensively theorised yet at the same time not well understood. \n\n\n## Growing trees in irises\n\n> Mastering the details of tree growth and management is an excellent way to understand the activities of learning machine generally [@malley_statistical_2011, 118].\n\n```{r aid, echo=FALSE, cache=TRUE, message=FALSE, warning=FALSE, comment=NA , fig.cap='Early uses of the Automatic Interaction Detector'} \n    library(xtable)\n    aid_df = '../ml_lit/data/morgan_sonquist_WOS'\n    files <- dir(aid_df,full.names=TRUE)\n    recs <-lapply(files, read.csv, sep='\\t', header=TRUE, as.is=TRUE, quote='', row.names=NULL)  \n    aid_df <-do.call( rbind, recs )\n    colnames(aid_df)[1:52] <- colnames(aid_df)[2:53]\n    colnames(aid_df)[1] <-'PT'\n    aid_df$TI = tolower(aid_df$TI)\n    aid_df = aid_df[order(aid_df$PY, decreasing=FALSE),]\n    print(xtable(head(aid_df[,c('TI', 'PY' ,'TC')],10)))\n```\n\nWork on classification and regression techniques using decision tree algorithms goes back to the  early 1960s when social scientists James Morgan and John Sonquist at the University of Michigan's Institute for Social Research were attempting to analyse increasingly large social survey datasets [@morgan_problems_1963].  As Dan Steinberg describes in his brief history of decision trees [@steinberg_cart_2009, 180], the  'automatic interaction detector' (AID) as it was known, sought to automate the practice of data analysts looking for interactions between different variables. The variety and sheer optimism of subsequent applications of these prototype decision tree techniques  is striking.  In the 1960s and 1970s, papers that drew on the AID paper or use AID techniques can be found, as the table shows, in education, politics, economics, population control, advertising, mass media and family planning. \n\nBut a decade after initial work, the AID was the object of trenchant criticism by statisticians and others. Writing in the 1970s, statisticians in the bebavioural sciences such as Hillel Einhorn at the University of Chicago castigated the use of such techniques.   The criticisms stemmed partly from  a general distrust of 'purely empirical methods':\n\n> The purely empirical approach is particularly dangerous in an age when computers and packaged programs are readily available, since there is temptation to substitute immediate empirical analysis for more analytic thought and theory building. It is also probably too much to hope that a majority of researchers will take the time to find out how and why a particular program works. The chief interest will continue to be in the output-the results-with as little delay as possible [@einhorn_alchemy_1972, 368]\n\nEinhorn discusses AID alongside other  techniques such as factor analysis and multi-dimensional scaling (both still widely used) before concluding 'it should be clear that proceeding without a theory and with powerful data analytic techniques can lead to large numbers of Type I errors' [@einhorn_alchemy_1972, 378]. His specific objections to AID are  particularly focused on the problematic power of the technique: 'it may make sense out of \"noise\"' (369). Consequently, researchers easily misuse the technique: they 'overfit' the data, and do not pay enough attention to issues of validation (369-370). Most of these criticisms can be seen as expressing conventional statistical reservations. Similarly the British marketing researcher Peter Doyle, criticising the use of AID in assessing store performance and site selection by operation researchers, complained that searching for patterns in data using small data sets was bound to lead to spurious results and the decision trees, although intuitively appealing (that is, they could be easily interpreted), were afflicted with arbitrariness: 'a second variable may be almost as discriminating as the one chosen, but if the program is made to split on this, quite a different tree occurs' [@doyle_use_1973, 465-466].  These objections and resistances to early decision trees echo today in discussions around pattern recognition, knowledge discovery and data-mining in science and commerce.    The problem of what computers do to the analysis of empirical data is long-standing. \n\nAs Einhorn expected, it was too much to hope that a majority of researchers would take time to investigate how a particular program works. Some researchers however did take time, years in fact, to investigate how decision trees work. As a result, writing in 2008, Hastie, Tibshirani and Friedman, who have excellent academic statistical credentials by any standards, could happily recommend decision trees as the best off-the-shelf classifier:  'Of all the well-known learning methods, decision trees comes closest to meeting the requirements for serving as an off-the-shelf procedure for data-mining' [@hastie_elements_2009, 352]. In 2013, Salford Systems, the purveyors of the leading contemporary commercial decision tree software, CART, claim:\n\n > CART is the ultimate classification tree that has revolutionized the entire field of advanced analytics and inaugurated the current era of data mining. CART, which is continually being improved, is one of the most important tools in modern data mining. Others have tried to copy CART but no one has succeeded as evidenced by unmatched accuracy, performance, feature set, built-in automation and ease of use. [Salford Systems](http://www.salford-systems.com/products/cart)\n\nWhat happened between 1973 and 2008?  The happy plight of decision trees emblematises the emerging, evolving flow of techniques associated with machine learning.  Decision trees somehow travelled from the statistically murky waters of the social sciences and business schools in the early 1970s to inaugurate the 'current era of datamining' (which the scientific literature suggests starts in the early 1990s). As the earlier citation from U.S. National Institutes of Health biostatisticians Malley, Malley and Pajevic indicates, decision trees now enjoy high regard even in biomedical research where statistical rigour is highly encouraged for life and death reasons. \n\n### 1984: Breiman, Friedman, Olshen and Stone's _Classification and Regression Trees (CART)_\n\nAs it turns out, working at the US Department of Energy's Stanford Linear Accelerator during the late 1970s, Jerome Friedman, the third author of Hastie, Tibshirani  and Friedman, was  instrumental in rescuing  decision trees from ignominy of unstability they had entered in the late  1960s. I am not writing a history of machine learning techniques, so I will not trace in any detail the reorganisation and statisticial retrofitting of the decision tree. It was not a single or focused effort. During the 1980s, statisticians such  as Friedman and Leo Breiman renovated the decision tree as a statistical tool at the same time as  computer scientists such as  Ross Quinlan in Sydney were re-implementing decision trees as rule-based induction technique for artificial intelligence [@quinlan_induction_1986]. Quinlan's papers  and book on versions of the decision tree (`ID3` and `c4.5`) are both amongst the top ten the most highly cited references in the machine literature itself. Google Scholar reports over 20,00 citations of the Quinlan's book _C4.5: Programs for Machine Learning_ [@quinlan_c4._1993]. This uneasy parallel effort between computer science and statistics  still characterises machine learning today. Both statisticians and computer scientists  do and use the same techniques, but often with slightly different inflections, formalizations and nomenclature.  Here I want to introduce only those steps in the renovation of the technique that helped consolidate decision trees as 'best off-the-shelf classifier,' such that today the standard  `R`package `rpart` constructs decision trees at a moment's notice and `C4.5` is voted the top data mining algorithm [@wu_top_2008]. \n\nAs usual, I treat the implementation of techniques in R as offering one path into  their wider practice, a path that perhaps favours the statistical side of that practice, but that nevertheless has certain forensic virtues not offered by commercial or closed-source software.  In this case, the name of the `R` package itself attests to something: `rpart` is a contraction of 'recursive partitioning' and this term generally describes how the decision tree algorithm works.  The term 'decision tree,' although still widely used in the research literature and elsewhere was replaced by 'classification and regression tree' during the late 1970s and 1980s work on decision tree. The terms 'classification and regression tree' is sometimes contracted to 'CART,' and that term strictly speaking refers to a computer program described in [@breiman_cart_1984]. It is also a registered trademark of Salford Systems, the software company mentioned above, who sell the leading commercial implementation of classification and regression trees. Hence, the `R` package `rpart` cannot call itself the more obvious name `cart, ` and instead invokes the algorithmic process it relies on: 'recursive partitioning.'\n\nTo get a sense of what happened between the 1970s and 1980s process leading up to `CART` and `C4.5`, we can again load  R.A. Fisher's _iris_ dataset, which contains measurements made in the 1930s of petal and sepal lengths of _iris virginica, iris setosa_ and _iris versicolor_. I should point out that this is a very small dataset. It is definitely a pre-computational miniature, but that diminutive character makes it into a useful illustration. While the _iris_ dataset is quite small, it supports a rhizomatic ecosystem of examples scattered across the machine learning literature.    The usual framing of the classification problem is how to decide whether a given iris blossom is of the species _virginica_, _setosa_ or _versicolor_.   These irises don't grow in forests -- they are more often found in riverbanks and meadows --  but they do offer a variety of illustrations of how machine learning classifiers are brought to bear on classification problems. Here the classification problem is taxonomic - the  iris genus has various sub-genera, and sections within the sub-genera. _Setosa, _virginica_ and _versicolor_ all belong to the sub-genus _Limniris_. This botanical context is  routinely ignored in  machine learning applications. In machine learning textbooks and tutorials, iris typically would be used to demonstrate how cleanly a classifier can separate the different kinds of irises. \n\n\n```{r iris_tree, echo=TRUE, cache=TRUE, message=FALSE, warning=FALSE, comment=NA, size='smallsize', results='asis' } \n\n    #load iris dataset\n    data(iris)\n\n    #load decision tree library\n    library(rpart)\n    head(iris)\n\n    #construct decision tree to classify iris by species\n    iris_tree =rpart(Species ~ ., iris)\n```\n\nThe code shown here loads the _iris_ data (the dataset  is routinely installed with many data analysis tools), loads the `R` decision tree library, prints a few lines of the iris dataset and builds a decision to classify the irises by species.  What has happened to the iris data in this decision tree? The `R` code that invokes the decision tree model is so brief `iris_tree =rpart(Species ~ ., iris)` that we can't tell much about how the data has been 'recursively partitioned.' We know that the _iris_ has `r nrow(iris)` rows, and that there are equal numbers of the three iris varieties.  From the standpoint of CART, and in particular the landmark _Classification and Regression Trees_ monograph [@breiman_cart_1984]  written by four statisticians working in California, the  rows of _iris_ are 'measurement vectors':\n\n> Define the measurements $(x_1, x_2, ...)$ made on a case as the _measurement vector_ $\\mathbf{x}$ corresponding to the case. [@breiman_cart_1984, 3]\n\nThe _iris_ dataset, small though it is, defines a larger space that Leo Breiman working  and co-authors called a 'measurement space' ('Take the _measurement space_ $\\mathcal{X}$ to be defined as containing all possible measurement vectors' [@breiman_cart_1984, 3]). In beginning to re-implement decision trees as classification trees, Breiman first address issues of data dimensionality. This talk of vectors and space is pervasive in the machine learning literature. What they call the measurement space is more often today called the 'vector space', but otherwise their characterisation of the difficulties of navigating that space, and how CART occupies that space remain more or less current.  In order to get into that space, and in order for us to grasp how _iris_ has been classified through the one liner `rpart(Species _., iris)`, Breiman and Friedman orchestrate a  whole series of formalizations that again, I won't follow in any great detail, but simply excerpt a few salient points.  These formalizations allow trees to be constructed algorithmically, and in that respect, directly promote tree growth. \n\nFirst of all,  all construction and use of  the decision tree  are renovated in terms of functions, sets, vector spaces and cost functions. (We have seen this process before in the previous chapter in the case study of logistic regression and gradient algorithms; refer to that for an account of functions in practice.)  For instance, rather simply invoking the notion of a classifier, the CART monograph defines a classifier in terms of a function: \n\n> A classifier or classification rule is a function $d(\\mathbf{x})$ defined on $\\mathcal{X}$ so that for every $\\mathbf{x}$, $d(\\mathbf{x})$ is equal to one of the numbers $1, 2, ..., J$. [@breiman_cart_1984, 4]\n \n The definition of the classifier as a function depends on the existence of a vector space $\\mathcal{X}$, a set of measurement, feature or predictor vectors  $\\mathbf{x}$ and a set of responses  $1, 2, ..., J$. Second, classification is understood in terms of a series of binary splits that comprise the tree structure. The problem here is that many splits are possible. \n\n> The first problem in tree construction is how to use $\\mathcal{L}$ to determine the binary splits of $\\mathcal{X}$ into smaller pieces. The fundamental idea is to select each split of a subset so that the data in each of the descendant subsets are \"purer\" than the data in the parent subset [@breiman_cart_1984, 23].\n\nTree construction hinges on the notion of purity or more precisely 'node impurity', a function that measures how data of labelled as belonging to different classes are mixed together at a given branch or node in a decision tree: 'that is, the node impurity is largest when all classes are equally mixed together in it, and smallest when the node contains only one class' [@breiman_cart_1984, 24]. As Malley and co-authors note, 'the collection of purity measures is still a subject of research' [@malley_statistical_2011, 123], but Breiman, Friedman, Olshen and Stone promoted a particular form of impurity measure known as 'Gini index of diversity' [@breiman_cart_1984, 38].  Like the cost function used in optimising linear and logistic regression models, the measures of node impurity allow the process of tree construction to be understood as a kind of movement through space. Whereas in gradient descent, the intuition was 'always go down to the valley as quickly as possible', here the intuition is more like: ' split things in ways that minimize mixing'. Good splits decrease the level of impurity in the tree. In an ideal tree with maximum purity, each terminal node -- the nodes at the base of the tree -- would contain a single class. \n\nThe results of the application of measures of node impurity can be seen below in two plots. \n\n```{r iris_tree_plot, echo=FALSE,  fig.cap ='Decision tree on _iris_ dataset', cache=TRUE, message=FALSE, warning=FALSE, comment=NA, results='asis' } \n\n    #plot the tree\n    par(mfrow = c(1,2), xpd=NA)\n    plot(iris_tree, main = 'tree recursive partitioning of iris')\n    text(iris_tree, cex=0.8, use.n=TRUE)\n    table(iris$Species) # is data.frame with 'Species' factor\n     iS <- iris$Species == \"setosa\"\n     iV <- iris$Species == \"versicolor\"\n     op <- par(bg = \"bisque\")\n     matplot(c(1, 8), c(0, 4.5), type =  \"n\", xlab = \"Length\", ylab = \"Width\",\n             main = \"Petal and Sepal Dimensions in Iris Blossoms\")\n     matpoints(iris[iS,c(1,3)], iris[iS,c(2,4)], pch = \"sS\", col = c(2,4))\n     matpoints(iris[iV,c(1,3)], iris[iV,c(2,4)], pch = \"vV\", col = c(2,4))\n     legend(1, 4, c(\"    Setosa Petals\", \"    Setosa Sepals\",\n                    \"Versicolor Petals\", \"Versicolor Sepals\"),\n            pch = \"sSvV\", col = rep(c(2,4), 2))\n```\n\nThe plot on the left shows the decision tree and the plot on the right shows just _setosa_ and _versicolor_  plotted by petal and sepal widths and lengths.  As the plot on the right shows, most of the measurements are well clustered. Only the _setosa_ petal lengths and widths seem to vary widely. All the other measurements are tightly bunched. This means that the decision tree shown on the left has little trouble classifying the irises. Decision trees are read from the top down, left to right. The top level of this tree can be read, for instance, as saying, if the length of petal is less 2.45, then the iris is _setosa_. Hastie, Tibshirani and Friedman  suggest that 'a key advantage of the recursive binary tree is its interpretability. The feature space partition is fully described a by single tree.  ... This representation is popular among medical scientists, perhaps because it mimics the way a doctor thinks.  The tree stratifies the population into strata of high and low outcome, on the basis of patient characteristics' [@hastie_elements_2009, 306-7]. I would differ from them on this point.  Decision trees do indeed have a rich  medical, as well  commercial and industrial history of use. And yes, decision trees and their later variations (such  as`C4.5`, the 'top' data-mining algorithm according to a survey of data miners in 2009 [@wu_top_2008] was developing during the 1980s in response to Friedman's work on decision trees) are often presented as easy to use because they are 'not unlike the serious of troubleshooting questions you might find in your car's manual to help determine what could be wrong with the vehicle' [@wu_top_2008,2]. (But who actually reads car manuals?) While that scenario is unlikely today, especially as Google sends autonomous-driving cars out onto the roads of California undoubtedly controlled by a variety of classifiers such as decision trees, neural networks and support vector machines, the recursive partitioning technique still has a great deal of traction in machine learning practice precisely because of its simplicity.\n\nThe problem with _iris_ is that the species actually ontically separate. The pattern of separation that the decision tree algorithm finds exists in the world for us too, except when, as happens, species of iris hybridise with each other. In many cases, things as Whitehead suggests, are not cleanly separable. Often there is some pattern of separation, perhaps in the form of overlapping clusters or clouds of points, but not enough to define a simple set of decision rules.  (As we will see that support vector machines, with their 'maximum margin hyperplanes,' take these overlaps as given, and build models focused on the overlaps.) How then does a decision tree decide how to split things? What counts as a good split has been a long standing topic of debate in the decision tree literature. Choosing where to cut: this is a key problem for classification or decision trees. As Malley, Malley and Pajevic observe, 'the challenge is to define _good_ when its clear that no obviously excellent split is easily available' [@malley_statistical_2011, 121].  The definition of 'good' that has emerged in the decision tree literature is centred on 'node purity.'\n\nIt is worthwhile looking at what actually happens to the data as  a decision tree is constructed in standard decision tree software such as `rpart` or its Python equivalents. 'Tree-based methods partition the feature space into a set of rectangles and then fit a simple model (like a constant) in each one. They are conceptually simple yet powerful' [@hastie_elements_2009, 305].  This specification is quite precise. The tree method  partitions data into _rectangles_, but the rectangles cannot overlap or in any way deviate alignment with the horizontal or vertical axes. The binary partitions produced are like housing plots of various sizes lined up within a block. As usual in  descriptions of machine learning techniques, the metaphors and analogies are two or three dimensional, but the practical implementations work in much higher dimensions. \n\n```{r \"tree_decision_surface\", echo=FALSE, cache=TRUE, message=FALSE, warning=FALSE, comment=NA,  results='asis', engine='python' } \n\nimport numpy as np\nimport pylab as pl\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Parameters\nn_classes = 3\nplot_colors = \"bry\"\nplot_step = 0.02\n\n# Load data\niris = load_iris()\n\nfor pairidx, pair in enumerate([[0, 1], [0, 2], [0, 3],\n                                [1, 2], [1, 3], [2, 3]]):\n     # We only take the two corresponding features\n    X = iris.data[:, pair]\n    y = iris.target\n\n    # Shuffle\n    idx = np.arange(X.shape[0])\n    np.random.seed(13)\n    np.random.shuffle(idx)\n    X = X[idx]\n    y = y[idx]\n\n    # Standardize\n    mean = X.mean(axis=0)\n    std = X.std(axis=0)\n    X = (X - mean) / std\n\n    # Train\n    clf = DecisionTreeClassifier().fit(X, y)\n\n    # Plot the decision boundary\n    pl.subplot(2, 3, pairidx + 1)\n\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n                         np.arange(y_min, y_max, plot_step))\n\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    cs = pl.contourf(xx, yy, Z, cmap=pl.cm.Paired)\n\n    pl.xlabel(iris.feature_names[pair[0]])\n    pl.ylabel(iris.feature_names[pair[1]])\n    pl.axis(\"tight\")\n\n    # Plot the training points\n    for i, color in zip(range(n_classes), plot_colors):\n        idx = np.where(y == i)\n        pl.scatter(X[idx, 0], X[idx, 1], c=color, label=iris.target_names[i],\n                   cmap=pl.cm.Paired)\n\n    pl.axis(\"tight\")\n\npl.suptitle(\"Decision surface of a decision tree using paired features\")\npl.legend()\npl.savefig('figure/tree_graph.png')\nprint('![Iris decision tree](figure/tree_graph.png)')\nprint('Hello')\n```\nIn the example shown above, the different shadings of the rectangles indicate areas that the decision tree has classified in different species. While these decision surfaces look somewhat irregularly , they are all   combinations of rectangles.  The plots above, again, show 'decision surfaces' in two dimensions, but the dataset has at least five dimensions (petal width, petal length, sepal width, sepal length, species). The splitting of variables into pairs and then plotting them against each other is a way to put this decision surface in some perspective. As often happens in machine learning, it is not possible to see data in all its dimensionality. Instead, machine learning techniques often construct some perspective that allows the reshaping of the data to become visible in some part. \n\nWhat happens in decision trees to differences and in particular those differences that we regard as important enough to develop classifications for? Whitehead suggests that 'sharp-cut scientific classifications are essential for scientific method. But they are dangerous for philosophy. Such classification hides the truth that the different modes of natural existence shade off into each other' [@whitehead_modes_1956, 215]. Decision trees actually take this truth to heart. In higher dimensional feature spaces, many different classifications are possible and a huge number of different trees could be constructed for any given dataset. Even for the _iris_ dataset,  different implementations of the algorithm generate different results because of the way they optimise the tree they select.  Given that the aim is to generate a rule that best separates categories or classes of subjects, events or things of interest, how can decision tree techniques deal with this potential proliferation of rules?  In some ways, it seems that the decision tree researchers were reading actor-network theory as they developed the recursive partitioning algorithms. On the one hand, they are almost agnostic about differences in the world. As the biostatisticians Malley, Malley and Pajevic write in their account, 'no distributional assumptions or other statistical premises are made in decision trees concerning the features or the subjects' [@malley_statistical_2011, 20]. 'Wait,' readers of actor network theory might say, 'that sounds like the ontological pluralism of actor network theory, or William James or A.N. Whitehead.' Indeed, decision trees (and as we'll see, random forests) are emphatically agnostic about what differences matter in principle.  But their almost inexhaustible capacity to accommodate differences can make them unstable. \n\n### Cannot see the trees for the forest\n \n\n ```{r dt_lit_2, echo=FALSE, cache=TRUE, message=FALSE, warning=FALSE, comment=NA } \n    \n    library(xtable)\n    ml_dir = '../ml_lit/data/decision_tree_WOS'\n    files <- dir(ml_dir,full.names=TRUE, pattern='savedrecs.*')\n    recs <-lapply(files, read.csv, sep='\\t', header=TRUE, as.is=TRUE, quote='', row.names=NULL)  \n    ml_df <-do.call( rbind, recs )\n    colnames(ml_df)[1:52] <- colnames(ml_df)[2:53]\n    colnames(ml_df)[1] <-'PT'\n    ml_df = ml_df[order(ml_df$TC, decreasing=TRUE),]\n    print(xtable(head(ml_df[,c('TI', 'TC')])))\n```\n\n [@breiman_cart_1984]  is a comet-like publication in the machine learning literature. A  minor decision tree literature trails in the wake of its formalised treatment of construction and shaping of classification and regression trees.  This prolixity or associativity of decision trees can be seen in the scientific literature from the mid-1980s onwards. Decision trees again accelerate across behavioural, biomedical and associated statistical sciences.  A Thomson Reuters 'Web of Science' query on 'decision tree' across all indexes returns roughly 10,000 results, but most of these appear in the wake of [@breiman_cart_1984]. Just looking at the most highly cited results in this set, we can see that 'decision tree' plays a role in a psychiatric diagnostic procedure in 1992, in clinical decision making in 1993,  in helping sort out astronomical observations in 2000, in a serum protein matching test for distinguishing benign and non-benign prostate cancer in 2002,  or in bringing biometric data together to uniquely identify individuals in 2003. (A similar but more restricted Web of Science query on `CART` itself, the software developed by Jerome Friedman for the 1984 book _Classification and Regression Trees_ yields 1600 results, and the fields of application are just as varied.) While the most cited paper (the psychiatric diagnostic test) does not actually use machine learning, all the others do, and in greatly varying ways. The question then is how such a huge lateral slippage  occurring? What is it about decision trees that allows them to move across different settings so readily?\n\nWhen we track the movement of techniques, whether in the form of algorithms, software implementations, embodied skills,  devices and their uses, I would argue that there is always something in the technique itself that animates its movement. This is to treat techniques like decision trees as if they were a form of life, with their own specificity and their own materiality. We have seen one side of this already: the statistical and algorithmic formalization of the 1960s decision tree work from the late 1970s onwards adds an increasing number of moving parts to the technique in the form of mathematical functions, splitting rules, measures of tree purity and feature importance, all of which are meant to allow the technique to deal with heterogeneous variations in data. Not only does the data become higher dimensional over these decades, but the techniques do too. But the more primary impetus to movement comes from the technique itself.  In _Alien Phenomenology_, Ian Bogost puts this point in  terms of 'unit operations':\n\n> Objects try to make sense of each other through the qualities and logics they possess. When one object caricatures another, the first grasps the second in abstract, enough for the one to make some sense of the other given its own internal properties [@bogost_alien_2012, 66]\n\nLike logistic regression models, artificial neural networks, support vector machines or any other machine learning technique, decision trees have certain qualities and logics, and they make sense of the worlds they encounter in terms of those qualities and logics. We have been discussed the node and tree impurity measures that guide splitting in trees. The quality of decision trees as classifiers Although these splitting rules have strong statistical justifications, they do not at all eliminate the problem of instability in trees. For instance, they easily end up 'overfitting' the data. Overfitting is a problem for all machine learning techniques. Algorithms sometimes find it hard to know when to stop. During construction of a decision tree, various features in the data are split into smaller and smaller groups. ''The goodness of the split', wrote Breiman and co-authors, 'is defined to be the decrease in impurity' [@breiman_cart_1984, 25]. Under this definition of goodness, the terminal nodes or leaves of the tree can end up containing a single case, or a single class of cases. The decision tree respects the singularity of the individual case to such a degree that it sees differences everywhere. Driven too maximise the purity of the nodes it creates, it leans heavily on  data it has been trained on to see relevant similarities when fresh data appears. Trees that branch too much are too sensitive (or unstable), and need to be pruned. An associated literature on pruning decision trees using measures of tree complexity has also developed.  _The Elements of Statistical Learning_ asks, 'how large should we grow the tree? Clearly a large tree might overfit the data, while a small tree might not capture the important structure. Tree size is a tuning parameter governing the model's complexity, and the optimal tree size should be adaptively chosen from the data' [@hastie_elements_2009,  307-308]. All of these questions and problems relate to the core test of any machine learning algorithm: does it generalize well?\n\nDecision trees  also deliver some fresh problems of interpretation.  The look of a tree is not to be trusted: 'how a tree _looks_, complex, simple or something in between, is not a reliable indication of its predictive ability' [@malley_statistical_2011, 125].  Indeed  _The Elements of Statistical Learning_ warns us: \n\n> One major problem with trees is their high variance. Often a small change in the data can result in a very different series of splits, making interpretation somewhat precarious. The major reason for this instability is the hierarchical nature of the process: the effect of an error in the top split is propagated down to all of the splits below it.  312\n@steinberg_cart_2009\n\nWhile easy to intepret, decision tree interpretations can easily diverge. This divergence heavily affects their predictive power, and hence their capacity to engage in changing matters of fact in the world.  At the same time, the top splits in made by a tree at its root are not necessarily the most important ones in terms of prediction. Malley conclude that 'what a tree does at the very top is almost certainly not a good way to assess how it does by the time it gets to the bottom. And it is only at the bottom, where the terminal nodes appears, that the tree is making decisions on new cases' [@malley_statistical_2011, 136]. Many subtle variations and divergences affect trees, and they all attempt address problems of mobilization and generalization of the technique. \n\nYet all of this modification, reconfiguration, tuning and tweaking of decision trees was subsumed by another form of movement -- the automation of the construction of trees through machine learning itself. In a move that I would suggest is symptomatic of the machine learning, the construction of trees using splitting rules and measures of node impurity, and then their subsequent pruning using measures of tree complexity was swept away by techniques such as Random Forests(TM).  Around 2001, Leo Breiman writes: \n\n> A random forest is a classifier consisting of a collection of tree-structured classifiers $\\{h(x, \\Theta_k ), k = 1, . . .\\}$ where the $\\{ \\Theta_k \\}$ are independent identically distributed random vectors and each tree casts a unit vote for the most popular class at input $\\mathbf{x}$. [@breiman_random_2001, 6]\n\nIf we refer back to the formal definition of a classifier in _Classification and Regression Trees_  fifteen years earlier, there ares some suble but important transformations here: \n\n> A classifier or classification rule is a function $d(\\mathbf{x})$ defined on $\\mathcal{X}$ so that for every $\\mathbf{x}$, $d(\\mathbf{x})$ is equal to one of the numbers $1, 2, ..., J$. [@breiman_cart_1984, 4]\n\nBreiman has added something that was not present in any of the previous definitions of decision trees, whether they were produced by `CART`, `ID3` or `C4.5` (Quinlan's popular techniques of constructing trees): 'independently identically distributed random vectors.' This has a strongly statistical tone, and brings  a series of developments in several decades of research into statistical inference to bear on the problem of classification. Again attention to small details in mathematical typography can illuminate these shifts: the presence of the curly bracks $\\{\\}$ points to the existence of a _set_ or collection; the capitalised Greek letter $\\Theta$ in statistical texts normal attests to something that has a multiple and often somewhat probabilistic mode of existence (in a later chapter I discuss probability and statistical distributions in much more detail). \n\nHow does this injection of statistical techniques based on 'independent identically distributed random vectors' change machine learning? From the earlier decision tree techniques of the 1960s  -- the Automatic Interaction Detector and the Concept Learning System - automated repetition of tree construction had been central. In the early 1980s work, this automation was refined and reshaped by a certain amount of formalism -- the introduction of functions such as the Gini diversity index and measures of information entropy ('cross-entropy' [@hastie_elements_2009, 309]). Some measures arrived via statistics, and others via information theory and computer science. Random forests and similar methods developed in the 1990s added a second level of formalisation working to reduce the _variance_ or the 'spread' of the predictions produced by decision trees. Practically, techniques based on practices of either _bagging_ or _boosting_ all bring a further level of recursivity to machine learning techniques. The random forest algorithm constructs a large number of shallow decision trees by randomly sampling a subset of variables as the basis of a tree and then building the tree. As Breiman writes: 'The forests studied here consist of using randomly selected inputs or combinations of inputs at each node to grow each tree' [@breiman_random_2001, 10].  \n\n```{r titanic, echo=TRUE, cache=TRUE, message=FALSE, warning=FALSE, comment=NA } \n    library(randomForest)\n    titanic_train = read.csv('data/titanic_train.csv')\n    titanic_test = read.csv('data/titanic_test.csv')\n    head(titanic_train)\n    titanic_rp = rpart(Survived~Sex+Age+SibSp+Parch+Pclass + Fare+Embarked, titanic_train)    \n    summary(titanic_rp)\n    plot(titanic_rp)\n    text(titanic_rp, cex=0.8, use.n=TRUE)\n\n    #now the random forest\n    #remove missing values\n    titanic_train_r = na.omit(titanic_train)\n    titanic_train_r$Survived = factor(titanic_train_r$Survived)\n    titanic_rf=randomForest(Survived~Sex+Age+SibSp+Pclass + Parch+Fare+Embarked, titanic_train_r, proximity=TRUE)\n    print(titanic_rf)\n    print(round(importance(titanic_rf), 2))\n    MDSplot(fac=titanic_train_r$Survived, titanic_rf, palette = c('red', 'green'))\n\n```\nThis process is illustrated by the output of the `R` library `randomForest` at work on the `titanic` data, a dataset whose provenance and character I will discuss in greater detail in the next chapter. In this example, both a decision tree and a random forest classify the passengers of the titanic according to whether they survived or not.  The dataset is not very large as it only includes `r dim(titanic_train)[1]` passengers.  The decision tree chooses to split according to `r titanic_rp$variable.importance`. In the decision tree, `sex` and `fare` are followed by `pclass`, the passenger class.  The random forest model suggests that sex, followed by amount of fare paid, then age, then passenger class are the most important predictors of survival versus non-survivors from the Titanic: `r titanic_rf$importance`. How is it that the importance of variables can shift as we move from tree to forest? \n\nThe summary of the titanic random forest refers to 500 decision trees, and it also reports that it chose two of the six variables in the dataset in constructing each tree. How can these reduced decision trees produce better predictions, such that random forests have become one of the most popular machine learning models?  The so-called 'proximity plot' for the random forest grown on the `titanic` data suggests something of what happens here. As Hastie et. al. comment, 'the idea is that even though the data may be high-dimensional, involved mixed variables, etc., the promixity plot gives an indication of which observations are effectively close together in the eyes of the random forest classifier' [@hastie_elements_2009, 595]. With this plot, it seems that patterns do start to emerge from the data that might be helpful in understanding what happened in a situation. \n\nBreiman attributes the strength of random forests to classify without overfitting to  the interplay between two dynamics:\n\n> For random forests, an upper bound can be derived for the generalization error in terms of two parameters that are measures of how accurate the individual classifiers are and of the dependence between them. The interplay between these two gives the foundation for understanding the workings of random forests.  [@breiman_random_2001, 7]\n\nIn comparison to the decision trees, with their propensity to partition differences _ad absurdem_, random forests do not overfit because they  combine two different forms of movement. On the one hand, the sheer number of trees multiplies the chances that a good proportion of the trees will find a relevant pattern in the data. On the other hand, the de-coupling or lack of correlation between the trees helps ensure that the trees will provide independent views on the classification. Together these two features of the ensemble of decision allow random forests to 'generalize'  or, put more technically, to minimize the 'generalization error.'\n\n> Our results indicate that better (lower generalization error) random forests have lower correlation between classifiers and higher strength. The randomness used in tree construction has to aim for low correlation $\\overline{\\rho}$ while maintaining reasonable strength [@breiman_random_2001, 20].\n\nAs in much machine learning, better means lower error rates. But here, the key point is that the components of the machine themselves appear to become statistical processes. That is, their construction is guided by the Law of Large Numbers (that is, 'the _law of large numbers_ says that the sample average $\\overline{X}_n = n^-1\\sum_{i=1}^nX_i$ converges in probability to the expectation $\\mu = \\mathbb{E}X_i$. This means that $X_n$ is close to $\\mu$ with high probability ' [@wasserman_all_2003, 72]).  The injection of randomness, as Breiman terms it (10), only relates to certain aspects of the data and the components. It randomly selects small groups of features to use, and it randomly selects a subset of the training data. \n\n\n`\n## Neural net\n\n```{r nnet, echo=TRUE, cache=TRUE, message=FALSE, warning=FALSE, comment=NA, size='smallsize', results='markup' } \nlibrary(nnet)\n```\n\n## Support vector machine and infinite dimensionality\n\nThe second most highly cited reference in the ten of thousands of articles and papers is a paper by Cortes and Vapnik [@cortes_support-vector_1995]. \n\n```{r pattern_recognition1, echo=TRUE, cache=TRUE, message=FALSE, warning=FALSE, comment=NA, size='smallsize', results='asis' } \n    dir = '../ml_lit/data/pattern_recognition_WOS'\n    files <- dir(dir,full.names=TRUE, pattern='savedrecs.*')\n    recs <-lapply(files, read.csv, sep='\\t', header=TRUE, as.is=TRUE, quote='', row.names=NULL)  \n    df <-do.call( rbind, recs )\n    colnames(df)[1:52] <- colnames(df)[2:53]\n    colnames(df)[1] <-'PT'\n    df = df[order(df$TC, decreasing=TRUE),]\n    print(head(df$TI))\n```\nIf the shift from decision trees to random forests illustrate an increasingly statistical movement in machine learning, propelled by controlled injections of randomness,  the technique I turn to now, the support vector machine, demonstrates a different way of coping with the problems of generalization.  It is difficult to overstate the importance of support vector machine. While the name is somewhat forbiddingly technical compared to more easily understood terms such as 'decision tree' or 'neural network,' the underlying intuition of the technique is much older, and can be found in the models developed by the British statistician R. A. Fisher.  The route by which these long-standing intuitions were reconstructed during the 1990s in the form of the support vector machine is of interest, because it brings to light an important form of movement in  contemporary data. This movement we might call infinitist.\n\nThe vector space model of data encourages a certain form of classification -- the search for the best line, the line of best fit, or the most discriminating line, the line that best divides things from each other. Linear regression is not called 'linear' for no reason. Clearly there is a massive idealisation of classification and prediction here. The Platonic form of the line, the plane, and sometimes the sphere, function as ideal forms to which all actual processes, events and practices can only approximate. This idealism of the perfect plane, however, is something that almost the entire gamut of machine learning techniques are aware of. In many ways, the last two decades of research in machine learning, whether it has been primarily statistical, mathematical, or computational, directly countenances and addresses in different ways.  A primary strategy has been to increase the dimensionality of the predictive model in order to find hidden, latent or otherwise invisible forms of linearity that underpin the apparent non-ideality of the data.  The support vector machine, as we will, powerfully exemplifies this expansive form of generalisation. \n\n\n\nTBA -- this section has to deal with the idealism of the plane vs the curviness of the kernel\nIt also has to bring in ANW on the pattern  ....\n\n\n## Conclusion\n\nAn enormous elephant remains in the room here. The various techniques we have just been discussing -- decision trees, random forests, linear discriminant analysis and support vector machines -- have changed over time. As we've seen, in many cases newer techniques can be seen as recursive applications of machine learning techniques to themselves. The random forest is a higher level formalization of the many decision trees. The decision tree itself algorithmically formalizes earlier hierarchical classification techniques. Similarly, the support vector machine takes the idea of the separating hyperplane and flips the data into a many dimensional space in order to find the linear discriminant or separating plane there.  But across all these conversions, transformations, formalizations and automations, even buoyed up injections of probabilistic reasoning and randomness, one assumption remains almost completely intact: that the process that generated the data itself does not change.  In slightly more philosophical terms, the exuberant multiplication of dimensions in the data opens up huge internal vistas, canyons and paths through the data, but cannot countenance becoming in data.  At the same time, and perhaps machine learning is not  at all unusual in this, its own becoming is largely a puzzling encumbrance to it. Could machine learning learning to make sense of itself as anything other than an increasingly powerful mobilisation of techniques? \n\nThere is some grounds on which we might think about this differently. Often machine learning techniques have multiple incarnations and tangled genealogies. Decision trees demsonstrate this kind of phylogeny. The statistical trajectory of their development moves in a different way to the artificial intelligence/computer science version. Even if they end up appearing almost interchangeable in the contemporary data mining literature, the classification trees developed by Friedman/Breiman/Olshen and TBC have a different mode of existence to the ID3/C4.5 decision trees developed by computer scientists such as Quinlan.  Similarly, the kind of probabilistic scaffolding that random forests adds to decision trees is quite distinct from the incorporation by Vapnik of infinite dimensional spaces via the mathematics of kernel functions.  By virtue of these different paths of development, and in the light of the very different modes of moving through data they entail, machine learning techniques themselves incorporate the world in specific ways.  \n\nBut how do these sometimes subtle differences in incorporation matter given a massive and often seemingly crushing assimilation of data in the server farms of network media and large enterprises, or in commercial and government surveillance programs? There are massive disparities in scale here. The analytic or predictive power of the US National Security Agency  or Google Search or certain financial trading systems heavily blankets the terrain. These apparatuses are not, I would suggest, easily steered by any persons since the appetite of machine learning for diverse data seems to immediately override existing limits or constraints. But even just understanding the  form of movements through the data we in classification trees, random forests and in support vector machines might help position us differently in relation to these voracious systems. It is hard to know exactly what they are doing, but I'm fairly strongly persuaded that they do not different anything radically different from what we can see in the machine learning literature I have been discussing. The technical expertise of engineers, statisticians and computer scientists are Google, Facebook, Walmart or the NSA has developed in and amongst the same forest of classification systems we encounter in that literature. The popularity of the support vector machine in the last decade is general. The proliferation of decision trees in the 1980s and then random forests in the 2000s is likewise general. These are techniques whose power to generalize generalizes them.  Understanding this power to generalize in terms of its specific material operations is tremendously important TBC. ",
			"file": "ch3_dimensionality/ch3_dimensional_exuberance.rmd",
			"file_size": 69175,
			"file_write_time": 130251937549650390,
			"settings":
			{
				"buffer_size": 69255,
				"line_ending": "Unix"
			}
		},
		{
			"file": "ch7_topologies/ch7_genomic_topologies.rmd",
			"settings":
			{
				"buffer_size": 101,
				"line_ending": "Unix"
			}
		},
		{
			"file": "ch4_subjects/ch4_learning_subjects.rmd",
			"settings":
			{
				"buffer_size": 40264,
				"line_ending": "Unix"
			}
		},
		{
			"file": "ch5_probability/ch5_naive_informed.rmd",
			"settings":
			{
				"buffer_size": 270,
				"line_ending": "Unix"
			}
		},
		{
			"file": "/home/mackenza/Documents/notes/wasserman_all_of_stat2003.mdown",
			"settings":
			{
				"buffer_size": 964,
				"line_ending": "Unix"
			}
		},
		{
			"file": "/home/mackenza/Documents/notes/Breiman_2001_andom Forests.md",
			"settings":
			{
				"buffer_size": 1962,
				"line_ending": "Unix",
				"name": "Breiman 2001, Random Forests"
			}
		},
		{
			"file": "/home/mackenza/Documents/notes/Breiman_Friedman.md",
			"settings":
			{
				"buffer_size": 2722,
				"line_ending": "Unix",
				"name": "Breiman & Friedman"
			}
		},
		{
			"file": "/home/mackenza/Documents/notes/mitchell_machine_learning_2007.mdown",
			"settings":
			{
				"buffer_size": 243,
				"line_ending": "Unix"
			}
		},
		{
			"file": "proposal.md",
			"settings":
			{
				"buffer_size": 38876,
				"line_ending": "Unix"
			}
		},
		{
			"file": "ch3_dimensionality/tree.py",
			"settings":
			{
				"buffer_size": 1772,
				"line_ending": "Unix"
			}
		},
		{
			"file": "ml_lit/ml_lit_anal.py",
			"settings":
			{
				"buffer_size": 14668,
				"line_ending": "Unix"
			}
		},
		{
			"file": "ml_lit/web_of_science.R",
			"settings":
			{
				"buffer_size": 6762,
				"line_ending": "Unix"
			}
		},
		{
			"file": "/home/mackenza/Documents/notes/Xue_SVM: Support Vector Machines_2009.md",
			"settings":
			{
				"buffer_size": 879,
				"line_ending": "Unix",
				"name": "# SVM: Support Vector Machines, Xue in Xindong Wu,"
			}
		},
		{
			"file": "/home/mackenza/R/web_of_science.R",
			"settings":
			{
				"buffer_size": 6776,
				"line_ending": "Unix"
			}
		},
		{
			"file": "/home/mackenza/R/phrase_structures.R",
			"settings":
			{
				"buffer_size": 8007,
				"line_ending": "Unix"
			}
		},
		{
			"file": "/home/mackenza/Documents/notes/Malley_Statistical Learning_2011.md",
			"settings":
			{
				"buffer_size": 2605,
				"line_ending": "Unix"
			}
		},
		{
			"file": "/home/mackenza/Documents/notes/Hastie_2008.md",
			"settings":
			{
				"buffer_size": 5214,
				"line_ending": "Unix"
			}
		},
		{
			"file": "/home/mackenza/Documents/notes/Stengers_d&G_2005.txt",
			"settings":
			{
				"buffer_size": 3125,
				"line_ending": "Unix"
			}
		},
		{
			"file": "/home/mackenza/Documents/notes/vapnik_nature of statistical learning theory 2000.md",
			"settings":
			{
				"buffer_size": 2305,
				"line_ending": "Unix",
				"name": "vapnik_nature of statistical learning theory 2000"
			}
		},
		{
			"file": "/home/mackenza/Documents/notes/bogost_alien_2012.md",
			"settings":
			{
				"buffer_size": 944,
				"line_ending": "Unix",
				"name": "bogost"
			}
		},
		{
			"file": "/home/mackenza/Documents/notes/WhiteheadModes.mdown",
			"settings":
			{
				"buffer_size": 5877,
				"line_ending": "Unix"
			}
		},
		{
			"file": "/home/mackenza/Documents/notes/rabinow_anthropos_2003.md",
			"settings":
			{
				"buffer_size": 4283,
				"line_ending": "Unix",
				"name": "Rabinow, P. 2003. Anthropos Today. Reflections on"
			}
		},
		{
			"file": "references/data_forms_thought.bib",
			"settings":
			{
				"buffer_size": 23988,
				"line_ending": "Unix"
			}
		},
		{
			"file": "references/refs.bib",
			"settings":
			{
				"buffer_size": 41626,
				"line_ending": "Unix"
			}
		},
		{
			"file": "references/machine_learning.bib",
			"settings":
			{
				"buffer_size": 42511,
				"line_ending": "Unix"
			}
		}
	],
	"build_system": "Packages/knitr/knitr-Markdown.sublime-build",
	"command_palette":
	{
		"height": 392.0,
		"selected_items":
		[
			[
				"set",
				"Set Syntax: Markdown"
			],
			[
				"pan",
				"Pandoc: Render Markdown to temp PDF and View"
			],
			[
				"pand",
				"Pandoc: Render Markdown to temp PDF and View"
			],
			[
				"mark",
				"Set Syntax: Markdown"
			],
			[
				"and",
				"Pandoc: Render Markdown to temp PDF and View"
			],
			[
				"pdf",
				"Pandoc: Render Markdown to temp PDF and View"
			],
			[
				"Snippet: ",
				"Snippet: kcf"
			],
			[
				"upgr",
				"Package Control: Upgrade Package"
			],
			[
				"pack",
				"Package Control: Upgrade Package"
			],
			[
				"mak",
				"Pandoc: Render Markdown to temp PDF and View"
			],
			[
				"kni",
				"knitr: Send Chunk to R"
			],
			[
				"",
				"Side Bar: Reveal File"
			],
			[
				"inst",
				"Package Control: Install Package"
			],
			[
				"in",
				"Package Control: Install Package"
			],
			[
				"Package Control: ",
				"Package Control: Install Package"
			],
			[
				"lis",
				"Package Control: List Packages"
			],
			[
				"pa",
				"Package Control: Add Repository"
			],
			[
				"up",
				"Package Control: Upgrade Package"
			],
			[
				"pak",
				"Package Control: Disable Package"
			],
			[
				"list",
				"Package Control: List Packages"
			],
			[
				"upg",
				"Package Control: Upgrade/Overwrite All Packages"
			],
			[
				"R",
				"R: Choose Application"
			],
			[
				":w",
				":w - Save"
			],
			[
				"set mark",
				"Set Syntax: Markdown"
			],
			[
				"install",
				"Package Control: Install Package"
			],
			[
				"set m",
				"Set Syntax: Markdown"
			],
			[
				"set mar",
				"Set Syntax: Markdown"
			],
			[
				"set am",
				"Set Syntax: Markdown"
			],
			[
				"set lat",
				"Set Syntax: LaTeX"
			],
			[
				"set pyth",
				"Set Syntax: Python"
			],
			[
				"set syntaxm",
				"Set Syntax: Markdown"
			],
			[
				"set syntax py",
				"Set Syntax: Python"
			],
			[
				"pac",
				"Package Control: Install Package"
			],
			[
				"set s",
				"Set Syntax: R"
			],
			[
				"p",
				"Package Control: Install Package"
			],
			[
				"se",
				"Set Syntax: Markdown"
			],
			[
				"s",
				"Set Syntax: BibTeX"
			],
			[
				"S",
				"Set Syntax: R"
			]
		],
		"width": 449.0
	},
	"console":
	{
		"height": 290.0,
		"history":
		[
			"os.listdir('.')",
			"os.dir('.')",
			"sys.path",
			"sys.path()",
			"import sys",
			"os.dir()",
			"os.chdir('ch3_dimensionality')",
			"import os",
			"setwd('ch3_dimensionality')",
			"pwd()",
			"sys.path.append('/home/mackenza/Documents/data_intensive/book/ml_lit')",
			"sys.path",
			"sys.path.append('ml_lit')",
			"import sys",
			"import ml_lit_anal",
			"sys.path.append('/home/mackenza/Documents/data_intensive/book/ml_lit')",
			"import ml_lit_anal",
			"sys.path.append('~/Documents/data_intensive/book/ml_lit')",
			"from ml_lit import ml_lit_anal",
			"import book",
			"sys.path",
			"sys.path.append('~/Documents/data_intensive/book')",
			"sys.append('~/Documents/data_intensive/book')",
			"import __init__",
			"import tree",
			"sys.path",
			"sys.path.append('.')",
			"sys.path",
			"import sys",
			"import ml_lit_anal as ml",
			"sys.path.append('/home/mackenza/Documents/data_intensive/book/ml_lit')",
			"sys.path",
			"path",
			"import ml_lit_anal as ml",
			"sys.path.append('home/mackenza/Documents/data_intensive/book/ml_lit/')",
			"import ml_lit_anal as ml",
			"sys.path",
			"sys.path.append('home/mackenza/Documents/data_intensive/book/ml_lit')",
			"sys.path",
			"sys.path()",
			"import sys",
			"import ml_lit_anal",
			"print(platform.python_version())",
			"print platform.python_version()",
			"import platform",
			"import numpy as np",
			"$PYTHONPATH",
			"import ml_lit.ml_lit_anal",
			"PYTHONPATH",
			"$PYTHONPATH"
		]
	},
	"distraction_free":
	{
		"menu_visible": true,
		"show_minimap": false,
		"show_open_files": false,
		"show_tabs": false,
		"side_bar_visible": false,
		"status_bar_visible": false
	},
	"file_history":
	[
		"/home/mackenza/Documents/data_intensive/book/ch3_dimensionality/ch3_dimensional_exuberance.md",
		"/tmp/.fr-DNz46C/Order details.txt",
		"/home/mackenza/.config/sublime-text-3/Packages/User/svm.sublime-snippet",
		"/home/mackenza/Documents/data_intensive/book/ch3_dimensionality/test.rmd",
		"/home/mackenza/Documents/data_intensive/book/ch3_dimensionality/test.md",
		"/home/mackenza/R/biofuels/crunchbase_api.txt",
		"/home/mackenza/Documents/data_intensive/book/ch3_dimensionality/ch3_dimensional_exuberance.rmd",
		"/home/mackenza/Documents/data_intensive/book/data_book.sublime-project",
		"/home/mackenza/.config/sublime-text-3/Packages/User/elements.sublime-snippet",
		"/home/mackenza/gdrive/Socialising Big Data/Collaboratory1_DNA_Data/todo_list.txt",
		"/home/mackenza/Documents/notes/mitchell_machine_learning_2007.mdown",
		"/usr/local/lib/python2.7/dist-packages/pandas-0.12.0-py2.7-linux-x86_64.egg/pandas/core/series.py",
		"/home/mackenza/.config/sublime-text-3/Packages/Jedi - Python autocompletion/Default.sublime-keymap",
		"/home/mackenza/Documents/data_intensive/book/ml_lit/ML_literature.py",
		"/home/mackenza/Documents/notes/wasserman_all_of_stat2003.mdown",
		"/tmp/.fr-xOYGVF/pgb20021231rpt.txt",
		"/home/mackenza/.config/sublime-text-3/Packages/LaTeXTools/LaTeXTools Preferences.sublime-settings",
		"/home/mackenza/.config/sublime-text-3/Packages/User/myknitr.sublime-build",
		"/home/mackenza/.config/sublime-text-3/Packages/User/Preferences.sublime-settings",
		"/home/mackenza/.config/sublime-text-3/Packages/Default/Preferences.sublime-settings",
		"/home/mackenza/Documents/data_intensive/book/ch3_dimensionality/tree.md",
		"/home/mackenza/.config/sublime-text-3/Packages/User/kcf.sublime-snippet",
		"/home/mackenza/C:\\nppdf32Log\\debuglog.txt",
		"/home/mackenza/Documents/data_intensive/book/ch3_dimensionality/tree.py",
		"/home/mackenza/Documents/data_intensive/book/ch3_dimensionality/figure/tree_graph.svg",
		"/usr/lib/pymodules/python2.7/matplotlib/pyplot.py",
		"/usr/local/lib/python2.7/dist-packages/scikit_learn-0.14.1-py2.7-linux-x86_64.egg/sklearn/tree/tree.py",
		"/usr/local/lib/python2.7/dist-packages/scikit_learn-0.14.1-py2.7-linux-x86_64.egg/sklearn/datasets/base.py",
		"/home/mackenza/.config/sublime-text-3/Packages/Pylinter/Default (Linux).sublime-keymap",
		"/usr/local/lib/python2.7/dist-packages/scikit_learn-0.14.1-py2.7-linux-x86_64.egg/sklearn/__init__.py",
		"/home/mackenza/Documents/admin/sublime_text_licence.txt",
		"/home/mackenza/.config/sublime-text-3/Packages/User/Pandown.sublime-settings",
		"/home/mackenza/Documents/data_intensive/book/.gitignore",
		"/home/mackenza/.cache/.fr-xI8lEo/knitr-Markdown.sublime-build",
		"/home/mackenza/Documents/data_intensive/book/ch3_dimensionality/pandoc-config.json",
		"/home/mackenza/.config/sublime-text-3/Packages/Pandown/Pandown.sublime-settings",
		"/home/mackenza/Documents/data_intensive/book/test.rmd",
		"/home/mackenza/.cache/.fr-hmwA6d/knitr-Markdown.sublime-build",
		"/home/mackenza/Documents/data_intensive/book/ch3_dimensionality/ch3_dimensional_exuberance.pdf",
		"/home/mackenza/.config/sublime-text-3/Packages/User/R.sublime-snippet",
		"/home/mackenza/Documents/data_intensive/book/ml_lit/data/pattern_recognition_WOS/savedrecs(86).txt",
		"/home/mackenza/.cache/.fr-6rFbxj/pandoc-1.11.1/INSTALL",
		"/home/mackenza/.cache/.fr-uxwHMH/pandoc-1.11.1/README",
		"/home/mackenza/.cache/.fr-LeECTp/README.mdown",
		"/home/mackenza/.cache/.fr-qK7GaY/Pandown Markdown.sublime-build",
		"/home/mackenza/.cache/.fr-qK7GaY/pandoc-config.json",
		"/home/mackenza/Desktop/runif.R",
		"/home/mackenza/Documents/data_intensive/book/test.md",
		"/home/mackenza/Documents/data_intensive/book/pytest.Rmd",
		"/home/mackenza/Documents/data_intensive/book/ch1_learning/__init__.py",
		"/home/mackenza/Documents/data_intensive/book/__init__.py",
		"/home/mackenza/Documents/data_intensive/book/ch5_probability/# Naive and informed chances.rmd",
		"/home/mackenza/Documents/data_intensive/book/pytest.md",
		"/home/mackenza/Documents/data_intensive/lse_cambridge_markets_algorithms/markets_algorithms.Rmd",
		"/home/mackenza/.config/sublime-text-3/Packages/Enhanced-R/Enhanced-R.sublime-settings",
		"/home/mackenza/.config/sublime-text-3/Packages/User/Default (Linux).sublime-keymap",
		"/home/mackenza/.config/sublime-text-3/Packages/Default/Default (Linux).sublime-keymap",
		"/home/mackenza/Documents/data_intensive/warwick_june2013/warwick_presentation.rmd",
		"/home/mackenza/Documents/data_intensive/book/ch3-dimensionality/ch3_dimensional_exuberance.rmd",
		"/home/mackenza/Documents/data_intensive/book/references/machine_learning.bib",
		"/home/mackenza/Documents/data_intensive/book/ml_lit/ml_lit_anal.py",
		"/home/mackenza/R/corpus_constructors.R",
		"/home/mackenza/.config/sublime-text-3/Packages/User/Enhanced-R.sublime-settings",
		"/home/mackenza/.config/sublime-text-3/Packages/Enhanced-R/README.md",
		"/home/mackenza/Documents/data_intensive/book/ml_lit/ml_coword_analysis.py",
		"/home/mackenza/Documents/data_intensive/book/techniques.md",
		"/home/mackenza/Documents/data_intensive/book/ch8-conclusion/ch8_conclusion.rmd",
		"/home/mackenza/R/citation-network-analysis/Citation Network Analysis.ipynb",
		"/home/mackenza/Documents/notes/deleuze_guattari_what_is_1997.md",
		"/home/mackenza/Documents/notes/Malley_Statistical Learning_2011.md",
		"/home/mackenza/Documents/notes/Hastie_2008.md",
		"/home/mackenza/Documents/notes/James_1996.md",
		"/home/mackenza/Documents/notes/Negri-HardtCommonwealth-2009.md",
		"/home/mackenza/R/coursera/ml-ng/ex1/computeCost.R",
		"/home/mackenza/Documents/data_intensive/book/proposal.rmd",
		"/home/mackenza/R/coursera/ml-ng/ex1/gradientDescent.R",
		"/home/mackenza/Documents/data_intensive/book/ch2-curves/pandoc.sh",
		"/home/mackenza/Documents/data_intensive/book/technique_demos.rmd",
		"/home/mackenza/Documents/notes/Whitehead_PR.mdown",
		"/home/mackenza/Documents/notes/Whitehead_science_mw.mdown",
		"/home/mackenza/Documents/data_intensive/book/functions.mdown",
		"/home/mackenza/Documents/data_intensive/book/ch2-curves/ml_lit_anal.py",
		"/home/mackenza/Downloads/bar_stacked.py",
		"/home/mackenza/Documents/data_intensive/book/ch2-curves/google_scholar_parser.py",
		"/home/mackenza/Desktop/test.rmd",
		"/home/mackenza/Documents/data_intensive/book/ch2-curves/ch2_curves_function.rmd",
		"/home/mackenza/R/coursera/ml-ng/mlclass-ex3/ex3.m",
		"/home/mackenza/.config/sublime-text-3/Packages/User/gd.sublime-snippet",
		"/tmp/savedrecs-1.txt",
		"/home/mackenza/Documents/data_intensive/book/ch2-curves/test.md",
		"/home/mackenza/R/coursera/ml-ng/ex1/ex1.R",
		"/home/mackenza/Documents/notes/james_pluralistic_1996.odt",
		"/home/mackenza/R/coursera/ml-ng/mlclass-ex2/ex2.R",
		"/home/mackenza/R/coursera/ml-ng/ex1/computeCostMulti.m",
		"/home/mackenza/Documents/data_intensive/transformation_soc_sci_oxford_2013/mackenzie_slides_oxford_feb_2013.Rmd",
		"/home/mackenza/Documents/notes/Wilson,_Affect and Artificial Intelligence2009.md",
		"/home/mackenza/Documents/data_intensive/book/animations/grad_desc.R",
		"/home/mackenza/Documents/data_intensive/book/pandoc.sh",
		"/home/mackenza/Documents/data_intensive/book/build.sh",
		"/home/mackenza/Documents/data_intensive/book/references/Exported Items.bib",
		"/media/0031-014A/digital contours_ software materiality_review.md",
		"/home/mackenza/Documents/data_intensive/book/ch1/ch1_praxis.rmd",
		"/tmp/data_forms/ch1-learning/ch1_praxis.rmd",
		"/home/mackenza/Documents/data_intensive/book/ch1_praxis.md",
		"/home/mackenza/Documents/data_intensive/book/ch1_praxis.rmd",
		"/home/mackenza/Documents/data_intensive/book/knit_all.sh",
		"/var/tmp/kdecache-mackenza/krun/19894.0.international_collaboration.shtml",
		"/home/mackenza/R/coursera/ml-ng/ex1/gradientDescent.m",
		"/home/mackenza/R/coursera/ml-ng/ex1/computeCost.m",
		"/home/mackenza/R/coursera/ml-ng/ex1/ex1.m",
		"/home/mackenza/R/coursera/ml-ng/ex1/ex1data1.txt",
		"/home/mackenza/bank.txt",
		"/home/mackenza/.cache/.fr-k5nMam/ghProjectInfo2013-Feb.txt",
		"/home/mackenza/Documents/data_intensive/book/knit_all.R",
		"/home/mackenza/Documents/data_intensive/book/proposal.md",
		"/home/mackenza/Documents/notes/hayles_my_mother_2005.mdown",
		"/home/mackenza/Dropbox/epistle/Data forms.txt",
		"/home/mackenza/Desktop/jss725.Rnw",
		"/home/mackenza/Documents/notes/milleplateux.md",
		"/home/mackenza/Desktop/jss725-tikzDictionary",
		"/home/mackenza/Documents/data_intensive/book/template.latex",
		"/home/mackenza/Documents/data_intensive/book/references/refs.bib",
		"/home/mackenza/.cache/.fr-BlOcAR/2012-04-08-9.json",
		"/home/mackenza/Documents/google_analytics_spet2012/google.sublime-workspace",
		"/home/mackenza/Documents/notes/dewey_reconstruction_1957.odt",
		"/home/mackenza/Documents/data_intensive/book/Sources.md",
		"/home/mackenza/Desktop/drawing-1.svg",
		"/home/mackenza/Documents/data_intensive/book/markets_algorithms.Rmd"
	],
	"find":
	{
		"height": 29.0
	},
	"find_in_files":
	{
		"height": 105.0,
		"where_history":
		[
			"/home/mackenza/Documents/data_intensive/book/ml_lit",
			""
		]
	},
	"find_state":
	{
		"case_sensitive": true,
		"find_history":
		[
			"overline",
			"100",
			"trim_edges",
			"wasserman_all_2003",
			"bellman_adaptive_1961",
			"function",
			"classifier",
			"wu_top_2008",
			"df",
			"cow",
			"topics",
			"feature",
			"artificial",
			"nomenclature",
			"quinlan_induction_1986",
			"domingos_few_2012",
			"classification",
			"bogost_alien_2012",
			"matplotlib.pyplo",
			"matplotlib",
			"ml.",
			"iris_tree",
			"breiman_cart_1983",
			"CART",
			"ir.rp",
			"rp",
			"    if wos_df.columns.tolist().count('CR') > 0:\n",
			"`iris`",
			"doyle_use_1973",
			"einhorn_alchemy_1972",
			"ml_df",
			"ml_dir",
			"ml_df",
			"ml",
			"ml_dir",
			">>>",
			"whitehead_modes_1956",
			"The top ten",
			">\n>\n",
			"}}\",\n",
			"Friedman",
			"dir",
			"df",
			"cortes_support",
			"malley_statistical_2011",
			"Hastie",
			"breiman_random_2001",
			"coword_matrix",
			"enter",
			"map",
			"Top Ten",
			"g2",
			"g",
			"tg",
			"sc",
			"] ",
			"i2",
			"i1",
			"kc",
			"kc ",
			"E_ij",
			"df",
			"dir",
			"landeker",
			"landecker",
			"de_set1",
			"de_all1",
			"science brings to light partial observers ",
			"Whitehead",
			"value",
			"values",
			"value",
			"white",
			"bogost_alien_2012",
			"bogo",
			"imdf",
			"Rabinow",
			"rabinow",
			"rabin",
			"lr",
			"wasserman",
			"logistic",
			"beta",
			"theta",
			"KDD",
			"conway_machine_2012",
			"conway",
			"Ng",
			"library",
			"white",
			"X_3",
			"X_2",
			"colmeans",
			"gradientDescent_two_dim",
			"    >",
			"print",
			"The recent",
			"technique_demos",
			"Whitehead",
			"savage",
			"savag",
			"jss",
			"logo",
			"month",
			"biagioli_situated_1999",
			"munster",
			"latour_drawing_1990",
			"carlsson_topology_2009",
			"carl",
			"lury_introduction_2012",
			"conn",
			"Law",
			"SRA",
			"Law",
			" 'months')",
			"'months'",
			"sra_con",
			"runs",
			"sapply",
			"runs",
			"samples",
			"studies",
			"experiments",
			"Ankeny",
			"especially",
			"ncbi_stat",
			"img/",
			"SRA"
		],
		"highlight": true,
		"in_selection": false,
		"preserve_case": true,
		"regex": false,
		"replace_history":
		[
			"breiman_cart_1984",
			"_iris_",
			"wos_df",
			"df",
			"—"
		],
		"reverse": false,
		"show_context": true,
		"use_buffer2": true,
		"whole_word": false,
		"wrap": true
	},
	"groups":
	[
		{
			"selected": 4,
			"sheets":
			[
				{
					"buffer": 0,
					"file": "proposal.rmd",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 38966,
						"regions":
						{
						},
						"selection":
						[
							[
								28470,
								28470
							]
						],
						"settings":
						{
							"WordCountShouldRun": true,
							"syntax": "Packages/knitr/knitr-Markdown.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 6678.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 1,
					"file": "ch0_introduction/ch0_introduction.rmd",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 2388,
						"regions":
						{
						},
						"selection":
						[
							[
								784,
								784
							]
						],
						"settings":
						{
							"WordCountShouldRun": true,
							"syntax": "Packages/knitr/knitr-Markdown.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 186.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 2,
					"file": "ch1_learning/ch1_praxis.rmd",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 74038,
						"regions":
						{
						},
						"selection":
						[
							[
								1762,
								1762
							]
						],
						"settings":
						{
							"WordCountShouldRun": true,
							"syntax": "Packages/Markdown/Markdown.tmLanguage",
							"tab_size": 4,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 253.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 3,
					"file": "ch2_curves/ch2_curves_function.rmd",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 74379,
						"regions":
						{
						},
						"selection":
						[
							[
								19366,
								19366
							]
						],
						"settings":
						{
							"WordCountShouldRun": true,
							"syntax": "Packages/knitr/knitr-Markdown.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 5440.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 4,
					"file": "ch3_dimensionality/ch3_dimensional_exuberance.rmd",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 69255,
						"regions":
						{
						},
						"selection":
						[
							[
								13804,
								13804
							]
						],
						"settings":
						{
							"WordCountShouldRun": true,
							"syntax": "Packages/knitr/knitr-Markdown.tmLanguage",
							"tab_size": 4,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 4763.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 5,
					"file": "ch7_topologies/ch7_genomic_topologies.rmd",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 101,
						"regions":
						{
						},
						"selection":
						[
							[
								101,
								101
							]
						],
						"settings":
						{
							"WordCountShouldRun": true,
							"syntax": "Packages/knitr/knitr-Markdown.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 6,
					"file": "ch4_subjects/ch4_learning_subjects.rmd",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 40264,
						"regions":
						{
						},
						"selection":
						[
							[
								980,
								980
							]
						],
						"settings":
						{
							"WordCountShouldRun": true,
							"syntax": "Packages/knitr/knitr-Markdown.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 7,
					"file": "ch5_probability/ch5_naive_informed.rmd",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 270,
						"regions":
						{
						},
						"selection":
						[
							[
								270,
								270
							]
						],
						"settings":
						{
							"WordCountShouldRun": true,
							"syntax": "Packages/knitr/knitr-Markdown.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 8,
					"file": "/home/mackenza/Documents/notes/wasserman_all_of_stat2003.mdown",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 964,
						"regions":
						{
						},
						"selection":
						[
							[
								739,
								739
							]
						],
						"settings":
						{
							"WordCountShouldRun": true,
							"syntax": "Packages/Markdown/Markdown.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 9,
					"file": "/home/mackenza/Documents/notes/Breiman_2001_andom Forests.md",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 1962,
						"regions":
						{
						},
						"selection":
						[
							[
								1690,
								1690
							]
						],
						"settings":
						{
							"WordCountShouldRun": true,
							"auto_name": "Breiman 2001, Random Forests",
							"syntax": "Packages/Markdown/Markdown.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 10,
					"file": "/home/mackenza/Documents/notes/Breiman_Friedman.md",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 2722,
						"regions":
						{
						},
						"selection":
						[
							[
								26,
								26
							]
						],
						"settings":
						{
							"WordCountShouldRun": true,
							"auto_name": "Breiman & Friedman",
							"syntax": "Packages/Markdown/Markdown.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 11,
					"file": "/home/mackenza/Documents/notes/mitchell_machine_learning_2007.mdown",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 243,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"WordCountShouldRun": true,
							"syntax": "Packages/Markdown/Markdown.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 12,
					"file": "proposal.md",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 38876,
						"regions":
						{
						},
						"selection":
						[
							[
								22136,
								22136
							]
						],
						"settings":
						{
							"WordCountShouldRun": true,
							"syntax": "Packages/Markdown/Markdown.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 5808.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 13,
					"file": "ch3_dimensionality/tree.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 1772,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"WordCountShouldRun": true,
							"syntax": "Packages/Python/Python.tmLanguage",
							"tab_size": 4,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 14,
					"file": "ml_lit/ml_lit_anal.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 14668,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"WordCountShouldRun": true,
							"syntax": "Packages/Python/Python.tmLanguage",
							"tab_size": 4,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 15,
					"file": "ml_lit/web_of_science.R",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 6762,
						"regions":
						{
						},
						"selection":
						[
							[
								646,
								646
							]
						],
						"settings":
						{
							"WordCountShouldRun": true,
							"syntax": "Packages/Enhanced-R/Support/Enhanced-R.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 3929.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 16,
					"file": "/home/mackenza/Documents/notes/Xue_SVM: Support Vector Machines_2009.md",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 879,
						"regions":
						{
						},
						"selection":
						[
							[
								422,
								422
							],
							[
								878,
								878
							]
						],
						"settings":
						{
							"WordCountShouldRun": true,
							"auto_name": "# SVM: Support Vector Machines, Xue in Xindong Wu,",
							"syntax": "Packages/Markdown/Markdown.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 17,
					"file": "/home/mackenza/R/web_of_science.R",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 6776,
						"regions":
						{
						},
						"selection":
						[
							[
								565,
								565
							]
						],
						"settings":
						{
							"WordCountShouldRun": true,
							"syntax": "Packages/Enhanced-R/Support/Enhanced-R.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 18,
					"file": "/home/mackenza/R/phrase_structures.R",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 8007,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"WordCountShouldRun": true,
							"syntax": "Packages/Enhanced-R/Support/Enhanced-R.tmLanguage",
							"translate_tabs_to_spaces": false
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 19,
					"file": "/home/mackenza/Documents/notes/Malley_Statistical Learning_2011.md",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 2605,
						"regions":
						{
						},
						"selection":
						[
							[
								2171,
								2171
							]
						],
						"settings":
						{
							"WordCountShouldRun": true,
							"syntax": "Packages/Markdown/Markdown.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 186.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 20,
					"file": "/home/mackenza/Documents/notes/Hastie_2008.md",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 5214,
						"regions":
						{
						},
						"selection":
						[
							[
								4976,
								4976
							]
						],
						"settings":
						{
							"WordCountShouldRun": true,
							"syntax": "Packages/Markdown/Markdown.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 1267.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 21,
					"file": "/home/mackenza/Documents/notes/Stengers_d&G_2005.txt",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 3125,
						"regions":
						{
						},
						"selection":
						[
							[
								1135,
								1135
							]
						],
						"settings":
						{
							"WordCountShouldRun": true,
							"syntax": "Packages/Text/Plain text.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 22,
					"file": "/home/mackenza/Documents/notes/vapnik_nature of statistical learning theory 2000.md",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 2305,
						"regions":
						{
						},
						"selection":
						[
							[
								876,
								876
							]
						],
						"settings":
						{
							"WordCountShouldRun": true,
							"auto_name": "vapnik_nature of statistical learning theory 2000",
							"syntax": "Packages/Markdown/Markdown.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 110.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 23,
					"file": "/home/mackenza/Documents/notes/bogost_alien_2012.md",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 944,
						"regions":
						{
						},
						"selection":
						[
							[
								8,
								8
							]
						],
						"settings":
						{
							"WordCountShouldRun": true,
							"auto_name": "bogost",
							"syntax": "Packages/Markdown/Markdown.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 24,
					"file": "/home/mackenza/Documents/notes/WhiteheadModes.mdown",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 5877,
						"regions":
						{
						},
						"selection":
						[
							[
								4338,
								4338
							]
						],
						"settings":
						{
							"WordCountShouldRun": true,
							"syntax": "Packages/Markdown/Markdown.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 1380.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 25,
					"file": "/home/mackenza/Documents/notes/rabinow_anthropos_2003.md",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 4283,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"WordCountShouldRun": true,
							"auto_name": "Rabinow, P. 2003. Anthropos Today. Reflections on",
							"syntax": "Packages/Markdown/Markdown.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 26,
					"file": "references/data_forms_thought.bib",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 23988,
						"regions":
						{
						},
						"selection":
						[
							[
								10052,
								10052
							]
						],
						"settings":
						{
							"WordCountShouldRun": true,
							"syntax": "Packages/LaTeX/Bibtex.tmLanguage",
							"translate_tabs_to_spaces": false
						},
						"translation.x": 0.0,
						"translation.y": 6407.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 27,
					"file": "references/refs.bib",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 41626,
						"regions":
						{
						},
						"selection":
						[
							[
								29180,
								29180
							]
						],
						"settings":
						{
							"WordCountShouldRun": true,
							"syntax": "Packages/LaTeX/Bibtex.tmLanguage",
							"translate_tabs_to_spaces": false
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"type": "text"
				},
				{
					"buffer": 28,
					"file": "references/machine_learning.bib",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 42511,
						"regions":
						{
						},
						"selection":
						[
							[
								42106,
								42106
							]
						],
						"settings":
						{
							"WordCountShouldRun": true,
							"syntax": "Packages/LaTeX/Bibtex.tmLanguage",
							"translate_tabs_to_spaces": false
						},
						"translation.x": 0.0,
						"translation.y": 2289.0,
						"zoom_level": 1.0
					},
					"type": "text"
				}
			]
		}
	],
	"incremental_find":
	{
		"height": 39.0
	},
	"input":
	{
		"height": 38.0
	},
	"layout":
	{
		"cells":
		[
			[
				0,
				0,
				1,
				1
			]
		],
		"cols":
		[
			0.0,
			1.0
		],
		"rows":
		[
			0.0,
			1.0
		]
	},
	"menu_visible": true,
	"output.exec":
	{
		"height": 197.0
	},
	"project": "data_book.sublime-project",
	"replace":
	{
		"height": 54.0
	},
	"save_all_on_build": true,
	"select_file":
	{
		"height": 0.0,
		"selected_items":
		[
			[
				"vap",
				"~/Documents/notes/vapnik_nature of statistical learning theory 2000.md"
			],
			[
				"",
				"ml_lit/ml_lit_anal.py"
			],
			[
				"ml",
				"ml_lit/ml_lit_anal.py"
			],
			[
				"svg",
				"ch3_dimensionality/figure/tree_graph.svg"
			],
			[
				"h",
				"~/Documents/notes/Hastie_2008.md"
			],
			[
				"data",
				"references/data_forms_thought.bib"
			],
			[
				"ref",
				"references/refs.bib"
			],
			[
				"da",
				"~/Documents/data_intensive/book/references/data_forms_thought.bib"
			],
			[
				"dat",
				"~/Documents/data_intensive/book/references/data_forms_thought.bib"
			],
			[
				"refs",
				"~/Documents/data_intensive/book/references/refs.bib"
			],
			[
				"re",
				"book/references/refs.bib"
			]
		],
		"width": 0.0
	},
	"select_project":
	{
		"height": 500.0,
		"selected_items":
		[
			[
				"",
				"~/Documents/reviews/current_reviews.sublime-project"
			]
		],
		"width": 380.0
	},
	"select_symbol":
	{
		"height": 201.0,
		"selected_items":
		[
			[
				"wass",
				"wasserman_all_2003"
			],
			[
				"bellman",
				"bellman_adaptive_1961"
			],
			[
				"qui",
				"quinlan_c4._1993"
			],
			[
				"wu",
				"wu_top_2008"
			],
			[
				"domin",
				"domingos_few_2012"
			],
			[
				"bog",
				"bogost_alien_2012"
			],
			[
				"bre",
				"breiman_cart:_1984"
			],
			[
				"mor",
				"morgan_problems_1963"
			],
			[
				"doyl",
				"doyle_use_1973"
			],
			[
				"einhor",
				"einhorn_alchemy_1972"
			],
			[
				"morng",
				"morgan_problems_1963"
			],
			[
				"morgan",
				"morgan_problems_1963"
			],
			[
				"white",
				"whitehead_modes_1956"
			],
			[
				"bra",
				"breiman_random_2001"
			],
			[
				"cort",
				"cortes_support-vector_1995"
			],
			[
				"stei",
				"steinberg_cart:_2009"
			],
			[
				"mall",
				"malley_statistical_2011"
			],
			[
				"brei",
				"breiman_random_2001"
			],
			[
				"rab",
				"rabinow_anthropos_2003"
			]
		],
		"width": 647.0
	},
	"settings":
	{
	},
	"show_minimap": true,
	"show_open_files": true,
	"show_tabs": true,
	"side_bar_visible": true,
	"side_bar_width": 338.0,
	"status_bar_visible": true,
	"template_settings":
	{
	}
}
