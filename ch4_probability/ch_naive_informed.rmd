
\chapter{$N = all$: machine learning gets all the data}
\label{ch:probability}

# {$N = all$: machine learning gets all the data}

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=12, tidy=TRUE, fig.height=8, 
                      echo=FALSE,  results='hide', warning=FALSE, message=FALSE, dev='pdf')
library(RSQLite)
con <- dbConnect(RSQLite::SQLite(),'../ml_lit/all_refs.sqlite3')
res = dbGetQuery(con, statement ="select * from basic_refs limit 10;")
library(ggplot2)
library(stringr)
library(xtable)
options(xtable.comment = FALSE)
simpleCap <- function(x) {
    s <- strsplit(x, " |-")[[1]]
    return (paste(toupper(substring(s, 1,1)), tolower(substring(s, 2)), sep="", collapse=" "))
  }
```

In the final pages of _The Taming of Chance_, the philosopher Ian Hacking describes the work of the geodesic philosopher C.S. Peirce in terms of a twin affirmation of chance. On the one hand, Peirce, following the work of the psychophysicist Gustav Fechner and before him the astronomer-sociologist Adolphe Quetelet, makes the normal curve into an underlying reality.[^4.1] The 'personal equation,' the variation in measurements made by any observer, become 'a reality underneath the phenomena of consciousness' [@Hacking_1990, 205]. At the same time, and in order to show the underlying reality of the normal curve, 'Peirce deliberately used the properties of chance devices to introduce a new level of control into his experimentation. Control not by getting rid of chance fluctuations, but by adding some more' [@Hacking_1990, 205]. Peirce's belief in absolute chance, 'a universe of chance' as Hacking puts it, succeeded a series of 'realizations' of curves, in which first social, then biological and finally psychological variations were all understood as evidence of a generative function, the normal distribution or Gaussian function. \index{function!Gaussian} In the century or so since, what happened to the thorough-going affirmation of statistical thought and probabilistic practice epitomised by Peirce? Hacking stresses that he does not understand Peirce as the precursor or the innovator of twentieth century statistical thought (Hacking's _Taming of Chance_ ends at 1900), but rather as 'the first philosopher completely to internalize the way chance had been tamed in the nineteenth century' (215). What would the equivalent philosopher-machine learner internalize today? What would such persons, working in science or media or government, hold firm in relation to chance, probability and statistics? \index{Hacking, Ian!on C.S. Peirce}

[^4.1]: The historian of statistics Stephen Stigler provides a lengthy account of Fechner's work in [@Stigler_1986, 239-259].

In a broad sense, the setting that concerned Peirce in his work at the U.S. Government Coast Survey in the 1870s does not differ greatly from the setting that machine learners encounter. In the opening lines of the First Edition of _Elements of Statistical Learning_, Hastie, Tibshirani and Friedman write:

>The field of Statistics is constantly challenged by the problems that science and industry brings to its door. In the early days, these problems often came from agricultural and industrial experiments and were relatively small in scope [@Hastie_2009, xi]

At the end of the preface, they also cite, we might note in passing, Hacking's work: 'The quiet statisticians have changed our world' [@Hastie_2009, xii]. With some justification, we might ask therefore: what difference do the 'vast amounts of data ... generated in many fields' (xi) make to what machine learners internalize of their world? This question of how worlds becomes thinkable through machine learning can be addressed partly by contrasting the 'taming of chance' achieved  eighteenth and nineteenth century statistical, and the statistical practices of machine learning today. Is machine learning a further taming of chance? What role does randomness and probability play in machine learning? \index{machine learning!statistical practices}

The broadest claim associated with statistical machine learning might be the simple expression shown in:

\begin {equation}
\label {eq:n_all}
N = \forall X
\end {equation}

In Equation \ref{eq:n_all}, $N$ refers to the number of observations (and hence the size of the dataset), the symbol $\forall$ means 'all' since this is the level of inclusion which many fields of knowledge in science, government, media, commerce and industry envisage, and $X$ refers to the data itself arrayed in common vector space. Note that this expression leaves some things out. $Y$, the response variable, for instance, may or may not be known.  While both the expansion of data in the common vector space and the machine learners that traverse it have appeared in previous chapters, I focus here  on changes in probability practices associated with machine learning, and in particular, $N = \forall X$, the claim that with all the data, statistical thinking and the potentials of the statistical reasoning fundamentally change. The claim that with $N=\forall X$ everything changes has been widely discussed.[^4.2]  Viktor Mayer-Schönberger and Kenneth Cukier's _Big Data: A Revolution That Will Transform How We Live, Work and Think_  present this shift in many different ways in the course of the vignettes and comparisons that have become typical of the data revolution genre. In a chapter entitled 'More,' they sketch the transition from data practices reliant on sampling to data practices that deal with all the data:

>Using all the data makes it possible to spot connections and details that are otherwise cloaked in the vastness of the information. For instance, the detection of credit card fraud works by looking for anomalies, and the best way to find them is to crunch all the data rather than a sample [@Mayer-Schonberger_2013, 2013,  27]

In the several hundred pages that follow in _Big Data_, the problem of how to 'crunch all the data' is not a major topic. \index{Mayer-Schőnberger, Viktor} \index{Cukier, Kenneth} While they mention the role of social network theory (30), 'sophisticated computational analysis' (55),  'predictive analytics' (58) and 'correlations' (7),  and they observe that 'the revolution' is 'about applying math to huge quantities of data in order to infer probabilities' (12), any further consideration of a change in statistical practices is largely confined to a business-oriented contrast between having some of the data and having all the data (that is, businesses often have all the data on their customers). This is not to criticize a book that sets out to describe trends affecting business and government for a general readership, but without a sense of how statistical thinking animates almost all salient features of crunching the data and particularly the predictions, it becomes hard to see how the 'revolution' takes place. Just as nineteenth century statistics transformed measurements (for instance, the mean as average of all measured values) into real quantities (for instance, mean as the ideal or abstract property of a population; e.g. life expectancy), the shift between $n$ and $\forall X$, a shift very much animating and dependent on  machine learning, internalizes, I will suggest, a probability and chance into machinic operations. This is a statistical event akin to the advent of the Normal distribution (and indeed, $\mathnormal{N}$ is a standard symbol for the Normal distribution in statistics textbooks) as a way of knowing and controlling populations [@Hacking_1975, 108]. 

## Machine learning as statistics inside out

In   _The Taming of Chance_, Hacking argues that modern statistical thought transposed a way of calculating errors in experimental measurements and astronomical observations into real quantities typically described by the normal distribution. \index{statistics!history of!from error to real quantity} This transposition or inversion relied on four intermediate steps passing through probability calculus (particularly the work of Jacob Bernoulli and the binomial or heads-tails probability distribution in the 1690s [@Hacking_1975, 143]), on large numbers of measurements (the most famous being the chest measurements of soldiers in Scottish regiments, but these were only one flurry amidst an avalanche of numbers in the 1830-1840s), on an idea of multiple, minute independent causes producing events (particularly as developed in medicine but also in studies of crime), and the law of errors applying to measurements made by, amongst others, astronomers [@Hacking_1990, 111-112]. \index{statistics!probability distributions!normal} As Hacking observes, coins, suicides, crime, chest measurements, and astronomical observations all come together in a picture of statistical reality which remains, although somewhat blurred, indelible in contemporary statistical thought. In this entanglement, observers and the observed changed places. The distribution of errors made by astronomers measuring the position of stars or planets became a distribution or variation inherent in things.  All of this seems a long way from machine learning, and in terms of years, the work of figures such as Poisson, Laplace, Quetelet and even Galton, is well-removed. In terms of tables and functions (the concerns of the preceding two chapters), the distance is not so great. There is greater variety in tables (partly due to the common vector space) and in functions, but the entwining or even swapping between what relates to an observation and what concerns the real, continues. Machine learners engage in that swapping or re-distributing of numbers all the time. Viewed from the standpoint of Hacking, machine learning reverse-engineers the invention of modern statistical thinking.  It takes back the 'real quantities' that modern statistics had attributed to the populations of the world and puts them into devices, machine learners that people then observe, monitor and indeed measure again in many ways. The direct swapping between uncertainty in measurement and variation in real attributes that statistics achieved now finds itself re-routed and intensified as machine learners measure devices. 

This swapping or re-distribution is not a simple operation of attribution, as if machine learners somehow mistake a device for the world. Machine learners do constantly take statistical thinking as a basic semantic frame for their devices. When Hastie and co-authors write (as we saw in the last chapter) 'our goal is to find a useful approximation $\hat(f)(x)$ to the function $f(x)$ that underlies the predictive relationship between input and output' [@Hastie_2009, 28], they invoke the 'real quantities' first elaborated and articulated by statistical thinkers such as Quetelet.  The major structuring differences in machine learning as a field of knowledge-practice show the marks of this commitment to the reality of the statistical.

\begin{table}
\centering
\begin{tabular}{|l|l|}
\hline
parametric &  non-parametric \\
bias &        variance \\
prediction &  inference \\
generative &  discriminative \\
\hline
\end{tabular}
\caption{Some structuring differences in machine learning}
\lab{tab:ml_diffs}
\end{table}

Reading and working with machine learning techniques usually means encountering and responding to some of that statistical apparatus drawn from statistics, but these are not typically the statistical tests of significance or variation. In contrast to a statistics textbook such as the widely used _Basic Practice of Statistics_ [@Moore_2009] or even a more advanced guide such as _All of Statistics_ [@Wasserman_2003], where statistics (t-test, chi-squared test, etc.) hypothesis testing, and analysis of uncertainties (confidence intervals, etc) order the exposition, the machine learning texts invoke a thoroughly probabilistic conceptual apparatus curiously de-populated of the community fo practice found in statistics. Statistical underpinnings may be fundamental, but this does not mean that  machine learners simply automate statistics.[^4.4] For instance,  a basic set of contrasts or indeed oppositions that owe much to statistical thinking pattern statements on machine learning. \index{machine learning!structure differences} The contrasts shown in Table \ref{tab:ml_diffs} all have a statistical facet and anchoring  to them. Some refer to variations and errors that affect how a machine learner refers to data (bias and variance); some designate an underlying statistical intuition about how particular machine learners treat data (does the model seek to generate the data or classify -- discriminate -- it; e.g. Naive Bayes or Latent Dirichlet Allocation are _generative_ models whereas logistic regression or support vector machines are _discriminative_); parametric and non-parametric describe the role of probability distributions in the model; and others indicate different kinds of statistical knowledge practice (prediction seeks to anticipate while inference seeks to interpret, etc.). \index{machine learning!structuring differences} These broad structuring differences reach down deeply into the architecture, the diagrams, the practices, statements and visual objects and computer code associated with $N=\forall X$. Because they  anchor basic elements of  machine learning in statistical statements, a profuse set of techniques and formalisms derived from statistics has come to populate the field, furnishing and rearranging  its diagrammatic references \index{diagram!reference}  to the worlds of industry, agriculture, earth science, genomics, etc,. but also, crucially, triggering ontological mutations in machine learners themselves. [^4.113]  \index{machine learners!ontological mutations of}

HERE

While these structuring differences are practically very important, and deeply shape certain kinds of practice in machine learning, the underlying operator that allows swapping between knowledge and the world, and then between events and devices is probability thinking, and in particular, the functions that describe variations in probability, probability distributions. \index{function!probability distributions} Historically, two distributions loom large. The binomial distribution was explored extensively in the seventeenth and eighteenth centuries in the context of games of chance [@Hacking_1975, 57-134]. The normal distribution pervades nineteenth century statistical thinking as it generalizes across law, medicine, agriculture, finance and not least, sociology. In all of these settings, probability distributions are a common way of showing and  talking about _random variables_ in statistics. These distributions appear in countless shapes and forms in scientific, government and popular literature of many different kinds. Statistical graphics have a rich history and semiology that I do not discuss here (see [@Bertin_1983]). Perhaps the most famous function or mapping is the normal or Gaussian distribution:

\begin {equation}
\label {eq:gaussian_distribution}
f(x;\mu, \sigma^2) = \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}
\end {equation}


The  function shown in equation (\ref{eq:gaussian_distribution}) is the so-called normal or Gaussian distribution. Its  mathematics were intensively worked over during the late eighteenth and early nineteenth centuries in what has been termed 'one of the major success stories in the history of science' [@Stigler_1986, 158], and it has a power-laden biopolitical history closely tied with knowledges and governing of  national and other  populations in terms of morality, health, and wealth (see [@Hacking_1975, 113-124]. The key symbols here include $\mu$, the mean and $\sigma$, the variance, a number that describes how widely dispersed the values of the variable, $x$ are. These two parameters together describe the shape of the curve. Given $\mu$ and $\sigma$, it become possible to map different outcomes to probabilities. Put more statistically, viewed in terms of functions such as the Gaussian distribution, events become random variables. Following the kind of definition that we find in a standard statistics textbook, every variable potentially becomes a function: 'a random variable is a mapping that assigns a real number to each outcome' [@Wasserman_2003,19]. The possibility of treating all variables as random variables, that is, as probability distributions was a significant historical achievement, but one that continues to develop. We might note the real power of probability distributions when conceptualised as real quantities in the world, not epiphenomenal by-products of inaccuracies in our observations or measuring devices. For instance, given the normal distribution, it is possible, under certain circumstances, to effectively subjectify someone on the spot. If an educational psychologist indicates to someone that their intelligence lies towards the left-hand side of the normal curve peak in Figure 2 (and hence less than the population mean), they quickly render them somehow subject to a potentially institutionally and economically consequential trajectory. This is not a recent development. Since its inception in the social physics of Quetelet as a way of referring to a property of populations, the normal curve has promised or threatened to support the re-shaping and control of populations (in terms of health, morality and wealth). Since then, probability distributions and their corresponding curves have multiplies. Subsequent statisticians used dozens of different probability distributions to map continuous and discrete variations to real numbers. Other probability distributions abound — normal (Gaussian), uniform, Cauchy exponential, gamma, beta,  hypergeometric, binomial, Poisson, chi-squared, Dirichlet, Boltzmann-Gibbs distributions, etc. (see [@NIST_2012] for a gallery of distributions) — because outcomes occur in widely differing patterns. The  queuing times at airport check-ins do not, for instance, easily fit a normal distribution. Queues are usually modelled using a Poisson distribution, which unfortunately for travellers, distributes waiting times very differently.  Similarly, it might be better to think of the probability of rain today in Lancaster in terms of a Poisson distribution that models that queue of clouds in the Atlantic just waiting to land on the northwest coast of England. Rather than addressing the question of if it will rain or not, a Poisson-based model might address the question of how soon.

## Conclusion

Mayer-Schönberger and Cukier's argument that having much data or all data ($N=\forall{X}$) is a leitmotif in accounts of predictive modelling and analytics during the last decade.

>Using all the data makes it possible to spot connections and details that are otherwise cloaked in the vastness of the information. For instance, the detection of credit card fraud works by looking for anomalies, and the best way to find them is to crunch all the data rather than a sample [@Mayer-Schonberger_2013, 27]. 

Versions of this claim can be found running through various scientific and business settings throughout the 20th century.[^4.61] In certain settings, $N=all$ has been around for quite a while (as for instance, in many document classification settings where the whole corpus of documents have been electronically curated for decades). Mayer-Schönberger and Cukier rightly emphasize, it seems to me, that the huge quantities of data sluicing through some contemporary infrastructures support inferences of probabilities (11). The way they describe statistical sampling as a concept 'developed to solve a particular problem at a particular moment in time under specific technological constraints' [@Mayer-Schonberger_2013, 31]] does not, however, take into account the material consequences of statistical thinking. They do not suggest that the very possibility of spotting connections or details that might matter deeply itself relies on statistical configurations that deeply affect all machine learners.  Whether or not someone uses Naive Bayes, a topic model, neural networks or logistic regression, probabilistic practice, random variables,  probability distributions and statistical inferences run deep in the model. In many ways, the Mayer-Schönberger and Cukier account pays so much attention to the potentials of data accumulation that they cannot easily attend to the question of what happens to the data as people try to 'spot connections and details', or of how what is put into the data that goes beyond $N=all$.  Here sampling, estimation, likelihoods, and a whole gamut of dynamic relationships between random variables in joint probability distributions reassert themselves. The data may not be sampled, but the model is sampling as it tries to move through the high-dimensional feature spaces opened up by having 'all' the data. 


Machine learning inhabits worlds that had already witnessed the advent of  statistical realities  at least a century earlier, whether through the social physics of Quetelet, the biopolitical normals of Francis Galton and his regression to the mean (remember that the linear model of regression is probably the basic machine learning model) or later in the probability functions of quantum mechanics in early twentieth century physics. I have been suggesting the machine learning should be understood as a re-inversion of statistical thought. In this inversion, the probability distributions that had become the underlying reality of many different kinds of populations fold back or re-distribute themselves in devices such as machine learners whose variations and uncertainties powerfully generate classifications, predictions and knowledge statements (scientific and otherwise). Populations of models are sampled, measured, and aggregated in the ongoing production of statistical realities whose object is no longer a property of individual members of a population (their height, their life-expectancy, their chance of HIV/AIDS), but a meta-population of models of populations and their probability distributions. We should note that not all machine learners are strictly speaking probabilistic models [^4.60], but the referential relation of any machine learner to the world is statistical by virtue of the both the ancestral probabilities and the bias-variance decomposition. 


The re-distribution of probability that takes place in machine learning through multiple-swapping of observational positions bears several implications for thinking about the power of the techniques. The lure of the techniques is a kind of trap since the swapping or inversion of statistical thought takes what was put into the world by modern statistical thought and puts it into machines whose intricate workings themselves become the event to be observed. At the same time, although taking back the real quantities, the machine learners remain closely in contact with worlds populated by texts, documents, images, prices, sensor measurements, transactions and records. 


[^4.2]: Rob Kitchin provides a very useful overview of these claims in [@Kitchin_2014]. While I will not analyse the claims about 'big data' in specific cases in any great detail, the growing literature on this topic suggests that machine learning in its various operations -- epistopic construction of common vector space, function finding as association of partial observers and a re-internalisation of probability -- generates considerable difficulties and challenges for knowledge, power and production. 

[^4.20]: See chapter \ref{ch:vector} on this notion of data dimensionality.

[^4.51]: In 1964, N.J Bailey was taking the same approach to medical diagnosis [@Bailey_1965].  

[^4.61]: Part  II of this book will track several actual instances of having all the data in the sciences, in government and in business in order to show what having all the data entails in different settings. 

[^4.60]:  Machine learning textbooks written by computer scientists tend to define probabilistic models more narrowly.  As Peter Flach suggests:

    >Probabilistic models view learning as a process of reducing uncertainty using data. For instance, a Bayesian classifier models the posterior distribution $P(Y|X)$ (or its counterpart, the likelihood function $P(X|Y)$) which tells me the class distribution $Y$ after observing the features values $X$ [@Flach_2012, 47]

    But whether they are probabilistic in this sense or not, the evaluation and configuring of machine learners irreducibly depends on a statistical treatment of errors and their trade-offs. 


[^4.3]: Part of this development has already been related in the previous chapter on 'learning' and function estimation (see chapter \ref{ch:function}). That chapter postponed any real discussion of statistical thought. Instead it explored the various sense of function and function finding that underpin machine learning. But much of that function finding and approximation garners referential weight through the statistical practices and modes of thought that accompany them. 

[^4.4]: Leo Breiman writing in 2001 during the heyday of academic development of machine learning argues, describes the 'two cultures' of statistics: 'in the past fifteen years, the growth in algorithmic modeling applications and methodology has been rapid. It has occurred largely outside statistics in a new community—often called machine learning—that is mostly young computer scientists (Section 7). The advances, particularly over the last five years, have been startling' [@Breiman_2001a, 200].\index{Breiman, Leo!on statistical cultures} Soon afterwards, statisticians such as Hastie, Tibshirani and Friedman publish the first major statistical machine learning textbooks [@Hastie_2001]. \index{Breiman, Leo!on statistical cultures}

[^4.113]: Maurizio Lazzarato describes ontological mutations as 'always machinic. They are never the simple result of the actions or choices of the "man" who, leaving the assemblage, removes himself from the non-human, technical, or incorporeal elements that constitute him -- all that is pure abstraction' [@Lazzarato_2014, 83]. Lazzarato's account of diagrams as processes that slip past signification and representation echoes Deleuze and Guattari's, but usefully highlights the plurality of machines that inhabit science, music, art, cities and markets.  \index{Lazzarato, Maurizio!machinic ontological mutation}
