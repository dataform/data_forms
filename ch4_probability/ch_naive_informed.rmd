# 4.  $N = all$: machine learning gets all the data

## structure

- hacking on how statistics became real -- tables of numbers; retrofitting of laws as regularities; the role of the social; constant crossover between medicine, state and science, 
- the fact that machine learning books are full of stats, but the traditional stats of hypothesis testing, tests of significance, measures of uncertainty or variation although some of this appears.
- statistical mechanisms in the service of something else -- follows on from argument about functions as partial observers and as things in the world (code) -- statistics becomes a device to order the progress of machines; and the laws that matter -- law of large of numbers; the normal distribution -- become techniques of controlling the proliferation of movements. 
- not so much the deluge of tables giving rise to statistical laws, measurement preceding regularity, but ....

## Introduction

In the final pages of _The Taming of Chance_, the philosopher Ian Hacking describes the work of the geodesic philosopher C.S. Peirce in terms of a twin affirmation of chance. On the one hand, Peirce, following the work of the psychophysicist Gustav Fechner, makes the normal curve into an underlying reality.[^4.1] The 'personal equation,' the variation in measurements made by any observer, become 'a reality underneath the phenomena of consciousness' [@Hacking_1990, 205]. At the same time, and in order to show the underlying reality of the curve, 'Peirce deliberately used the properties of chance devices to introduce a new level of control into his experimentation. Control not by getting rid of chance fluctuations, but by adding some more' [@Hacking_1990, 205]. Peirce's belief in absolute chance, 'a universe of chance' as Hacking puts it, came in the way of a series of 'realization' of curves, in which first social, then biological and finally psychological variations were all understood as evidence of a generative function, the normal distribution or Gaussian function. In the century or so since, what happened to the thorough-going affirmation of statistical thought and probabilistic practice epitomised by Peirce? Hacking stresses that he does not understand Peirce as the precursor or the innovator of twentieth century statistical thought (Hacking's _Taming of Chance_ ends at 1900), but rather as 'the first philosopher completely to internalize the way chance had been tamed in the nineteenth century' (215). What would the equivalent philosopher-machine learner internalize today? What would such persons, working in science or media or government, hold firm in relation to chance, probability and statistics? 

[^4.1]: The historian of statistics Stephen Stigler provides a lengthy account of Fechner's work [@Stigler_1986].  TBA -- in what chapter?

In a broad sense, the setting that concerned Peirce in his work at the U.S. Government Coast Survey in the 1870s does not differ greatly from the setting that machine learner encounter. In the opening lines of the First Edition of _Elements of Statistical Learning_, Hastie, Tibshirani and Friedman write:

>The field of Statistics is constantly challenged by the problems that science and industry brings to its door. In the early days, these problems often came from agricultural and industrial experiments and were relatively small in scope [@Hastie_2009, xi]

At the end of the preface, they also cite, we might note in passing, Hacking's work: 'The quiet statisticians have changed our world' [@Hastie_2009, xii]. With some justification, we might ask what difference the 'vast amounts of data ... generated in many fields' (xi) make to what machine learners internalize of their world. This question of what kind of world becomes thinkable through machine learning can be addressed partly by contrasting the 'taming of chance' achieved during the eighteenth and nineteenth centuries, and the statistical practices of machine learning today. Is machine learning a further taming of chance? What role does randomness and probability play in machine learning?

The broadest claim associated with statistical machine learning might be the simple expression shown in:

\begin {equation}
\label {eq:linear_model}
N = \forall X
\end {equation}


In Equation \ref{eq:n_all}, $N$ refers to the number of observations (and hence the size of the dataset), the symbol $\forall$ means 'all' since this is the level of inclusion which many fields of knowledge in science, government, media, commerce and industry envisage, and $X$ refers to the data itself. Note that this expression leaves some things out. $Y$, the response variable, for instance, may or may not be known.  While statistical techniques and practices have appeared in previous chapters, I focus here  on changes in probability practices associated with machine learning, and in particular, $N = \forall $X, the claim that with all the data, statistical thinking and the potentials of the statistical reasoning fundamentally change. The claim that with $N=\forall X$ everything changes has been widely discussed.[^4.2]  Viktor Mayer-Schönberger and Kenneth Cukier's _Big Data: A Revolution That Will Transform How We Live, Work and Think_  present this shift in many different ways in the course of the vignettes and comparisons that have become typical of the data revolution genre. In a chapter entitled 'More,' they sketch the transition from data practices reliant on sampling to data practices that deal with all the data:

>Using all the data makes it possible to spot connections and details that are otherwise cloaked in the vastness of the information. For instance, the detection of credit card fraud works by looking for anomalies, and the best way to find them is to crunch all the data rather than a sample [@Mayer-Schonberger_2013, 2013,  27]

In the several hundred pages that follow in _Big Data_, the problem of how to 'crunch all the data' is never really discussed. While they mention the role of social network theory (30), 'sophisticated computational analysis' (55),  'predictive analytics' (58) and 'correlations' (7),  and they do say that 'the revolution' is 'about applying math to huge quantities of data in order to infer probabilities' (12), any further consideration of specific techniques of data crunching or the math is largely left aside. This is not to criticize a book that sets out to describe trends affecting business and government for a general readership, but without a sense of how statistical thinking animates almost all salient features of crunching the data and particularly the predictions, it becomes hard to see how the 'revolution' takes place. In other words, the shift from $N=n$ (some of the data) to $N=\forall X$ does not occur without other transpositions and rearrangements that do not simply concern choices about how much data to use, but also concern how data is given in the world and how it is thinkable.[^4.3]

[^4.2]: Rob Kitchin provides a very useful overview of these claims in [@Kitchin_2014]. While I will not analyse the claims about 'big data' in specific cases in any great detail, I  

[^4.3]: Part of this development has already been related in the previous chapter on 'learning' and function estimation. That chapter avoided any real discussion of statistical thought. Instead it explored the various sense of function and function finding that underpin machine learning. But much of that function finding and approximation garners referential weight through the statistical practices and modes of thought that accompany them. 

## Multiple probabilities in machine learning


----        -----
supervised  unsupervised
parametric  non-parametric
prediction  inference
generative  discriminative
bias        variance
-----------------------------------

Table: Structuring differences in machine learning


Every text on machine learning is structured by a basic set of contrasts or indeed oppositions. The contrasts shown in \ref{table:} all have a statistical bent to them. Some refer to variations and errors (bias and variance), some refer to the underpinning statistical intuition in particular techniques (e.g. Naive Bayes or Latent Dirichlet Allocation are generative models whereas logistic regression or support vector machines are discriminative), and others indicate different kinds of statistical knowledge (prediction seeks to anticipate while inference seeks to interpret, etc.). These broad structuring differences reach down deeply into the architecture, the diagrams, the practices, statements and visual objects associated with $N=\forall X$. While they  anchor some basic elements of  machine learning, a much more profuse set of techniques and formalisms derived from statistics and referring to probability populate the field and organise its knowledge of its own techniques and its orientation to the worlds of industry, agriculture, earth science, genomics, etc.

[HERE]

Certain strands of social and cultural theory have taken a strong interest in algorithmic processes. For instance, the sociologist Scott Lash distinguishes  the operational rules found in  algorithms from the regulative and constitutive rules in many social settings and studied by social scientists:

>in a society of pervasive media and ubiquitous coding, at stake is a third type of rule, algorithmic, generative rules. ‘Generative’ rules are, as it were, virtuals that generate a whole variety of actuals. They are compressed and hidden and we do not encounter them in the way that we encounter constitutive and regulative rules. Yet this third type of generative rules is more and more pervasive in our social and cultural life of the post-hegemonic order. They do not merely open up opportunity for invention, however. They are also pathways through which capitalist power works, in, for example, biotechnology companies and software giants more generally [@Lash_2007a, 71].

The term 'generative' is somewhat resonant in the field of machine learning as generative models, models that treat modelling as a problem of specifying the operations or dynamics that could have given rise to the observed data, are extremely important. If we consider only Andrew Ng's CS229 machine learning lectures  on Youtube [@Ng_2008], we can see that they introduce generative models in Lecture 5 and 6. Although this seems to be only a small part of the 18 lectures given in the course, later lectures on the expectation maximisation algorithm (12-13), and then on unsupervised learning techniques such as factor analysis and principal component analysis, independent component analysis, are also effectively exploring generative models.  A similar distribution of topics can be found in _Elements of Statistical Machine Learning_[@Hastie_2009].   Generative models, while perhaps slightly less common in practice than discriminative models, nevertheless capture the sense that algorithms are not just implementations of rules for filtering, sorting, or deciding, but carry within them ontological commitments that might actually challenge social theory in interesting ways. In contrast to Lash, I would suggest that the generativity of these algorithms needs to be differentiated from the algorithmic processes that implement rules more generally. Moving into the data via a generative probabilistic model is very different to moving into the data through say a database query. The models, whether generative or discriminative (models  such as decision tree,  logistic regression or even neural networks that are more limited in their probabilistic underpinnings), are more like meta-algorithms that reorganize other algorithmic processes on varying scales. 
## References
