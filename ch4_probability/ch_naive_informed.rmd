
\chapter{$N = all$: machine learning as probabilisation
\label{ch:probability}


```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=12, tidy=TRUE, fig.height=8, 
                      echo=FALSE,  fig.show='hide', results='hide', warning=FALSE, message=FALSE, dev='pdf')
library(RSQLite)
con <- dbConnect(RSQLite::SQLite(),'../ml_lit/all_refs.sqlite3')
res = dbGetQuery(con, statement ="select * from basic_refs limit 10;")
library(ggplot2)
library(stringr)
library(xtable)
options(xtable.comment = FALSE)
simpleCap <- function(x) {
    s <- strsplit(x, " |-")[[1]]
    return (paste(toupper(substring(s, 1,1)), tolower(substring(s, 2)), sep="", collapse=" "))
  }
```

In the final pages of _The Taming of Chance_, the philosopher Ian Hacking describes the work of the geodesic philosopher C.S. Peirce in terms of a twin affirmation of chance. On the one hand, Peirce, following the work of the psychophysicist Gustav Fechner and before him the astronomer-sociologist Adolphe Quetelet, makes the normal curve into an underlying reality.[^4.1] The 'personal equation,' the variation in measurements made by any observer, becomes 'a reality underneath the phenomena of consciousness' [@Hacking_1990, 205]. At the same time, and in order to show the underlying reality of the normal curve, 'Peirce deliberately used the properties of chance devices to introduce a new level of control into his experimentation. Control not by getting rid of chance fluctuations, but by adding some more' [@Hacking_1990, 205]. Peirce's belief in absolute chance or a stochastic ontology, \index{ontology!stochastic} 'a universe of chance' as Hacking puts it, succeeded a series of 'realizations' of curves, in which first social, then biological and finally psychological variations were all understood as evidence of a generative function, the normal distribution or Gaussian function. \index{function!probability distribution!Gaussian} In the century or so since, what happened to the thorough-going affirmation of statistical thought and probabilistic practice epitomised by Peirce? Hacking stresses that he does not understand Peirce as the precursor or the innovator of twentieth century statistical thought (Hacking's _Taming of Chance_ ends at 1900), but rather as 'the first philosopher to conceptually internalize the way chance had been tamed in the nineteenth century' (215). What would the equivalent philosopher-machine learner internalize today? What would such persons, working in science or media or government, hold firm in relation to chance, probability and statistics? \index{Hacking, Ian!on C.S. Peirce}

[^4.1]: The historian of statistics Stephen Stigler provides a lengthy account of Fechner's work in [@Stigler_1986, 239-259].

In a broad sense, the setting that concerned Peirce in his work at the U.S. Government Coast Survey in the 1870s does not differ greatly from the setting that machine learners encounter today. In the opening lines of the First Edition of _Elements of Statistical Learning_, Hastie, Tibshirani and Friedman write:

>The field of Statistics is constantly challenged by the problems that science and industry brings to its door. In the early days, these problems often came from agricultural and industrial experiments and were relatively small in scope [@Hastie_2009, xi]

At the end of the preface, they also cite, we might note in passing, Hacking's work: 'The quiet statisticians have changed our world' [@Hastie_2009, xii]. One of the challenges science and industry has brought to its door in recent years has been more data but also learning machines. With some justification, we might ask therefore: what difference do the 'vast amounts of data ... generated in many fields' (xi) make to the field of statistics? Statistics has, I will suggest in this chapter, gradually endowed machine learners with an increasing probabilistic semantics. This deeply affects how machine learners internalize their world. Conversely, we should ask: what do machine learners do to statistics? Is machine learning a further taming of chance? What role does randomness and probability play in machine learning? These questions of how worlds becomes thinkable through machine learning can be addressed partly by contrasting the 'taming of chance' achieved  by eighteenth and nineteenth century statistics and the statistical practices of machine learning today. \index{machine learning!statistical practices}

The broadest claim associated with statistical machine learning might be the simple expression shown in:

\begin {equation}
\label {eq:n_all}
N = \forall X
\end {equation}

In Equation \ref{eq:n_all}, $N$ refers to the number of observations (and hence the size of the dataset), the symbol $\forall$ means 'all' since this is the level of inclusion which many fields of knowledge in science, government, media, commerce and industry envisage, and $X$ refers to the data itself arrayed in common vector space. Note that this expression leaves some things out. $Y$, the response variable, for instance, may or may not be known.  While both the expansion of data in the common vector space and the machine learners that traverse it have appeared in previous chapters, I focus here  on changes in probability practices associated with machine learning, and in particular, $N = \forall X$, the claim that with all the data, statistical thinking and the potentials of the statistical reasoning fundamentally change. The claim that with $N=\forall X$ everything changes has been widely discussed.[^4.2]  Viktor Mayer-Schönberger and Kenneth Cukier's _Big Data: A Revolution That Will Transform How We Live, Work and Think_  present this shift in many different ways in the course of the vignettes and comparisons that have become typical of the data revolution genre. In a chapter entitled 'More,' they sketch the transition from data practices reliant on sampling to data practices that deal with all the data:

>Using all the data makes it possible to spot connections and details that are otherwise cloaked in the vastness of the information. For instance, the detection of credit card fraud works by looking for anomalies, and the best way to find them is to crunch all the data rather than a sample [@Mayer-Schonberger_2013, 2013,  27]

In the several hundred pages that follow in _Big Data_, the problem of how to 'crunch all the data' is not a major topic. \index{Mayer-Schőnberger, Viktor} \index{Cukier, Kenneth} While they mention the role of social network theory (30), 'sophisticated computational analysis' (55),  'predictive analytics' (58) and 'correlations' (7),  and they observe that 'the revolution' is 'about applying math to huge quantities of data in order to infer probabilities' (12), any further consideration of a change in statistical practices subsumed in 'infer probabilities' is largely confined to a business-oriented contrast between having some of the data and having all the data (that is, businesses often have all the data on their customers). This is not to criticize a book that sets out to describe trends affecting business and government for a general readership, but without a sense of how statistical thinking animates almost all salient features of crunching the data and particularly the predictions, it becomes hard to see how the 'revolution' takes place. Just as nineteenth century statistics transformed measurements (for instance, the mean as average of all measured values) into real quantities (for instance, mean as the ideal or abstract property of a population; e.g. life expectancy), the shift between $n$ and $\forall X$, a shift very much animating and dependent on  machine learning, internalizes, I will suggest, probability and chance into machinic operations. This is a statistical event akin to the advent of the Normal distribution (and indeed, $\mathnormal{N}$ is a standard symbol for the Normal distribution in statistics textbooks) as a way of knowing and controlling populations [@Hacking_1975, 108]. To signal its continuity with the invention of probability, I term it here 'probabilisation,' a pleonasm that refers to rendering in terms of probabilities. \index{probabilisation|seealso{diagram!probabilisation of}  

# Machine learning as statistics inside out

In   _The Taming of Chance_, Hacking argues that modern statistical thought transposed a way of calculating errors in experimental measurements and astronomical observations into real quantities typically described by the normal distribution. \index{statistics!history of!from error to real quantity} This transposition or inversion relied on four intermediate steps passing through probability calculus (particularly the work of Jacob Bernoulli and the binomial or heads-tails probability distribution in the 1690s [@Hacking_1975, 143]), on large numbers of measurements (the most famous being the chest measurements of soldiers in Scottish regiments, but these were only one flurry amidst an avalanche of numbers in the 1830-1840s), on an idea of multiple, minute independent causes producing events (particularly as developed in medicine but also in studies of crime), and the 'law of errors' applying to measurements made by, amongst others, astronomers [@Hacking_1990, 111-112]. \index{statistics!probability distributions!normal} As Hacking observes, coins, suicides, crime, chest measurements, and astronomical observations all accumulate in a picture of statistical reality which remains, although somewhat altered, indelible in contemporary statistical thought in its frequent recourse to probability distributions. In this entanglement, observers and the observed changed places. The distribution of errors made by astronomers measuring the position of stars or planets became a distribution or variation inherent in things.  All of this seems a long way from machine learning, and in terms of years, the work of figures such as Poisson, Laplace, Quetelet and even Galton, is well-removed. In terms of tables and functions (the concerns of the preceding two chapters), the distance is not so great. There is greater variety in tables (partly due to the common vector space) and in functions, but the entwining or even swapping between what relates to an observation and what concerns the real, continues. Machine learners engage in that swapping or re-distributing of numbers all the time. Viewed from the standpoint of Hacking, machine learning reverse-engineers the invention of modern statistical thinking.  It takes back the 'real quantities' that modern statistics had attributed to the populations of the world and puts them into devices, machine learners that people then observe, monitor and indeed measure again in many ways. The direct swapping between uncertainty in measurement and variation in real attributes that statistics achieved now finds itself re-routed and intensified as machine learners measure the errors, the bias and the variance of devices. 

This swapping or re-distribution is not a simple mirror-image reversal, as if machine learners mistake a device for the world. Machine learners do constantly take statistical thinking as a basic semantic frame for their devices. When Hastie and co-authors write (as we saw in the previous chapter) 'our goal is to find a useful approximation $\hat(f)(x)$ to the function $f(x)$ that underlies the predictive relationship between input and output' [@Hastie_2009, 28], they invoke the 'real quantities' first elaborated and articulated by proto-statisticians such as Quetelet grappling with population and sample parameters.  The major structuring differences in machine learning as a field of knowledge-practice show the marks of increasingly strong commitment to the reality of the statistical.

\begin{table}
\centering
\begin{tabular}{|l|l|}
parametric &  non-parametric \\
bias &        variance \\
prediction &  inference \\
generative &  discriminative \\
\end{tabular}
\caption{Some structuring differences in machine learning}
\label{tab:ml_diffs}
\end{table}

Reading and working with machine learning techniques usually means encountering and responding to some of that statistical apparatus drawn from statistics, but these are not typically the statistical tests of significance or variation. In contrast to a statistics textbook such as the widely used _Basic Practice of Statistics_ [@Moore_2009] or even a more advanced guide such as _All of Statistics_ [@Wasserman_2003], where statistics (t-test, chi-squared test, etc.) hypothesis testing, and analysis of uncertainties (confidence intervals, etc) order the exposition, the machine learning texts describe a conceptual apparatus curiously unadorned by the plethora of methods found in mainstream statistics. Statistical underpinnings may be fundamental, but this does not mean that  machine learners simply automate statistics.[^4.4] Instead,  a basic set of contrasts or indeed oppositions that owe much to probabilistic thinking pattern statements on machine learning. \index{machine learning!structure differences} The contrasts shown in Table \ref{tab:ml_diffs} all have a statistical facet and anchoring  to them. Some refer to errors that affect how a machine learner refers to data (bias and variance; see discussion below); some designate an underlying statistical intuition about how particular machine learners treat data (does the model seek to generate the data or classify -- discriminate -- it; e.g. Naive Bayes or Latent Dirichlet Allocation are _generative_ models whereas logistic regression or support vector machines are _discriminative_); parametric and non-parametric describe the role of probability distributions in the model; and others indicate different kinds of statistical knowledge practice (prediction seeks to anticipate while inference seeks to interpret, etc.; also see discussion below). \index{machine learning!structuring differences} These broad structuring differences reach down deeply into the architecture, the diagrams, the practices, statements and visual objects and computer code associated with $N=\forall X$. Because they  anchor basic operations of  machine learning in probability statements, techniques and formalisms derived from statistics have in the last two decades increasingly populated the field, furnishing and rearranging  its diagrammatic references \index{diagram!reference}  to the worlds of industry, agriculture, earth science, genomics, etc,. but also, crucially, triggering ontological mutations in machine learners themselves. [^4.113]  \index{machine learners!ontological mutations of} These structuring differences in machine learning arise from probabilisation. \index{machine learning!probabilisation of|seealso{probabilisation}}

While these structuring differences are practically very important, and deeply shape certain kinds of practice in machine learning, the underlying operator that allows swapping between knowledge and the world, and then between events and devices is probability thinking, and in particular, the functions that describe variations in probability, probability distributions. \index{function!probability distributions} Historically, two probability distributions loom large. The binomial distribution, which describes the probability of the number of $n$ 'successes' out of $N$ trials or observations, was explored extensively in the seventeenth and eighteenth centuries in the context of games of chance [@Hacking_1975, 57-134]. Because it accommodates only two discrete outcomes, the binomial distribution anchors many machine learning classifiers. The normal distribution pervades nineteenth century statistical thinking as it generalizes across law, medicine, agriculture, finance and not least, sociology. These distributions appear in countless variations in scientific, government and popular literature of many different kinds.[^4.205]


\begin {equation}
\label {eq:gaussian_distribution}
f(x;\mu, \sigma^2) = \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}
\end {equation}


The  function shown in equation (\ref{eq:gaussian_distribution}) is the so-called normal or Gaussian distribution. Its  mathematics were intensively worked over during the late eighteenth and early nineteenth centuries in what has been termed 'one of the major success stories in the history of science' [@Stigler_1986, 158], and it has a power-laden biopolitical history closely tied with knowledges and governing of  national and other  populations in terms of morality, health, and wealth (see [@Hacking_1975, 113-124]. The key symbols here include $\mu$, the mean and $\sigma$, the variance, a number that describes how widely dispersed the values of the variable, $x$ are. These two parameters together describe the shape of the curve. Given knowledge of $\mu$ and $\sigma$, the normal or Gaussian probability distribution maps all outcomes to probabilities (or numbers in the range $0$ to $1$). Put more statistically, viewed in terms of functions such as the Gaussian distribution, events become random variables.  Every variable potentially becomes a function: 'a random variable is a mapping that assigns a real number to each outcome' [@Wasserman_2003,19]. \index{random variable|seealso{function!probability distribution}}
 
The possibility of treating all variables as random variables, that is, as probability distributions, was a significant historical achievement, but one that continues to develop and ramify.[^4.114] The concept of the random variable tends to lend ontological weight to probability. When conceptualised as real quantities in the world rather than epiphenomenal by-products of inaccuracies in our observations or measuring devices, probability distributions weave directly into the productive operations of power. Distribution in the sense of locating, positioning, partitioning, sectioning, serialising or queuing operations pervades the development of disciplinary power, as Michel Foucault richly details [@Foucault_1977], but in almost every setting, distribution in the sense of counting and weighting of different outcomes also operates. This constant interweaving of spatial, architectural, logistical and calculative processes has energised statistical thought for several centuries.[^4.115] \index{distribution!see{function!probability distribution} For instance, given the normal distribution, it is possible, under certain circumstances, to effectively subjectify someone on the spot. If an educational psychologist indicates to someone that their intelligence lies towards the left-hand side of the normal curve peak (and hence less than the population mean), they quickly assign them to a potentially institutionally and economically consequential trajectory. Since its inception in the social physics of Adolphe Quetelet as a way of referring to a property of populations, the normal curve has promised or threatened to support the re-shaping and control of populations (in terms of health, morality and wealth). Since then, probability distributions and their corresponding curves have multiplies.

Subsequent statisticians developed dozens of different probability distributions to map continuous and discrete variations to real numbers. Other probability distributions abound — normal (Gaussian), uniform, Cauchy exponential, gamma, beta,  hypergeometric, binomial, Poisson, chi-squared, Dirichlet, Boltzmann-Gibbs distributions, etc. (see [@NIST_2012] for a gallery of distributions) — because outcomes occur in widely differing patterns. The  queuing times at airport check-ins do not, for instance, easily fit a normal distribution. Statisticians model queues using a Poisson distribution, in which, unfortunately for travellers, distributes the number of events in a given time interval quite broadly.  Similarly, it might be better to think of the probability of rain today in north-west England in terms of a Poisson distribution that models clouds in the Atlantic queuing to rain on the northwest coast of England. (Rather than addressing the question of whether it will rain or not, a Poisson-based model might address the question of how many times it will rain today.)

If functions such as equation (\ref{eq:gaussian_distribution}) have persisted for so long as operational underpinnings, what happen to them in machine learning? The pages of a book such as _Elements of Statistical Learning_ show many signs of an ongoing re-distribution of probability distributions. Superficially we could simply note their abundance. Hastie and co-authors diversely invoke probability distributions. They speak of 'Gaussian mixtures,' 'bivariate Gaussian distributions,' standard Gaussian,' 'Gaussian kernels,' 'Gaussian errors', 'Gaussian assumptions,' 'Gaussian errors,' 'Gaussian noise,' 'Gaussian radial basis function,' 'Gaussian variables,' 'Gaussian densities,' 'Gaussian process,' and so forth. The term 'normal' appears in an even wider spectrum of similar guises, but they uniformly entail treating events, things, properties or attributes as probability distributions. The wide distribution of the Gaussian function does not mean that one function, the Gaussian probability density function reigns supreme above all machine learners. \index{function!probability distribution!Gaussian}

The diverse curves of probability distributions — and we will see below some reasons why we can expect them to proliferate in certain settings — attests to the variety of ways in which events (occurrence of cancer, occurrence of the word 'Viagra' in an email, the click on a hyperlink, etc.)  might be mapped to real numbers. Despite  the sometimes dense mathematical diagrammatics, the term *distribution* emphasises a tangible and practically resonant way of thinking about how events or possible outcomes shift about as the parameters of a function vary. Machine learners adjust these parameters in different ways. For instance, parametric and non-parametric models (see table \ref{tab:ml_differences}) differ  in that the former have a limited number of parameters and the latter an undefined number of parameters (for instance, Naive Bayes, _k_ nearest neighbours or support vector machine models). But both kinds assume that an underlying probability distribution -- a function, ‘unobservable’ or not -- operates, even if it changes with new data. A probability distribution under these assumptions becomes the closest reality we have to whatever process generated  all the variations in data gathered through experiments and observations. From a probabilistic perspective, the task of machine learning is to estimate the parameters (the mean $\mu$ and variance $\sigma$ in the case of Gaussian curve) that shape of the curve of the probability distribution. Given the shape of that curve, many inferences and predictions become possible. The probability distribution function is a crucial control surface for machine learning understood as a form of movement through data.  \index{machine learner!probability distribution as control surface} In contrast to the gradual endowment of realities with probability that we see in the history of statistics (and later in natural sciences such as physics and biology), statistical machine learning endows devices and control systems with probability. 

# Naive Bayes and the distribution of probabilities

What does this mean in practice? The mathematical expression for one of the most popular of all machine learning classifiers, the Naive Bayes classifier, stands out for its simplicity and lack of parameters. \index{machine learner!Naive Bayes}

\begin {equation}
\label {eq:naive_bayes}
f_j(X) = \prod_{k=1}^{p}f_{jk}(X_k)
\end {equation}

>[@Hastie_2009, 211]

Some machine learners are so simple that they can be implemented in a few lines of code.  Their simplicity, however, belies their power. The function shown above in equation (\ref{eq:naive_bayes}) is about the simplest one to be found in most machine textbooks yet easily adapts for high dimensional data, the kind of data associated with $N = \forall X$.[^4.116]  While the Naive Bayes classifier is one of the most popular machine learning algorithms, it is more than 50 years old [@Hand_2001]. The key diagrammatic elements of the classifier as expressed in the equation are $\prod$, an operator that multiplies all the values of the matrix of $X$ values (from $1$ to $p$) to generate a product. What product does the Naive Bayes classifier produce? The expression $f_j(X)$ refers to a probability density; that is, it describes the probability that a particular thing (a document, an image, an email message, a set of URLs, etc.) belongs to the class of things $j$. In constructing this estimate of the probability that a thing is an instance of class $j$, $p$ different features of the thing are taken into account. (To use the language of chapter \ref{ch:vector}, the subscript $k$ indexes the $p$ dimensions of the common vector space.) The subscripts $k=1$ on the $\prod$ operator, and $k$ on the data $X_k$ indicate that the Naive Bayes classifier makes use of a series of features or variables  in calculating the overall probability that a given thing or observation belongs to a specific class. The classifier produces a product $f_j(X)$ by calculating the _joint probability_ of all the _conditional_ probabilities of the features or predictor variables in $X$ for the class $j$. As  _Elements of Statistical Learning_ rather tersely puts it, 'each of the class densities are products of the marginal densities' [@Hastie_2009,108]. 

Almost everything about the  Naive Bayes classifier concerns probability (including its name, with its reference to the Bayes Theorem, an important late eighteenth century concept), yet there is little obvious connection to statistics in its modern form of tests of significance. As Drew Conway and John Myles-White write in _Machine Learning for Hackers_, 

>At its core, [Naive Bayes] ... is a 20th century application of the 18th century concept of _conditional probability_. A conditional probability is the likelihood of observing some thing given some other thing we already know about [@Conway_2012, 77] 

They point here to the role of 'conditional probability,' a probability conditioned on the probability of something else. Conditional probability lies at the heart of many of the data transformation associated with prediction or pattern recognition since it links different variable or features together as we see in Naive Bayes often by simply multiplying probabilities.[^4.117] \index{probability!conditional} As any of the many accounts of the technique will explain, the name comes from Bayes Theorem, one of the most basic yet widely used results in probability theory (again dating from the eighteenth century), yet Naive Bayes does not even fully embrace Bayes Theorem as the principle of its operation. The classifier has a simple probabilistic architecture based on the concepts of conditional probability and joint probability; it calculates a probability density function $f_j(X)$ or probability distribution for each possible class of things. Its architecture is simple, however, because it makes a drastically reductionist assumption that features are independent of each other (hence the name 'naive'), where 'independent' means that they do not affect each other, or that they have no relation to each other. We will see below that dramatic simplifications do not necessarily weaken the referential grasp of machine learners on the world, but in certain ways allow them to relate to it more directly.

# Spam: when $\forall{N}$ is too much?

While equation (\ref{eq:naive_bayes}) does not set out all the steps in transforming some data into a prediction of the class that a given instance, event, thing or observation belongs to, the code needed to do this are relatively brief. In _Doing Data Science_, Rachel Schutt and Cathy O'Neill furnish a bash script (that is, command line instructions) to download a well-known email dataset and build a Naive Bayes classifier that labels email as spam or not. In many ways, this is canonical machine learner pedagogy, and for Naive Bayes, email spam detection has become the standard example (Andrew Ng uses it in CSS 229, Lecture 5 [@Ng_2008]). In this setting, machine learners operating as spam filters coping with too much communication. $\forall{N}$ can be a bother. Nevertheless, the concision of the small script, which fetches the Enron email dataset, calculates the 'marginal densities' or conditional probabilities for each word given how often it is associated with either spam or non-spam email, and then outputs the probability that a particular email is spam, is striking.[^4.7] These few dozen lines convert some part of a communicative reality -- the Enron email dataset -- into a predictive classifier that tests the world:  is a given message spam or not? \index{dataset!spam email!Enron}

A typical spam email in the Enron dataset, a dataset that derives from the U.S Federal Energy Regulatory Commission's investigation into Enron Corporation [@Klimt_2004],  looks like this:

> Subject: it's cheating, but it works !
can you guess how old she is ? the woman in this photograph looks like a happy teenager about to go to her high school prom, doesn' t she ? she' s an international, professional model whose photographs have appeared in hundreds of ads and articles whenever a client needs a photo of an attractive, teenage girl.but guess what ? this model is not a teenager ! no, she is old enough to have a 7-year-old daughter.. and...the model' s real age is in her 30' s.all she will say about her age to her close friends is, " i'm dangerously close to 40." she also says, " if it weren't for this amazing new cosmetic cream called ' deception, ' i would lose hundreds of modeling assignments...because...there is no way i could pass myself off as a teenager." 
service dept
9420 reseda blvd # 133
northridge, ca 91324

The text of a typical non-spam email like this:

> Subject: industrials suggestions......
----------------------forwarded by kenneth seaman / hou / ect on 01 / 04 / 2000
12 : 47 pm-------------------------- -
pat clynes @ enron
01 / 04 / 2000 12 : 46 pm
to : kenneth seaman / hou / ect @ ect, robert e lloyd / hou / ect @ ect
cc :
subject : industrials
ken and robert,
the industrials should be completely transitioned to robert as of january 1, 2000.please let me know if this is not complete and what else is left to transition .
thanks, pat

These are both recognisable things in the world for anyone who uses email. How do they become $X$ or even $f_j(X)$ in the Naive Bayes classifier? How do words in a document because probability distributions? The code is instructive:


```{r enron_nb_bash, engine='bash', echo=TRUE}
    #!/bin/bash
    # file: enron_naive_bayes.sh
    # description: trains a simple one-word naive bayes spam
    # filter using enron email data
    # usage: ./enron_naive_bayes.sh <word>
    # requirements:
    #    wget
    #
    # author: jake hofman (gmail: jhofman)
    # how to use the code
    if [ $# -eq 1 ]
    then
        word=$1
    else
        echo "usage: enron_naive_bayes.sh <word>"
        exit
    fi

     ### PART 1
    if ! [ -e enron1.tar.gz ]
    then
        wget 'http://www.aueb.gr/users/ion/data/enron-spam/preprocessed/enron1.tar.gz'
    fi

    if ! [ -d enron1 ]
    then
        tar zxvf enron1.tar.gz
    fi

    cd enron1

    ### PART 2
    Nspam=`ls -l spam/*.txt | wc -l`
    Nham=`ls -l ham/*.txt | wc -l`
    Ntot=$Nspam+$Nham
    echo $Nspam spam examples
    echo $Nham ham examples

    Nword_spam=`grep -il $word spam/*.txt | wc -l`
    Nword_ham=`grep -il $word ham/*.txt | wc -l`
    echo $Nword_spam "spam examples containing $word"
    echo $Nword_ham "ham examples containing $word"

    ### PART 3
    Pspam=`echo "scale=4; $Nspam / ($Nspam+$Nham)" | bc`
    Pham=`echo "scale=4; 1-$Pspam" | bc`
    echo
    echo "estimated P(spam) =" $Pspam
    echo "estimated P(ham) =" $Pham
    Pword_spam=`echo "scale=4; $Nword_spam / $Nspam" | bc`
    Pword_ham=`echo "scale=4; $Nword_ham / $Nham" | bc`
    echo "estimated P($word|spam) =" $Pword_spam
    echo "estimated P($word|ham) =" $Pword_ham

    ### PART 4
    Pspam_word=`echo "scale=4; $Pword_spam*$Pspam" | bc`
    Pham_word=`echo "scale=4; $Pword_ham*$Pham" | bc`
    Pword=`echo "scale=4; $Pspam_word+$Pham_word" | bc`
    Pspam_word=`echo "scale=4; $Pspam_word / $Pword" | bc`
    echo
    echo "P(spam|$word) =" $Pspam_word
    cd ..
```

[@Schutt_2013, 105-106]

```{r enron_nb_r, echo = FALSE}

        f_ham = list.files('enron1/ham', full.names = TRUE)
        f_spam = list.files('enron1/spam', full.names = TRUE)
        ham_count = length(f_ham) 
        spam_count = length(f_spam)

        ham = sapply(f_ham, readLines, warn=FALSE)
        spam = sapply(f_spam, readLines, warn=FALSE)
        P_ham = ham_count/(ham_count + spam_count)
        P_spam = spam_count/(ham_count + spam_count)
        word = 'gas'

        predict_spam <- function(word) {
            ## count the occurrence of the word in each
            Nword_in_ham = sum(grepl(word, ham))
            Nword_in_spam = sum(grepl(word, spam))
            #cat(Nword_in_ham, ' ham examples contain ', word,  '\n')
            #cat(Nword_in_spam, ' spam examples contain ', word,  '\n')

            #cat('estimated P(spam) = ', P_spam,  '\n')
            #cat('estimated P(ham) = ', P_ham,  '\n')

            Pword_spam = Nword_in_spam/spam_count
            Pword_ham = Nword_in_ham/ham_count
            #cat("P(spam|", word, ") = ", Pword_spam,  '\n')
            #cat("P(ham|", word, ") = ", Pword_ham,  '\n')

            Pham_word = Pword_ham * P_ham
            Pspam_word = Pword_spam * P_spam
            Pword = Pspam_word + Pham_word
            Pspam_word = Pspam_word/Pword

            #cat("P(spam|", word, ")=",  Pspam_word,  '\n')
            return(Pspam_word)
        }
```

After fetching the dataset from a website, the code counts the number of emails in each category and prints them, and the counts the number of times that the chosen word (e.g. 'finance' or 'deal') occurs in both the spam and non-spam or ham categories. Using these counts, the script  estimates probabilities of any email being spam or ham, and then given that email is spam or ham, that the particular word occurs. To estimate a probability means, in this case, to divide the word count for the chosen word by the count of the number of spam emails, and ditto for the ham emails.  In Part 4, the final transformation of the data, these probabilities are used to calculate the probability of any one email being spam given the presence of that word. Again, the mathematical operations here are no more complicated than adding, multiplying and dividing. The probability that the chosen word is a spam word is, for instance, the probability of occurrence of the word in a spam email multiplied by the overall probability that an email is spam. Finally, given that the probability of the chosen word occurring in the email dataset is the probability of it occurring in spam plus the probability of it occurring in ham, the overall probability that an email in the Enron data is spam given the presence of that word can be calculated. It is the probability that the chosen word is a spam word divided by the probability of that word in general. 

This slightly bamboozling and mechanical description of what the script does shows something of how the joint probability function in equation (\ref{eq:naive_bayes}) diagrams the world. \index{diagram!world} Not all machine learning models are so simple that they can be conveyed in 30 lines of code (including downloading the data and comments). This script indicates that nothing that occurs in relation to  classification is intrinsically mysterious, elusive or indeed particularly abstract. On the contrary, the referentiality and operational power of classifiers takes place through the accumulated counting, adding, multiplying (that is, repeated adding) and dividing (that is, multiplying by parts or fractions) constrained by the probability distribution. Probability re-distributes things such as emails or documents as, in this case, events in a population of words. The Naive Bayes classifies endows every word found in the Enron datasets with a probability density function. The classification of each email becomes a matter of estimating a conditional probability based on the joint probability distribution that quantifies the chance of all the words in that email appearing together.  Probabilities are always between `0` and `1`, or between 0% and 100%, and classification entails selected a cutoff or dividing line. For instance, greater than `0.5` might result in a classification as `spam`.   In the `enron` dataset, 'finance' has a  `r round(predict_spam('finance'),2)` chance of being spam, while 'sexy' has a chance of `r round(predict_spam('sexy'), 2)`.   The re-distribution of textual forms -- the emails -- into probabilistic distributions is practically straightforward. Ironically, like the Naive Bayes classifier's own reliance on seventeenth and eighteenth century probability calculus, the frequent application of this machine learner to document classification and retrieval echoes the seventeenth century thinking   that first conceived of the very notion of 'probability' in relation to the evidential weight of documents [@Hacking_1975, 85].  \index{probability!history of}

# The improbable success of the Naive Bayes classifier

Naive Bayes classifiers have been surprisingly successful. Like statistics more generally, Naive Bayes treats the world as a set of probabilistic processes, as fields of random variables in process of change. The world comprises populations, and populations generate events. Emails have a real probability of being spam, and this probability is acted on many times every day for email users. There is something quite artificial at work in the construction of these populations and their associated probability distributions.  They are intentionally artificial and limited.  They do not correspond or refer directly to what we know, for instance, of how language works, but instead to a rather different set of concerns.  They remain but less directly invested in a distribution as the generative reality of the population. Like most machine learning techniques encountering complex realities, classifiers such as Naive Bayes ignore many obvious structural features of emails as documents (for instance, word order, or co-occurrences of words). Yet this very artificiality or simplicity in their reference to the world allows machine learners to appear  in many different guises.

```{r nb_applications, results='asis'}
    q = "select * from basic_refs where TI like '%Naive Bayes%' or DE like '%Naive Bayes%' or anchor = 'naive_bayes_WOS'"
    res = dbGetQuery(con, q)
    dim(res)
    colnames(res)
    nb = res[order(res$TC, decreasing=TRUE),]
    nb2 = unique(nb[, c('TI',  'PY')])
    colnames(nb2) = c('Title', 'Year')
    tab = xtable(head(nb2[order(nb2$PY),],30), caption = 'Most cited Naive Bayes publications 1945-2015', align = c("p{0.05\\textwidth}",  "p{0.85\\textwidth}",  "p{0.1\\textwidth}", "p{0.15\\textwidth}"), label='tab:nb_most_cited')
    print(tab, type='latex', row.names=FALSE)
```

The altered relation between modern statistical and machine learning practice starts to appear in Naive Bayes from the early 1990's as statisticians begins to generalize and re-diagram Naive Bayes by examining its statistical properties more carefully. Table \ref{tab:nb_most_cited} shows 30 of the most cited Naive Bayes-related scientific publications.[^4.118]  The list of titles sketches a double movement. On the one hand, we see the typical diagonal movement of a machine learner across disciplines -- computer science, statistics, molecular biology (especially of cancer), software engineering, internet portal construction, sentiment classification, and image 'keypoint' recognition. On the other hand, highly cited papers such as [@Friedman_1997] and [@Hand_2001] point to an intensified statistical treatment of machine learners themselves taking place during these years, an intensified observation of machine learners that strongly affects their ongoing development (leading, for instance, to the much more heavily probabilistic topic models appearing in the following decade [@Blei_2003]).  

In _Elements of Statistical Learning_, Hastie, Tibshirani and Friedman characterise the capacity of the  Naive Bayes classifier to deal with high dimensional data:    

>It is especially appropriate when the dimension $p$ of the feature space is high, making density estimation unattractive. The naive Bayes model assumes that given a class $G = j$, the features $X_k$ are independent [@Hastie_2009, 211]. 

Similar formulations can  be found in most of the machine learning books and instructional materials currently available. This appropriateness relates directly to $\forall{N}$, and the expansion of the common vector space of data. As we saw above in equation (\ref{eq:naive_bayes}), $p$ stands for the number of different dimensions or variables in the data set. In the spam classifier, the number of dimensions is probably quite large because every unique word adds a new dimension to the 'feature space' or common vector space. Compared to the complications of logistic regression, neural networks or support vector machines, \ref{eq:naive_bayes}  seems incredibly simple. How is it that a simple multiplication of probabilities and the assumption that 'features ... are independent' can, as Hastie and co-authors write: 'often outperform far more sophisticated alternatives' [@Hastie_2009, 211]?

The answer to this conundrum of success does not lie in statistics or in the increasingly availability of data to train machine learners on. I want to explore two other ways to view the improbable success of Naive Bayes. Both contrasts -- inference/prediction, bias/variance -- appeared in the table \ref{tab:ml_diffs}. Both are germane to the broader question of how machine learning re-iterates the 'taming of chance' accomplished by modern statistics. The first way to view this success is in terms of _ancestral communities_ of probabilisation. The second concerns the statistical decomposition of machine learners in terms of their statistical bias and variance. \index{probabilisation!ancestral communities of} \index{machine learner!statistical decomposition of}

# Ancestral probabilities in documents: inference and prediction

Along with linear regression, Naive Bayes might be _the_ standard introductory machine learning technique. The Naive Bayes classifier is almost always demonstrated on the problem of filtering spam email [@Conway_2012; @Schutt_2013, 93-113; @Kirk_2014, 53; @Lantz_2013, 92-93; @Flach_2012; @Ng_2008b], and in particular dealing with the abundance of spam emails concerning a drug for erectile dysfunction sold under the tradename 'Viagra' (a drug that was itself the  byproduct of the clinical trial for hypertension and heart disease). What are we to make of this reiteration of the same problem?  Admittedly spam, and spam trying to sell Viagra in particular, has been a very familiar part of most email users since 1997 when Viagra was approved for sale, and of all the documents that machine learners mundanely encounter in quantity in those years, email might be the most numerous as well as one of the simplest. (The other would be scientific publications. Many more recent machine learners train as classifiers on scientific publications [@Blei_2007])  Naive Bayes classifiers and variations of them also became practical devices in  managing email traffic for most people, whether they know it or not, during the mid-1990s (see for instance, [SpamAssassin](http://spamassassin.apache.org/).

The constant reiteration of email spam filtering using Naive Bayes is also a side-effect of another process, a process akin to the transposition that attributed probability distributions to populations in the nineteenth century. Like many machine learners, the genealogy of Naive Bayes has one important lineage derived from documents and the problem of their classification and retrieval. This genealogical affiliation with a particular problem such as document classification (or image recognition) generates many re-iterations and versions of machine learners over time.  As Lucy Suchman and Randall Trigg wrote in their study of work on artificial intelligence, 

>rather than beginning with documented instances of situated inference ... researchers begin with ... postulates and problems handed down by the ancestral communities of computer science, systems engineering, philosophical logic, and the like [@Suchman_1992,174]. 

While Bayes Theorem dates from the 18th century, the highly successive use of Naive Bayes classifiers in email spam filtering in recents decades effectively draws on an ancestral community of document classification and information retrieval methods.[^4.200]  \index{Suchman, Lucy!on ancestral communities}  

Early attempts to use what is now called Naive Bayes in the early 1960s re-iterated the engagements with the evidential weight of documents that accompanied the emergence of probabilistic thinking  as a quantification of belief in the seventeenth century [@Hacking_1975, 35-49].  Working at the RAND Corporation in the early 1960s, M.E. Maron described how 'automatic indexing' of documents -- Maron used papers published in computer engineering journals -- could become 'probabilistic automatic indexing.' The necessary statistical assumption was:

>The fundamental thesis says, in effect, that statistics on kind, frequency, location, order, etc., of selected words are adequate to make reasonably good predictions about the subject matter of documents containing those words [@Maron_1961, 406] \index{Maron, M.E.}

This fundamental thesis has remained somewhat fundamental in text classification and information retrieval applications, as well as many other machine learning approaches since. Maron's work focused on a collection of several hundred abstracts of papers published in the March and June 1959 issues of the _IRE Transactions on Electronic Computers_. As in contemporary supervised learning, these abstracts were divided into two groups, a training and a test set ('group 1' and 'group 2' in Maron's terminology [@Maron_1961, 407]), and the training set was classified according to 32 different categories that had already been in use by the Professional Group on Electronic Computers, the publishers of the _IRE Transactions_. Given these classifications, word counts for all distinct words in the abstracts were made, the most common terms ('the', 'is', 'of', 'machine', 'data', 'computer') and the most uncommon words removed, and the remaining set of around 1000 words were actually used for classification. This treatment of the abstracts as documents, then as lists of words, and then as frequencies of terms, and finally as a filtered list of most information rich terms continues in much text classification work today. A typical contemporary information retrieval textbook such as [@Manning_2008] devotes a chapter to the topic, including the canonical discussion of how naive assumptions about language and meaning do not vitiate the Naive Bayes classifier. Whenever machine learners affirm the unlikely efficacy of classifiers, we might attend to the ways in which previous 'ancestral probabilisations' of the domain in question prepare the ground for that success. \index{machine learner!Naive Bayes!history of} \index{probabilisation!ancestral communities of}


# Statistical decompositions: high bias, low variance and observation of errors

Even with an eye on the ancestral communities that constantly accompany and heavily shape the indexical diagram of machine learning in the world, we still need a way of accounting for the unreasonable artificiality of something like the Naive Bayes. \index{diagram!indexical} It generates highly arbitrary probabilities of document class membership, yet these arbitrary probabilities allow effective classification. Machine learners view the persistence of manifest artifice (in the case of Naive Bayes, a model that eschews any modelling of relations between things in the word such as words) in terms of another of the structuring differences of machine learning: the so-called _bias-variance_decomposition_[@Hastie_2009,24]. \index{errors!bias-variance} The terms 'bias' and 'variance' stem from the long history of statistical interest in errors (as Hacking's account of the transposition of measurement errors into population norms illustrates). The bias and variance of 'estimators' -- the estimates of the parameters of the models usually written as $\hat{\beta}$ or $\hat{\theta}$-- feature in heavily machine learning discussions of prediction errors. The terms point to tensions that all machine learners experience. On the one hand, _variance_ refers to the inevitable reliance of a machine learner on the data it 'learns.' To put it more formally, 'variance refers to the amount by which $\hat{f}$ would change if we estimated it using a different training data set' [@James_2013,34]. On the other hand, _bias_ 'refers to the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model' [@James_2013, 35]. These two sources of error, one which results from sampling and the other arising from the structure of the model or approximating function, can be reduced or at least subject to trade-off in what _Elements of Statistical Learning_ terms 'the bias-variance decomposition' [@Hastie_2009, 223].[^4.202] From the standpoint of the bias-variance decomposition, every  machine learner makes a trade-off between the errors deriving from differences between samples, and errors due to the difference between the approximating function and the actual process that generated the data. Note that both sources of error in the bias-variance decomposition derive from the relation between the model and the data, one relating to how the model encounters the world (as a set of small samples or as, at the other end, a massive $N=\forall{X}$ dataset), the other relating to how the model 'sees' the world (as a set of almost coin-toss like independent events, as a geometrical problem of finding a line or curve that runs through a cloud of points, etc.).


```{r bias_variance, echo=FALSE}
q = 'select PY, TI, DE, TC, AU from basic_refs where DE like "%bias%" or DE like "%variance%"'
res = dbGetQuery(con, q)
bv = res[order(res$TC, decreasing=TRUE),]
```

Both prediction and inference in machine learning cannot fully circumvent the tensions between the different errors at work in the bias-variance decomposition. Yet, these sources of error do not always prove harmful. The counter-intuitive success of the Naive Bayes and _k_ nearest neighbours classifiers works against the long standing trend in statistics to construct increasingly sophisticated models of the domains they encounter. Writing in 1997, Jerome Friedman describes how very simple classifiers perform surprisingly well:

>These effects are seen to be somewhat counter intuitive in both their strength and nature. In particular the bias and variance components of the estimation error combine to influence classification in a very different way than with squared error on the probabilities themselves. Certain types of (very high) bias can be canceled by low variance to produce accurate classification [@Friedman_1997,55] \index{Friedman, Jerome!on bias-variance decomposition}

A rather elaborate set of concepts and techniques address the bias-variance decomposition. These techniques focus on managing the _test_ or _generalization_ error, the difference between the actual and predicted values produced by the  machine learner when it encounters a fresh, hitherto unseen data sample. \index{error!generalization} Note that this problem of generalization error is not eliminated in any of the 'big data' or $N=\forall{X}$ scenarios often discussed in association with contemporary information infrastructures. Here too, prediction and inference encounters fresh data in some form or other, and  machine learners in such settings still suffer from the  bias-variance trade-off. This trade-off has to deal with the fact that training errors -- the difference between what the model predicts and what the training data actually shows -- is not a good guide to the test or generalization error.  The process of fitting a model or finding a function (see previous chapter) will tend to reduce the training error by fitting the function more and more closely to the shape of the training data, but when it encounters fresh data that function might no longer fit well. In other words, a more sophisticated function may well reduce the bias but increase the variance. 'Richer collections of models' [@Hastie_2009, 224] reduce bias, but tend to increase variance. Conversely, models that cope well with fresh data (and Naive Bayes is a good example of such a machine learner), display low variance but high bias. The exact nature of the trade-offs between bias and variance shift  markedly  between different types of models, and generates many different conceptual analyses of error in machine learning literature ('optimism of the training error rate' (228), 'estimates of in-sample prediction error' (230), 'Bayesian information criterion' (233), 'Vapnik-Chervonenkis dimension' (237), 'minimum description length' (235)) and technical methods of estimating prediction error ('cross-validation' (241), 'bootstrap methods'  (249),  'expectation-maximization algorithm' (272), 'bagging' (282), or 'Markov Chain Monte Carlo (MCMC)' (279)), many of which date from the 1970s (e.g. cross-validation [@Stone_1974], bootstrap [@Efron_1979], expectation-maximization [@Dempster_1977]). 

The daunting armory of concepts and methods all respond to the same underlying problem of reference. \index{referentiality} They invoke in some cases sophisticated mathematical or statistical constructs, but also very often rely on computational iteration to optimise values of parameters in models whose underlying intuitions remain quite simple and straightforward (as in a linear regression or Naive Bayes). In some cases, and Naive Bayes would be a good example, the implementation of a model may be very simple, but analysis of how the machine learner manages to control a source of error such as bias or variance entails much more sophisticated statistical understanding.  Many techniques and re-conceptualisations of how a model becomes a 'useful approximation' treat the models themselves as a kind of population whose variations and uncertainties, whose tendencies and predispositions must be sampled, tested and monitored.[^4.203]  Again, in terms of the structuring differences shown in Table \ref{table:differences}, while the model may be predictive in its relation to a given domain or field, the model itself as a machine learner  is the object of inference, and sometimes prediction (as in the case of the generalization error) in machine learning. 

# Does machine learning construct a new statistical reality?

The bias-variance decomposition points to an irreducible tension in the way that machine learning structures differences in the world. Following a broadly Foucaultean line of argument, Hacking proposes that statistical thinking and practice in the nineteenth and early twentieth century ontologically re-configured things in terms of probability distributions (and the Gaussian distribution in particular). What happens in worlds where the statistical treatment of error -- the bias-variance decomposition is a shorthand term for this -- structures the selection and assessment of machine learners? \index{diagram!statistical decomposition} I have suggested that both the ancestral probabilisation of domains and the statistical decomposition of prediction error come together in statistical machine learning.  The referential structure of the bias-variance decomposition includes both tightly bound points  and certainly 'relatively free or unbound points, points of creativity, change and resistance' [@Deleuze_1988, 44], as we saw in the case of the Naive Bayes classifier. It generates highly erroneous probability estimates but performs well as a classifier. The 'unbound points' in any diagrammatic process matter greatly to the relations of force at work in a knowledge-power conjunction.  The referential power of the models varies, and every attempt to construct a machine learner  in  a given setting relies either on the re-iteration of ancestral probabilities (that is, prior structuring of experience in conformity with the curve of some probability distribution) or on the many interactive adjustments, re-distributions and re-samplings of the world _and_ of the models associated with the bias-variance decomposition. 

Mayer-Schönberger and Cukier's argue that having much data or all data ($N=\forall{X}$) changes the epistemic power of data is a leitmotif in accounts of predictive modelling and analytics during the last decade.

>Using all the data makes it possible to spot connections and details that are otherwise cloaked in the vastness of the information. For instance, the detection of credit card fraud works by looking for anomalies, and the best way to find them is to crunch all the data rather than a sample [@Mayer-Schonberger_2013, 27]. 

Versions of this claim can be found running through various scientific and business settings throughout the 20th century.[^4.61] In certain settings, $N=all$ has been around for quite a while (as for instance, in many document classification settings where the whole corpus of documents have been electronically curated for decades). Mayer-Schönberger and Cukier rightly emphasize that the huge quantities of data sluicing through some contemporary infrastructures support inferences of probabilities (11). Their description of statistical sampling as a concept 'developed to solve a particular problem at a particular moment in time under specific technological constraints' [@Mayer-Schonberger_2013, 31]] does not, however, accommodate or acknowledge the entanglements of statistical techniques with power and knowledge, and in particular, the biopolitical structuring of differences since the late 18th century.   The very possibility of spotting connections or details that might matter itself relies on machine learners that structure differences in specific ways (generative/discriminative, parametric/non-parametric, bias/variance, prediction/inference). Looking for anomalies becomes possible, but only at the cost of machine learners becoming statistical.  Whether or not someone uses Naive Bayes, a topic model, neural networks or logistic regression, random variables,  probability distributions, error and model selection practices crowd in around  and re-configure machine learners, heavily affecting their referentiality. \index{referentiality} In many ways, the Mayer-Schönberger and Cukier account pays so much attention to the potentials of data accumulation that they cannot easily attend to the question of how machine learners go into the data as people try to 'spot connections and details', or of how what is put into the data that goes beyond $N=all$.  Here sampling, estimation, likelihoods, and a whole gamut of dynamic relationships between random variables in joint probability distributions reassert themselves amidst a population of models. The data may not be sampled, but models moving  through the high-dimensional common vector  spaces opened up by having 'all' the data are sampled. Not all machine learners are strictly speaking probabilistic models [^4.60], but the referential relation of all machine learners to the world has become increasingly statistical by virtue of the both the ancestral probabilisation and the bias-variance decomposition. 

Machine learning inhabits worlds that had already absorbed  statistical realities  at least a century earlier, whether through the social physics of Quetelet, the biopolitical normals of Francis Galton and his regression to the mean (remember that the linear model of regression is probably the basic machine learning model) or later in the probability functions of quantum mechanics in early twentieth century physics. Given a stochastic ontology, machine learning should be understood as a re-inversion. In this inversion, the probability distributions that had become the underlying reality of many different kinds of populations fold back or re-distribute themselves into populations of devices such as machine learners whose variations and uncertainties mesmerize contemporary classificatory and knowledge (scientific and otherwise) practice. Populations of models are sampled, measured, and aggregated in the ongoing production of statistical realities whose object is no longer a property of individual members of a population (their height, their life-expectancy, their chance of HIV/AIDS), but a meta-population of models of populations and their probability distributions. 


[^4.2]: Rob Kitchin provides a very useful overview of these claims in [@Kitchin_2014]. While I will not analyse the claims about 'big data' in specific cases in any great detail, the growing literature on this topic suggests that machine learning in its various operations -- epistopic construction of common vector space, function finding as association of partial observers and a re-internalisation of probability -- generates considerable difficulties and challenges for knowledge, power and production. 

[^4.20]: See chapter \ref{ch:vector} on this notion of data dimensionality.

[^4.51]: In 1964, N.J Bailey was taking the same approach to medical diagnosis [@Bailey_1965].  

[^4.61]: Later chapters of this book will track several instances of having all the data in the sciences, in government and in business in order to show what having all the data entails in different settings. 

[^4.60]:  Machine learning textbooks written by computer scientists tend to define probabilistic models more narrowly.  As Peter Flach suggests:

    >Probabilistic models view learning as a process of reducing uncertainty using data. For instance, a Bayesian classifier models the posterior distribution $P(Y|X)$ (or its counterpart, the likelihood function $P(X|Y)$) which tells me the class distribution $Y$ after observing the features values $X$ [@Flach_2012, 47]

    But whether they are probabilistic in this sense or not, the evaluation and configuring of machine learners irreducibly depends on a statistical treatment of errors and their trade-offs. 


[^4.3]: Part of this development has already been related in the previous chapter on 'learning' and function estimation (see chapter \ref{ch:function}). That chapter postponed any real discussion of statistical thought. Instead it explored the various sense of function and function finding that underpin machine learning. But much of that function finding and approximation garners referential weight through the statistical practices and modes of thought that accompany them. 

[^4.4]: Leo Breiman writing in 2001 during the heyday of academic development of machine learning argues, describes the 'two cultures' of statistics: 'in the past fifteen years, the growth in algorithmic modeling applications and methodology has been rapid. It has occurred largely outside statistics in a new community—often called machine learning—that is mostly young computer scientists (Section 7). The advances, particularly over the last five years, have been startling' [@Breiman_2001a, 200].\index{Breiman, Leo!on statistical cultures} Soon afterwards, statisticians such as Hastie, Tibshirani and Friedman publish the first major statistical machine learning textbooks [@Hastie_2001]. \index{Breiman, Leo!on statistical cultures}

[^4.113]: Maurizio Lazzarato describes ontological mutations as 'always machinic. They are never the simple result of the actions or choices of the "man" who, leaving the assemblage, removes himself from the non-human, technical, or incorporeal elements that constitute him -- all that is pure abstraction' [@Lazzarato_2014, 83]. Lazzarato's account of diagrams as processes that slip past signification and representation echoes Deleuze and Guattari's, but usefully highlights the plurality of machines that inhabit science, music, art, cities and markets.  \index{Lazzarato, Maurizio!machinic ontological mutation}

[^4.114]: The mapping that assigns numbers to outcomes (heads v. tails; cancer v. benign; spam v. not-spam) is a probability distribution.  As I have argued in [@Mackenzie_2015d], random variables have become much more widespread in statistical practice due to changes in computational techniques. \index{random variable}

[^4.115]: 'Distribution' pervades Foucault's account of power and knowledge from *The Order of Things* [@Foucault_1992] onwards.   \index{Foucault, Michel!on distribution} Foucault treats distributions in several different ways: as spatial or logistical techniques,  as mathematical orderings of large numbers of people or things, and as a methodological and theoretical framing device. In _Discipline and Punish_ [@Foucault_1977], the spatial sense prevails, but in later works, the population or demographic sense of distribution takes precedence [@Foucault_1998]. Distribution certainly has theoretical primacy in his account of power: 'relations of power-knowledge are not static forms of distribution, they are "matrices of transformations"' [@Foucault_1998, 99]. 

[^4.116]: The other contender for simplest machine learner would be the also very popular _k_ nearest neighbours. As Hastie et. al. observe: 'these classifiers are memory-based and require no model to be fit' [@Hastie_2009, 463]. Like the  Naive Bayes classifier, the equation for _k_ nearest neighbours is simple:

    \begin {equation}
    \label {eq:knn}
   \hat{Y}(x) =\frac{1}{k} \sum_{x_i \in \textit{N}_{k}(x)} y_i
    \end {equation}

    where $\textit{N}_{k}(x)$ is the neighborhood of $x$ defined by the $k$ closest points $x_{i}$ in the training sample [@Hastie_2009, 14].
    
    In equation \ref{eq:knn}, a parameter appears: $k$, the number of neighbours.  This contrasts greatly with the linear models discussed in chapters \ref{ch:vector} and \ref{ch:function} where the number of parameters $p$ usually equals the number of variables in the dataset or dimensions in the common vector space.  \index{machine learner!_k_ nearest neighbours}

[^4.117]: In [@Mackenzie_2014c], I have suggested that the intensification of multiplication associated with probabilistic calculation may constitute an important mutation in the ontological and practical texture of numbers. The epidemiological modelling of H1N1 influenza in London 2009 involved multiplying a great variety of probability distributions in order to calculate the conditional probability of influenza over time.  

[^4.7]: The input to the script is a single word such as 'finance' or 'deal'. The model is so simple that it only classifies a single word as spam. The `bash` script carries out four different transformations of the data in building the model. It uses only command line tools such as `wc` (word count), `bc` (basic calculator), `grep` (text search using pattern matching) and `echo` (display a line of text). These tools or utilities are readily available in almost any UNIX-based operating system (e.g. Linux, MacOS, etc). The point of using only these utilities is to illustrate the simplicity of the algorithmic implementation of the model.  The first part of the code downloads the sample dataset of Enron emails (and I will discuss spam emails and their role in machine learning below). Note that this dataset has already been divided into two classes - 'spam' and 'ham' -- and emails of each class have been placed in separate directories or folders as individual text files. \index{code!command line}

[^4.118]: Citation counts, even from the more reliable Reuters-Thomson Web of Science database, are difficult to evaluate when moving between disciplines. Some fields, such as computer science and biology, publish huge numbers of papers compared to smaller disciplines such as astronomy or plant ecology.

[^4.200]: The other lineage descends from medical diagnosis. For instance, starting in 1960, Homer Warner, Alan Toronto and George Veasy, working at the University of Utah and Latter-day Saints Hospital in Salt Lake City, began to develop a probabilistic computer model for diagnosis of heart disease [@Warner_1961; @Warner_1964]. Their model used exactly the same 'equation of conditional probability' we see in equation \ref{eq:naive_bayes} but now used to 'express the logical process used by a clinician in making a diagnosis based on clinical data' [@Warner_1961, 177]. Despite the mention of logic in this description, the diagnostic model was thoroughly probabilistic in the sense that the model itself has no representation of logic included in its workings. Rather it calculates the probability of a given type of heart disease given 'statistical data on the incidence of symptoms' [@Warner_1964, 558]. Somewhat ironically, as they point out, physicians involved in preparing and submitting data to the diagnostic program improved the accuracy in their own diagnoses. In 1964, N.J Bailey was taking the same approach to medical diagnosis [@Bailey_1965].  \index{medical diagnosis} \index{machine learner!Naive Bayes!history of} Heart disease to a central topic in machine learning (see chapter \ref{ch:function} for discussion of the `South African Heart Disease` dataset \index{dataset!South African Heart Disease}). 

[^4.202]: Another source of error, the 'irreducible error' [@Hastie_2009, 37] is noise that no model can eliminate.  

[^4.203]: The population of machine learners as a plurality requiring management and control forms a key topic of chapter \ref{ch:dimension} 

[^4.205]: Statistical graphics have a rich history and semiology that I do not discuss here (see [@Bertin_1983]).
