# The probabilistic event: machine learning into programming 

## alternative titles 

Believing in the strength of numbers


## overview

- what is happening to probability and why it matters?
  - much talk about N=All, and the end of sampling, but is that right?
    - not from the perspective of machine learning -- it just shifts where the probability and randomness is being done
    - quotes from Mayer-Schonberger, etc to illustrate this
- multiplying probabilities
    - why naive bayes is important -- multiplying probabilities, assuming they are independent of each other
    - the way machine learning techniques are wrapped/undergirded by statistical thinking: Ng lectures on learning theory, empirical risk minimization, cross-validation and regularization   and especially _bias-variance tradeoff_, underfitting and overfitting-- lectures 9-10;
    - how probability affects machine learning, support vector machine, or otherwise
    - more data beats better algorithms -- why naive bayes, why association rules, etc; simple density estimation vs parametric model
-hidden probabilities
    - why EM is important -- assume that there are hidden or latent random variables that can account for how things are mixed up. This is a major site to introduce unsupervised learning. Ng lectures 13-14-15;
    - the role of tests in machine learning
- synthetic probabilities
    -why MCMC is important - assume that what we think changes the numbers that have to be calculated;
    - the interplay between personalization and probabilistic classification: the ethical-political cost of this
- Implications
    - what low probability events mean; why they matter and how to find them- long tail distributions
    - how statistical thinking re-thinks the algorithmic in terms of the probabilistic - Parisi on this
    - how probabilistic programming matters
- how probability undercuts other more sophisticated takes on the world -- Hacking's argument about Peirce

## to do

- principal component analysis and unsupervised learning from the early 20C -- Pearson; the time when probability is objective
- naive bayes, including the ipython notebook; the Lewis article - 40 years, the spam filter example -- from schutt, from segaran, from myles-white; from hastie, flach, from top10 algo, etc
- discussions of Naive bayes in malley, flach, hastie, ripley
- friedman paper -- highly cited on virtues of naive bayes
- tom mitchell on why Bayes is a good way to think about machine learning
- Rassmussen & Williams on Gaussian processes
- likelihood function Flach, 27
- references cited by Ripler -- Warner 1961, Tittington, 1981; cf Michie 1994 textbook
- sequence of references: 
	- Warner 1961, Maron, 1961,  Tittington 1981, Michie, Ripley (DONE), Bishop (DONE), Mitchell (DONE), Domingos 1997, Lewis 1998 Witten (2001/2011),Hastie (2001/2009), Jordan (2004), Alpaydin (2010)(DONE), Manning (2009)(DONE),  Flach, top10-algo, Myles-White(DONE), Schutt (DONE), Hastie/R book (2013), Thoughtful Machine Learning (Kirk 2014)

## from proposal


The topic in this chapter is the role of randomness, chance and number in machine learning. Machine learning techniques are suffused with probabilistic modes of thought. This chapter foregrounds probability and  the changes in probability associated with both statistical pattern recognition models such as the Naive Bayes classifier (often used to filter spam email) and the so-called 'Bayesian revolution' in statistical practice that took shape in the early 1990s in particular in the form of Markov Chain Monte Carlo simulation (MCMC). The computationally intensive techniques of Bayesian analysis treat all numbers as potentially random variables; that is, as best described by probability distributions whose interactions need to be carefully explored.  By contrast, the probabilistic machine learning models exemplified by Naive Bayes  treat numbers as if they have little relation to each other.  The chapter traces two important implications of this contrast between ways of working with probability. The popularity of MCMC is a striking example of a technique moving  across fields, and the chapter will trace some of the ramifications of the heavily-used MCMC technique in fields ranging from nuclear physics, image processing to political science and online gaming. Second, although they both treat potentially all important numbers as a matter of probability calculus, the contrast between  computationally intensive, MCMC and probabilistic models such as Naive Bayes suggest very different beliefs in the power of computation.  A broader question here will be framed by reference to notions of expectation and belief: what mode of belief in probability better sensitises us to what machine learning or pattern recognition models do in given situations?

>Since its Baroque invention [@Hacking_1975], probability has been a double-sided coin. On one side, it concerns degrees of belief (the so-called 'subjective' view), and on the other side, frequencies, or how often things happen in the world (the so-called 'objective' view). In the last few centuries, one side of this coin has come up more often -- the frequency version of probability. Yet, as many historians of statistics, and statisticians themselves recognise, probability as degree of belief has never disappeared. It has only occurred less often, and been less often the object of belief. This ineluctable entwining of belief and events, of subjective-objective faces, in probability seems quintessentially Baroque in its interweaving and folding together of inside and outside. Drawing on contemporary statistical practice, and Gilles Deleuze's understanding of monads as 'simple, inverse, distributive numbers' [@Deleuze_1993], this paper examines the resurgence of the probability as degree of belief in the face of a world seemingly teeming with data. It argues that in the last few decades of statistical practice associated especially with 'Bayesian inference' and the techniques of Markov Chain Monte Carlo (MCMC) simulation, we see a re-configured and super-imposed concept of probability taking shape. As these practices pervade diverse scientific fields, commerce, government and industry, we might be seeing a different epistemic materialisation taking shape in which beliefs and events are less separate. On the contrary, through computation, subjective belief is exteriorised in simulated events, and a certain staging of events are reshaped as updateable beliefs. 

## quotes to use

from hacking_1990

>By the 1930s, however, the world teemed with frequencies, and the 'objective' notion would come to seem more important than the 'subjective' one for the rest of the century -- simply because there were soo many more frequencies to be known. 97

>It is pointless to debate which of the two ideas is correct. We note only that one or the other may be more dominant at different times. 97

>To return to _chance_ and _probabilite_: the fundamental distinction between 'objective' and 'subjective' in probability -- so often put in terms of frequency _vs._ belief -- is between modelling and inference. 98

stuff from parisi

> Instead, I take Chaitin’s computational proof of Omega or infinite probabilities, which are at once computably enumerable and algorithmically random, to explain that the automation of data implies an irreversible encounter with incomputable probabilities.  This is to say that the automated operations of prehension, and the mechanical procedures of computers, have their own blind spots, anomalies, and alien logic of calculation, which far from being computational failures are instead to be taken as symptoms of algorithmic thought. This is the point at which the sequential order of algorithms gives way to the conceptual prehension of computational infinities, when algorithms process data beyond what has already been programmed.192

stuff from amoore

stuff from Hastie on Naive Bayes
>It is especially appropriate when the dimension p of the feature space is high, making density estimation unattractive. The naive Bayes model assumes that given a class $G = j$, the features $X_k$ are independent 211

>While this assumption is generally not true, it does simplify the estimation
dramatically:

>• The individual class-conditional marginal densities $f_{jk}$ can each be estimated separately using one-dimensional kernel density estimates. This is in fact a generalization of the original naive Bayes procedures, which used univariate Gaussians to represent these marginals. • If a component $X_j$ of $X$ is discrete, then an appropriate histogram estimate can be used. This provides a seamless way of mixing variable types in a feature vector. 211
 
>Despite these rather optimistic assumptions, naive Bayes classifiers often outperform far more sophisticated alternatives. The reasons are related to Figure 6.15: although the individual class density estimates may be biased, this bias might not hurt the posterior probabilities as much, especially near the decision regions. In fact, the problem may be able to withstand considerable bias for the savings in variance such a “naive” assumption earns. 211

## Key examples:  Microsoft TrueSkill; Obama election data team
- the viagra spam example
Key techniques: Monte Carlo simulations and MCMC; Bayesian networks; 


 - probability and Bayesian inference - belief and desire in data  - belief chance, Bayes, internal proliferation of numbers; event-belief oscillation

## Introduction

The topic in this chapter is the role of randomness and probability in machine learning. While statistical techniques and practices have been discussed in previous chapters, this chapter foregrounds the changes in probability practices associated with machine learning, and in particular, $N = All$, the claim that with all the data, statistical thinking and the potentials of the statistical reasoning fundamentally change. Viktor Mayer-Schönberger and Kenneth Cukier's _Big Data: A Revolution That Will Transform How We Live, Work and Think_  present this shift in many different ways in the course of the vignettes and comparisons that have become typical of the data revolution genre. In a chapter entitled 'More,' they sketch the transition from data practices reliant on sampling to data practices that deal with all the data. 

>Using all the data makes it possible to spot connections and details that are otherwise cloaked in the vastness of the information. For instance, the detection of credit card fraud works by looking for anomalies, and the best way to find them is to crunch all the data rather than a sample [@Mayer-Schonberger_2013, 2013,  27]


In the several hundred pages that follow in _Big Data_, the problem of how to 'crunch all the data' is never really discussed. While they mention the role of social network theory (30), 'sophisticated computational analysis' (55),  'predictive analytics' (58) and 'correlations' (7),  and they do say that 'the revolution' is 'about applying math to huge quantities of data in order to infer probabilities' (12), any further consideration of specific techniques of data crunching or the math is largely left aside. This is not to criticize a book that sets out to describe trends affecting business and government for a general readership, but without a sense of how statistical thinking animates almost all salient features of crunching the data and particularly the predictions, it becomes hard to see how the 'revolution' takes place. In other words, the shift from $N=n$ to $N=all$ does not occur without other transpositions and rearrangements that do not simply concern choices about how much data to use.

I approach these transpositions  mainly through a contrast between the extremely simple and easily implemented Naive Bayes classifier and the so-called 'Bayesian revolution' in statistical practice that took shape in the early 1990s, and in particular, the key algorithmic technique used in Bayesian statistics, Markov Chain Monte Carlo simulation (MCMC) as it is applied in a typical 'big data' technique of 'topic modelling' [@Blei_2011]. Across the gamut of differences between the very simple Naive Bayes approach and the sophisticated MCMC techniques at work in topic modelling, we can see a broad ranging shift in probability practice has been occurring. This shift is not captured by the shift from $N=n$ to  $N=all$.  The key argument here, following on from the discussions of vector space, the function of optimisation and the development of decisionistic mechanisms developed in the preceding chapters is that what is happening to data, the ways in which are being corralled and marshalled, and the kinds of pattern-finding, anticipation or pre-emption delivered by machine learning need to be considered alongside an introjection of probability. This is not the first such introjection, since probability, as we will see, has undergone convoluted transformations before (for instance between subjective and objective: see [@Hacking_1975; @Hacking_1990] for standard accounts of these changes). The multiple, overlapping and often cantilevered ways in which probability practice runs through machine learning, however, suggests that another transformation is in train. Accompanying this transformation, amplifying and ramifying it, certain techniques -- Naive Bayes, MCMC, but also so-called 'ensemble' methods that stage populations of models and specific techniques such Expectation Maximization or the bootstrap -- carry this transformation wide and deep. Via a combination of these techniques, many more things become probabilistic. Words, images, faces, places, feelings, prices, and all kinds of derivative numbers display an increasingly probabilistic mode of existence that may not completely replace or suppress other ways in which they inhabit the world, but certainly induce existential oscillations in them. 

Put in slightly awkward technical terms, we might say that many more aspects of data, of prediction, of classification and pattern, exist more _probabilistically_. The computationally intensive MCMC techniques used in Bayesian analysis treat all numbers as potentially random variables. The Naive Bayes technique treats all outcomes as conditional probabilities, not too far removed from coin tosses in certain respects. The bootstrap techniques change the way that data relates as samples to populations, and presents datasets as populations that can be intensively re-sampled in order to effectively make the dataset a bigger portion of the population it was originally cut from. In contrast to the line drawing, curve fitting, maximising and minimising of the linear models, the decision trees, and the support vector machines, the probabilistic transformations wrought through machine learning start, I would suggest, to affect how we experience collectives, how we apprehend individuality, as well as more diffuse collective entities such as populations, and perhaps markets. In a world where probability and its associated practices of randomization are no longer confined to specific centres of calculation such as laboratories or bureaus of statistics, chance, randomness and probability become part of the fabric.

Immediately below, I introduce some new operating terminology for this chapter (terms such as random variable, probability distribution and likelihood).  I'm suggesting that we countenace a scene in which multiple different probability practices stack on top of each other. Amidst these, random variables and their associated probability distributions particularly concern us. As we will see, the power of the transformation in probability associated with machine learning algorithms resides in their capacity to draw in many more relations, features and components of data in support of a probabilistic outcome. When things are best described as probability distributions, they take on a different form of temporal and multiplicative existence. A probabilistically generated airline seat price attracts different kinds of transactions than a fixed priced seat. Similarly, a probabilistically generated tumour classification implies different modes of responsiveness and care. Gaining some understanding of the concrete arrangements of forces in these probabilistic modes might allow us to account for the ways in which certain methods -- Bayesian inference is a striking example of transverse momentum of methods across fields  -- go on the move, or why certain problems -- automatic text classification, image recognition, etc -- suddenly hove into feasibility.

[TODO: the definitional work; move stuff up from below to do this]

The chapter traces two important implications of this technique. First, because it is so computationally intensive, MCMC and Bayesian inference, although statistically powerful, are difficult to apply to many dimensional datasets. So Bayesian computation iconically figures the limits of contemporary data practices, with their ambitions to incorporate all available data into calculation. Second, in certain ways this technique challenges us to re-evaluate how we think about numbers. By following some of the ways numbers circulate through MCMC algorithms, we can discern to a semiotic-material faultline running through contemporary number formations. Numbers semiotically and materially embrace both events and degrees of belief. If numbers are crucial in the data economy, then instabilities in their mode of existence will affect much of what happens to data. While much of the machine learning taking place in commercial and operational settings is decidedly non-Bayesian, the popularity of MCMC and Bayesian approaches in contemporary sciences suggests a tension in what counts as number.


## Naive Bayes

\begin {equation}
\label {eq:naive_bayes}
$$f_j(X) = \prod_{k=1}^{p}f_{jk}(X_k)$$
\end {equation} [@Hastie_2009, 211s]

Some machine learning techniques are so simple that they can be implemented in a few lines of code. Their simplicity, however, belies their power. The function shown above \ref{eq:naive_bayes} is about the simplest one to be found in most machine textbooks. It expresses the Naive Bayes classifier, one of the most popular machine learning algorithms, even though it is more than 50 years old [@Hand_2001]. While \ref{eq:naive_bayes} does not set out all the steps in transforming some training data into a predictive model, the lines of code needed to do this are similarly brief. In _Doing Data Science_, Rachel Schutt and Cathy O'Neill furnish a bash script (that is, command line instructions) to download a well-known email dataset and build a Naive Bayes classifier for spam:

```{r enron_nb_bash, engine='bash', echo=TRUE}

    #!/bin/bash
    # file: enron_naive_bayes.sh
    # description: trains a simple one-word naive bayes spam
    # filter using enron email data
    # usage: ./enron_naive_bayes.sh <word>
    # requirements:
    #    wget
    #
    # author: jake hofman (gmail: jhofman)
    # how to use the code
    if [ $# -eq 1 ]
    then
        word=$1
    else
        echo "usage: enron_naive_bayes.sh <word>"
        exit
    fi

     ### PART 1
    if ! [ -e enron1.tar.gz ]
    then
        wget 'http://www.aueb.gr/users/ion/data/enron-spam/preprocessed/enron1.tar.gz'
    fi

    if ! [ -d enron1 ]
    then
        tar zxvf enron1.tar.gz
    fi

    cd enron1

    ### PART 2
    Nspam=`ls -l spam/*.txt | wc -l`
    Nham=`ls -l ham/*.txt | wc -l`
    Ntot=$Nspam+$Nham
    echo $Nspam spam examples
    echo $Nham ham examples

    Nword_spam=`grep -il $word spam/*.txt | wc -l`
    Nword_ham=`grep -il $word ham/*.txt | wc -l`
    echo $Nword_spam "spam examples containing $word"
    echo $Nword_ham "ham examples containing $word"

    ### PART 3
    Pspam=`echo "scale=4; $Nspam / ($Nspam+$Nham)" | bc`
    Pham=`echo "scale=4; 1-$Pspam" | bc`
    echo
    echo "estimated P(spam) =" $Pspam
    echo "estimated P(ham) =" $Pham
    Pword_spam=`echo "scale=4; $Nword_spam / $Nspam" | bc`
    Pword_ham=`echo "scale=4; $Nword_ham / $Nham" | bc`
    echo "estimated P($word|spam) =" $Pword_spam
    echo "estimated P($word|ham) =" $Pword_ham

    ### PART 4
    Pspam_word=`echo "scale=4; $Pword_spam*$Pspam" | bc`
    Pham_word=`echo "scale=4; $Pword_ham*$Pham" | bc`
    Pword=`echo "scale=4; $Pspam_word+$Pham_word" | bc`
    Pspam_word=`echo "scale=4; $Pspam_word / $Pword" | bc`
    echo
    echo "P(spam|$word) =" $Pspam_word
    cd ..
    ```
[@Schutt_2013, 105-106]

```{r enron_nb_r, echo = FALSE}

        f_ham = list.files('enron1/ham', full.names = TRUE)
        f_spam = list.files('enron1/spam', full.names = TRUE)
        ham_count = length(f_ham) 
        spam_count = length(f_spam)

        ham = sapply(f_ham, readLines, warn=FALSE)
        spam = sapply(f_spam, readLines, warn=FALSE)
        P_ham = ham_count/(ham_count + spam_count)
        P_spam = spam_count/(ham_count + spam_count)
        word = 'gas'

        predict_spam <- function(word) {
            ## count the occurrence of the word in each
            Nword_in_ham = sum(grepl(word, ham))
            Nword_in_spam = sum(grepl(word, spam))
            #cat(Nword_in_ham, ' ham examples contain ', word,  '\n')
            #cat(Nword_in_spam, ' spam examples contain ', word,  '\n')

            #cat('estimated P(spam) = ', P_spam,  '\n')
            #cat('estimated P(ham) = ', P_ham,  '\n')

            Pword_spam = Nword_in_spam/spam_count
            Pword_ham = Nword_in_ham/ham_count
            #cat("P(spam|", word, ") = ", Pword_spam,  '\n')
            #cat("P(ham|", word, ") = ", Pword_ham,  '\n')

            Pham_word = Pword_ham * P_ham
            Pspam_word = Pword_spam * P_spam
            Pword = Pspam_word + Pham_word
            Pspam_word = Pspam_word/Pword

            #cat("P(spam|", word, ")=",  Pspam_word,  '\n')
            return(Pspam_word)
        }
```


The input to the script is a single word such as 'finance' or 'deal'. The model is so simple that it only classifies a single word as spam. The `bash` script carries out four different transformations of the data in building the model. It uses only command line tools such as `wc` (word count), `bc` (basic calculator), `grep` (text search using pattern matching) and `echo` (display a line of text). These tools or utilities are readily available in almost any UNIX-based operating system (e.g. Linux, MacOS, etc). The point of using only these utilities is to illustrate the simplicity of the algorithmic implementation of the model.  The first part of the code downloads the sample dataset of Enron emails (and I will discuss spam emails and their role in machine learning below). Note that this dataset has already been divided into two classes - 'spam' and 'ham' -- and emails of each class have been placed in separate directories or folders as individual text files.

The text of a typical spam email in the Enron datasets looks like this:
>
Subject: it ' s cheating , but it works !
can you guess how old she is ? the woman in this photograph looks like a happy teenager about to go to her high school prom , doesn ' t she ? she ' s an international , professional model whose photographs have appeared in hundreds of ads and articles whenever a client needs a photo of an attractive , teenage girl . but guess what ? this model is not a teenager ! no , she is old enough to have a 7 - year - old daughter . . and . . . the model ' s real age is in her 30 ' s . all she will say about her age to her close friends is , " i ' m dangerously close to 40 . " she also says , " if it weren ' t for this amazing new cosmetic cream called ' deception , ' i would lose hundreds of modeling assignments . . . because . . . there is no way i could pass myself off as a teenager . " learn more about this amazing new product . . . please refer all questions , opinions or additional feedback to :
service dept
9420 reseda blvd # 133
northridge , ca 91324

The text of a typical non-spam email like this:

> Subject: industrials
suggestions . . . . . .
- - - - - - - - - - - - - - - - - - - - - - forwarded by kenneth seaman / hou / ect on 01 / 04 / 2000
12 : 47 pm - - - - - - - - - - - - - - - - - - - - - - - - - - -
pat clynes @ enron
01 / 04 / 2000 12 : 46 pm
to : kenneth seaman / hou / ect @ ect , robert e lloyd / hou / ect @ ect
cc :
subject : industrials
ken and robert ,
the industrials should be completely transitioned to robert as of january 1 , 2000 . please let me know if this is not complete and what else is left to transition .
thanks , pat

The second part of the code ('Part 2') counts the number of emails in each category and prints them, and the counts the number of times that the chosen word (e.g. 'finance' or 'deal') occurs in the spam and ham categories. Using these counts, the script (in what I am calling Part 3) estimates probabilities of any email being spam or ham, and then given that email is spam or ham, that the particular word occurs. To estimate a probability means, in this case, to divide the word count for the chosen word by the count of the number of spam emails, and ditto for the ham emails.  In Part 4, the final transformation of the data, these probabilities are used to calculate the probability of an email being spam given the presence of that word. Again, the mathematical operations here are no more complicated than adding, multiplying and dividing. The probability that the chosen word is a spam word is, for instance, the probability of occurrence of the word in a spam email multiplied by the overall probability that an email is spam. Finally, given that the overall probability of the chosen word occurring in the email dataset is the probability of it occurring in spam plus the probability of it occurring in ham, the overall probability that an email in the Enron data is spam given then presence of that word can be calculated. It is the probability that the chosen word is a spam word divide by the probability of that word in general. 

The point of this slightly bamboozling and mechanical description of what the script does is to show something of how \ref{eq:naive_bayes} can be translated into code. Hardly any machine learning models are so simple that they can be conveyed in 30 lines of code (including downloading the data and comments). The second feature of this script is that it shows that nothing that occurs in relation to probability is intrinsically mysterious, elusive or somehow transcendental. On the contrary, everything here is counting, adding, multiplying (that is, repeated adding) and dividing (that is, multiplying by parts or fractions). Everything is constrained by a certain limit on the results. Probabilities are always between `0` and `1`, or between 0% and 100%.  'Finance' has a  `r round(predict_spam('finance'),2)` chance of being spam, while 'sexy' has a chance of `r round(predict_spam('sexy'), 2)`, at least in the Enron emails.  Finally, the transformation of textual forms -- the emails -- into probabilistic events is relatively straightforward. It relies on counting how often things occur. This is a trivial point in some ways. Everyone knows probability and statistics concerns counting things. The quantification entailed in statistics is a counting of events that have been named or labelled in some way. The perhaps more profound point is that this counting of atomic events can be combined in a seemingly limitless variety of combinations. In the emails, individual words are events, and therefore each email is a complicated composite event. Similarly, in machine learning on images, each pixel could be treated as an event, and the image as a whole becomes an immensely complicated aggregate colour and light event. As we will see, many other machine learning techniques try to deal with this combinatory character of composite events directly. 

## The surprising success of Naive Bayes classifiers

Hastie, Tibshirani and Friedman characterise the value of the  Naive Bayes technique in relation to its capacity to deal with high dimensional data (see Chapter 2 on this notion of data dimensionality):    

>It is especially appropriate when the dimension $p$ of the feature space is high, making density estimation unattractive. The naive Bayes model assumes that given a class $G = j$, the features $X_k$ are independent [@Hastie_2009, 211]. 

In \ref{eq:naive_bayes}, $p$ stands for the number of different dimensions or variables in the data set, and that the outcomes can be classified in $j$ different classes. In the spam classifier, the number of dimensions is probably quite large. That is, every unique word adds a new dimension to the 'feature space.' By contrast, there are are only two classes $G$, spam and non-spam.  As we see in both \ref{eq:naive_bayes} and from the `bash` script, the only operation carried out on the data is a multiplication. The $\prod$ operator multiplies probabilities in order to generate a classification. Compared to the complications of logistic regression, neural networks or support vector machines, \ref{eq:naive_bayes}  seems incredibly simple. How is it that a simple multiplication of probabilities can, as Hastie and co-authors write: 'often outperform far more sophisticated alternatives' [@Hastie_2009, 211]. Similar formulations can  be found in most of the machine learning books and instructional materials currently available. Along with linear regression, Naive Bayes might be _the_ standard introductory machine learning technique, where the Naive Bayes classifier is almost always demonstrated to the problem of filtering spam email [@Conway_2012; @Schutt_2013, 93-113; @Kirk_2014, 53; @Lantz_2013, 92-93; @Flach_2012, @Ng_2008b], and in particular dealing with the abundance of spam emails concerning Viagra (itself a byproduct of the failure of a statistical model). It is remarkable how Naive Bayes comes up in machine learning textbooks especially given how simple the technique is. Admittedly spam, and spam trying to sell Viagra in particular, has been a very familiar part of most email users lives for a decade and more. In practice, Naive Bayes classifiers and variations of them have become an integral part of managing email traffic for most people, whether they know it or not. And large sample datasets of email tagged as 'spam' or 'ham' are widely available.

I wonder, however, whether the constant reiteration of spam filtering using Naive Bayes stands in as some kind of prototypical learning about machine learning situation, and in this learning situation, the application of probability to language and texts has a central importance. The ambition to algorithmically  classify written texts responds to the sheer vastness and density of written communications in contemporary media environments. As Drew Conway and John Myles-White write in _Machine Learning for Hackers_, 

>At its core, text classification is a 20th century application of the 18th century concept of _conditional probability_. A conditional probability is the likelihood of observing some thing given some other thing we already know about [@Conway_2012, 77] 

They point here to the role of 'conditional probability,' a kind of probability that lies at the heart of many of the data transformation associated with prediction or pattern recognition. As any of the many accounts of the technique will explain, the name comes from Bayes Theorem, one of the most basic yet widely used results in probability theory (dating from the 18th century), yet Naive Bayes does not even fully embrace Bayes Theorem as the principle of its operation (hence the name 'naive' Bayes or previously 'idiots' Bayes). It simply looks for differences in terms of which words are more likely to appear in spam messages and which words are more likely to appear in non-spam or ham messages. Moreover, although it produces probabilistic results, the actual values of those probabilities matters less than their relative value (for instance, are they greater than or less than 0.5). 

For our purposes, Naive Bayes classifiers, as well some other more sophisticated statistical techniques  such as the Expectation Maximisation algorithm and the Monte Carlo Markov Chain technique that underpin the topic model text classifiers, attest to the deeply probabilistic character of machine learning. In contrast to the geometrical and topological concerns of dimensionality and features, or to the rule-based structures generated as decision trees, the probabilistic character of Naive Bayes entails a more unstable and perhaps in certain respects more supple relation to worlds. This suppleness comes at the cost of much greater uncertainty, and certainly it changes what we think of algorithmic processes if they start to become probabilistic in their operations. 

The tranformations that machine learning bring to probability itself are not insignificant. As we will see, the growth of these techniques operationalises probabilities in new ways, or at least in ways that render more and more untenable the long-standing dichotomy between probability as objective randomness in the world and probability as measure of our own uncertain knowledge of the world. At core, both the objective and subjective treatments of probability find themselves reassembled in a new algo-probablistic assemblage. By virtue of its simplicity, the Naive Bayes classifier helps to show this. 

## More data or better algorithms?

It has been known for almost two decades that Naive Bayes can perform almost as well on classification tasks as decision trees or neural networks [@Mitchell_1997, 177]. This is surprising since statistically speaking, the Naive Bayes model is very simplistic. Every account of it immediately points out the naivete of the model:

>naive Bayes .. assumes statistical independence of all the features: at a very minimum this means the features are assumed to be all uncorrelated with each other. ... This is often a wildly implausible assumption but the method has been shown to work surprisingly well [@Malley_2011, 46]. 

The assumption that all of the features are 'statistically independent' effectively means that the model observes the world as a set of incoherent or unrelated processes. While such a world is a fascinating possibility, it does not resonate with many of the patterns that even the most insistent empiricism would admit largely hold sway over time. Hence the term 'naive' points to this innocence in relation to patterns of events. If the Naive Bayes classifier is so agnostic about the relation between events, how does it manage to 'learn' (in the sense that machine learning thinks about learning)? Or to put this question more practically, how is it that Naive Bayes classifiers can tell the difference between spam and non-spam email?

The conditions under which Naive Bayes perform well are rather specific it turns out. The assumption in spam classification is that all emails can be definitively classified as either spam or non-spam. Given that hard and fast classification, the presence of words such as 'sildenafil' or 'Viagra' are very much more likely to be associated with spam email than non-spam. (For people who want to get Viagra, the assumption does not hold so well. How a Naive Bayes classifier would sort the email of a Viagra user is an interesting question.) Granted this assumption,  the problem of spam classification, a setting in which machine learning approaches have long been in mundane operation, is understood by Naive Bayes models as one in which words are used in emails somewhat randomly, or with a degree of unusual degree of unrelatedness. While some words are definitely more like to be associated with spam - 'viagra', 'inheritance', 'lottery' or 'prize' -- these words have no relation to each from the Naive Bayes perspective. The kind of partial observer at the core of the model simply counts the frequency of occurrence of all words found in emails, calculates the probability of any individual word occurring, and then estimates the probability of each observed conjunction of words. As Tom Mitchell puts it:


>The naive Bayes classifier is based on the simplifying assumption that the attribute values are conditionally independent given the target value. In other words, the assumption is that given the target value of the instance, the probability of observing the conjunction $a_, a_2 ... a_n$ is just the product of the probabilities for the individual attributes: $P(a_1, a_2 ... a_n|v_j) = P(\prod_i P(a_i|v_j)$ [@Mitchell_1997, 177]

In this formulation, the 'target value' is the label 'spam' or 'not-spam.' The 'attribute values' are the presence of particular words. The 'instance' is the actual email under consideration. The product of the probabilites we have already seen in \ref{eq:naive_bayes} in the form of the $\prod$ operator that multiplies the probabilities of different features to produce the probability that a specific combination of words is likely to occur.  As in all supervised machine learning, the techniques relies on a training dataset of already labelled emails in order to estimate the probability that a given email is spam or not. It bears repeating that in almost no respect are machine learning techniques. They always rely on someone's previous work or ongoing work to do classification. The only respect in which they might be seen as automatic concerns how that ongoing work produces effects of scale. While the training dataset for an email spam classifier might consist of 100,000 emails, the email classifier could in practice classify millions of emails once it has been trained on the labelled email dataset. 

## The ancestral community of Naive Bayes

Like many of machine learning techniques, Naive Bayes has a long standing running history, and this history runs through all current demonstrations and implementations of the technique. As Lucy Suchman and Randall Trigg wrote in their study of work on artificial intelligence, 

>rather than beginning with documented instances of situated inference ... researchers begin with ... postulates and problems handed down by the ancestral communities of computer science, systems engineering, philosophical logic, and the like [@Suchman_1992,174]. 

Not only does Bayes Theorem date from the 18th century, but the first attempts to use what is now called Naive Bayes in the early 1960s already display many of the traits prominent in current deployments of the technique. Working at the RAND Corporation in the early 1960s, M.E. Maron described how 'automatic indexing' could become 'probabilistic automatic indexing.' The necessary statistical assumption was:

>The fundamental thesis says, in effect, that statistics on kind, frequency, location, order, etc., of selected words are adequate to make reasonably good predictions about the subject matter of documents containing those words [@Maron_1961, 406]

This fundamental thesis has remained somewhat fundamental in text classification and information retrieval applications, as well as many other machine learning approaches. Statistics on kind, frequency, order and location are often the only chains of reference that these techniques maintain in relating to their source data. The fact that certain complexities or relationalities might be filtered out by the fundamental thesis is acknowledged explicitly, but compensated by a practical interest in automatic classification when confronted by large numbers of documents. As we will see, even in the most recent iterations of these techniques in topic modelling [@Blei_2011], with all its statistical sophistication, the fundamental thesis largely applies.  

In addition to the longevity of the technique, we might attend to the shape of the datasets. Maron's work focused on a collection of several hundred abstracts of papers published in the March and June 1959 issues of the _IRE Transactions on Electronic Computers_. As in contemporary supervised learning, these abstracts were divided into two groups, a training and a test set ('group 1' and 'group 2' in Maron's terminology [@Maron_1961, 407]), and the training set was classified according to 32 different categories that had already been in use by the Professional Group on Electronic Computers, the publishers of the _IRE Transactions_. Given these classifications, word counts for all distinct words in the abstracts were made, the most common terms ('the', 'is', 'of', 'machine', 'data', 'computer') and the most uncommon words removed, and the remaining set of around 1000 words were actually used for classification. This treatment of the abstracts as documents, then as lists of words, and then as frequencies of terms, and finally as a filtered list of most information rich terms continues in much text classification work today. Interestingly, they are rarely performed in relation to the spam classification demonstration (and I will discuss some possible reasons for this below). 

Not just words, but elements of many different datasets -- images, instrument measurements, clocktimes or timestamps, and in short almost anything that can be counted -- could be handled in the same way by a Naive Bayes classifier. In 1964, N.J Bailey was taking the same approach to medical diagnosis [@Bailey_1965]. The key point is that the categories -- Maron's 32 categories drawn from _IRE Transactions_ or the two categories of 'spam' or 'ham' from the Enron dataset -- appear as 'response' or target variables, and the remainder of the dataset -- the words or other elements -- appear as 'predictors' or 'feature variables.' Everywhere, numbers are increasingly in variation, and display increasingly distributional characteristics.  


[TODO] Say how these practices are dealing with words statistically. What does it matter than certain words are dropped because they are too frequent or too rare. How does the scaling up from hundreds of docs in Maron to millions of docs in Blei matter?

## Words as random variables

Everywhere, numbers are increasingly in variation, and display increasingly distributional characteristics.  While some events have discrete outcomes (Obama was re-elected in 2012), many happenings are not. In Lancaster here in north-west England, the probability of rain on a given day would be say 70%, but days have very different amounts of rain.  Some days, it rains once briefly and lightly. Other days it rains frequently and heavily. A gamut of rain events can occur, and each would distribute different amounts of water on Lancaster. Given the amount of variation,  a much better way to face the weather is to say that Lancaster's rain is a *random variable*: ‘a random variable is a mapping that assigns a real number to each outcome’ [@Wasserman_2003,19]. If events have probabilities, random variables have a range of outcomes that are mapped to numbers. Again, the deceptive simplicity of 'mapping' hides many variations. Mapping is a form of one-to-one correspondence, usually expressed as a mathematical function. A random variable links events to numbers through functions.  Again, all this remains rather formal. The practical reality of random variables is variation, variations that are  visually expressed by curves and by probability distributions. 

```{r  distributions2, echo=FALSE, message=FALSE, warning=FALSE, fig.cap='Distributions'} 

	#*source('mcmc_examples.R')*/
	#*generate_distributions()*/
```
Probability distributions are a common way of showing and  talking about random variables. The curves shown in Figure 2 could refer to almost anything (the chances of rain at different times of day in Lancaster, the seasonal variation in precipitation, etc.). These distributions appear in countless shapes and forms in scientific, government and popular literature of many different kinds. Statistical graphics have a rich history and semiology that I do not discuss here (see [@Bertin_1983]). Perhaps the most famous function or mapping is the normal or Gaussian distribution:

>$f(x;\mu, \sigma^2) = \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}$

This  function, whose mathematics were intensively worked over during the 18-19th centuries, has a power-laden biopolitical history closely tied with knowledges and governing of  national and other  populations. The key symbols here include $\mu$, the mean and $\sigma$, the variance. These two parameters together describe  the shape of the curve. Given $\mu$ and $\sigma$, it become possible to map different outcomes to probabilities. Given the normal distribution, it is possible, under certain circumstances, to effectively subjectify someone on the spot. For instance, if an educational psychologist indicates to someone that their intelligence lies towards the left-hand side of the normal curve peak in Figure 2 (and hence less than the population mean), they quickly render them somehow subject to the normal curve. But statistics uses dozens of different probability distributions to map continuous and discrete variations to real numbers. Other probability distributions abound — normal (Gaussian), uniform, Cauchy exponential, gamma, beta,  hypergeometric, binomial, Poisson, chi-squared, Boltzmann-Gibbs distributions, etc (see [@NIST_2012] for a gallery of distributions) — because outcomes occur in widely differing patterns. The  queuing times at airport check-ins do not, for instance, easily fit a normal distribution. Queues are usually modelled using a Poisson distribution, which unfortunately for travellers, distributes waiting times very differently.  Similarly, it might be better to think of the probability of rain today in Lancaster in terms of a Poisson distribution that models that queue of clouds in the Atlantic just waiting to land on the northwest coast of England. Rather than addressing the question of if it will rain or not, a Poisson-based model might address the question of how soon.

The diverse range of probability distributions — and we will see below some reasons why we can expect them to proliferate in certain settings — attests to the variety of ways in which events might be mapped to real numbers. Despite  the sometime forbidding mathematical equations, the term *distribution* emphasises a quite material or tangible way of thinking about how probabilities vary. The curves in both Figure 1 and Figure 2 are examples of the most common mathematical descriptions in any data analysis setting: they are  *probability density* functions (pdf). (There are also  *probability mass* functions for variables that have discrete values; for instance: 1,2,3,4,5). Pdfs such as the Gaussian function shown above are usually graphed as a curve that indicates how likely a random variable is to take on a particular value. In many cases, statistical practice seeks to estimate distribution functions such as pdfs (or their close relatives, cdfs — *cumulative distribution functions*) for the given data. Statisticians speak of 'fitting a density' to data, emphasising their assumption that events can be incorporated in the forms of probability distributions. The underlying probability distribution is in principle ‘unobservable’ as such, but a probability density function is  assumed to give rise to all the variations in data gathered through experiments and observations. The task is to estimate the shape of that curve, and its defining parameters (means, variance, etc.). Given that curve, areas under the pdf equate to the likely range of value of a variable. While the total 'probability mass' under the probability density function curve always must be equal to one (since the combined probability of all possible outcomes = 1), finding the area under particular parts of the curve is a key issue. Finding the area under probability density curves becomes the way in which many epistemic processes envisage lived states of affairs as random variables or as numbers in variation.


## The random variable $X$ and the random variable $G$
As Flach suggests:
>Probabilistic models view learning as a process of reducing uncertainty using data. For instance, a Bayesian classifier models the posterior distribution $P(Y|X)$ (or its counterpart, the likelihood function $P(X|Y)$) which tells me the class distribution $Y$ after observing the features values $X$ [@Flach_2012, 47]

The process of reducing uncertainty differs from the hyperplane and decision tree treatments of classification because it allows the construction of a _generative_ rather than a _discriminative_ predictive model. 


>Such models are called 'generative' because we can sample from the joint distribution to obtain new data points together with their labels. [Flach_2012, 263]


