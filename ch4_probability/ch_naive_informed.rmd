\chapter{$N = \forall\boldsymbol{X}$ Probabilisation and the Taming of Machines}
\label{ch:probability}

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=12, tidy=TRUE, fig.height=8, 
                      echo=FALSE,  fig.show='hide', results='hide', warning=FALSE, message=FALSE, dev='pdf')
library(RSQLite)
con <- dbConnect(RSQLite::SQLite(),'../ml_lit/all_refs.sqlite3')
res = dbGetQuery(con, statement ="select * from basic_refs limit 10;")
library(ggplot2)
library(stringr)
library(xtable)
options(xtable.comment = FALSE)
options(xtable.comment = FALSE)
options(xtable.size = 'tiny')
options(xtable.tableplacement = '!htbp')
options(xtable.rownames = FALSE)
simpleCap <- function(x) {
    s <- strsplit(x, " |-")[[1]]
    return (paste(toupper(substring(s, 1,1)), tolower(substring(s, 2)), sep="", collapse=" "))
  }
```

In the final pages of _The Taming of Chance_,  Ian Hacking describes the work of the philosopher C.S. Peirce in terms of a twin affirmation of chance. First, Peirce, following the work of the psychophysicist Gustav Fechner and before him the astronomer-sociologist Adolphe Quetelet, ontologizes the normal curve.[^4.1] The 'personal equation,' the variation in measurements made by any observer, becomes 'a reality underneath the phenomena of consciousness' [@Hacking_1990, 205]. \index{Hacking, Ian!\textit{The Taming of Chance}} Peirce's belief in absolute chance or a stochastic ontology, \index{ontology!stochastic} 'a universe of chance' as Hacking puts it, continued a series of 'realizations' of curves, in which astronomical, social, biological and finally psychological variations were all understood as generated by processes of chance.  Second, and in order to show the underlying reality of the normal curve, 'Peirce deliberately used the properties of chance devices to introduce a new level of control into his experimentation. Control not by getting rid of chance fluctuations, but by adding some more' [@Hacking_1990, 205]. In the century or so since, what happened to the thorough-going affirmation of statistical thought and probabilistic practice epitomised by Peirce? \index{Peirce, Charles Sanders!chance} Hacking stresses that he does not understand Peirce as the precursor or the innovator of twentieth century statistical thought (Hacking's _Taming of Chance_ ends at 1900), but rather as 'the first philosopher to conceptually internalize the way chance had been tamed in the nineteenth century' (215). What would the equivalent philosopher-machine learner internalize today? What would such persons, working in science or media or government, hold firm in relation to chance, probability and statistics? \index{Hacking, Ian!on C.S. Peirce}

[^4.1]: The historian of statistics Stephen Stigler provides a lengthy account of Fechner's work in [@Stigler_1986, 239-259].

Does the setting that machine learners encounter differ from that concerned Peirce in his work at the U.S. Government Coast Survey in the 1870s? In the opening lines of the preface to the First Edition of _Elements of Statistical Learning_, Hastie, Tibshirani and Friedman write:

>The field of Statistics is constantly challenged by the problems that science and industry brings to its door. In the early days, these problems often came from agricultural and industrial experiments and were relatively small in scope [@Hastie_2009, xi] \index{\textit{Elements of Statistical Learning}!on statistics}

At the end of the preface, they also cite, we might note in passing, Hacking's work: 'The quiet statisticians have changed our world' [@Hastie_2009, xii]. One of the challenges science and industry has brought to the  door of statistics in recent years has not only been more data but also machine learners.  What difference do the 'vast amounts of data ... generated in many fields' (xi) make to the field of statistics? Statistics has, I will suggest in this chapter, gradually _probabilised_ machine learners, or endowed with a substratum of chance that flows directly from their operation. \index{probabilisation} To grasp this \gls{probabilisation}, we need to determine what role randomness, change and the probabilistic distribution of elements and events play in machine learning.  These questions of how worlds becomes thinkable through machine learning can be addressed partly by contrasting the 'taming of chance' achieved  by eighteenth and nineteenth century statistics and the taming of data -- and machines -- in statistical practices of machine learning today. \index{machine learning!statistical practices}

# Data reduces uncertainty? 

The broadest claim associated with machine learning hinges on  the simple expression shown:

\begin {equation}
\label {eq:n_all}
N = \forall\boldsymbol{X}
\end {equation}

In Equation \ref{eq:n_all}, $N$ refers to the number of observations (and hence the size of the dataset), the logical operator  $\forall$ means 'all' since this is the level of inclusion which many fields of knowledge in science, government, media, commerce and industry envisage, and $\boldsymbol{X}$ refers to the data itself arrayed in vector space. Note that this expression leaves some things out. $Y$, the response variable, for instance, may or may not be known or part of the data $\boldsymbol{X}$. \index{data!all of}   While both the expansion of data in the vector space and the machine learners that transform and observe  it have appeared in previous chapters, I focus here  on changes in probability practices associated with machine learning, and in particular, $N = \forall\boldsymbol{X}$, the claim that with all the data, the production of knowledge fundamentally changes. \index{'Big Data'|see {data!all of}}

The claim that with $N=\forall\boldsymbol{X}$ the nature of knowledge changes has been widely discussed.[^4.2]  Viktor Mayer-Schönberger and Kenneth Cukier's _Big Data: A Revolution That Will Transform How We Live, Work and Think_  present this shift in many different settings in the course of the vignettes and teeming comparisons that have become typical of the data revolution genre. In a chapter entitled 'More,' they sketch the transition from data practices reliant on sampling to data practices that deal with all the data:\index{data!sampling!limits of} 

>Using all the data makes it possible to spot connections and details that are otherwise cloaked in the vastness of the information. For instance, the detection of credit card fraud works by looking for anomalies, and the best way to find them is to crunch all the data rather than a sample [@Mayer-Schonberger_2013, 2013,  27]

In the several hundred pages that follow in _Big Data_, the problem of how to 'crunch all the data' is not a major topic. Machine learning remains almost completely invisible as a practice of transforming data in the name of knowledge. \index{Mayer-Schőnberger, Viktor} \index{Cukier, Kenneth} While they mention the role of social network theory (30), 'sophisticated computational analysis' (55),  'predictive analytics' (58) and 'correlations' (7),  and they observe that 'the revolution' is 'about applying math to huge quantities of data in order to infer probabilities' (12), any further consideration of a change in data practices is largely confined to a business-oriented contrast between having some of the data and having all the data (that is, businesses often have all the data on their customers).

Without a sense of how statistical practices animate and configure key features of 'crunching the data' to make predictions, it becomes hard to see how the 'revolution' takes place. Just as nineteenth century statistics transformed many measurements  into population attributes (for instance, mean as the ideal or abstract property of a population), the shift between $n$ and $\forall\boldsymbol{X}$, between some and all, \index{statistics!mean} a shift very much dependent on  machine learning, internalizes, I will suggest, population attributes into the operations of machine learners.\index{machine learner!population of}  This is a statistical event akin to the advent of the Normal distribution (and indeed, $\mathnormal{N}$ is a standard symbol for the Normal distribution in statistics textbooks) as a way of knowing and controlling populations [@Hacking_1975, 108]. To signal its continuity with the invention of probability, I term it 'probabilisation,' a pleonasm that refers that facet of the operational formation that renders knowledge in terms of probabilities. \index{probabilisation}  

# Machine learning as statistics inside out

The argument mimics Hacking's. In   _The Taming of Chance_, Hacking argues that modern statistical thought transposed a way of calculating errors in experimental measurements and astronomical observations into the real and constitute attributes of populations understood as processes of reproductive growth.  \index{statistics!history of!from error to real quantity} This transposition or inversion relied on four intermediate steps passing through the development of a probability calculus (particularly the work of Jacob Bernoulli and the binomial or heads-tails probability distribution in the 1690s [@Hacking_1975, 143]), the accumulation of large numbers of measurements (the most famous being the chest measurements of soldiers in Scottish regiments, but these were only one flurry amidst an avalanche of numbers in the 1830-1840s), the emergence of the idea of multiple, minute independent causes producing events (particularly as developed in medicine but also in studies of crime), and the 'law of errors' applying to measurements made by, amongst others, astronomers [@Hacking_1990, 111-112].\index{dataset!Scottish chest measurements}  \index{statistics!probability distributions!normal} \index{statistics!measurements in}  As Hacking observes, coins, suicides, crime, chest measurements, and astronomical observations all pile up in a statistical aggregate which remains, although somewhat altered, indelible in contemporary statistical knowledges, particularly in its frequent recourse to notions of population, probability and distribution. In this entanglement, observers and the observed changed places. The distribution of errors made by astronomers measuring the position of stars or planets became a distribution or variation inherent in a population. \index{population}

Machine learning reverse-engineers the invention of modern statistical thinking.  It takes back the 'real quantities' -- probabilities --  that modern statistics had attributed to the populations in the world and attributes them to devices, to machine learners that people then observe, monitor and indeed measure again in many ways. The direct swapping between uncertainty in measurement and variation in real attributes that statistics achieved now finds itself re-routed and intensified as machine learners measure the errors, the bias and the variance of devices. 

The swapping or re-distribution is not a simple mirror-image reversal, as if machine learners mistake devices for a population. Machine learning constantly takes statistical thinking as a basic condition for its operations and devices. When _Elements of Statistical Learning_  states that  (as we saw in the previous chapter) 'our goal is to find a useful approximation $\hat(f)(x)$ to the function $f(x)$ that underlies the predictive relationship between input and output' [@Hastie_2009, 28], they invoke the 'real quantities' first elaborated and articulated by proto-statisticians such as Quetelet grappling with population and sample parameters.  The major structuring operational practices in machine learning as a field of knowledge-practice show the marks of increasingly strong commitment to the reality of the statistical, and to the ongoing probabilisation of machine learners. \index{probabilisation!as statistical practice} 

What is probabilisation in practice? Reading and working with machine learning techniques usually means encountering and responding to apparatus drawn from statistics, but the apparatus is not typically the statistical tests of significance or variation. In contrast to a statistics textbook such as the widely used _Basic Practice of Statistics_ [@Moore_2009] or a more advanced guide such as _All of Statistics_ [@Wasserman_2003], where statistical tests (t-test, chi-squared test, etc.),  hypothesis testing, and analysis of uncertainties (confidence intervals, etc.) order the exposition, \index{statistics!textbok}  machine learning textbooks rely on  a conceptual apparatus curiously stripped of statistical tests and measurements.  Statistical underpinnings may be fundamental, but this does not mean that  machine learners simply automate statistics.

\begin{table}
\centering
\begin{tabular}{|l|l|}
parametric &  non-parametric \\
bias &        variance \\
prediction &  inference \\
generative &  discriminative \\
\end{tabular}
\caption{Some structuring differences in machine learning}
\label{tab:ml_diffs}
\end{table}

Instead,  a basic set of contrasts or indeed oppositions that owe much to probabilistic thinking order, compose, associate and link the statements of machine learners. \index{machine learning!structure differences} The contrasts shown in Table \ref{tab:ml_diffs} all have a statistical facet and anchoring  to them. Some refer to errors that affect how a machine learner refers to data (bias and variance; see discussion below); some designate an underlying statistical intuition about how particular machine learners treat data (does the model seek to generate the data or classify -- discriminate -- it; e.g. Naive Bayes or Latent Dirichlet Allocation are \gls{generative} models whereas logistic regression or support vector machines are _discriminative_); \index{model!generative} \index{model!discriminative}  parametric and non-parametric describe the role of probability distributions in the model; \index{model!parametric and non-parametric}  and others indicate different kinds of statistical knowledge practice (prediction seeks to anticipate while inference seeks to interpret, etc.; also see discussion below). \index{machine learning!structuring differences} These broad structuring differences reach down deeply into the architecture, the diagrams, the practices, statements and visual objects and computer code associated with $N=\forall\boldsymbol{X}$. Because they  anchor basic operations of  machine learning in probability, formalisms derived from statistics have in the last two decades increasingly populated the field, furnishing and rearranging  its diagrammatic references \index{diagram!reference}  to the worlds of industry, agriculture, earth science, genomics, etc., but also, crucially, triggering ontological mutations in machine learners themselves. \index{machine learning!probabilisation of|seealso{probabilisation}}

# Probabilisation as distribution

While these structuring differences deeply shape practice in machine learning, the underlying operator that allows swapping between knowledge and the world, between events and devices, is probability, and in particular, functions that describe variations in populations, probability distributions. \index{function!probability distributions} Probability distributions both map population variations and, as we will see, multiply the number of things that count as populations. \index{population!as probability distribution} \index{probabilisation!in probability distributions}

The normal distribution pervades nineteenth century statistical thinking as it affects populations  across law, medicine, agriculture, finance and not least, sociology as a domain of knowledge. Normal  distributions appear in countless variations in scientific, government and institutional settings as functions that map events, measurements, observations and records to evidential probability quantities.[^4.205]


\begin {equation}
\label {eq:gaussian_distribution}
f(x;\mu, \sigma^2) = \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}
\end {equation}


The  function shown in equation (\ref{eq:gaussian_distribution}) expresses the probability of a given value of the variable $x$ given a population whose variations (with respect to $x$) can be expressed in terms of two parameters, $\mu$ and $\sigma$, the mean and variance. This  is the so-called normal or Gaussian distribution.[^4.3341] Its  mathematics were intensively worked over during the late eighteenth and early nineteenth centuries in what has been termed 'one of the major success stories in the history of science' [@Stigler_1986, 158]. It has a power-laden biopolitical history closely tied with knowledges and governing of  populations in terms of morality, mortality,  health, and wealth (see [@Hacking_1975, 113-124]. \index{population} The key parameters  here include $\mu$, the mean and $\sigma$, the variance, a number that describes the dispersion of values of the variable, $x$ are. These two parameters together describe the shape of the curve. Given knowledge of $\mu$ and $\sigma$, the normal or Gaussian probability distribution maps all outcomes to probabilities (or numbers in the range $0$ to $1$). Put statistically, functions such as the Gaussian distribution probabilise events as random variables.  Every variable potentially becomes a function: 'a random variable is a mapping that assigns a real number to each outcome' [@Wasserman_2003,19]. \index{random variable|seealso{function!probability distribution}} \index{parameters!of a probability distribution}
 
[^4.3341]: Dozens of differently shaped  probability distributions map continuous and discrete variations to real numbers. Other probability distributions — normal (Gaussian), uniform, Cauchy exponential, gamma, beta,  hypergeometric, binomial, Poisson, chi-squared, Dirichlet, Boltzmann-Gibbs distributions, etc. (see [@NIST_2012] for a gallery of distributions) — functionally express widely differing patterns. \index{function!probability distribution!variety of} The  queuing times at airport check-ins do not, for instance, easily fit a normal distribution. Statisticians model queues using a Poisson distribution, in which, unfortunately for travellers, distributes the number of events in a given time interval quite broadly.  Similarly, it might be better to think of the probability of rain today in north-west England in terms of a Poisson distribution that models clouds in the Atlantic queuing to rain on the northwest coast of England. (Rather than addressing the question of whether it will rain or not, a Poisson-based model might address the question of how many times it will rain today.)

The possibility of treating population variations as random variables, that is, as probability distributions, was a significant historical achievement,  one that continues to develop and ramify.[^4.114] Random variables distribute probability in the world. When conceptualised as real quantities in the world rather than epiphenomenal by-products of inaccuracies in our observations or measuring devices, probability distributions weave directly into the productive operations of power. \index{probabilisation!as distribution}  Distribution in the sense of locating, positioning, partitioning, sectioning, serialising or queuing operations  has received much more attention in critical thought (particularly in the many uses of Foucault's concept of disciplinary power [@Foucault_1977]),  but in almost every setting, distribution in the sense of counting, apportioning and weighting of different outcomes also operates. This constant interweaving of spatial, architectural, logistical and functional processes has energised statistical thought for several centuries.[^4.115] \index{distribution|see{function!probability distribution}} For instance, given the normal distribution, it is possible, under certain circumstances, to effectively subjectify someone on the spot. If an educational psychologist indicates to someone that their intelligence lies towards the left-hand side of the normal curve peak (and hence less than the population mean), they quickly assign them to a potentially institutionally and economically consequential trajectory. Since its inception in the social physics of Adolphe Quetelet as a way of referring to a property of populations, the normal curve has not only described but modulated and re-shaped populations (in terms of health, morality and wealth). \index{statistics!history!Quetelet, Adolphe}

If functions such as equation (\ref{eq:gaussian_distribution}) have persisted for so long as elements of population governmentality or biopolitics, \index{biopolitics!populations in}  what happen to them in machine learning? The pages of a book such as _Elements of Statistical Learning_ show many signs of an ongoing invocation of probability distributions. We could simply observe their abundance. Hastie and co-authors invoke probability distributions. They speak of 'Gaussian mixtures,' 'bivariate Gaussian distributions,' standard Gaussian,' 'Gaussian kernels,' 'Gaussian assumptions,' 'Gaussian errors,' 'Gaussian noise,' 'Gaussian radial basis function,' 'Gaussian variables,' 'Gaussian densities,' 'Gaussian process,' and so forth. (The term 'normal' appears in an even wider spectrum of similar guises.) Events, things, properties, operations, functions, and  attributes all associate with  probability distributions.  \index{function!probability distribution!Gaussian}

The multiple invocations of probability distributions attests to the variety of events (occurrence of cancer, occurrence of the word 'Viagra' in an email, a click on a hyperlink, etc.)  map to real numbers. Despite  the sometimes dense mathematical diagrammatics, the term *distribution* emphasises a tangible and practically resonant way of thinking about how events or possible outcomes shift about as the parameters of a function vary.[^4.667] Whatever inferences and predictions become possible, probability distributions are a crucial control surface for machine learning understood as a form of movement through data.  \index{machine learner!probability distribution as control surface} In contrast to the endowment of living aggregates such as populations with probability that we see in the biopolitical history of statistics (and later in natural sciences such as physics and biology), statistical machine learning increasingly constitutes devices as populations via probability distributions. \index{probabilisation}

[^4.667]:  Machine learners adjust these parameters in different ways. For instance, parametric and non-parametric models (see table \ref{tab:ml_diffs}) differ  in that the former have a limited number of parameters and the latter an undefined number of parameters (for instance, Naive Bayes, _k_ nearest neighbours or support vector machine models). But both kinds assume that an underlying probability distribution -- a function, ‘unobservable’ or not -- operates, even if it changes with new data. A probability distribution under these assumptions becomes the closest reality we have to whatever process generated  all the variations in data gathered through experiments and observations. From a probabilistic perspective, the task of machine learning is to estimate the parameters (the mean $\mu$ and variance $\sigma$ in the case of Gaussian curve) that shape of the curve of the probability distribution. 

# Naive Bayes and the distribution of probabilities

How could machine learners become a population?  The mathematical expression for one of the most popular of all machine learning classifiers, the Naive Bayes classifier, stands out for its probabilistic simplicity and seeming lack of 'moving parts'. \index{machine learner!Naive Bayes}

\begin {equation}
\label {eq:naive_bayes}
f_j(X) = \prod_{k=1}^{p}f_{jk}(X_k)
\end {equation}

>[@Hastie_2009, 211]\index{machine learner!Naive Bayes}

Some machine learners are so simple that they can be implemented in a few lines of code.  Along with the perceptron, linear regression, and _k_ nearest neighbours, the function shown in equation (\ref{eq:naive_bayes}) is one of the simplest one to be found in most machine textbooks yet easily adapts for high dimensional data, the kind of data associated with contemporary network infrastructures, scientific instruments,  online communications and $N = \forall\boldsymbol{X}$ in general.[^4.116]  Even though the Naive Bayes classifier is one of the most popular machine learning algorithms, it is more than 50 years old [@Hand_2001].

The key diagrammatic elements of the classifier in the equation are $\prod$, an operator that multiplies all the values of the matrix of $X$ values (from $1$ to $p$) to generate a product. What product does the Naive Bayes classifier produce? The expression $f_j(X)$ refers to a probability density; that is, it describes the probability that a particular thing (a document, an image, an email message, a set of URLs, etc.) belongs to the class of things $j$. \index{probabilisation!probability density} In constructing an estimate of the probability that a given message, image or event is an instance of class $j$, $p$ different features are taken into account. ( The subscript $k$ indexes the $p$ dimensions of the vector space.) The subscripts $k=1$ on the $\prod$ operator, and $k$ on the data $X_k$ indicate that the Naive Bayes classifier makes use of a series of features or variables  in calculating the overall probability that a given thing or observation belongs to a specific class. Put in the language of probability calculus,  the classifier produces a probability density  $f_j(X)$ by calculating the _joint probability_ of all the _conditional_ probabilities of the features or predictor variables in $X$ for the class $j$. As  _Elements of Statistical Learning_ rather tersely puts it, 'each of the class densities are products of the marginal densities' [@Hastie_2009,108]. 

The  Naive Bayes classifier directly invokes probability (including its name, with its reference to the Bayes Theorem, an important late eighteenth century concept), yet there is little obvious connection to statistics in its modern form of tests of significance. \index{statistics!Bayes Theorem} As Drew Conway and John Myles-White write in _Machine Learning for Hackers_, 

>At its core, [Naive Bayes] ... is a 20th century application of the 18th century concept of _conditional probability_. A conditional probability is the likelihood of observing some thing given some other thing we already know about [@Conway_2012, 77] \index{Conway, Drew!on Naive Bayes}  \index{Myles-White, John!on Naive Bayes}

They point to the application of 'conditional probability,' a probability conditioned on the probability of something else. Conditional probability lies at the heart of many of the data transformation associated with prediction or pattern recognition since it links a class to the occurrence of combinations of variables or features.   Naive Bayes links variables by simply multiplying probabilities.[^4.117] \index{probability!conditional} As any of the many accounts of the technique will explain, the name comes from Bayes Theorem, one of the most basic yet widely used results in probability theory (again dating from the eighteenth century), yet Naive Bayes does not even fully embrace Bayes Theorem as the principle of its operation. The classifier has a simple architecture based on the concepts of conditional probability and joint probability; it calculates a probability density function $f_j(X)$ or probability distribution for each possible class of things as a combination of the probabilities of all the many features or attributes of populations that come together in data. It makes a drastically naïve assumption that features or variables are statistically independent of each other, where 'independent' means that they do not affect each other, or that they have no relation to each other. We will see below that dramatic simplifications such as independence do not necessarily weaken the referential grasp of machine learners on the world, but in certain ways allow them to reconfigure the operations of machine learners as a population of learners.

# Spam: when $\forall{N}$ is too much?

$\forall{N}$ can be a bother.\index{data!all of!as a problem}  In _Doing Data Science_, Rachel Schutt and Cathy O'Neill furnish a `bash` script (that is, command line instructions) to download the well-known`Enron` email dataset \index{dataset!\texttt{Enron}} and build a Naive Bayes classifier that labels email as spam or not. In many ways, this is canonical machine learner pedagogy.  For Naive Bayes, email spam detection has become the standard example (Andrew Ng uses it in CSS 229, Lecture 5 [@Ng_2008]).\index{\textit{Doing Data Science}} \index{Ng, Andrew!on spam email}  In this setting, machine learners operate as filters coping with too much communication. \index{communication!too much} 

A typical spam email in the `Enron` dataset, a dataset that derives from the U.S Federal Energy Regulatory Commission's investigation into Enron Corporation [@Klimt_2004],  looks like this:

> Subject: it's cheating, but it works !
can you guess how old she is ? the woman in this photograph looks like a happy teenager about to go to her high school prom, doesn' t she ? she' s an international, professional model whose photographs have appeared in hundreds of ads and articles whenever a client needs a photo of an attractive, teenage girl.but guess what ? this model is not a teenager ! no, she is old enough to have a 7-year-old daughter.. she also says, " if it weren't for this amazing new cosmetic cream called ' deception, ' i would lose hundreds of modeling assignments...because...there is no way i could pass myself off as a teenager." 
service dept
9420 reseda blvd # 133
northridge, ca 91324

The text of a typical non-spam email like this:

> Subject: industrials suggestions......
----------------------forwarded by kenneth seaman / hou / ect on 01 / 04 / 2000
12 : 47 pm-------------------------- -
pat clynes @ enron
01 / 04 / 2000 12 : 46 pm
to : kenneth seaman / hou / ect @ ect, robert e lloyd / hou / ect @ ect
cc :
subject : industrials
ken and robert,
the industrials should be completely transitioned to robert as of january 1, 2000.please let me know if this is not complete and what else is left to transition .
thanks, pat

Such communications, with their mixture of solicitation and imperative  are familiar to  anyone who uses email. How does Naive Bayes probablise their differences? \index{differences!probabilisation of} How do they become $X$ or even $f_j(X)$ in the Naive Bayes classifier? The code that _Doing Data Science_ supplies is instructive:

```{r enron_nb_bash_pre, engine='bash', echo=FALSE}

    if [ $# -eq 1 ]
    then
        word=$1
    else
        echo "usage: enron_naive_bayes.sh <word>"
        exit
    fi

     ### PART 1
    if ! [ -e enron1.tar.gz ]
    then
        wget 'http://www.aueb.gr/users/ion/data/enron-spam/preprocessed/enron1.tar.gz'
    fi

    if ! [ -d enron1 ]
    then
        tar zxvf enron1.tar.gz
    fi

    cd enron1
```


\begin{lstlisting}[language=bash, label={lst:nb_spam}]
    #!/bin/bash
    # description: trains a simple one-word naive bayes spam
    # filter using enron email data
    # usage: ./enron_naive_bayes.sh <word>
    # author: jake hofman (gmail: jhofman)

    ### PART 1
    Nspam=`ls -l spam/*.txt | wc -l`
    Nham=`ls -l ham/*.txt | wc -l`
    Ntot=$Nspam+$Nham
    echo $Nspam spam examples
    echo $Nham ham examples

    Nword_spam=`grep -il $word spam/*.txt | wc -l`
    Nword_ham=`grep -il $word ham/*.txt | wc -l`
    echo $Nword_spam "spam examples containing $word"
    echo $Nword_ham "ham examples containing $word"

    ### PART 2
    Pspam=`echo "scale=4; $Nspam / ($Nspam+$Nham)" | bc`
    Pham=`echo "scale=4; 1-$Pspam" | bc`
    echo
    echo "estimated P(spam) =" $Pspam
    echo "estimated P(ham) =" $Pham
    Pword_spam=`echo "scale=4; $Nword_spam / $Nspam" | bc`
    Pword_ham=`echo "scale=4; $Nword_ham / $Nham" | bc`
    echo "estimated P($word|spam) =" $Pword_spam
    echo "estimated P($word|ham) =" $Pword_ham

    ### PART 3
    Pspam_word=`echo "scale=4; $Pword_spam*$Pspam" | bc`
    Pham_word=`echo "scale=4; $Pword_ham*$Pham" | bc`
    Pword=`echo "scale=4; $Pspam_word+$Pham_word" | bc`
    Pspam_word=`echo "scale=4; $Pspam_word / $Pword" | bc`
    echo
    echo "P(spam|$word) =" $Pspam_word
    cd ..
\end{lstlisting}

[@Schutt_2013, 105-106]

```{r enron_nb_r, echo = FALSE}

        f_ham = list.files('enron1/ham', full.names = TRUE)
        f_spam = list.files('enron1/spam', full.names = TRUE)
        ham_count = length(f_ham) 
        spam_count = length(f_spam)

        ham = sapply(f_ham, readLines, warn=FALSE)
        spam = sapply(f_spam, readLines, warn=FALSE)
        P_ham = ham_count/(ham_count + spam_count)
        P_spam = spam_count/(ham_count + spam_count)

        predict_spam <- function(word) {
            ## count the occurrence of the word in each
            Nword_in_ham = sum(grepl(word, ham))
            Nword_in_spam = sum(grepl(word, spam))

            Pword_spam = Nword_in_spam/spam_count
            Pword_ham = Nword_in_ham/ham_count

            Pham_word = Pword_ham * P_ham
            Pspam_word = Pword_spam * P_spam
            Pword = Pspam_word + Pham_word
            Pspam_word = Pspam_word/Pword

            return(Pspam_word)
        }
```


The script draws out something of how the joint probability function in equation (\ref{eq:naive_bayes}) probabilises a single word.[^4.7] \index{diagram!world} Not all machine learning models are so simple that they can be conveyed in 30 lines of code (including downloading the data and comments), \index{code!brevity} but the script signals that nothing that occurring in probabilisation \index{probabilisation} is intrinsically mysterious, elusive or indeed particularly abstract.[^4.777]  On the contrary, the power of classifiers operates through the accumulated counting, adding, multiplying (that is, repeated adding) and dividing (that is, multiplying by parts or fractions) constrained by the joint probability distribution. Probability re-distributes things such as emails or documents as, in this case, events in a population of words. The Naive Bayes classifies endows every word in the `Enron` dataset with a probability density function. The classification of each email becomes a matter of estimating a conditional probability based on the joint probability distribution that quantifies the chance of all the words in that email appearing together.  Probabilities are always between `0` and `1`,  and classification entails selected a cutoff or dividing line. For instance, greater than `0.5` might result in a classification as `spam`.   In the `enron` dataset, 'finance' has a  `r round(predict_spam('finance'),2)` chance of being spam, while 'sexy' has a chance of `r round(predict_spam('sexy'), 2)`. Ironically, like the Naive Bayes classifier's own reliance on seventeenth and eighteenth century probability calculus, the frequent application of this machine learner to document classification and retrieval echoes the seventeenth century thinking   that first conceived of the very notion of 'probability' in relation to the evidential weight of documents [@Hacking_1975, 85].  \index{probability!history of}

[^4.777]: After fetching the dataset from a website, the code excerpted  in \ref{lst:nb_spam} counts the number of emails in each category `spam` or `ham`, and then counts the number of times that the chosen word (e.g. 'finance' or 'deal') occurs in both the spam and non-spam or ham categories. In Part 2, using these counts the script  estimates probabilities of any email being spam or ham, and then given that email is spam or ham, that the particular word occurs. (To estimate a probability means, in this case, to divide the word count for the chosen word by the count of the number of spam emails, and ditto for the ham emails.)  In Part 3, the final transformation of the data, these probabilities are used to calculate the probability of any one email being spam given the presence of that word. Again, the mathematical operations here are no more complicated than adding, multiplying and dividing. The probability that the chosen word is a spam word is, for instance, the probability of occurrence of the word in a spam email multiplied by the overall probability that an email is spam. Finally, given that the probability of the chosen word occurring in the email dataset is the probability of it occurring in spam plus the probability of it occurring in ham, the overall probability that an email in the Enron data is spam given the presence of that word can be calculated. (It is the probability that the chosen word is a spam word divided by the probability of that word in general.) 

# The improbable success of the Naive Bayes classifier

There is something quite artificial at work in the construction of these populations and their associated probability distributions. \index{probabilisation!construction of populations}  They are intentionally artificial and limited.  They do not correspond or refer directly to what we know, for instance, of how language works, but instead to a rather different set of concerns.  Like most machine learning techniques encountering complex realities, classifiers such as Naive Bayes ignore many obvious structural or semiotic features of emails as documents (for instance, word order, or co-occurrences of words). Yet this very artificiality or limitation in their reference to the world allows machine learners to appear in many different guises. Despite their simple architecture, Naive Bayes classifiers have been surprisingly successful. Many machine learners transform vectorised data into probability distributions populated by fields of random variables in process of change. They render all things as populations.  

```{r nb_applications, echo=FALSE, results='asis'}
    q = "select * from basic_refs where TI like '%Naive Bayes%'"
    res = dbGetQuery(con, q)
    nb = res[order(res$TC, decreasing=TRUE)[1:20],]
    nb2 = unique(nb[, c('PY', 'TI')])
    colnames(nb2) = c('Year', 'Title')
    tab = xtable(nb2[order(nb2$Year),],  caption = 'Most cited Naive Bayes publications 1945-2015', align = c("p{0.05\\textwidth}", "p{0.10\\textwidth}",  "p{0.85\\textwidth}"), label='tab:nb_most_cited')
    print(tab, type='latex', row.names=FALSE)
```

The altered relation between modern statistical and machine learning practice starts to appear in Naive Bayes from the early 1990's as statisticians begins to generalize and re-diagram Naive Bayes by examining its statistical properties more carefully. Table \ref{tab:nb_most_cited} shows 30 of the most cited Naive Bayes-related scientific publications.[^4.118]  The list of titles sketches a double movement. On the one hand, we see the typical diagonal forms of accumulation or positivity \index{positivity} of a machine learner across disciplines -- computer science, statistics, molecular biology (especially of cancer), software engineering, internet portal construction, sentiment classification, and image 'keypoint' recognition. On the other hand, highly cited papers such as [@Friedman_1997] and [@Hand_2001] point to an intensified statistical treatment of machine learners during these years, an intensified probabilisation of machine learners that strongly affects their ongoing development (leading, for instance, to the much more document-oriented, heavily probabilistic topic models appearing in the following decade [@Blei_2003]).  \index{machine learner!topic model}

In _Elements of Statistical Learning_, Hastie, Tibshirani and Friedman characterise the Naive Bayes classifier in terms of its capacity to deal with high dimensional data:    

>It is especially appropriate when the dimension $p$ of the feature space is high, making density estimation unattractive. The naive Bayes model assumes that given a class $G = j$, the features $X_k$ are independent [@Hastie_2009, 211]. \index{machine learner!Naive Bayes!success of}

Similar formulations can  be found in most of the machine learning books and instructional materials. This appropriateness relates directly to $\forall{\boldsymbol{X}}$, and the expansion of the vector space. As we saw above in equation (\ref{eq:naive_bayes}), $p$ stands for the number of different dimensions or variables in the data set. In the spam classifier, the number of dimensions balloon hundreds of thousands  because every unique word adds a new dimension to the vector space. Compared to the complications of logistic regression, neural networks or support vector machines, \ref{eq:naive_bayes}  seems incredibly simple. How is it that a simple multiplication of probabilities and the assumption that 'features ... are independent' can, as Hastie and co-authors write: 'often outperform far more sophisticated alternatives' [@Hastie_2009, 211]?

The answer to this conundrum of success does not lie in the increasing availability of data to train machine learners on. I want to explore two other contrasts as ways of viewing the probabilising processes at work in Naive Bayes. The first way to view this success is in terms of _ancestral communities_ of probabilisation. The second concerns the statistical decomposition of machine learners in terms of their sources of error. \index{probabilisation!ancestral communities of} \index{machine learner!statistical decomposition of} \index{probabilisation!errors in}

# Ancestral probabilities in documents: inference and prediction

Why is the Naive Bayes classifier is almost always demonstrated on the problem of filtering spam email [@Conway_2012; @Schutt_2013, 93-113; @Kirk_2014, 53; @Lantz_2013, 92-93; @Flach_2012; @Ng_2008b], and in particular dealing with the abundance of spam emails mentioning a drug for erectile dysfunction sold under the tradename 'Viagra' (a drug that was itself the  byproduct of the clinical trial for hypertension and heart disease)? \index{machine learner!Naive Bayes!spam}  What are we to make of this regularity in production of statements?  Admittedly spam, and spam trying to sell Viagra in particular, has been a very familiar part of most email since 1997 when Viagra was approved for sale, and of all the documents that machine learners mundanely encounter in quantity in those years, email might be the most numerous as well as one of the mundanely shared.  Naive Bayes classifiers and variations of them also became practical devices in  managing email traffic for most people, whether they know it or not, during the mid-1990s (see for instance, [SpamAssassin](http://spamassassin.apache.org/).(The other would be scientific publications. Many more recent machine learners train as classifiers on scientific publications [@Blei_2007] \index{science!publications!classification of}) 

From an archaeological standpoint, the  reiteration of email spam filtering using Naive Bayes is the effect of another process, a process akin to the attribution of probability distributions to populations in the nineteenth century. Like many machine learners, Naive Bayes has one important lineage derived from the problem of classifying and retrieving documents amidst archives. The operational practice of document classification is specified in the element of the archive. \index{data!archives of} Genealogical affiliation with a particular problem such as document classification (or image recognition) generates many re-iterations and versions of machine learners over time.  As Lucy Suchman and Randall Trigg wrote in their study of work on artificial intelligence, 

>rather than beginning with documented instances of situated inference ... researchers begin with ... postulates and problems handed down by the ancestral communities of computer science, systems engineering, philosophical logic, and the like [@Suchman_1992,174]. \index{artificial intelligence!ancestral communities in} 

While Bayes Theorem dates from the 18th century, the highly successive use of Naive Bayes classifiers in email spam filtering in recent decades effectively draws on an ancestral community of document classification and information retrieval methods reaching back to the mid-20th century.[^4.200]  \index{Suchman, Lucy!on ancestral communities}  

Early attempts to use what is now called Naive Bayes in the early 1960s re-iterated engagements with the evidential weight of documents that accompanied the emergence of probabilistic thinking  as a quantification of belief in the seventeenth century [@Hacking_1975, 35-49]. \index{probability!emergence of}  Working at the RAND Corporation in the early 1960s, M.E. Maron described how 'automatic indexing' of documents -- Maron used papers published in computer engineering journals -- could become 'probabilistic automatic indexing.'  The necessary statistical assumption was:

>The fundamental thesis says, in effect, that statistics on kind, frequency, location, order, etc., of selected words are adequate to make reasonably good predictions about the subject matter of documents containing those words [@Maron_1961, 406] \index{Maron, M.E.}

This thesis has remained somewhat fundamental in text classification and information retrieval applications, as well as many other machine learning approaches since. Maron's work focused on a collection of several hundred abstracts of papers published in the March and June 1959 issues of the _IRE Transactions on Electronic Computers_. As in contemporary supervised learning, these abstracts were divided into two groups, a training and a test set ('group 1' and 'group 2' in Maron's terminology [@Maron_1961, 407]), and the training set was classified according to 32 different categories that had already been in use by the Professional Group on Electronic Computers, the publishers of the _IRE Transactions_. Given these classifications, word counts for all distinct words in the abstracts were made, the most common terms ('the', 'is', 'of', 'machine', 'data', 'computer') and the most uncommon words removed, and the remaining set of around 1000 words were actually used for classification.

This treatment of the abstracts as documents, then as lists of words, and then as frequencies of terms, and finally as a filtered list of most information rich terms continues in much document and text  classification work today. \index{dataset!engineering paper abstracts}  A typical contemporary information retrieval textbook such as [@Manning_2008] devotes a chapter to the topic, including the canonical discussion of how simplifying  assumptions about language and meaning do not vitiate the Naive Bayes classifier. Whenever machine learners announce the unlikely efficacy of classifiers, we might attend to the ways in which previous 'ancestral probabilisations' and archival constitution of the domain in question prepare the ground for that success. \index{machine learner!Naive Bayes!history of} \index{probabilisation!ancestral communities of}


# Statistical decompositions: bias, variance and observed errors

Even with an eye on the ancestral communities that constantly accompany and heavily shape the indexical diagram of machine learning in the world, we still need a way of accounting for the artificiality of Naive Bayes. \index{diagram!indexical} The classifiers generates highly arbitrary probabilities of document class membership, yet these arbitrary probabilities still allow effective classification. Machine learners view the persistence of manifest artifice (in the case of Naive Bayes, a model that eschews any modelling of relations between things in the word such as words) in terms of another of the structuring differences of machine learning: the so-called _bias-variance_decomposition_[@Hastie_2009,24]. \index{error!bias-variance|(}

The terms 'bias' and 'variance' stem from the long history of statistical interest in errors (as Hacking's account of the transposition of measurement errors into population norms illustrates). \index{statistics!errors} The \gls{bias} and \gls{variance} of 'estimators' -- the estimates of the parameters of the models usually written as $\hat{\beta}$ or $\hat{\theta}$-- feature heavily in machine learning discussions of prediction errors. The terms point to tensions that all machine learners experience. On the one hand, _variance_ refers to the inevitable reliance of a machine learner on the data it 'learns.' To put it more formally, 'variance refers to the amount by which $\hat{f}$ would change if we estimated it using a different training data set' [@James_2013,34]. On the other hand, _bias_ 'refers to the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model' [@James_2013, 35].

These two sources of error, one which results from sampling and the other arising from the structure of the model or approximating function, can be reduced or at least subject to trade-off in what _Elements of Statistical Learning_ terms 'the bias-variance decomposition' [@Hastie_2009, 223].[^4.202] From the standpoint of the bias-variance decomposition, every  machine learner makes a trade-off between the errors deriving from differences between samples, and errors due to the difference between the approximating function and the actual process that generated the data. Note that both sources of error in the bias-variance decomposition derive from  transformations of the  data. Variance affects how the model encounters the world (as a set of small samples or as, at the other end, a massive $N=\forall{\boldsymbol{X}}$ dataset). Bias relating to how the model 'apprehends'  the data (as a set of almost coin-toss like independent events, as a geometrical problem of finding a line or curve that runs through a cloud of points, etc.).


```{r bias_variance, echo=FALSE}
q = 'select PY, TI, DE, TC, AU from basic_refs where DE like "%bias%" or DE like "%variance%"'
res = dbGetQuery(con, q)
bv = res[order(res[, 'TC'], decreasing=TRUE),]
```

Even with all the data, machine learning cannot fully circumvent the tensions between the different errors at work in the bias-variance decomposition. \index{data!all of!insufficient}  Yet, sources of error do not always prove harmful. \index{error!value of} The success of Naive Bayes (and _k_ nearest neighbours classifier) runs counter to the long standing trend in statistics to construct increasingly sophisticated models of the domains they encounter. Writing in 1997, Jerome Friedman describes how very simple classifiers perform surprisingly well:

> Certain types of (very high) bias can be canceled by low variance to produce accurate classification [@Friedman_1997,55] \index{Friedman, Jerome!on bias-variance decomposition}

A rather elaborate set of concepts and techniques address the bias-variance decomposition in the context of data availability. These techniques focus on managing the _test_ or _generalization_ error, the difference between the actual and predicted values produced by the  machine learner when it encounters a fresh, hitherto unseen data sample. Machine learners in such settings still encounter the  bias-variance trade-off as they select some data for training and some data for testing.  This trade-off has to deal with the fact that training errors -- the observed difference between what the model predicts and what the training data actually shows -- are not a good guide to test or generalization error.  \index{error!training} \index{error!test|see {error!generalization}}  The process of fitting a model or finding a function (see previous chapter) will tend to reduce the training error by fitting the function more and more closely to the shape of the training data, but when it encounters fresh data that function might no longer fit well. In other words, a more sophisticated function may well reduce the bias but increase the variance. 'Richer collections of models' [@Hastie_2009, 224] reduce bias, but tend to increase variance. Conversely, models that cope well with fresh data (and Naive Bayes is a good example of such a machine learner), display low variance but high bias.

The trade-offs between bias and variance shift  markedly  between different types of models, and generates many different conceptual analyses of error in machine learning literature ('optimism of the training error rate' (228), 'estimates of in-sample prediction error' (230), 'Bayesian information criterion' (233), 'Vapnik-Chervonenkis dimension' (237), 'minimum description length' (235)) and technical methods of estimating prediction error ('cross-validation' (241), 'bootstrap methods'  (249),  'expectation-maximization algorithm' (272), 'bagging' (282), or 'Markov Chain Monte Carlo (MCMC)' (279)), many of which date from the 1970s (e.g. cross-validation [@Stone_1974], bootstrap [@Efron_1979], expectation-maximization [@Dempster_1977]). \index{error!analysis of in machine learning} \index{error!techniques of estimating} 

A daunting field of concepts, themes, techniques and methods all gravitate to the threshold of probabilisation.  \index{probabilisation!threshold of} They invoke in some cases sophisticated mathematical or statistical constructs. They also very often rely on computational iteration or infrastructural scale to optimise parameters in models whose underlying intuitions remain quite  straightforward (as in a linear regression or Naive Bayes). In some cases, the implementation of a model may be very simple, but analysis of how the machine learner manages to curtail a source of error such as bias or variance entails much more sophisticated statistical understanding.  Many analyses  of how a model becomes a 'useful approximation' reconfigure treat the models themselves as members of a population whose variations and uncertainties, whose tendencies and predispositions must be sampled, tested and monitored.\index{population!machine learners} The bias-variance decomposition points to an irreducible friction in the way that machine learning structures differences in the world. \index{error!bias-variance|)}

# Does machine learning construct a new statistical reality?

 \index{differences!errors in} Following a broadly Foucaultean line of argument, Hacking proposes that statistical thinking and practice in the nineteenth and early twentieth century ontologically re-configured things in terms of probability distributions (and the Gaussian distribution in particular).\index{Hacking, Ian!statistics, history of}  What happens in worlds where the statistical treatment of error -- the bias-variance decomposition is a shorthand term for this -- structures an operational formation?  \index{operational formation!statistical composition of} I have suggested that  an ancestral probabilisation of domains and the statistical decomposition of error come together in statistical machine learning.  The bias-variance decomposition includes both tightly bound points  and certainly relatively free or unbound points, as we saw in the case of the Naive Bayes classifier in its encounter with data. It generates highly erroneous probability estimates but performs well as a classifier. 

Viewed diagrammatically,  unbound points  matter greatly to the relations of force at work in a knowledge-power conjunction.  Probabilisation gives machine learning a relation to its own plurality, to the tendencies of its models to proliferate and vary. \index{probabilisation!as relation to machine learning} Every attempt to construct a machine learner  in  a given setting draws on both the re-iteration of ancestral probabilities (that is, prior structuring of settings in conformity with the curve of some probability distribution) or on the many interactive adjustments, re-distributions and re-samplings of the data _and_ transformations of the models associated with the bias-variance decomposition. 

Like many others, Mayer-Schönberger and Cukier's argue that having much data or all data ($N=\forall{\boldsymbol{X}}$) re-bases knowledge. \index{data!all of} Versions of this claim can be found running through various scientific and business settings throughout the 20th century.[^4.61] In certain settings, $N=all$ has been around for quite a while (as for instance, in many document classification settings where the whole archive or corpus of documents have been electronically curated for decades). Mayer-Schönberger and Cukier rightly emphasize that the huge quantities of data sluicing through some contemporary infrastructures support wider inferences (11). Their discounting of statistical sampling as a concept 'developed to solve a particular problem at a particular moment in time under specific technological constraints' [@Mayer-Schonberger_2013, 31] does not, however, accommodate the operational practices of sampling that pervade  machine learning, particularly in the forms of probablisation.  

Whether or not someone uses Naive Bayes, a topic model, neural networks or logistic regression, does not greatly alter the processes of probabilisation.  Random variables,  probability distributions, errors and model selection practices crowd in around  and re-configure machine learners as members of a population generating statements. \index{population!of machine learners} In many ways, the Mayer-Schönberger and Cukier account bobs in the wake of the enterprise-wide accumulations of data. They pay so much attention to the capital potentials of data accumulation that they cannot easily attend to the question of how machine learners probabilise that data.  Sampling, estimation, likelihoods, and a whole gamut of dynamic relationships between random variables in joint probability distributions reassert themselves amidst a population of models. The data may not be sampled, but models moving  through the high-dimensional vector  spaces opened up by having 'all' the data transform it probabilistically. While not all machine learners are strictly speaking probabilistic models,[^4.60] machine learners relate to themselves and the data as populations defined by probability distributions. 

Machine learning inhabits a reality that had already introjected  statistical realities  at least a century earlier, whether through the social physics of Quetelet, the biopolitical normals of Francis Galton and his regression to the mean (remember that the linear model of regression is probably the basic machine learning model) or later in the probability functions of quantum mechanics in early twentieth century physics. \index{Galton, Francis!regression to mean} \index{Quetelet, Adolphe!social physics} \index{probabilisation!quantum mechanics} Generating a reality aggregating many devices, machine learning inverts statistics, or transcribes it in reverse.  In this inversion, probability distributions, which had become the operational statement and model of truth for  many different kinds of populations, fold back or re-distribute themselves into devices such as machine learners whose variations and uncertainties become populations. Populations of models are sampled, measured, and aggregated in the ongoing production of statistical realities whose object is no longer a property of individual members of a population (their height, their life-expectancy, their chance of HIV/AIDS), but a population of models of populations. 


[^4.2]: Rob Kitchin provides a very useful overview of these claims in [@Kitchin_2014]. While I will not analyse the claims about 'big data' in specific cases in any great detail, the growing literature on this topic suggests that machine learning in its various operations -- epistopic construction of vector space, function finding as association of partial observers and a re-internalisation of probability -- generates considerable difficulties and challenges for knowledge, power and production. \index{Kitchin, Rob!on big data}

[^4.20]: See chapter \ref{ch:vector} on this notion of data dimensionality.

[^4.51]: In 1964, N.J Bailey was taking the same approach to medical diagnosis [@Bailey_1965].  

[^4.61]: Later chapters of this book will track several instances of having all the data in the sciences, in government and in business in order to show what having all the data entails in different settings. \index{data!all of}

[^4.60]:  Machine learning textbooks written by computer scientists tend to define probabilistic models more narrowly.  As Peter Flach suggests:

    >Probabilistic models view learning as a process of reducing uncertainty using data. For instance, a Bayesian classifier models the posterior distribution $P(Y|X)$ (or its counterpart, the likelihood function $P(X|Y)$) which tells me the class distribution $Y$ after observing the features values $X$ [@Flach_2012, 47]

    But whether they are probabilistic in this sense or not, the evaluation and configuring of machine learners irreducibly depends on a statistical treatment of errors and their trade-offs. \index{Flach, Peter} 


[^4.3]: Part of this development has already been related in the previous chapter on 'learning' and function estimation (see chapter \ref{ch:function}). That chapter postponed any real discussion of statistical thought. Instead it explored the various sense of function and function finding that underpin machine learning. But much of that function finding and approximation garners referential weight through the statistical practices and modes of thought that accompany them. 

[^4.4]: Leo Breiman writing in 2001 during the heyday of academic development of machine learning argues, describes the 'two cultures' of statistics: 'in the past fifteen years, the growth in algorithmic modeling applications and methodology has been rapid. It has occurred largely outside statistics in a new community—often called machine learning—that is mostly young computer scientists (Section 7). The advances, particularly over the last five years, have been startling' [@Breiman_2001a, 200].\index{Breiman, Leo!on statistical cultures} Soon afterwards, statisticians such as Hastie, Tibshirani and Friedman publish the first major statistical machine learning textbooks [@Hastie_2001].


[^4.114]: The mapping that assigns numbers to outcomes (heads v. tails; cancer v. benign; spam v. not-spam) is a probability distribution.  As I have argued in [@Mackenzie_2015d], random variables have become much more widespread in statistical practice due to changes in computational techniques. \index{random variable}

[^4.115]: 'Distribution' pervades Foucault's account of power and knowledge from *The Order of Things* [@Foucault_1992] onwards.   \index{Foucault, Michel!on distribution} Foucault treats distributions in several different ways: as spatial or logistical techniques,  as mathematical orderings of large numbers of people or things, and as a methodological and theoretical framing device. In _Discipline and Punish_ [@Foucault_1977], the spatial sense prevails, but in later works, the population or demographic sense of distribution takes precedence [@Foucault_1998]. Distribution certainly has theoretical primacy in his account of power: 'relations of power-knowledge are not static forms of distribution, they are "matrices of transformations"' [@Foucault_1998, 99]. 

[^4.116]: The other contender for simplest machine learner would be the also very popular _k_ nearest neighbours. As Hastie et. al. observe: 'these classifiers are memory-based and require no model to be fit' [@Hastie_2009, 463]. Like the  Naive Bayes classifier, the equation for _k_ nearest neighbours is simple:

    \begin {equation}
    \label {eq:knn}
   \hat{Y}(x) =\frac{1}{k} \sum_{x_i \in \textit{N}_{k}(x)} y_i
    \end {equation}

    where $\textit{N}_{k}(x)$ is the neighborhood of $x$ defined by the $k$ closest points $x_{i}$ in the training sample [@Hastie_2009, 14].
    
    In equation \ref{eq:knn}, a parameter appears: $k$, the number of neighbours.  This contrasts greatly with the linear models discussed in chapters \ref{ch:vector} and \ref{ch:function} where the number of parameters $p$ usually equals the number of variables in the dataset or dimensions in the vector space.  \index{machine learner!\textit{k}-nearest neighbours}

[^4.117]: In [@Mackenzie_2014c], I have suggested that the intensification of multiplication associated with probabilistic calculation may constitute an important mutation in the ontological and practical texture of numbers. The epidemiological modelling of H1N1 influenza in London 2009 involved multiplying a great variety of probability distributions in order to calculate the conditional probability of influenza over time.  

[^4.7]: The input to the script is a single word such as 'finance' or 'deal'. The model is so simple that it only classifies a single word as spam. The `bash` script carries out four different transformations of the data in building the model. It uses only command line tools such as `wc` (word count), `bc` (basic calculator), `grep` (text search using pattern matching) and `echo` (display a line of text). These tools or utilities are readily available in almost any UNIX-based operating system (e.g. Linux, MacOS, etc). The point of using only these utilities is to illustrate the simplicity of the algorithmic implementation of the model.  The first part of the code downloads the sample dataset of Enron emails (and I will discuss spam emails and their role in machine learning below). Note that this dataset has already been divided into two classes - 'spam' and 'ham' -- and emails of each class have been placed in separate directories or folders as individual text files. \index{code!command line} \index{Unix}

[^4.118]: Citation counts, even from the more reliable Reuters-Thomson Web of Science database, are difficult to evaluate when moving between disciplines. Some fields, such as computer science and biology, publish huge numbers of papers compared to smaller disciplines such as astronomy or plant ecology.

[^4.200]: The other lineage descends from medical diagnosis. For instance, starting in 1960, Homer Warner, Alan Toronto and George Veasy, working at the University of Utah and Latter-day Saints Hospital in Salt Lake City, began to develop a probabilistic computer model for diagnosis of heart disease [@Warner_1961; @Warner_1964]. Their model used exactly the same 'equation of conditional probability' we see in equation \ref{eq:naive_bayes} but now used to 'express the logical process used by a clinician in making a diagnosis based on clinical data' [@Warner_1961, 177]. Despite the mention of logic in this description, the diagnostic model was thoroughly probabilistic in the sense that the model itself has no representation of logic included in its workings. Rather it calculates the probability of a given type of heart disease given 'statistical data on the incidence of symptoms' [@Warner_1964, 558]. Somewhat ironically, as they point out, physicians involved in preparing and submitting data to the diagnostic program improved the accuracy in their own diagnoses. In 1964, N.J Bailey was taking the same approach to medical diagnosis [@Bailey_1965].  \index{medical diagnosis} \index{machine learner!Naive Bayes!history of} Heart disease to a central topic in machine learning (see chapter \ref{ch:function} for discussion of the `South African Heart Disease` dataset \index{dataset!South African Heart Disease}). 

[^4.202]: Another source of error, the 'irreducible error' [@Hastie_2009, 37] is noise that no model can eliminate.  


[^4.205]: Statistical graphics have a rich history and semiology that I do not discuss here (see [@Bertin_1983]).
