<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Machine Learning: Archaeology of a Data Practice</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="Machine Learning: Archaeology of a Data Practice">
  <meta name="generator" content="bookdown 0.3 and GitBook 2.6.7">

  <meta property="og:title" content="Machine Learning: Archaeology of a Data Practice" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Machine Learning: Archaeology of a Data Practice" />
  
  
  

<meta name="author" content="Adrian Mackenzie">


<meta name="date" content="2016-12-10">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="61-competitions-as-examination.html">
<link rel="next" href="63-ranked-subject-positions.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="1-acknowledgments.html"><a href="1-acknowledgments.html"><i class="fa fa-check"></i><b>1</b> Acknowledgments</a></li>
<li class="chapter" data-level="2" data-path="2-preface.html"><a href="2-preface.html"><i class="fa fa-check"></i><b>2</b> Preface</a></li>
<li class="chapter" data-level="3" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html"><i class="fa fa-check"></i><b>3</b> Introduction: Into the Data</a><ul>
<li class="chapter" data-level="3.1" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#three-accumulations-settings-data-and-devices"><i class="fa fa-check"></i><b>3.1</b> Three accumulations: settings, data and devices</a></li>
<li class="chapter" data-level="3.2" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#who-or-what-is-a-machine-learner"><i class="fa fa-check"></i><b>3.2</b> Who or what is a machine learner?</a></li>
<li class="chapter" data-level="3.3" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#algorithmic-control-to-the-machine-learners"><i class="fa fa-check"></i><b>3.3</b> Algorithmic control to the machine learners?</a></li>
<li class="chapter" data-level="3.4" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#the-archaeology-of-operations"><i class="fa fa-check"></i><b>3.4</b> The archaeology of operations</a></li>
<li class="chapter" data-level="3.5" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#asymmetries-in-common-knowledge"><i class="fa fa-check"></i><b>3.5</b> Asymmetries in common knowledge</a></li>
<li class="chapter" data-level="3.6" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#what-cannot-be-automated"><i class="fa fa-check"></i><b>3.6</b> What cannot be automated?</a></li>
<li class="chapter" data-level="3.7" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#different-fields-in-machine-learning"><i class="fa fa-check"></i><b>3.7</b> Different fields in machine learning?</a></li>
<li class="chapter" data-level="3.8" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#the-diagram-in-critical-thought"><i class="fa fa-check"></i><b>3.8</b> The diagram in critical thought</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html"><i class="fa fa-check"></i><b>4</b> Diagramming machines}</a><ul>
<li class="chapter" data-level="4.1" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#we-dont-have-to-write-programs"><i class="fa fa-check"></i><b>4.1</b> ‘We don’t have to write programs’?</a></li>
<li class="chapter" data-level="4.2" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#the-elements-of-machine-learning"><i class="fa fa-check"></i><b>4.2</b> The elements of machine learning</a></li>
<li class="chapter" data-level="4.3" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#who-reads-machine-learning-textbooks"><i class="fa fa-check"></i><b>4.3</b> Who reads machine learning textbooks?</a></li>
<li class="chapter" data-level="4.4" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#r-a-matrix-of-transformations"><i class="fa fa-check"></i><b>4.4</b> <code>R</code>: a matrix of transformations</a></li>
<li class="chapter" data-level="4.5" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#the-obdurate-mathematical-glint-of-machine-learning"><i class="fa fa-check"></i><b>4.5</b> The obdurate mathematical glint of machine learning</a></li>
<li class="chapter" data-level="4.6" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#cs229-2007-returning-again-and-again-to-certain-features"><i class="fa fa-check"></i><b>4.6</b> CS229, 2007: returning again and again to certain features</a></li>
<li class="chapter" data-level="4.7" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#the-visible-learning-of-machine-learning"><i class="fa fa-check"></i><b>4.7</b> The visible learning of machine learning</a></li>
<li class="chapter" data-level="4.8" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#the-diagram-of-an-operational-formation"><i class="fa fa-check"></i><b>4.8</b> The diagram of an operational formation</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html"><i class="fa fa-check"></i><b>5</b> Vectorisation and its consequences}</a></li>
<li class="chapter" data-level="6" data-path="6-vector-space-and-geometry.html"><a href="6-vector-space-and-geometry.html"><i class="fa fa-check"></i><b>6</b> Vector space and geometry</a></li>
<li class="chapter" data-level="7" data-path="7-mixing-places.html"><a href="7-mixing-places.html"><i class="fa fa-check"></i><b>7</b> Mixing places</a></li>
<li class="chapter" data-level="8" data-path="8-truth-is-no-longer-in-the-table.html"><a href="8-truth-is-no-longer-in-the-table.html"><i class="fa fa-check"></i><b>8</b> Truth is no longer in the table?</a></li>
<li class="chapter" data-level="9" data-path="9-the-epistopic-fault-line-in-tables.html"><a href="9-the-epistopic-fault-line-in-tables.html"><i class="fa fa-check"></i><b>9</b> The epistopic fault line in tables</a></li>
<li class="chapter" data-level="10" data-path="10-surface-and-depths-the-problem-of-volume-in-data.html"><a href="10-surface-and-depths-the-problem-of-volume-in-data.html"><i class="fa fa-check"></i><b>10</b> Surface and depths: the problem of volume in data</a></li>
<li class="chapter" data-level="11" data-path="11-vector-space-expansion.html"><a href="11-vector-space-expansion.html"><i class="fa fa-check"></i><b>11</b> Vector space expansion</a></li>
<li class="chapter" data-level="12" data-path="12-drawing-lines-in-a-common-space-of-transformation.html"><a href="12-drawing-lines-in-a-common-space-of-transformation.html"><i class="fa fa-check"></i><b>12</b> Drawing lines in a common space of transformation</a></li>
<li class="chapter" data-level="13" data-path="13-implicit-vectorization-in-code-and-infrastructures.html"><a href="13-implicit-vectorization-in-code-and-infrastructures.html"><i class="fa fa-check"></i><b>13</b> Implicit vectorization in code and infrastructures</a></li>
<li class="chapter" data-level="14" data-path="14-lines-traversing-behind-the-light.html"><a href="14-lines-traversing-behind-the-light.html"><i class="fa fa-check"></i><b>14</b> Lines traversing behind the light</a></li>
<li class="chapter" data-level="15" data-path="15-the-vectorised-table.html"><a href="15-the-vectorised-table.html"><i class="fa fa-check"></i><b>15</b> The vectorised table?</a></li>
<li class="chapter" data-level="16" data-path="16-machines-finding-functions.html"><a href="16-machines-finding-functions.html"><i class="fa fa-check"></i><b>16</b> Machines finding functions}</a></li>
<li class="chapter" data-level="17" data-path="17-learning-functions.html"><a href="17-learning-functions.html"><i class="fa fa-check"></i><b>17</b> Learning functions</a></li>
<li class="chapter" data-level="18" data-path="18-supervised-unsupervised-reinforcement-learning-and-functions.html"><a href="18-supervised-unsupervised-reinforcement-learning-and-functions.html"><i class="fa fa-check"></i><b>18</b> Supervised, unsupervised, reinforcement learning and functions</a></li>
<li class="chapter" data-level="19" data-path="19-which-function-operates.html"><a href="19-which-function-operates.html"><i class="fa fa-check"></i><b>19</b> Which function operates?</a></li>
<li class="chapter" data-level="20" data-path="20-what-does-a-function-learn.html"><a href="20-what-does-a-function-learn.html"><i class="fa fa-check"></i><b>20</b> What does a function learn?</a></li>
<li class="chapter" data-level="21" data-path="21-observing-with-curves-the-logistic-function.html"><a href="21-observing-with-curves-the-logistic-function.html"><i class="fa fa-check"></i><b>21</b> Observing with curves: the logistic function</a></li>
<li class="chapter" data-level="22" data-path="22-the-cost-of-curves-in-machine-learning.html"><a href="22-the-cost-of-curves-in-machine-learning.html"><i class="fa fa-check"></i><b>22</b> The cost of curves in machine learning</a></li>
<li class="chapter" data-level="23" data-path="23-curves-and-the-variation-in-models.html"><a href="23-curves-and-the-variation-in-models.html"><i class="fa fa-check"></i><b>23</b> Curves and the variation in models</a></li>
<li class="chapter" data-level="24" data-path="24-observing-costs-losses-and-objectives-through-optimisation.html"><a href="24-observing-costs-losses-and-objectives-through-optimisation.html"><i class="fa fa-check"></i><b>24</b> Observing costs, losses and objectives through optimisation</a></li>
<li class="chapter" data-level="25" data-path="25-gradients-as-partial-observers.html"><a href="25-gradients-as-partial-observers.html"><i class="fa fa-check"></i><b>25</b> Gradients as partial observers</a></li>
<li class="chapter" data-level="26" data-path="26-the-power-to-learn.html"><a href="26-the-power-to-learn.html"><i class="fa fa-check"></i><b>26</b> The power to learn</a></li>
<li class="chapter" data-level="27" data-path="27-probabilisation-and-the-taming-of-machines.html"><a href="27-probabilisation-and-the-taming-of-machines.html"><i class="fa fa-check"></i><b>27</b> Probabilisation and the Taming of Machines}</a></li>
<li class="chapter" data-level="28" data-path="28-data-reduces-uncertainty.html"><a href="28-data-reduces-uncertainty.html"><i class="fa fa-check"></i><b>28</b> Data reduces uncertainty?</a></li>
<li class="chapter" data-level="29" data-path="29-machine-learning-as-statistics-inside-out.html"><a href="29-machine-learning-as-statistics-inside-out.html"><i class="fa fa-check"></i><b>29</b> Machine learning as statistics inside out</a></li>
<li class="chapter" data-level="30" data-path="30-distributed-probabilities.html"><a href="30-distributed-probabilities.html"><i class="fa fa-check"></i><b>30</b> Distributed probabilities</a></li>
<li class="chapter" data-level="31" data-path="31-naive-bayes-and-the-distribution-of-probabilities.html"><a href="31-naive-bayes-and-the-distribution-of-probabilities.html"><i class="fa fa-check"></i><b>31</b> Naive Bayes and the distribution of probabilities</a></li>
<li class="chapter" data-level="32" data-path="32-spam-when-foralln-is-too-much.html"><a href="32-spam-when-foralln-is-too-much.html"><i class="fa fa-check"></i><b>32</b> Spam: when <span class="math inline">\(\forall{N}\)</span> is too much?</a></li>
<li class="chapter" data-level="33" data-path="33-the-improbable-success-of-the-naive-bayes-classifier.html"><a href="33-the-improbable-success-of-the-naive-bayes-classifier.html"><i class="fa fa-check"></i><b>33</b> The improbable success of the Naive Bayes classifier</a></li>
<li class="chapter" data-level="34" data-path="34-ancestral-probabilities-in-documents-inference-and-prediction.html"><a href="34-ancestral-probabilities-in-documents-inference-and-prediction.html"><i class="fa fa-check"></i><b>34</b> Ancestral probabilities in documents: inference and prediction</a></li>
<li class="chapter" data-level="35" data-path="35-statistical-decompositions-bias-variance-and-observed-errors.html"><a href="35-statistical-decompositions-bias-variance-and-observed-errors.html"><i class="fa fa-check"></i><b>35</b> Statistical decompositions: bias, variance and observed errors</a></li>
<li class="chapter" data-level="36" data-path="36-does-machine-learning-construct-a-new-statistical-reality.html"><a href="36-does-machine-learning-construct-a-new-statistical-reality.html"><i class="fa fa-check"></i><b>36</b> Does machine learning construct a new statistical reality?</a></li>
<li class="chapter" data-level="37" data-path="37-patterns-and-differences.html"><a href="37-patterns-and-differences.html"><i class="fa fa-check"></i><b>37</b> Patterns and differences</a></li>
<li class="chapter" data-level="38" data-path="38-splitting-and-the-growth-of-trees.html"><a href="38-splitting-and-the-growth-of-trees.html"><i class="fa fa-check"></i><b>38</b> Splitting and the growth of trees</a></li>
<li class="chapter" data-level="39" data-path="39-differences-in-recursive-partitioning.html"><a href="39-differences-in-recursive-partitioning.html"><i class="fa fa-check"></i><b>39</b> 1984: Differences in recursive partitioning</a></li>
<li class="chapter" data-level="40" data-path="40-limiting-differences.html"><a href="40-limiting-differences.html"><i class="fa fa-check"></i><b>40</b> Limiting differences</a></li>
<li class="chapter" data-level="41" data-path="41-the-successful-dispersion-of-the-support-vector-machine.html"><a href="41-the-successful-dispersion-of-the-support-vector-machine.html"><i class="fa fa-check"></i><b>41</b> The successful dispersion of the support vector machine</a></li>
<li class="chapter" data-level="42" data-path="42-differences-blur.html"><a href="42-differences-blur.html"><i class="fa fa-check"></i><b>42</b> Differences blur?</a></li>
<li class="chapter" data-level="43" data-path="43-bending-the-decision-boundary.html"><a href="43-bending-the-decision-boundary.html"><i class="fa fa-check"></i><b>43</b> Bending the decision boundary</a></li>
<li class="chapter" data-level="44" data-path="44-instituting-patterns.html"><a href="44-instituting-patterns.html"><i class="fa fa-check"></i><b>44</b> Instituting patterns</a></li>
<li class="chapter" data-level="45" data-path="45-regularizing-and-materializing-objects.html"><a href="45-regularizing-and-materializing-objects.html"><i class="fa fa-check"></i><b>45</b> Regularizing and materializing objects}</a></li>
<li class="chapter" data-level="46" data-path="46-genomic-referentiality-and-materiality.html"><a href="46-genomic-referentiality-and-materiality.html"><i class="fa fa-check"></i><b>46</b> Genomic referentiality and materiality</a></li>
<li class="chapter" data-level="47" data-path="47-the-genome-as-threshold-object.html"><a href="47-the-genome-as-threshold-object.html"><i class="fa fa-check"></i><b>47</b> The genome as threshold object</a></li>
<li class="chapter" data-level="48" data-path="48-genomic-knowledges-and-their-datasets.html"><a href="48-genomic-knowledges-and-their-datasets.html"><i class="fa fa-check"></i><b>48</b> Genomic knowledges and their datasets</a></li>
<li class="chapter" data-level="49" data-path="49-the-advent-of-wide-dirty-and-mixed-data.html"><a href="49-the-advent-of-wide-dirty-and-mixed-data.html"><i class="fa fa-check"></i><b>49</b> The advent of ‘wide, dirty and mixed’ data</a></li>
<li class="chapter" data-level="50" data-path="50-cross-validating-machine-learning-in-genomics.html"><a href="50-cross-validating-machine-learning-in-genomics.html"><i class="fa fa-check"></i><b>50</b> Cross-validating machine learning in genomics</a></li>
<li class="chapter" data-level="51" data-path="51-proliferation-of-discoveries.html"><a href="51-proliferation-of-discoveries.html"><i class="fa fa-check"></i><b>51</b> Proliferation of discoveries</a></li>
<li class="chapter" data-level="52" data-path="52-variations-in-the-object-or-in-the-machine-learner.html"><a href="52-variations-in-the-object-or-in-the-machine-learner.html"><i class="fa fa-check"></i><b>52</b> Variations in the object or in the machine learner?</a></li>
<li class="chapter" data-level="53" data-path="53-whole-genome-functions.html"><a href="53-whole-genome-functions.html"><i class="fa fa-check"></i><b>53</b> Whole genome functions</a></li>
<li class="chapter" data-level="54" data-path="54-propagating-subject-positions.html"><a href="54-propagating-subject-positions.html"><i class="fa fa-check"></i><b>54</b> Propagating subject positions}</a></li>
<li class="chapter" data-level="55" data-path="55-propagation-across-human-machine-boundaries.html"><a href="55-propagation-across-human-machine-boundaries.html"><i class="fa fa-check"></i><b>55</b> Propagation across human-machine boundaries</a></li>
<li class="chapter" data-level="56" data-path="56-competitive-positioning.html"><a href="56-competitive-positioning.html"><i class="fa fa-check"></i><b>56</b> Competitive positioning</a></li>
<li class="chapter" data-level="57" data-path="57-a-privileged-machine-and-its-diagrammatic-forms.html"><a href="57-a-privileged-machine-and-its-diagrammatic-forms.html"><i class="fa fa-check"></i><b>57</b> A privileged machine and its diagrammatic forms</a></li>
<li class="chapter" data-level="58" data-path="58-varying-subject-positions-in-code.html"><a href="58-varying-subject-positions-in-code.html"><i class="fa fa-check"></i><b>58</b> Varying subject positions in code</a></li>
<li class="chapter" data-level="59" data-path="59-the-subjects-of-a-hidden-operation.html"><a href="59-the-subjects-of-a-hidden-operation.html"><i class="fa fa-check"></i><b>59</b> The subjects of a hidden operation</a></li>
<li class="chapter" data-level="60" data-path="60-algorithms-that-propagate-errors.html"><a href="60-algorithms-that-propagate-errors.html"><i class="fa fa-check"></i><b>60</b> Algorithms that propagate errors</a></li>
<li class="chapter" data-level="61" data-path="61-competitions-as-examination.html"><a href="61-competitions-as-examination.html"><i class="fa fa-check"></i><b>61</b> Competitions as examination</a></li>
<li class="chapter" data-level="62" data-path="62-superimposing-power-and-knowledge.html"><a href="62-superimposing-power-and-knowledge.html"><i class="fa fa-check"></i><b>62</b> Superimposing power and knowledge</a></li>
<li class="chapter" data-level="63" data-path="63-ranked-subject-positions.html"><a href="63-ranked-subject-positions.html"><i class="fa fa-check"></i><b>63</b> Ranked subject positions</a></li>
<li class="chapter" data-level="64" data-path="64-conclusion-out-of-the-data.html"><a href="64-conclusion-out-of-the-data.html"><i class="fa fa-check"></i><b>64</b> Conclusion: Out of the Data}</a></li>
<li class="chapter" data-level="65" data-path="65-machine-learners.html"><a href="65-machine-learners.html"><i class="fa fa-check"></i><b>65</b> 250,000 machine learners</a></li>
<li class="chapter" data-level="66" data-path="66-a-summary-of-the-argument.html"><a href="66-a-summary-of-the-argument.html"><i class="fa fa-check"></i><b>66</b> A summary of the argument</a></li>
<li class="chapter" data-level="67" data-path="67-in-situ-hybridization.html"><a href="67-in-situ-hybridization.html"><i class="fa fa-check"></i><b>67</b> In-situ hybridization</a></li>
<li class="chapter" data-level="68" data-path="68-critical-operational-practice.html"><a href="68-critical-operational-practice.html"><i class="fa fa-check"></i><b>68</b> Critical operational practice?</a></li>
<li class="chapter" data-level="69" data-path="69-obstacles-to-the-work-of-freeing-machine-learning.html"><a href="69-obstacles-to-the-work-of-freeing-machine-learning.html"><i class="fa fa-check"></i><b>69</b> Obstacles to the work of freeing machine learning</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning: Archaeology of a Data Practice</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="superimposing-power-and-knowledge" class="section level1">
<h1><span class="header-section-number">62</span> Superimposing power and knowledge</h1>
<p>It would be possible to explore in much greater ethnographic depth the practices of Kaggle competitors, the spectrum of participants (ranging from undergraduate student teams through to retired scientists, from hedge fund financial analysts to physicists), and the ways in which the topics of competitions relate to different scientific, governmental and commercial problems. Here I am interested mainly in the form of the competition as a test or examination centred on errors. The competitions take the form of examinations that set a problem, define some limits or constraints on its solution, and create a space that qualifies, ranks and displays the work of individuals or groups according to rates of generalization error (the error that arises when a machine learner encounters new data). </p>
<p>Machine learning competitions instance practices of examination that Foucault described in <em>Discipline and Punish</em>:</p>
<blockquote>
<p>The examination combines the techniques of an observing hierarchy and those of a normalizing judgement. … It establishes over individuals a visibility through which one differentiates them and judges them. That is why, in all the mechanisms of discipline, the examination is highly ritualized. In it are combined the ceremony of power and the form of the experiment, the deployment of force and the establishment of truth. At the heart of the procedures of discipline, it manifests the subjection of those who are perceived as objects and the objectification of those who are subjected. The superimposition of the power relations and knowledge relations assumes in the examination all its visible brilliance <span class="citation">[@Foucault_1977, 183-185]</span>. </p>
</blockquote>
<p>The disciplinary form of the examination of errors links statements and operations. Examinations combine ceremony, ritual, experiment, force and truth in subject and object positioning operations.  The consolidation of machine learning as a data practice today in competitions occurs via a much more pervasive practice of examining and testing. The forms of visibility created by competitions individualize and normalize machine learners (often by proper names), and optimise extractions of force, time, propensities and aptitudes.</p>

<p>In many Kaggle competitions (some titles are shown in table ), winning entries come from machine learners working together.  In the National Data Science Bowl competition of 2015, competitors were asked to classify images of more than 100 species of plankton. The winning team comprised seven graduate and post-doctoral researchers from Ghent University, Belgium. In a jointly written blog account of their winning entry, team ‘Deep Sea’ describe something of the construction of the deep learning models they built. These were convolutional neural nets, neural nets in which elements of the network only ‘look’ at overlapping tiles of the input images: </p>
<blockquote>
<p>We started with a fairly shallow models by modern standards (~ 6 layers) and gradually added more layers when we noticed it improved performance (it usually did). Near the end of the competition, we were training models with up to 16 layers. The challenge, as always, was balancing improved performance with increased overfitting <span class="citation">[@Dieleman_2015a]</span>.</p>
</blockquote>
<p>Like many of the entrants in image-based classification competitions such as the ImageNet Large Scale Visual Recognition Challenge <span class="citation">[@ILSVRC_2014]</span>, ‘Deep Sea’ built their machine learner in several stages, first deriving features  from the data by creating various layers that looked for common features across various scales, rotations and other transformations of the plankton images, and then adding neural net layers to classify those derived features using the labels supplied in the training set. In this respect, and in almost perfect synchrony with the deep learning teams at Google, Facebook and many other places, ‘Deep Sea’ combined supervised and unsupervised learning techniques . The lower convolutional layers that process the images are strictly speaker unsupervised because they make no use of the known labels or categories of the plankton; the upper layers are supervised because they make use of the labels in the normal back-propagation process of neural net training.</p>
<p>In comparison to the plain or ‘vanilla’ neural nets discussed above, deep belief networks involve many more parameters, stages of observation and modelling, configuration of hardware and infrastructural arrangements and comparison of results. ‘Deep Sea’ describe the architecture of one of their more successful models:</p>
<blockquote>
<p>It has 13 layers with parameters (10 convolutional, 3 fully connected) and 4 spatial pooling layers. The input shape is (32, 1, 95, 95), in bc01 order (batch size, number of channels, height, width). The output shape is (32, 121). For a given input, the network outputs 121 probabilities that sum to 1, one for each class.</p>
</blockquote>
<p>They go on to describe the different layers – cyclic slice, convolutional, spatial pooling – that derive features from the data or augmenting it (by examining overlapping tiles, by rotating or scaling the images, so that any given image, is ‘seen’ in a number of different ways, and the model learns to detect these variations). The combination of diverse layers in a stratified model introduces a range of learners into the operation, just as Kaggle itself networks many machine learners through its competitions.</p>
<p>A massive parallel computation allows ‘deep’ learning. Infrastructure and cognition entwine heavily here, since the very possibility of training large many-layered neural nets depends heavily on vectorised transformations of image data. Probably few other competitors in this competition would have had access to the Tesla K40 or ‘NVIDIA GTX 980 Superclocked’ GPU cards that ‘Deep Sea’ relied on.<a href="#fn101" class="footnoteRef" id="fnref101"><sup>101</sup></a>  Even with that intensive computational resource, their models required ‘between 24 and 48 hours to reach convergence.’ They constructed around 300 models. Because of the plethora of models with different architectures and parameters, ‘we had to select how many and which models to use in the final blend’ <span class="citation">[@Dieleman_2015a]</span>. As is often the case, competition engenders populations of machine learners whose aggregate tendencies model optimum performance.<a href="#fn102" class="footnoteRef" id="fnref102"><sup>102</sup></a> The ‘DeepSea’ team might epitomise machine learning subject positions. Like the ‘wonderful people’ described by Hilary Mason, they bring together infrastructure, engineering, mathematics/statistics and some knowledge of human behaviour (although the knowledge of human behaviour in this case might have more to do with what other Kaggle competitors might be doing, as well as an awareness of cutting edge research leaders in image recognition techniques).</p>
</div>
<div class="footnotes">
<hr />
<ol start="101">
<li id="fn101"><p>As another competitor in the National Data Science Bowl mentions:</p><blockquote><p>One example is here the Kaggle plankton detection competition. At first I thought about entering the competition as I might have a huge advantage through my 4 GPU system. I reasoned I might be able to train a very large convolutional net in a very short time – one thing that others cannot do because they lack the hardware <span class="citation">[@Dettmers_2015]</span></p></blockquote><p>Hardware parallelism and vectorization, at least in the area of deep learning, seems to matter more than the ability to test, examine, observe or invest new model configurations. <a href="62-superimposing-power-and-knowledge.html#fnref101">↩</a></p></li>
<li id="fn102"><p>On the command line, <code>git clone https://github.com/benanne/kaggle-ndsb</code> makes a copy of the model code. The code in that github repository gives some idea of the mosaic of techniques, configurations, variations and tests undertaken by ‘DeepSea.’<a href="62-superimposing-power-and-knowledge.html#fnref102">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="61-competitions-as-examination.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="63-ranked-subject-positions.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
