<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Machine Learning: Archaeology of a Data Practice</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="Machine Learning: Archaeology of a Data Practice">
  <meta name="generator" content="bookdown 0.3 and GitBook 2.6.7">

  <meta property="og:title" content="Machine Learning: Archaeology of a Data Practice" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Machine Learning: Archaeology of a Data Practice" />
  
  
  

<meta name="author" content="Adrian Mackenzie">


<meta name="date" content="2016-12-10">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="40-proliferation-of-discoveries.html">
<link rel="next" href="42-whole-genome-functions.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="1-acknowledgments.html"><a href="1-acknowledgments.html"><i class="fa fa-check"></i><b>1</b> Acknowledgments</a></li>
<li class="chapter" data-level="2" data-path="2-preface.html"><a href="2-preface.html"><i class="fa fa-check"></i><b>2</b> Preface</a></li>
<li class="chapter" data-level="3" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html"><i class="fa fa-check"></i><b>3</b> Introduction: Into the Data</a><ul>
<li class="chapter" data-level="3.1" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#three-accumulations-settings-data-and-devices"><i class="fa fa-check"></i><b>3.1</b> Three accumulations: settings, data and devices</a></li>
<li class="chapter" data-level="3.2" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#who-or-what-is-a-machine-learner"><i class="fa fa-check"></i><b>3.2</b> Who or what is a machine learner?</a></li>
<li class="chapter" data-level="3.3" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#algorithmic-control-to-the-machine-learners"><i class="fa fa-check"></i><b>3.3</b> Algorithmic control to the machine learners?</a></li>
<li class="chapter" data-level="3.4" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#the-archaeology-of-operations"><i class="fa fa-check"></i><b>3.4</b> The archaeology of operations</a></li>
<li class="chapter" data-level="3.5" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#asymmetries-in-common-knowledge"><i class="fa fa-check"></i><b>3.5</b> Asymmetries in common knowledge</a></li>
<li class="chapter" data-level="3.6" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#what-cannot-be-automated"><i class="fa fa-check"></i><b>3.6</b> What cannot be automated?</a></li>
<li class="chapter" data-level="3.7" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#different-fields-in-machine-learning"><i class="fa fa-check"></i><b>3.7</b> Different fields in machine learning?</a></li>
<li class="chapter" data-level="3.8" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#the-diagram-in-critical-thought"><i class="fa fa-check"></i><b>3.8</b> The diagram in critical thought</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html"><i class="fa fa-check"></i><b>4</b> Diagramming machines}</a><ul>
<li class="chapter" data-level="4.1" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#we-dont-have-to-write-programs"><i class="fa fa-check"></i><b>4.1</b> ‘We don’t have to write programs’?</a></li>
<li class="chapter" data-level="4.2" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#the-elements-of-machine-learning"><i class="fa fa-check"></i><b>4.2</b> The elements of machine learning</a></li>
<li class="chapter" data-level="4.3" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#who-reads-machine-learning-textbooks"><i class="fa fa-check"></i><b>4.3</b> Who reads machine learning textbooks?</a></li>
<li class="chapter" data-level="4.4" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#r-a-matrix-of-transformations"><i class="fa fa-check"></i><b>4.4</b> <code>R</code>: a matrix of transformations</a></li>
<li class="chapter" data-level="4.5" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#the-obdurate-mathematical-glint-of-machine-learning"><i class="fa fa-check"></i><b>4.5</b> The obdurate mathematical glint of machine learning</a></li>
<li class="chapter" data-level="4.6" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#cs229-2007-returning-again-and-again-to-certain-features"><i class="fa fa-check"></i><b>4.6</b> CS229, 2007: returning again and again to certain features</a></li>
<li class="chapter" data-level="4.7" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#the-visible-learning-of-machine-learning"><i class="fa fa-check"></i><b>4.7</b> The visible learning of machine learning</a></li>
<li class="chapter" data-level="4.8" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#the-diagram-of-an-operational-formation"><i class="fa fa-check"></i><b>4.8</b> The diagram of an operational formation</a></li>
<li class="chapter" data-level="4.9" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#vectorisation-and-its-consequences"><i class="fa fa-check"></i><b>4.9</b> Vectorisation and its consequences}</a></li>
<li class="chapter" data-level="4.10" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#vector-space-and-geometry"><i class="fa fa-check"></i><b>4.10</b> Vector space and geometry</a></li>
<li class="chapter" data-level="4.11" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#mixing-places"><i class="fa fa-check"></i><b>4.11</b> Mixing places</a></li>
<li class="chapter" data-level="4.12" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#truth-is-no-longer-in-the-table"><i class="fa fa-check"></i><b>4.12</b> Truth is no longer in the table?</a></li>
<li class="chapter" data-level="4.13" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#the-epistopic-fault-line-in-tables"><i class="fa fa-check"></i><b>4.13</b> The epistopic fault line in tables</a></li>
<li class="chapter" data-level="4.14" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#surface-and-depths-the-problem-of-volume-in-data"><i class="fa fa-check"></i><b>4.14</b> Surface and depths: the problem of volume in data</a></li>
<li class="chapter" data-level="4.15" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#vector-space-expansion"><i class="fa fa-check"></i><b>4.15</b> Vector space expansion</a></li>
<li class="chapter" data-level="4.16" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#drawing-lines-in-a-common-space-of-transformation"><i class="fa fa-check"></i><b>4.16</b> Drawing lines in a common space of transformation</a></li>
<li class="chapter" data-level="4.17" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#implicit-vectorization-in-code-and-infrastructures"><i class="fa fa-check"></i><b>4.17</b> Implicit vectorization in code and infrastructures</a></li>
<li class="chapter" data-level="4.18" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#lines-traversing-behind-the-light"><i class="fa fa-check"></i><b>4.18</b> Lines traversing behind the light</a></li>
<li class="chapter" data-level="4.19" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#the-vectorised-table"><i class="fa fa-check"></i><b>4.19</b> The vectorised table?</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-machines-finding-functions.html"><a href="5-machines-finding-functions.html"><i class="fa fa-check"></i><b>5</b> Machines finding functions}</a></li>
<li class="chapter" data-level="6" data-path="6-learning-functions.html"><a href="6-learning-functions.html"><i class="fa fa-check"></i><b>6</b> Learning functions</a></li>
<li class="chapter" data-level="7" data-path="7-supervised-unsupervised-reinforcement-learning-and-functions.html"><a href="7-supervised-unsupervised-reinforcement-learning-and-functions.html"><i class="fa fa-check"></i><b>7</b> Supervised, unsupervised, reinforcement learning and functions</a></li>
<li class="chapter" data-level="8" data-path="8-which-function-operates.html"><a href="8-which-function-operates.html"><i class="fa fa-check"></i><b>8</b> Which function operates?</a></li>
<li class="chapter" data-level="9" data-path="9-what-does-a-function-learn.html"><a href="9-what-does-a-function-learn.html"><i class="fa fa-check"></i><b>9</b> What does a function learn?</a></li>
<li class="chapter" data-level="10" data-path="10-observing-with-curves-the-logistic-function.html"><a href="10-observing-with-curves-the-logistic-function.html"><i class="fa fa-check"></i><b>10</b> Observing with curves: the logistic function</a></li>
<li class="chapter" data-level="11" data-path="11-the-cost-of-curves-in-machine-learning.html"><a href="11-the-cost-of-curves-in-machine-learning.html"><i class="fa fa-check"></i><b>11</b> The cost of curves in machine learning</a></li>
<li class="chapter" data-level="12" data-path="12-curves-and-the-variation-in-models.html"><a href="12-curves-and-the-variation-in-models.html"><i class="fa fa-check"></i><b>12</b> Curves and the variation in models</a></li>
<li class="chapter" data-level="13" data-path="13-observing-costs-losses-and-objectives-through-optimisation.html"><a href="13-observing-costs-losses-and-objectives-through-optimisation.html"><i class="fa fa-check"></i><b>13</b> Observing costs, losses and objectives through optimisation</a></li>
<li class="chapter" data-level="14" data-path="14-gradients-as-partial-observers.html"><a href="14-gradients-as-partial-observers.html"><i class="fa fa-check"></i><b>14</b> Gradients as partial observers</a></li>
<li class="chapter" data-level="15" data-path="15-the-power-to-learn.html"><a href="15-the-power-to-learn.html"><i class="fa fa-check"></i><b>15</b> The power to learn</a></li>
<li class="chapter" data-level="16" data-path="16-probabilisation-and-the-taming-of-machines.html"><a href="16-probabilisation-and-the-taming-of-machines.html"><i class="fa fa-check"></i><b>16</b> Probabilisation and the Taming of Machines}</a></li>
<li class="chapter" data-level="17" data-path="17-data-reduces-uncertainty.html"><a href="17-data-reduces-uncertainty.html"><i class="fa fa-check"></i><b>17</b> Data reduces uncertainty?</a></li>
<li class="chapter" data-level="18" data-path="18-machine-learning-as-statistics-inside-out.html"><a href="18-machine-learning-as-statistics-inside-out.html"><i class="fa fa-check"></i><b>18</b> Machine learning as statistics inside out</a></li>
<li class="chapter" data-level="19" data-path="19-distributed-probabilities.html"><a href="19-distributed-probabilities.html"><i class="fa fa-check"></i><b>19</b> Distributed probabilities</a></li>
<li class="chapter" data-level="20" data-path="20-naive-bayes-and-the-distribution-of-probabilities.html"><a href="20-naive-bayes-and-the-distribution-of-probabilities.html"><i class="fa fa-check"></i><b>20</b> Naive Bayes and the distribution of probabilities</a></li>
<li class="chapter" data-level="21" data-path="21-spam-when-foralln-is-too-much.html"><a href="21-spam-when-foralln-is-too-much.html"><i class="fa fa-check"></i><b>21</b> Spam: when <span class="math inline">\(\forall{N}\)</span> is too much?</a></li>
<li class="chapter" data-level="22" data-path="22-the-improbable-success-of-the-naive-bayes-classifier.html"><a href="22-the-improbable-success-of-the-naive-bayes-classifier.html"><i class="fa fa-check"></i><b>22</b> The improbable success of the Naive Bayes classifier</a></li>
<li class="chapter" data-level="23" data-path="23-ancestral-probabilities-in-documents-inference-and-prediction.html"><a href="23-ancestral-probabilities-in-documents-inference-and-prediction.html"><i class="fa fa-check"></i><b>23</b> Ancestral probabilities in documents: inference and prediction</a></li>
<li class="chapter" data-level="24" data-path="24-statistical-decompositions-bias-variance-and-observed-errors.html"><a href="24-statistical-decompositions-bias-variance-and-observed-errors.html"><i class="fa fa-check"></i><b>24</b> Statistical decompositions: bias, variance and observed errors</a></li>
<li class="chapter" data-level="25" data-path="25-does-machine-learning-construct-a-new-statistical-reality.html"><a href="25-does-machine-learning-construct-a-new-statistical-reality.html"><i class="fa fa-check"></i><b>25</b> Does machine learning construct a new statistical reality?</a></li>
<li class="chapter" data-level="26" data-path="26-patterns-and-differences.html"><a href="26-patterns-and-differences.html"><i class="fa fa-check"></i><b>26</b> Patterns and differences</a></li>
<li class="chapter" data-level="27" data-path="27-splitting-and-the-growth-of-trees.html"><a href="27-splitting-and-the-growth-of-trees.html"><i class="fa fa-check"></i><b>27</b> Splitting and the growth of trees</a></li>
<li class="chapter" data-level="28" data-path="28-differences-in-recursive-partitioning.html"><a href="28-differences-in-recursive-partitioning.html"><i class="fa fa-check"></i><b>28</b> 1984: Differences in recursive partitioning</a></li>
<li class="chapter" data-level="29" data-path="29-limiting-differences.html"><a href="29-limiting-differences.html"><i class="fa fa-check"></i><b>29</b> Limiting differences</a></li>
<li class="chapter" data-level="30" data-path="30-the-successful-dispersion-of-the-support-vector-machine.html"><a href="30-the-successful-dispersion-of-the-support-vector-machine.html"><i class="fa fa-check"></i><b>30</b> The successful dispersion of the support vector machine</a></li>
<li class="chapter" data-level="31" data-path="31-differences-blur.html"><a href="31-differences-blur.html"><i class="fa fa-check"></i><b>31</b> Differences blur?</a></li>
<li class="chapter" data-level="32" data-path="32-bending-the-decision-boundary.html"><a href="32-bending-the-decision-boundary.html"><i class="fa fa-check"></i><b>32</b> Bending the decision boundary</a></li>
<li class="chapter" data-level="33" data-path="33-instituting-patterns.html"><a href="33-instituting-patterns.html"><i class="fa fa-check"></i><b>33</b> Instituting patterns</a></li>
<li class="chapter" data-level="34" data-path="34-regularizing-and-materializing-objects.html"><a href="34-regularizing-and-materializing-objects.html"><i class="fa fa-check"></i><b>34</b> Regularizing and materializing objects}</a></li>
<li class="chapter" data-level="35" data-path="35-genomic-referentiality-and-materiality.html"><a href="35-genomic-referentiality-and-materiality.html"><i class="fa fa-check"></i><b>35</b> Genomic referentiality and materiality</a></li>
<li class="chapter" data-level="36" data-path="36-the-genome-as-threshold-object.html"><a href="36-the-genome-as-threshold-object.html"><i class="fa fa-check"></i><b>36</b> The genome as threshold object</a></li>
<li class="chapter" data-level="37" data-path="37-genomic-knowledges-and-their-datasets.html"><a href="37-genomic-knowledges-and-their-datasets.html"><i class="fa fa-check"></i><b>37</b> Genomic knowledges and their datasets</a></li>
<li class="chapter" data-level="38" data-path="38-the-advent-of-wide-dirty-and-mixed-data.html"><a href="38-the-advent-of-wide-dirty-and-mixed-data.html"><i class="fa fa-check"></i><b>38</b> The advent of ‘wide, dirty and mixed’ data</a></li>
<li class="chapter" data-level="39" data-path="39-cross-validating-machine-learning-in-genomics.html"><a href="39-cross-validating-machine-learning-in-genomics.html"><i class="fa fa-check"></i><b>39</b> Cross-validating machine learning in genomics</a></li>
<li class="chapter" data-level="40" data-path="40-proliferation-of-discoveries.html"><a href="40-proliferation-of-discoveries.html"><i class="fa fa-check"></i><b>40</b> Proliferation of discoveries</a></li>
<li class="chapter" data-level="41" data-path="41-variations-in-the-object-or-in-the-machine-learner.html"><a href="41-variations-in-the-object-or-in-the-machine-learner.html"><i class="fa fa-check"></i><b>41</b> Variations in the object or in the machine learner?</a></li>
<li class="chapter" data-level="42" data-path="42-whole-genome-functions.html"><a href="42-whole-genome-functions.html"><i class="fa fa-check"></i><b>42</b> Whole genome functions</a></li>
<li class="chapter" data-level="43" data-path="43-propagating-subject-positions.html"><a href="43-propagating-subject-positions.html"><i class="fa fa-check"></i><b>43</b> Propagating subject positions}</a></li>
<li class="chapter" data-level="44" data-path="44-propagation-across-human-machine-boundaries.html"><a href="44-propagation-across-human-machine-boundaries.html"><i class="fa fa-check"></i><b>44</b> Propagation across human-machine boundaries</a></li>
<li class="chapter" data-level="45" data-path="45-competitive-positioning.html"><a href="45-competitive-positioning.html"><i class="fa fa-check"></i><b>45</b> Competitive positioning</a></li>
<li class="chapter" data-level="46" data-path="46-a-privileged-machine-and-its-diagrammatic-forms.html"><a href="46-a-privileged-machine-and-its-diagrammatic-forms.html"><i class="fa fa-check"></i><b>46</b> A privileged machine and its diagrammatic forms</a></li>
<li class="chapter" data-level="47" data-path="47-varying-subject-positions-in-code.html"><a href="47-varying-subject-positions-in-code.html"><i class="fa fa-check"></i><b>47</b> Varying subject positions in code</a></li>
<li class="chapter" data-level="48" data-path="48-the-subjects-of-a-hidden-operation.html"><a href="48-the-subjects-of-a-hidden-operation.html"><i class="fa fa-check"></i><b>48</b> The subjects of a hidden operation</a></li>
<li class="chapter" data-level="49" data-path="49-algorithms-that-propagate-errors.html"><a href="49-algorithms-that-propagate-errors.html"><i class="fa fa-check"></i><b>49</b> Algorithms that propagate errors</a></li>
<li class="chapter" data-level="50" data-path="50-competitions-as-examination.html"><a href="50-competitions-as-examination.html"><i class="fa fa-check"></i><b>50</b> Competitions as examination</a></li>
<li class="chapter" data-level="51" data-path="51-superimposing-power-and-knowledge.html"><a href="51-superimposing-power-and-knowledge.html"><i class="fa fa-check"></i><b>51</b> Superimposing power and knowledge</a></li>
<li class="chapter" data-level="52" data-path="52-ranked-subject-positions.html"><a href="52-ranked-subject-positions.html"><i class="fa fa-check"></i><b>52</b> Ranked subject positions</a></li>
<li class="chapter" data-level="53" data-path="53-conclusion-out-of-the-data.html"><a href="53-conclusion-out-of-the-data.html"><i class="fa fa-check"></i><b>53</b> Conclusion: Out of the Data}</a></li>
<li class="chapter" data-level="54" data-path="54-machine-learners.html"><a href="54-machine-learners.html"><i class="fa fa-check"></i><b>54</b> 250,000 machine learners</a></li>
<li class="chapter" data-level="55" data-path="55-a-summary-of-the-argument.html"><a href="55-a-summary-of-the-argument.html"><i class="fa fa-check"></i><b>55</b> A summary of the argument</a></li>
<li class="chapter" data-level="56" data-path="56-in-situ-hybridization.html"><a href="56-in-situ-hybridization.html"><i class="fa fa-check"></i><b>56</b> In-situ hybridization</a></li>
<li class="chapter" data-level="57" data-path="57-critical-operational-practice.html"><a href="57-critical-operational-practice.html"><i class="fa fa-check"></i><b>57</b> Critical operational practice?</a></li>
<li class="chapter" data-level="58" data-path="58-obstacles-to-the-work-of-freeing-machine-learning.html"><a href="58-obstacles-to-the-work-of-freeing-machine-learning.html"><i class="fa fa-check"></i><b>58</b> Obstacles to the work of freeing machine learning</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning: Archaeology of a Data Practice</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="variations-in-the-object-or-in-the-machine-learner" class="section level1">
<h1><span class="header-section-number">41</span> Variations in the object or in the machine learner?</h1>

<p>‘The method of k-nearest neighbors makes very mild structural assumptions: its predictions are often accurate but can be unstable’ write Hastie and co-authors <span class="citation">[@Hastie_2009, 23]</span>. The algorithm, first described by Evelyn Fix and Joseph Hodges working at Berkeley in the early 1950s <span class="citation">[@Fix_1951]</span>, is extremely simple in mathematical terms.<a href="#fn88" class="footnoteRef" id="fnref88"><sup>88</sup></a> Equation  shows almost the entire algorithm: </p>

<p>‘where <span class="math inline">\(N_k(x)\)</span> is the neighbourhood of <span class="math inline">\(x\)</span> defined by the <span class="math inline">\(k\)</span> closest points <span class="math inline">\(x_i\)</span> in the training sample’<span class="citation">[@Hastie_2009, 14]</span>. The algorithm effectively takes the average values of points in the neighbourhood <span class="math inline">\(N_k\)</span>, and uses that value to predict the result (a classification or a prediction) for a given point or instance. As Hastie and co-authors put it, the neighbourhood is just those <span class="math inline">\(k\)</span> points near the case under consideration. The assumption here, as in nearly all machine learners transforming the vector space, is that proximity in vector space implies similarity in class or grouping.  This assumption was formally described in the late 1960s in another highly cited paper <span class="citation">[@Cover_1967]</span> on ‘Nearest Neighbour Pattern Classification.’ Neighbouring points in the vector-space are more similar than those at a distance. As equation  shows, <em>k</em> nearest neighbours seems to have only one parameter, the value <span class="math inline">\(k\)</span>, the number of neighbours that a given model includes in its definition of a ‘neighbourhood.’ In contrast to the linear forms of the models (formulated in equations  or ), equation  seems to require little training, supervision or regularization to work as a classifier. While nearly all of the models discussed in this and earlier chapters work with a smooth functional form of the line or curve as their basic way of transforming vector space, <em>k</em> nearest neighbours generates highly non-linear boundaries wending their way through the data. Because they are not guided by parameters (apart from the value of the hyper-parameter <span class="math inline">\(k\)</span>), these boundaries can be unstable.</p>

<p></p>
<p>Even when data belongs to two classes (e.g. <code>normal</code> vs. <code>not-normal</code>), decision boundaries produced by <em>k</em>-nn can be unstable. The example in figure  shows two models, one for <span class="math inline">\(k=5\)</span> and the other for <span class="math inline">\(k=2\)</span>. Each model examines the relations between 2, 5 points in deciding whether a particular case belongs to one class or another. While <em>k-nn</em> constructs local clusters and traces out an irregular decision boundary, this classificatory power comes at the cost of instability. (This is another version of the bias-variance decomposition of machine learner errors discussed in chapter .)</p>
<p>More data, or wider data exacerbates the instability. As dimensions or features in the dataset increase, the local neighbourhood needed to capture a fraction of the volume of the data expands. It becomes more likely that most sample points will lie close to the boundary of the sample space, where they will be affected by the neighbouring space. The result is that ‘in high dimensions all feasible training samples sparsely populate the input space’ <span class="citation">[@Hastie_2009, 23]</span>. Because <em>k-nn</em> allows for non-linear interactions between features, for instance, small differences in the number of points in particular neighbourhoods can drastically affect some stretches of the boundaries (as we see in comparing the right and left hand plots in figure ). These kinds of topological instability account for the propensity of machine learning treatments of feature-rich genomic data to produce accurate but unstable predictions. We can begin to see how a MAQC-II team might have produced 463,000 <em>k</em> nearest neighbour models in an effort to normalise and regulate predictive predictions. The price of accurate predictivity in genomics is variation in prediction.</p>
</div>
<div class="footnotes">
<hr />
<ol start="88">
<li id="fn88"><p>Fix and Hodges frame their suggestion of the <em>k</em> nearest neighbour model in this way: ‘there seems to be a need for discriminative procedures whose validity does not require the amount of knowledge implied by by the normality assumption, the homoscedastic assumption, or any assumption of parametric form. The present paper is, as far as the authors are aware, the first one to attack subproblem (iii): can reasonable discrimination procedures be found which will work even if no parametric form can be assumed?’ <span class="citation">[@Fix_1951,7]</span>. Subproblem (iii) in this quote refers to the challenge of deciding which of two populations an observed case belongs to if we know nothing about the parameters describing the two populations. <a href="41-variations-in-the-object-or-in-the-machine-learner.html#fnref88">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="40-proliferation-of-discoveries.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="42-whole-genome-functions.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
