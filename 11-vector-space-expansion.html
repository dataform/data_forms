<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Machine Learning: Archaeology of a Data Practice</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="Machine Learning: Archaeology of a Data Practice">
  <meta name="generator" content="bookdown 0.3 and GitBook 2.6.7">

  <meta property="og:title" content="Machine Learning: Archaeology of a Data Practice" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Machine Learning: Archaeology of a Data Practice" />
  
  
  

<meta name="author" content="Adrian Mackenzie">


<meta name="date" content="2016-12-10">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="10-surface-and-depths-the-problem-of-volume-in-data.html">
<link rel="next" href="12-drawing-lines-in-a-common-space-of-transformation.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="1-acknowledgments.html"><a href="1-acknowledgments.html"><i class="fa fa-check"></i><b>1</b> Acknowledgments</a></li>
<li class="chapter" data-level="2" data-path="2-preface.html"><a href="2-preface.html"><i class="fa fa-check"></i><b>2</b> Preface</a></li>
<li class="chapter" data-level="3" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html"><i class="fa fa-check"></i><b>3</b> Introduction: Into the Data</a><ul>
<li class="chapter" data-level="3.1" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#three-accumulations-settings-data-and-devices"><i class="fa fa-check"></i><b>3.1</b> Three accumulations: settings, data and devices</a></li>
<li class="chapter" data-level="3.2" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#who-or-what-is-a-machine-learner"><i class="fa fa-check"></i><b>3.2</b> Who or what is a machine learner?</a></li>
<li class="chapter" data-level="3.3" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#algorithmic-control-to-the-machine-learners"><i class="fa fa-check"></i><b>3.3</b> Algorithmic control to the machine learners?</a></li>
<li class="chapter" data-level="3.4" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#the-archaeology-of-operations"><i class="fa fa-check"></i><b>3.4</b> The archaeology of operations</a></li>
<li class="chapter" data-level="3.5" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#asymmetries-in-common-knowledge"><i class="fa fa-check"></i><b>3.5</b> Asymmetries in common knowledge</a></li>
<li class="chapter" data-level="3.6" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#what-cannot-be-automated"><i class="fa fa-check"></i><b>3.6</b> What cannot be automated?</a></li>
<li class="chapter" data-level="3.7" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#different-fields-in-machine-learning"><i class="fa fa-check"></i><b>3.7</b> Different fields in machine learning?</a></li>
<li class="chapter" data-level="3.8" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#the-diagram-in-critical-thought"><i class="fa fa-check"></i><b>3.8</b> The diagram in critical thought</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html"><i class="fa fa-check"></i><b>4</b> Diagramming machines}</a><ul>
<li class="chapter" data-level="4.1" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#we-dont-have-to-write-programs"><i class="fa fa-check"></i><b>4.1</b> ‘We don’t have to write programs’?</a></li>
<li class="chapter" data-level="4.2" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#the-elements-of-machine-learning"><i class="fa fa-check"></i><b>4.2</b> The elements of machine learning</a></li>
<li class="chapter" data-level="4.3" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#who-reads-machine-learning-textbooks"><i class="fa fa-check"></i><b>4.3</b> Who reads machine learning textbooks?</a></li>
<li class="chapter" data-level="4.4" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#r-a-matrix-of-transformations"><i class="fa fa-check"></i><b>4.4</b> <code>R</code>: a matrix of transformations</a></li>
<li class="chapter" data-level="4.5" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#the-obdurate-mathematical-glint-of-machine-learning"><i class="fa fa-check"></i><b>4.5</b> The obdurate mathematical glint of machine learning</a></li>
<li class="chapter" data-level="4.6" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#cs229-2007-returning-again-and-again-to-certain-features"><i class="fa fa-check"></i><b>4.6</b> CS229, 2007: returning again and again to certain features</a></li>
<li class="chapter" data-level="4.7" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#the-visible-learning-of-machine-learning"><i class="fa fa-check"></i><b>4.7</b> The visible learning of machine learning</a></li>
<li class="chapter" data-level="4.8" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#the-diagram-of-an-operational-formation"><i class="fa fa-check"></i><b>4.8</b> The diagram of an operational formation</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html"><i class="fa fa-check"></i><b>5</b> Vectorisation and its consequences}</a></li>
<li class="chapter" data-level="6" data-path="6-vector-space-and-geometry.html"><a href="6-vector-space-and-geometry.html"><i class="fa fa-check"></i><b>6</b> Vector space and geometry</a></li>
<li class="chapter" data-level="7" data-path="7-mixing-places.html"><a href="7-mixing-places.html"><i class="fa fa-check"></i><b>7</b> Mixing places</a></li>
<li class="chapter" data-level="8" data-path="8-truth-is-no-longer-in-the-table.html"><a href="8-truth-is-no-longer-in-the-table.html"><i class="fa fa-check"></i><b>8</b> Truth is no longer in the table?</a></li>
<li class="chapter" data-level="9" data-path="9-the-epistopic-fault-line-in-tables.html"><a href="9-the-epistopic-fault-line-in-tables.html"><i class="fa fa-check"></i><b>9</b> The epistopic fault line in tables</a></li>
<li class="chapter" data-level="10" data-path="10-surface-and-depths-the-problem-of-volume-in-data.html"><a href="10-surface-and-depths-the-problem-of-volume-in-data.html"><i class="fa fa-check"></i><b>10</b> Surface and depths: the problem of volume in data</a></li>
<li class="chapter" data-level="11" data-path="11-vector-space-expansion.html"><a href="11-vector-space-expansion.html"><i class="fa fa-check"></i><b>11</b> Vector space expansion</a></li>
<li class="chapter" data-level="12" data-path="12-drawing-lines-in-a-common-space-of-transformation.html"><a href="12-drawing-lines-in-a-common-space-of-transformation.html"><i class="fa fa-check"></i><b>12</b> Drawing lines in a common space of transformation</a></li>
<li class="chapter" data-level="13" data-path="13-implicit-vectorization-in-code-and-infrastructures.html"><a href="13-implicit-vectorization-in-code-and-infrastructures.html"><i class="fa fa-check"></i><b>13</b> Implicit vectorization in code and infrastructures</a></li>
<li class="chapter" data-level="14" data-path="14-lines-traversing-behind-the-light.html"><a href="14-lines-traversing-behind-the-light.html"><i class="fa fa-check"></i><b>14</b> Lines traversing behind the light</a></li>
<li class="chapter" data-level="15" data-path="15-the-vectorised-table.html"><a href="15-the-vectorised-table.html"><i class="fa fa-check"></i><b>15</b> The vectorised table?</a></li>
<li class="chapter" data-level="16" data-path="16-machines-finding-functions.html"><a href="16-machines-finding-functions.html"><i class="fa fa-check"></i><b>16</b> Machines finding functions}</a></li>
<li class="chapter" data-level="17" data-path="17-learning-functions.html"><a href="17-learning-functions.html"><i class="fa fa-check"></i><b>17</b> Learning functions</a></li>
<li class="chapter" data-level="18" data-path="18-supervised-unsupervised-reinforcement-learning-and-functions.html"><a href="18-supervised-unsupervised-reinforcement-learning-and-functions.html"><i class="fa fa-check"></i><b>18</b> Supervised, unsupervised, reinforcement learning and functions</a></li>
<li class="chapter" data-level="19" data-path="19-which-function-operates.html"><a href="19-which-function-operates.html"><i class="fa fa-check"></i><b>19</b> Which function operates?</a></li>
<li class="chapter" data-level="20" data-path="20-what-does-a-function-learn.html"><a href="20-what-does-a-function-learn.html"><i class="fa fa-check"></i><b>20</b> What does a function learn?</a></li>
<li class="chapter" data-level="21" data-path="21-observing-with-curves-the-logistic-function.html"><a href="21-observing-with-curves-the-logistic-function.html"><i class="fa fa-check"></i><b>21</b> Observing with curves: the logistic function</a></li>
<li class="chapter" data-level="22" data-path="22-the-cost-of-curves-in-machine-learning.html"><a href="22-the-cost-of-curves-in-machine-learning.html"><i class="fa fa-check"></i><b>22</b> The cost of curves in machine learning</a></li>
<li class="chapter" data-level="23" data-path="23-curves-and-the-variation-in-models.html"><a href="23-curves-and-the-variation-in-models.html"><i class="fa fa-check"></i><b>23</b> Curves and the variation in models</a></li>
<li class="chapter" data-level="24" data-path="24-observing-costs-losses-and-objectives-through-optimisation.html"><a href="24-observing-costs-losses-and-objectives-through-optimisation.html"><i class="fa fa-check"></i><b>24</b> Observing costs, losses and objectives through optimisation</a></li>
<li class="chapter" data-level="25" data-path="25-gradients-as-partial-observers.html"><a href="25-gradients-as-partial-observers.html"><i class="fa fa-check"></i><b>25</b> Gradients as partial observers</a></li>
<li class="chapter" data-level="26" data-path="26-the-power-to-learn.html"><a href="26-the-power-to-learn.html"><i class="fa fa-check"></i><b>26</b> The power to learn</a></li>
<li class="chapter" data-level="27" data-path="27-probabilisation-and-the-taming-of-machines.html"><a href="27-probabilisation-and-the-taming-of-machines.html"><i class="fa fa-check"></i><b>27</b> Probabilisation and the Taming of Machines}</a></li>
<li class="chapter" data-level="28" data-path="28-data-reduces-uncertainty.html"><a href="28-data-reduces-uncertainty.html"><i class="fa fa-check"></i><b>28</b> Data reduces uncertainty?</a></li>
<li class="chapter" data-level="29" data-path="29-machine-learning-as-statistics-inside-out.html"><a href="29-machine-learning-as-statistics-inside-out.html"><i class="fa fa-check"></i><b>29</b> Machine learning as statistics inside out</a></li>
<li class="chapter" data-level="30" data-path="30-distributed-probabilities.html"><a href="30-distributed-probabilities.html"><i class="fa fa-check"></i><b>30</b> Distributed probabilities</a></li>
<li class="chapter" data-level="31" data-path="31-naive-bayes-and-the-distribution-of-probabilities.html"><a href="31-naive-bayes-and-the-distribution-of-probabilities.html"><i class="fa fa-check"></i><b>31</b> Naive Bayes and the distribution of probabilities</a></li>
<li class="chapter" data-level="32" data-path="32-spam-when-foralln-is-too-much.html"><a href="32-spam-when-foralln-is-too-much.html"><i class="fa fa-check"></i><b>32</b> Spam: when <span class="math inline">\(\forall{N}\)</span> is too much?</a></li>
<li class="chapter" data-level="33" data-path="33-the-improbable-success-of-the-naive-bayes-classifier.html"><a href="33-the-improbable-success-of-the-naive-bayes-classifier.html"><i class="fa fa-check"></i><b>33</b> The improbable success of the Naive Bayes classifier</a></li>
<li class="chapter" data-level="34" data-path="34-ancestral-probabilities-in-documents-inference-and-prediction.html"><a href="34-ancestral-probabilities-in-documents-inference-and-prediction.html"><i class="fa fa-check"></i><b>34</b> Ancestral probabilities in documents: inference and prediction</a></li>
<li class="chapter" data-level="35" data-path="35-statistical-decompositions-bias-variance-and-observed-errors.html"><a href="35-statistical-decompositions-bias-variance-and-observed-errors.html"><i class="fa fa-check"></i><b>35</b> Statistical decompositions: bias, variance and observed errors</a></li>
<li class="chapter" data-level="36" data-path="36-does-machine-learning-construct-a-new-statistical-reality.html"><a href="36-does-machine-learning-construct-a-new-statistical-reality.html"><i class="fa fa-check"></i><b>36</b> Does machine learning construct a new statistical reality?</a></li>
<li class="chapter" data-level="37" data-path="37-patterns-and-differences.html"><a href="37-patterns-and-differences.html"><i class="fa fa-check"></i><b>37</b> Patterns and differences</a></li>
<li class="chapter" data-level="38" data-path="38-splitting-and-the-growth-of-trees.html"><a href="38-splitting-and-the-growth-of-trees.html"><i class="fa fa-check"></i><b>38</b> Splitting and the growth of trees</a></li>
<li class="chapter" data-level="39" data-path="39-differences-in-recursive-partitioning.html"><a href="39-differences-in-recursive-partitioning.html"><i class="fa fa-check"></i><b>39</b> 1984: Differences in recursive partitioning</a></li>
<li class="chapter" data-level="40" data-path="40-limiting-differences.html"><a href="40-limiting-differences.html"><i class="fa fa-check"></i><b>40</b> Limiting differences</a></li>
<li class="chapter" data-level="41" data-path="41-the-successful-dispersion-of-the-support-vector-machine.html"><a href="41-the-successful-dispersion-of-the-support-vector-machine.html"><i class="fa fa-check"></i><b>41</b> The successful dispersion of the support vector machine</a></li>
<li class="chapter" data-level="42" data-path="42-differences-blur.html"><a href="42-differences-blur.html"><i class="fa fa-check"></i><b>42</b> Differences blur?</a></li>
<li class="chapter" data-level="43" data-path="43-bending-the-decision-boundary.html"><a href="43-bending-the-decision-boundary.html"><i class="fa fa-check"></i><b>43</b> Bending the decision boundary</a></li>
<li class="chapter" data-level="44" data-path="44-instituting-patterns.html"><a href="44-instituting-patterns.html"><i class="fa fa-check"></i><b>44</b> Instituting patterns</a></li>
<li class="chapter" data-level="45" data-path="45-regularizing-and-materializing-objects.html"><a href="45-regularizing-and-materializing-objects.html"><i class="fa fa-check"></i><b>45</b> Regularizing and materializing objects}</a></li>
<li class="chapter" data-level="46" data-path="46-genomic-referentiality-and-materiality.html"><a href="46-genomic-referentiality-and-materiality.html"><i class="fa fa-check"></i><b>46</b> Genomic referentiality and materiality</a></li>
<li class="chapter" data-level="47" data-path="47-the-genome-as-threshold-object.html"><a href="47-the-genome-as-threshold-object.html"><i class="fa fa-check"></i><b>47</b> The genome as threshold object</a></li>
<li class="chapter" data-level="48" data-path="48-genomic-knowledges-and-their-datasets.html"><a href="48-genomic-knowledges-and-their-datasets.html"><i class="fa fa-check"></i><b>48</b> Genomic knowledges and their datasets</a></li>
<li class="chapter" data-level="49" data-path="49-the-advent-of-wide-dirty-and-mixed-data.html"><a href="49-the-advent-of-wide-dirty-and-mixed-data.html"><i class="fa fa-check"></i><b>49</b> The advent of ‘wide, dirty and mixed’ data</a></li>
<li class="chapter" data-level="50" data-path="50-cross-validating-machine-learning-in-genomics.html"><a href="50-cross-validating-machine-learning-in-genomics.html"><i class="fa fa-check"></i><b>50</b> Cross-validating machine learning in genomics</a></li>
<li class="chapter" data-level="51" data-path="51-proliferation-of-discoveries.html"><a href="51-proliferation-of-discoveries.html"><i class="fa fa-check"></i><b>51</b> Proliferation of discoveries</a></li>
<li class="chapter" data-level="52" data-path="52-variations-in-the-object-or-in-the-machine-learner.html"><a href="52-variations-in-the-object-or-in-the-machine-learner.html"><i class="fa fa-check"></i><b>52</b> Variations in the object or in the machine learner?</a></li>
<li class="chapter" data-level="53" data-path="53-whole-genome-functions.html"><a href="53-whole-genome-functions.html"><i class="fa fa-check"></i><b>53</b> Whole genome functions</a></li>
<li class="chapter" data-level="54" data-path="54-propagating-subject-positions.html"><a href="54-propagating-subject-positions.html"><i class="fa fa-check"></i><b>54</b> Propagating subject positions}</a></li>
<li class="chapter" data-level="55" data-path="55-propagation-across-human-machine-boundaries.html"><a href="55-propagation-across-human-machine-boundaries.html"><i class="fa fa-check"></i><b>55</b> Propagation across human-machine boundaries</a></li>
<li class="chapter" data-level="56" data-path="56-competitive-positioning.html"><a href="56-competitive-positioning.html"><i class="fa fa-check"></i><b>56</b> Competitive positioning</a></li>
<li class="chapter" data-level="57" data-path="57-a-privileged-machine-and-its-diagrammatic-forms.html"><a href="57-a-privileged-machine-and-its-diagrammatic-forms.html"><i class="fa fa-check"></i><b>57</b> A privileged machine and its diagrammatic forms</a></li>
<li class="chapter" data-level="58" data-path="58-varying-subject-positions-in-code.html"><a href="58-varying-subject-positions-in-code.html"><i class="fa fa-check"></i><b>58</b> Varying subject positions in code</a></li>
<li class="chapter" data-level="59" data-path="59-the-subjects-of-a-hidden-operation.html"><a href="59-the-subjects-of-a-hidden-operation.html"><i class="fa fa-check"></i><b>59</b> The subjects of a hidden operation</a></li>
<li class="chapter" data-level="60" data-path="60-algorithms-that-propagate-errors.html"><a href="60-algorithms-that-propagate-errors.html"><i class="fa fa-check"></i><b>60</b> Algorithms that propagate errors</a></li>
<li class="chapter" data-level="61" data-path="61-competitions-as-examination.html"><a href="61-competitions-as-examination.html"><i class="fa fa-check"></i><b>61</b> Competitions as examination</a></li>
<li class="chapter" data-level="62" data-path="62-superimposing-power-and-knowledge.html"><a href="62-superimposing-power-and-knowledge.html"><i class="fa fa-check"></i><b>62</b> Superimposing power and knowledge</a></li>
<li class="chapter" data-level="63" data-path="63-ranked-subject-positions.html"><a href="63-ranked-subject-positions.html"><i class="fa fa-check"></i><b>63</b> Ranked subject positions</a></li>
<li class="chapter" data-level="64" data-path="64-conclusion-out-of-the-data.html"><a href="64-conclusion-out-of-the-data.html"><i class="fa fa-check"></i><b>64</b> Conclusion: Out of the Data}</a></li>
<li class="chapter" data-level="65" data-path="65-machine-learners.html"><a href="65-machine-learners.html"><i class="fa fa-check"></i><b>65</b> 250,000 machine learners</a></li>
<li class="chapter" data-level="66" data-path="66-a-summary-of-the-argument.html"><a href="66-a-summary-of-the-argument.html"><i class="fa fa-check"></i><b>66</b> A summary of the argument</a></li>
<li class="chapter" data-level="67" data-path="67-in-situ-hybridization.html"><a href="67-in-situ-hybridization.html"><i class="fa fa-check"></i><b>67</b> In-situ hybridization</a></li>
<li class="chapter" data-level="68" data-path="68-critical-operational-practice.html"><a href="68-critical-operational-practice.html"><i class="fa fa-check"></i><b>68</b> Critical operational practice?</a></li>
<li class="chapter" data-level="69" data-path="69-obstacles-to-the-work-of-freeing-machine-learning.html"><a href="69-obstacles-to-the-work-of-freeing-machine-learning.html"><i class="fa fa-check"></i><b>69</b> Obstacles to the work of freeing machine learning</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning: Archaeology of a Data Practice</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="vector-space-expansion" class="section level1">
<h1><span class="header-section-number">11</span> Vector space expansion</h1>
<p>To show how this space opens up, we might follow what happens to just one or two columns of the <code>prostate</code> data in the vector space as it is vectorized.  In the <code>prostate</code> dataset, some variables are continuous quantitative values, some are categorical (they represent membership in a group or category) and some are ordinal variables (they represent a ranking or order).    How can different data types be located in vector space? In order to put classifications or categories into vector space, they need to be translated into the same <em>basis</em> as the quantitative variables with their rather more obvious geometrical and linear coordinate values.  How does one geometrically or indeed algebraically render a category or a qualitative difference? The problem is solved via an expansion of the vector space through a form of binary coding that generates a new variable and hence a new dimension for each category:</p>
<blockquote>
<p>Qualitative variables are typically represented numerically by codes. The easiest case is when there are only two classes or categories, such as “success” or “failure,” “survived” or “died.” These are often represented by a single binary digit or bit as 0 or 1, or else by 1 and 1 … When there are more than two categories, several alternatives are available. The most useful and commonly used coding is via dummy variables. Here a K-level qualitative variable is represented by a vector of K binary variables or bits, only one of which is “on” at a time <span class="citation">[@Hastie_2009, 12]</span> </p>
</blockquote>
<p>Again, the details are not so important here as the transformations that the vector space accommodates. A single qualitative or categorical variable expands into ‘a vector of K binary variables or bits.’ Qualitative data, once coded in this way, can be multiplied, added, and in short, handled algebraically using the same aggregate operations applied to numerical or continuous variables. Not only has the vector space expanded here, its expansion smooths over important fault-lines of difference that vertically divided the tabular data. Complex natures become simple natures. The different kinds of variables – qualitative and quantitative, discrete and continuous, nominal and ordinal – can be accommodated by adding dimensions to the vector space.  As Whitehead says, ‘all things are vectors’ <span class="citation">[@Whitehead_1960, 309]</span></p>
<p>Adding dimensions to vector space subsumes differences, but makes seeing the geometrically regular loci – lines, planes, smooth surfaces – in data distributed in this space more challenging. The many transformations in <code>prostate</code> that ensue in <em>Elements of Statistical Learning</em> become the locus of machine learning.  In a historically significant transfiguration of the table, these expansions – and we will see others, including <em>de novo</em> creations of constructed dimensions – subtend differences in a vector space comprising elements defined purely by coordinate position and vectoral (having direction and extent) movement.  Once this hidden, expandable and transformable (by rotation, displacement, or scaling) distribution of elements in space exists, strenuous efforts will be made to bring loci to light. Machine learners search for these loci or or feel for , to use Whitehead’s term,  along different lines. Sometimes a machine learner prehends vector space as filled with constantly varying proximities. It gathers and orders these proximities (for instance, as in the <em>k</em> nearest neighbours model ) or in unsupervised methods such as k-means clustering <span class="citation">[@Hastie_2009, 513]</span>. More commonly, machine learning draws lines or flat surfaces that con-strain the volume. </p>
<p> The importance of lines and flat surfaces can hardly be under-estimated in machine learning. Finding lines of best fit underpins many of the machine learners that attract more attention (neural nets, support vector machines, random forests). Linear regression with its pursuit of the straight line or plane projects the basic alignments of vector space. It renders all differences as distances and directions of movement. Drawing lines or flat surfaces at various angles and directions is perhaps the main way in which the volume of data is traversed, and a relation between input and output, between predictors and prediction, consolidated as a loci or data strain loci or data strain.<a href="#fn22" class="footnoteRef" id="fnref22"><sup>22</sup></a> The line of best fit has a ready generalization to higher dimensions, and a line can be diagrammed in the equations of linear algebra, the field of mathematics that operates on lines in spaces of arbitrary dimensions.  Linear algebra operations exist for finding intersections between lines and planes, for manipulating collections of elements and aggregate forms such matrices  through mappings and transformations (rotations, displacements or translations, skewing, and scaling), and above all, handling whole vector spaces as operational sets. It brings with it a set of formalisations – vector space, dimension, matrix, determinant, coordinate system, linear independence, eigenvectors and eigenvalues, inner-product space, etc. – that machine learners constantly and implicitly resort to invoke.<a href="#fn23" class="footnoteRef" id="fnref23"><sup>23</sup></a> </p>
<p>Many of these operations quickly become difficult to geometrically figure. <a href="#fn24" class="footnoteRef" id="fnref24"><sup>24</sup></a> Let us return to the equations for linear regression models (remembering that both C.S. Pierce and Andrew Ng advocate returning often to equations). The ‘mainstay of statistics,’ the linear regression model, usually appears diagrammatically in a more or less algebraic form:</p>


<p>Equations  and  express a plane (or hyperplane) in increasingly diagrammatic abstraction. The possibility of diagramming a high dimensional space derives largely from linear algebra. Reading Equation  from left to right, the expression <span class="math inline">\(\hat{Y}\)</span> already points to a set of calculated, predicted values, or a vector of <span class="math inline">\(y\)</span> values, such as all the <code>lpsa</code> or PSA readings included in the <code>prostate</code> dataset. Similarly, the term <span class="math inline">\(X_j\)</span> points to the table of all the other variables in the <code>prostate</code> dataset.  Since there are 8 other variables, and close to 100 rows, <span class="math inline">\(X\)</span> is a <em>matrix</em> – a higher dimensional table – of values, addressable by coordinates.  Finally <span class="math inline">\(\beta_j\)</span> are the pivotal coefficients or multiplying quantities that determine the slope or direction of the lines drawn.  The second expression Equation  relies more fully on linear algebra. This is the linear model written in ‘vector form’ <span class="citation">[@Hastie_2009, 11]</span>, or vectorized. The right hand side comprises two operations <span class="math inline">\(X^T\)</span>, the transpose or rotation of the data, and implicitly – multiplication is hardly ever shown, but diagrammed by putting terms alongside each other – an <em>inner product</em> of the <span class="math inline">\(X\)</span> matrix and the <span class="math inline">\(\beta\)</span> parameters (to use model talk) or coefficients (to use linear algebra talk). <a href="#fn25" class="footnoteRef" id="fnref25"><sup>25</sup></a> </p>
<p>The vector form in equation  diagrams an inclined plane that cannot be fully drawn in any figure, only projected perspectivally onto the surface of a graphic plot. While that line can never fully come to light, the diagrammatics of equations  and  express a way of constructing it and orienting it in vector space. Such expressions are epistopic in that they connect the local complex of activities indexed as tabulated data together through the diagonal diagrammatic element of a line or plane angling through vector space.</p>
</div>
<div class="footnotes">
<hr />
<ol start="22">
<li id="fn22"><p>One sign of the centrality of the line in machine learning can be seen, for instance, from the contents page of the book <span class="citation">[@Hastie_2009, xiii-xxii]</span>. After the introduction of the linear model in the first chapter and its initial exposition in chapter 2 (‘overview of supervised learning’), it forms the central topics of chapter 3 (‘linear methods for regression’), chapter 4 (‘linear methods for classification’), chapter 5 (‘basis expansions and regularization’), chapter 6 (‘kernel smoothing methods’), much of chapter 7 (‘model assessment and selection’), chapter 8 (‘model inference and averaging’), major parts of chapter 9 (‘additive models, trees and related methods’), important parts of chapter 11 (‘neural networks’ – neural networks can be understood as a kind of regression model), the anchoring point of chapter 12 (‘support vector machines and flexible discriminants’) and the main focus in the final chapter (‘high dimensional problems’). A similar topic distribution can be found in Andrew Ng’s Cs229 lectures on machine learning. More than half of the 20 lectures concern linear models and their variants. See <span class="citation">[@Ng_2008a; @Ng_2008b;@Ng_2008c;@Ng_2008d]</span>.<a href="11-vector-space-expansion.html#fnref22">↩</a></p></li>
<li id="fn23"><p>Along with statistics and probability, linear algebra is a such an important part of machine learning that many books and courses recommend students complete a linear algebra course before they study machine learning. Cathy O’Neill and Rachel Schutt advise: When you’re developing your skill set as a data scientist, certain foundational pieces need to be in place first—statistics, linear algebra, some programming <span class="citation">[@Schutt_2013, 17]</span><a href="11-vector-space-expansion.html#fnref23">↩</a></p></li>
<li id="fn24"><p>We might add also approach the epistopic fault line in machine learning topologically . Over a decade ago, the cultural theorist Brian Massumi wrote that ‘the space of experience is really, literally, physically a topological hyperspace of transformation’ <span class="citation">[@Massumi_2002, 184]</span> . Much earlier, Gilles Deleuze had conceptualised Michel Foucault’s philosophy as a topology, or ‘thought of the outside’ <span class="citation">[@Deleuze_1988]</span>, as a set of movements that sought to map the diagrams that generated a ‘kind of reality, a new model of truth’ <span class="citation">[@Deleuze_1988, 35]</span>. More recently, this topological thinking has been extended and developed by Celia Lury amongst others. In ‘The Becoming Topological of Culture,’ Lury, Luciana Parisi and Tiziana Terranova suggests that ‘a new rationality is emerging: the moving ratio of a topological culture’ <span class="citation">[@Lury_2012, 4]</span> .   In this new rationality, practices of ordering, modelling, networking and mapping co-constitute culture, technology and science <span class="citation">[@Lury_2012, 5]</span>. At the core of this new rationality, however, lies a new ordering of continuity. The ‘ordering of continuity,’ Lury, Parisi and Terranova propose, takes shape ‘in practices of sorting, naming, numbering, comparing, listing, and calculating’ (4). The phrase ‘ordering of continuity’ is interesting, since we don’t normally think of continuities as subject to ordering. In many ways, that which is continuous bears within it its own ordering, its own immanent seriation or lamination. But in the becoming topological of culture, movement itself undergoes a transformation according to these authors. Rather than movement as something moving from place to place relatively unchanged (as in geometrical translation), movement should be understood as more like an animation, a set of shape-changing operations. These transformations, I would suggest, should be legible in the way that machine learning, almost the epitome of the processes of modelling and calculation that Lury, Parisi and Terranova point to, itself moves through the data. And indeed, the juxtaposition of spam, biomedical data, gene expression data and handwritten digits already suggests that topological equivalences, and a ‘radical expansion’ of comparison might be occurring. Bringing epistopics and topologies together might, I suggest, help trace, map and importantly diagram some of the movements into the data occurring today.<a href="11-vector-space-expansion.html#fnref24">↩</a></p></li>
<li id="fn25"><p>Carl Friedrich Gauss  and Adrien-Marie Legendre’s work on linear regression at this time is well-known. The first independent use of linear regression was Gauss’ prediction of the location of an ‘occluded volume,’ the position of the asteroid Ceres after it reappeared in its orbit behind the sun. <span class="citation">[@Stigler_2002]</span> – TBA page ref<a href="11-vector-space-expansion.html#fnref25">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="10-surface-and-depths-the-problem-of-volume-in-data.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="12-drawing-lines-in-a-common-space-of-transformation.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
