<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Machine Learning: Archaeology of a Data Practice</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="Machine Learning: Archaeology of a Data Practice">
  <meta name="generator" content="bookdown 0.3 and GitBook 2.6.7">

  <meta property="og:title" content="Machine Learning: Archaeology of a Data Practice" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Machine Learning: Archaeology of a Data Practice" />
  
  
  

<meta name="author" content="Adrian Mackenzie">


<meta name="date" content="2017-03-25">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="4-diagramming-machines.html">
<link rel="next" href="6-machines-finding-functions.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="1-acknowledgments.html"><a href="1-acknowledgments.html"><i class="fa fa-check"></i><b>1</b> Acknowledgments</a></li>
<li class="chapter" data-level="2" data-path="2-preface.html"><a href="2-preface.html"><i class="fa fa-check"></i><b>2</b> Preface</a></li>
<li class="chapter" data-level="3" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html"><i class="fa fa-check"></i><b>3</b> Introduction: Into the Data</a><ul>
<li class="chapter" data-level="3.1" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#three-accumulations-settings-data-and-devices"><i class="fa fa-check"></i><b>3.1</b> Three accumulations: settings, data and devices</a></li>
<li class="chapter" data-level="3.2" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#who-or-what-is-a-machine-learner"><i class="fa fa-check"></i><b>3.2</b> Who or what is a machine learner?</a></li>
<li class="chapter" data-level="3.3" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#algorithmic-control-to-the-machine-learners"><i class="fa fa-check"></i><b>3.3</b> Algorithmic control to the machine learners?</a></li>
<li class="chapter" data-level="3.4" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#the-archaeology-of-operations"><i class="fa fa-check"></i><b>3.4</b> The archaeology of operations</a></li>
<li class="chapter" data-level="3.5" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#asymmetries-in-common-knowledge"><i class="fa fa-check"></i><b>3.5</b> Asymmetries in common knowledge</a></li>
<li class="chapter" data-level="3.6" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#what-cannot-be-automated"><i class="fa fa-check"></i><b>3.6</b> What cannot be automated?</a></li>
<li class="chapter" data-level="3.7" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#different-fields-in-machine-learning"><i class="fa fa-check"></i><b>3.7</b> Different fields in machine learning?</a></li>
<li class="chapter" data-level="3.8" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#the-diagram-in-critical-thought"><i class="fa fa-check"></i><b>3.8</b> The diagram in critical thought</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html"><i class="fa fa-check"></i><b>4</b> Diagramming machines</a><ul>
<li class="chapter" data-level="4.1" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#we-dont-have-to-write-programs"><i class="fa fa-check"></i><b>4.1</b> ‘We don’t have to write programs’?</a></li>
<li class="chapter" data-level="4.2" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#the-elements-of-machine-learning"><i class="fa fa-check"></i><b>4.2</b> The elements of machine learning</a></li>
<li class="chapter" data-level="4.3" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#who-reads-machine-learning-textbooks"><i class="fa fa-check"></i><b>4.3</b> Who reads machine learning textbooks?</a></li>
<li class="chapter" data-level="4.4" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#r-a-matrix-of-transformations"><i class="fa fa-check"></i><b>4.4</b> <code>R</code>: a matrix of transformations</a></li>
<li class="chapter" data-level="4.5" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#the-obdurate-mathematical-glint-of-machine-learning"><i class="fa fa-check"></i><b>4.5</b> The obdurate mathematical glint of machine learning</a></li>
<li class="chapter" data-level="4.6" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#cs229-2007-returning-again-and-again-to-certain-features"><i class="fa fa-check"></i><b>4.6</b> CS229, 2007: returning again and again to certain features</a></li>
<li class="chapter" data-level="4.7" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#the-visible-learning-of-machine-learning"><i class="fa fa-check"></i><b>4.7</b> The visible learning of machine learning</a></li>
<li class="chapter" data-level="4.8" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#the-diagram-of-an-operational-formation"><i class="fa fa-check"></i><b>4.8</b> The diagram of an operational formation</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html"><i class="fa fa-check"></i><b>5</b> Vectorisation and its consequences</a><ul>
<li class="chapter" data-level="5.1" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html#vector-space-and-geometry"><i class="fa fa-check"></i><b>5.1</b> Vector space and geometry</a></li>
<li class="chapter" data-level="5.2" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html#mixing-places"><i class="fa fa-check"></i><b>5.2</b> Mixing places</a></li>
<li class="chapter" data-level="5.3" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html#truth-is-no-longer-in-the-table"><i class="fa fa-check"></i><b>5.3</b> Truth is no longer in the table?</a></li>
<li class="chapter" data-level="5.4" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html#the-epistopic-fault-line-in-tables"><i class="fa fa-check"></i><b>5.4</b> The epistopic fault line in tables</a></li>
<li class="chapter" data-level="5.5" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html#surface-and-depths-the-problem-of-volume-in-data"><i class="fa fa-check"></i><b>5.5</b> Surface and depths: the problem of volume in data</a></li>
<li class="chapter" data-level="5.6" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html#vector-space-expansion"><i class="fa fa-check"></i><b>5.6</b> Vector space expansion</a></li>
<li class="chapter" data-level="5.7" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html#drawing-lines-in-a-common-space-of-transformation"><i class="fa fa-check"></i><b>5.7</b> Drawing lines in a common space of transformation</a></li>
<li class="chapter" data-level="5.8" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html#implicit-vectorization-in-code-and-infrastructures"><i class="fa fa-check"></i><b>5.8</b> Implicit vectorization in code and infrastructures</a></li>
<li class="chapter" data-level="5.9" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html#lines-traversing-behind-the-light"><i class="fa fa-check"></i><b>5.9</b> Lines traversing behind the light</a></li>
<li class="chapter" data-level="5.10" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html#the-vectorised-table"><i class="fa fa-check"></i><b>5.10</b> The vectorised table?</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html"><i class="fa fa-check"></i><b>6</b> Machines finding functions</a><ul>
<li class="chapter" data-level="6.1" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html#learning-functions"><i class="fa fa-check"></i><b>6.1</b> Learning functions</a></li>
<li class="chapter" data-level="6.2" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html#supervised-unsupervised-reinforcement-learning-and-functions"><i class="fa fa-check"></i><b>6.2</b> Supervised, unsupervised, reinforcement learning and functions</a></li>
<li class="chapter" data-level="6.3" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html#which-function-operates"><i class="fa fa-check"></i><b>6.3</b> Which function operates?</a></li>
<li class="chapter" data-level="6.4" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html#what-does-a-function-learn"><i class="fa fa-check"></i><b>6.4</b> What does a function learn?</a></li>
<li class="chapter" data-level="6.5" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html#observing-with-curves-the-logistic-function"><i class="fa fa-check"></i><b>6.5</b> Observing with curves: the logistic function</a></li>
<li class="chapter" data-level="6.6" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html#the-cost-of-curves-in-machine-learning"><i class="fa fa-check"></i><b>6.6</b> The cost of curves in machine learning</a></li>
<li class="chapter" data-level="6.7" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html#curves-and-the-variation-in-models"><i class="fa fa-check"></i><b>6.7</b> Curves and the variation in models</a></li>
<li class="chapter" data-level="6.8" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html#observing-costs-losses-and-objectives-through-optimisation"><i class="fa fa-check"></i><b>6.8</b> Observing costs, losses and objectives through optimisation</a></li>
<li class="chapter" data-level="6.9" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html#gradients-as-partial-observers"><i class="fa fa-check"></i><b>6.9</b> Gradients as partial observers</a></li>
<li class="chapter" data-level="6.10" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html#the-power-to-learn"><i class="fa fa-check"></i><b>6.10</b> The power to learn</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="7-probabilisation-and-the-taming-of-machines.html"><a href="7-probabilisation-and-the-taming-of-machines.html"><i class="fa fa-check"></i><b>7</b> Probabilisation and the Taming of Machines}</a><ul>
<li class="chapter" data-level="7.1" data-path="7-probabilisation-and-the-taming-of-machines.html"><a href="7-probabilisation-and-the-taming-of-machines.html#data-reduces-uncertainty"><i class="fa fa-check"></i><b>7.1</b> Data reduces uncertainty?</a></li>
<li class="chapter" data-level="7.2" data-path="7-probabilisation-and-the-taming-of-machines.html"><a href="7-probabilisation-and-the-taming-of-machines.html#machine-learning-as-statistics-inside-out"><i class="fa fa-check"></i><b>7.2</b> Machine learning as statistics inside out</a></li>
<li class="chapter" data-level="7.3" data-path="7-probabilisation-and-the-taming-of-machines.html"><a href="7-probabilisation-and-the-taming-of-machines.html#distributed-probabilities"><i class="fa fa-check"></i><b>7.3</b> Distributed probabilities</a></li>
<li class="chapter" data-level="7.4" data-path="7-probabilisation-and-the-taming-of-machines.html"><a href="7-probabilisation-and-the-taming-of-machines.html#naive-bayes-and-the-distribution-of-probabilities"><i class="fa fa-check"></i><b>7.4</b> Naive Bayes and the distribution of probabilities</a></li>
<li class="chapter" data-level="7.5" data-path="7-probabilisation-and-the-taming-of-machines.html"><a href="7-probabilisation-and-the-taming-of-machines.html#spam-when-foralln-is-too-much"><i class="fa fa-check"></i><b>7.5</b> Spam: when <span class="math inline">\(\forall{N}\)</span> is too much?</a></li>
<li class="chapter" data-level="7.6" data-path="7-probabilisation-and-the-taming-of-machines.html"><a href="7-probabilisation-and-the-taming-of-machines.html#the-improbable-success-of-the-naive-bayes-classifier"><i class="fa fa-check"></i><b>7.6</b> The improbable success of the Naive Bayes classifier</a></li>
<li class="chapter" data-level="7.7" data-path="7-probabilisation-and-the-taming-of-machines.html"><a href="7-probabilisation-and-the-taming-of-machines.html#ancestral-probabilities-in-documents-inference-and-prediction"><i class="fa fa-check"></i><b>7.7</b> Ancestral probabilities in documents: inference and prediction</a></li>
<li class="chapter" data-level="7.8" data-path="7-probabilisation-and-the-taming-of-machines.html"><a href="7-probabilisation-and-the-taming-of-machines.html#statistical-decompositions-bias-variance-and-observed-errors"><i class="fa fa-check"></i><b>7.8</b> Statistical decompositions: bias, variance and observed errors</a></li>
<li class="chapter" data-level="7.9" data-path="7-probabilisation-and-the-taming-of-machines.html"><a href="7-probabilisation-and-the-taming-of-machines.html#does-machine-learning-construct-a-new-statistical-reality"><i class="fa fa-check"></i><b>7.9</b> Does machine learning construct a new statistical reality?</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="8-patterns-and-differences.html"><a href="8-patterns-and-differences.html"><i class="fa fa-check"></i><b>8</b> Patterns and differences</a><ul>
<li class="chapter" data-level="8.1" data-path="8-patterns-and-differences.html"><a href="8-patterns-and-differences.html#splitting-and-the-growth-of-trees"><i class="fa fa-check"></i><b>8.1</b> Splitting and the growth of trees</a></li>
<li class="chapter" data-level="8.2" data-path="8-patterns-and-differences.html"><a href="8-patterns-and-differences.html#differences-in-recursive-partitioning"><i class="fa fa-check"></i><b>8.2</b> 1984: Differences in recursive partitioning</a></li>
<li class="chapter" data-level="8.3" data-path="8-patterns-and-differences.html"><a href="8-patterns-and-differences.html#limiting-differences"><i class="fa fa-check"></i><b>8.3</b> Limiting differences</a></li>
<li class="chapter" data-level="8.4" data-path="8-patterns-and-differences.html"><a href="8-patterns-and-differences.html#the-successful-dispersion-of-the-support-vector-machine"><i class="fa fa-check"></i><b>8.4</b> The successful dispersion of the support vector machine</a></li>
<li class="chapter" data-level="8.5" data-path="8-patterns-and-differences.html"><a href="8-patterns-and-differences.html#differences-blur"><i class="fa fa-check"></i><b>8.5</b> Differences blur?</a></li>
<li class="chapter" data-level="8.6" data-path="8-patterns-and-differences.html"><a href="8-patterns-and-differences.html#bending-the-decision-boundary"><i class="fa fa-check"></i><b>8.6</b> Bending the decision boundary</a></li>
<li class="chapter" data-level="8.7" data-path="8-patterns-and-differences.html"><a href="8-patterns-and-differences.html#instituting-patterns"><i class="fa fa-check"></i><b>8.7</b> Instituting patterns</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="9-regularizing-and-materializing-objects.html"><a href="9-regularizing-and-materializing-objects.html"><i class="fa fa-check"></i><b>9</b> Regularizing and materializing objects}</a><ul>
<li class="chapter" data-level="9.1" data-path="9-regularizing-and-materializing-objects.html"><a href="9-regularizing-and-materializing-objects.html#genomic-referentiality-and-materiality"><i class="fa fa-check"></i><b>9.1</b> Genomic referentiality and materiality</a></li>
<li class="chapter" data-level="9.2" data-path="9-regularizing-and-materializing-objects.html"><a href="9-regularizing-and-materializing-objects.html#the-genome-as-threshold-object"><i class="fa fa-check"></i><b>9.2</b> The genome as threshold object</a></li>
<li class="chapter" data-level="9.3" data-path="9-regularizing-and-materializing-objects.html"><a href="9-regularizing-and-materializing-objects.html#genomic-knowledges-and-their-datasets"><i class="fa fa-check"></i><b>9.3</b> Genomic knowledges and their datasets</a></li>
<li class="chapter" data-level="9.4" data-path="9-regularizing-and-materializing-objects.html"><a href="9-regularizing-and-materializing-objects.html#the-advent-of-wide-dirty-and-mixed-data"><i class="fa fa-check"></i><b>9.4</b> The advent of ‘wide, dirty and mixed’ data</a></li>
<li class="chapter" data-level="9.5" data-path="9-regularizing-and-materializing-objects.html"><a href="9-regularizing-and-materializing-objects.html#cross-validating-machine-learning-in-genomics"><i class="fa fa-check"></i><b>9.5</b> Cross-validating machine learning in genomics</a></li>
<li class="chapter" data-level="9.6" data-path="9-regularizing-and-materializing-objects.html"><a href="9-regularizing-and-materializing-objects.html#proliferation-of-discoveries"><i class="fa fa-check"></i><b>9.6</b> Proliferation of discoveries</a></li>
<li class="chapter" data-level="9.7" data-path="9-regularizing-and-materializing-objects.html"><a href="9-regularizing-and-materializing-objects.html#variations-in-the-object-or-in-the-machine-learner"><i class="fa fa-check"></i><b>9.7</b> Variations in the object or in the machine learner?</a></li>
<li class="chapter" data-level="9.8" data-path="9-regularizing-and-materializing-objects.html"><a href="9-regularizing-and-materializing-objects.html#whole-genome-functions"><i class="fa fa-check"></i><b>9.8</b> Whole genome functions</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="10-propagating-subject-positions.html"><a href="10-propagating-subject-positions.html"><i class="fa fa-check"></i><b>10</b> Propagating subject positions}</a><ul>
<li class="chapter" data-level="10.1" data-path="10-propagating-subject-positions.html"><a href="10-propagating-subject-positions.html#propagation-across-human-machine-boundaries"><i class="fa fa-check"></i><b>10.1</b> Propagation across human-machine boundaries</a></li>
<li class="chapter" data-level="10.2" data-path="10-propagating-subject-positions.html"><a href="10-propagating-subject-positions.html#competitive-positioning"><i class="fa fa-check"></i><b>10.2</b> Competitive positioning</a></li>
<li class="chapter" data-level="10.3" data-path="10-propagating-subject-positions.html"><a href="10-propagating-subject-positions.html#a-privileged-machine-and-its-diagrammatic-forms"><i class="fa fa-check"></i><b>10.3</b> A privileged machine and its diagrammatic forms</a></li>
<li class="chapter" data-level="10.4" data-path="10-propagating-subject-positions.html"><a href="10-propagating-subject-positions.html#varying-subject-positions-in-code"><i class="fa fa-check"></i><b>10.4</b> Varying subject positions in code</a></li>
<li class="chapter" data-level="10.5" data-path="10-propagating-subject-positions.html"><a href="10-propagating-subject-positions.html#the-subjects-of-a-hidden-operation"><i class="fa fa-check"></i><b>10.5</b> The subjects of a hidden operation</a></li>
<li class="chapter" data-level="10.6" data-path="10-propagating-subject-positions.html"><a href="10-propagating-subject-positions.html#algorithms-that-propagate-errors"><i class="fa fa-check"></i><b>10.6</b> Algorithms that propagate errors</a></li>
<li class="chapter" data-level="10.7" data-path="10-propagating-subject-positions.html"><a href="10-propagating-subject-positions.html#competitions-as-examination"><i class="fa fa-check"></i><b>10.7</b> Competitions as examination</a></li>
<li class="chapter" data-level="10.8" data-path="10-propagating-subject-positions.html"><a href="10-propagating-subject-positions.html#superimposing-power-and-knowledge"><i class="fa fa-check"></i><b>10.8</b> Superimposing power and knowledge</a></li>
<li class="chapter" data-level="10.9" data-path="10-propagating-subject-positions.html"><a href="10-propagating-subject-positions.html#ranked-subject-positions"><i class="fa fa-check"></i><b>10.9</b> Ranked subject positions</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="11-conclusion-out-of-the-data.html"><a href="11-conclusion-out-of-the-data.html"><i class="fa fa-check"></i><b>11</b> Conclusion: Out of the Data}</a><ul>
<li class="chapter" data-level="11.1" data-path="11-conclusion-out-of-the-data.html"><a href="11-conclusion-out-of-the-data.html#machine-learners"><i class="fa fa-check"></i><b>11.1</b> 250,000 machine learners</a></li>
<li class="chapter" data-level="11.2" data-path="11-conclusion-out-of-the-data.html"><a href="11-conclusion-out-of-the-data.html#a-summary-of-the-argument"><i class="fa fa-check"></i><b>11.2</b> A summary of the argument</a></li>
<li class="chapter" data-level="11.3" data-path="11-conclusion-out-of-the-data.html"><a href="11-conclusion-out-of-the-data.html#in-situ-hybridization"><i class="fa fa-check"></i><b>11.3</b> In-situ hybridization</a></li>
<li class="chapter" data-level="11.4" data-path="11-conclusion-out-of-the-data.html"><a href="11-conclusion-out-of-the-data.html#critical-operational-practice"><i class="fa fa-check"></i><b>11.4</b> Critical operational practice?</a></li>
<li class="chapter" data-level="11.5" data-path="11-conclusion-out-of-the-data.html"><a href="11-conclusion-out-of-the-data.html#obstacles-to-the-work-of-freeing-machine-learning"><i class="fa fa-check"></i><b>11.5</b> Obstacles to the work of freeing machine learning</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="12-references.html"><a href="12-references.html"><i class="fa fa-check"></i><b>12</b> References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning: Archaeology of a Data Practice</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="vectorisation-and-its-consequences" class="section level1">
<h1><span class="header-section-number">5</span> Vectorisation and its consequences</h1>
<p></p>
<blockquote>
<p>All things are vectors <span class="citation">(Whitehead <a href="#ref-Whitehead_1960">1960</a>, 309)</span></p>
</blockquote>
<p>The operational power of machine learning locates data practice in an expanding epistemic space. The space derives, I will suggest, from a specific operational diagram that maps data into a . It  data according to axes, coordinates, and scales. Machine learning, in turn, inhabits a vectorised space, and its operations vectorise data. </p>
<p>Often data is represented as an homogeneous mass or a continuous flowing stream. My aim here, however, is to archaeologically examine some of the transformations that allow different shapes and densities of data, whether in the form of numbers, words or images, to become machine learnable. Data in its local complexes spaces out in many different density shapes, depending on how the data has been generated or instanced.<a href="#fn16" class="footnoteRef" id="fnref16"><sup>16</sup></a>  Whatever the starting point – a measuring instrument, people clicking and typing on websites, a device like a camera, a random number generator, etc. – machine learners only ever encounter data in specific <em>vectorised</em> shapes (vectors, matrices, arrays, etc.), mapped to a geometrically coordinate volume. The mapping and forming, when mentioned at all, is sometimes referred to as ‘data cleaning’ but that term covers over important but largely taken for granted and constitutive transformations.  The archaeology of data shapes presented in this chapter explores a range of transformations focused around the table or row-column grid. </p>
<p>The reshaping and re-flowing of data densities into vectors deeply affects machine learning. This forming and reforming of data is evidence of implicated relations. The philosopher A.N. Whitehead called ‘strain’:</p>
<blockquote>
<p>a feeling in which the forms exemplified in the datum concern geometrical, straight, and flat loci will be called a “strain.” In a strain, qualitative elements, other than the geometrical forms, express themselves as qualities implicated in those forms <span class="citation">(Whitehead <a href="#ref-Whitehead_1960">1960</a>, 310)</span>.  </p>
</blockquote>
<p>In many machine learning models the exemplified forms are straight or flat loci (as we see in chapter <a href="#ch:function"><strong>??</strong></a>). Yet different practices also elicit relations that strain the linear shaping of data and these divergent relations sometimes combine in generative and provocative ways.</p>
<div id="vector-space-and-geometry" class="section level2">
<h2><span class="header-section-number">5.1</span> Vector space and geometry</h2>
<p>Statistical modelling, data-mining, pattern recognition, recommendation systems, network modelling and machine learning rely very much on the operation called ‘fitting a model.’  Fitting as a spatial practice has some elements that resemble the phenomenologist Edmund Husserl’s account of the origin of geometry. Husserl writes:</p>
<blockquote>
<p>First to be singled out from the thing-shapes are surfaces – more or less “smooth,” more or less perfect surfaces; edges, more or less rough or fairly “even”; in other words, more or less pure lines, angles, more or less perfect points; then again, among the lines, for examples, straight lines are especially preferred, and among surfaces, the even surfaces. … Thus the production of even surfaces and their perfection (polishing) always plays its role in praxis <span class="citation">(Derrida <a href="#ref-Derrida_1989">1989</a>, 178)</span> </p>
</blockquote>
<p>Husserl here refers is attempting to describe something of the way in which forms such as planes, lines, circles, triangles, squares, and points became objects of geometrical practice. A similar polishing and smoothing of surfaces is certainly taking place today in the thing-shapes we call data. The basic machine learning work of ‘fitting a model’ (or many models) to data is often literally implemented, as we will see, by constraining data within a coordinate, discretized space, which I term the  </p>
<p>Critical thought from phenomenology to social theory has a long-standing nervousness about the power of geometry and its gradual movement away from shapes and things towards mathematical operations. The philosopher Hannah Arendt, for instance, observes:</p>
<blockquote>
<p>decisive is the entirely un-Platonic subjection of geometry to algebraic treatment, which discloses the modern idea of reducing terrestrial sense data and movements to mathematical symbols <span class="citation">(Arendt <a href="#ref-Arendt_1998">1998</a>, 265)</span> </p>
</blockquote>
<p>The crux of the problem rests on the ‘treatment’ or operations that ‘reduce terrestrial sense data and movements’ to symbols. One challenge for contemporary thought is how to orient itself to such operations, particularly in their data-intensive forms, without assuming that the familiar story of scientific and technical reduction of sense and movement to the lines and planes of modern geometry is simply reinforced in contemporary data practice.  If an archaeology of data vectorization does anything, it needs to offer an alternative to that reduction.</p>
<p>They also, as we will see, reach down into the practices of programming, infrastructure and hardware production in ways that differ somewhat from increases in computational power or speed. Familiar narratives of Moore’s Law increases in speed or efficiency of computation do not account for transformations of data in <code>R</code> and other computing environments (for instance, the popular <code>Map-Reduce</code> architecture invented at Google Corporation to speed up its search engine services <span class="citation">(Mackenzie <a href="#ref-Mackenzie_2011">2011</a>)</span> ). Vectorisation transforms data along more diagrammatic lines.<a href="#fn17" class="footnoteRef" id="fnref17"><sup>17</sup></a></p>
</div>
<div id="mixing-places" class="section level2">
<h2><span class="header-section-number">5.2</span> Mixing places</h2>

<p>Data appears in <em>Elements of Statistical Learning</em> in multiple forms.  Maps of New Zealand fishing patterns lie next to plots of factors in South African heart disease. The 21 datasets shown in table @ref(tab:datasets_elemstatlearn) typify the variety found in <span class="citation">(Hastie, Tibshirani, and Friedman <a href="#ref-Hastie_2009">2009</a>)</span>. They span scientific, clinical, commercial and media fields. Note that they include many patches and pathways of everyday life – speaking, seeing, writing and reading – as well as specific scientific objects of knowledge such as galaxies, cancer, climate and national economies. This mixture is not unusual for machine learners. To give another example, the statistician Leo Breiman’s 2001 article on random forests <span class="citation">(Breiman <a href="#ref-Breiman_2001">2001</a><a href="#ref-Breiman_2001">b</a>)</span>,  perhaps the most cited journal paper in the machine learning literature, displays a similarly diagonal line across human and non-human worlds: </p>
<blockquote>
<p>Glass, Breast cancer, Diabetes, Sonar, Vowel, Ionosphere, Vehicle, Soybean, German credit, Image, Ecoli, Votes, Liver, Letters, Sat-images, Zip-code, Waveform, Twonorm, Threenorm, Ringnorm, <span class="citation">(Breiman <a href="#ref-Breiman_2001">2001</a><a href="#ref-Breiman_2001">b</a>, 12)</span>.</p>
</blockquote>
<p>In some ways, the improbable conjunction of spam email and cancer detection in machine learning continues what statistics as a field has always done: rove across scattered fields ranging from astronomy to statecraft, from zoology to epidemiology, gleaning data as it goes (see <span class="citation">(Stigler <a href="#ref-Stigler_1986">1986</a>; Hacking <a href="#ref-Hacking_1990">1990</a>)</span> for samples of itineraries). </p>
<p>In <em>Elements of Statistical Learning</em> and the field of machine learning more generally, something more is moving through and coordinating these datasets. The coordination might be rhetorical: the colligation of datasets – vowels, ozone, bone density, marketing, prostate cancer, and spam – in all their diversity suggests the mobility of machine learners. The combination of datasets deriving from network media, from medicine, from business administration and from cutting-edge life science (c.2000) suggests a tremendous, indeed almost spectacular miscibility, one that in principle could surprise us because there is otherwise little mixing between the settings and knowledge domains these datasets come from. How is this mixing, conformation and homogenisation being done? The repetition of data sets, the juxtaposition of discontinuous domains, and forms of movement construct and order continuities in the service of various forms of predictive and inferential knowledge. The miscible juxtapositions we encounter in machine learning enter into, it seems, a <em>regularity</em> or a common space, a space that displays strong tendencies to expand, accumulate and archive relations. Rather than peripatetic learning, the accumulation of diverse datasets attests to a prior ordering of data to afford its traversal.</p>
<p>The practices of naming and ordering, the sorting of different data types, the addressing and expansion of data exemplified in these diverse datasets can tell us a lot about how machine learning learns from data.  The shapes, compositions and loci formed from the datasets enable the functioning of machine learners as they operate to generate statements, classifications and decisions. If machine learning can be understood as a constantly evolving diagram of practical abstractions, the way it draws on and relates to different forms of data matter. As we will see, machine learners transpose data in an increasingly extensive, heavily coordinated space, the vector space.</p>
</div>
<div id="truth-is-no-longer-in-the-table" class="section level2">
<h2><span class="header-section-number">5.3</span> Truth is no longer in the table?</h2>
<p>‘This book is about learning from data’ write Hastie, Tibshirani and Friedman on the first page of <em>Elements of Statistical Learning</em>, as they rapidly begin to iterate through some datasets.  On the second page of the book, a table of spam email word frequencies appears (and the problem of spam classification is canonical in the machine learning literature - we return to in chapter <a href="#ch:probability"><strong>??</strong></a>).  They come from the dataset <code>spam</code> <span class="citation">(Cranor and LaMacchia <a href="#ref-Cranor_1998">1998</a>)</span>. On the third page, a complicated data graphic appears (Figure 1.1, <span class="citation">(Hastie, Tibshirani, and Friedman <a href="#ref-Hastie_2009">2009</a>, 3)</span>. It is a scatterplot matrix of the <code>prostate</code> dataset included in the <code>R</code> package <code>ElemStatLearn,</code> the companion <code>R</code> package for the book. In a third example, a set of scanned handwritten numbers appears. These scans are images of zipcode or postcode numbers written on postal envelopes taken from the dataset <code>zip</code> <span class="citation">(LeCun and Cortes <a href="#ref-LeCun_2012">2012</a>)</span>, and they differ from both the <code>spam</code> table and <code>prostate</code> plots because they directly resemble something in the world, which, however, happens to be numbers, and is, therefore, probably already recruited into data-making and data-circulating processes (a dataset we return to in chapter ). The final example in the introduction, ‘Example 4: DNA Expression Microarrays,’ draws from biology, and particularly, high-throughput genomic biology, a science that produces large amounts of data about biological processes by running many tests, or by constructing devices that generate many measurements, in this case, a DNA microarray.<a href="#fn18" class="footnoteRef" id="fnref18"><sup>18</sup></a></p>
<p>The table or the row-column addressable grid is common to all of these datasets. And yet, as we are about to see, machine learning in many ways deals with the collapse or liquidation of tabular datasets. ‘Things, in their fundamental truth,’ writes Foucault in <em>The Order of Things</em> ‘have now escaped from the space of the table’ <span class="citation">(Foucault 1992 [1966], 239)</span>.   Foucault writes in these pages about the fabled emergence of life, labour and language as the anchoring vertexes of a new triangle of knowledge and power structuring the figure of the ‘human’ in the 19th century.</p>
<p>Before the emergence of the characteristic sciences of the human – political economy, linguistics and biology – knowledges such as natural history, the general grammars, and philosophies of wealth (such as Adam Smith’s work) had ordered empirical materials of diverse provenance in tables or grids. While the history of tables as data forms reaches a long way back (see <span class="citation">(Marchese <a href="#ref-Marchese_2013">2013</a>)</span> for a broad historical overview that reaches back to Mesopotamia), Foucault argues that the Classical age first developed the system of grids that permitted ranking, sorting and ordering in tables. These grids replace the Renaissance tabulations based on ‘buried similitudes’ and ‘invisible analogies’ <span class="citation">(Foucault 1992 [1966], 26)</span>. In pre-Classical tables, an image or figure from myth might lie alongside a measurement or a count of occurrences, and this proximity was ordered by systems of analogical association that spanned what we might today, in the wake of the 19th century, might see as incongruous (for instance, associations between medicine and Biblical prophecy).</p>
<p>The Classical Age grid or table, by contrast, brought plural and diverse resemblances into exhaustive systematic visible enumeration. As Foucault puts it:</p>
<blockquote>
<p>The space of order, which served as a <em>common place</em> for representation and for things, for empirical visibility and for the essential rules, which united the regularities of nature and the resemblances of imagination in the grid of identities and differences, which displayed the empirical sequence of representations in a simultaneous table, and made it possible to scan step by step, in accordance with a logical sequence, the totality of nature’s elements thus rendered contemporaneous with one another <span class="citation">(Foucault <a href="#ref-Foucault_1972">1972</a>, 239)</span></p>
</blockquote>
<p>The table as space of order did not stand in isolation. It served a localized epistemic function in conjunction with other of knowledge such as experiment and mathematical proof. Since algebra or experimental <em>mathesis</em> only applied to ‘simple natures’ (planets in movement, dynamics of falling bodies, etc.),  table-based knowledges such as taxonomy dealt with more complex natures. In the tables, systems of signs – for instance, the groupings established by the eighteenth century taxonomist Carl Linnaeus – sought to reduce complex natures (plants, animals, etc.) to simpler forms as columns and rows in a table based on resemblances and similarities.   Importantly, the table as space of order was a space of imagination in that one could begin to see continuities and differences between things (organisms, words, nations) by carefully ordering and scanning the table. ‘Hedged in by calculus and genesis,’ Foucault suggest, ‘we have the area of the <em>table</em>’ <span class="citation">(Foucault 1992 [1966], 73)</span>.  Note in passing that calculus and calculations bound the table only in relation to ‘simple natures’ whose identity and difference can be understood in the form of movements, rates and change in position.<a href="#fn19" class="footnoteRef" id="fnref19"><sup>19</sup></a> This is an important limitation since it is precisely the complex natures in genesis that machine learning tries to engage using algebra, calculus, statistics and computation.</p>
<p>Finally (at least for our purposes), in the nineteenth century, a different form of ordering shattered tabulation based on enumerated similarity and ordered resemblance. Foucault figures this change as shattering the table into shards of order:</p>
<blockquote>
<p>this space of order is from now on shattered: there will be things, with their own organic structures, their hidden veins, the spaces that articulates them, the time that produces them; and then representation, a purely temporal succession, in which those things address themselves (always partially) to a subjectivity <span class="citation">(Foucault 1992 [1966], 239–40)</span>.</p>
</blockquote>
<p>Life, labour and language – Foucault’s famous historically emergent triadic figure of the human – replace the enumerative, synoptic classificatory tables of the Classical age. Tables still abound in newer temporal, genetic orderings (almost any episode from the history of nineteenth and twentieth century statistics will confirm that; see <span class="citation">(Stigler <a href="#ref-Stigler_1986">1986</a>; Stigler <a href="#ref-Stigler_2002">2002</a>)</span>), but from now on tables are localized, relating to a place and changing in time, and functioning as shards of representational order addressed to (and often constitutive) of subject position.  A double interiority takes over. Things such as a language, a species or an economic system have their own genesis, their own temporality and historical existence. Our knowledges and indeed experience of them also become finite, historical, with their own dynamics and internal life. In this change, the table itself is no longer the foundation or distillation of knowledge. It is one knowledge apparatus amongst many.  Tables, we might say, become merely data, inscriptions pendant on hidden structures and their genesis. </p>
<p>This brief résumé of a thread of argument in <em>The Order of Things</em> might help us reassess how machine learning goes into the data. The tables of <code>spam</code>, <code>prostate</code>, <code>image</code> and <code>microarray</code> data in contemporary machine learning do not operate in the way Linnaeus would have tabulated a table of living things based on similarities and resemblances or a Renaissance medical textbook might assemble analogical resemblances between disease and astronomy. As I will argue, measures of similarity and resemblance still operate strongly in machine learning as it moves through tables. In this respect, the Classical table and perhaps even the pre-Classical table returns with extended relevance. The repeated juxtaposition of tables of diverse provenance alongside each other, the incorporation of ‘complex natures,’ and the operational superimposition of different tables suggests machine learners still suggest synoptic alignments, but along different lines than the row-column grid of the Renaissance, Classical or even the later life-labour-language tables of the human sciences. Even if the tables in the opening pages of <em>Elements of Statistical Learning</em> concern work, life, language and economy and map very readily onto the anchor points, the new ‘empiricities’ (Foucault’s term for the empirical problems), of labour, life and language or biopower  that took root at this time <span class="citation">(Foucault <a href="#ref-Foucault_1991">1991</a>)</span>, the ways machine learners traverse them may not be recognisable in terms of these empiricities. </p>
<p>Instead, we are now confronted by kaleidoscopically transmuted tables whose expansion and open margins afford many formulations of similarity and difference. Already in the handwritten digits and the microarray data, scale and dimensions thwart tabular display of the data. In settings such as social media platforms or genomics in which machine learning operates, tables change rapidly in scale and sometimes in organisation. The multiplication and juxtaposition of tables suggests that we might be seeing the advent of a post-order space for regularities and resemblances, for simple and complex natures, encompassing knowledges ranging from humanities to traffic engineering.</p>
</div>
<div id="the-epistopic-fault-line-in-tables" class="section level2">
<h2><span class="header-section-number">5.4</span> The epistopic fault line in tables</h2>
<pre><code>##  [1] &quot;SAheart&quot;         &quot;bone&quot;            &quot;countries&quot;      
##  [4] &quot;galaxy&quot;          &quot;marketing&quot;       &quot;mixture.example&quot;
##  [7] &quot;nci&quot;             &quot;orange10.test&quot;   &quot;orange10.train&quot; 
## [10] &quot;orange4.test&quot;    &quot;orange4.train&quot;   &quot;ozone&quot;          
## [13] &quot;phoneme&quot;         &quot;prostate&quot;        &quot;spam&quot;           
## [16] &quot;vowel.test&quot;      &quot;vowel.train&quot;     &quot;waveform.test&quot;  
## [19] &quot;waveform.train&quot;  &quot;zip.test&quot;        &quot;zip.train&quot;</code></pre>
<p>The <code>ElemStatLearn</code> <code>R</code> package brings, as we have seen in the previous chapter, with it around 20 different datasets, including the four mentioned in the introduction to <em>Elements of Statistical Learning</em>. On the one hand, every data table indexes a localised complex of activities (clinical research, social media platform, financial transactions, etc.) with possible referential importance. On the other hand, for machine learners, the table is a space of potential similarities and differences both internal to the table itself (e.g. how much does row number 1000 differ from row 1,000,000) and associated with other tables (e.g. how much does this table of clinical test relate to that table of microarray data?).  These internal and external differences entwine with each other in ways that create a fault line, an unstable yet generative line of diagonal movement. It is this fault or fold line, I propose, that diagrammatically distributes data tables into the expansive and moving substrata of the vector space. </p>
<p>The table has been vectorised. We might say that the vectorization of the table is epistopic. The term ‘epistopic’  comes from the work of the science studies scholar Mike Lynch  whose account of scientific practice is particularly focused on ordinariness. Akin to what Foucault in the <em>Archaeology of Knowledge</em> terms a threshold of epistemologization <span class="citation">(Foucault <a href="#ref-Foucault_1972">1972</a>, 195)</span>  Lynch characterizes the ‘epistopic’ as connecting localized practices (‘topics’) with ‘familiar themes from epistemology and general methodology’ in the local achievement of coherence in knowledge <span class="citation">(Lynch <a href="#ref-Lynch_1993">1993</a>, 280)</span>.  In other words, as the term itself suggests, an epistopic connects general epistemic themes such as validity, precision, specificity, error, confidence, expectation, likelihood, uncertainty, or approximation with a place, a ‘local complex of activities’ (281).  This emphasis on epistemic location frames the problem of how the ‘local complex’ of a specific dataset encounters a generalizing epistemic practice such as machine learning. </p>
</div>
<div id="surface-and-depths-the-problem-of-volume-in-data" class="section level2">
<h2><span class="header-section-number">5.5</span> Surface and depths: the problem of volume in data</h2>
<p>The local complex of activities for associating datasets shatters tables from a different direction than that described by Foucault in <em>The Order of Things</em>. Algebra, linear algebra in particular, organizes and distributes differences in vector space. <em>Mathesis</em> in the form of algebraic operations of addition and multiplication of collections of tabular elements such as rows and columns, now re-defined as vectors, re-structure tabular data as a vector space, as a ‘set’ whose membership is only limited by the applicability of the constructing relations. These operations absorb and subtend differences in quality, type, kind and quantity. </p>
<div class="figure"><span id="fig:prostate-plot-matrix"></span>
<img src="_main_files/figure-html/prostate-plot-matrix-1.png" alt="First rows of the `prostate` dataset" width="1152" />
<p class="caption">
Figure 5.1: First rows of the <code>prostate</code> dataset
</p>
</div>

<p>Of the three example datasets (<code>prostate</code>, <code>spam</code> and <code>zip</code>), <em>Elements of Statistical Learning</em> returns most frequently to <code>prostate.</code> This dataset derives from the work of urologists working at Stanford <span class="citation">(Stamey et al. <a href="#ref-Stamey_1989">1989</a>)</span>, and concerns various clinical measurements performed on men who were about to undergo radical prostatectomy. The measurements range across the volume and weight of the prostate, as well as levels of various prostate-related biomarkers such as PSA – prostate specific antigen. Several rows from the dataset are shown in Table <a href="#tab:prostate"><strong>??</strong></a>. The first pages of the book had already exhaustively plotted all the variables in the dataset against each other using the table-related form of a scatter plot matrix <span class="citation">(Hastie, Tibshirani, and Friedman <a href="#ref-Hastie_2009">2009</a>, 3)</span> (shown in figure <a href="5-vectorisation-and-its-consequences.html#fig:prostate-plot-matrix">5.1</a>, and they return to the same data on almost a dozen occasions in the course of the book, subjecting it to repeated vectorization.  </p>
<p>The contrast between the Table <a href="#tab:prostate"><strong>??</strong></a> and the Figure <a href="5-vectorisation-and-its-consequences.html#fig:prostate-plot-matrix">5.1</a> already depends on a transformation intrinsic to vectorization. On the one hand, the table arrays all different data types in rows and columns. In the table, the relation between the different data types (the log of the weight of prostate - <code>lwp</code> and <code>age</code> for instance) is quite hard to see. Moreover, different kinds of variables stand side by side. <code>svi</code>, short for ‘seminal vesicle invasion’ is a categorical variable.  It takes the values ‘true’ or ‘false,’ shown here as <code>1</code> or <code>0</code>, but the other variables either measure or count things (years, sizes, or levels of antigens).</p>
<p>On the other hand, the scatter plot matrix also takes the form of a grid-like figure, but the cells of the grid are not occupied by numbers but by <code>x-y</code> plots of different pairs of variables in the <code>prostate</code> dataset. The ‘matrix’ of figures shows of 72 plots is mirrored across the diagonal that runs from the top-life to bottom right in figure <a href="5-vectorisation-and-its-consequences.html#fig:prostate-plot-matrix">5.1</a>. Taking this folding into account, we see 36 unique plots with different data in each one. Each sub-plot displays the relation between two variables in the dataset as a scatter plot. Some variables such as <code>svi</code> are not very amenable to plotting in this way. More importantly perhaps, certain combinations of variables appear as flat loci that can be read as signs of relations between different variables. The scatter plot matrix constructs a tabular space in which relational contrasts between pairs of variables start to appear. In the light of these contrasts (and I use ‘light’ here in an almost literal sense to refer to the way in which the architecture of the figure creates a space in which light scatters in varying patterns), the <code>prostate</code> dataset begins to expose relations that might be worth knowing about. We have moved on from the bare table of the dataset to a transformed tabulation, from a textual-numerical grid to a geometrical-numerical grid. Everything remains on the surface of a grid here, but the grid permits differences in relationality to begin to appear. </p>
<p>All of this somewhat precedes the operational formation of machine learning. Similar tables and plots are part and parcel of statistical data exploration more generally (see <span class="citation">(Beniger and Robyn <a href="#ref-Beniger_1978">1978</a>)</span> for an historical account of quantitative graphics in statistics).  The scatterplot matrix does not exhaust differences or relationality in the <code>prostate</code> dataset, but highlights a tendency to approach it from different angles (12 times in the <em>Elements of Statistical Learning</em>) in order to map the multiple relations or influences that remain opaque to even the most exhaustive matrices of plots. The scatterplot matrix shows pairs of variables in relation. If the crucial diagnostic factor in this case is an elevated level of the PSA (prostate specific antigen), how do we know what combinations of other measurements might be associated with its elevation? What if multiple variables affect the level of PSA?<a href="#fn20" class="footnoteRef" id="fnref20"><sup>20</sup></a> This question can just about be pursued by scanning the matrix of plots, but not very stably since different data analysts might see different associations combining with each other there. Different statements or epistopics could be supported by the same figure. </p>
<p>The very question of relation between multiple variables and the predicted levels of PSA suggests the existence of a hidden, occluded or internal space that cannot be seen in a data table, and that cannot be brought to light even in the more complex geometry of a plot. This volume contains the locus of multiple relations, a locus inhering in a higher dimensional space, in this case, the nine dimensional space subtended by treating each of the nine variables or columns in the <code>prostate</code> dataset as occupying its own dimension. A different basis of order – the vector space –  begins to take shape when dataset variables (usually columns in a table) become dimensions.<a href="#fn21" class="footnoteRef" id="fnref21"><sup>21</sup></a></p>
</div>
<div id="vector-space-expansion" class="section level2">
<h2><span class="header-section-number">5.6</span> Vector space expansion</h2>
<p>To show how this space opens up, we might follow what happens to just one or two columns of the <code>prostate</code> data in the vector space as it is vectorized.  In the <code>prostate</code> dataset, some variables are continuous quantitative values, some are categorical (they represent membership in a group or category) and some are ordinal variables (they represent a ranking or order).    How can different data types be located in vector space? In order to put classifications or categories into vector space, they need to be translated into the same <em>basis</em> as the quantitative variables with their rather more obvious geometrical and linear coordinate values.  How does one geometrically or indeed algebraically render a category or a qualitative difference? The problem is solved via an expansion of the vector space through a form of binary coding that generates a new variable and hence a new dimension for each category:</p>
<blockquote>
<p>Qualitative variables are typically represented numerically by codes. The easiest case is when there are only two classes or categories, such as “success” or “failure,” “survived” or “died.” These are often represented by a single binary digit or bit as 0 or 1, or else by 1 and 1 … When there are more than two categories, several alternatives are available. The most useful and commonly used coding is via dummy variables. Here a K-level qualitative variable is represented by a vector of K binary variables or bits, only one of which is “on” at a time <span class="citation">(Hastie, Tibshirani, and Friedman <a href="#ref-Hastie_2009">2009</a>, 12)</span> </p>
</blockquote>
<p>Again, the details are not so important here as the transformations that the vector space accommodates. A single qualitative or categorical variable expands into ‘a vector of K binary variables or bits.’ Qualitative data, once coded in this way, can be multiplied, added, and in short, handled algebraically using the same aggregate operations applied to numerical or continuous variables. Not only has the vector space expanded here, its expansion smooths over important fault-lines of difference that vertically divided the tabular data. Complex natures become simple natures. The different kinds of variables – qualitative and quantitative, discrete and continuous, nominal and ordinal – can be accommodated by adding dimensions to the vector space.  As Whitehead says, ‘all things are vectors’ <span class="citation">(Whitehead <a href="#ref-Whitehead_1960">1960</a>, 309)</span></p>
<p>Adding dimensions to vector space subsumes differences, but makes seeing the geometrically regular loci – lines, planes, smooth surfaces – in data distributed in this space more challenging. The many transformations in <code>prostate</code> that ensue in <em>Elements of Statistical Learning</em> become the locus of machine learning.  In a historically significant transfiguration of the table, these expansions – and we will see others, including <em>de novo</em> creations of constructed dimensions – subtend differences in a vector space comprising elements defined purely by coordinate position and vectoral (having direction and extent) movement.  Once this hidden, expandable and transformable (by rotation, displacement, or scaling) distribution of elements in space exists, strenuous efforts will be made to bring loci to light. Machine learners search for these loci or or feel for , to use Whitehead’s term,  along different lines. Sometimes a machine learner prehends vector space as filled with constantly varying proximities. It gathers and orders these proximities (for instance, as in the <em>k</em> nearest neighbours model ) or in unsupervised methods such as k-means clustering <span class="citation">(Hastie, Tibshirani, and Friedman <a href="#ref-Hastie_2009">2009</a>, 513)</span>. More commonly, machine learning draws lines or flat surfaces that con-strain the volume. </p>
<p> The importance of lines and flat surfaces can hardly be under-estimated in machine learning. Finding lines of best fit underpins many of the machine learners that attract more attention (neural nets, support vector machines, random forests). Linear regression with its pursuit of the straight line or plane projects the basic alignments of vector space. It renders all differences as distances and directions of movement. Drawing lines or flat surfaces at various angles and directions is perhaps the main way in which the volume of data is traversed, and a relation between input and output, between predictors and prediction, consolidated as a loci or data strain loci or data strain.<a href="#fn22" class="footnoteRef" id="fnref22"><sup>22</sup></a> The line of best fit has a ready generalization to higher dimensions, and a line can be diagrammed in the equations of linear algebra, the field of mathematics that operates on lines in spaces of arbitrary dimensions.  Linear algebra operations exist for finding intersections between lines and planes, for manipulating collections of elements and aggregate forms such matrices  through mappings and transformations (rotations, displacements or translations, skewing, and scaling), and above all, handling whole vector spaces as operational sets. It brings with it a set of formalisations – vector space, dimension, matrix, determinant, coordinate system, linear independence, eigenvectors and eigenvalues, inner-product space, etc. – that machine learners constantly and implicitly resort to invoke.<a href="#fn23" class="footnoteRef" id="fnref23"><sup>23</sup></a> </p>
<p>Many of these operations quickly become difficult to geometrically figure.<a href="#fn24" class="footnoteRef" id="fnref24"><sup>24</sup></a> Let us return to the equations for linear regression models (remembering that both C.S. Pierce and Andrew Ng advocate returning often to equations). The ‘mainstay of statistics,’ the linear regression model, usually appears diagrammatically in a more or less algebraic form:</p>


<p>Equations  and  express a plane (or hyperplane) in increasingly diagrammatic abstraction. The possibility of diagramming a high dimensional space derives largely from linear algebra. Reading Equation  from left to right, the expression <span class="math inline">\(\hat{Y}\)</span> already points to a set of calculated, predicted values, or a vector of <span class="math inline">\(y\)</span> values, such as all the <code>lpsa</code> or PSA readings included in the <code>prostate</code> dataset. Similarly, the term <span class="math inline">\(X_j\)</span> points to the table of all the other variables in the <code>prostate</code> dataset.  Since there are 8 other variables, and close to 100 rows, <span class="math inline">\(X\)</span> is a <em>matrix</em> – a higher dimensional table – of values, addressable by coordinates.  Finally <span class="math inline">\(\beta_j\)</span> are the pivotal coefficients or multiplying quantities that determine the slope or direction of the lines drawn.  The second expression Equation  relies more fully on linear algebra. This is the linear model written in ‘vector form’ <span class="citation">(Hastie, Tibshirani, and Friedman <a href="#ref-Hastie_2009">2009</a>, 11)</span>, or vectorized. The right hand side comprises two operations <span class="math inline">\(X^T\)</span>, the transpose or rotation of the data, and implicitly – multiplication is hardly ever shown, but diagrammed by putting terms alongside each other – an <em>inner product</em> of the <span class="math inline">\(X\)</span> matrix and the <span class="math inline">\(\beta\)</span> parameters (to use model talk) or coefficients (to use linear algebra talk). <a href="#fn25" class="footnoteRef" id="fnref25"><sup>25</sup></a> </p>
<p>The vector form in equation  diagrams an inclined plane that cannot be fully drawn in any figure, only projected perspectivally onto the surface of a graphic plot. While that line can never fully come to light, the diagrammatics of equations  and  express a way of constructing it and orienting it in vector space. Such expressions are epistopic in that they connect the local complex of activities indexed as tabulated data together through the diagonal diagrammatic element of a line or plane angling through vector space.</p>
</div>
<div id="drawing-lines-in-a-common-space-of-transformation" class="section level2">
<h2><span class="header-section-number">5.7</span> Drawing lines in a common space of transformation</h2>
<div class="figure"><span id="fig:common-space"></span>
<img src="figure/vector_space.png" alt="Vector space comprises transformations" width="3504" />
<p class="caption">
Figure 5.2: Vector space comprises transformations
</p>
</div>
<p>Once data is distributed in vector space, machine learners operate as transformations of that space into other vector spaces, the flat loci. Indeed from the perspective of vector space, machine learners are simple transformations or operations that map vector spaces into different ones, usually of lower but sometimes of higher dimensionality.  For instance, ‘drawing’ the line of best fit through the <code>prostate</code> data or ‘fitting a line’ can be understood as a purely algebraic operation (although in practice, most machine learners are not purely algebraic – they optimise and probabilise, as we will see).  Viewed in terms of linear algebra, the analytical or ‘closed form solution’ for the parameters of the linear model is given in equation :  </p>

<p>In this expression, linear algebraic operations on the data shown as <span class="math inline">\(\mathbf{X}\)</span> calculate the coefficients <span class="math inline">\(\hat{\beta}\)</span> that orient a plane cutting through the vector space.<a href="#fn26" class="footnoteRef" id="fnref26"><sup>26</sup></a> The derivation of the analytical ‘ordinary least squares’ solution  relies on some differential calculus  as well as a range of linear algebra operations such as matrix transpose, inner product and matrix inversion, the details of which need not trouble us here. The relevant point is that equation  constructs a plane – a new vector – that traverses the density-shape of a dataset in its full dimensional vector space (nine dimensions in the case of <code>prostate</code>).<a href="#fn27" class="footnoteRef" id="fnref27"><sup>27</sup></a></p>
</div>
<div id="implicit-vectorization-in-code-and-infrastructures" class="section level2">
<h2><span class="header-section-number">5.8</span> Implicit vectorization in code and infrastructures</h2>
<p>Vectorised transformations of data lie are the moving substrate of machine learning as it expands, but they are largely taken for granted as given space or commonsense ground. <code>R</code> coding practice instantiates vectorization in multiple ways, and is sometimes described as a ‘vectorised programming language.’ The vector space  appears and operates just as directly in other programming languages designed for data practice (Octave, Matlab, Python’s NumPy, or C++ Armadillo).</p>
<p>In vectorised languages such as <code>R</code>, transformations of a data structure expressed in one line of code simultaneously affect all the elements of the data structure. As the widely used <em>R Cookbook</em> puts it, ‘many functions [in <code>R</code>] operate on entire vectors … and return a vector result’ <span class="citation">(Teetor <a href="#ref-Teetor_2011">2011</a>, 38)</span>. Or as <em>The Art of <code>R</code> Programming: A Tour of Statistical Software Design</em> by Norman Matloff puts it, ‘the fundamental data type in <code>R</code> is the <em>vector</em>’ <span class="citation">(Matloff <a href="#ref-Matloff_2011">2011</a>, 24)</span>, and indeed in <code>R</code>, all data is vector. There are no individual data types, only varieties of vectors in <code>R</code>.  There are many vectorised operations in the <code>R</code> core language and many to be found in packages (the popular ‘plyr’ package; vectorised operations can also be found in recent Python data analysis libraries such as <code>numpy</code> or <code>pandas</code> <span class="citation">(McKinney <a href="#ref-McKinney_2012">2012</a>)</span>).  The fact that many of these vectorised operations occur implicitly suggests how pervasive vector space has become in data practice.<a href="#fn28" class="footnoteRef" id="fnref28"><sup>28</sup></a></p>

<pre><code> [1]  0  2  4  6  8 10 NA 16 18 20</code></pre>
<pre><code> [1]  0  2  4  6  8 10 12 16 18 20</code></pre>
<p>The practical difference between the two approaches to moving through data is illustrated in the code listing  in which two ‘vectors’ of numbers are added together, in the first case using a classic <code>for</code>-loop construct, and in the second case using an implicitly vectorised arithmetic operation <code>+</code>. The difference between adding <span class="math inline">\(1 ... 10\)</span> using a loop or vector arithmetic is completely trivial here, or as we will soon see, nested operations are involved, these differences in coding significantly affect human-machine relations.  This simultaneity is only apparent, since somehow the underlying code has to deal with all the individual elements, but vectorised programming languages take advantage of hardware optimisations or carefully-crafted low-level linear algebra libraries.<a href="#fn29" class="footnoteRef" id="fnref29"><sup>29</sup></a> More importantly, this is a different mode of movement. Operations now longer step through a series of coordinates that address data elements, but wield planes, diagonals, cross-sections and whole-space transformations. Vectorized code reduces both data and computational frictions. The real stake in vectorizing data is not speed but transformation. It makes working with data less like iteration through data structures (lists, indexes, arrays, fields, dictionaries, variables), and more like folding a pliable material. Such practical shifts in feeling for data are mundane yet crucial to the epistopic movements in data. <a href="#fn30" class="footnoteRef" id="fnref30"><sup>30</sup></a></p>
<p>Vectorization also motivates increasingly parallel contemporary chip architectures, clusters of computers such as <code>hadoop</code> or <code>spark</code>, reallocation of computation to GPUs (Graphic Processing Unit),  data-centre usage of FPGAs (Field Programmable Gate Arrays)  and various other Cyclopean infrastructures of cloud computing.  Many of these condensing and expanding movements of data are diagrammed in miniature in the <code>R</code> constructs as operators in vector space.</p>
</div>
<div id="lines-traversing-behind-the-light" class="section level2">
<h2><span class="header-section-number">5.9</span> Lines traversing behind the light</h2>
<p>How does the combination of algebraic vector space and vectorised code play out in data? ‘We fit a linear model’ write Hastie and co-authors, referring to one epistopic operation on <code>prostate</code> data in <em>Elements of Statistical Learning</em>. In <code>R</code> this might look like the code excerpt shown below:</p>


<p>Table <a href="#tab:prostate-linear-regression"><strong>??</strong></a> displays estimates of the coefficients or parameters $} that define the direction of a flat surface running through the vector space of the <code>prostate</code> data.<a href="#fn31" class="footnoteRef" id="fnref31"><sup>31</sup></a>  This new vector is a product of operations in the vectorized <code>prostate</code> data. Some vectorizing operations can be seen in <code>R</code> code in listing  (for instance, <code>as.matrix</code> or <code>scale(prostate_standard)</code>).</p>
<p>The ‘unique solution’ to the problem of fitting a linear model to a given dataset using the popular method of ‘least squares’ <span class="citation">(Hastie, Tibshirani, and Friedman <a href="#ref-Hastie_2009">2009</a>, 12)</span> is given by the operations we have seen in equation . This tightly coiled expression calculates the <span class="math inline">\(\hat{\beta}\)</span> parameters that set the slope and location of a flat surface or plane in nine-dimensional vector space using all of the <code>prostate</code> variables apart from one variable chosen as the response or predicted variable, in this case <code>lpsa</code>. <span class="math inline">\(X\)</span> and <span class="math inline">\(y\)</span> matrices are multiplied, transposed (a form of rotation that swaps rows for columns) and inverted (a more complex operation that finds another matrix) in a series of linear algebra transformations. Epitomising the implicitly vectorised code often seen in machine learning, calculating <span class="math inline">\(\hat{\beta}\)</span> for the <code>prostate</code> data only requires one line of <code>R</code> code: </p>

<p>The implicit vectorization of the <code>R</code> code in the code listing , the fact that it already concretely operates in the vector space, operationalizes the concise diagrammaticism of equation  as a machine process.  More importantly, the vectorised multiplication, transposition and inversion of data creates the new vector <span class="math inline">\(\hat{\beta}\)</span> whose variations can be explored, observed, graphed and varied in ways that go well beyond the statistical tests of significance, variation, and error reported in Table @ref(tab:prostate_linear_regression).  (We will have occasion to return to these statistical estimates in chapter <a href="#ch:probability"><strong>??</strong></a>.) The play of values that starts to appear even in fitting one linear model will become much more significant when fitting hundreds or thousands of models, as some machine learners do.<a href="#fn32" class="footnoteRef" id="fnref32"><sup>32</sup></a> </p>
</div>
<div id="the-vectorised-table" class="section level2">
<h2><span class="header-section-number">5.10</span> The vectorised table?</h2>
<p>I started out from the observation that <em>Elements of Statistical Learning</em> mixes many datasets.  The more abstract implications of vectorization and the forms of transformation movement it encourages and proliferates bring us back to the problem of how machine learning mixes datasets that span different settings. In short, vectorising computation makes the vector space, which we might understand as a resurgent form of the pre-Classical table, a table that tolerates all many of relations and similarities, operationally concrete and machinically abstract. It is no longer a visible diagram, but a machinic process that multiplies and propagates into the world along many diagonal lines.</p>
<p>Machine learning relies on a broad but subtle transformation of data into vectors and a vector space. Slightly re-purposing Foucault’s archaeology of tables in <em>The Order of Things</em>, I have suggested that vectorization remaps the grid of the table into the expanding dimensions of the vector space. This space accommodates both simple and complex natures. This is not the first such expansion of the table. We need only think of the relational database systems of the late 1960s, and their multiplication of tables <span class="citation">(Mackenzie <a href="#ref-Mackenzie_2012">2012</a>)</span>. But in the vectorised and matrix-form practices of the vector space, machine learning produces for the first time a meta(s)table volume that cannot be surfaced on a page or screen.  </p>
<p>Does the vectorization of data lie a ‘a long way from sense data’ as Arendt suggest? In the diagrammatic operations of linear algebra on data, and the vectorization of code, machine learning traverses dimensions that, as Arendt observes, cannot be immediately sensed. Whitehead’s notion of data strain  as ‘a complex distribution of geometrical significance’ suggests, however, that vectorization is not a complete loss of feeling. Every machine learner inhabits and moves through the vector space along different strains. Sometimes their operations flatten the vector space down into lower dimensional sub-spaces as in the many ‘dimensional reduction’ machine learners such as principal component analysis, Latent Dirichlet Allocation or indeed the linear regression model that maps an irregular volume onto a plane.   Sometimes they expand the vector space into a great many new dimensions or ‘features’  (as we saw with ‘dummy variables’ that embody categories, and as we will see with support vector machine classifiers in chapter <a href="#ch:pattern"><strong>??</strong></a> or the deep learners of chapter ). </p>
<p>The epistopic transformation of datasets and tables into vector space reaches into and re-aligns communication and infrastructures. It acts as a powerful tensor on knowledges and operations of many different kinds as it transposes, inverts, and re-maps local complexes of activity. In following what happens to vectors, lists, matrices, arrays, dictionaries, sets, dataframes, series or tuples in data, we might get a sense of how the epistemic operations of predictive models, the supervised and unsupervised learners, the classifiers, the decision trees and the neural networks have purchase in data.</p>
<p>What is at stake in vectorizing data?  It produces a common space that juxtaposes and mixes localized complex realities. The <code>prostate</code> dataset could be aggregated and melded as vectors with a microarray, heart disease or bone density datasets. In vector space, identities and differences change in nature.  Similarity and belonging no longer rely on resemblance or a common genesis, but on measures of proximity or distance, on flat loci that run as vectors through the space. Vectorization, the deep saturation of the table by algebra, constitutes all relations as movements of transformation, diagonalization, inversion or rotation. The epistopic power of vectorization takes root in the elementary practices of machine learning, and engenders many variations amongst machine learners. Vectorisation also strains the production of knowledge through a loss of the visible geometry of tabular comparison. This loss of visibility is, as we will see met by the production of new groups of statements, new visible forms and operational devices and infrastructures that accommodate the dimensional expansion of vector space. Infrastructural vectorization has often been called ‘big data.’  </p>
<p>The fascination of machine learning, its seemingly endless applications (I refer the reader back to the diagram of machine learning’s vastness in chapter <a href="#ch:diagram"><strong>??</strong></a>), owes much to the vector feeling, with its twin lures of ideal operationality – everything is a vector operation – and its tantalizing tendency to expand and to move. This feeling, the vector feeling, we might note, is not surprising. ‘characteristically for Whitehead ’Feelings are “vectors”; for they feel what is there and transform it into what is here’ <span class="citation">(Whitehead <a href="#ref-Whitehead_1960">1960</a>, 87)</span>.  </p>
<p>Expansive data vectorization challenges contemporary critical to develop intuitions and value-relevant concepts describing vector-feelings or data strains. We lack good intuitions of how to do that partly because machine learning implicitly vectorizes its practice in code, in infrastructures and in highly condensed diagrammatic forms. My aim in undertaking an archaeology of the transformations of tables into vector spaces is to unwind or de-diagonalise some of the operations rippling through different treatments of data.  The act of diagramming how machine learners vectorise data densities begins to locate and unravel the processes of knowing, predicting and deciding on which many aspects of the turn to data rely. The vectoral operations we have just been viewing are themselves organised and aligned by other lines of diagrammatic movement  that shape surfaces in more convoluted forms.</p>

</div>
</div>
<h3> References</h3>
<div id="refs" class="references">
<div id="ref-Whitehead_1960">
<p>Whitehead, Alfred North. 1960. <em>Process and Reality, an Essay in Cosmology</em>. New York, Macmillan.</p>
</div>
<div id="ref-Derrida_1989">
<p>Derrida, Jacques. 1989. <em>Edmund Husserl’s Origin of Geometry, an Introduction</em>. Translated by John Leavey. Lincoln: University of Nebraska Press.</p>
</div>
<div id="ref-Arendt_1998">
<p>Arendt, Hannah. 1998. <em>The Human Condition</em>. Chicago ; London: University of Chicago Press.</p>
</div>
<div id="ref-Mackenzie_2011">
<p>Mackenzie, Adrian. 2011. “More Parts Than Elements: How Databases Multiply.” <em>Environment and Planning D: Society and Space</em> 29 (6): 335–50.</p>
</div>
<div id="ref-Hastie_2009">
<p>Hastie, Trevor, Robert Tibshirani, and Jerome H. 2009. <em>The Elements of Statistical Learning: Data Mining, Inference, and Prediction</em>. 2nd edition. New York: Springer.</p>
</div>
<div id="ref-Breiman_2001">
<p>Breiman, Leo. 2001b. “Statistical Modeling: The Two Cultures (with Comments and a Rejoinder by the Author).” <em>Statistical Science</em> 16 (3): 199–231. <a href="http://projecteuclid.org/euclid.ss/1009213726" class="uri">http://projecteuclid.org/euclid.ss/1009213726</a>.</p>
</div>
<div id="ref-Stigler_1986">
<p>Stigler, Stephen M. 1986. <em>The History of Statistics: The Measurement of Uncertainty Before 1900</em>. Cambridge, Mass.: Harvard University Press.</p>
</div>
<div id="ref-Hacking_1990">
<p>Hacking, Ian. 1990. <em>The Taming of Chance</em>. Cambridge University Press.</p>
</div>
<div id="ref-Cranor_1998">
<p>Cranor, Lorrie Faith, and Brian A. LaMacchia. 1998. “Spam!” <em>Communications of the ACM</em> 41 (8): 74–83. <a href="http://dl.acm.org/citation.cfm?id=280336" class="uri">http://dl.acm.org/citation.cfm?id=280336</a>.</p>
</div>
<div id="ref-LeCun_2012">
<p>LeCun, Yann, and Corinna Cortes. 2012. “MNIST Handwritten Digit Database, Yann LeCun, Corinna Cortes and Chris Burges.” <a href="http://yann.lecun.com/exdb/mnist/" class="uri">http://yann.lecun.com/exdb/mnist/</a>.</p>
</div>
<div id="ref-Marchese_2013">
<p>Marchese, Dr Francis T. 2013. “Tables and Early Information Visualization.” In <em>Knowledge Visualization Currents</em>, edited by Francis T. Marchese and Ebad Banissi, 35–61. Springer London. <a href="http://link.springer.com/chapter/10.1007/978-1-4471-4303-1_3" class="uri">http://link.springer.com/chapter/10.1007/978-1-4471-4303-1_3</a>.</p>
</div>
<div id="ref-Foucault_1972">
<p>Foucault, Michel. 1972. <em>The Archaeology of Knowledge and the Discourse on Language</em>. Translated by Allan Sheridan-Smith. New York: Pantheon Books.</p>
</div>
<div id="ref-Stigler_2002">
<p>Stigler, Stephen M. 2002. <em>Statistics on the Table: The History of Statistical Concepts and Methods</em>. Harvard University Press. <a href="http://books.google.co.uk/books?hl=en&amp;lr=&amp;id=qQusWukdPa4C&amp;oi=fnd&amp;pg=PR9&amp;dq=stigler+concepts+statistics&amp;ots=6ekF1LqIQL&amp;sig=nNwRfar3NtBAIv-wAy-xp6WL7Ww" class="uri">http://books.google.co.uk/books?hl=en&amp;lr=&amp;id=qQusWukdPa4C&amp;oi=fnd&amp;pg=PR9&amp;dq=stigler+concepts+statistics&amp;ots=6ekF1LqIQL&amp;sig=nNwRfar3NtBAIv-wAy-xp6WL7Ww</a>.</p>
</div>
<div id="ref-Foucault_1991">
<p>Foucault, Michel. 1991. <em>The History of Sexuality</em>. Translated by Robert Hurley. London: Penguin : Viking : Pantheon.</p>
</div>
<div id="ref-Lynch_1993">
<p>Lynch, Michael. 1993. <em>Scientific Practice and Ordinary Action : Ethnomethodology and Social</em>. Cambridge: Cambridge University Press.</p>
</div>
<div id="ref-Stamey_1989">
<p>Stamey, Thomas A., John N. Kabalin, John E. McNeal, Iain M. Johnstone, F. Freiha, Elise A. Redwine, and Norman Yang. 1989. “Prostate Specific Antigen in the Diagnosis and Treatment of Adenocarcinoma of the Prostate. II. Radical Prostatectomy Treated Patients.” <em>The Journal of Urology</em> 141 (5): 1076–83. <a href="http://europepmc.org/abstract/med/2468795" class="uri">http://europepmc.org/abstract/med/2468795</a>.</p>
</div>
<div id="ref-Beniger_1978">
<p>Beniger, James R., and Dorothy L. Robyn. 1978. “Quantitative Graphics in Statistics: A Brief History.” <em>The American Statistician</em> 32 (1): 1–11. doi:<a href="https://doi.org/10.2307/2683467">10.2307/2683467</a>.</p>
</div>
<div id="ref-Teetor_2011">
<p>Teetor, Paul. 2011. <em>R Cookbook</em>. O’Reilly Media, Incorporated. <a href="http://books.google.co.uk/books?hl=en&amp;lr=&amp;id=KIHuSXyhawEC&amp;oi=fnd&amp;pg=PR5&amp;dq=teetor+R+cookbook&amp;ots=d4Uj6FIU_p&amp;sig=6QY0uaRENGFS3a1B6cmJ2I9yw1g" class="uri">http://books.google.co.uk/books?hl=en&amp;lr=&amp;id=KIHuSXyhawEC&amp;oi=fnd&amp;pg=PR5&amp;dq=teetor+R+cookbook&amp;ots=d4Uj6FIU_p&amp;sig=6QY0uaRENGFS3a1B6cmJ2I9yw1g</a>.</p>
</div>
<div id="ref-Matloff_2011">
<p>Matloff, Norman S. 2011. <em>Art of R Programming</em>. San Francisco: No Starch Press. <a href="http://www.books24x7.com/marc.asp?bookid=44507" class="uri">http://www.books24x7.com/marc.asp?bookid=44507</a>.</p>
</div>
<div id="ref-McKinney_2012">
<p>McKinney, Wes. 2012. <em>Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython</em>. Sebastapol, CA: O’Reilly &amp; Associates Inc. <a href="http://shop.oreilly.com/product/0636920023784.do" class="uri">http://shop.oreilly.com/product/0636920023784.do</a>.</p>
</div>
<div id="ref-Mackenzie_2012">
<p>Mackenzie, Adrian. 2012. “Sets.” In <em>Devices and the Happening of the Social</em>, edited by Celia Lury and Nina Wakeford, 219–31. Routledge.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="16">
<li id="fn16"><p>I loosely borrow the term ‘density’ from statistics, where <em>probability density functions</em> are often used to describe the hardly ever uniform distribution of probabilities of different values of a variable. Sensing density as a form of variation matters greatly both in machine learning itself, where algorithms seek purchase on unevenly distributed data, and in any broader diagram of data. Probability densities are discussed in much more detail in Chapters <a href="#ch:probability"><strong>??</strong></a>. The collection <em>“Raw Data” is an Oxymoron</em> <span class="citation">(Gitelman <a href="#ref-Gitelman_2013">2013</a>)</span> evinces some of these different densities and distributions of data.<a href="5-vectorisation-and-its-consequences.html#fnref16">↩</a></p></li>
<li id="fn17"><p>Later chapters will discuss various ways in which the vectoral dimensionality of data or its rendering as <em>vector space</em> scales up and scales down in machine learning. In terms of multiplying matrices, dimensionality both constrains and enables many aspects of the prediction. Perhaps on the grounds of data dimensionality alone, we should attend to dimensionality practices in <code>R</code>. A fuller discussion of dimensionality of data is the topic of chapter <a href="#ch:pattern"><strong>??</strong></a>. There I discuss how machine learning re-dimensions data in various ways, sometimes reducing dimensions and at other times, multiplying dimensions.<a href="5-vectorisation-and-its-consequences.html#fnref17">↩</a></p></li>
<li id="fn18"><p>Some genomic data will be the focus of a later chapter <a href="#ch:genome"><strong>??</strong></a>. Machine learning during the 1990s and 2000s was in some ways boosted heavily by the advent of genomic biology with its large, enterprise style knowledge endeavours such as the Human Genome Project.<a href="5-vectorisation-and-its-consequences.html#fnref18">↩</a></p></li>
<li id="fn19"><p>In <em>Surveiller et Punir</em> or <em>Discipline and Punish</em> <span class="citation">(Foucault <a href="#ref-Foucault_1977">1977</a>)</span>, Foucault returned to the question of the table in a slightly different context: an account of the operation of power in disciplinary institutions and knowledges. In these knowledges, the table becomes generative of ‘as many effects as possible.’ <a href="5-vectorisation-and-its-consequences.html#fnref19">↩</a></p></li>
<li id="fn20"><p>Despite the intensive work that Hastie and co-authors conduct on the <code>prostate</code> data, all with a view to better predicting PSA levels using volumes and weights of prostates, etc., Stamey and other urologists more than a decade or so concluded that PSA is not a good biomarker for prostate cancer.  Stamey writes in 2004:</p><p>What is urgently needed is a serum marker for prostate cancer that is truly proportional to the volume and grade of this ubiquitous cancer, and solid observations on who should and should not be treated which will surely require randomized trials once such a marker is available. Since there is no such marker for any other organ confined cancer, little is likely to change the current state of overdiagnosis (and over-treatment) of prostate cancer, a cancer we all get if we live long enough. <span class="citation">(Stamey et al. <a href="#ref-Stamey_2004">2004</a>, 1301)</span><a href="5-vectorisation-and-its-consequences.html#fnref20">↩</a></p></li>
<li id="fn21"><p>Every distinct column in a table practically adds a new dimension to the vector space. Since the 1950s, problems of classification and prediction in high-dimensional spaces have been the object of mathematical interest. The mathematician Richard Bellman coined the term ‘the curse of dimensionality’ to describe how partitioning becomes more unstable as the dimensions of the space increase <span class="citation">(Bellman <a href="#ref-Bellman_1961">1961</a>)</span>.  The problem is that while the volume of a space increases exponentially with dimensions, the number of data points (actual measurements or observations) usually does not usually increase at the same rate. In high dimensional spaces, the data becomes more thinly spread out. It is hard to partitions sparsely populated spaces because they accommodate many different boundaries. <a href="5-vectorisation-and-its-consequences.html#fnref21">↩</a></p></li>
<li id="fn22"><p>One sign of the centrality of the line in machine learning can be seen, for instance, from the contents page of the book <span class="citation">(Hastie, Tibshirani, and Friedman <a href="#ref-Hastie_2009">2009</a>, xiii-xxii)</span>. After the introduction of the linear model in the first chapter and its initial exposition in chapter 2 (‘overview of supervised learning’), it forms the central topics of chapter 3 (‘linear methods for regression’), chapter 4 (‘linear methods for classification’), chapter 5 (‘basis expansions and regularization’), chapter 6 (‘kernel smoothing methods’), much of chapter 7 (‘model assessment and selection’), chapter 8 (‘model inference and averaging’), major parts of chapter 9 (‘additive models, trees and related methods’), important parts of chapter 11 (‘neural networks’ – neural networks can be understood as a kind of regression model), the anchoring point of chapter 12 (‘support vector machines and flexible discriminants’) and the main focus in the final chapter (‘high dimensional problems’). A similar topic distribution can be found in Andrew Ng’s Cs229 lectures on machine learning. More than half of the 20 lectures concern linear models and their variants. See <span class="citation">(<em>Lecture 13 | Machine Learning (Stanford)</em> <a href="#ref-Ng_2008a">2008</a>; <em>Lecture 6 | Machine Learning (Stanford)</em> <a href="#ref-Ng_2008b">2008</a>; <em>Lecture 7 | Machine Learning (Stanford)</em> <a href="#ref-Ng_2008c">2008</a>; <em>Lecture 10 | Machine Learning (Stanford)</em> <a href="#ref-Ng_2008d">2008</a>)</span>.<a href="5-vectorisation-and-its-consequences.html#fnref22">↩</a></p></li>
<li id="fn23"><p>Along with statistics and probability, linear algebra is a such an important part of machine learning that many books and courses recommend students complete a linear algebra course before they study machine learning. Cathy O’Neill and Rachel Schutt advise: When you’re developing your skill set as a data scientist, certain foundational pieces need to be in place first—statistics, linear algebra, some programming <span class="citation">(Schutt and O’Neil <a href="#ref-Schutt_2013">2013</a>, 17)</span><a href="5-vectorisation-and-its-consequences.html#fnref23">↩</a></p></li>
<li id="fn24"><p>We might add also approach the epistopic fault line in machine learning topologically . Over a decade ago, the cultural theorist Brian Massumi wrote that ‘the space of experience is really, literally, physically a topological hyperspace of transformation’ <span class="citation">(Massumi <a href="#ref-Massumi_2002">2002</a>, 184)</span> . Much earlier, Gilles Deleuze had conceptualised Michel Foucault’s philosophy as a topology, or ‘thought of the outside’ <span class="citation">(Deleuze <a href="#ref-Deleuze_1988">1988</a><a href="#ref-Deleuze_1988">b</a>)</span>, as a set of movements that sought to map the diagrams that generated a ‘kind of reality, a new model of truth’ <span class="citation">(Deleuze <a href="#ref-Deleuze_1988">1988</a><a href="#ref-Deleuze_1988">b</a>, 35)</span>. More recently, this topological thinking has been extended and developed by Celia Lury amongst others. In ‘The Becoming Topological of Culture,’ Lury, Luciana Parisi and Tiziana Terranova suggests that ‘a new rationality is emerging: the moving ratio of a topological culture’ <span class="citation">(Lury, Parisi, and Terranova <a href="#ref-Lury_2012">2012</a>, 4)</span> .   In this new rationality, practices of ordering, modelling, networking and mapping co-constitute culture, technology and science <span class="citation">(Lury, Parisi, and Terranova <a href="#ref-Lury_2012">2012</a>, 5)</span>. At the core of this new rationality, however, lies a new ordering of continuity. The ‘ordering of continuity,’ Lury, Parisi and Terranova propose, takes shape ‘in practices of sorting, naming, numbering, comparing, listing, and calculating’ (4). The phrase ‘ordering of continuity’ is interesting, since we don’t normally think of continuities as subject to ordering. In many ways, that which is continuous bears within it its own ordering, its own immanent seriation or lamination. But in the becoming topological of culture, movement itself undergoes a transformation according to these authors. Rather than movement as something moving from place to place relatively unchanged (as in geometrical translation), movement should be understood as more like an animation, a set of shape-changing operations. These transformations, I would suggest, should be legible in the way that machine learning, almost the epitome of the processes of modelling and calculation that Lury, Parisi and Terranova point to, itself moves through the data. And indeed, the juxtaposition of spam, biomedical data, gene expression data and handwritten digits already suggests that topological equivalences, and a ‘radical expansion’ of comparison might be occurring. Bringing epistopics and topologies together might, I suggest, help trace, map and importantly diagram some of the movements into the data occurring today.<a href="5-vectorisation-and-its-consequences.html#fnref24">↩</a></p></li>
<li id="fn25"><p>Carl Friedrich Gauss  and Adrien-Marie Legendre’s work on linear regression at this time is well-known. The first independent use of linear regression was Gauss’ prediction of the location of an ‘occluded volume,’ the position of the asteroid Ceres after it reappeared in its orbit behind the sun. <span class="citation">(Stigler <a href="#ref-Stigler_2002">2002</a>)</span> – TBA page ref<a href="5-vectorisation-and-its-consequences.html#fnref25">↩</a></p></li>
<li id="fn26"><p>Perhaps more importantly, the linear algebraic expression of these operations presupposes that all the data, both the values used to build the model and the predicted values the model may generate as it is refined or put into operation somewhere, are contained in a common space, the vector space, a space whose formation and transformation can be progressively ramified and reiterated by various lines that either separate volumes in the space, or head in a direction that brings along most of the data. Not all of these lines are bound to be straight, and much of the variety and dispersion visible in machine learning techniques comes from efforts to construct different kinds of lines or different kinds of ‘decision boundaries’ (in the case of classification problems) in vector space (for instance, the k-nearest neighbors method does not construct straight lines, but somewhat meandering curves that weave between nearby vectors in the vector space; see <span class="citation">(Hastie, Tibshirani, and Friedman <a href="#ref-Hastie_2009">2009</a>, 14–16)</span>).  Whether they are straight or not, the epistopic aspect of these lines remains prominent. Typically, many different statistical tests (Z-scores or standard errors, F-tests, confidence intervals, and then prediction errors) will be applied to any estimate of the coefficients of even the basic linear regression model, well before most advanced or sophisticated models and techniques (cross-validation, bootstrap testing, subset and shrinkage selection) begin to re-configure the model in more radical ways. <a href="5-vectorisation-and-its-consequences.html#fnref26">↩</a></p></li>
<li id="fn27"><p>As we will see in the following chapter (chapter <a href="#ch:function"><strong>??</strong></a>), it is not always possible to calculate the parameters of a model analytically. Especially in relation to contemporary datasets that have very many variables and many instances (rows in the table), linear algebra approaches become unwieldy in their attempt to produce exact results, and machine learning steps in with a variety of computational optimisation techniques. <a href="5-vectorisation-and-its-consequences.html#fnref27">↩</a></p></li>
<li id="fn28"><p><code>R</code> sometimes presents difficulties for programmers trained to code using so-called procedural programming languages because it so thoroughly embraces the notion of the <em>vector</em> – and hence, regards all data as inhabiting vector space. In many mainstream programming languages, transformations of data rely on loops and array constructs in which some operation is successively repeated on each element of a data structure. <a href="5-vectorisation-and-its-consequences.html#fnref28">↩</a></p></li>
<li id="fn29"><p>Learning machine learning, and learning to implement machine learning techniques, is largely a matter of implementing series of matrix multiplications. As Andrew Ng advises his students,</p><blockquote><p>Almost any programming language you use will have great linear algebra libraries. And they will be high optimised to do that matrix-matrix multiplication very efficiently including taking advantage of any parallelism your computer. So that you can very efficiently make lots of predictions of lots of hypotheses <span class="citation">(<em>Lecture 13 | Machine Learning (Stanford)</em> <a href="#ref-Ng_2008a">2008</a>, 10:50)</span></p></blockquote><p>In other parts of his teaching, and indeed throughout the practice exercises and assignments, Ng stresses the value of implementing machine learning techniques for both understanding them and using them properly. But this is one case where implementation does not facilitate learning. Ng advises his learners against implementing their own matrix handling code. They should instead use the ‘great linear algebra libraries’ found in ‘almost any programming language.’ ‘linear algebra libraries’ multiply, transpose, decompose, invert and generally transform matrices and vectors.  they will be ‘highly optimised’ not because every programming language has been prepared for the advent of machine learning on a large scale, but rather more likely because matrix operations are just so widely used in image and audio processing. Happily, Ng observes, that means that ‘you can make lots of predictions’ <span class="citation">(A. Ng <a href="#ref-Ng_2008e">2008</a><a href="#ref-Ng_2008e">b</a>)</span>. It seems that generating predictions and hypotheses outweighs the value of understanding how things work on this point. <a href="5-vectorisation-and-its-consequences.html#fnref29">↩</a></p></li>
<li id="fn30"><p>A further level of vectorization appears in specific <code>R</code> constructs such as <code>apply</code>, <code>sapply</code>, <code>tapply</code>, <code>lapply</code>, and <code>mapply</code>.  All of the <code>-ply</code> constructs have a common feature: they take some collection of things (it may be ordered in many different ways – as a list, as a table, as an array, etc.), do something to it, and return a collection. While most programming languages in common use offer constructs to help deal with collections of things sequentially (for instance, by accessing each element of a list in turn and doing something with it), <code>R</code> offers ways of expressing a simultaneous operation on them all. The <code>-ply</code> constructs ultimately derive from the functional logic developed by the mathematician Alonzo Church in the 1930s <span class="citation">(Church <a href="#ref-Church_1936">1936</a>; Church <a href="#ref-Church_1996">1996</a>)</span>.  The functional programming style of applying functions to functions seems strangely abstract. <a href="5-vectorisation-and-its-consequences.html#fnref30">↩</a></p></li>
<li id="fn31"><p>From the epistopic viewpoint, the most obvious result of fitting a linear model is the production not of a line on a diagram or in a graphic. As we have seen, such lines cannot be easily rendered visible. Instead, the model generates a new column-vector of coefficients (see Table @ref(tab:prostate_linear_regression)) and some new numbers, <em>statistics</em>. This table is not as extensive as the original data, the <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(Y\)</span> vectors. But the names of the variables in the dataset appear as rows in the new table, a table that describes something of how a line has been fitted by the linear model to the data. The columns of the table now bear abbreviated and much more statistical names such as <code>estimate</code> (the estimated values of <span class="math inline">\(\hat{\beta}\)</span>, the key parameters in any linear model), <code>Std. Error</code>, <code>t value</code>, and the all important <em>p</em> values written as <code>Pr(|t|)</code>. The numerical values ranging along the rows mostly range from -1 to 1, but the final column includes values that are incredibly small: <code>1.47e-06</code> is a few millionths. Other statistics range around the outside the table: the <code>F-statistic</code>, the <code>R-squared</code> statistic, and the <code>Residual standard error</code>. The numbers of the table <a href="#tab:prostate"><strong>??</strong></a> become epistopic here, since they now appear as a set of standard errors, estimates, t-statistics, and <em>p</em> values, that together indicate how likely the estimated values of <span class="math inline">\(\beta\)</span> are, and therefore how well the diagonal line expresses the relations between different dimensions of the dataset in the vector space. <a href="5-vectorisation-and-its-consequences.html#fnref31">↩</a></p></li>
<li id="fn32"><p>This is an important differentiation: it is not typical machine learning practice to construct one model, characterised by a single set of statistics (F scores, R^2 scores, <em>t</em> values, etc.). In practice, most machine learning techniques construct many models, and the efficacy of some predictive techniques derives often from the multiplication or indeed proliferation of models. Techniques such as neural networks, cross-validation, bagging, shrinkage and subset selection, and random forests, to name a few, generate many statistics, and navigating the multiple or highly variable models that result becomes a major concern. An epistopic abundance will appear here – bias, variance, precision, recall, training error, test error, expectation, Bayesian Information Criteria, etc. as well as graphisms such as ROC (Receiver-Operator-Characteristics) curves. Put simply, the proliferation of models start to drive the dimensional expansion of the vector space. At the same time, the multiplicity of models multiplied by the machine learners becomes the topic of statistical analysis.<a href="5-vectorisation-and-its-consequences.html#fnref32">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="4-diagramming-machines.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="6-machines-finding-functions.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
