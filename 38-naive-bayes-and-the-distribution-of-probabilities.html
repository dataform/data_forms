<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Machine Learning: Archaeology of a Data Practice</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="Machine Learning: Archaeology of a Data Practice">
  <meta name="generator" content="bookdown 0.3 and GitBook 2.6.7">

  <meta property="og:title" content="Machine Learning: Archaeology of a Data Practice" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Machine Learning: Archaeology of a Data Practice" />
  
  
  

<meta name="author" content="Adrian Mackenzie">


<meta name="date" content="2016-12-10">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="37-distributed-probabilities.html">
<link rel="next" href="39-spam-when-foralln-is-too-much.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="1-acknowledgments.html"><a href="1-acknowledgments.html"><i class="fa fa-check"></i><b>1</b> Acknowledgments</a></li>
<li class="chapter" data-level="2" data-path="2-preface.html"><a href="2-preface.html"><i class="fa fa-check"></i><b>2</b> Preface</a></li>
<li class="chapter" data-level="3" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html"><i class="fa fa-check"></i><b>3</b> Introduction: Into the Data</a><ul>
<li class="chapter" data-level="3.1" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#three-accumulations-settings-data-and-devices"><i class="fa fa-check"></i><b>3.1</b> Three accumulations: settings, data and devices</a></li>
<li class="chapter" data-level="3.2" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#who-or-what-is-a-machine-learner"><i class="fa fa-check"></i><b>3.2</b> Who or what is a machine learner?</a></li>
<li class="chapter" data-level="3.3" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#algorithmic-control-to-the-machine-learners"><i class="fa fa-check"></i><b>3.3</b> Algorithmic control to the machine learners?</a></li>
<li class="chapter" data-level="3.4" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#the-archaeology-of-operations"><i class="fa fa-check"></i><b>3.4</b> The archaeology of operations</a></li>
<li class="chapter" data-level="3.5" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#asymmetries-in-common-knowledge"><i class="fa fa-check"></i><b>3.5</b> Asymmetries in common knowledge</a></li>
<li class="chapter" data-level="3.6" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#what-cannot-be-automated"><i class="fa fa-check"></i><b>3.6</b> What cannot be automated?</a></li>
<li class="chapter" data-level="3.7" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#different-fields-in-machine-learning"><i class="fa fa-check"></i><b>3.7</b> Different fields in machine learning?</a></li>
<li class="chapter" data-level="3.8" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#the-diagram-in-critical-thought"><i class="fa fa-check"></i><b>3.8</b> The diagram in critical thought</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-we-dont-have-to-write-programs.html"><a href="4-we-dont-have-to-write-programs.html"><i class="fa fa-check"></i><b>4</b> ‘We don’t have to write programs’?</a></li>
<li class="chapter" data-level="5" data-path="5-the-elements-of-machine-learning.html"><a href="5-the-elements-of-machine-learning.html"><i class="fa fa-check"></i><b>5</b> The elements of machine learning</a></li>
<li class="chapter" data-level="6" data-path="6-who-reads-machine-learning-textbooks.html"><a href="6-who-reads-machine-learning-textbooks.html"><i class="fa fa-check"></i><b>6</b> Who reads machine learning textbooks?</a></li>
<li class="chapter" data-level="7" data-path="7-r-a-matrix-of-transformations.html"><a href="7-r-a-matrix-of-transformations.html"><i class="fa fa-check"></i><b>7</b> <code>R</code>: a matrix of transformations</a></li>
<li class="chapter" data-level="8" data-path="8-the-obdurate-mathematical-glint-of-machine-learning.html"><a href="8-the-obdurate-mathematical-glint-of-machine-learning.html"><i class="fa fa-check"></i><b>8</b> The obdurate mathematical glint of machine learning</a></li>
<li class="chapter" data-level="9" data-path="9-cs229-2007-returning-again-and-again-to-certain-features.html"><a href="9-cs229-2007-returning-again-and-again-to-certain-features.html"><i class="fa fa-check"></i><b>9</b> CS229, 2007: returning again and again to certain features</a></li>
<li class="chapter" data-level="10" data-path="10-the-visible-learning-of-machine-learning.html"><a href="10-the-visible-learning-of-machine-learning.html"><i class="fa fa-check"></i><b>10</b> The visible learning of machine learning</a></li>
<li class="chapter" data-level="11" data-path="11-the-diagram-of-an-operational-formation.html"><a href="11-the-diagram-of-an-operational-formation.html"><i class="fa fa-check"></i><b>11</b> The diagram of an operational formation</a></li>
<li class="chapter" data-level="12" data-path="12-vectorisation-and-its-consequences.html"><a href="12-vectorisation-and-its-consequences.html"><i class="fa fa-check"></i><b>12</b> Vectorisation and its consequences}</a></li>
<li class="chapter" data-level="13" data-path="13-vector-space-and-geometry.html"><a href="13-vector-space-and-geometry.html"><i class="fa fa-check"></i><b>13</b> Vector space and geometry</a></li>
<li class="chapter" data-level="14" data-path="14-mixing-places.html"><a href="14-mixing-places.html"><i class="fa fa-check"></i><b>14</b> Mixing places</a></li>
<li class="chapter" data-level="15" data-path="15-truth-is-no-longer-in-the-table.html"><a href="15-truth-is-no-longer-in-the-table.html"><i class="fa fa-check"></i><b>15</b> Truth is no longer in the table?</a></li>
<li class="chapter" data-level="16" data-path="16-the-epistopic-fault-line-in-tables.html"><a href="16-the-epistopic-fault-line-in-tables.html"><i class="fa fa-check"></i><b>16</b> The epistopic fault line in tables</a></li>
<li class="chapter" data-level="17" data-path="17-surface-and-depths-the-problem-of-volume-in-data.html"><a href="17-surface-and-depths-the-problem-of-volume-in-data.html"><i class="fa fa-check"></i><b>17</b> Surface and depths: the problem of volume in data</a></li>
<li class="chapter" data-level="18" data-path="18-vector-space-expansion.html"><a href="18-vector-space-expansion.html"><i class="fa fa-check"></i><b>18</b> Vector space expansion</a></li>
<li class="chapter" data-level="19" data-path="19-drawing-lines-in-a-common-space-of-transformation.html"><a href="19-drawing-lines-in-a-common-space-of-transformation.html"><i class="fa fa-check"></i><b>19</b> Drawing lines in a common space of transformation</a></li>
<li class="chapter" data-level="20" data-path="20-implicit-vectorization-in-code-and-infrastructures.html"><a href="20-implicit-vectorization-in-code-and-infrastructures.html"><i class="fa fa-check"></i><b>20</b> Implicit vectorization in code and infrastructures</a></li>
<li class="chapter" data-level="21" data-path="21-lines-traversing-behind-the-light.html"><a href="21-lines-traversing-behind-the-light.html"><i class="fa fa-check"></i><b>21</b> Lines traversing behind the light</a></li>
<li class="chapter" data-level="22" data-path="22-the-vectorised-table.html"><a href="22-the-vectorised-table.html"><i class="fa fa-check"></i><b>22</b> The vectorised table?</a></li>
<li class="chapter" data-level="23" data-path="23-machines-finding-functions.html"><a href="23-machines-finding-functions.html"><i class="fa fa-check"></i><b>23</b> Machines finding functions}</a></li>
<li class="chapter" data-level="24" data-path="24-learning-functions.html"><a href="24-learning-functions.html"><i class="fa fa-check"></i><b>24</b> Learning functions</a></li>
<li class="chapter" data-level="25" data-path="25-supervised-unsupervised-reinforcement-learning-and-functions.html"><a href="25-supervised-unsupervised-reinforcement-learning-and-functions.html"><i class="fa fa-check"></i><b>25</b> Supervised, unsupervised, reinforcement learning and functions</a></li>
<li class="chapter" data-level="26" data-path="26-which-function-operates.html"><a href="26-which-function-operates.html"><i class="fa fa-check"></i><b>26</b> Which function operates?</a></li>
<li class="chapter" data-level="27" data-path="27-what-does-a-function-learn.html"><a href="27-what-does-a-function-learn.html"><i class="fa fa-check"></i><b>27</b> What does a function learn?</a></li>
<li class="chapter" data-level="28" data-path="28-observing-with-curves-the-logistic-function.html"><a href="28-observing-with-curves-the-logistic-function.html"><i class="fa fa-check"></i><b>28</b> Observing with curves: the logistic function</a></li>
<li class="chapter" data-level="29" data-path="29-the-cost-of-curves-in-machine-learning.html"><a href="29-the-cost-of-curves-in-machine-learning.html"><i class="fa fa-check"></i><b>29</b> The cost of curves in machine learning</a></li>
<li class="chapter" data-level="30" data-path="30-curves-and-the-variation-in-models.html"><a href="30-curves-and-the-variation-in-models.html"><i class="fa fa-check"></i><b>30</b> Curves and the variation in models</a></li>
<li class="chapter" data-level="31" data-path="31-observing-costs-losses-and-objectives-through-optimisation.html"><a href="31-observing-costs-losses-and-objectives-through-optimisation.html"><i class="fa fa-check"></i><b>31</b> Observing costs, losses and objectives through optimisation</a></li>
<li class="chapter" data-level="32" data-path="32-gradients-as-partial-observers.html"><a href="32-gradients-as-partial-observers.html"><i class="fa fa-check"></i><b>32</b> Gradients as partial observers</a></li>
<li class="chapter" data-level="33" data-path="33-the-power-to-learn.html"><a href="33-the-power-to-learn.html"><i class="fa fa-check"></i><b>33</b> The power to learn</a></li>
<li class="chapter" data-level="34" data-path="34-probabilisation-and-the-taming-of-machines.html"><a href="34-probabilisation-and-the-taming-of-machines.html"><i class="fa fa-check"></i><b>34</b> Probabilisation and the Taming of Machines}</a></li>
<li class="chapter" data-level="35" data-path="35-data-reduces-uncertainty.html"><a href="35-data-reduces-uncertainty.html"><i class="fa fa-check"></i><b>35</b> Data reduces uncertainty?</a></li>
<li class="chapter" data-level="36" data-path="36-machine-learning-as-statistics-inside-out.html"><a href="36-machine-learning-as-statistics-inside-out.html"><i class="fa fa-check"></i><b>36</b> Machine learning as statistics inside out</a></li>
<li class="chapter" data-level="37" data-path="37-distributed-probabilities.html"><a href="37-distributed-probabilities.html"><i class="fa fa-check"></i><b>37</b> Distributed probabilities</a></li>
<li class="chapter" data-level="38" data-path="38-naive-bayes-and-the-distribution-of-probabilities.html"><a href="38-naive-bayes-and-the-distribution-of-probabilities.html"><i class="fa fa-check"></i><b>38</b> Naive Bayes and the distribution of probabilities</a></li>
<li class="chapter" data-level="39" data-path="39-spam-when-foralln-is-too-much.html"><a href="39-spam-when-foralln-is-too-much.html"><i class="fa fa-check"></i><b>39</b> Spam: when <span class="math inline">\(\forall{N}\)</span> is too much?</a></li>
<li class="chapter" data-level="40" data-path="40-the-improbable-success-of-the-naive-bayes-classifier.html"><a href="40-the-improbable-success-of-the-naive-bayes-classifier.html"><i class="fa fa-check"></i><b>40</b> The improbable success of the Naive Bayes classifier</a></li>
<li class="chapter" data-level="41" data-path="41-ancestral-probabilities-in-documents-inference-and-prediction.html"><a href="41-ancestral-probabilities-in-documents-inference-and-prediction.html"><i class="fa fa-check"></i><b>41</b> Ancestral probabilities in documents: inference and prediction</a></li>
<li class="chapter" data-level="42" data-path="42-statistical-decompositions-bias-variance-and-observed-errors.html"><a href="42-statistical-decompositions-bias-variance-and-observed-errors.html"><i class="fa fa-check"></i><b>42</b> Statistical decompositions: bias, variance and observed errors</a></li>
<li class="chapter" data-level="43" data-path="43-does-machine-learning-construct-a-new-statistical-reality.html"><a href="43-does-machine-learning-construct-a-new-statistical-reality.html"><i class="fa fa-check"></i><b>43</b> Does machine learning construct a new statistical reality?</a></li>
<li class="chapter" data-level="44" data-path="44-patterns-and-differences.html"><a href="44-patterns-and-differences.html"><i class="fa fa-check"></i><b>44</b> Patterns and differences</a></li>
<li class="chapter" data-level="45" data-path="45-splitting-and-the-growth-of-trees.html"><a href="45-splitting-and-the-growth-of-trees.html"><i class="fa fa-check"></i><b>45</b> Splitting and the growth of trees</a></li>
<li class="chapter" data-level="46" data-path="46-differences-in-recursive-partitioning.html"><a href="46-differences-in-recursive-partitioning.html"><i class="fa fa-check"></i><b>46</b> 1984: Differences in recursive partitioning</a></li>
<li class="chapter" data-level="47" data-path="47-limiting-differences.html"><a href="47-limiting-differences.html"><i class="fa fa-check"></i><b>47</b> Limiting differences</a></li>
<li class="chapter" data-level="48" data-path="48-the-successful-dispersion-of-the-support-vector-machine.html"><a href="48-the-successful-dispersion-of-the-support-vector-machine.html"><i class="fa fa-check"></i><b>48</b> The successful dispersion of the support vector machine</a></li>
<li class="chapter" data-level="49" data-path="49-differences-blur.html"><a href="49-differences-blur.html"><i class="fa fa-check"></i><b>49</b> Differences blur?</a></li>
<li class="chapter" data-level="50" data-path="50-bending-the-decision-boundary.html"><a href="50-bending-the-decision-boundary.html"><i class="fa fa-check"></i><b>50</b> Bending the decision boundary</a></li>
<li class="chapter" data-level="51" data-path="51-instituting-patterns.html"><a href="51-instituting-patterns.html"><i class="fa fa-check"></i><b>51</b> Instituting patterns</a></li>
<li class="chapter" data-level="52" data-path="52-regularizing-and-materializing-objects.html"><a href="52-regularizing-and-materializing-objects.html"><i class="fa fa-check"></i><b>52</b> Regularizing and materializing objects}</a></li>
<li class="chapter" data-level="53" data-path="53-genomic-referentiality-and-materiality.html"><a href="53-genomic-referentiality-and-materiality.html"><i class="fa fa-check"></i><b>53</b> Genomic referentiality and materiality</a></li>
<li class="chapter" data-level="54" data-path="54-the-genome-as-threshold-object.html"><a href="54-the-genome-as-threshold-object.html"><i class="fa fa-check"></i><b>54</b> The genome as threshold object</a></li>
<li class="chapter" data-level="55" data-path="55-genomic-knowledges-and-their-datasets.html"><a href="55-genomic-knowledges-and-their-datasets.html"><i class="fa fa-check"></i><b>55</b> Genomic knowledges and their datasets</a></li>
<li class="chapter" data-level="56" data-path="56-the-advent-of-wide-dirty-and-mixed-data.html"><a href="56-the-advent-of-wide-dirty-and-mixed-data.html"><i class="fa fa-check"></i><b>56</b> The advent of ‘wide, dirty and mixed’ data</a></li>
<li class="chapter" data-level="57" data-path="57-cross-validating-machine-learning-in-genomics.html"><a href="57-cross-validating-machine-learning-in-genomics.html"><i class="fa fa-check"></i><b>57</b> Cross-validating machine learning in genomics</a></li>
<li class="chapter" data-level="58" data-path="58-proliferation-of-discoveries.html"><a href="58-proliferation-of-discoveries.html"><i class="fa fa-check"></i><b>58</b> Proliferation of discoveries</a></li>
<li class="chapter" data-level="59" data-path="59-variations-in-the-object-or-in-the-machine-learner.html"><a href="59-variations-in-the-object-or-in-the-machine-learner.html"><i class="fa fa-check"></i><b>59</b> Variations in the object or in the machine learner?</a></li>
<li class="chapter" data-level="60" data-path="60-whole-genome-functions.html"><a href="60-whole-genome-functions.html"><i class="fa fa-check"></i><b>60</b> Whole genome functions</a></li>
<li class="chapter" data-level="61" data-path="61-propagating-subject-positions.html"><a href="61-propagating-subject-positions.html"><i class="fa fa-check"></i><b>61</b> Propagating subject positions}</a></li>
<li class="chapter" data-level="62" data-path="62-propagation-across-human-machine-boundaries.html"><a href="62-propagation-across-human-machine-boundaries.html"><i class="fa fa-check"></i><b>62</b> Propagation across human-machine boundaries</a></li>
<li class="chapter" data-level="63" data-path="63-competitive-positioning.html"><a href="63-competitive-positioning.html"><i class="fa fa-check"></i><b>63</b> Competitive positioning</a></li>
<li class="chapter" data-level="64" data-path="64-a-privileged-machine-and-its-diagrammatic-forms.html"><a href="64-a-privileged-machine-and-its-diagrammatic-forms.html"><i class="fa fa-check"></i><b>64</b> A privileged machine and its diagrammatic forms</a></li>
<li class="chapter" data-level="65" data-path="65-varying-subject-positions-in-code.html"><a href="65-varying-subject-positions-in-code.html"><i class="fa fa-check"></i><b>65</b> Varying subject positions in code</a></li>
<li class="chapter" data-level="66" data-path="66-the-subjects-of-a-hidden-operation.html"><a href="66-the-subjects-of-a-hidden-operation.html"><i class="fa fa-check"></i><b>66</b> The subjects of a hidden operation</a></li>
<li class="chapter" data-level="67" data-path="67-algorithms-that-propagate-errors.html"><a href="67-algorithms-that-propagate-errors.html"><i class="fa fa-check"></i><b>67</b> Algorithms that propagate errors</a></li>
<li class="chapter" data-level="68" data-path="68-competitions-as-examination.html"><a href="68-competitions-as-examination.html"><i class="fa fa-check"></i><b>68</b> Competitions as examination</a></li>
<li class="chapter" data-level="69" data-path="69-superimposing-power-and-knowledge.html"><a href="69-superimposing-power-and-knowledge.html"><i class="fa fa-check"></i><b>69</b> Superimposing power and knowledge</a></li>
<li class="chapter" data-level="70" data-path="70-ranked-subject-positions.html"><a href="70-ranked-subject-positions.html"><i class="fa fa-check"></i><b>70</b> Ranked subject positions</a></li>
<li class="chapter" data-level="71" data-path="71-conclusion-out-of-the-data.html"><a href="71-conclusion-out-of-the-data.html"><i class="fa fa-check"></i><b>71</b> Conclusion: Out of the Data}</a></li>
<li class="chapter" data-level="72" data-path="72-machine-learners.html"><a href="72-machine-learners.html"><i class="fa fa-check"></i><b>72</b> 250,000 machine learners</a></li>
<li class="chapter" data-level="73" data-path="73-a-summary-of-the-argument.html"><a href="73-a-summary-of-the-argument.html"><i class="fa fa-check"></i><b>73</b> A summary of the argument</a></li>
<li class="chapter" data-level="74" data-path="74-in-situ-hybridization.html"><a href="74-in-situ-hybridization.html"><i class="fa fa-check"></i><b>74</b> In-situ hybridization</a></li>
<li class="chapter" data-level="75" data-path="75-critical-operational-practice.html"><a href="75-critical-operational-practice.html"><i class="fa fa-check"></i><b>75</b> Critical operational practice?</a></li>
<li class="chapter" data-level="76" data-path="76-obstacles-to-the-work-of-freeing-machine-learning.html"><a href="76-obstacles-to-the-work-of-freeing-machine-learning.html"><i class="fa fa-check"></i><b>76</b> Obstacles to the work of freeing machine learning</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning: Archaeology of a Data Practice</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="naive-bayes-and-the-distribution-of-probabilities" class="section level1">
<h1><span class="header-section-number">38</span> Naive Bayes and the distribution of probabilities</h1>
<p>How could machine learners become a population? The mathematical expression for one of the most popular of all machine learning classifiers, the Naive Bayes classifier, stands out for its probabilistic simplicity and seeming lack of ‘moving parts’. </p>

<blockquote>
<p><span class="citation">[@Hastie_2009, 211]</span></p>
</blockquote>
<p>Some machine learners are so simple that they can be implemented in a few lines of code. Along with the perceptron, linear regression, and <em>k</em> nearest neighbours, the function shown in equation () is one of the simplest one to be found in most machine textbooks yet easily adapts for high dimensional data, the kind of data associated with contemporary network infrastructures, scientific instruments, online communications and <span class="math inline">\(N = \forall\boldsymbol{X}\)</span> in general.<a href="#fn54" class="footnoteRef" id="fnref54"><sup>54</sup></a> Even though the Naive Bayes classifier is one of the most popular machine learning algorithms, it is more than 50 years old <span class="citation">[@Hand_2001]</span>.</p>
<p>The key diagrammatic elements of the classifier in the equation are <span class="math inline">\(\prod\)</span>, an operator that multiplies all the values of the matrix of <span class="math inline">\(X\)</span> values (from <span class="math inline">\(1\)</span> to <span class="math inline">\(p\)</span>) to generate a product. What product does the Naive Bayes classifier produce? The expression <span class="math inline">\(f_j(X)\)</span> refers to a probability density; that is, it describes the probability that a particular thing (a document, an image, an email message, a set of URLs, etc.) belongs to the class of things <span class="math inline">\(j\)</span>.  In constructing an estimate of the probability that a given message, image or event is an instance of class <span class="math inline">\(j\)</span>, <span class="math inline">\(p\)</span> different features are taken into account. ( The subscript <span class="math inline">\(k\)</span> indexes the <span class="math inline">\(p\)</span> dimensions of the vector space.) The subscripts <span class="math inline">\(k=1\)</span> on the <span class="math inline">\(\prod\)</span> operator, and <span class="math inline">\(k\)</span> on the data <span class="math inline">\(X_k\)</span> indicate that the Naive Bayes classifier makes use of a series of features or variables in calculating the overall probability that a given thing or observation belongs to a specific class. Put in the language of probability calculus, the classifier produces a probability density <span class="math inline">\(f_j(X)\)</span> by calculating the <em>joint probability</em> of all the <em>conditional</em> probabilities of the features or predictor variables in <span class="math inline">\(X\)</span> for the class <span class="math inline">\(j\)</span>. As <em>Elements of Statistical Learning</em> rather tersely puts it, ‘each of the class densities are products of the marginal densities’ <span class="citation">[@Hastie_2009,108]</span>.</p>
<p>The Naive Bayes classifier directly invokes probability (including its name, with its reference to the Bayes Theorem, an important late eighteenth century concept), yet there is little obvious connection to statistics in its modern form of tests of significance.  As Drew Conway and John Myles-White write in <em>Machine Learning for Hackers</em>,</p>
<blockquote>
<p>At its core, [Naive Bayes] … is a 20th century application of the 18th century concept of <em>conditional probability</em>. A conditional probability is the likelihood of observing some thing given some other thing we already know about <span class="citation">[@Conway_2012, 77]</span>  </p>
</blockquote>
<p>They point to the application of ‘conditional probability,’ a probability conditioned on the probability of something else. Conditional probability lies at the heart of many of the data transformation associated with prediction or pattern recognition since it links a class to the occurrence of combinations of variables or features. Naive Bayes links variables by simply multiplying probabilities.<a href="#fn55" class="footnoteRef" id="fnref55"><sup>55</sup></a>  As any of the many accounts of the technique will explain, the name comes from Bayes Theorem, one of the most basic yet widely used results in probability theory (again dating from the eighteenth century), yet Naive Bayes does not even fully embrace Bayes Theorem as the principle of its operation. The classifier has a simple architecture based on the concepts of conditional probability and joint probability; it calculates a probability density function <span class="math inline">\(f_j(X)\)</span> or probability distribution for each possible class of things as a combination of the probabilities of all the many features or attributes of populations that come together in data. It makes a drastically naïve assumption that features or variables are statistically independent of each other, where ‘independent’ means that they do not affect each other, or that they have no relation to each other. We will see below that dramatic simplifications such as independence do not necessarily weaken the referential grasp of machine learners on the world, but in certain ways allow them to reconfigure the operations of machine learners as a population of learners.</p>
</div>
<div class="footnotes">
<hr />
<ol start="54">
<li id="fn54"><p>The other contender for simplest machine learner would be the also very popular <em>k</em> nearest neighbours. As Hastie et. al. observe: ‘these classifiers are memory-based and require no model to be fit’ <span class="citation">[@Hastie_2009, 463]</span>. Like the Naive Bayes classifier, the equation for <em>k</em> nearest neighbours is simple:</p><p>where <span class="math inline">\(\textit{N}_{k}(x)\)</span> is the neighborhood of <span class="math inline">\(x\)</span> defined by the <span class="math inline">\(k\)</span> closest points <span class="math inline">\(x_{i}\)</span> in the training sample <span class="citation">[@Hastie_2009, 14]</span>.</p><p>In equation , a parameter appears: <span class="math inline">\(k\)</span>, the number of neighbours. This contrasts greatly with the linear models discussed in chapters  and  where the number of parameters <span class="math inline">\(p\)</span> usually equals the number of variables in the dataset or dimensions in the vector space. <a href="38-naive-bayes-and-the-distribution-of-probabilities.html#fnref54">↩</a></p></li>
<li id="fn55"><p>In <span class="citation">[@Mackenzie_2014c]</span>, I have suggested that the intensification of multiplication associated with probabilistic calculation may constitute an important mutation in the ontological and practical texture of numbers. The epidemiological modelling of H1N1 influenza in London 2009 involved multiplying a great variety of probability distributions in order to calculate the conditional probability of influenza over time.<a href="38-naive-bayes-and-the-distribution-of-probabilities.html#fnref55">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="37-distributed-probabilities.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="39-spam-when-foralln-is-too-much.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
