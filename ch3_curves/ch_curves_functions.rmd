# Machines finding functions

'Machines finding functions': there is a triple word play in the title of this chapter. From an _almost_ purely technical standpoint, machine learning can be understood as function finding, that is, finding a mathematical expression that approximates to the process that generated the data in question. The second sense of the title is that machine learning offers a plethora of techniques, and much machine learning work, at least for many practitioners, concerns not so much implementation of particular techniques (neural network, decision tree, support vector machine, logistic regression, etc.), but rather navigating the maze of methods and variations that might be relevant to a particular situation. Here we could recall that 'functions' abound in software and code, not in the mathematical sense, but in the sense of operational units of code( the `print` function in `Python` writes to the screen, for instance).   Finding the right function amidst the variety of functions affects any learning of machine learning. The third sense of the title is less technical, and concerns something more dynamically transcontextual. That is, how machine learning itself has itself come to operate as a powerful diagram for infrastructural, operational, financial, scientific, governmental and marketing processes. Here, the machines -- the classifiers and the predictive models -- have found functions as they have been inserted into infrastructures, experiments, and organisations. How do these different functions function together? What generality or positivity concatenates them? 

## Which sense of function?

A  mathematical sense of the function is writ large in nearly all machine learning literature. 

>A classifier or classification rule is a function $d(\mathbf{x})$ defined on $\mathcal{X}$ so that for every $\mathbf{x}$, $d(\mathbf{x})$ is equal to one of the numbers $1, 2, ..., J$ [@Breiman_1984, 4]

Writing in the 1980s, the statistician Leo Breiman describes classifiers -- perhaps the key technical achievement of machine learning -- in terms of functions. A classifier _is_ a function. The equation of classifiers to functions is quite pervasive. The learning, the predictions, and the classifications produced by machine learning depend on functions. So we need an account of what goes on in this equation of function and classification if we are to make sense of the broader problematisation associated with machine learning. 

The identification of machine learning with functions appears in the first pages of most machine learning textbooks. Learning in machine learning means finding a function that can identify or predict patterns in the data. As _Elements of Statistical Learning_ puts it,

>our goal is to find a useful approximation $\hat{f}(x)$ to the function $f(x)$ that underlies the predictive relationship between input and output [@Hastie_2009, 28]. 

In a highly compressed form, this statement of goals performs the triple play on function that referred to above. It contains _the_ function that generated the data as a foundation, it refers to 'finding  ... $\hat{f}(x)$', where the '^' indicates an approximation, and it avers to 'use'. Similar formulations pile up in the literature. A leading theorist of learning theory Vladimir Vapnik puts it this way: 'learning is a problem of _function estimation_ on the basis of empirical data' [@Vapnik_1999, 291]. (Vapnik is said to have invented the support vector machine, one of the most heavily used machine learning technique of recent years on the basis of his theory of computational learning.) The use of the term 'learning' in machine learning displays affiliations to the field of artificial intelligence, but the  attempt to find a 'useful approximation' -- the 'function-fitting paradigm' as [@Hastie_2009, 29] terms it -- stems mainly from statistics.  Not all accounts of machine learning frame the techniques in terms of function fitting. Some retain the language of intelligent machines (see for example, [@Alpaydin_2010, xxxvi] who writes: 'we do not need to come up with new algorithms if machines can learn themselves'). Despite any differences in the  framing of the techniques, all accounts of machine learning, even those such as _Machine Learning for Hackers_ [@Conway_2012] that eschew any explicit recourse to mathematical formula,  rely on the  formalism and modes of thought associated with mathematical functions. Whether they are seen as forms of artificial intelligence or statistical models, the formalisms are directed to build 'a good and useful approximation to the desired output' [@Alpaydin_2010, 41], or, put more statistically,  'to use the sample to find the function from the set of admissable functions that minimizes the probability of error' [@Vapnik_1999, 31].
We have seen some of this process already in the  previous chapter: the linear  regression model that fits a line to a set of points  is just such a useful approximation to  the actual function that generated the data. The kind of visual pattern it identifies is really elementary -- a straight line -- but the lengths to which machine learning is prepared to go to fit lines to situations is, as we will see, quite extraordinary.  The linear model undergoes some drastic deformations as lines stretch and fold into planes, hyperplanes,  and various curved and fitted surfaces, but it remains a function in the mathematical sense of a mapping between input or $X$ values and output or $Y$ values. 

As is often the case in working with a massive technical literature, the first problem in making sense of what is happening with function in machine learning concerns sheer abundance. The pages of [@Hastie_2009] are marked with score of references to 'functions': quadratic function, likelihood function, sigmoid function, loss function, regression function, basis function, activation function, penalty functions, additive functions, kernel functions,step function,  error function, constraint function, discriminant function, probability density function, weight function, coordinate function, neighborhood function, and the list goes on. Clearly we cannot expect to understand the functioning of all these functions in any great detail. However, even a glance through this prickly list of terms begins to suggest that not only is there quite a heavy reliance on functions in this field (as perhaps in many others), and that we need to understand the operations occurring in and around functions if we are to make sense of the statements and forms of seeing associated with the field. We can also already see in this list that the qualifiers of the term function are diverse. Sometimes, the qualifier refers to a mathematical form -- 'quadratic,' 'coordinate', 'basis' or 'kernel'; sometimes it refers to modelling or statistical considerations -- 'likelihood', 'regression', 'error,' or 'probability density'; and sometimes it refers to some other concern that might relate to a particular modelling device or diagram -- 'activation,' 'weight', 'loss,'  'constraint,' or 'discriminant.' These multiple modes of functions matter, since they permit the triple-play of function-finding in machine learning.     

Implicit too in the formal descriptions of machine learning as function finding cited above (for instance, Vapnik's or Hastie's formulation) is the core mathematical definition of a function. The primary mathematical sense refers to a relation between sets of values or variables. (A variable is a symbol that can stand for a set of numbers or other values.) A function is one-to-one relation between two sets of values. It maps a set of arguments (inputs) to a set of values (outputs, or to use slightly more technical language, it maps between a _domain_ and a _co-domain_.) As we have already seen, mathematical functions are often written in formulae of varying degrees of complexity. They are of various genres, provenances, textures and shapes: polynomial functions, trigonometric functions, exponential functions, differential equations, series functions, algebraic or topological functions, etc. Various fields of mathematics have pursued the invention of functions. In machine learning and information retrieval, important functions would include the logistic function (discussed below), probability density functions (PDF) for different probability distributions (Gaussian, Bernoulli, Binomial, Beta, Gamma, etc.;[^2]).

[^2]:  I will discuss these in greater depth in Chapter 5-6), as well as  cost functions and Langrangian functions, etc. Not all functions take numbers as inputs or outputs. Letters, words or almost any other symbol can be values in a function. 

Functions appear in machine learning in several different ways: as statements or utterances, as formula-diagrams, as graphic forms and in technical implementations as code. Any account of machine learning as function finding needs to map the concatenation of these different elements, none of which alone can anchor the 'learning' that goes on in machine learning. 

While functions are often diagrammatically written in formulae, they can often appear in different graphic or ooperational forms. Take the example of the logistic (or sigmoid) function. This quite simple function underpins many classifiers and animates many of the operations of neural network, including their recent re-incarnations in 'deep learning' [@Hinton_2006; @Mohamed_2011].  It can be written as:

$$f(x) = 1/(1+e^{-x})$$

And can be graphed as:

```{r logistic, echo=TRUE, cache=TRUE, message=FALSE, warning=FALSE, comment=NA, size='smallsize', results='markup'}
	
	x = seq(-100, 100, 0.01)
	y = 1/(1+exp(-x))
	plot(x,y)

```

This curve of this function, as we will see, is very important in many classification and decision settings. Curves play a critical role in the operation of many of the techniques. The shape of the sigmoid curve is a good example of the polyvalence of curves. 

[HERE] - add in stuff about the curve, about how functions came about as a way of relating to curves and lines, and then how many curves appear in Hastie, etc. Look at all the graphics here --- choose a couple


How does it refer or get into these settings? The second sense of function comes from programming and computer science. A function there is a part of the code of a program that performs some operation. The three lines of R code written to produce the plot of the logistic function are almost too trivial, but they show something of the transformations that occur when mathematical functions are operationalised in algorithmic form. The function is wrapped in a set of references. First, the domain of $x$ values is made much more specific. The formulaic expression $f(x) = 1/(1+e^{-x})$ says nothing explicitly about the $x$ values. They are implicitly real numbers (that is, $x \in \mathbb{R}$) in this formula but in the algorithmic expression of the function they become a sequence of `r length(x)` generated by the code. Second, the function itself is flattened into a single line of characters in code, whereas the typographically the mathematical formula had spanned 2-3 lines. Third, a key component of the function $e^-x$ itself refers to Euler's number $e$, which is perhaps the number most widely used in contemporary sciences due to its connection to patterns of growth and decay (as in the exponential function $e^x$ where $e = 2.718282$ approximately).  This number, because it is 'irrational,' has to be computed approximately in the algorithmic implementation. Finally, the plot of the function invokes a whole set of spatial and graphic conventions. For instance, it shows the $x$ values along aa horizontal axis, with negative values on the left and positive values on the right, and the $y$ values on a vertical axis at right angles to the $x$ axis, etc. These transformation between the formula expression of the function, the algorithmic and the graphic form are very mundane, mostly taken for granted in contemporary data practice. But in certain cases, they become much problematic and unstable. 

This description of the differences between functions in a mathematical sense as a mapping and functions in an algorithmic sense as an implementation of some repeated operations that might express a mathematical function is meant to highlight a key issue in learning functions. As we move from the mathematical formula to the three lines of R code that produces a plot of the function what has happened?  This is a kind of implementation of a function, and perhaps we learn something about the logistic function, that for instance, that  the $y$ values change  decisively between $0$ and $1$ across a very brief interval of $x$ values. Unlike the linear functions we saw in the house-price models (see previous chapter), logistic functions approximate a switch between $1$ and $0$, or other values such as  `yes` and `no`.  This rapid change in value will, as we see, proves incredibly useful  in machine learning: it opens up the possibility of using continuous-value function to approximate states of affairs where differences are much more heavily marked. That is, the logistic function can be used to classify or decide. 

The  S-shaped curve of the logistic function has quite a long history in statistics [tBA - stigler], but also suggests another important transformation, somewhat orthogonal to the transformation between the mathematical function as formal abstraction and algorithm as implemented abstraction. The mathematical function $f(x) = 1/(1+e^{-x})$ holds together continuously varying numbers (the $x$ values) and discontinuous values: because $f(x)$ tends very quickly to converge on values of $1$ or $0$, it can be code as 'yes'/'no'; 'survived/deceased', or any other binary difference. The transformation between the $x$ values sliding continuously and the binary difference pivots on the combination of the  exponential function ($e^{-x}$), which rapidly tends towards zero as $x$ increases and rapidly tends towards $\inf$ as $x$ decreases, and the $1/(1+ ...) $, which converts high value denominators to almost zero, and low value demominators to one. This constrained path between variations in $x$ and their mapping to the value of the function $f(x)$ is mathematically elementary, but typical of the relaying of references that allows functions to intersect with and constitute matters of fact and states of affairs.  



There is deep disturbance in the practice of machine learning around the problem of knowing whether it works or not. While the field is almost despotically pragmatic in its concerns with classification and prediction (although in certain ways, curiously idealistic too), it is troubled by the persistence of two broadly different kinds of _learning_: supervised and unsupervised (as well as hybrid kinds such as semi-supervised learning). Writing around 2000, Hastie et. al. state: 

> With supervised learning there is a clear measure of success or lack thereof, that can be used to judge adequacy in particular situations and to compare the effectiveness of different methods over various situations. Lack of success is directly measured by expected loss over the  joint distribution $Pr(X,Y)$. This can be estimated in a variety of ways including cross-validation. In the context of unsupervised learning, there is no such direct measure of success. ... This uncomfortable situation has led to heavy proliferation of proposed methods, since effectiveness is a matter of opinion and cannot be verified directly.  [@Hastie_2009, 486-7]


Curves lie at
