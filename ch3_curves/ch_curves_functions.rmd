\chapter{Machines finding functions}
\label{ch:function}


```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=12, tidy=TRUE, fig.height=8, 
                      echo=FALSE,  warning=FALSE, message=FALSE, dev='pdf')

library(RSQLite)
con <- dbConnect(RSQLite::SQLite(),'../ml_lit/all_refs.sqlite3')
res = dbGetQuery(con, statement ="select * from basic_refs limit 10;")
library(ggplot2)
library(stringr)
library(xtable)
options(xtable.comment = FALSE)
```

> Because of a gradient that no doubt characterizes our cultures, discursive formations are constantly becoming epistemologized [@Foucault_1972, 195]

The opening pages of machine learning textbooks often warn or enthuse about the profusion of techniques, algorithms, tools and machines. \index{machine learners!variety of|(} 'The first problem facing you', writes Pedro Domingos, 'is the bewildering variety of learning algorithms available. Which one to use? There are literally thousands available, and hundreds more are published each year [@Domingos_2012, 1]. \index{Domingos, Pedro} 'The literature on machine learning is vast, as is the overlap with the relevant areas of statistics and engineering' writes David Barber in _Bayesian Reasoning and Machine Learning_[@Barber_2011,4]; 'statistical learning refers to a vast set of tools for understanding data' writes James and co-authors in an _Introduction to Statistical Learning with R_ [@James_2013,1]; or writing in in *Statistical Learning for Biomedical Data* the biostatisticians James Malley, Karen Malley and Sinisa Pajevic 'freely admit that many machines studied in this text are somewhat mysterious, though powerful engines'  [@Malley_2011,  257]. In _Thoughtful Machine Learning_ Matthew Kirk adds: 'flexibility is also what makes machine learning daunting. It can solve many problems, but how do we know whether we’re solving the right problem, or actually solving it in the first place?' [@Kirk_2014, ix]. \index{Kirk, Matthew}  The prefatory comments from Domingos, Barber, James, Malley and Kirk suggest  that machine learning comprises a rampant abundance of techniques. Much machine learning work, at least for many practitioners, concerns not so much implementation of particular techniques (neural network, decision tree, support vector machine, logistic regression, etc.), but rather navigating the maze of methods and variations that might be relevant to a particular situation. 

\begin{figure}
  \centering
      \includegraphics[width=0.95\textwidth]{figure/ml_map_scikit.pdf}
        \caption{`scikit-learn` map of machine learning techniques [TBA: ref to diagram]}
  \label{fig:mapping_functions}
\end{figure}

How does this effect of accumulation and profusion arise and what do machine learners do about it? The publications I have just been referring to attempt to both present that profusion as a problem of publications and to manage it by writing textbooks that provide indexes, maps and guides to the bewildering variety of machine learners. The anchoring textbook _Elements of Statistical Learning_ deploys tables, overviews, theories, assessment and comparison techniques to aid in navigating them. Parallel and complementary mappings accompany software libraries. The map of machine learning techniques shown in Figure \ref{fig:mapping_functions} comes from a particular software library written in `Python,` `scikit-learn` [@Pedregosa_2011].  \index{Python!scikit-learn library} This software library is widely used in industry, research and commerce. In contrast to the pedagogical expositions, theoretical accounts or guides to reference implementation, code libraries such as  `scikit-learn` tend to order the range of techniques by offering recipes and maps for the use of the *functions* the libraries supply. \index{function!code|(} The branches in the figure lay down paths through the profusion of techniques as a decision tree.[^3.01] Similarly, for `R` code, the *Comprehensive R Archive Network* tabulates key libraries of `R` code in  a  machine learning 'task view' [@Hothorn_2014].  
\index{machine learners!variety of|)}



```{r scikit-learn, engine='python', echo=TRUE, messages=FALSE, warnings=FALSE, results='asis'}
import sklearn
from sklearn import *
modules = dir(sklearn)
modules_clean = [m for m in modules if not m.startswith('_')]
print(modules_clean)
```

The architecture of these software libraries itself classifies and orders machine learners.  `Scikit-learn` for instance comprises a number of sub-packages:
Modules such as `lda` (linear discriminant analysis), `svm` (support vector machine) or `neighbors` (_k_ nearest neighbours) point to well-known machine learners, whilst `cross-validation` or `feature_selection` refer to ways of testing models or transforming data respectively. Again, machine learning patches together a variety of abstractive practices. \index{abstraction!levels of} These divisions, maps and classifications help order the techniques, but they obscure the problematic process that first generated a competing profusion of machine learners. That profusion comes from a slippage between two main senses of the function: function as mathematical relation and function as concrete machinic operation.  \index{function!code|)}

Internally, machine learners grapple with problems of finding functions. \index{function!mathematical} From an _almost_ purely mathematical standpoint, machine learning can be understood as function finding operations. Implicitly or explicitly, machine learners find a mathematical expression -- a function -- approximating an outcome of the social, technical, financial, transactional, biological, brain, heart or group process that generated the data in question. Regardless of the application, no single mathematical function perfectly or uniques expresses data. Many if not infinite functions can approximate any given data. This blurs the diagonal lines that run between a mathematical function and an operational machine learner. The mathematical function and the software function are in diagrammatic relation, but the _diagonals_ that run between them are not always in a one-to-one mapping. \index{diagrammatic!diagonal}

## Supervised or unsupervised, who learns what?

The `scikit-learn` map of techniques addresses the problem of choosing a machine learner. This is only a starting point. No matter how powerful machine learners become,  they do not operate autonomously.  The techniques, models, forms of abstraction, data formats, and performance properties of algorithms have to be learned by people (reading books, attending classes, watching demonstrations, trying out software and code, etc. See Chapter \ref{ch:subjects}). Once learned, maps such as the one shown in Figure \ref{fig:mapping_functions} may become redundant. But other forms of close attention, monitoring, and observation remain crucial whenever  machine learners encounter data or wherever they enter the common vector space. 

The need to observe the machine organises the field of machine learning. Machine learning textbooks and courses distinguish 'supervised', 'unsupervised' and sometimes 'semi-supervised' learning. While the field is almost despotically pragmatic in its commitment to optimisation of  classification and prediction (although in certain ways, curiously idealistic too in its constant reuse of well-worked datasets such as `iris` or `South African heart disease`), it reluctantly accepts the existence of these two broadly different kinds of _learning_. Writing around 2000, Hastie et. al. state: 

> With supervised learning there is a clear measure of success or lack thereof, that can be used to judge adequacy in particular situations and to compare the effectiveness of different methods over various situations. Lack of success is directly measured by expected loss over the  joint distribution $Pr(X,Y)$. This can be estimated in a variety of ways including cross-validation. In the context of unsupervised learning, there is no such direct measure of success. ... This uncomfortable situation has led to heavy proliferation of proposed methods, since effectiveness is a matter of opinion and cannot be verified directly.  [@Hastie_2009, 486-7]

Supervised learning in general terms constructs a model by training it on some sample data (the training data \index{data!training} ), and then evaluating its effectiveness in classifying or predicting  test data \index{data!test} whose actual values are already known. The 'clear measure of success' they refer to in relation to in so-called 'supervised learning' is of relatively recent date.[^3.02]  Unsupervised machine learning techniques generally look for patterns in the data without any training or testing phases (for instance, _k_-means or principal component analysis do this, and both techniques have been heavily used for more than fifty years). In both supervised and unsupervised learning people look at the models to find out how the models traverse, fit, partition or map the data. At a general level, machine learning is a system of making statements and rendering relations visible through supervision. As I will suggest below, partial observers -- a term drawn from the work of Félix Guattari and Gilles Deleuze -- supervise the diagrammatic functioning of machine learning.   At the same time, opacity -- 'no direct measure of success' -- is generative in machine learning. \index{partial observer|see{function!partial observer}} Amidst the optically dense pages of mathematical functions, plots of datasets and listing of algorithms, _Elements of Statistical Learning_'s frank admission that something cannot be measured and that this difficulty has led to proliferating methods seems to me quite promising ground to explore for possible transformations and changes. But  this discomfort about learning does not encompass the entirety of the field. Even if, as the first part of the quoted text puts it, with supervised learning there is a clear measure of success,' that success may only allow comparisons between machine learners. The almost unbounded optimism associated with machine learning (for instance as more or less unspoken foundation of  any analysis of 'big data')  runs pell-mell across different techniques, but levels of predictive success vary widely with different techniques and different situations. 


[^3.01]: See Chapter \ref{ch:pattern} for discussion of decision trees in machine learning.

[^3.02]:  Only in the mid-1980s were the first theories of algorithmic learning formalised [@Valiant_1984].
