<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Machine Learning: Archaeology of a Data Practice</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="Machine Learning: Archaeology of a Data Practice">
  <meta name="generator" content="bookdown 0.3 and GitBook 2.6.7">

  <meta property="og:title" content="Machine Learning: Archaeology of a Data Practice" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Machine Learning: Archaeology of a Data Practice" />
  
  
  

<meta name="author" content="Adrian Mackenzie">


<meta name="date" content="2016-12-21">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="3-introduction-into-the-data.html">
<link rel="next" href="5-vectorisation-and-its-consequences.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="1-acknowledgments.html"><a href="1-acknowledgments.html"><i class="fa fa-check"></i><b>1</b> Acknowledgments</a></li>
<li class="chapter" data-level="2" data-path="2-preface.html"><a href="2-preface.html"><i class="fa fa-check"></i><b>2</b> Preface</a></li>
<li class="chapter" data-level="3" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html"><i class="fa fa-check"></i><b>3</b> Introduction: Into the Data</a><ul>
<li class="chapter" data-level="3.1" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#three-accumulations-settings-data-and-devices"><i class="fa fa-check"></i><b>3.1</b> Three accumulations: settings, data and devices</a></li>
<li class="chapter" data-level="3.2" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#who-or-what-is-a-machine-learner"><i class="fa fa-check"></i><b>3.2</b> Who or what is a machine learner?</a></li>
<li class="chapter" data-level="3.3" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#algorithmic-control-to-the-machine-learners"><i class="fa fa-check"></i><b>3.3</b> Algorithmic control to the machine learners?</a></li>
<li class="chapter" data-level="3.4" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#the-archaeology-of-operations"><i class="fa fa-check"></i><b>3.4</b> The archaeology of operations</a></li>
<li class="chapter" data-level="3.5" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#asymmetries-in-common-knowledge"><i class="fa fa-check"></i><b>3.5</b> Asymmetries in common knowledge</a></li>
<li class="chapter" data-level="3.6" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#what-cannot-be-automated"><i class="fa fa-check"></i><b>3.6</b> What cannot be automated?</a></li>
<li class="chapter" data-level="3.7" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#different-fields-in-machine-learning"><i class="fa fa-check"></i><b>3.7</b> Different fields in machine learning?</a></li>
<li class="chapter" data-level="3.8" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#the-diagram-in-critical-thought"><i class="fa fa-check"></i><b>3.8</b> The diagram in critical thought</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html"><i class="fa fa-check"></i><b>4</b> Diagramming machines</a><ul>
<li class="chapter" data-level="4.1" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#we-dont-have-to-write-programs"><i class="fa fa-check"></i><b>4.1</b> ‘We don’t have to write programs’?</a></li>
<li class="chapter" data-level="4.2" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#the-elements-of-machine-learning"><i class="fa fa-check"></i><b>4.2</b> The elements of machine learning</a></li>
<li class="chapter" data-level="4.3" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#who-reads-machine-learning-textbooks"><i class="fa fa-check"></i><b>4.3</b> Who reads machine learning textbooks?</a></li>
<li class="chapter" data-level="4.4" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#r-a-matrix-of-transformations"><i class="fa fa-check"></i><b>4.4</b> <code>R</code>: a matrix of transformations</a></li>
<li class="chapter" data-level="4.5" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#the-obdurate-mathematical-glint-of-machine-learning"><i class="fa fa-check"></i><b>4.5</b> The obdurate mathematical glint of machine learning</a></li>
<li class="chapter" data-level="4.6" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#cs229-2007-returning-again-and-again-to-certain-features"><i class="fa fa-check"></i><b>4.6</b> CS229, 2007: returning again and again to certain features</a></li>
<li class="chapter" data-level="4.7" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#the-visible-learning-of-machine-learning"><i class="fa fa-check"></i><b>4.7</b> The visible learning of machine learning</a></li>
<li class="chapter" data-level="4.8" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#the-diagram-of-an-operational-formation"><i class="fa fa-check"></i><b>4.8</b> The diagram of an operational formation</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html"><i class="fa fa-check"></i><b>5</b> Vectorisation and its consequences</a><ul>
<li class="chapter" data-level="5.1" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html#vector-space-and-geometry"><i class="fa fa-check"></i><b>5.1</b> Vector space and geometry</a></li>
<li class="chapter" data-level="5.2" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html#mixing-places"><i class="fa fa-check"></i><b>5.2</b> Mixing places</a></li>
<li class="chapter" data-level="5.3" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html#truth-is-no-longer-in-the-table"><i class="fa fa-check"></i><b>5.3</b> Truth is no longer in the table?</a></li>
<li class="chapter" data-level="5.4" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html#the-epistopic-fault-line-in-tables"><i class="fa fa-check"></i><b>5.4</b> The epistopic fault line in tables</a></li>
<li class="chapter" data-level="5.5" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html#surface-and-depths-the-problem-of-volume-in-data"><i class="fa fa-check"></i><b>5.5</b> Surface and depths: the problem of volume in data</a></li>
<li class="chapter" data-level="5.6" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html#vector-space-expansion"><i class="fa fa-check"></i><b>5.6</b> Vector space expansion</a></li>
<li class="chapter" data-level="5.7" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html#drawing-lines-in-a-common-space-of-transformation"><i class="fa fa-check"></i><b>5.7</b> Drawing lines in a common space of transformation</a></li>
<li class="chapter" data-level="5.8" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html#implicit-vectorization-in-code-and-infrastructures"><i class="fa fa-check"></i><b>5.8</b> Implicit vectorization in code and infrastructures</a></li>
<li class="chapter" data-level="5.9" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html#lines-traversing-behind-the-light"><i class="fa fa-check"></i><b>5.9</b> Lines traversing behind the light</a></li>
<li class="chapter" data-level="5.10" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html#the-vectorised-table"><i class="fa fa-check"></i><b>5.10</b> The vectorised table?</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html"><i class="fa fa-check"></i><b>6</b> Machines finding functions</a><ul>
<li class="chapter" data-level="6.1" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html#learning-functions"><i class="fa fa-check"></i><b>6.1</b> Learning functions</a></li>
<li class="chapter" data-level="6.2" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html#supervised-unsupervised-reinforcement-learning-and-functions"><i class="fa fa-check"></i><b>6.2</b> Supervised, unsupervised, reinforcement learning and functions</a></li>
<li class="chapter" data-level="6.3" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html#which-function-operates"><i class="fa fa-check"></i><b>6.3</b> Which function operates?</a></li>
<li class="chapter" data-level="6.4" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html#what-does-a-function-learn"><i class="fa fa-check"></i><b>6.4</b> What does a function learn?</a></li>
<li class="chapter" data-level="6.5" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html#observing-with-curves-the-logistic-function"><i class="fa fa-check"></i><b>6.5</b> Observing with curves: the logistic function</a></li>
<li class="chapter" data-level="6.6" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html#the-cost-of-curves-in-machine-learning"><i class="fa fa-check"></i><b>6.6</b> The cost of curves in machine learning</a></li>
<li class="chapter" data-level="6.7" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html#curves-and-the-variation-in-models"><i class="fa fa-check"></i><b>6.7</b> Curves and the variation in models</a></li>
<li class="chapter" data-level="6.8" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html#observing-costs-losses-and-objectives-through-optimisation"><i class="fa fa-check"></i><b>6.8</b> Observing costs, losses and objectives through optimisation</a></li>
<li class="chapter" data-level="6.9" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html#gradients-as-partial-observers"><i class="fa fa-check"></i><b>6.9</b> Gradients as partial observers</a></li>
<li class="chapter" data-level="6.10" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html#the-power-to-learn"><i class="fa fa-check"></i><b>6.10</b> The power to learn</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="7-probabilisation-and-the-taming-of-machines.html"><a href="7-probabilisation-and-the-taming-of-machines.html"><i class="fa fa-check"></i><b>7</b> Probabilisation and the Taming of Machines}</a><ul>
<li class="chapter" data-level="7.1" data-path="7-probabilisation-and-the-taming-of-machines.html"><a href="7-probabilisation-and-the-taming-of-machines.html#data-reduces-uncertainty"><i class="fa fa-check"></i><b>7.1</b> Data reduces uncertainty?</a></li>
<li class="chapter" data-level="7.2" data-path="7-probabilisation-and-the-taming-of-machines.html"><a href="7-probabilisation-and-the-taming-of-machines.html#machine-learning-as-statistics-inside-out"><i class="fa fa-check"></i><b>7.2</b> Machine learning as statistics inside out</a></li>
<li class="chapter" data-level="7.3" data-path="7-probabilisation-and-the-taming-of-machines.html"><a href="7-probabilisation-and-the-taming-of-machines.html#distributed-probabilities"><i class="fa fa-check"></i><b>7.3</b> Distributed probabilities</a></li>
<li class="chapter" data-level="7.4" data-path="7-probabilisation-and-the-taming-of-machines.html"><a href="7-probabilisation-and-the-taming-of-machines.html#naive-bayes-and-the-distribution-of-probabilities"><i class="fa fa-check"></i><b>7.4</b> Naive Bayes and the distribution of probabilities</a></li>
<li class="chapter" data-level="7.5" data-path="7-probabilisation-and-the-taming-of-machines.html"><a href="7-probabilisation-and-the-taming-of-machines.html#spam-when-foralln-is-too-much"><i class="fa fa-check"></i><b>7.5</b> Spam: when <span class="math inline">\(\forall{N}\)</span> is too much?</a></li>
<li class="chapter" data-level="7.6" data-path="7-probabilisation-and-the-taming-of-machines.html"><a href="7-probabilisation-and-the-taming-of-machines.html#the-improbable-success-of-the-naive-bayes-classifier"><i class="fa fa-check"></i><b>7.6</b> The improbable success of the Naive Bayes classifier</a></li>
<li class="chapter" data-level="7.7" data-path="7-probabilisation-and-the-taming-of-machines.html"><a href="7-probabilisation-and-the-taming-of-machines.html#ancestral-probabilities-in-documents-inference-and-prediction"><i class="fa fa-check"></i><b>7.7</b> Ancestral probabilities in documents: inference and prediction</a></li>
<li class="chapter" data-level="7.8" data-path="7-probabilisation-and-the-taming-of-machines.html"><a href="7-probabilisation-and-the-taming-of-machines.html#statistical-decompositions-bias-variance-and-observed-errors"><i class="fa fa-check"></i><b>7.8</b> Statistical decompositions: bias, variance and observed errors</a></li>
<li class="chapter" data-level="7.9" data-path="7-probabilisation-and-the-taming-of-machines.html"><a href="7-probabilisation-and-the-taming-of-machines.html#does-machine-learning-construct-a-new-statistical-reality"><i class="fa fa-check"></i><b>7.9</b> Does machine learning construct a new statistical reality?</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="8-patterns-and-differences.html"><a href="8-patterns-and-differences.html"><i class="fa fa-check"></i><b>8</b> Patterns and differences</a><ul>
<li class="chapter" data-level="8.1" data-path="8-patterns-and-differences.html"><a href="8-patterns-and-differences.html#splitting-and-the-growth-of-trees"><i class="fa fa-check"></i><b>8.1</b> Splitting and the growth of trees</a></li>
<li class="chapter" data-level="8.2" data-path="8-patterns-and-differences.html"><a href="8-patterns-and-differences.html#differences-in-recursive-partitioning"><i class="fa fa-check"></i><b>8.2</b> 1984: Differences in recursive partitioning</a></li>
<li class="chapter" data-level="8.3" data-path="8-patterns-and-differences.html"><a href="8-patterns-and-differences.html#limiting-differences"><i class="fa fa-check"></i><b>8.3</b> Limiting differences</a></li>
<li class="chapter" data-level="8.4" data-path="8-patterns-and-differences.html"><a href="8-patterns-and-differences.html#the-successful-dispersion-of-the-support-vector-machine"><i class="fa fa-check"></i><b>8.4</b> The successful dispersion of the support vector machine</a></li>
<li class="chapter" data-level="8.5" data-path="8-patterns-and-differences.html"><a href="8-patterns-and-differences.html#differences-blur"><i class="fa fa-check"></i><b>8.5</b> Differences blur?</a></li>
<li class="chapter" data-level="8.6" data-path="8-patterns-and-differences.html"><a href="8-patterns-and-differences.html#bending-the-decision-boundary"><i class="fa fa-check"></i><b>8.6</b> Bending the decision boundary</a></li>
<li class="chapter" data-level="8.7" data-path="8-patterns-and-differences.html"><a href="8-patterns-and-differences.html#instituting-patterns"><i class="fa fa-check"></i><b>8.7</b> Instituting patterns</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="9-regularizing-and-materializing-objects.html"><a href="9-regularizing-and-materializing-objects.html"><i class="fa fa-check"></i><b>9</b> Regularizing and materializing objects}</a><ul>
<li class="chapter" data-level="9.1" data-path="9-regularizing-and-materializing-objects.html"><a href="9-regularizing-and-materializing-objects.html#genomic-referentiality-and-materiality"><i class="fa fa-check"></i><b>9.1</b> Genomic referentiality and materiality</a></li>
<li class="chapter" data-level="9.2" data-path="9-regularizing-and-materializing-objects.html"><a href="9-regularizing-and-materializing-objects.html#the-genome-as-threshold-object"><i class="fa fa-check"></i><b>9.2</b> The genome as threshold object</a></li>
<li class="chapter" data-level="9.3" data-path="9-regularizing-and-materializing-objects.html"><a href="9-regularizing-and-materializing-objects.html#genomic-knowledges-and-their-datasets"><i class="fa fa-check"></i><b>9.3</b> Genomic knowledges and their datasets</a></li>
<li class="chapter" data-level="9.4" data-path="9-regularizing-and-materializing-objects.html"><a href="9-regularizing-and-materializing-objects.html#the-advent-of-wide-dirty-and-mixed-data"><i class="fa fa-check"></i><b>9.4</b> The advent of ‘wide, dirty and mixed’ data</a></li>
<li class="chapter" data-level="9.5" data-path="9-regularizing-and-materializing-objects.html"><a href="9-regularizing-and-materializing-objects.html#cross-validating-machine-learning-in-genomics"><i class="fa fa-check"></i><b>9.5</b> Cross-validating machine learning in genomics</a></li>
<li class="chapter" data-level="9.6" data-path="9-regularizing-and-materializing-objects.html"><a href="9-regularizing-and-materializing-objects.html#proliferation-of-discoveries"><i class="fa fa-check"></i><b>9.6</b> Proliferation of discoveries</a></li>
<li class="chapter" data-level="9.7" data-path="9-regularizing-and-materializing-objects.html"><a href="9-regularizing-and-materializing-objects.html#variations-in-the-object-or-in-the-machine-learner"><i class="fa fa-check"></i><b>9.7</b> Variations in the object or in the machine learner?</a></li>
<li class="chapter" data-level="9.8" data-path="9-regularizing-and-materializing-objects.html"><a href="9-regularizing-and-materializing-objects.html#whole-genome-functions"><i class="fa fa-check"></i><b>9.8</b> Whole genome functions</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="10-propagating-subject-positions.html"><a href="10-propagating-subject-positions.html"><i class="fa fa-check"></i><b>10</b> Propagating subject positions}</a><ul>
<li class="chapter" data-level="10.1" data-path="10-propagating-subject-positions.html"><a href="10-propagating-subject-positions.html#propagation-across-human-machine-boundaries"><i class="fa fa-check"></i><b>10.1</b> Propagation across human-machine boundaries</a></li>
<li class="chapter" data-level="10.2" data-path="10-propagating-subject-positions.html"><a href="10-propagating-subject-positions.html#competitive-positioning"><i class="fa fa-check"></i><b>10.2</b> Competitive positioning</a></li>
<li class="chapter" data-level="10.3" data-path="10-propagating-subject-positions.html"><a href="10-propagating-subject-positions.html#a-privileged-machine-and-its-diagrammatic-forms"><i class="fa fa-check"></i><b>10.3</b> A privileged machine and its diagrammatic forms</a></li>
<li class="chapter" data-level="10.4" data-path="10-propagating-subject-positions.html"><a href="10-propagating-subject-positions.html#varying-subject-positions-in-code"><i class="fa fa-check"></i><b>10.4</b> Varying subject positions in code</a></li>
<li class="chapter" data-level="10.5" data-path="10-propagating-subject-positions.html"><a href="10-propagating-subject-positions.html#the-subjects-of-a-hidden-operation"><i class="fa fa-check"></i><b>10.5</b> The subjects of a hidden operation</a></li>
<li class="chapter" data-level="10.6" data-path="10-propagating-subject-positions.html"><a href="10-propagating-subject-positions.html#algorithms-that-propagate-errors"><i class="fa fa-check"></i><b>10.6</b> Algorithms that propagate errors</a></li>
<li class="chapter" data-level="10.7" data-path="10-propagating-subject-positions.html"><a href="10-propagating-subject-positions.html#competitions-as-examination"><i class="fa fa-check"></i><b>10.7</b> Competitions as examination</a></li>
<li class="chapter" data-level="10.8" data-path="10-propagating-subject-positions.html"><a href="10-propagating-subject-positions.html#superimposing-power-and-knowledge"><i class="fa fa-check"></i><b>10.8</b> Superimposing power and knowledge</a></li>
<li class="chapter" data-level="10.9" data-path="10-propagating-subject-positions.html"><a href="10-propagating-subject-positions.html#ranked-subject-positions"><i class="fa fa-check"></i><b>10.9</b> Ranked subject positions</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="11-conclusion-out-of-the-data.html"><a href="11-conclusion-out-of-the-data.html"><i class="fa fa-check"></i><b>11</b> Conclusion: Out of the Data}</a><ul>
<li class="chapter" data-level="11.1" data-path="11-conclusion-out-of-the-data.html"><a href="11-conclusion-out-of-the-data.html#machine-learners"><i class="fa fa-check"></i><b>11.1</b> 250,000 machine learners</a></li>
<li class="chapter" data-level="11.2" data-path="11-conclusion-out-of-the-data.html"><a href="11-conclusion-out-of-the-data.html#a-summary-of-the-argument"><i class="fa fa-check"></i><b>11.2</b> A summary of the argument</a></li>
<li class="chapter" data-level="11.3" data-path="11-conclusion-out-of-the-data.html"><a href="11-conclusion-out-of-the-data.html#in-situ-hybridization"><i class="fa fa-check"></i><b>11.3</b> In-situ hybridization</a></li>
<li class="chapter" data-level="11.4" data-path="11-conclusion-out-of-the-data.html"><a href="11-conclusion-out-of-the-data.html#critical-operational-practice"><i class="fa fa-check"></i><b>11.4</b> Critical operational practice?</a></li>
<li class="chapter" data-level="11.5" data-path="11-conclusion-out-of-the-data.html"><a href="11-conclusion-out-of-the-data.html#obstacles-to-the-work-of-freeing-machine-learning"><i class="fa fa-check"></i><b>11.5</b> Obstacles to the work of freeing machine learning</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="12-references.html"><a href="12-references.html"><i class="fa fa-check"></i><b>12</b> References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning: Archaeology of a Data Practice</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="diagramming-machines" class="section level1">
<h1><span class="header-section-number">4</span> Diagramming machines</h1>
<p></p>
<blockquote>
<p>Machine learning is not magic; it cannot get something from nothing. What it does is get more from less. Programming, like all engineering, is a lot of work: we have to build everything from scratch. Learning is more like farming, which lets nature do most of the work. Farmers combine seeds with nutrients to grow crops. Learners combine knowledge with data to grow programs. <span class="citation">(Domingos <a href="#ref-Domingos_2012">2012</a>, 81)</span> </p>
</blockquote>
<blockquote>
<p>The tools or material machines have to be chosen first of all by a diagram <span class="citation">(Deleuze <a href="#ref-Deleuze_1988">1988</a><a href="#ref-Deleuze_1988">b</a>, 39)</span>  </p>
</blockquote>
<p>The ‘learning’ in machine learning embodies a change in programming practice or indeed the programmability of machines. Our sense of the potentials of machine learning can be understood, according to Pedro Domingos, in terms of a contrast between programming as ‘a lot of [building] work’ and the ‘farming’ done by machine learners to ‘grow programs.’ In characterising machine learning, the tensions between the programming ‘we’ (programmers, computer scientists?) do and the programming that learners do (‘growing’) are worth pursuing.  While Domingos suggests that machine learners ‘get more from less,’ I will propose that an immense constellation of documents, software, publications, blog pages, books, spreadsheets, databases, data centre architectures, whiteboard and blackboard drawings, and an inordinate amount of talk and visual media orbit around machine learning.  There has been lively growth in machine learning, but this liveliness and the sometimes life-like growth of machine learners are a regional expression of a distributed formation. ‘Wherever there is a region of nature,’ the philosopher Alfred North Whitehead writes ‘which is itself the primary field of the expressions issuing from each of its parts, that region is alive’ <span class="citation">(Whitehead <a href="#ref-Whitehead_1956">1956</a>, 31)</span>. </p>
<p>In this chapter I attend to the problem of identifying and describing the distributed practices that give rise to a sense of machine learners framed as growth, or liveliness. I will argue these practices can only be traced partially through code written in generic or specialized programming languages such as <code>Python</code> and <code>R</code>, in libraries of machine learning code such as <code>R</code>’s <code>caret</code>  or <code>Python</code>’s <code>scikit-learn</code>  or <code>TensorFlow</code>  to do machine learning. Code obscures and reveals multiple transformations at work in the operational formation. Science studies scholars such as Anne-Marie Mol have urged the need to keep practice together with theories of what exists. Mol writes:</p>
<blockquote>
<p>If it is not removed from the practices that sustain it, reality is multiple. This may be read as a description that beautifully fits the facts. But attending to the multiplicity of reality is also an act. It is something that may be done – or left undone <span class="citation">(Mol <a href="#ref-Mol_2003">2003</a>, 6)</span> </p>
</blockquote>
<p>Mol advocates thinking reality as multiplicity. Her insistence on the nexus of practice or doing and the plural existence of things suggests a way of handling the code that machine learners produce.  Code should be approached multiplicity. In the case of machine learners, this means following through pedagogical expositions of machine learning focused on both mathematical derivations and the accumulation of scientific or technical research publications, ranging from textbooks to research articles, that vary, explore, experiment and implement machine learners in code. The effect of liveliness or growth issues from many parts. In relation to machine learning, reading and writing code alongside scientific papers, Youtube lectures, machine learning books and competitions is not only a form of observant participation, but directly forms part of the diagrammatic multiplicity.</p>
<p>While machine learning utterly depends on code, I will suggest therefore that no matter how expressive or well-documented it may be, code alone cannot fully  how machine learners make programs or how they combine knowledge with data. Domingos writes that ‘learning algorithms … are algorithms that make other algorithms. With machine learning, computers write their own programs, so we don’t have to’ <span class="citation">(Domingos <a href="#ref-Domingos_2015a">2015</a>, 6)</span>. Yet the writing performed by machine learners cannot be read textually or procedurally as other programs might be read for instance in work known as critical code studies. The difference between the reading of an Atari computer game console in Nick Montfort and Ian Bogost’s <em>Racing the Beam</em> <span class="citation">(Montfort and Bogost <a href="#ref-Montfort_2009">2009</a>)</span> and the machine learning of Atari’s games undertaken by DeepMind in London during recent years <span class="citation">(Mnih et al. <a href="#ref-Mnih_2013">2013</a>; Mnih et al. <a href="#ref-Mnih_2015">2015</a>)</span> is hard to read from program code.  The learning or making by learning is far from homogeneous, stable or automatic in practice. Materially, code is only one element in the diagram of machine learning. It displays, with greater or lesser degrees of visibility relations between a variety of forces (infrastructures, scientific knowledges, mathematical formalisations, etc.). It itself is aligned by and exposes other institutional, infrastructural, epistemic and economic positions. </p>
<p>Coding practices and the pedagogical expositions of machine learning have shifted substantially over the last decade or so due to the growth in open source programming languages and as a part of the broader and well-known expansion of digital media cultures. The fact that data scientists, software developers and other machine learners across scientific and commercial settings use programming languages such as <code>Python</code> and <code>R</code>   more than specialized commercial statistical and data software packages such as <code>Matlab</code>, <code>SAS</code> or <code>SPSS</code> <span class="citation">(Muenchen <a href="#ref-Muenchen_2014">2014</a>)</span> is perhaps symptomatic of shifts in computational culture. Coding cultures are crucial to the recent growth of machine learning. Although scientific computing languages such as <code>FORTRAN</code> – ‘Formula Translator’ – have long underpinned scientific research and engineering applications in various fields <span class="citation">(Campbell-Kelly <a href="#ref-Campbell-Kelly_2003">2003</a>, 34–35)</span>, the development in recent decades of data-analytic and statistical programming languages and coding frameworks has crystallized a repertoire of standard operations, patterns and functions for reshaping data and constructing models that classify and predict events and associations between things, people, processes, etc. This development continues apace, especially in research and engineering driven by social media and internet platforms such as Facebook and Google. While Domingos speaks of ‘growing’ programs, the accumulating sediment of certain well-established data practices are the soil in which programs take root. The different elements of coding practice are precisely the faceted levels of abstraction which we need to access and traverse in order to know and come to grips empirically with contemporary compositions of power and knowledge in machine learning. </p>
<div id="we-dont-have-to-write-programs" class="section level2">
<h2><span class="header-section-number">4.1</span> ‘We don’t have to write programs’?</h2>
<p>In machine learning, coding changes from what we might call symbolic logical diagrams to statistical algorithmic diagrams. While many machine learning techniques have long statistical lineages (running back to the 1900s in the case of Karl Pearson’s development of the still-heavily used Principal Component Analysis <span class="citation">(Pearson <a href="#ref-Pearson_1901">1901</a>)</span>),  machine learning techniques often embody a certain dissatisfaction with the classical computer science understanding of programs as manipulation of symbols, even as they rely on such symbolic operations to function. Symbolic manipulation, epitomised by deductive logic or predicate calculus, was very much at the centre of many AI projects during the 1950s and 1960s <span class="citation">(Dreyfus <a href="#ref-Dreyfus_1972">1972</a>; Edwards <a href="#ref-Edwards_1996">1996</a>)</span>.  In machine learning, the privileged symbolic-cognitive forms of logic are subject to a statistical transformation.</p>
<p>Take for instance once of the most common operations of the Boolean logical calculus, the <code>NOT-AND</code> or <code>NAND</code> function shown in table <a href="#tab:boolean"><strong>??</strong></a>. The truth table summarises a logical function  that combines three input variables <code>X1</code>, <code>X2</code>, and <code>X3</code> and produces the output variable <code>Y</code>. Because in Boolean calculus, variables or predicates can only take the values <code>true</code> or <code>false</code>, they can be coded in as <code>1</code> and <code>0</code>.</p>
<table>
<caption>The truth table for the Boolean function NOT-AND truth </caption>
<thead>
<tr class="header">
<th>X1</th>
<th>X2</th>
<th>X3</th>
<th align="left">Y</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>0</td>
<td>0</td>
<td align="left">1</td>
</tr>
<tr class="even">
<td>0</td>
<td>0</td>
<td>1</td>
<td align="left">1</td>
</tr>
<tr class="odd">
<td>0</td>
<td>1</td>
<td>1</td>
<td align="left">1</td>
</tr>
<tr class="even">
<td>0</td>
<td>1</td>
<td>0</td>
<td align="left">1</td>
</tr>
<tr class="odd">
<td>1</td>
<td>0</td>
<td>0</td>
<td align="left">1</td>
</tr>
<tr class="even">
<td>1</td>
<td>0</td>
<td>1</td>
<td align="left">1</td>
</tr>
<tr class="odd">
<td>1</td>
<td>1</td>
<td>0</td>
<td align="left">1</td>
</tr>
<tr class="even">
<td>1</td>
<td>1</td>
<td>1</td>
<td align="left">0</td>
</tr>
</tbody>
</table>
<p>Now in Foucaultean terms, the truth table and its component propositions constitutes a statement. This statement has triple relevance for  of machine learning.  The spatial arrangement of the table is fundamental (and this is the topic of chapter <a href="#ch:vector"><strong>??</strong></a>).  Most datasets come as tables, or end up as tables at some point in their analysis. Second, the elements or cells of this table are numbers. The numbers <span class="math inline">\(1\)</span> and <span class="math inline">\(0\)</span> are the binary digits as well as the ‘truth’ values ‘True’ and ‘False’ in classical logic. These numbers are readable as symbolic logical propositions governed by the rule <span class="math inline">\(Y = \neg{X_1 \and X_2 \and X_3}\)</span>. The table acts as a hinge between numbers and symbolic thought or cognition. Third, the NAND table in particular has an obvious operational relevance in digital logic, since digital circuits of all kinds – memory, processing, and communication – comprise such logical functions knitted together in the intricate gated labyrinths of contemporary calculation.<a href="#fn7" class="footnoteRef" id="fnref7"><sup>7</sup></a> </p>
<p>A pre-machine learning programmer, tasked with implementing the logical NAND function might write:</p>

<p>The trivial simplicity of the code stands out. This looks like the kind of symbol manipulation that that computers can easily be programmed to do. How, by contrast, could such a truth table be learned by a machine, even a machine whose <em>modus operandi</em> and indeed whose very fabric is nothing other than a massive mosaic of <code>NAND</code> operations inscribed in semiconductors? Machine learning of on elementary truth tables is not a typical operation today, but usefully illustrates something of the diagrammatic transformations that programming or coding (and classical AI) has undergone .</p>
<p>Machine learners such as decision trees or neural nets typically know nothing of the logical calculus and its elementary logical operations. Can they be induced to learn it? A perceptron, an elementary machine learner dating from the 1950s, that ‘learns’ the binary logical operation NAND (Not-AND) is expressed in twenty lines of python code on the Wikipedia ‘Perceptron’ page <span class="citation">(Wikipedia <a href="#ref-Wikipedia_2013">2013</a>)</span> (see listing ). It is a standard machine learner almost always included in machine learning textbooks and usually taught in introductory machine learning classes. The code outputs a series of numbers shown in table @ref(tab:nand_weights).</p>


<p>What does the code in listing  show or say about the transformation in programmability or the writing of programs?  First of all, we should note the relative conciseness of the code vignette.  Much of the code here is familiar, generic programming. It defines variables, sets up data structures (lists of numerical values), checks conditions, loops through statements or prints results. In citing this code, I am not resorting to a technical publication or scientific literature as such, or even to a machine learning software library or package (in <code>scikit-learn</code>, the same model could be written <code>p = scikit-learn.linearmode.Perceptron(X,Y)</code>), just to a Wikipedia page, and the relatively generic and widely used programming language <code>Python</code>.[^1.31] Whatever the levels of abstraction associated with machine learning, the code is hardly ever hermetically opaque. As statements, everything lies on the surface. </p>
<p>Second, while the shaping of data, the counting of errors, and the optimisation of models are topics of later discussion, the code shows some typical features of a machine learner in the form of elements such the <code>learning rate</code>, a <code>training_set,</code> <code>weights</code>, an <code>error count</code>, and a loop function that multiplies values (<code>dot_product</code>). Some of the names such as <code>learning_rate</code> or <code>error_count</code> present in the code bear the marks of the theory of learning machines that we will discuss.</p>
<p>Third, executing this code (by copying and pasting it into a <code>Python</code> terminal, for instance) produces several dozen lines of numbers. They are initially different to each other, but gradually converge on the same values (see table @ref(tab:nand_weights)). These numbers are the ‘weights’ of the nodes of the perceptron as it iteratively ‘learns’ to recognise patterns in the input values. None of the workings of the perceptron need concern us at the moment. Again, what runs across all of these observations are the numbers that the algorithm produces as output – they embody the program the perceptron has written. How has the learning happened in code? The NAND truth table has been re-drawn as a dataset (see line 4 of the code that defines the variable <code>training_set</code>). The perceptron has learned the data by approaching it as a set of training examples, and then adjusting its internal model – the weights that are printed during each loop of the model as the output – repeatedly until the model is producing the correct result values <span class="math inline">\(Y\)</span> of the truth table. The algorithm exits its main loop (<code>while True:</code>) when there are no errors.</p>
<p>The perceptron algorithm computes numbers - <code>0.79999</code>, <code>2.0</code> – as weights or parameters.  These numbers display no direct correspondence with the symbolic categories of boolean True and False or the binary digits <code>1</code> and <code>0</code>. There may be a relation but it is not obvious at first glance. The problem of mapping these calculated parameters – and they truly abound in machine learning – triggers many different diagrammatic movements in machine learning (and these different movements will be discussed in chapters <a href="#ch:vector"><strong>??</strong></a> and ). These numbers engender much statistical ratiocination (see chapter ). Here we need only note the contrast between symbolically organised statements like the NAND truth table of Table <a href="#tab:boolean"><strong>??</strong></a> and the operational statements in table <a href="#tab:nand-weights"><strong>??</strong></a>.</p>
<p>The operation here is recursive: a model or algorithm implemented in digital logic (as <code>Python</code> code) has ‘learned’ – a term we need to explore more carefully – a basic rule of digital logic (the <code>NAND</code> function) at least approximately by treating logical propositions as data. This mode of transformation is symptomatic. The learning done in machine learning has few cognitive or symbolic underpinnings. It differs from classical AI in that it treats existing symbolic, control, communicative and increasingly, signifying processes (such as the cat faces that <code>kittydar</code> tries to find), and latches onto them programmatically only in the form of weights.</p>
</div>
<div id="the-elements-of-machine-learning" class="section level2">
<h2><span class="header-section-number">4.2</span> The elements of machine learning</h2>
<p>If this growing of programs through modelling data is a different mode of coding operations, and a different mode of knowing, how does one learn to do it?  In the course of writing this book, as well as reading the academic textbooks, popular how-to books, software manuals, help documents and blog-how-to posts, I attended graduate courses in Bayesian statistics, genomic data analysis, data mining, and missing data. I also participated in online machine learning courses and some machine learning competitions. These are all widely shared activities for people learning to do machine learning. I watched and copied down by hand many statements, equations and propositions from Youtube videos of the eighteen lectures in Andrew Ng’s  Stanford University lectures CS229 computer science course recorded in 2008. These lectures have cumulative viewing figures of around 500,000 <span class="citation">(A. Ng <a href="#ref-Ng_2008">2008</a><a href="#ref-Ng_2008">a</a>)</span>. I spent extended hours learning to use various code libraries, packages and platforms in <code>R</code>, <code>Python</code> and to a limited extent <code>Torch</code> . Finally, I gradually accumulated and worked with a set of around 400,000 articles drawn from various fields of science in response to search queries on particular machine learners such as <code>support vector machine</code> or <code>expectation maximization</code>.  This accumulation of knowledge appears in figures <a href="4-diagramming-machines.html#fig:vast-diagram">4.1</a> as a network-cloud.</p>
<div class="figure"><span id="fig:vast-diagram"></span>
<img src="figure/vast_diagram.png" alt="Elements of learning to machine learn" width="2480" />
<p class="caption">
Figure 4.1: Elements of learning to machine learn
</p>
</div>
<p>Some broadly shared topic structures help in any navigation of the software libraries, pedagogical and research literatures. The textbooks, the how-to recipe books <span class="citation">(Segaran <a href="#ref-Segaran_2007">2007</a>; Kirk <a href="#ref-Kirk_2014">2014</a>; Russell <a href="#ref-Russell_2011">2011</a>; Conway and White <a href="#ref-Conway_2012">2012</a>)</span> and the online university courses on machine learning often have a similar topic structure.  They nearly always begin with ‘linear models’ (fitting a line to the data), then move to logistic regression (a way of using the linear model to classify binary outcomes; for example, spam/not-spam; malignant/benign; cat/non-cat), and afterwards move to some selection of neural networks, decision trees, support vector machine and clustering algorithms.  They add in some decision theory, techniques of optimization, and ways of selecting predictive models (especially the bias-variance trade-off).<a href="#fn8" class="footnoteRef" id="fnref8"><sup>8</sup></a> The topic structures have in recent years started to become increasingly uniform. This coagulation around certain topics, problems and mathematical formalisms is both something worth analyzing (since, for instance, it definitely affects how machine learning is taken up in different settings), but should not be taken as obvious or given since it results from many iterations.</p>
<p>Amidst this avalanching machine learning materials and practice, a single highly cited and compendious textbook, <em>Elements of Statistical Learning: Data Mining, Inference, and Prediction</em> dating from from around 2000 and currently in its second edition <span class="citation">(Hastie, Tibshirani, and Friedman <a href="#ref-Hastie_2009">2009</a>)</span>, can be seen from almost any point of the terrain.<a href="#fn9" class="footnoteRef" id="fnref9"><sup>9</sup></a>  At least for archaeological purposes, I regard this book as an assemblage,  and a diagram that presents many important statements, forms of visibility and relations between forces at work in machine learning. The authors of the book, Jeff Hastie, Rob Tibshirani and Jerome Friedman    are statisticians working at Stanford and Columbia University.</p>
<p><em>Elements of Statistical Learning</em> is a massive textual object, densely radiant with equations, tables, algorithms, graphs and references to other scientific literature. From the first pages proper of the book, almost every page has a figure or a table or a formal algorithm (counting these together: 1670 equations; 291 figures; 34 tables; and 94 algorithms, giving a total of 2089 operational statements threaded through the book). Equations rivet the text with mathematical abstractions of varying sophistication. On each page of the book we are seeing, reading, puzzling over and perhaps learning from the products of code execution. The graphic figures are all produced by code. The tables are mostly produced by code. The algorithms specify how to implement code, and the equations diagram various operations, spaces and movements that meant to run as code.</p>
<p>In the range of references, combinations of code, diagram, equation, scientific disciplines and computational elements, and perhaps in the somewhat viscous, inter-objectively diverse referentiality that impinges on any reading of it, <em>Elements of Statistical Learning</em> betrays some hyperobject-like positivity  <span class="citation">(Morton <a href="#ref-Morton_2013">2013</a>)</span>.  It is an accumulation of forms, techniques, practices, propositions and referential relations. <em>Elements of Statistical Learning</em> combines statistical science with various algorithms to ‘learn from data’ <span class="citation">(Hastie, Tibshirani, and Friedman <a href="#ref-Hastie_2009">2009</a>, 1)</span>. The data range across various kinds of problems (identifying spam email, predicting risk of heart disease, recognising handwritten digits, etc.). The learning takes the form of various machine learning techniques, methods and algorithms (linear regression, k-nearest neighbours, neural networks, support vector machines, the Google Page Rank algorithm, etc.).</p>
<p>There are other juggernaut machine learning textbooks.  Ethem Alpaydin’s <em>Introduction to Machine Learning</em> <span class="citation">(Alpaydin <a href="#ref-Alpaydin_2010">2010</a>)</span> (a more computer science-base account), Christopher Bishop’s heavily mathematical <em>Pattern recognition and machine learning</em> <span class="citation">(Bishop <a href="#ref-Bishop_2006">2006</a>)</span>, Brian Ripley’s luminously illustrated and almost coffee-table formatted <em>Pattern Recognition and Neural Networks</em> <span class="citation">(Ripley <a href="#ref-Ripley_1996">1996</a>)</span> , Tom Mitchell’s earlier artificial intelligence-centred <em>Machine learning</em> <span class="citation">(Mitchell <a href="#ref-Mitchell_1997">1997</a>)</span> , Peter Flach’s perspicuous <em>Machine Learning: The Art and Science of Algorithms that Make Sense of Data</em> <span class="citation">(Flach <a href="#ref-Flach_2012">2012</a>)</span> , or further afield, the sobering and laconic <em>Statistical Learning for Biomedical Data</em> <span class="citation">(Malley, Malley, and Pajevic <a href="#ref-Malley_2011">2011</a>)</span> all cover a similar range of data and approaches. These and quite a few other recent machine learning textbooks display a range of emphases, ranging from the highly theoretical to the very practical, from an orientation to statistical inference to an emphasis on computational processes, from science to commercial applications.</p>
</div>
<div id="who-reads-machine-learning-textbooks" class="section level2">
<h2><span class="header-section-number">4.3</span> Who reads machine learning textbooks?</h2>
<p>How does such textbook help us assess and engage with claim to learn from data or to produce knowledge differently? While it certainly does not comprehend everything taking place in and around machine learning, it diagrams several <em>elementary</em> tendencies or traits.  It’s readership as we will see is widespread. It has a heterogeneous texture in terms of the examples, formalisms, disciplines and domains it covers. It starkly renders the problems of making sense of mathematical operations, diagrams and transformations carried on through calculation, simulation, deduction or analysis. It draws on a matrix of operational practices, particularly in the form of the <code>R</code> code it heavily but somewhat latently relies on. In short, <em>Elements of Statistical Learning</em> presents a multi-faceted and somewhat monumental layering of abstractive practice that might be open to archaeological inquiry.</p>

<p>Who reads the <em>Elements of Statistical Learning</em>?  It is often cited by academic machine learning practitioners as an authoritative guide. On the other hand, students participating in new data science courses often come from different disciplinary backgrounds and find the tome unhelpful (see the comment by students during an introductory data science course documented in <span class="citation">(Schutt and O’Neil <a href="#ref-Schutt_2013">2013</a>)</span>). Whether the citations are friendly or not, it is hard to find a field of contemporary science, engineering, natural, applied, health and indeed social science that has not cited it. A Thomson-Reuters Scientific ‘Web of Science’(TM) search for references citing either the first or second edition of <span class="citation">(Hastie, Tibshirani, and Friedman <a href="#ref-Hastie_2009">2009</a>)</span> yields around 9000 results. These publications sprawl across over 100 different fields of research.  While computer science, mathematics and statistics dominate, a very diverse set of references comes from disciplines from archaeology, through fisheries and forestry, genetics, robotics, telecommunications and toxicology ripple out from this book since 2001. Table @ref(tab:fields_citing_hastie) shows the top 20 fields by count. One could learn something about the diagrammatic movement of machine learners from that reference list, which itself spans biomedical, engineering, telecommunications, ecology, operations research and many other fields. While it is not surprising to see computer science, mathematics and engineering appearing at highest concentration in the literature, molecular biology, control and automation, operation research, business and public health soon appear, suggesting something of the propagating accumulation or positivity of machine learning. </p>
<div class="figure"><span id="fig:hastie-pages"></span>
<img src="_main_files/figure-html/hastie-pages-1.png" alt="Pages cited from     extit{Elements of Statistical Learning }" width="1152" />
<p class="caption">
Figure 4.2: Pages cited from extit{Elements of Statistical Learning }
</p>
</div>
<p>So we know that <em>Elements of Statistical Learning</em> passes into many fields. But what do people read in the textual environment of book? In general, the thousands of citations of the book themselves compose a diagram of the book’s intersection with different domains of knowledge. The relative concentrations and sparsities of these citations suggest there may be specific sites of engagement in the techniques, approaches and machines that the book documents. Of the around 760 pages in the two editions (<span class="citation">(Hastie, Tibshirani, and Friedman <a href="#ref-Hastie_2009">2009</a>)</span> and <span class="citation">(Hastie, Tibshirani, and Friedman <a href="#ref-Hastie_2001">2001</a>)</span>), around 83 distinct pages are referenced in the citing literature. As figure <a href="4-diagramming-machines.html#fig:hastie-pages">4.2</a> indicates, certain portions of the book are much more heavily cited than others. This distribution of page references in the literature that cites <em>Elements of Statistical Learning</em> is a rough guide to how the book has been read in different settings. For instance, the most commonly cited page in the book is page 553. That page begins a section called ‘Non-negative Matrix Factorization’,  a technique frequently used to process digital images to compress their visual complexity into a simpler set of visual signals <span class="citation">(Hastie, Tibshirani, and Friedman <a href="#ref-Hastie_2009">2009</a>, 553)</span>. (The underlying reference here is the highly cited paper <span class="citation">(Lee and Seung <a href="#ref-Lee_1999">1999</a>)</span>) Like <code>kittydar</code>, it, as Hastie and co-authors write, ‘learns to represent faces with a set of basis images resembling parts of faces’ <span class="citation">(Hastie, Tibshirani, and Friedman <a href="#ref-Hastie_2009">2009</a>, 555)</span>. (So <code>kittydar</code>, which doesn’t use NMF, might do better if it did, because it could work just with parts of the images that lie somewhere near the parts of a cat’s face – its nose, its eyes, its ears.) </p>
<p>Conversely, what do the authors of <em>Elements of Statistical Learning</em> read? The book gathers elements from many different quarters and seeks to integrate them in terms of statistical theory. The hyperobject-like aspect of the book comes from the thick weave of equations, diagrams, tables, algorithms, bibliographic apparatus, and numbers wreathed in typographic ornaments drawn from many other sources. For instance, in terms of outgoing references or the literature that it cites, <em>Elements of Statistical Learning</em> webs together a field of scientific and technical work with data and predictive models ranging across half a century. The reference list beginning at page 699 <span class="citation">(Hastie, Tibshirani, and Friedman <a href="#ref-Hastie_2009">2009</a>, 699)</span> runs for around 35 pages, and the five hundred or so references there point in many directions. The weave of these elements differs greatly from citational patterns in the humanities or social sciences. Reading this book almost necessitates an archaeological approach since it comprises so many parts and fragments. </p>

<p>The citational fabric of <em>Elements of Statistical Learning</em> is woven with different threads, some reaching back into early twentieth statistics, some from post-WW2 cybernetics, many from information theory and then in the 1980s onwards, increasingly, from cognitive science and computer science (see figure <a href="#fig:hastie-references"><strong>??</strong></a> to get some sense of their distribution over time). While some of these references either point to Hastie, Tibshirani or Friedman’s own publications, or that of their statistical colleagues, the references rove quite widely in other fields and over time. <em>Elements of Statistical Learning</em> as a text is processing, assimilating and recombining techniques, diagrams and data from many different places and times. (The different waves appearing in the references cited in <span class="citation">(Hastie, Tibshirani, and Friedman <a href="#ref-Hastie_2009">2009</a>)</span> will shape discussion in later chapters in certain ways; for instance, late 1990s biology is the topic of chapter <a href="#ch:genome"><strong>??</strong></a> and optimization functions dating from the 1950s are discussed in chapter ).</p>
<p>Both the inward and outward movements of citation suggest that <em>Elements of Statistical Learning</em>, like much in field of machine learning, has a matrix-like character that constantly superimposes and transposes elements across boundaries and barriers. The implication here is that machine learning as a knowledge practice has a highly interwoven texture, and in this respect differs somewhat from the classical understandings of scientific disciplines as bounded by communities of practice, norms and problems (as for instance, in Thomas Kuhn’s account of normal science <span class="citation">(Kuhn <a href="#ref-Kuhn_1996">1996</a>)</span>.  This aggregate or superimposed character of machine learning should definitely figure in any sense we make of it, and will inevitably affect how critical thought might test itself against it. .</p>
</div>
<div id="r-a-matrix-of-transformations" class="section level2">
<h2><span class="header-section-number">4.4</span> <code>R</code>: a matrix of transformations</h2>
<p> Although barely a single line of code appears in <em>Elements of Statistical Learning</em>, all of the learners presented there are implemented in a single programming language, <code>R</code>. Coding is the operational practice that links the different planes and elements of machine learning in an operational formation.   The authors say ‘we used the R and S-PLUS programming languages in our courses’ <span class="citation">(Hastie, Tibshirani, and Friedman <a href="#ref-Hastie_2009">2009</a>, 9)</span> but many elements of the book derive from <code>R</code> code.<a href="#fn10" class="footnoteRef" id="fnref10"><sup>10</sup></a> The proliferation of programming languages such as <code>FORTRAN</code> (dating from the 1950s), <code>C</code> (1970s), <code>C++</code> (1980s), then <code>Perl</code> (1990s), <code>Java</code> (1990s), <code>Python</code> (1990s) and <code>R</code> (1990s), and computational scripting environments such as Matlab, multiplied the paths along which machine learners move through operational formations. It would be difficult to comprehend the propagation of machine learners across domains of science, business and government without paying attention to coding practices. Even if textbooks and research articles are not read, software packages and libraries for machine learning are used. Code has a mobility that extends the diagrammatic practices of machine learning into a variety of settings and places where the scientific reading apparatuses of equations, statistical plots, and citations of research articles would not be operative. </p>

<p>\begin{table}[!htb] \caption[<code>ElemStatLearn</code> depends on R packages]{<code>R</code> packages depended on by the <code>ElemStatLearn</code> package}  \end{table}</p>
<p>How should we think of the <code>R</code> code in <em>Elements of Statistical Learning</em> in its operational specificity?  Its growth is perhaps just as important as its operation <span class="citation">(Mackenzie <a href="#ref-Mackenzie_2014">2014</a><a href="#ref-Mackenzie_2014">b</a>)</span>. An open source programming language, according to surveys of business and scientific users, at the time of writing, <code>R</code> has replaced popular statistical software packages such as SPSS, SAS and Stata as the statistical and data analysis tool of choice for many people in business, government, and sciences ranging from political science to genomics, from quantitative finance to climatology <span class="citation">(RexerAnalytics <a href="#ref-RexerAnalytics_2015">2015</a>)</span>. Developed in New Zealand in the mid-1990s, and like many open source software projects, emulating <code>S</code>, a commercialised predecessor developed at AT &amp; T Bell Labs during the 1980s, <code>R</code> is now extremely widely used across life and physical sciences, as well as quantitative social sciences. John Chambers, the designer of <code>S</code>, was awarded the Association for Computing Machinery (ACM) ‘Software System Award’ in 1998 for ‘the S system, which has forever altered how people analyze, visualize, and manipulate data’ <span class="citation">(ACM <a href="#ref-ACM_2013">2013</a>)</span>.  Many undergraduate and graduate students today earn <code>R</code> as a basic tool for statistics. Skills in <code>R</code> are often seen as essential pre-requisite for scientific researchers, especially in the life sciences. (In engineering, <code>Matlab</code> is widely used.) Research articles and textbooks in statistics commonly both use <code>R</code> to demonstrate methods and techniques, and create <code>R</code> packages to distribute the techniques and sample data. Nearly all of these publication-related software packages, including quite a few from the authors of <em>Elements of Statistical Learning</em> are soon or later available from the ‘Comprehensive <code>R</code> Archive Network (CRAN)’ <span class="citation">(CRAN <a href="#ref-CRAN_2010">2010</a>)</span>. Estimates of its number of users range between 250000 and 2 million. Increasingly, <code>R</code> is integrated into commercial services and products (for instance, SAS, a widely used business data analysis system now has an <code>R</code> interface; Norman Nie, one of the original developers of the SPSS package heavily used in social sciences, now leads a business, Revolution, devoted to commercialising <code>R</code>; <code>R</code> is heavily used at Google, at FaceBook, and by quantitative traders in hedge funds; in 2013 <a href="http://www.r-bloggers.com/r-usage-skyrocketing-rexer-poll/">‘R usage is sky-rocketing’</a>; etc.). In general terms, <code>R</code> has a kind of disciplinary polyglot currency as a form of expression, and exhibits a fine-grained relationality with many different epistemic and operational situations associated with machine learning.</p>
<p>Two early proponents of <code>R</code> and <code>S</code> describe the motivation for the language:</p>
<blockquote>
<p>The goal of the S language … is “to turn ideas into software, quickly and faithfully” … it is the duty of the responsible data analysts to engage in this process … the exercise of drafting an algorithm to the level of precision that programming requires can in itself clarify ideas and promote rigorous intellectual scrutiny. … Turning ideas into software in this way need not be an unpleasant duty. <span class="citation">(Venables and Ripley <a href="#ref-Venables_2002">2002</a>, 2)</span>  </p>
</blockquote>
<p>Bill Venables and Brian Ripley, statisticians working on developing <code>S</code>, the almost identical commercial predecessor to <code>R</code>, wrote in the early 1990s of the responsibility of data analysts to write not just use software.   They write ‘software’ here not in the sense of a product, but in the sense that today would more likely be called ‘code.’ Their sense of coding and programming as clarifying and concretising ideas with precision – as abstractions – has thoroughly taken hold in contemporary data analysis. If code, as they suggest, entails a threshold of idealisation, it differs from mathematical formalization in that it changes the positions and relations of knowledge to include machines, devices and infrastructures.</p>
<p>While the view that code is a precise expression of ideas makes sense, it does not capture the relational complexity of <code>R</code> code as it operates in a setting such as <em>Elements of Statistical Learning</em>. In machine learning, code, along with mathematics, is a primary operational form that ideas take as they become machine learners. But code as an expressive operation by which ‘an individual formulates an idea’ or as ‘a rational activity that may operate in a system of inference’ <span class="citation">(Foucault <a href="#ref-Foucault_1972">1972</a>, 117)</span> does not exhaust and should not be conflated with operational practice in machine learning.  Similarly, the intersection of <code>R</code> with machine learning also lies somewhat at odds with Pedro Domingos’ characterisation of machine learning as a shift away from people building to learners growing programs.</p>

<p>What in <code>R</code> (let alone other programming languages) overflows both the ideas of code as expression of ideas and code as automation? Alongside expression and automation, much <code>R</code> code furnishes a matrix of practice crossing network infrastructures, display screens, statistical techniques, software engineering architectures as well as publication and documentation standards. For instance, the line of <code>R</code> code shown in the listing  when executed opens another way of reading <em>Elements of Statistical Learning</em> and getting a feel for the dragnet of practical relations and infrastructural configurations running through it. Take the part of the line <code>dependencies = 'Suggests'</code>. When the line of code executes, the stipulation of <code>dependencies</code> leads to a quite wide-ranging installation event that installs many <code>R</code> packages. If the installation works (and that assumes quite a lot of configuration work has already taken place; for instance, installing a recent version of the <code>R</code> platform), then <em>Elements of Statistical Learning</em> is now augmented by various pieces of code, and by various datasets that in some ways echo or mimic the book but in other ways extend it operationally (see tables @ref(tab:elemstat_learn_depends) and @ref(tab:elemstat_learn_suggests)).<a href="#fn11" class="footnoteRef" id="fnref11"><sup>11</sup></a>  These code elements are often stunningly specialised. As Karl Marx wrote of the 500 different hammers made in Birmingham, ‘not only is each adapted to one particular process, but several varieties often serve exclusively for the different operations in one and the same process’ <span class="citation">(Marx <a href="#ref-Marx_1986">1986</a>, 375)</span> . Something similar holds in <code>R</code>: thousands of software packages in online repositories suggest that a highly specialised division of labour and possibly refined co-operative labour processes operate around data. </p>
<p>Because of this almost incoherent plurality, and its labile status as both a programming environment and a statistical analysis package, <code>R</code> is an evocative object, to use the psychoanalyst Christopher Bollas’ term <span class="citation">(Bollas <a href="#ref-Bollas_2008">2008</a>)</span> , an object through which many different ways of thinking circulate. Standing somewhere at the intersection of statistics and computing, modelling and programming, many different disciplines, techniques, domains and actors intersect in <code>R</code>. It engages immediately, practically and widely with words, numbers, images, symbols, signals, sensors, forms, instruments and above all abstract forms such as mathematical functions like probability distributions and many different architectural forms (vectors, matrices, arrays, etc.), as it employs data.  If, as Bollas suggests, ‘our encounter, engagement with, and sometimes our employment of, actual things is a <em>way</em> of thinking’ <span class="citation">(Bollas <a href="#ref-Bollas_2008">2008</a>, 92)</span>,  then it plausible that <code>R</code> not only gathers a plurality of data practices – working with measurements, numbers, text, images, models and equations, with techniques for sampling and sorting, with probability distributions and random numbers – but that it embodies the kernel of a mode of thought relevant to contemporary realities. </p>
</div>
<div id="the-obdurate-mathematical-glint-of-machine-learning" class="section level2">
<h2><span class="header-section-number">4.5</span> The obdurate mathematical glint of machine learning</h2>
<p>If scientific research literature and operational <code>R</code> code constitute the elements of an operational formation presented in <em>Elements of Statistical Learning</em>, what of the mathematics? While references from many different places flow in and out of <em>Elements of Statistical Learning</em>, they are nearly all articulated in mathematical form. Machine learning as a grouping of statements relies heavily on mathematics. Given that mathematics is itself diverse and multi-stranded, what kind of mathematics matters here? While later chapters will explore some of the main mathematical practices (linear algebra, statistical inference, etc.), machine learners in <em>Elements of Statistical Learning</em> coalesce around a single exemplary technique: linear regression models or fitting a line to points.  The linear regression model is pivotal, not just in <em>Elements of Statistical Learning</em> but in much of the scientific and engineering literature. The linear regression model pushes up some of the citational peaks in Figure @ref(fig:hastie_cited_refs). Even though it is an old technique dating back to Francis Galton in the 1890s (see <span class="citation">(Stigler <a href="#ref-Stigler_1986">1986</a>, chap. 8)</span>),  it remains perhaps the central working element of machine learning. </p>
<p><em>Elements of Statistical Learning</em> acknowledges the statistical legacy and inheritance in machine learning:</p>
<blockquote>
<p>The linear model has been a mainstay of statistics for the past 30 years and remains one of our most important tools. Given a vector of inputs <span class="math inline">\(X^T = (X_1 , X_2, . . . , X_p)\)</span>, we predict the output <span class="math inline">\(Y\)</span> via the model <span class="math display">\[\hat{Y} = \hat{\beta_0}  + \sum^p_(j=1)X_j\hat{\beta_j)}\]</span> The term <span class="math inline">\(\hat{\beta_0}\)</span> is the intercept, also known as the <em>bias</em> in machine learning <span class="citation">(Hastie, Tibshirani, and Friedman <a href="#ref-Hastie_2009">2009</a>, 11)</span>.</p>
</blockquote>
<p>In the course of the book, this mathematical expression appears in many variations, iterations, expansions and modifications (‘ridge regression’; ‘least angle regression’; ‘project pursuit,’ etc.). But this introduction of the ‘mainstay of statistics,’ the linear model, already introduces a diagrammatic element – the mathematical equation – that is perhaps the most prominent feature in the text.</p>
<p>Any reading of the book has to work out a way to traverse the forms show in equation .</p>
<div class="figure"><span id="fig:linear-model-labelled"></span>
<img src="figure/formula_labelled.svg" alt="The linear regression model"  />
<p class="caption">
Figure 4.3: The linear regression model
</p>
</div>
<p>In its relatively compressed typographic weave, expressions such as Equation @ref(fig:linear_model) operationalize movements through data or ‘a vector of inputs <span class="math inline">\(X^T\)</span>.’ These expressions, which are not comfortable reading for non-technical readers, are however worth looking at carefully if we want to move ‘at the same level of abstraction as the algorithm’ <span class="citation">(Pasquinelli <a href="#ref-Pasquinelli_2015">2015</a>)</span>.  They can be found in hundreds in <span class="citation">(Hastie, Tibshirani, and Friedman <a href="#ref-Hastie_2009">2009</a>)</span>, but also in many other places. While their presence distinguishes machine learning from parts of computer science where mathematical equations are less common, mathematical formalizations also allow the book to suture a panoply of scientific publications and datasets in fields of statistics, artificial intelligence and computer science. Along with the citations, the graphical plots, and the code relationality, these equations are integral connective tissue in machine learning.</p>
<p>In dealing with the obscuring dazzle of mathematical formalisation, I find it useful here to follow Charles Sanders Peirce account of mathematics.  ‘Mathematical reasoning,’ he writes, ‘is diagrammatic’ <span class="citation">(Peirce <a href="#ref-Peirce_1998">1998</a>, 206)</span>.   For Peirce, we should see mathematics, whether it takes an algebraic or geometrical form, whether it appears in symbols, letters, lines or curves as diagrams. For Peirce, a diagram is a kind of ‘icon.’  The icon is a sign that resembles the object it refers to: it has a relation of likeness. What likeness appears in equation @ref(fig:linear_model)? Peirce says: ‘many diagrams resemble their objects not all in their looks; it is only in respect to the relations of their parts that their likeness consists’ <span class="citation">(Peirce <a href="#ref-Peirce_1998">1998</a>, 13)</span>. As we have seen in the perceptron code, machine learners can be expressed in statements in a programming language like <code>Python</code> or <code>R.</code> In code, however, the relations between parts cannot be observed in the same way as they can in the algebraic form. The ‘very idea of the art,’ as Peirce puts it <span class="citation">(Peirce <a href="#ref-Peirce_1992">1992</a>, 228)</span>, of algebraic expressions is that the formulae can be manipulated, or that component elements can be moved without much effort through substitutions, transformations and variations. The graphic form of the expression include the various classical Greek operator symbols such as  or , as well as the letters <span class="math inline">\(x, y, z\)</span> and the indices (indexical signs) such as <span class="math inline">\(j\)</span>  that appear in subscript or superscript, as well as the spatial arrangement of all these elements in lines and sometimes arrays. A variety of relations run between these different symbols and spatial arrangements. For instance, in all such expressions, the relation between the left hand side of the ‘=’ and the right hand side is very important. By convention, the left hand side of the expression is the value that is predicted or calculated (the ‘response’ variable) and the right hand side are the input variables or ‘features’ that contribute data to the model or algorithm.  This spatial arrangement fundamentally affects the design of algorithms. In the case of equations , the ‘^’ over <span class="math inline">\(\hat{Y}\)</span> symbolises a predicted value rather than a value that can be known completely through deduction, derivation or calculation. This distinction between predicted and actual values organizes a panoply of different practices and imperatives (for instance, to investigate the disparities between the predicted and actual values – machine learning practitioners spend a lot of time on that problem).</p>
<p>The broad point is that the whole formulae is a diagram, or an icon that ‘<em>exhibits</em>, by means of the algebraical signs (which are not themselves icons), the relations of the quantities concerned’ <span class="citation">(Peirce <a href="#ref-Peirce_1998">1998</a>, 13)</span>. Because diagrams suppress so many details, they allow one to focus on a selected range of relations between parts. This affordance of diagrammatic mathematical forms is extremely important in the operational formation of machine learning. Diagrams can diagram other diagrams. Operations can themselves become the subject of operations. The nesting of diagram is highly generative since it allows what Peirce calls ‘transformations’ <span class="citation">(Peirce <a href="#ref-Peirce_1998">1998</a>, 212)</span> or the construction of ‘a new general predicate’<span class="citation">(Peirce <a href="#ref-Peirce_1992">1992</a>, 303)</span>.<a href="#fn12" class="footnoteRef" id="fnref12"><sup>12</sup></a> </p>
<p>While I seek to relate to the equations as diagrams, and will present a selection of them (nowhere near as many as found in <em>Elements of Statistical Learning</em>) in the following pages, I am not assuming their operation is transparent or fully legible. Just as much as the analysis of a photograph, a literary work or an ethnographic observation, their diagrammatic composition calls for repeated consideration. Peirce advises not to begin with examples that are too simple: ‘in simple cases, the essential features are often so nearly obliterated that they can only be discerned when one knows what to look for’ <span class="citation">(Peirce <a href="#ref-Peirce_1998">1998</a>, 206)</span>. He also suggests ‘it is of great importance to return again and again to certain features’ <span class="citation">(Peirce <a href="#ref-Peirce_1998">1998</a>, 206)</span>. Looking at these diagrammatic expressions repeatedly might allows us to map something of how transformations, generalisations or intensification flow across disciplinary boundaries, across social stratifications, and sometimes, generate potentially different ways of thinking about collectives, inclusion and belonging.</p>
</div>
<div id="cs229-2007-returning-again-and-again-to-certain-features" class="section level2">
<h2><span class="header-section-number">4.6</span> CS229, 2007: returning again and again to certain features</h2>
<p>If we were to follow Peirce’s injunction to ‘return again and again to certain features,’ how would we do that? <em>Elements of Statistical Learning</em> is a difficult book to read in isolation (although it does pay re-reading). Even after several years, the diagrammatic density of its ‘elements’ or statements (equations, citations, tables, datasets, plots) leaves me with a refractory feeling of ‘not quite understanding.’ This feeling is inevitable because the book condenses finished work from several disciplines, and partly because it seeks to organize a great diversity of materials <em>epistemically.</em> Indeed, the book might seen as evidence that machine learning has crossed an epistemic threshold formulated in a statistical apparatus.  (The statistical aspects of machine learning are the main topic of chapter <a href="#ch:probability"><strong>??</strong></a>).</p>
<p>Does a computer science course offer a more easily followed route? Andrew Ng’s course ‘Machine Learning’ CS229 at Stanford (<a href="http://cs229.stanford.edu/" class="uri">http://cs229.stanford.edu/</a>) might provide a supplementary path into machine learning <span class="citation">(A. Ng <a href="#ref-Ng_2008">2008</a><a href="#ref-Ng_2008">a</a>)</span>.<a href="#fn13" class="footnoteRef" id="fnref13"><sup>13</sup></a>  The course description runs as follows:</p>
<blockquote>
<p>This course provides a broad introduction to machine learning and statistical pattern recognition. Topics include supervised learning, unsupervised learning, learning theory, reinforcement learning and adaptive control. Recent applications of machine learning, such as to robotic control, data mining, autonomous navigation, bioinformatics, speech recognition, and text and web data processing are also discussed <span class="citation">(A. Ng <a href="#ref-Ng_2008">2008</a><a href="#ref-Ng_2008">a</a>)</span></p>
</blockquote>
<p>CS229 is in many ways a typical computer science pedagogical exposition of machine learning. Machine learning expositions usually begin with simple datasets and the simplest possible statistical models and machine learners (linear regression ), and then, with a greater or lesser degree of attention to issues of implementation, move through a succession of increasingly sophisticated and specialised techniques. This pattern is found in many of the how-to books, in the online courses, and in the academic textbooks, including <span class="citation">(Hastie, Tibshirani, and Friedman <a href="#ref-Hastie_2009">2009</a>)</span>.</p>
<p>Ng’s CS229 lectures differ from most other pedagogical materials in that we see someone writing and deriving line after line of equations using chalk on a blackboard. Occasionally, questions come from students in the audience (not shown on the Youtube videos), but mostly Ng’s transcription of equations and other diagrams from the paper notes he holds to blackboard continues uninterrupted.<a href="#fn14" class="footnoteRef" id="fnref14"><sup>14</sup></a> Ng’s Youtube anachronistic but popular lectures have a certain diagrammatic density that comes from the many hours of chalked writing he generates during the course deriving equations. In a time when PowerPoint presentations or some other electronic textuality would very much have been the norm (2007), why is a computer science professor, teaching a fairly advanced postgraduate course, writing on a chalkboard by hand?</p>
<div class="figure"><span id="fig:class-notes"></span>
<img src="figure/ng_lecture_5_generative_discriminative.png" alt="Class notes lecture 5, Stanford CS229, 2007" width="421" />
<p class="caption">
Figure 4.4: Class notes lecture 5, Stanford CS229, 2007
</p>
</div>
<p>Figure <a href="4-diagramming-machines.html#fig:class-notes">4.4</a> shows a brief portion of around 100 pages of notes I made on this course. The act of writing down these equations and copying the many hand-drawn graphs Ng produced was a deliberative diagrammatic experiment in ‘returning again and again’ to what is perhaps overly hardened in <em>Elements of Statistical Learning</em>. Like the 50,000 or so other people who had watched this video, I partly complied with Ng’s injunction to ‘copy it, write it out, cover it, and see if you can reproduce it’ <span class="citation">(<em>Lecture 10 | Machine Learning (Stanford)</em> <a href="#ref-Ng_2008d">2008</a>)</span>. While it occasions much writing and drawing, and many struggles to keep up with the transformations and substitutions that Ng narrates as he writes, it seems to me that writing out derivations, with all their substitutions and variations, alongside the graphic sketches of intuitions about the machine learners, accesses something of the <em>diagrammatic composition</em> of machine learning that is quite hard to track in <em>Elements of Statistical Learning</em>.   There the diagrammatic weave between the expressions of linear algebra, calculus, statistics, and the off-stage implementation in code is almost too tight to work with. In Ng’s CS229 lectures, by contrast, the weaving, while still complex, is much more open. The lectures lack the citational tapestry of <em>Elements of Statistical Learning</em>. They are not able to wield the datasets and the panoply of graphic forms found there, and virtually no machine learning code appears on the blackboard (although the CS229 student assignments, also to be found online, are code implementations of the algorithms and models). But Ng’s lectures move more slowly, and we begin to see some of the different groupings and associations comprising the operational formation. More importantly perhaps, this absorbing process of writing derivations might begin to transform ways of thinking, saying and knowing. </p>
</div>
<div id="the-visible-learning-of-machine-learning" class="section level2">
<h2><span class="header-section-number">4.7</span> The visible learning of machine learning</h2>
<p>For a book with ‘learning’ in its title, <em>Elements of Statistical Learning</em> has visibly little to say about how to learn machine learning.  ‘Learning’ is briefly discussed on the first page of <em>Elements of Statistical Learning</em>, but the book hardly ever returns to the topic or even that term explicitly. We learn on page 2 that a ‘learner’ in machine learning is a model that predicts outcomes. (As I discuss in chapter <a href="#ch:function"><strong>??</strong></a>, learning is comprehensively understood in machine learning as finding a mathematical <em>function</em> that could have generated the data, and optimising the search for that function as much as possible.)  The notion of learning in machine learning derives from the field of artificial intelligence. The broad project of artificial intelligence, at least as envisaged in its 1960s-1970s heyday as a form of symbolic reasoning, is today largely regarded as a dead-end. </p>
<p>How then did learning get into the title of <em>Elements of Statistical Learning</em>? Does ‘learning’ anthropomorphize statistical modelling or computer programming?  The so-called ‘learning problem’ and the theory of learning machines developed by researchers in the 1960-1970s was largely based on work already done in the 1950s on cybernetic devices such as the perceptron, the prototypical neural network model developed by the psychologist Frank Rosenblatt in the 1950s <span class="citation">(Rosenblatt <a href="#ref-Rosenblatt_1958">1958</a>)</span>.   Drawing on the McCulloch-Pitts model of the neurone and mathematical techniques of optimization <span class="citation">(Bellman <a href="#ref-Bellman_1961">1961</a>)</span>, Rosenblatt implemented the perceptron, which today would be called a single-layer neural network <span class="citation">(Hastie, Tibshirani, and Friedman <a href="#ref-Hastie_2009">2009</a>, 393)</span> as an electro-mechanical device at the Cornell University Aeronautical Laboratory in 1957. For present purposes, it is interesting to see what diagrams Rosenblatt used and how they differ from contemporary diagrams.</p>
<div class="figure"><span id="fig:perceptron-1958"></span>
<img src="figure/perceptron_rosenblatt_1958_389.png" alt="The neurological perceptron (Rosenblatt, 1958, 389)" width="407" />
<p class="caption">
Figure 4.5: The neurological perceptron (Rosenblatt, 1958, 389)
</p>
</div>
<div class="figure"><span id="fig:hastie-ann"></span>
<img src="figure/hastie_ann_2009_393.png" alt="Neural network from Hastie (2009); Single layer feedforward artificial neural network" width="298" />
<p class="caption">
Figure 4.6: Neural network from Hastie (2009); Single layer feedforward artificial neural network
</p>
</div>
<p>If we compare the diagram of Rosenblatt’s perceptron shown in Figure <a href="4-diagramming-machines.html#fig:perceptron-1958">4.5</a> to the typical contemporary diagram of a neural network shown in Figure <a href="4-diagramming-machines.html#fig:hastie-ann">4.6</a>, the differences are not that great in many ways. The diagram of a neural network found in Rosenblatt’s paper <span class="citation">(Rosenblatt <a href="#ref-Rosenblatt_1958">1958</a>)</span> has no mathematical symbols on it, but the one from <span class="citation">(Hastie, Tibshirani, and Friedman <a href="#ref-Hastie_2009">2009</a>)</span> does. Rosenblatt’s retains neuro-cognitive-anatomical reference points (retina, association area, projection area) whereas Hastie et. al.‘s replaces them with the symbols that we have already seen in play in the expression for the linear model . What has happened between the two diagrams? As Vladimir Vapnik, a leading machine learning theorist, observes: ’the perceptron was constructed to solve pattern recognition problems; in the simplest case this is the problem of constructing a rule for separating data of two different categories using given examples’ <span class="citation">(Vapnik <a href="#ref-Vapnik_1999">1999</a>, 2)</span>.  While computer scientists in artificial intelligence of the time, such as Marvin Minsky and Seymour Papert, were sceptical about the capacity of the perceptron model to distinguish or ‘learn’ different patterns <span class="citation">(Minsky and Papert <a href="#ref-Minsky_1969">1969</a>)</span>, later work showed that perceptrons could ‘learn universally’<a href="#fn15" class="footnoteRef" id="fnref15"><sup>15</sup></a>. For present purposes, the key point is not that neural networks have turned out several decades later to be extremely powerful algorithms in learning to distinguish patterns, and that intense research in neural networks has led to their ongoing development and increasing sophistication in many ‘real world’ applications (see for instance, for their use in sciences <span class="citation">(Hinton and Salakhutdinov <a href="#ref-Hinton_2006">2006</a>)</span>, or in commercial applications such as drug prediction <span class="citation">(Dahl <a href="#ref-Dahl_2013">2013</a>)</span> and above all in the current surge of interest in ‘deep learning’ by social media platform and search engines such as Facebook and Google).  For our purposes, the important point is that the notion of the learning machine began to establish an ongoing diagonalization that transforms basic diagrammatic pattern through substitutions of increasingly convoluted or nested operations. The whole claim that machines ‘learn’ rests on this diagrammatization that recurrently and sometimes recursively transforms the icon of relations, sometimes in the graphic forms shown above and more often in the algebraic patterns.</p>
<p>The changes in graphics suggest transformations in the operational formation. I am suggesting, then, that we should follow the transformations associated with machine learning diagrammatically, provided we maintain a rich understanding of diagram, and remain open to multiple vectors of abstraction.  Following Peirce, we might begin to see machine learning as a diagrammatic practice in which different semiotic forms – lines, numbers, symbols, operators, patches of colour, words, images, marks such as dots, crosses, ticks and arrowheads – are constantly connected, substituted, embedded or created from existing diagrams. The diagrams we have already seen from <em>Elements of Statistical Learning</em> - algebraic formulae and network topology - don’t exhaust the variations at all. Just a brief glance through this book or almost any other in the field shows not only many formulae, but tables, matrices, arrays, line graphs, contour plots, scatter plots, dendrograms and trees, as well as algorithms expressed as pseudo-code. The connections between these diagrams are not always very tight or close. Learning to machine learning (whether you are a human learner or a learner in the sense of a machine) means dancing between diagrams. This dance is relatively silent and sometimes almost motionless as signs slide between different diagrammatic articulations. Diagrammatization offers then a way to track the ongoing project which tries treat data like farmers treat crops (see epigraph from Domingos in this chapter). To understand what machines can learn, we need to look at how they have been drawn, designed, or formalised. But what in this work of designing and formalising predictive models is like farming? Some very divergent trajectories open up here. On the one hand, the diagrams become machines when they are implemented. On the other hand, the machines generate new diagrams when they function. We need to countenance both forms of movement in order to understand any of the preceding diagrams – the algebraic expressions or the diagrams of models such as the perceptron or its descendants, the neural network. This means going downstream from the textbooks into actual implementations and places where people, algorithms, and machines mingle more than they do in the relatively neat formality of the textbooks. Rather than history or controversies in the field, I focus on the migratory patterns of methods, and the many configurational shifts associated with their implementations as the same things appears in different places. </p>
</div>
<div id="the-diagram-of-an-operational-formation" class="section level2">
<h2><span class="header-section-number">4.8</span> The diagram of an operational formation</h2>
<p>Does machine learning radically change programming practice? Programmability does change but only through gyrations between epistemic, infrastructural and discursive heterogeneities practised and practised in code. I have been suggesting that we can get a sense of the heterogeneities and regularities of machine learning by treating <em>Elements of Statistical Learning</em> as a diagram  that links and indexes the operations of machine learners in publication, in computation, in code.  Mapped through research publications, pedagogical materials or code libraries in <code>R</code>, these operations form a primary field of expressions issuing from many parts. These parts include the accumulation or positivity of scientific literature with all its referentiality.  They include mathematical derivation and formalization as an accelerated diagrammatic movement. The parts include code as materially weaving of infrastructures, conventions, standards, techniques, devices and collective relations.  Statements (especially linear algebra, calculus, statistics), problems and techniques from multiple scientific disciplines (especially computer science, but also biology, medicine and others), devices such as computing platforms, data formats, and code repositories populate the operational formation. The operational power of machine learning does not stem from a single layer of abstraction. The diagrammatic forms of movement we have begun to discern in the polymorphic <em>Elements of Statistical Learning</em> suggest key lines and paths worth following in opening up that engagement. Like the perceptron calculating weights that allow it to express the logic of the NAND function, we might first of all turn to the table, the ordering and aligning of numbers on which nearly all machine learning depends.</p>

</div>
</div>
<h3> References</h3>
<div id="refs" class="references">
<div id="ref-Domingos_2012">
<p>Domingos, Pedro. 2012. “A Few Useful Things to Know About Machine Learning.” <em>Communications of the ACM</em> 55 (10): 78–87. <a href="http://dl.acm.org/citation.cfm?id=2347755" class="uri">http://dl.acm.org/citation.cfm?id=2347755</a>.</p>
</div>
<div id="ref-Deleuze_1988">
<p>Deleuze, Gilles. 1988b. <em>Foucault</em>. Translated by Seân Hand. Minneapolis: University of Minnesota Press.</p>
</div>
<div id="ref-Whitehead_1956">
<p>Whitehead, Alfred North. 1956. <em>Modes of Thought; Six Lectures Delivered in Wellesley College, Massachusetts, and Two Lectures in the University of Chicago</em>. New York, Cambridge University Press.</p>
</div>
<div id="ref-Mol_2003">
<p>Mol, Annemarie. 2003. <em>The Body Multiple: Ontology in Medical Practice</em>. Durham, N.C: Duke University Press.</p>
</div>
<div id="ref-Domingos_2015a">
<p>Domingos, Pedro. 2015. <em>The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World</em>. New York: Basic Civitas Books.</p>
</div>
<div id="ref-Montfort_2009">
<p>Montfort, Nick, and Ian Bogost. 2009. <em>Racing the Beam: The Atari Video Computer System</em>. MIT Press.</p>
</div>
<div id="ref-Mnih_2013">
<p>Mnih, Volodymyr, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. 2013. “Playing Atari with Deep Reinforcement Learning,” December. <a href="http://arxiv.org/abs/1312.5602" class="uri">http://arxiv.org/abs/1312.5602</a>.</p>
</div>
<div id="ref-Mnih_2015">
<p>Mnih, Volodymyr, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, et al. 2015. “Human-Level Control Through Deep Reinforcement Learning.” <em>Nature</em> 518 (7540): 529–33. doi:<a href="https://doi.org/10.1038/nature14236">10.1038/nature14236</a>.</p>
</div>
<div id="ref-Muenchen_2014">
<p>Muenchen, Robert A. 2014. “The Popularity of Data Analysis Software.” <em>R4stats.com</em>. <a href="http://r4stats.com/articles/popularity/" class="uri">http://r4stats.com/articles/popularity/</a>.</p>
</div>
<div id="ref-Campbell-Kelly_2003">
<p>Campbell-Kelly, Martin. 2003. <em>From Airline Reservations to Sonic the Hedgehog: A History of the Software Industry</em>. Cambridge, MA: MIT Press.</p>
</div>
<div id="ref-Pearson_1901">
<p>Pearson, Karl. 1901. “LIII. On Lines and Planes of Closest Fit to Systems of Points in Space.” <em>Philosophical Magazine Series 6</em> 2 (11): 559–72. doi:<a href="https://doi.org/10.1080/14786440109462720">10.1080/14786440109462720</a>.</p>
</div>
<div id="ref-Dreyfus_1972">
<p>Dreyfus, Hubert L. 1972. <em>What Computers Can’t Do</em>. New York: Harper &amp; Row. <a href="http://philpapers.org/rec/DREWCC" class="uri">http://philpapers.org/rec/DREWCC</a>.</p>
</div>
<div id="ref-Edwards_1996">
<p>Edwards, Paul N. 1996. <em>The Closed World : Computers and the Politics of Discourse in Cold War</em>. Inside Technology. Cambridge, MA; London: MIT Press.</p>
</div>
<div id="ref-Wikipedia_2013">
<p>Wikipedia. 2013. “Perceptron.” <em>Wikipedia, the Free Encyclopedia</em>. <a href="http://en.wikipedia.org/w/index.php?title=Perceptron&amp;oldid=557301943" class="uri">http://en.wikipedia.org/w/index.php?title=Perceptron&amp;oldid=557301943</a>.</p>
</div>
<div id="ref-Ng_2008">
<p>Ng, Andrew, dir. 2008a. <em>Lecture 1 | Machine Learning (Stanford)</em>. <a href="http://www.youtube.com/watch?v=UzxYlbK2c7E&amp;feature=youtube_gdata_player" class="uri">http://www.youtube.com/watch?v=UzxYlbK2c7E&amp;feature=youtube_gdata_player</a>.</p>
</div>
<div id="ref-Segaran_2007">
<p>Segaran, Toby. 2007. <em>Programming Collective Intelligence: Building Smart Web 2.0 Applications</em>. Sebastapol CA.: O’Reilly.</p>
</div>
<div id="ref-Kirk_2014">
<p>Kirk, Matthew. 2014. <em>Thoughtful Machine Learning: A Test-Driven Approach</em>. 1 edition. Sebastopol, Calif.: O’Reilly Media.</p>
</div>
<div id="ref-Russell_2011">
<p>Russell, Matthew A. 2011. <em>Mining the Social Web</em>. Sebastopol, CA: O’Reilly.</p>
</div>
<div id="ref-Conway_2012">
<p>Conway, Drew, and John Myles White. 2012. <em>Machine Learning for Hackers</em>. Sebastopol, CA: O’Reilly. <a href="http://search.ebscohost.com/login.aspx?direct=true&amp;scope=site&amp;db=nlebk&amp;db=nlabk&amp;AN=436647" class="uri">http://search.ebscohost.com/login.aspx?direct=true&amp;scope=site&amp;db=nlebk&amp;db=nlabk&amp;AN=436647</a>.</p>
</div>
<div id="ref-Hastie_2009">
<p>Hastie, Trevor, Robert Tibshirani, and Jerome H. 2009. <em>The Elements of Statistical Learning: Data Mining, Inference, and Prediction</em>. 2nd edition. New York: Springer.</p>
</div>
<div id="ref-Morton_2013">
<p>Morton, Timothy. 2013. <em>Hyperobjects: Philosophy and Ecology After the End of the World</em>. Univ Of Minnesota Press.</p>
</div>
<div id="ref-Alpaydin_2010">
<p>Alpaydin, Ethem. 2010. <em>Introduction to Machine Learning</em>. Cambridge, Massachusetts; London: MIT Press.</p>
</div>
<div id="ref-Bishop_2006">
<p>Bishop, Christopher M. 2006. <em>Pattern Recognition and Machine Learning</em>. Vol. 1. New York: Springer.</p>
</div>
<div id="ref-Ripley_1996">
<p>Ripley, Brian. 1996. <em>Pattern Recognition and Neural Networks. 1996</em>. Cambridge ; New York: Cambridge University Press.</p>
</div>
<div id="ref-Mitchell_1997">
<p>Mitchell, Tom M. 1997. <em>Machine Learning</em>. New York, NY [u.a.: McGraw-Hill.</p>
</div>
<div id="ref-Flach_2012">
<p>Flach, Peter. 2012. <em>Machine Learning: The Art and Science of Algorithms That Make Sense of Data</em>. Cambridge University Press.</p>
</div>
<div id="ref-Malley_2011">
<p>Malley, James D., Karen G. Malley, and Sinisa Pajevic. 2011. <em>Statistical Learning for Biomedical Data</em>. 1st ed. Cambridge University Press.</p>
</div>
<div id="ref-Schutt_2013">
<p>Schutt, Rachel, and Cathy O’Neil. 2013. <em>Doing Data Science</em>. Sebastopol, Calif.: O’Reilly &amp; Associates Inc.</p>
</div>
<div id="ref-Hastie_2001">
<p>Hastie, Trevor, Robert Tibshirani, and Jerome H. Friedman. 2001. <em>The Elements of Statistical Learning: Data Mining, Inference, and Prediction</em>. 1st edition. New York: Springer.</p>
</div>
<div id="ref-Lee_1999">
<p>Lee, D. D., and H. S. Seung. 1999. “Learning the Parts of Objects by Non-Negative Matrix Factorization.” <em>Nature</em> 401 (6755): 788–91. doi:<a href="https://doi.org/10.1038/44565">10.1038/44565</a>.</p>
</div>
<div id="ref-Kuhn_1996">
<p>Kuhn, Thomas S. 1996. <em>The Structure of Scientific Revolutions</em>. Chicago, IL: University of Chicago Press.</p>
</div>
<div id="ref-Mackenzie_2014">
<p>Mackenzie, Adrian. 2014b. “UseR! Aggression, Alterity and Unbound Affects in Statistical Programming.” In <em>Fun and Software: Exploring Pleasure, Paradox and Pain in Computing</em>, edited by Olga Goriunova. New York: Bloomsbury Academic.</p>
</div>
<div id="ref-RexerAnalytics_2015">
<p>RexerAnalytics. 2015. “Rexer Analytics 7th Annual Data Miner Survey - 2015.” <a href="http://www.rexeranalytics.com/Data-Miner-Survey-2015-Intro.html" class="uri">http://www.rexeranalytics.com/Data-Miner-Survey-2015-Intro.html</a>.</p>
</div>
<div id="ref-ACM_2013">
<p>ACM. 2013. “John M. Chambers - Award Winner.” <em>Awards: Software System</em>. <a href="http://awards.acm.org/award_winners/chambers_6640862.cfm" class="uri">http://awards.acm.org/award_winners/chambers_6640862.cfm</a>.</p>
</div>
<div id="ref-CRAN_2010">
<p>CRAN. 2010. “The Comprehensive R Archive Network.” <a href="http://www.stats.bris.ac.uk/R/" class="uri">http://www.stats.bris.ac.uk/R/</a>.</p>
</div>
<div id="ref-Venables_2002">
<p>Venables, William N., and Brian D. Ripley. 2002. <em>Modern Applied Statistics with S</em>. Springer.</p>
</div>
<div id="ref-Foucault_1972">
<p>Foucault, Michel. 1972. <em>The Archaeology of Knowledge and the Discourse on Language</em>. Translated by Allan Sheridan-Smith. New York: Pantheon Books.</p>
</div>
<div id="ref-Marx_1986">
<p>Marx, Karl. 1986. <em>Capital A Critique of Political Economy. The Process of Production of Capital</em>. Moscow: Progress.</p>
</div>
<div id="ref-Bollas_2008">
<p>Bollas, Christopher. 2008. <em>The Evocative Object World</em>. London &amp; New York: Routledge.</p>
</div>
<div id="ref-Stigler_1986">
<p>Stigler, Stephen M. 1986. <em>The History of Statistics: The Measurement of Uncertainty Before 1900</em>. Cambridge, Mass.: Harvard University Press.</p>
</div>
<div id="ref-Pasquinelli_2015">
<p>Pasquinelli, Matteo. 2015. “Anomaly Detection: The Mathematization of the Abnormal in the Metadata Society.” In. Berlin. <a href="https://www.academia.edu/10369819/Anomaly_Detection_The_Mathematization_of_the_Abnormal_in_the_Metadata_Society" class="uri">https://www.academia.edu/10369819/Anomaly_Detection_The_Mathematization_of_the_Abnormal_in_the_Metadata_Society</a>.</p>
</div>
<div id="ref-Peirce_1998">
<p>Peirce, Charles Sanders. 1998. <em>The Essential Peirce - Volume 2: Selected Philosophical Writings: (1893-1913) V. 2</em>. Indiana University Press.</p>
</div>
<div id="ref-Peirce_1992">
<p>Peirce, Charles Sanders. 1992. <em>The Essential Peirce: 1867-1893 V. 1: Selected Philosophical Writings</em>. John Wiley &amp; Sons.</p>
</div>
<div id="ref-Ng_2008d">
<p><em>Lecture 10 | Machine Learning (Stanford)</em>. 2008. <a href="https://www.youtube.com/watch?v=0kWZoyNRxTY&amp;feature=youtube_gdata_player" class="uri">https://www.youtube.com/watch?v=0kWZoyNRxTY&amp;feature=youtube_gdata_player</a>.</p>
</div>
<div id="ref-Rosenblatt_1958">
<p>Rosenblatt, F. 1958. “The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain.” <em>Psychological Review</em> 65 (6): 386–408. doi:<a href="https://doi.org/10.1037/h0042519">10.1037/h0042519</a>.</p>
</div>
<div id="ref-Bellman_1961">
<p>Bellman, Richard. 1961. <em>Adaptive Control Processes: A Guided Tour</em>. Vol. 4. Princeton N.J.: Princeton University Press. <a href="http://www.getcited.org/pub/101191710" class="uri">http://www.getcited.org/pub/101191710</a>.</p>
</div>
<div id="ref-Vapnik_1999">
<p>Vapnik, Vladimir. 1999. <em>The Nature of Statistical Learning Theory</em>. 2nd ed. 2000. Springer.</p>
</div>
<div id="ref-Minsky_1969">
<p>Minsky, Marvin, and Seymour Papert. 1969. “Perceptron: An Introduction to Computational Geometry.” <em>The MIT Press, Cambridge, Expanded Edition</em> 19: 88.</p>
</div>
<div id="ref-Hinton_2006">
<p>Hinton, Geoffrey E., and Ruslan R. Salakhutdinov. 2006. “Reducing the Dimensionality of Data with Neural Networks.” <em>Science</em> 313 (5786): 504–7. <a href="http://www.sciencemag.org/content/313/5786/504.short" class="uri">http://www.sciencemag.org/content/313/5786/504.short</a>.</p>
</div>
<div id="ref-Dahl_2013">
<p>Dahl, George. 2013. “Deep Learning How I Did It: Merck 1st Place Interview.” <em>No Free Hunch</em>. <a href="http://blog.kaggle.com/2012/11/01/deep-learning-how-i-did-it-merck-1st-place-interview/" class="uri">http://blog.kaggle.com/2012/11/01/deep-learning-how-i-did-it-merck-1st-place-interview/</a>.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="7">
<li id="fn7"><p>The philosophy Charles Sanders Peirce himself  had first shown that combinations of NAND operations could stand in for any logical expression whatsoever, thus paving the way for the diagrammatic weave of contemporary digital memory and computation in all their permutations. Today, NAND-based logic is norm in digital electronics. NOR – NOT OR – logic is also used in certain applications.<a href="4-diagramming-machines.html#fnref7">↩</a></p></li>
<li id="fn8"><p>They differ, however, in several important respects. Reading the <em>Elements of Statistical Learning</em> textbook or one of machine learning books written for programmers (for example, <em>Programming Collective Intelligence</em> or <em>Machine Learning for Hackers</em> <span class="citation">(Segaran <a href="#ref-Segaran_2007">2007</a>; Conway and White <a href="#ref-Conway_2012">2012</a>)</span>) does not directly subject the reader to machine learning By contrast, doing a Coursera course on machine learning brings with it an ineluctable sense of being machine-learned, of oneself becoming an object of machine learning. The students on Coursera are the target of machine learning. Daphne Koller and Andrew Ng are leading researchers in the field of machine learning, but they also co-founded the online learning site <a href="http://coursera.org">Coursera</a>.  As experts in machine learning, it is hard to imagine how they would not treat teaching as a learning problem. And indeed, Daphne Koller sees things this way:</p><blockquote><p>There are some tremendous opportunities to be had from this kind of framework. The first is that it has the potential of giving us a completely unprecedented look into understanding human learning. Because the data that we can collect here is unique. You can collect every click, every homework submission, every forum post from tens of thousands of students. So you can turn the study of human learning from the hypothesis-driven mode to the data-driven mode, a transformation that, for example, has revolutionized biology. You can use these data to understand fundamental questions like, what are good learning strategies that are effective versus ones that are not? And in the context of particular courses, you can ask questions like, what are some of the misconceptions that are more common and how do we help students fix them? <span class="citation">(<em>What We’re Learning from Online Education | Video on TED.com</em> <a href="#ref-Koller_2012">2012</a>)</span> </p></blockquote><p>Whether the turn from ‘hypothesis-driven mode to the data-driven mode’ has ‘revolutionized biology’ is debatable (I return to this in a later chapter). And whether or not the data generated by my participation in Coursera’s courses on machine learning generates data supports understanding of fundamental questions about learning also seems an open question. Nevertheless, the loopiness of this description interests and appeals to me. I learn about machine learning, a way for computer models to optimise their predictions on the basis of ‘experience’/data, but at the same time, my learning is learned by machine learners. This is not something that could happen very easily with a printed text, although versions of it happen all the time as teachers work with students on reading texts. While Coursera and other MOOCs promise something that mass education struggles to offer (individually profiled educational services), it also negatively highlights the possibility that machine learning in practice can, somewhat recursively, help us make sense of machine learning as it develops.<a href="4-diagramming-machines.html#fnref8">↩</a></p></li>
<li id="fn9"><p>The complete text of the book can be downloaded from the website <a href="http://statweb.stanford.edu/~tibs/ElemStatLearn/" class="uri">http://statweb.stanford.edu/~tibs/ElemStatLearn/</a>. At the end of short intensive course on data mining at the Centre of Postgraduate Statistics, Lancaster University, the course convenor, Brian Francis, recommended this book as the authoritative text. Some part of me rues that day. That book is a poisoned chalice; that is, something shiny, valuable but also somewhat toxic.<a href="4-diagramming-machines.html#fnref9">↩</a></p></li>
<li id="fn10"><p>In a later book by some of the same authors with the very similar sounding title <em>An Introduction to Statistical Machine Learning with Applications in <code>R</code></em> <span class="citation">(James et al. <a href="#ref-James_2013">2013</a>)</span>, <code>R</code> does appear in abundance. This book, however, is much shorter and less inclusive in various ways. There are in any case many online manuals, guides and tutorials relating to <code>R</code> <span class="citation">(Wikibooks <a href="#ref-Wikibooks_2013">2013</a>)</span>.  For present purposes, I draw mainly on semi-popular books such as <em>R in a Nutshell</em> <span class="citation">(Adler and Beyer <a href="#ref-Adler_2010">2010</a>)</span>, <em>The Art of <code>R</code> Programming</em> <span class="citation">(Matloff <a href="#ref-Matloff_2011">2011</a>)</span>, <em>R Cookbook</em> <span class="citation">(Teetor <a href="#ref-Teetor_2011">2011</a>)</span>, <em>Machine Learning with R</em> <span class="citation">(Lantz <a href="#ref-Lantz_2013">2013</a>)</span> or <em>An Introduction to Statistical Learning with Applications in R</em> <span class="citation">(James et al. <a href="#ref-James_2013">2013</a>)</span>. These books are not written for academic audiences, although academics often write them and use them in their work.  They are largely made up of illustrations and examples of how to do different things with different types of data, and their examples are typically oriented towards business or commercial settings, where, presumably, the bulk of the readers work, or aspire to work. Given a certain predisposition (that is, geekiness), these books and other learning materials can make for enjoyable reading. While they vary highly in quality, it is sometimes pleasing to see the economical way in which they solve common data problems. This genre of writing about programming specialises in posing a problem and solving it directly. In these settings, learning takes place largely through following or emulating what someone else has done with either a well known dataset (Fisher’s <code>iris</code>) or a toy dataset generated for demonstration purposes. The many frictions, blockages and compromises that often affect data practice are largely occluded here in the interests of demonstrating the neat application of specific techniques. Yet are not those frictions, neat compromises and strains around data and machine learning precisely what we need to track diagrammatically? In order to demonstrate both the costs and benefits of approaching <code>R</code> through such materials, rather than through ethnographic observation of people using <code>R</code>, it will be necessary to stage encounters here with data that has not been completely transformed or cleaned in the interests of neatly demonstrating the power of a technique. One way to do this is by writing about code while coding.<a href="4-diagramming-machines.html#fnref10">↩</a></p></li>
<li id="fn11"><p>Most of the packages associated with the <code>ElemStatLearn</code>  implement methods or techniques developed by Hastie, Tibshirani or Friedman, but some are much more generic. <code>MASS</code> for instance is highly cited <code>R</code> package. (Of the 9232 packages in the R CRAN system, 0 depend on the library <code>MASS</code>,  itself an adjunct to the influential and highly cited <em>Modern Applied Statistics with S</em> <span class="citation">(Venables and Ripley <a href="#ref-Venables_2002">2002</a>)</span>, a textbook that presents many machine learning techniques using <code>S</code> , AT &amp; T Bell Labs commercial precursor to the open sourced <code>R</code>. ) For our purposes, this hardly accidental mixing of academic or research work with a programming languages and its associated infrastructures is fortuitous. It allows us to transit between different strata of the social fields of science, engineering, health, medicine, business media and government more easily.<a href="4-diagramming-machines.html#fnref11">↩</a></p></li>
<li id="fn12"><p>Félix Guattari makes direct use of Peirce’s account of diagrams as icons of relation in his account of ‘abstract machines’ <span class="citation">(Guattari <a href="#ref-Guattari_1984">1984</a>)</span>. He writes that ‘diagrammaticism brings into play more or less de-territorialized trans-semiotic forces, systems of signs, of code, of catalysts and so on, that make it possible in various specific ways to cut across stratifications of every kind’ <span class="citation">(Guattari <a href="#ref-Guattari_1984">1984</a>, 145)</span>.   Here the ‘trans-semiotic forces’ include mathematical formulae and operations (such as the banking system of Renaissance Venice, Pisa and Genoa). They are trans-semiotic because they are not tethered by the signifying processes that code experience or speaking positions according to given stratifications such as class, gender, nation and so forth. While Guattari (and Deleuze in turn in their co-written works <span class="citation">(Guattari and Deleuze <a href="#ref-Guattari_1988">1988</a>)</span>) is strongly critical of the way which signification territorializes (we might think of cats patrolling, marking and displaying in order to maintain their territories), he is much more affirmative of diagrammatic processes. He calls them ‘a-signifying’ to highlight their difference from the signifying processes that order social strata. He suggests that diagrams become the foundation for ‘abstract machines’ and the ‘simulation of physical machinic processes.’ Writing in the 1960s, Guattari powerfully anticipates the abstract machines and their associated diagrams that have taken shape and physical form in the succeeding decades. <a href="4-diagramming-machines.html#fnref12">↩</a></p></li>
<li id="fn13"><p>A heavily shortened version of this course has been delivered under the title ‘Machine Learning’ on <a href="https://class.coursera.org/ml-003/class/index">Coursera.org</a>, a MOOC (Massive Open Online Course) platform.<a href="4-diagramming-machines.html#fnref13">↩</a></p></li>
<li id="fn14"><p>After sitting through 20 hours of Ng’s online lectures, and attempting some of the review questions and programming exercises, including implementing well-known algorithms using <code>R</code>, one comes to know datasets such as the San Francisco house price dataset and Fisher’s <code>iris</code> <span class="citation">(Fisher <a href="#ref-Fisher_1936">1936</a>)</span> quite well. Like the textbook problems that the historian of science Thomas Kuhn long ago described as one of the anchor points in scientific cultures <span class="citation">(Kuhn <a href="#ref-Kuhn_1996">1996</a>)</span>, these iconic datasets provide an entry point to the ‘disciplinary matrix’ of machine learning. Through them, one gains some sense of how predictive models are constructed, and what kinds of algorithmic architectures and forms of data are preferred in machine learning.<a href="4-diagramming-machines.html#fnref14">↩</a></p></li>
<li id="fn15"><p>In describing the entwined elements of machine learning techniques, and citing various machine learning textbooks, I’m not attempting to provide any detailed history of their development. My   of these developments does not explore the archives of institutions, laboratories or companies where these techniques took shape. It derives much more either from following citations back out of the highly distilled textbooks into the teeming collective labour of research on machine learning as published in hundreds of thousands of articles in science and engineering journals, or from looking at, experimenting with and implementing techniques in code. For instance, <span class="citation">(Olazaran <a href="#ref-Olazaran_1996">1996</a>)</span> offers a history of the perceptron controversy from a science studies perspective.  During the 1980s, artificial intelligence and associated approaches (expert systems, automated decision support, neural networks, etc.) were a matter of some debate in the sociology and anthropology of science. The work of Harry Collins would be one example of this <span class="citation">(Collins <a href="#ref-Collins_1990">1990</a>)</span>, Paul Edwards on artificial intelligence and Cold War <span class="citation">(Edwards <a href="#ref-Edwards_1996">1996</a>)</span>, Nathan Ensmenger on chess <span class="citation">(Ensmenger <a href="#ref-Ensmenger_2012">2012</a>)</span>, and Lucy Suchman on plans and expert systems <span class="citation">(Suchman <a href="#ref-Suchman_1987">1987</a>)</span> would be others. Philosophers such as Hubert Dreyfus (<em>What Computers Can’t Do</em> <span class="citation">(Dreyfus <a href="#ref-Dreyfus_1972">1972</a>; Dreyfus <a href="#ref-Dreyfus_1992">1992</a>)</span> had already extensively criticised AI.  In the 1990s, the appearance of new forms of simulation, computational fields such as a-life and new forms of robotics such as Rodney Brook’s much more insect-like robots at MIT attracted the interest of social scientists <span class="citation">(Helmreich <a href="#ref-Helmreich_2000">2000</a>)</span> amongst many others. Sometimes this interest was critical of claims to expertise (Collins), and at other times, interested in how to make sense of the claims without foreclosing their potential novelty (Helmreich). By and large, I don’t attend to controversies in machine learning as a scientific field, and I don’t directly contest the epistemic authority of the proponents of the techniques. I share with Lucy Suchman an interest in how the ‘effect of machine-as-agent is generated’ and in how ‘translations … render former objects as emergent subjects’<span class="citation">(Suchman <a href="#ref-Suchman_2006">2006</a>, 2)</span>.  I diverge around the site of empirical attention. I’m persevering with the diagrams in each of the following chapters in order to track the movement of tendencies that are not so visible in terms of either the controversies or the assumptions of agency embodied in many AI systems of the 1980s. The agency of machine learning, in short, might not reside so much in any putative predictive or classificatory power if manifests, but rather its capacity to mutate and migrate across contexts. <a href="4-diagramming-machines.html#fnref15">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="3-introduction-into-the-data.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="5-vectorisation-and-its-consequences.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
