<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Machine Learning: Archaeology of a Data Practice</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="Machine Learning: Archaeology of a Data Practice">
  <meta name="generator" content="bookdown 0.3 and GitBook 2.6.7">

  <meta property="og:title" content="Machine Learning: Archaeology of a Data Practice" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Machine Learning: Archaeology of a Data Practice" />
  
  
  

<meta name="author" content="Adrian Mackenzie">


<meta name="date" content="2016-12-10">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="12-curves-and-the-variation-in-models.html">
<link rel="next" href="14-gradients-as-partial-observers.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="1-acknowledgments.html"><a href="1-acknowledgments.html"><i class="fa fa-check"></i><b>1</b> Acknowledgments</a></li>
<li class="chapter" data-level="2" data-path="2-preface.html"><a href="2-preface.html"><i class="fa fa-check"></i><b>2</b> Preface</a></li>
<li class="chapter" data-level="3" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html"><i class="fa fa-check"></i><b>3</b> Introduction: Into the Data</a><ul>
<li class="chapter" data-level="3.1" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#three-accumulations-settings-data-and-devices"><i class="fa fa-check"></i><b>3.1</b> Three accumulations: settings, data and devices</a></li>
<li class="chapter" data-level="3.2" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#who-or-what-is-a-machine-learner"><i class="fa fa-check"></i><b>3.2</b> Who or what is a machine learner?</a></li>
<li class="chapter" data-level="3.3" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#algorithmic-control-to-the-machine-learners"><i class="fa fa-check"></i><b>3.3</b> Algorithmic control to the machine learners?</a></li>
<li class="chapter" data-level="3.4" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#the-archaeology-of-operations"><i class="fa fa-check"></i><b>3.4</b> The archaeology of operations</a></li>
<li class="chapter" data-level="3.5" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#asymmetries-in-common-knowledge"><i class="fa fa-check"></i><b>3.5</b> Asymmetries in common knowledge</a></li>
<li class="chapter" data-level="3.6" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#what-cannot-be-automated"><i class="fa fa-check"></i><b>3.6</b> What cannot be automated?</a></li>
<li class="chapter" data-level="3.7" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#different-fields-in-machine-learning"><i class="fa fa-check"></i><b>3.7</b> Different fields in machine learning?</a></li>
<li class="chapter" data-level="3.8" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#the-diagram-in-critical-thought"><i class="fa fa-check"></i><b>3.8</b> The diagram in critical thought</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html"><i class="fa fa-check"></i><b>4</b> Diagramming machines}</a><ul>
<li class="chapter" data-level="4.1" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#we-dont-have-to-write-programs"><i class="fa fa-check"></i><b>4.1</b> ‘We don’t have to write programs’?</a></li>
<li class="chapter" data-level="4.2" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#the-elements-of-machine-learning"><i class="fa fa-check"></i><b>4.2</b> The elements of machine learning</a></li>
<li class="chapter" data-level="4.3" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#who-reads-machine-learning-textbooks"><i class="fa fa-check"></i><b>4.3</b> Who reads machine learning textbooks?</a></li>
<li class="chapter" data-level="4.4" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#r-a-matrix-of-transformations"><i class="fa fa-check"></i><b>4.4</b> <code>R</code>: a matrix of transformations</a></li>
<li class="chapter" data-level="4.5" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#the-obdurate-mathematical-glint-of-machine-learning"><i class="fa fa-check"></i><b>4.5</b> The obdurate mathematical glint of machine learning</a></li>
<li class="chapter" data-level="4.6" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#cs229-2007-returning-again-and-again-to-certain-features"><i class="fa fa-check"></i><b>4.6</b> CS229, 2007: returning again and again to certain features</a></li>
<li class="chapter" data-level="4.7" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#the-visible-learning-of-machine-learning"><i class="fa fa-check"></i><b>4.7</b> The visible learning of machine learning</a></li>
<li class="chapter" data-level="4.8" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#the-diagram-of-an-operational-formation"><i class="fa fa-check"></i><b>4.8</b> The diagram of an operational formation</a></li>
<li class="chapter" data-level="4.9" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#vectorisation-and-its-consequences"><i class="fa fa-check"></i><b>4.9</b> Vectorisation and its consequences}</a></li>
<li class="chapter" data-level="4.10" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#vector-space-and-geometry"><i class="fa fa-check"></i><b>4.10</b> Vector space and geometry</a></li>
<li class="chapter" data-level="4.11" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#mixing-places"><i class="fa fa-check"></i><b>4.11</b> Mixing places</a></li>
<li class="chapter" data-level="4.12" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#truth-is-no-longer-in-the-table"><i class="fa fa-check"></i><b>4.12</b> Truth is no longer in the table?</a></li>
<li class="chapter" data-level="4.13" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#the-epistopic-fault-line-in-tables"><i class="fa fa-check"></i><b>4.13</b> The epistopic fault line in tables</a></li>
<li class="chapter" data-level="4.14" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#surface-and-depths-the-problem-of-volume-in-data"><i class="fa fa-check"></i><b>4.14</b> Surface and depths: the problem of volume in data</a></li>
<li class="chapter" data-level="4.15" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#vector-space-expansion"><i class="fa fa-check"></i><b>4.15</b> Vector space expansion</a></li>
<li class="chapter" data-level="4.16" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#drawing-lines-in-a-common-space-of-transformation"><i class="fa fa-check"></i><b>4.16</b> Drawing lines in a common space of transformation</a></li>
<li class="chapter" data-level="4.17" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#implicit-vectorization-in-code-and-infrastructures"><i class="fa fa-check"></i><b>4.17</b> Implicit vectorization in code and infrastructures</a></li>
<li class="chapter" data-level="4.18" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#lines-traversing-behind-the-light"><i class="fa fa-check"></i><b>4.18</b> Lines traversing behind the light</a></li>
<li class="chapter" data-level="4.19" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#the-vectorised-table"><i class="fa fa-check"></i><b>4.19</b> The vectorised table?</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-machines-finding-functions.html"><a href="5-machines-finding-functions.html"><i class="fa fa-check"></i><b>5</b> Machines finding functions}</a></li>
<li class="chapter" data-level="6" data-path="6-learning-functions.html"><a href="6-learning-functions.html"><i class="fa fa-check"></i><b>6</b> Learning functions</a></li>
<li class="chapter" data-level="7" data-path="7-supervised-unsupervised-reinforcement-learning-and-functions.html"><a href="7-supervised-unsupervised-reinforcement-learning-and-functions.html"><i class="fa fa-check"></i><b>7</b> Supervised, unsupervised, reinforcement learning and functions</a></li>
<li class="chapter" data-level="8" data-path="8-which-function-operates.html"><a href="8-which-function-operates.html"><i class="fa fa-check"></i><b>8</b> Which function operates?</a></li>
<li class="chapter" data-level="9" data-path="9-what-does-a-function-learn.html"><a href="9-what-does-a-function-learn.html"><i class="fa fa-check"></i><b>9</b> What does a function learn?</a></li>
<li class="chapter" data-level="10" data-path="10-observing-with-curves-the-logistic-function.html"><a href="10-observing-with-curves-the-logistic-function.html"><i class="fa fa-check"></i><b>10</b> Observing with curves: the logistic function</a></li>
<li class="chapter" data-level="11" data-path="11-the-cost-of-curves-in-machine-learning.html"><a href="11-the-cost-of-curves-in-machine-learning.html"><i class="fa fa-check"></i><b>11</b> The cost of curves in machine learning</a></li>
<li class="chapter" data-level="12" data-path="12-curves-and-the-variation-in-models.html"><a href="12-curves-and-the-variation-in-models.html"><i class="fa fa-check"></i><b>12</b> Curves and the variation in models</a></li>
<li class="chapter" data-level="13" data-path="13-observing-costs-losses-and-objectives-through-optimisation.html"><a href="13-observing-costs-losses-and-objectives-through-optimisation.html"><i class="fa fa-check"></i><b>13</b> Observing costs, losses and objectives through optimisation</a></li>
<li class="chapter" data-level="14" data-path="14-gradients-as-partial-observers.html"><a href="14-gradients-as-partial-observers.html"><i class="fa fa-check"></i><b>14</b> Gradients as partial observers</a></li>
<li class="chapter" data-level="15" data-path="15-the-power-to-learn.html"><a href="15-the-power-to-learn.html"><i class="fa fa-check"></i><b>15</b> The power to learn</a></li>
<li class="chapter" data-level="16" data-path="16-probabilisation-and-the-taming-of-machines.html"><a href="16-probabilisation-and-the-taming-of-machines.html"><i class="fa fa-check"></i><b>16</b> Probabilisation and the Taming of Machines}</a></li>
<li class="chapter" data-level="17" data-path="17-data-reduces-uncertainty.html"><a href="17-data-reduces-uncertainty.html"><i class="fa fa-check"></i><b>17</b> Data reduces uncertainty?</a></li>
<li class="chapter" data-level="18" data-path="18-machine-learning-as-statistics-inside-out.html"><a href="18-machine-learning-as-statistics-inside-out.html"><i class="fa fa-check"></i><b>18</b> Machine learning as statistics inside out</a></li>
<li class="chapter" data-level="19" data-path="19-distributed-probabilities.html"><a href="19-distributed-probabilities.html"><i class="fa fa-check"></i><b>19</b> Distributed probabilities</a></li>
<li class="chapter" data-level="20" data-path="20-naive-bayes-and-the-distribution-of-probabilities.html"><a href="20-naive-bayes-and-the-distribution-of-probabilities.html"><i class="fa fa-check"></i><b>20</b> Naive Bayes and the distribution of probabilities</a></li>
<li class="chapter" data-level="21" data-path="21-spam-when-foralln-is-too-much.html"><a href="21-spam-when-foralln-is-too-much.html"><i class="fa fa-check"></i><b>21</b> Spam: when <span class="math inline">\(\forall{N}\)</span> is too much?</a></li>
<li class="chapter" data-level="22" data-path="22-the-improbable-success-of-the-naive-bayes-classifier.html"><a href="22-the-improbable-success-of-the-naive-bayes-classifier.html"><i class="fa fa-check"></i><b>22</b> The improbable success of the Naive Bayes classifier</a></li>
<li class="chapter" data-level="23" data-path="23-ancestral-probabilities-in-documents-inference-and-prediction.html"><a href="23-ancestral-probabilities-in-documents-inference-and-prediction.html"><i class="fa fa-check"></i><b>23</b> Ancestral probabilities in documents: inference and prediction</a></li>
<li class="chapter" data-level="24" data-path="24-statistical-decompositions-bias-variance-and-observed-errors.html"><a href="24-statistical-decompositions-bias-variance-and-observed-errors.html"><i class="fa fa-check"></i><b>24</b> Statistical decompositions: bias, variance and observed errors</a></li>
<li class="chapter" data-level="25" data-path="25-does-machine-learning-construct-a-new-statistical-reality.html"><a href="25-does-machine-learning-construct-a-new-statistical-reality.html"><i class="fa fa-check"></i><b>25</b> Does machine learning construct a new statistical reality?</a></li>
<li class="chapter" data-level="26" data-path="26-patterns-and-differences.html"><a href="26-patterns-and-differences.html"><i class="fa fa-check"></i><b>26</b> Patterns and differences</a></li>
<li class="chapter" data-level="27" data-path="27-splitting-and-the-growth-of-trees.html"><a href="27-splitting-and-the-growth-of-trees.html"><i class="fa fa-check"></i><b>27</b> Splitting and the growth of trees</a></li>
<li class="chapter" data-level="28" data-path="28-differences-in-recursive-partitioning.html"><a href="28-differences-in-recursive-partitioning.html"><i class="fa fa-check"></i><b>28</b> 1984: Differences in recursive partitioning</a></li>
<li class="chapter" data-level="29" data-path="29-limiting-differences.html"><a href="29-limiting-differences.html"><i class="fa fa-check"></i><b>29</b> Limiting differences</a></li>
<li class="chapter" data-level="30" data-path="30-the-successful-dispersion-of-the-support-vector-machine.html"><a href="30-the-successful-dispersion-of-the-support-vector-machine.html"><i class="fa fa-check"></i><b>30</b> The successful dispersion of the support vector machine</a></li>
<li class="chapter" data-level="31" data-path="31-differences-blur.html"><a href="31-differences-blur.html"><i class="fa fa-check"></i><b>31</b> Differences blur?</a></li>
<li class="chapter" data-level="32" data-path="32-bending-the-decision-boundary.html"><a href="32-bending-the-decision-boundary.html"><i class="fa fa-check"></i><b>32</b> Bending the decision boundary</a></li>
<li class="chapter" data-level="33" data-path="33-instituting-patterns.html"><a href="33-instituting-patterns.html"><i class="fa fa-check"></i><b>33</b> Instituting patterns</a></li>
<li class="chapter" data-level="34" data-path="34-regularizing-and-materializing-objects.html"><a href="34-regularizing-and-materializing-objects.html"><i class="fa fa-check"></i><b>34</b> Regularizing and materializing objects}</a></li>
<li class="chapter" data-level="35" data-path="35-genomic-referentiality-and-materiality.html"><a href="35-genomic-referentiality-and-materiality.html"><i class="fa fa-check"></i><b>35</b> Genomic referentiality and materiality</a></li>
<li class="chapter" data-level="36" data-path="36-the-genome-as-threshold-object.html"><a href="36-the-genome-as-threshold-object.html"><i class="fa fa-check"></i><b>36</b> The genome as threshold object</a></li>
<li class="chapter" data-level="37" data-path="37-genomic-knowledges-and-their-datasets.html"><a href="37-genomic-knowledges-and-their-datasets.html"><i class="fa fa-check"></i><b>37</b> Genomic knowledges and their datasets</a></li>
<li class="chapter" data-level="38" data-path="38-the-advent-of-wide-dirty-and-mixed-data.html"><a href="38-the-advent-of-wide-dirty-and-mixed-data.html"><i class="fa fa-check"></i><b>38</b> The advent of ‘wide, dirty and mixed’ data</a></li>
<li class="chapter" data-level="39" data-path="39-cross-validating-machine-learning-in-genomics.html"><a href="39-cross-validating-machine-learning-in-genomics.html"><i class="fa fa-check"></i><b>39</b> Cross-validating machine learning in genomics</a></li>
<li class="chapter" data-level="40" data-path="40-proliferation-of-discoveries.html"><a href="40-proliferation-of-discoveries.html"><i class="fa fa-check"></i><b>40</b> Proliferation of discoveries</a></li>
<li class="chapter" data-level="41" data-path="41-variations-in-the-object-or-in-the-machine-learner.html"><a href="41-variations-in-the-object-or-in-the-machine-learner.html"><i class="fa fa-check"></i><b>41</b> Variations in the object or in the machine learner?</a></li>
<li class="chapter" data-level="42" data-path="42-whole-genome-functions.html"><a href="42-whole-genome-functions.html"><i class="fa fa-check"></i><b>42</b> Whole genome functions</a></li>
<li class="chapter" data-level="43" data-path="43-propagating-subject-positions.html"><a href="43-propagating-subject-positions.html"><i class="fa fa-check"></i><b>43</b> Propagating subject positions}</a></li>
<li class="chapter" data-level="44" data-path="44-propagation-across-human-machine-boundaries.html"><a href="44-propagation-across-human-machine-boundaries.html"><i class="fa fa-check"></i><b>44</b> Propagation across human-machine boundaries</a></li>
<li class="chapter" data-level="45" data-path="45-competitive-positioning.html"><a href="45-competitive-positioning.html"><i class="fa fa-check"></i><b>45</b> Competitive positioning</a></li>
<li class="chapter" data-level="46" data-path="46-a-privileged-machine-and-its-diagrammatic-forms.html"><a href="46-a-privileged-machine-and-its-diagrammatic-forms.html"><i class="fa fa-check"></i><b>46</b> A privileged machine and its diagrammatic forms</a></li>
<li class="chapter" data-level="47" data-path="47-varying-subject-positions-in-code.html"><a href="47-varying-subject-positions-in-code.html"><i class="fa fa-check"></i><b>47</b> Varying subject positions in code</a></li>
<li class="chapter" data-level="48" data-path="48-the-subjects-of-a-hidden-operation.html"><a href="48-the-subjects-of-a-hidden-operation.html"><i class="fa fa-check"></i><b>48</b> The subjects of a hidden operation</a></li>
<li class="chapter" data-level="49" data-path="49-algorithms-that-propagate-errors.html"><a href="49-algorithms-that-propagate-errors.html"><i class="fa fa-check"></i><b>49</b> Algorithms that propagate errors</a></li>
<li class="chapter" data-level="50" data-path="50-competitions-as-examination.html"><a href="50-competitions-as-examination.html"><i class="fa fa-check"></i><b>50</b> Competitions as examination</a></li>
<li class="chapter" data-level="51" data-path="51-superimposing-power-and-knowledge.html"><a href="51-superimposing-power-and-knowledge.html"><i class="fa fa-check"></i><b>51</b> Superimposing power and knowledge</a></li>
<li class="chapter" data-level="52" data-path="52-ranked-subject-positions.html"><a href="52-ranked-subject-positions.html"><i class="fa fa-check"></i><b>52</b> Ranked subject positions</a></li>
<li class="chapter" data-level="53" data-path="53-conclusion-out-of-the-data.html"><a href="53-conclusion-out-of-the-data.html"><i class="fa fa-check"></i><b>53</b> Conclusion: Out of the Data}</a></li>
<li class="chapter" data-level="54" data-path="54-machine-learners.html"><a href="54-machine-learners.html"><i class="fa fa-check"></i><b>54</b> 250,000 machine learners</a></li>
<li class="chapter" data-level="55" data-path="55-a-summary-of-the-argument.html"><a href="55-a-summary-of-the-argument.html"><i class="fa fa-check"></i><b>55</b> A summary of the argument</a></li>
<li class="chapter" data-level="56" data-path="56-in-situ-hybridization.html"><a href="56-in-situ-hybridization.html"><i class="fa fa-check"></i><b>56</b> In-situ hybridization</a></li>
<li class="chapter" data-level="57" data-path="57-critical-operational-practice.html"><a href="57-critical-operational-practice.html"><i class="fa fa-check"></i><b>57</b> Critical operational practice?</a></li>
<li class="chapter" data-level="58" data-path="58-obstacles-to-the-work-of-freeing-machine-learning.html"><a href="58-obstacles-to-the-work-of-freeing-machine-learning.html"><i class="fa fa-check"></i><b>58</b> Obstacles to the work of freeing machine learning</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning: Archaeology of a Data Practice</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="observing-costs-losses-and-objectives-through-optimisation" class="section level1">
<h1><span class="header-section-number">13</span> Observing costs, losses and objectives through optimisation</h1>
<p>Faced with the impracticality of an analytical or mathematically closed form solution to the problem of finding a function, machine learners typically seek ways of observing how different models traverse the data. They replace the exactitude and precision of mathematically-deduced closed-form solutions with algorithms that generate varying solutions. A range of techniques search for optimal combinations of parameters. These optimisation techniques are the operational underpinning of machine learning.  Without their iterative processes, there is no machine in machine learning. They have names such as ‘batch gradient descent’, ‘stochastic gradient ascent,’ ‘coordinate descent,’ ‘coordinate ascent’ as well as the ‘Newtown-Raphson method’ or simply ‘convex optimisation’ <span class="citation">[@Boyd_2004]</span>.  These techniques have a variety of provenances (Newton’s work in the 17th century, for instance, but more typically fields such as operations research that were the focus of intense research efforts during and after World War ; see <span class="citation">[@Bellman_1961; @Petrova_1997; @Meza_2010]</span>). Much of the learning in machine learning occurs through these somewhat low-profile yet computationally intensive techniques of optimisation. </p>
<p>Optimisation is a practice of observation. ‘Science brings to light partial observers in relation to functions within systems of reference’ write Gilles Deleuze and Félix Guattari in their account of scientific functions <span class="citation">[@Deleuze_1994, 129]</span>.<a href="#fn45" class="footnoteRef" id="fnref45"><sup>45</sup></a>   In many machine learning techniques, the search for an approximation to the function that generated the data is optimised by reference to another function called the ‘cost function’ (also known as the ‘objective function’ or the ‘loss function’; the terms are somewhat evocative of both economics and cybernetics). Machine learning problems are framed in terms of minimizing or maximising the cost function.  ‘Cost’ or ‘loss’ takes the form of errors, and minimizing the cost function implies minimizing the number of errors made by a machine learner. As we saw earlier, in his formulation of the ‘learning problem’, the learning theorist Vladimir Vapnik speaks of choosing a function that approximates to the data, yet minimises the ‘probability of error’ <span class="citation">[@Vapnik_1999, 31]</span>.</p>
<p>The  compares predictions generated by a machine learner to known values in the data set. Every cost function implies some measure of the difference or distance between the prediction and the values actually measured. Cost functions in common use include squared error, hinge loss, log-likelihood and cross-entropy.  In classifying outcomes into two classes (the patient survives versus patient dies; the user clicks versus user doesn’t click; etc.), the cost function has to express their either/or outcome. Crucially, if cost functions re-configure ‘the act of fitting a model to data as an optimization problem’ <span class="citation">[@Conway_2012, 183]</span>, function finding and hence machine learning in general occurs iteratively. Given a cost function, a machine learner can vary its parameters keeping in view – or partially observing – whether the cost function increases or decreases. Just as the logistic function wraps the linear regression model in a sigmoid curve that switches smoothly between binary values, the cost functions diagram model parameters (usually noted as <span class="math inline">\(\beta\)</span>) in relation to known responses or output values in the data. If there is learning here, it does derive from mathematic forms or higher abstraction. Cost functions diagram relations between models, and render their predictive reference through the negative feedback loops described by Norbert Wiener <span class="citation">[@Wiener_1961]</span>.   Importantly, these feedback loops are not closed mechanisms but places from which variations can be viewed. </p>
<p>For instance, the log-likelihood function,  a typical and widely-used cost function associated with logistic regression is defined as:</p>
<span class="math display">\[\begin{equation}
\label {eq:cost_function_logreg}
J(\beta) = \sum_{i=1}^{m} y_i log{ h(x_i)} + (1 -  y_i) log{(1 - h(x_i)})
\end{equation}\]</span>
<p>where</p>
<p><span class="math display">\[h_{\beta}(x) = \frac{1}{1+e^{-\beta^T x}}\]</span></p>
<p>Equation  enfolds several manipulations and conceptual framings (particularly the Principle of Maximum Likelihood, a statistical principle; see chapter ). But key terms stand out. First, the cost function <span class="math inline">\(J(\beta)\)</span> is a function of all the parameters (<span class="math inline">\(\beta\)</span>) of the model. They substitute in through the subsidiary function <span class="math inline">\(h_{\beta}(x)\)</span>, the logistic function function encapsulating a linear function <span class="math inline">\(\beta^T X\)</span> . Second, the function defines a goal of maximising the overall value of the expression, <span class="math inline">\(J(\beta)\)</span> as a function of variations in the parameters <span class="math inline">\(\beta\)</span>. The <span class="math inline">\(min\)</span> describes the results of the repeated application of the function. Third, the heart of the cost function is balancing of two tendencies: it adds (<span class="math inline">\(\sum\)</span>) all the values where the probability of the predicted class of a particular case <span class="math inline">\(h(x_i)\)</span> matches the actual class <span class="math inline">\(y_i\)</span>, and subtracts (<span class="math inline">\(1-y\)</span>) all the cases where the probability of the predicted class does not match the actual class. This so-called <em>log likelihood</em> function can be maximised through optimisation, but not solved in closed form. The optimal values for <span class="math inline">\(\beta\)</span>, the model parameters that define the model function need to be found through some kind of search. </p>
</div>
<div class="footnotes">
<hr />
<ol start="45">
<li id="fn45"><p>Its hard to know whether Deleuze and Guattari were aware of the extensive work done on problems of mathematical optimization during the 1950-1960s, but their strong interest in the differential calculus as a way of thinking about change, variation and multiplicities somewhat unexpectedly makes their account of functions highly relevant to machine learning. <a href="13-observing-costs-losses-and-objectives-through-optimisation.html#fnref45">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="12-curves-and-the-variation-in-models.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="14-gradients-as-partial-observers.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
