# What flat data does to pluri-dimensional things
 Genomic topologies 

Transforming


## to do 

- use the ml-lit analysis to point to the rise of bioinformatics






# Machine Learning and Genomic Dimensionality: from Features to Landscapes

Adrian Mackenzie

Centre for Economic and Social Aspects of Genomics, 

Lancaster University



  









## Introduction

The Google Compute Engine, a globally-distributed ensemble of computers,  was briefly turned over to exploration of cancer genomics during 2012, and publicly demonstrated during the annual Google I/O conference. Midway through the demonstration, in which a human genome is visualized as a ring in 'Circos' form  (see Stevens' chapter, this volume; [@Krzywinski18062009]), the speaker, Urs Hölzle, Senior Vice President of Infrastructure at Google 'then went even further and scaled the application to run on 600,000 cores across Google’s global data centers' [@google_inc_behind_2012]. The audience clapped. The world's '3rd largest supercomputer', as it was called by TechCrunch, a prominent technology blog,  'learns associations between genomic features' [@anthony_google_2012]. We are in the midst of many such demonstrations of 'scaling applications' of data in the pursuit of associations between 'features.' 

Nearly all contemporary epistemic practices — scientific or not — depend on infrastructures of communication and computation. But some sciences are much more extensively invested in computing infrastructures as ways of making sense of data. These investments are not incidental, ancillary or superficial. They deeply permeate the practices, the imaginaries, and the modes of existence of high-profile, large-scale sciences.  As the Google Compute demonstration suggests, genomes feature strongly in the data economy. The abundance, size, complexity and subtle variability of genomes as _sequence data_ are a touchstone for the more general economisation of data we are currently experiencing. Genomic infrastructures — the ensemble of software, hardware, algorithms, networks and repositories that handle sequence data and other biological data — tell us something of how genomes come to be be what they are.  The power of computational machines such as the Google Compute Engine to learn associations in genomes is widely held to be the only way to make sense of the vast aggregates of sequence data produced by genomics, especially since the advent of high-throughput sequencing platforms in the mid-2000s. In classifying, predicting and exploring biological processes through sequence data, _machine learning_  makes sense of differences and variations that cannot be seen or grasped by eye or hand alone.  

Such techniques -- and I will describe a few examples below -- also change what a genome is for us. The mode of existence of genomes, ranging across  epistemic (see Fox Keller's chapter), affective (see Fortun's chapter),  political (see Fujimura & Rajagopalan's as well as Richardson's chapter), and ontological (see Dupre's chapter) aspects, changes as these techniques come into play. A complicated information ecology has developed in and around genomics  over the last few decades. I think we need to understand better how the heavy investment in information retrieval systems for biological data has gradually produced a substrate of biological data that supports shifts in epistemic and economic dynamics of genomes. The spectacle of the Google Compute demonstration attests to some of those shifts. Machine learning techniques crystallise this transformation in their heavy dependence on information retrieval systems and their almost aggressive efforts to reorganise  scientific knowing on a different scale. Machine learning does not exhaust or indeed come anywhere near replacing the sophisticated understandings of biological processes associated with epigenetics or with pervasive transcription (see again, Fox Keller, this volume).  But it cuts a swathe through scientific literatures, through the training  and recruitment of scientists, through public and private investments in genomic science, and it connects genomics to broader social, technological, economic and political dynamics.

Looking outwards, contemporary genomics  exemplifies and possibly points to some of the limits of 'data intensive' science, government, media, and business more generally. The attempts to machine learn genomes illustrate how difficult it is to fully domesticate  data or render it fully subject to calculation. In this chapter, I use code and data vignettes from contemporary genomics to explore genomic data practice increasingly resorts to Google Compute-style modes of operation. Writing code and working with genomic data as a social scientist or humanities scholar is a way to explore and perhaps think through some of the transformations associated with data practice more generally. At times, the code and data will be hard to read. I suggest readers might treat those moments of illegibility as  encounters with the often convoluted materialities of genomics. 

## 'Wide, dirty and mixed' datasets in genomics

The I/O conference audience, largely comprising software developers, could hardly be expected to have a detailed interest in what was being shown on the screen. Their interest was steered toward the immediate availability of huge computing power: from 10,000 to 600,000 cores in a few seconds. The principal chain of associations in the genomics demonstration was, presumably, something like: genome=>complexity=>cancer/disease=>life/death=>important. Yet the Google Compute demonstration is, I would suggest, typical of some recent transformations in how genomes are handled, and in what is being addressed in efforts to analyse genomic data. This transformation is only  hinted at in the Google I/O keynote address in Hölzle's talk of genomic features, gene expression and patient attributes.  The only concrete indication of what was happening in the demonstration consisted in the mention of one machine learning algorithm, RF-ACE (Random Forest- Artificial Contrasts with Ensembles).  As Google's webpages described it: 'it allows researchers to visually explore associations between factors such as gene expression, patient attributes, and mutations - a tool that will ultimately help find better ways to cure cancer. The primary computation that Google Compute Engine cluster performs is the RF-ACE code, a sophisticated machine learning algorithm which learns associations between genomic features, using an input matrix provided by ISB [Institute for Systems Biology](https://www.systemsbiology.org/). When running on the 10,000 cores on Google Compute Engine, a single set of association can be computed in seconds rather than ten minutes, the time it takes when running on ISB’s own cluster of nearly 1000 cores. The entire computation can be completed in an hour, as opposed to 15 hours' [@google_inc_behind_2012]. Amidst this mire of fairly technical computing jargon, we might observe that Google applies here an algorithm developed by engineers at Intel Corporation and Amazon to genomic datasets provided by the Institute of Systems Biology (ISB), Seattle, a doyen of big-data genomics. This confluence of commerce (Amazon), industry (Intel), media (Google) and genomic science (ISB) is, I suggest, symptomatic of data practices in genomics and elsewhere. 

Now the RF-ACE algorithm, first published in 2009, is a state-of-the art  attempt to  deal with 'modern data sets' that are 'wide, dirty, mixed with both numerical and categorical predictors, and may contain interactive effects that require complex models' [@tuv_feature_2009, 1341]. Such algorithms and the datasets they work on have a distinctive texture, which, I would suggest,  we should try to grasp if we want to see what is happening to genomes. Coming to grips with the ways in which algorithms traverse, shape and partition or segment  the notoriously monotonous and super-abundant sequence data that genomics has been generating for the last few decades might be a useful way to make sense of recent developments in  genomics more generally. We can see what is happening inside RF-ACE because Google has recently made the RF-ACE  algorithm accessible as a code library for the widely used statistical programming language, `R` [@r_development_core_team_r_2010].  In making sense of the reorganisation of sciences within a knowledge economy, being able to access and follow the transformations genome-related data undergoes in contemporary algorithms is a definite good. Unlike some other place where machine learning is used (national intelligence for instance), we can follow what happens to the data.  With a little knowledge of programming languages  such `R` or `Python` used by genomic scientists, it is possible to download the package (https://code.google.com/p/rf-ace/wiki/RPackageInstallation), experiment with the algorithm and thereby gain some sense of the texture of contemporary data practices in fields like genomics, but also business, etc.  

The beginning of an R session with RF-ACE, using a sample datasets whose shape and composition is typical of genomic datasets, is shown below:

```
         N:output C:class N:input N:noise_1 N:noise_2 N:noise_3 N:noise_4
sample_1    1.586       3   7.490    -0.220    -0.078     0.078    -0.166
sample_2    3.971       1   5.832    -0.357     0.097     0.684    -2.374
sample_3    6.202     NaN   6.477    -1.420     0.430     0.474     0.450
         N:noise_5 C:noise_6 N:noise_7
sample_1     1.464         1    -0.882
sample_2     1.032         3    -0.104
sample_3     1.333         3    -0.055
```
```
 Predictions .... 
```
```
[[1]]
[[1]]$target
[1] "N:output"

[[1]]$sample
[1] "sample_1"

[[1]]$true
[1] 1.586

[[1]]$prediction
[1] 1.635

[[1]]$error
[1] 1.223
```
I am not going to delve into the RF-ACE algorithm here, but already in this output from the code,  several  typical machine learning features appear. First, the algorithm produces 'predictions' whose values can be compared to the actual values of the data. Prediction plays a crucial role in working with genomic data, not only in the sense that one might wish to predict who will develop Hodgkins lymphoma on the basis of a DNA sequence produced from a saliva swab, but in the sense that one might want or need to predict how a particular domain or region of a genome functionally interacts with various biological processes.   Second, we can see here something of the shape and composition of contemporary datasets -- it contains numerical data (N:class), classifications (C:class), missing values ('NaN' - not a number).  What these different columns of data refer to is *almost* completely irrelevant. Column 3 (Class:N) might refer to population group, type of cancer, species, etc. Column 4 might be a cholesterol level, concentration of an enzyme, or average survival time, etc. Even at a generic level, this data extract looks different to the typical datasets of modern statistics. Compare for instance, the 'iris' dataset, one of the most heavily used datasets in all of statistics, dating from the late 1930s, and extensively analysed in the context of plant breeding and genetics by R.A. Fisher, a key contributor to 20th century statistics [@fisher_use_1936]:

```{r iris, echo=FALSE, message=FALSE, fig.cap ="Fisher's iris dataset, 1936",comment=NA, size='smallsize'} 
	data(iris)
	print(iris[1:5,])

```
While Fisher developed various ways of discriminating between different species of iris in terms of petal and sepal lengths, we can already see  from the extract of the dataset that it is shaped differently to the RF-ACE dataset. It is quite narrow as it has only a few columns, the data is nearly all of one type (lengths), and the data is clean (there are no missing values). If the RF-ACE dataset is typical of contemporary genomic data, iris is typical of classic statistics, and much  genetic data prior to genomics in its relatively homogeneity. Third and finally, there is much data missing from the table because it cannot be shown or easily displayed. In this example, the actual dataset is _wider_ than it is longer  (that is, 103 samples, each characterised by 300 different 'features' or variables), and this breadth of data simply cannot be displayed on the page. _Wide_ datasets are quite common in machine learning settings generally, but particularly common in genomics where they might only be a relatively small number of biological samples but a huge amount of sequence data for each sample. We will soon see how machine learning seeks to corral and contain their tendencies to sprawl.  In these features -- predictivity, heterogeneity, expansiveness -- we glimpse some problems that are not limited to,  but certain writ particularly large in,  genomics.

In order to see how machine learning comes to bear on genomes, we need to see how such wide and mixed datasets have become available.  The iconic data type in genomics is the sequence of DNA base pairs, often known as the 'base sequence.' The abundance of base sequence data is well-known and has been an integral, material component of genomes and of genomics. Even if sequencing was and is boring (see Fortun's chapter, this volume), generating sequence data has been and remains the  genomic *modus operandi.* 

```{r name, echo=FALSE, cache=FALSE, message=FALSE, warning=FALSE, comment=NA, size='smallsize', results='markup', fig.cap='GenBank sequence data for a gene from tanager (Ramphocelus)' } 
	library(ape)
	ref <- c("U15717", "U15718", "U15719", "U15720")
	rampho <- read.GenBank(ref, as.character=TRUE)
	cat(rampho[[1]][1:150], "\n", sep = "", fill=TRUE)
```

In a sense, the sequence shown above, the first 150 bases from the GenBank sequences for gene `U15717` from the *tanager (Ramphocelus),*  a bird that lives at the edge of tropical forests, do not fit the picture I have just been drawing. This data is monotonously flat and unvarying it seems. It might be wide in the sense described above, but it is not mixed.   Importantly, however, the abundance of sequence data has attracted and facilitated the many tools and databases of bioinformatics, the hybrid discipline that combines computer science and genomic biology. A teeming ecology of databases and software tools  focus on adding layers and annotations to such sequences. This means that base sequences not only accumulate but are transformed in this accumulation. What we might following Marx call the 'primitive accumulation' phase of genomics over the last 25 years has yield not only a highly accessible stock of sequence data, but sequences that can be mapped onto, annotated, tagged and generally augmented by many other forms of data. Again, the software tools of bioinformatics implement the combination of retrievability and augmentation. 

```{r biomart, cache=FALSE, echo=TRUE, message=FALSE,fig.cap = "Some of the many 'biomarts'",comment=NA, results='markup'} 
	library(biomaRt)
	marts <- listMarts()
	print(head(marts)) 

```

The code fragment above loads the evocatively named 'biomart' library, lists the first 10  available 'biomarts.' There are `r nrow(marts)` biomarts online at the time of writing. Already these names  - 'ensembl', 'snp', 'vega' -- suggest  that these databases __cover__ a wide range of biological data, some of which is sequence-based. Looking more closely at one of the more sequence-oriented databases:

```{r ensembl, cache=FALSE, echo=FALSE, comment=NA, message=FALSE,size='smallsize', results='markup'} 
	mart <- useMart("ensembl")
	datasets <- listDatasets(mart)
	print(datasets[1:10,1])
```

This code fragment connects to the European Bioinformatics Institute's Ensembl database, and lists the names of the first few datasets held in that database. As we can see from this list, a list that actually extends to `r nrow(datasets)` species, there are many datasets in Ensembl. At row 8, _hsapiens_gene_ensembl_ is likely to be a widely used one. What does it provide?

```{r homo_gene,cache=FALSE, echo=TRUE,message=FALSE,comment=NA, size='smallsize', results='markup'} 
		mart = useMart("ensembl", dataset="hsapiens_gene_ensembl")
		listAttributes(mart)[1:10,1]
		g = getGene( id = "100", type = "entrezgene", mart = mart)
		show(g)
```
 Two things happen here. Some of the attributes of the _hsapiens_gene_ensembl_ dataset are listed, and the code then retrieves the database record for a gene with id=100 in the database, but more commmonly known as ADA. The Ensemble record for ADA shows some information about the chromosomal location of the gene, its position in the human genome sequence (`r g$start_position[1]`, for instance, gives the base pair number at which the ADA starts in the full human genome sequence).

 These brief code snippets and vignettes demonstrate the accessibility of sequence-related data but also some of the ways in which related data can be aggregated or superimposed. In a few lines of code it is possible to move across many databases, and quickly move to almost any level of detail ranging from collections of genomes down to short sequences of DNA. The accessibility of sequence datasets is such that even a social science researcher can quickly write programs to retrieve this data. It attests to  several decades, if not longer, work on databases, web and network infrastructures, and analytical software, all, almost without exception, driven by the desire for aggregation, integration, archiving and annotation of sequence data that first became highly visible in the Human Genome Project of the 1990s.  The brevity of these lines of code -- half a dozen statements in R, no more -- suggests we are dealing with a high-sedimented set of practices, not something that has to be laboriously articulated, configured or artificed. Code brevity almost always signposts  highly-trafficked routes in contemporary network cultures. Without describing in any great detail the topography of databases, protocols and standards woven by and weaving through bioinformatics, these examples suggest that the mixed, dirty, wide datasets fed to algorithms such as RF-ACE depend on the couplings and congregations of different databases connected into meta-databases such as Ensembl. As the code shows, sequence and other genomic data (and we will see some other types of contemporary genomic data below)  are available to scientists not only as users searching for something in particular and  retrieving specific data, but to scientists as programmers developing  ways of connecting up, gathering and integrating many different data points into to produce the wide ( many-columned), mixed (different types of data)and dirty (missing data, data that is 'noisy') datasets digestible by RF-ACE. Somewhat recursively, corralling sequence data has led to highly developed genomic data infrastructures that come to serve almost as a kind of biological data commons whose primary resources -- base sequences -- are increasingly woven together with many other forms of comparison, searching, classifying and sorting. The interwoven assemblage of genomic and biological databases, archives, software, and data standards is perhaps unrivalled by any other science in its variety and sheer density. And again, the generic connectivity of these practices is not specific to genomics (although it plays out in specific ways there, affecting, for instance, the patterns of authorship, collaboration and reputation in this field; see Ankeny and Leonelli, this volume). The same sedimented, highly compressed layering of infrastructures, in which many details about protocols, transactions, architectures and interfaces withdraw from view, can be found in many settings (in business, social media, supply chain management, etc.). 

## Forests and lassos in genomics

Highly leveraged infrastructures for access to biological data reshape what counts as a genome. As a data form, genomes in many  ways becomes less linear or flat than the base sequences. The linear sequences of DNA data becomes more mixed and wide partly through the accessibility we have just seen that allows them to be superimposed, annotated and layered. But their shape also changes for a different reason. A recent review in the journal *Genomics* highlights the increasing importance of the random forest (RF) machine learning techniques we saw in use during the Google I/O keynote writes:

> High-throughput genomic technologies, including gene expression microarray, single Nucleotideide polymorphism (SNP) array, microRNA array, RNA-seq, ChIP-seq, and whole genome sequencing, are powerful tools that have dramatically changed the landscape of biological research. At the same time, large-scale genomic data present significant challenges for statistical and bioinformatic data analysis as the high dimensionality of genomic features makes the classical regression framework no longer feasible. As well, the highly correlated structure of genomic data violates the independent assumption required by standard statistical models[@chen_random_2012, 323].

This kind of commentary on the changing shape, not just the volume, of genomic data is quite common. First of all,  newer instruments or tools such as microarrays and faster sequencers (so-called 'next generation sequencers') loom large. The tropes of waves, deluges, floods and waves of sequence data being somewhat washed out, this account instead highlights the 'high dimensionality of genomic features' and the 'highly correlated structures of genomic data.'  The new ways of working with sequence data typically highlight changes in statistical or modelling approaches. So for instance, Chen and co-authors recommend the use of the random forest algorithm because it:

>is highly data adaptive, applies to “large p, small n” problems, and is able to account for correlation as well as interactions among features. This makes RF particularly appealing for high-dimensional genomic data analysis. In this article, we systematically review the applications and recent progresses of RF for genomic data, including prediction and classification, variable selection, pathway analysis, genetic association and epistasis detection, and unsupervised learning [@chen_random_2012, 323]

The key terms on the machine-learning-side of this formulation are 'large p, small n', 'high dimensional', 'prediction', 'classification,' 'variable selection' and 'unsupervised learning.' While these terms are widely used in machine-learning research since the early 1990s,  they are becoming increasingly visible in genomics. The key terms on the genomics side of this formulation would perhaps be 'pathway analysis', 'genetic association,' and 'epistasis.' These biological terms point to forms of  relationality typically associated with biologically interesting processes. Epistasis for instance broadly refers to linked gene action, a process that has been difficult to study before high-throughput methods of functional genomics were developed. In contemporary genomic science, these biological processes are increasingly understood in terms of eliciting and modelling the relations between *features* of genomic datasets in order to classify and predict biological outcomes. In between the machine learning and the genomic references appear several statistical terms: 'correlation' and 'interaction.' How does machine learning differ from the statistical practice that has underpinned much of modern biology?

The importance of statistics in life sciences is hardly new. The historian of biology Bruno Strasser suggests an ‘important novelty of contemporary data-driven science is the omnipresence of statistical methods’ [@strasser_data-driven_2012, 87]. In commenting on the development of ‘big data’ or ‘discovery mode’ genomic science, Strasser presents episodes from the history of biology that prefigure contemporary turns to data as fount of epistemic value. He argues, for instance, that natural history practices entailed large repositories and archives of data-rich specimens and samples whose ordering and relations were constantly revisited and reassessed in the pursuit of scientific discovery: ‘natural history had been “data-driven” for many centuries before the proponents of post-genomics approaches and systems biology began to claim the radical novelty of their methods. As I have argued here, many of what are claimed as novel features of contemporary data-driven science have parallels among earlier natural history practices’ [@strasser_data-driven_2012, 87]. As Strasser points out, the novelty of genomics lies not in collections of data that are interrogated for hitherto unseen patterns of relations, but rather in the ‘omnipresence of statistical methods.’ In Strasser's account of the precedents for genomics, much hinges  on what is meant by 'statistical methods.'  I would suggest that genomics data has increasingly troubled mainstream statistical practices, and this trouble has  led to many significant shifts in statistical technique since the 1990s.  Amongst these shifts, machine learning represents a kind of extreme or brutal solution. But whether it is  brutal or not, it cannot be simply seen as a continuation of statistical methods. 

Some of the difficulties in statistical practice can be illustrated by GWAS (Genome Wide Association Studies; note the resonance again of the term 'wide'), studies that compare selected variations in the genomes of individuals from a given population. Note that GWAS studies rely heavily on the accumulated, augmented genomic datasets described previously. And GWAS in turn provide, for instance, the knowledge base on which 23andMe, the consumer genetic profiling service, draws.  Typically, GWAS uses microarrays to look for statistically significant associations between the presence of SNPs (Single Nucleotide Polymorphisms) in individuals and the occurrence of diseases. While microarrays in many different forms are heavily used in research,  they have been surprisingly slow to affect clinical practice. In their account of the surprisingly slow shift of microarrays towards clinical practice, Paul Keating and Alberto Cambrosio identify the rate-determining role of statistics:

>The handling and processing of the massive data generated by microarrays has made bioinformatics a must, but has not exempted the domain from becoming answerable to statistical requirements. The centrality of statistical analysis emerged diachronically, as the field moved into the clinical domain, and is re-specified synchronically depending on the kind of experiments one carries out [@keating_too_2012,49].

Confronted by  the wide datasets produced by microarrays,  GWAS could only become answerable to statistics (that is, meet the standards of validity) by developing novel  statistical practices, and here machine learning has played an important role. While Keating and Cambrosio interestingly suggest that statistical and bioinformatics practices are hybridizing in the development of GWAS, their analysis pays less attention to  actual shifts in statistical practice coming into play in GWAS and many other areas of genomics.  GWAS and microarrays are a case, perhaps quite low profile in some ways, in which machine learning techniques have pervasively affected genomics.

The problem is, as mentioned already, the dimensionality of  datasets in GWAS. This dimensionality obstructs mainstream statistical practice. Again , as is often the case in genomics, thousands of these datasets are readily accessible in public databases such as the EBI's ArrayExpress and the NCBI's (National Centre for Biotechnology Information) GEO. 

```{r arrayexpress1, echo=FALSE, cache=FALSE, message=FALSE, comment=NA, size='smallsize', results='markup' } 
	
	library(ArrayExpress)
	pneumoHS = queryAE(keywords = "pneumonia", species = "homo+sapiens")

``` 

If we query ArrayExpress for datasets relating to pneumonia in humans, `r length(pneumoHS$ID)` are shown. (Most of these, we can see from the IDs, were deposited at NCBI's GEO, but have been subsequently mirrored by ArrayExpress; mirrioring is a crucial infrastructural practice in the  genomic infrastructures). Retrieving the processed, not the raw data, for one of them chosen at random:

```{r arrayexpress2, echo=FALSE, cache=FALSE, message=FALSE, warning=FALSE,comment=NA, size='smallsize', results='markup' } 
	
	mex = getAE(pneumoHS$ID[22], type = "processed", path='data')
	cnames = getcolproc(mex)
	MEXProc = procset(mex, cnames[2])
	experimentData(MEXProc)
```
Some of the various types of 'feature data' are listed. But the 'high dimensional' character -- the 'high *p*, small *n*' -- of the data can be seen in an excerpt from the metadata:

```{r arrayexpress3, echo=FALSE, cache=FALSE, message=FALSE, comment=NA, size='smallsize', results='markup' } 
	
	dims(MEXProc)
	featureNames(MEXProc)[1:10]
```

The datasets produced by such studies are wide rather than long because they have *many more* features (*p*=34441) than samples (*n*=31). In the context of GWAS, most of the features are SNPs. While a large study may have thousands of rows corresponding to tissue samples (for example, the Wellcome Case Control Consortium studied 14,000 individuals [@burton_genome-wide_2007]), it will certainly  have many more columns since each SNP tested on the microarray will have its own column in the dataset (up to a million or so). A relatively small number of the columns of a typical GWAS dataset often record the incidence of some biomarker (levels of blood lipids, etc.) or physiological variable (for instance, blood pressure, sex or height). The vast bulk of the columns in a GWAS dataset, however, will contain data from the microarray that points to the presence or absence of single mutations in the base sequence:

```{r arrayexpress4, echo=FALSE, cache=FALSE, message=FALSE, comment=NA, size='smallsize', results='markup' } 
	
	head(exprs(MEXProc))
```

Viewed from the standpoint of mainstream statistics, each cell on a microarray is effectively a separate assay or experiment, subject to its own statistical tests and inferences.  Bringing together the analysis of patterns of distributions of hundreds of thousands of SNPs in GWAS and their association with disease represents a significant statistical problem in juggling statistical hypotheses. Finding the association between a _single_ SNP and the disease or some phenotypical trait is quite straightforwardly done using standard statistical tests. But in GWAS, scientists do not know in advance which of the often several hundred thousand SNPs are associated with the trait. Furthermore, it is very unlikely that a single SNP is associated with a disease or given biological trait (for example, faster growth of a particular strain  of an agricultural plant) in any case, since that would suggest that we are dealing with the relatively uncommon case of a single-gene trait or single-gene disorder. Much more likely, a set of SNP’s scattered across the genome will be associated with the trait under study. 

```{r gwas_manhattan, echo=FALSE, message=FALSE, cache=FALSE, warning=FALSE, fig.cap='Manhattan plot showing SNP-trait associations', dpi=400} 
	
	library(gwascat)
	library(ggbio)
	gwtrunc = gwrngs
	mlpv = values(gwrngs)$Pvalue_mlog
	mlpv = ifelse(mlpv >25, 25, mlpv)
	values(gwtrunc)$Pvalue_mlog = mlpv
	gwlit = gwtrunc[which(seqnames(gwtrunc) %in% c("chr4", "chr5", "chr6")) ]
	mlpv = values(gwlit)$Pvalue_mlog
	mlpv = ifelse(mlpv >25, 25, mlpv)
	values(gwlit)$Pvalue_mlog = mlpv
	methods:::bind_activation(FALSE)
	autoplot(gwlit, geom="point", aes(y=Pvalue_mlog), xlab="chr4-chr6")

```

In statistical terms, GWAS analysis means testing hundreds of thousands of hypotheses simultaneously, and deciding suggest that the associations between SNPs are likely to occur by chance. GWAS publications typically use Manhattan plots to highlight the associations between the trait or disorder and the occurrence of SNPs in sample genomes. The plots show how likely particular SNPs are to occur by chance in the presence of the trait or disorder. The p-values become smaller, and hence the ($-log10$) ‘buildings’ on the Manhattan skyline become taller when SNPs are unlikely to occur by chance. The plot shows in other words the statistical significance of the occurrence of SNPs. The SNP at 19q13 has a p-value of around $10exp-15$, a vanishingly small probability of happening by chance. While there has been much debate about the proper ranges of p-values in GWAS, and the statistical treatment of GWAS has become increasingly sophisticated as analysis techniques take into the account the problems of false positives, these statistical developments are not to my mind the most significant shift in statistical practice associated with genomes as such.

At a genomic level, the main challenge lies not in the association of sets of SNPs with traits as such, but in working out which combination of the hundreds of thousands of associations between SNPs and trait are significant.  Take for instance the task of analyzing epistasis or gene interaction in a GWA study. In epistasis, the level of expression of one gene is affected by its interactions with other genes. This would an instance of a 'highly correlated structure' in genomic data. The expectation is that analysing how combinations of SNPs are associated with traits or disorders will yield insights into epistatic processes associated disease or development.  As a review of statistical reasoning in GWAS points out:

> SNPs that combine to make larger genetic effects can statistically reflect an epistatic interaction, where the alleles of one gene influence the effects of alleles of another on a trait value or risk of disease. These interactions are by definition nonlinear and thus can dramatically increase the trait or risk [@cantor_prioritizing_2010, 10]

Interaction between genes affects the expression of specific genes. This is seen to play an important biological role in complex disorders and also in development. Yet statistically modelling these interactions is, even in the eyes of statistically minded genomic researchers, extremely difficult. As Cantor and co-authors point out, ‘in GWAS, even for main effects, the number of predictors far exceeds the number of observations. Exhaustive examination of all pairwise interactions is possible, but for multiway interactions the task is totally impractical’ [@cantor_prioritizing_2010, 11]. The number of predictors or 'features' is given by the number of SNPs analysed in the study. This is currently around one million, suggesting an astronomical number of potential interactions. The network of possible interactions, and the combinatorial explosion of possible pathways between parts of genomes involved in these interactions, dwarfs readily available computational power. 

Finding and statistically validating the possible association of a set of SNPs chosen from so hundred of thousands of variables requires a different approach to significance testing and statistical modelling.  Cantor and co-authors write, ‘recent advances in data mining … can be used to prioritize GWAS results’ [@cantor_prioritizing_2010, 11]. What Cantor calls ‘data mining’ is commonly called 'machine learning’ or sometimes 'statistical machine learning' [@hastie_elements_2009] in order emphasize the altered practices of inference it brings to bear on this problem. Automated statistical model fitting — machine learning —  promises to locate epistasis or other biological functions in the datasets produced by GWAS when standard statistical inference would wander aimlessly because of its difficult in dealing with interactions or dependent processes. 

In standard statistical inference, linear regression is often used to explore the relationship between variables and some outcome. In GWAS, the variables would be SNPs, and the outcome might be presence of disease or not. Linear regression and classification models are a backbone of statistical practices. The intuition behind them is drawing a straight line that best fits the data.  Linear models ‘work’ because normal statistical studies have many observations of a small number of ‘features’ or ‘predictors,’ but as [@wu_genome-wide_2009] point out, ‘with hundreds of thousands of predictors, the standard methods of multivariate regression break down’ (714).

```{r lasso1,echo=FALSE, cache=FALSE, warning=FALSE, message=FALSE, fig.cap='Lasso analysis of blood pressure SNPs'} 

	library(lars)
	dat<- read.table('data/BPdata.txt')
	height <- dat$height
	bp <-dat[,'bloodpressure']
	
	par(mfrow=c(1,2))
	plot(bp,height)

	snps <- as.matrix(dat[,1:100])
	t1<-table(bp, snps[, "RS6659552"])
	snps1<-as.matrix(snps)
	height1<-height
	for(i in 1:100){
	height1<-height1[!is.na(snps1[,i])]
	snps1<-snps1[!is.na(snps1[,i]),]
	}
	
	lass <- lars(snps1, y=height1, max.steps=10)
	plot(lass)
	##coef(lass)
	##n<-nrow(snps1)
	##
	###to choose model
	##aic <- 2*lass$df + n*log(lass$RSS/n)
	##bic <- log(n)*lass$df + n*log(lass$RSS/n)
	##aic
	##bic
	##which.min(aic)
	##which.min(bic)

```

The shape of GWAS datasets, with many variables (~500,000-1,000,000) and relatively few observations (100-5000), means that linear regression cannot easily predict the best weighting of the variables. Faced with very wide datasets, the standard techniques of statistical analysis break down, and have been increasingly supplanted by statistical machine learning techniques.  The statistical techniques used in machine learning are somewhat different to mainstream statistical tests, modelling and inference. Just like GWAS experiments themselves in their use of microarrays to effectively conduct very many experimental tests at the same time, statistical machine learning usually constructs many models at the same time in order to then choose the most predictive model. (We have already glimpsed this with the random forest approach, which constructs many decision trees, and averages their result in order to classify or predict results.) 

The essence of many of the machine learning approaches consists in optimising hyper-parameters. Machine learning techniques often  find the optimum predictive combination of predictors (SNPs in this case) for the observed traits (disease, for instance), not by developing any kind of model of the phenomena under investigation. As in many other situations where machine learning is used (business analytics, image processing, etc;), the modelling techniques used are themselves somewhat agnostic as to biological explanation. (I will return to this point in the conclusion: what does the generic character of machine learning imply for genomics?). Techniques and algorithms such as *lasso penalized regression, neural networks, logic regression, and Bayesian partitioning* [@cantor_prioritizing_2010, 11]  that have been used to optimise the predictive mix of variables amidst the myriad of possible associations know nothing of  epistasis. Instead of trying to track biological processes or biochemical pathways, they trace structures present in the data. 

The somewhat daunting model *optimization function* for  one commonly used machine learning technique  called 'lasso regression' is shown below, and this mathematical function  displays some relevant features that might help us grasp what is how genomic data is handled in statistical machine learning:

>$\widehat{\beta}^{lasso} = argmin_\beta \sum\limits_{i}^{N}{(y_i - \beta_0 - \sum\limits_{j=1}^p{x_{ij}\beta_j})^2}$

>subject to $\sum\limits_{j=1}^{p }{\vert\beta_j\vert\ \leq t}$. 
 

>>	[@hastie_elements_2009, 68]

As in the case of the code-data vignettes for sequence database access, such mathematical expressions imply concrete operations that reshape genomic data. The forest of subscripts, letters and mathematical functions is hard for social scientists and humanities scholars to look at, but these expressions abound in the statistical machine learning literature, and litter the supplementary files of many genomic science publications. We do not need to understand them in great detail, but see in them some distilled expressions of things done to genomic data. For present purposes, perhaps the most outstanding feature of the lasso estimate function are the two $\sum$ operators that suggest that the function is repeatedly adding values. The inner $\sum$  effectively generates a whole series of standard linear regression models that express outcomes as a linear function of combinations of predictors. The outer $\sum$ creates an overall value for each model (using 'least sum of squares'), but all the while shaping the important predictor coefficients $\beta_j$ according to the second line of the model 'subject to...' This function seeks the minimum  value  (the `argmin`) of the hyper-parameter $\beta^lasso$, and at the same time constrains or 'lassos' the number of predictors that appear in the model: lasso does a kind of 'continuous subset [of features] selection' [@hastie_elements_2009, 69] Lowering the lasso variable $t$ effectively shrinks the number of features (SNPs in this case) that appear in the model of the epistatic interactions: ' making $t$ sufficiently small will cause some of the coefficients to be exactly zero' [@hastie_elements_2009, 69]. 

In some ways, the lasso technique does not radically break with mainstream statistics. It is a way of choosing one linear regression model amongst many. Like all linear regression models, lasso is still trying to fit the best line to the data (or actually, to a hyperplane). At the time, it reconstructs statistical practice fairly profoundly. Not only does lasso  sift through many possible interactions in order to highlight in a model those combinations or features with higher predictive potential, methods such as lasso seek to identify the most *sparse* model possible. A sparse linear model, one that minimizes the number of  features, facilitates biological interpretation. Given a smaller number of significant predictors, biological scientists can then  examine some combination of variables or predictors more closely, perhaps using laboratory techniques or through further integrative data analysis, in order to track down the pathways or the mechanisms through which the regions of DNA around the most highly predictive SNPs act biologically. In our example of analysis of epistasis, the most important feature of the machine learning techniques lies in the way that they seek to eliminate the highly correlated associations between SNPs that hold little predictive value (for instance, because they are clustered in one locus on a chromosome, and all relate to a single well known gene ) and highlight uncorrelated associations between SNPs that suggest hitherto unexplored pathways of gene interaction. 
	

## Localising variation:  463,320 *k-nn* and the clinical endpoints of micro-arrays

The broader point here, and I think it matters greatly for post-genomic science, is that the shift from one model to many models reorganises biological sciences. The machine-learned models flow back through genomic science, reorganising and refocusing research, experiments, databases and funding in subtle but important ways. Between the pre-HGP and post-HGP science, the status of significant differences in genomes shifted. Pre-HGP biology understood the significant differences between individual organisms largely in terms of gene alleles responsible for variations in phenotypes. Biological differences, and disease in particular, stemmed from different forms of genes. Understanding disease meant finding the disease genes. Even prominent proponents of genomics, such as Leroy Hood, writing of 'Biology and Medicine in the Twenty-First Century' in 1991, envisaged genomics as a way of simplifying 'the task of finding disease genes' [@hood_biology_1992,138]. Across the taxonomic hierarchies, genes were the object of much way of annotation, labelling and description. GenBank, the primary repository of gene sequences, still embodies this conception of variation. Two decades after the inception of the HGP, genomes present a different image of variation. According to Nikolas Rose, writing more recently, ‘there is no normal human genome; variation is the norm’ [@rose_normality_2009, 75]. ‘In this new configuration’, he writes, ‘what is required is not a binary judgment of normality and pathology, but a constant modulation of the relations between biology and forms of life, in the light of genomic knowledge.’ The emphasis in Rose’s formulation falls on ‘constant modulation’ of the relations between biology and forms of life. If post-genomic science departs from the understanding that there is no single genome but many genomes (see also Dupre, this volume), then according to Rose, variation itself becomes of primary interest. Here I want to argue that pursuit of variation is re-making the genome into ‘a form whose only object is the inseparability of distinct variations’ [@deleuze_what_1994, 21]. These variations are, as we will see, located within genomics understood as a technical enterprise, between genomes and within the genome. In all three loci of variation, machine learning comes into play.

Within genomics  itself,  machine learning figures as a way of correcting the for that fact that genomic instruments often produces different results, and that genomic knowledge claims shift unpredictably. That is, machine learning has increasingly been directed to the reorganisation of genomics itself. For instance, since microarray data itself suffers from many such problems of variation, the US Food and Drug Administration has since 2003 conducted a study of data analysis techniques for microarrays:  

>The US Food and Drug Administration MicroArray Quality Control (MAQC) project is a community-wide effort to analyze the technical performance and practical use of emerging biomarker technologies (such as DNA microarrays, genome-wide association studies and next generation sequencing) for clinical application and risk/safety assessment [@parry_k-nearest_2010, 292].

([@keating_too_2012] mention the  Microarray Quality Control Consortium (MACQ)  in their account of the hybridisation of genomics and biostatistics.) Phase I of the US Federal Drug Administration-led MACQ addressed many issues of data analysis in the context of the clinical applications of gene expression analysis using microarrays. The primary statistical issue there was minimizing the ‘false discovery rate’ [@slikker_genomics_2010,S1], a typical biostatistical problem. In MACQ-II however the focus rested on the construction of predictive models for ‘toxicological and clinical endpoints … and the impact of different methods for analyzing GWAS data’ [@slikker_genomics_2010, 2]. On both the clinical and GWAS fronts, the 36 participating research teams tried out many predictive classifier models. Different machine learning techniques generate different kinds of models, genomic and biomedical researchers are compelled to engage with the many variations in prediction. 

In the shift from MACQ-I to MACQ-II, the problem of variations in the predictions produced by the machine learning models moved to center-stage.  The problem of variation arises not because any of the different modelling strategies used in machine learning gene expression datasets are wrong or erroneous, but because every model moves through the ‘feature space’ [@parry_k-nearest_2010,292] in a different way. In the MACQ-II consortium, the teams were tasked to build 'classifiers,' models that predict whether a given sample or case belongs to a 'normal' or 'disease' group. This is a typical machine learning task.  On this point, the design and practice of MACQ studies, and indeed most GWAS, does not accord directly with Rose's suggestion that 'constant modulation' not 'binary judgment' is required. Classifiers often make binary judgments. Now, the most popular classifier in the MACQ consortiuim was the *k-nn* model: '[a]mong the 19 779 classification models submitted by 36 teams, 9742 were k-nearest neighbor-based (KNN-based) models (that is, 49.3% of the total) [@parry_k-nearest_2010, 293]. But, these models varied greatly in their predictions: 'there have been large variations in prediction performance among KNN models submitted by different teams' (293). Here we can begin to see that not only the genome itself varies, but the models themselves vary. In seeking to track and predict genomic variation, post-genomic science has constructed a new zone of variations at the intersection of database infrastructures and sequencing or gene expression technologies. 

As in many aspects of genomic sciences, a typical response to the problem of variation is to count things. Genomics is one of the greatest commitments to counting things  every undertaken.  In their attempt normalise the variations of a popular predictive models, one of the research groups in MACQ-II write that ‘for clinical end points and controls from breast cancer, neuroblastoma and multiple myeloma, we systematically generated 463,320 *k-nn* [*k*-nearest neighbour] models by varying feature ranking method, number of features, distance metric, number of neighbors, vote weighting and decision threshold’ [@parry_k-nearest_2010,292]. For present purposes, the striking feature here is the proliferation of models in an effort to  tame the variations of predictive models. The number of predictive models constructed here rivals the number of SNPs assayed by the microarrays.  Why do these models, in this case of exactly the same kind of data we have been discussed above, multiply so greatly? 

All data analysis faces the so-called ‘curse of dimensionality’ [@hastie_elements_2009,22], but genomic data is particularly 'cursed' by its high dimensionality, by what I have termed above its 'expansiveness.' Every distinct feature (e.g. a SNP) in a dataset effectively adds a new dimension to the data space. If every SNP, or every RNA transcript, adds a new dimension in the feature space, efforts to model this feature encounter  high dimensional spaces. Now nearly all classifier models seek some kind of regularity or boundary that cuts the space into two or more regions (e.g. 'diseased' and 'normal'). The line or surface that separates data points in a classifier can be linear (as in the lasso models) or non-linear (as in *k-nn* models). Since the 1950s, problems of classification and prediction in high-dimensional dataspaces have been the object of mathematical interest. The mathematician Richard Bellman coined the term ‘the curse of dimensionality’ to describe how partitioning becomes more unstable  as the dimensions of the dataspace increase  [@bellman1957dynamic]. The problem is that while the volume of a space increases exponentially with dimensions,  the volume of data usually does not usually increase at the same rate. In high dimensional spaces, the data becomes more thinly spread out. This makes it hard to construct good partitions. Sparsely populated spaces accommodate many different boundaries. 

Most machine learning techniques embody some way of managing the dimensionality of the feature space, either by effectively reducing that dimensionality (as we saw in the lasso technique, which corrals the numbers of predictive features in the model), or concentrating on localised regions of the feature space, as does *k-nn*. For instance, the *k-nn* models — ‘k-nearest neighbour models’ — analyzed by Parry as part of MACQ-II are widely used because they are the simplest way of the dealing with interactions in an ‘exceedingly large feature space’ [@parry_k-nearest_2010,292]. In *k-nn*, a training set of observations whose outcomes are known (e.g. clinical endpoints in cancer might be benign or malignant) are used to help classify (and hence predict) the test data. For our purposes, the interest of this technique lies in its way of travelling through the data. Rather than trying to find the line of best fit (as for instance, the linear regression models do), *k-nn* models find local clusters in the data, and create irregular boundaries in the data. 

```{r knn, echo=FALSE, cache=FALSE, message=FALSE, warning=FALSE, comment=NA, dpi=400, fig.cap='kNN model'} 
		source('genomic.R')
		k=c(5,15)
		knn(k)
```

As we can see from the *k-nn* figure above, in which data belongs to two classes (normal vs. not-normal), the decision boundaries produced by the algorithm can be unstable. The example shows two models, one for k=`r k[1]`, the other for `r k[2]`.  Each model examines the relations between `r k` points in deciding whether a particular case belongs to one class or another.  While *k-nn*’  finds local clusters and classifies on the basis of an irregular decision boundary, this classificatory power comes at the cost of instability. As [@hastie_elements_2009] put it, ‘the method of k-nearest neighbors makes very mild structural assumptions: its predictions are often accurate but can be unstable’ [@hastie_elements_2009,11]. These shifting boundaries  stem from the dimensionality of the data. The more dimensions or features in the dataset, the larger the local neighbourhood needed to capture a fraction of the volume of the data,  and the more likely that most sample points will lie close to the boundary of the sample space, where they will be affected by the neighbouring space. The result is that ‘in high dimensions all feasible training samples sparsely populate the input space’ [@hastie_elements_2009, 23]. Because *k-nn* allows for non-linear interactions between features, for instance, small differences in the number of points in particular neighbourhoods can drastically affect the boundaries (as we see in comparing the right and left hand plots). These kinds of topological instability account for the propensity of machine learning treatments of feature-rich genomic data to produce highly variable predictions. We can also see why the MACQ-II teams produced 463,000 *k-nn* models in an effort to normalise and regulate predictive practice.  The price of predictivity in genomics is variation in prediction.
	
## A self-organizing map of genomic states

If predictive models polymorphically proliferate in genomics, what of genomes themselves? Do biological understandings of the genome shift as a result? A final example, this time of a major genomic project, indicates what happens. The  vast consortial structure of the ENCODE project [@encode_2004; @encode_project_consortium_identification_2007; @encode_consortium_integrated_2012]  focused on DNA outside the more well-known protein coding regions of the human genome (the regions so extensively annotated in the millions of records in GenBank for instance). The architecture of this decade-long project ('a landmark in the understanding of the human genome' according to the *Nature* headline in September 2012)  is too complicated to follow here, but machine learning in some form or other figures in a substantial portion of the 30 publications that marked the completion of the initial analysis of the project data. In an overview of the results, the ENCODE authors describe the thousands of datasets and hundreds of cell types they  worked with in seeking to 'delineate all functional elements encoded in the human genome' [@encode_consortium_integrated_2012, 57]. The overview is replete with graphics that nearly all point to large amounts of data processing and statistical computation. Of the 72 discrete visual figures in the leading ENCODE article, only two are somewhat image-based (stained blood vessels in a transgenic mouse embryo; a transgenic medaka fish). The rest are model-derived figures. 

![[@encode_consortium_integrated_2012], figure 7](graphics/encode_2012_fig7.jpg)

Whereas the Human Genome Project sought to develop over three five-year periods three different kinds of maps -- genetic, physical, and sequence [@hood_biology_1992, 137] -- ENCODE presents a different kind of map, a *self-organising map* of genomic states (see Figure 7: ‘High-resolution segmentation of the ENCODE data by self-organizing maps (SOM)’ [@encode_consortium_integrated_2012, 67].  'Segmentation' of the human genome was a major concern in ENCODE. The term, not coincidentally, comes from data mining and marketing research rather than biology. Segmentation refers to a way of sorting ‘data into a small number of discrete states based on the continuous output values’ [@day_unsupervised_2007, 1424]. As [@hastie_elements_2009] describe it:

>Cluster analysis, also called data segmentation, has a variety of goals. All relate to grouping or segmenting a collection of objects into subsets or “clusters,” such that those within each cluster are more closely related to one another than objects assigned to different clusters. An object can be described by a set of measurements, or by its relation to other objects. In addition, the goal is sometimes to arrange the clusters into a natural hierarchy. [@hastie_elements_2009,501]

Segmentation is particularly important to ENCODE because it aims to encyclopaedically describe the functional  elements of the genome. The seven state types predicted by ENCODE, again using sophisticated prediction techniques (e.g. 'Segway': a dynamic Bayesian network modelling tool; see[@hoffman_unsupervised_2012]) refer to functionally differentiated sites on the genome (e.g. state E is an enhancer; state T is a predicted transcribed regions; see Table 3 and Figure 5, [@encode_consortium_integrated_2012] for summary of the states). Genomic segmentation is a major machine learning problem in its own right since it does refer simply to sequences but to patterns of biochemical signal that occur on many different scales, but here I am less interested in the different states attributed to  genome than in the way that the predictive models that segment the genome begin to change what a genome is. Using segmentation as the basis of its description, ENCODE begins to re-conceptualise genomic ‘function’  as segmentation. 

The ‘High resolution segmentation of ENCODE data using self-organizing maps (SOM)’ is a good example of what happens to genomes in machine learning. A self-organizing map (SOM), as [@hastie_elements_2009] tell us, is an unsupervised learning technique in which ‘the prototypes are encouraged to lie in a one- or two-dimensional manifold in the feature space. The resulting manifold is also referred to as a *constrained topological map*, since the original high-dimensional observations can be mapped down onto the two-dimensional coordinate system’ (528). Leaving aside the mathematical and computational details of how this is done, we  see here  dimensional reduction that embeds a high-dimensional manifold in a low dimensional space. Like many other figures in the ENCODE publications, the self-organizing map of genomic states leverages a variety of datasets against each other. This figure usefully points to the recombinant transformations and transpositions of sequence data in genomics as machine learning techniques reshape it. Figure 7 itself comprises 16 sub-figures that display progression from genome segments  (themselves already modelled, as discussed above) to training of the SOM using cell/signal measurements (shown as a toroid), then to illustration of the results of the SOM as applied to some elements of the [Gene Ontology](http://www.geneontology.org/), a major bioinformatics initiative aiming to standardise the naming of genes and gene attributes. 

The ENCODE article displays the self-organising map as  a toroid or doughnut, a manifold that has no edges. This toroidal map is initially randomly populated with all the segments found by the segmentation analysis. The ENCODE authors write, ‘we then trained the map using the signal of the 12 different ChIP-seq and DNA-seq assays in the six cell types analysed’ [@encode_consortium_integrated_2012, 67].  Once the training has been completed, ‘the resulting map can be overlaid with any class of ENCODE or other data to view the distribution of that data within this high-resolution segmentation’ [@encode_consortium_integrated_2012, 67]. They go on to show how other datasets can be overlaid on the map (see the final fold of the ENCODE Figure 7). In a recursive move that I would suggest is typical of the subductive force of machine learning in genomic research, the power of the self-organising  map is  demonstrated by reordering other genomic datasets, such as annotated sets of genes (GENCODE), or gene ontology (GO) datasets. In the final analysis, the toroidal self-organising map is used to again annotate regions on two tracks in a genome browser-style visualizations of DNA. The output of the predictive model becomes a way of reorganising existing maps and catalogs of genomic features:   SOM-annotated regions of the genome can be displayed as tracks in a genome browser. 

## Genomic landscapes: open or settled? 

There are different ways of making sense of the implication of computation in genomic and post-genomic science. We might attend to the long-standing and still operative metaphors of DNA as code, program [@kay_who_2000], communication system or network, to the rich and diverse development of information retrieval and database systems dedicated to retrieving and annotating biological data of many different kinds (the annual database issue of the journal *Nucleic Acids Research*  lists over 400), to the role of various kinds of system models in systems biology ([@szallasi_system_2006]) and dynamic models in biology more generally, to the crucial role played by certain kinds of matching and alignment techniques in  sequencing [@stevens_coding_2011], to the rise of bioinformatics as a bundle of techniques focused on  cross-linking sequences and annotations, or, stretching further afield, to the attempts to reconfigure DNA and RNA sequences as components that function like digital logic in synthetic biology.

In the predictive models of epistasis in GWAS, in standardisation of the predictions of clinical outcomes using *k-nn* models in MACQ-II, and in the self-organizing map of genomic states in ENCODE, we glimpse a set of transformations that are changing the shape of genomic data,  and thereby how genomes matter and what they mean. These examples, while by no means trivial, are inevitably quite limited in their scope. Yet they are not isolated. There are many other such examples. We could look at the large body of work that seeks to elicit sub-types in relation to  diseases, disorder or traits from populations (for example, [@watson_recombinant_2007,3]). We could examine the processes of imputation of genotypes that play a key role in firming up the statistical power of GWAS to identify associations (for example, ([@burton_genome-wide_2007])). 

Machine learning widely affects epistemic infrastructures, techniques and scientific knowledges. In the biosciences,  it touches on those aspects of genomes — their variation, their manifold spatial and temporal relationality in biological processes — that seem most distant and difficult to derive from the relatively stable, monolithic  and hence  tractable forms of order found in DNA sequences.  As we have seen in the various data excerpts above, that order is largely linear. DNA can be laid down in tracks, aligned and annotated, even though it undergoes subtle, pervasive and transient forms of temporal and spatial re-shaping in life-forms.  In the feature-rich spaces countenanced by machine learning, we see attempts to embed manifolds in local regions, local linearities. Sometimes these local regions are regions of annotated DNA, as in the ENCODE examples, or non-linear interactions between sets of genes, as in the GWAS analysis of epistasis. At other times, these local regions are forms of life in a more general sense — clinical outcomes or diagnostic tests — as in MACQ-II.

I have emphasised on several occasions the common thread of what could be the termed the ‘genomic curse of dimensionality.’ The genomic sciences are based around the extraction, sorting and ordering of  DNA sequences. Machine learning practices, I have suggested,  begin to construct different dimensional spaces amidst the sequences, lines and tracks of genomes. What perhaps most vexes and animates post-genomic science is the desire to separate out from the DNA sequences variations that matter to both life-forms and forms of life. This vexation generates strenuous infrastructural, technical and conceptual attempts to reorganise and re-conceptualise what it is to know a genome. The machine learning techniques I have discussed  have begun to transform  bioinformatics and biostatistics. To understand machine learning as either as intensification of statistics, as hybridization of bioinformatics and statistics, or as the product of a reciprocal interactive convergence of biology and computing misses some of the important features of the reorganisation of genomics associated with these practices. It misses what machine learning brings from elsewhere to genomics, and it overlooks how the genome itself serves as a showcase, as we saw in the Google I/O demonstration,  of certain much wider transformations in the practices of anticipation and prediction. 

What is at stake in machine learning for post-genomic science or practices of knowing and predicting more generally? Could we pose or address any normative questions by becoming aware of and articulating machine learning in practice  with greater clarity? Genomic science, in what it borrows from and in how it is affected from machine learning, displays some of the tendencies to reduce divergences and to corral differences typical of knowledge economies more generally. The philosopher of science, Isabelle Stengers writes:

>with the knowledge economy, we may have scientists at work everywhere, producing facts with the speed that new sophisticated instruments make possible, but that the way those facts are interpreted will now mostly follow the landscape of settled interests. … We will more and more deal with instrumental knowledge. (Stengers 2011:377)

As we see in the 600,000 cores of Google Compute applied to exploration of associations in cancer genomics using random forests, or the lasso applied to microarray SNP data,  machine learning rapidly produces facts.  Stengers suggests that the risk here is that divergence and unexpected forms of experimental result are somewhat diminished as a result. Machine-learning in genomics might produce a ‘self-organising map’ that poses questions following the 'landscape of settled interests' or *status quo*.  

The very justification for using such techniques is the inordinate difficulty of exploring the many dimensions of genomes otherwise. I have already hinted what I think is one core stake in these shifts. Genomes and genomics are touchstones for wider transformations in many sectors of science, industry, commerce, media and government susceptible to the imperatives of the knowledge economy. In contrast to some of these domains, where much that happens is obscured from view, the great virtue or genomic science is the relative openness of its workings and its dogged insistence on DNA as the generating set. The fact that data practices are relatively generic and accessible means that critical research into transformations associated with data and knowledge economies can accompany nearly every aspect of genomic practice. This is a forensic good. 


## References
