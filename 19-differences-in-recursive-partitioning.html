<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Machine Learning: Archaeology of a Data Practice</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="Machine Learning: Archaeology of a Data Practice">
  <meta name="generator" content="bookdown 0.3 and GitBook 2.6.7">

  <meta property="og:title" content="Machine Learning: Archaeology of a Data Practice" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Machine Learning: Archaeology of a Data Practice" />
  
  
  

<meta name="author" content="Adrian Mackenzie">


<meta name="date" content="2016-12-10">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="18-splitting-and-the-growth-of-trees.html">
<link rel="next" href="20-limiting-differences.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="1-acknowledgments.html"><a href="1-acknowledgments.html"><i class="fa fa-check"></i><b>1</b> Acknowledgments</a></li>
<li class="chapter" data-level="2" data-path="2-preface.html"><a href="2-preface.html"><i class="fa fa-check"></i><b>2</b> Preface</a></li>
<li class="chapter" data-level="3" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html"><i class="fa fa-check"></i><b>3</b> Introduction: Into the Data</a><ul>
<li class="chapter" data-level="3.1" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#three-accumulations-settings-data-and-devices"><i class="fa fa-check"></i><b>3.1</b> Three accumulations: settings, data and devices</a></li>
<li class="chapter" data-level="3.2" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#who-or-what-is-a-machine-learner"><i class="fa fa-check"></i><b>3.2</b> Who or what is a machine learner?</a></li>
<li class="chapter" data-level="3.3" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#algorithmic-control-to-the-machine-learners"><i class="fa fa-check"></i><b>3.3</b> Algorithmic control to the machine learners?</a></li>
<li class="chapter" data-level="3.4" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#the-archaeology-of-operations"><i class="fa fa-check"></i><b>3.4</b> The archaeology of operations</a></li>
<li class="chapter" data-level="3.5" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#asymmetries-in-common-knowledge"><i class="fa fa-check"></i><b>3.5</b> Asymmetries in common knowledge</a></li>
<li class="chapter" data-level="3.6" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#what-cannot-be-automated"><i class="fa fa-check"></i><b>3.6</b> What cannot be automated?</a></li>
<li class="chapter" data-level="3.7" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#different-fields-in-machine-learning"><i class="fa fa-check"></i><b>3.7</b> Different fields in machine learning?</a></li>
<li class="chapter" data-level="3.8" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#the-diagram-in-critical-thought"><i class="fa fa-check"></i><b>3.8</b> The diagram in critical thought</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html"><i class="fa fa-check"></i><b>4</b> Diagramming machines}</a><ul>
<li class="chapter" data-level="4.1" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#we-dont-have-to-write-programs"><i class="fa fa-check"></i><b>4.1</b> ‘We don’t have to write programs’?</a></li>
<li class="chapter" data-level="4.2" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#the-elements-of-machine-learning"><i class="fa fa-check"></i><b>4.2</b> The elements of machine learning</a></li>
<li class="chapter" data-level="4.3" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#who-reads-machine-learning-textbooks"><i class="fa fa-check"></i><b>4.3</b> Who reads machine learning textbooks?</a></li>
<li class="chapter" data-level="4.4" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#r-a-matrix-of-transformations"><i class="fa fa-check"></i><b>4.4</b> <code>R</code>: a matrix of transformations</a></li>
<li class="chapter" data-level="4.5" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#the-obdurate-mathematical-glint-of-machine-learning"><i class="fa fa-check"></i><b>4.5</b> The obdurate mathematical glint of machine learning</a></li>
<li class="chapter" data-level="4.6" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#cs229-2007-returning-again-and-again-to-certain-features"><i class="fa fa-check"></i><b>4.6</b> CS229, 2007: returning again and again to certain features</a></li>
<li class="chapter" data-level="4.7" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#the-visible-learning-of-machine-learning"><i class="fa fa-check"></i><b>4.7</b> The visible learning of machine learning</a></li>
<li class="chapter" data-level="4.8" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#the-diagram-of-an-operational-formation"><i class="fa fa-check"></i><b>4.8</b> The diagram of an operational formation</a></li>
<li class="chapter" data-level="4.9" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#vectorisation-and-its-consequences"><i class="fa fa-check"></i><b>4.9</b> Vectorisation and its consequences}</a></li>
<li class="chapter" data-level="4.10" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#vector-space-and-geometry"><i class="fa fa-check"></i><b>4.10</b> Vector space and geometry</a></li>
<li class="chapter" data-level="4.11" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#mixing-places"><i class="fa fa-check"></i><b>4.11</b> Mixing places</a></li>
<li class="chapter" data-level="4.12" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#truth-is-no-longer-in-the-table"><i class="fa fa-check"></i><b>4.12</b> Truth is no longer in the table?</a></li>
<li class="chapter" data-level="4.13" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#the-epistopic-fault-line-in-tables"><i class="fa fa-check"></i><b>4.13</b> The epistopic fault line in tables</a></li>
<li class="chapter" data-level="4.14" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#surface-and-depths-the-problem-of-volume-in-data"><i class="fa fa-check"></i><b>4.14</b> Surface and depths: the problem of volume in data</a></li>
<li class="chapter" data-level="4.15" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#vector-space-expansion"><i class="fa fa-check"></i><b>4.15</b> Vector space expansion</a></li>
<li class="chapter" data-level="4.16" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#drawing-lines-in-a-common-space-of-transformation"><i class="fa fa-check"></i><b>4.16</b> Drawing lines in a common space of transformation</a></li>
<li class="chapter" data-level="4.17" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#implicit-vectorization-in-code-and-infrastructures"><i class="fa fa-check"></i><b>4.17</b> Implicit vectorization in code and infrastructures</a></li>
<li class="chapter" data-level="4.18" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#lines-traversing-behind-the-light"><i class="fa fa-check"></i><b>4.18</b> Lines traversing behind the light</a></li>
<li class="chapter" data-level="4.19" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#the-vectorised-table"><i class="fa fa-check"></i><b>4.19</b> The vectorised table?</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-machines-finding-functions.html"><a href="5-machines-finding-functions.html"><i class="fa fa-check"></i><b>5</b> Machines finding functions}</a><ul>
<li class="chapter" data-level="5.1" data-path="5-machines-finding-functions.html"><a href="5-machines-finding-functions.html#learning-functions"><i class="fa fa-check"></i><b>5.1</b> Learning functions</a></li>
<li class="chapter" data-level="5.2" data-path="5-machines-finding-functions.html"><a href="5-machines-finding-functions.html#supervised-unsupervised-reinforcement-learning-and-functions"><i class="fa fa-check"></i><b>5.2</b> Supervised, unsupervised, reinforcement learning and functions</a></li>
<li class="chapter" data-level="5.3" data-path="5-machines-finding-functions.html"><a href="5-machines-finding-functions.html#which-function-operates"><i class="fa fa-check"></i><b>5.3</b> Which function operates?</a></li>
<li class="chapter" data-level="5.4" data-path="5-machines-finding-functions.html"><a href="5-machines-finding-functions.html#what-does-a-function-learn"><i class="fa fa-check"></i><b>5.4</b> What does a function learn?</a></li>
<li class="chapter" data-level="5.5" data-path="5-machines-finding-functions.html"><a href="5-machines-finding-functions.html#observing-with-curves-the-logistic-function"><i class="fa fa-check"></i><b>5.5</b> Observing with curves: the logistic function</a></li>
<li class="chapter" data-level="5.6" data-path="5-machines-finding-functions.html"><a href="5-machines-finding-functions.html#the-cost-of-curves-in-machine-learning"><i class="fa fa-check"></i><b>5.6</b> The cost of curves in machine learning</a></li>
<li class="chapter" data-level="5.7" data-path="5-machines-finding-functions.html"><a href="5-machines-finding-functions.html#curves-and-the-variation-in-models"><i class="fa fa-check"></i><b>5.7</b> Curves and the variation in models</a></li>
<li class="chapter" data-level="5.8" data-path="5-machines-finding-functions.html"><a href="5-machines-finding-functions.html#observing-costs-losses-and-objectives-through-optimisation"><i class="fa fa-check"></i><b>5.8</b> Observing costs, losses and objectives through optimisation</a></li>
<li class="chapter" data-level="5.9" data-path="5-machines-finding-functions.html"><a href="5-machines-finding-functions.html#gradients-as-partial-observers"><i class="fa fa-check"></i><b>5.9</b> Gradients as partial observers</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-the-power-to-learn.html"><a href="6-the-power-to-learn.html"><i class="fa fa-check"></i><b>6</b> The power to learn</a></li>
<li class="chapter" data-level="7" data-path="7-probabilisation-and-the-taming-of-machines.html"><a href="7-probabilisation-and-the-taming-of-machines.html"><i class="fa fa-check"></i><b>7</b> Probabilisation and the Taming of Machines}</a></li>
<li class="chapter" data-level="8" data-path="8-data-reduces-uncertainty.html"><a href="8-data-reduces-uncertainty.html"><i class="fa fa-check"></i><b>8</b> Data reduces uncertainty?</a></li>
<li class="chapter" data-level="9" data-path="9-machine-learning-as-statistics-inside-out.html"><a href="9-machine-learning-as-statistics-inside-out.html"><i class="fa fa-check"></i><b>9</b> Machine learning as statistics inside out</a></li>
<li class="chapter" data-level="10" data-path="10-distributed-probabilities.html"><a href="10-distributed-probabilities.html"><i class="fa fa-check"></i><b>10</b> Distributed probabilities</a></li>
<li class="chapter" data-level="11" data-path="11-naive-bayes-and-the-distribution-of-probabilities.html"><a href="11-naive-bayes-and-the-distribution-of-probabilities.html"><i class="fa fa-check"></i><b>11</b> Naive Bayes and the distribution of probabilities</a></li>
<li class="chapter" data-level="12" data-path="12-spam-when-foralln-is-too-much.html"><a href="12-spam-when-foralln-is-too-much.html"><i class="fa fa-check"></i><b>12</b> Spam: when <span class="math inline">\(\forall{N}\)</span> is too much?</a></li>
<li class="chapter" data-level="13" data-path="13-the-improbable-success-of-the-naive-bayes-classifier.html"><a href="13-the-improbable-success-of-the-naive-bayes-classifier.html"><i class="fa fa-check"></i><b>13</b> The improbable success of the Naive Bayes classifier</a></li>
<li class="chapter" data-level="14" data-path="14-ancestral-probabilities-in-documents-inference-and-prediction.html"><a href="14-ancestral-probabilities-in-documents-inference-and-prediction.html"><i class="fa fa-check"></i><b>14</b> Ancestral probabilities in documents: inference and prediction</a></li>
<li class="chapter" data-level="15" data-path="15-statistical-decompositions-bias-variance-and-observed-errors.html"><a href="15-statistical-decompositions-bias-variance-and-observed-errors.html"><i class="fa fa-check"></i><b>15</b> Statistical decompositions: bias, variance and observed errors</a></li>
<li class="chapter" data-level="16" data-path="16-does-machine-learning-construct-a-new-statistical-reality.html"><a href="16-does-machine-learning-construct-a-new-statistical-reality.html"><i class="fa fa-check"></i><b>16</b> Does machine learning construct a new statistical reality?</a></li>
<li class="chapter" data-level="17" data-path="17-patterns-and-differences.html"><a href="17-patterns-and-differences.html"><i class="fa fa-check"></i><b>17</b> Patterns and differences</a></li>
<li class="chapter" data-level="18" data-path="18-splitting-and-the-growth-of-trees.html"><a href="18-splitting-and-the-growth-of-trees.html"><i class="fa fa-check"></i><b>18</b> Splitting and the growth of trees</a></li>
<li class="chapter" data-level="19" data-path="19-differences-in-recursive-partitioning.html"><a href="19-differences-in-recursive-partitioning.html"><i class="fa fa-check"></i><b>19</b> 1984: Differences in recursive partitioning</a></li>
<li class="chapter" data-level="20" data-path="20-limiting-differences.html"><a href="20-limiting-differences.html"><i class="fa fa-check"></i><b>20</b> Limiting differences</a></li>
<li class="chapter" data-level="21" data-path="21-the-successful-dispersion-of-the-support-vector-machine.html"><a href="21-the-successful-dispersion-of-the-support-vector-machine.html"><i class="fa fa-check"></i><b>21</b> The successful dispersion of the support vector machine</a></li>
<li class="chapter" data-level="22" data-path="22-differences-blur.html"><a href="22-differences-blur.html"><i class="fa fa-check"></i><b>22</b> Differences blur?</a></li>
<li class="chapter" data-level="23" data-path="23-bending-the-decision-boundary.html"><a href="23-bending-the-decision-boundary.html"><i class="fa fa-check"></i><b>23</b> Bending the decision boundary</a></li>
<li class="chapter" data-level="24" data-path="24-instituting-patterns.html"><a href="24-instituting-patterns.html"><i class="fa fa-check"></i><b>24</b> Instituting patterns</a></li>
<li class="chapter" data-level="25" data-path="25-regularizing-and-materializing-objects.html"><a href="25-regularizing-and-materializing-objects.html"><i class="fa fa-check"></i><b>25</b> Regularizing and materializing objects}</a></li>
<li class="chapter" data-level="26" data-path="26-genomic-referentiality-and-materiality.html"><a href="26-genomic-referentiality-and-materiality.html"><i class="fa fa-check"></i><b>26</b> Genomic referentiality and materiality</a></li>
<li class="chapter" data-level="27" data-path="27-the-genome-as-threshold-object.html"><a href="27-the-genome-as-threshold-object.html"><i class="fa fa-check"></i><b>27</b> The genome as threshold object</a></li>
<li class="chapter" data-level="28" data-path="28-genomic-knowledges-and-their-datasets.html"><a href="28-genomic-knowledges-and-their-datasets.html"><i class="fa fa-check"></i><b>28</b> Genomic knowledges and their datasets</a></li>
<li class="chapter" data-level="29" data-path="29-the-advent-of-wide-dirty-and-mixed-data.html"><a href="29-the-advent-of-wide-dirty-and-mixed-data.html"><i class="fa fa-check"></i><b>29</b> The advent of ‘wide, dirty and mixed’ data</a></li>
<li class="chapter" data-level="30" data-path="30-cross-validating-machine-learning-in-genomics.html"><a href="30-cross-validating-machine-learning-in-genomics.html"><i class="fa fa-check"></i><b>30</b> Cross-validating machine learning in genomics</a></li>
<li class="chapter" data-level="31" data-path="31-proliferation-of-discoveries.html"><a href="31-proliferation-of-discoveries.html"><i class="fa fa-check"></i><b>31</b> Proliferation of discoveries</a></li>
<li class="chapter" data-level="32" data-path="32-variations-in-the-object-or-in-the-machine-learner.html"><a href="32-variations-in-the-object-or-in-the-machine-learner.html"><i class="fa fa-check"></i><b>32</b> Variations in the object or in the machine learner?</a></li>
<li class="chapter" data-level="33" data-path="33-whole-genome-functions.html"><a href="33-whole-genome-functions.html"><i class="fa fa-check"></i><b>33</b> Whole genome functions</a></li>
<li class="chapter" data-level="34" data-path="34-propagating-subject-positions.html"><a href="34-propagating-subject-positions.html"><i class="fa fa-check"></i><b>34</b> Propagating subject positions}</a></li>
<li class="chapter" data-level="35" data-path="35-propagation-across-human-machine-boundaries.html"><a href="35-propagation-across-human-machine-boundaries.html"><i class="fa fa-check"></i><b>35</b> Propagation across human-machine boundaries</a></li>
<li class="chapter" data-level="36" data-path="36-competitive-positioning.html"><a href="36-competitive-positioning.html"><i class="fa fa-check"></i><b>36</b> Competitive positioning</a></li>
<li class="chapter" data-level="37" data-path="37-a-privileged-machine-and-its-diagrammatic-forms.html"><a href="37-a-privileged-machine-and-its-diagrammatic-forms.html"><i class="fa fa-check"></i><b>37</b> A privileged machine and its diagrammatic forms</a></li>
<li class="chapter" data-level="38" data-path="38-varying-subject-positions-in-code.html"><a href="38-varying-subject-positions-in-code.html"><i class="fa fa-check"></i><b>38</b> Varying subject positions in code</a></li>
<li class="chapter" data-level="39" data-path="39-the-subjects-of-a-hidden-operation.html"><a href="39-the-subjects-of-a-hidden-operation.html"><i class="fa fa-check"></i><b>39</b> The subjects of a hidden operation</a></li>
<li class="chapter" data-level="40" data-path="40-algorithms-that-propagate-errors.html"><a href="40-algorithms-that-propagate-errors.html"><i class="fa fa-check"></i><b>40</b> Algorithms that propagate errors</a></li>
<li class="chapter" data-level="41" data-path="41-competitions-as-examination.html"><a href="41-competitions-as-examination.html"><i class="fa fa-check"></i><b>41</b> Competitions as examination</a></li>
<li class="chapter" data-level="42" data-path="42-superimposing-power-and-knowledge.html"><a href="42-superimposing-power-and-knowledge.html"><i class="fa fa-check"></i><b>42</b> Superimposing power and knowledge</a></li>
<li class="chapter" data-level="43" data-path="43-ranked-subject-positions.html"><a href="43-ranked-subject-positions.html"><i class="fa fa-check"></i><b>43</b> Ranked subject positions</a></li>
<li class="chapter" data-level="44" data-path="44-conclusion-out-of-the-data.html"><a href="44-conclusion-out-of-the-data.html"><i class="fa fa-check"></i><b>44</b> Conclusion: Out of the Data}</a></li>
<li class="chapter" data-level="45" data-path="45-machine-learners.html"><a href="45-machine-learners.html"><i class="fa fa-check"></i><b>45</b> 250,000 machine learners</a></li>
<li class="chapter" data-level="46" data-path="46-a-summary-of-the-argument.html"><a href="46-a-summary-of-the-argument.html"><i class="fa fa-check"></i><b>46</b> A summary of the argument</a></li>
<li class="chapter" data-level="47" data-path="47-in-situ-hybridization.html"><a href="47-in-situ-hybridization.html"><i class="fa fa-check"></i><b>47</b> In-situ hybridization</a></li>
<li class="chapter" data-level="48" data-path="48-critical-operational-practice.html"><a href="48-critical-operational-practice.html"><i class="fa fa-check"></i><b>48</b> Critical operational practice?</a></li>
<li class="chapter" data-level="49" data-path="49-obstacles-to-the-work-of-freeing-machine-learning.html"><a href="49-obstacles-to-the-work-of-freeing-machine-learning.html"><i class="fa fa-check"></i><b>49</b> Obstacles to the work of freeing machine learning</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning: Archaeology of a Data Practice</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="differences-in-recursive-partitioning" class="section level1">
<h1><span class="header-section-number">19</span> 1984: Differences in recursive partitioning</h1>
<p>As Einhorn expected, it was too much to hope that all researchers would take time to investigate how a particular program works. Some researchers however did take time in the following decade to investigate how decision trees work. Writing around 2000, Hastie, Tibshirani and Friedman, who could hardly by accused of not understanding decision trees, happily recommend decision trees as the best ‘off-the-shelf’ classifier: ‘of all the well-known learning methods, decision trees comes closest to meeting the requirements for serving as an off-the-shelf procedure for data-mining’ <span class="citation">[@Hastie_2009, 352]</span>. We might wonder here, however, whether they damn with faint praise, since ‘off-the-shelf’ suggests pre-packaged, and commodified, and the term ‘data-mining’ itself is not without negative connotations. As for its commercial realities, in 2013, Salford Systems, the purveyors of the leading contemporary commercial decision tree software, <code>CART</code>, could claim:</p>
<blockquote>
<p>CART is the ultimate classification tree that has revolutionized the entire field of advanced analytics and inaugurated the current era of data mining. CART, which is continually being improved, is one of the most important tools in modern data mining. Others have tried to copy CART but no one has succeeded as evidenced by unmatched accuracy, performance, feature set, built-in automation and ease of use. <a href="http://www.salford-systems.com/products/cart">Salford Systems</a> </p>
</blockquote>
<p>What happened between 1973 and 2013? Decision trees somehow stepped out of the statistically murky waters of social science departments and business schools in the early 1970s to inaugurate the ‘current era of data mining’ (which the scientific literature indicates starts in the early 1990s).  This was not only a commercial innovation. As the earlier citation from U.S. National Institutes of Health biostatisticians Malley, Malley and Pajevic indicates, decision trees enjoy high regard even in biomedical research, a setting where statistical rigour is highly valued for life and death reasons. The happy situation of decision trees four decades on suggests some kind of threshold was crossed in which the epistemological, statistical, or algorithmic (‘built-in automation’) power of the technique altered substantially. </p>
<p>The third author of <em>Elements of Statistical Learning</em>, Jerome Friedman, worked at the U.S. Department of Energy’s Stanford Linear Accelerator during the late 1970s. Friedman was instrumental in rescuing decision trees from the ignominy of profligate ease of use and pure empiricism they had endured since the late 1960s.  The reorganisation and statistical retrofitting of the decision tree was not a single or focused effort. During the 1980s, statisticians such as Friedman and Leo Breiman renovated the decision tree as a statistical tool <span class="citation">[@Breiman_1984]</span>. At the same time, computer scientists such as Ross Quinlan in Sydney were re-implementing decision trees guided by an artificial intelligence-based formalisation as rule-based induction technique <span class="citation">[@Quinlan_1986]</span>.<a href="#fn68" class="footnoteRef" id="fnref68"><sup>68</sup></a>   This uneasy parallel effort between computer science and statistics still somewhat strains relations in machine learning today. Statisticians and computer scientists do and use the same techniques, but often with the computer scientists focusing on optimisation and algorithmic scale and the statisticians inventing novel statistical formalizations and abstractions. The fateful embrace of statistics and computer science, the disciplinary binary that vectorizes machine learning, has been generative in the retrieval of the decision tree. </p>
<p>An initial symptom of the transformation of the technique appears in a name change. The term ‘decision tree,’ although still widely used in the research literature and machine learner parlance was supplanted by ‘classification and regression tree’ during the late 1970s and 1980s. The terms ‘classification and regression tree’ is sometimes contracted to ‘CART,’ and that term strictly speaking refers to a computer program described in <span class="citation">[@Breiman_1984]</span> as well as the title of that highly-cited monograph, <em>Classification and Regression Trees</em>.   As we have seen in previous chapters, classification and regression (predictive modelling using estimates of relations between variables)  stage the two main sides of machine learning practice. Their concatenation with ‘tree’ attests to a renovation of existing machine learning approaches behind a single facade.</p>
<p>The implementation of machine learning techniques in <code>R</code> accentuates the statistical side of decision tree practice, but that has certain forensic virtues not offered by commercial or closed-source software often produced by computer scientists. The name of one long-standing and widely-used <code>R</code> package itself attests to something: <code>rpart</code> is a contraction of ‘recursive partitioning’ and this term generally describes how the decision tree algorithm works to partition the vector space into the form shown in Figure  <span class="citation">[@Therneau_2015]</span>.  ‘CART,’ on the other hand, is a registered trademark of Salford Systems, the software company mentioned above, who sell the leading commercial implementation of classification and regression trees. Hence, the <code>R</code> package <code>rpart</code> cannot call itself the more obvious name <code>cart,</code> and instead invokes the underlying algorithmic process: recursive partitioning.<a href="#fn69" class="footnoteRef" id="fnref69"><sup>69</sup></a>  </p>

<p>R.A. Fisher’s <code>iris</code> dataset, which contains 150 measurements made in the 1930s of petal and sepal lengths of <em>iris virginica, iris setosa</em> and <em>iris versicolor</em> is a standard instructional example for decision trees <span class="citation">[@Fisher_1938]</span>.<a href="#fn70" class="footnoteRef" id="fnref70"><sup>70</sup></a>  The code shown here loads the <code>iris</code> data (the dataset is routinely installed with many data analysis tools), loads the <code>rpart</code> decision tree library, and builds a decision to classify the irises by species. What has happened to the iris data in this decision tree? The <code>R</code> code that invokes the recursive partitioning algorithm is so brief <code>iris_tree =rpart(Species ~ ., iris)</code> that we can’t tell much about how the data has been ‘recursively partitioned.’ We know that the <em>iris</em> has 150 rows, and that there are equal numbers of the three iris varieties.</p>
<p>Code brevity indicates a great deal of formalization of practice has accrued around decision trees.  Some of this formalization was described in the landmark <em>Classification and Regression Trees</em> monograph <span class="citation">[@Breiman_1984]</span>. Classification in decision trees operates by splitting each of the dimensions of vector space into two parts (as we saw in figure ). These splits institute branches along which differences are hierarchically ordered in a tree structure. The recursive splitting algorithm draws a diagram of hierarchical differences. The problem here is that many splits are possible. What is a good split or ordering of differences? </p>
<blockquote>
<p>The first problem in tree construction is how to use <span class="math inline">\(\mathcal{L}\)</span> to determine the binary splits of <span class="math inline">\(\mathcal{X}\)</span> into smaller pieces. The fundamental idea is to select each split of a subset so that the data in each of the descendant subsets are “purer” than the data in the parent subset <span class="citation">[@Breiman_1984, 23]</span>.</p>
</blockquote>
<p>Tree construction hinges on the notion of purity or more precisely ‘node impurity’, a function that measures to what extent data labelled as belonging to different classes are mixed together at a given branch or node in a decision tree: ‘that is, the node impurity is largest when all classes are equally mixed together in it, and smallest when the node contains only one class’ <span class="citation">[@Breiman_1984, 24]</span>. As Malley and co-authors note, ‘the collection of purity measures is still a subject of research’ <span class="citation">[@Malley_2011, 123]</span>, but Breiman, Friedman, Olshen and Stone promoted a particular form of impurity measure for classification trees known as ‘Gini index of diversity’ <span class="citation">[@Breiman_1984, 38]</span>.  Like the planar decision surface used in classifiers such as the perceptron or linear regression model, recursive partitioning combined with measures of node impurity transforms data by cuts or divides. Whereas in linear model-based machine learners, the intuition motivating the function-finding or learning was ‘find the line that best expresses the distribution of the data,  here the intuition is more like: ’find the cuts that minimize mixing’. Good splits decrease the level of impurity in the tree. In a tree with maximum purity, each terminal node – the nodes at the base of the tree – would contain a single class.</p>

<p>In Figure , the plot on the left shows the decision tree and the plot on the right shows just <em>setosa</em> and <em>versicolor</em> plotted by petal and sepal widths and lengths. Decision trees are read from the top down, left to right. The top level of this tree can be read, for instance, as saying, if the length of petal is less 2.45, then the iris is <em>setosa</em>.  As the plot on the right shows, most of the measurements are well clustered. Only the <em>setosa</em> petal lengths and widths seem to vary widely. All the other measurements are tightly bunched. A decision tree has little trouble ordering differences between species of iris.</p>
<p>Like logistic regression models, neural networks, support vector machines or any other machine learning technique, decision trees order differences in terms of specific qualities and logics. Recursive partitioning split and sub-divide the vector space to capture every minor difference between cases, and thereby achieve a ever-closer fit to the individual or sub-individual variations.  Although the partitioning or splitting rules have strong statistical justifications, they do not at all eliminate the problem of instability or variance in trees.  For instance, they easily end up ‘overfitting’ the data. Overfitting is a problem for all machine learning techniques. Algorithms sometimes find it hard to know when to stop identifying differences. During construction of a decision tree, recursive partitioning splits features in the data into smaller and smaller groups. ‘The goodness of the split’, wrote Breiman and co-authors, ‘is defined to be the decrease in impurity’ <span class="citation">[@Breiman_1984, 25]</span>. Under this definition of goodness, the terminal nodes or leaves of the tree can end up containing a single case, or a single class of cases. </p>
<p>The decision tree targets the differences of the individual case to such a degree that it could end up seeing categorical differences everywhere. Operating to maximise the purity of the partitions it creates, it leans too heavily on data it has been trained on to see relevant similarities when fresh data appears. Trees that branch too much are sensitive to differences and generalize poorly (that is, they suffer from generalization error \index{error!generalization}). Such a model will almost always <em>overfit</em>, since slight variations in the values of variables in a fresh case are likely to yield widely differing predictions. In the terminology of machine learning, such a decision tree may have low bias but high variance.  </p>
</div>
<div class="footnotes">
<hr />
<ol start="68">
<li id="fn68"><p>Quinlan’s papers and book on versions of the decision tree (<code>ID3</code> and <code>c4.5</code>) are both amongst the top ten the most highly cited references in the machine literature itself. Google Scholar reports over 20,00 citations of the Quinlan’s book <em>C4.5: Programs for Machine Learning</em> <span class="citation">[@Quinlan_1993]</span> (although far fewer appear in Thomson Reuters Web of Science). Several years ago, <code>C4.5</code> was voted the top data mining algorithm <span class="citation">[@Wu_2008]</span>. While I don’t discuss Quinlan’s work in much detail here, we should note as a computer scientist, Quinlan takes a much more rule-based approach to decision tree that Breiman and co-authors.  <a href="19-differences-in-recursive-partitioning.html#fnref68">↩</a></p></li>
<li id="fn69"><p>Other <code>R</code> packages such as <code>party</code> <span class="citation">[@Hothorn_2006]</span> and <code>tree</code> <span class="citation">[@Ripley_2014]</span> also use recursive partitioning, but with various tweaks and optimisations that I leave aside here.  <a href="19-differences-in-recursive-partitioning.html#fnref69">↩</a></p></li>
<li id="fn70"><p><code>iris</code> is a very small dataset, a pre-computational miniature.  That diminutive character makes it diagrammatically mobile. It supports a rhizomatic ecosystem of examples scattered across the machine learning literature. The usual framing of the classification problem is how to decide whether a given iris blossom is of the species <em>virginica</em>, <em>setosa</em> or _versicolor. These irises don’t grow in forests – they are more often found in riverbanks and meadows – but they do offer a variety of illustrations of how machine learning classifiers are brought to bear on classification problems. Here the classification problem is taxonomic - the <em>iris</em> genus has various sub-genera, and sections within the sub-genera.Setosa, <em>virginica</em> and <em>versicolor</em> all belong to the sub-genus <em>Limniris</em>. This botanical context is routinely ignored in machine learning applications. In machine learning textbooks and tutorials, <code>iris</code> typically would be used to demonstrate how cleanly a classifier can separate the different kinds of irises. <a href="19-differences-in-recursive-partitioning.html#fnref70">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="18-splitting-and-the-growth-of-trees.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="20-limiting-differences.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
