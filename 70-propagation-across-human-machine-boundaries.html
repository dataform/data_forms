<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Machine Learning: Archaeology of a Data Practice</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="Machine Learning: Archaeology of a Data Practice">
  <meta name="generator" content="bookdown 0.3 and GitBook 2.6.7">

  <meta property="og:title" content="Machine Learning: Archaeology of a Data Practice" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Machine Learning: Archaeology of a Data Practice" />
  
  
  

<meta name="author" content="Adrian Mackenzie">


<meta name="date" content="2016-12-10">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="69-propagating-subject-positions.html">
<link rel="next" href="71-competitive-positioning.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="1-acknowledgments.html"><a href="1-acknowledgments.html"><i class="fa fa-check"></i><b>1</b> Acknowledgments</a></li>
<li class="chapter" data-level="2" data-path="2-preface.html"><a href="2-preface.html"><i class="fa fa-check"></i><b>2</b> Preface</a></li>
<li class="chapter" data-level="3" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html"><i class="fa fa-check"></i><b>3</b> Introduction: Into the Data</a></li>
<li class="chapter" data-level="4" data-path="4-three-accumulations-settings-data-and-devices.html"><a href="4-three-accumulations-settings-data-and-devices.html"><i class="fa fa-check"></i><b>4</b> Three accumulations: settings, data and devices</a></li>
<li class="chapter" data-level="5" data-path="5-who-or-what-is-a-machine-learner.html"><a href="5-who-or-what-is-a-machine-learner.html"><i class="fa fa-check"></i><b>5</b> Who or what is a machine learner?</a></li>
<li class="chapter" data-level="6" data-path="6-algorithmic-control-to-the-machine-learners.html"><a href="6-algorithmic-control-to-the-machine-learners.html"><i class="fa fa-check"></i><b>6</b> Algorithmic control to the machine learners?</a></li>
<li class="chapter" data-level="7" data-path="7-the-archaeology-of-operations.html"><a href="7-the-archaeology-of-operations.html"><i class="fa fa-check"></i><b>7</b> The archaeology of operations</a></li>
<li class="chapter" data-level="8" data-path="8-asymmetries-in-common-knowledge.html"><a href="8-asymmetries-in-common-knowledge.html"><i class="fa fa-check"></i><b>8</b> Asymmetries in common knowledge</a></li>
<li class="chapter" data-level="9" data-path="9-what-cannot-be-automated.html"><a href="9-what-cannot-be-automated.html"><i class="fa fa-check"></i><b>9</b> What cannot be automated?</a></li>
<li class="chapter" data-level="10" data-path="10-different-fields-in-machine-learning.html"><a href="10-different-fields-in-machine-learning.html"><i class="fa fa-check"></i><b>10</b> Different fields in machine learning?</a></li>
<li class="chapter" data-level="11" data-path="11-the-diagram-in-critical-thought.html"><a href="11-the-diagram-in-critical-thought.html"><i class="fa fa-check"></i><b>11</b> The diagram in critical thought</a></li>
<li class="chapter" data-level="12" data-path="12-we-dont-have-to-write-programs.html"><a href="12-we-dont-have-to-write-programs.html"><i class="fa fa-check"></i><b>12</b> ‘We don’t have to write programs’?</a></li>
<li class="chapter" data-level="13" data-path="13-the-elements-of-machine-learning.html"><a href="13-the-elements-of-machine-learning.html"><i class="fa fa-check"></i><b>13</b> The elements of machine learning</a></li>
<li class="chapter" data-level="14" data-path="14-who-reads-machine-learning-textbooks.html"><a href="14-who-reads-machine-learning-textbooks.html"><i class="fa fa-check"></i><b>14</b> Who reads machine learning textbooks?</a></li>
<li class="chapter" data-level="15" data-path="15-r-a-matrix-of-transformations.html"><a href="15-r-a-matrix-of-transformations.html"><i class="fa fa-check"></i><b>15</b> <code>R</code>: a matrix of transformations</a></li>
<li class="chapter" data-level="16" data-path="16-the-obdurate-mathematical-glint-of-machine-learning.html"><a href="16-the-obdurate-mathematical-glint-of-machine-learning.html"><i class="fa fa-check"></i><b>16</b> The obdurate mathematical glint of machine learning</a></li>
<li class="chapter" data-level="17" data-path="17-cs229-2007-returning-again-and-again-to-certain-features.html"><a href="17-cs229-2007-returning-again-and-again-to-certain-features.html"><i class="fa fa-check"></i><b>17</b> CS229, 2007: returning again and again to certain features</a></li>
<li class="chapter" data-level="18" data-path="18-the-visible-learning-of-machine-learning.html"><a href="18-the-visible-learning-of-machine-learning.html"><i class="fa fa-check"></i><b>18</b> The visible learning of machine learning</a></li>
<li class="chapter" data-level="19" data-path="19-the-diagram-of-an-operational-formation.html"><a href="19-the-diagram-of-an-operational-formation.html"><i class="fa fa-check"></i><b>19</b> The diagram of an operational formation</a></li>
<li class="chapter" data-level="20" data-path="20-vectorisation-and-its-consequences.html"><a href="20-vectorisation-and-its-consequences.html"><i class="fa fa-check"></i><b>20</b> Vectorisation and its consequences}</a></li>
<li class="chapter" data-level="21" data-path="21-vector-space-and-geometry.html"><a href="21-vector-space-and-geometry.html"><i class="fa fa-check"></i><b>21</b> Vector space and geometry</a></li>
<li class="chapter" data-level="22" data-path="22-mixing-places.html"><a href="22-mixing-places.html"><i class="fa fa-check"></i><b>22</b> Mixing places</a></li>
<li class="chapter" data-level="23" data-path="23-truth-is-no-longer-in-the-table.html"><a href="23-truth-is-no-longer-in-the-table.html"><i class="fa fa-check"></i><b>23</b> Truth is no longer in the table?</a></li>
<li class="chapter" data-level="24" data-path="24-the-epistopic-fault-line-in-tables.html"><a href="24-the-epistopic-fault-line-in-tables.html"><i class="fa fa-check"></i><b>24</b> The epistopic fault line in tables</a></li>
<li class="chapter" data-level="25" data-path="25-surface-and-depths-the-problem-of-volume-in-data.html"><a href="25-surface-and-depths-the-problem-of-volume-in-data.html"><i class="fa fa-check"></i><b>25</b> Surface and depths: the problem of volume in data</a></li>
<li class="chapter" data-level="26" data-path="26-vector-space-expansion.html"><a href="26-vector-space-expansion.html"><i class="fa fa-check"></i><b>26</b> Vector space expansion</a></li>
<li class="chapter" data-level="27" data-path="27-drawing-lines-in-a-common-space-of-transformation.html"><a href="27-drawing-lines-in-a-common-space-of-transformation.html"><i class="fa fa-check"></i><b>27</b> Drawing lines in a common space of transformation</a></li>
<li class="chapter" data-level="28" data-path="28-implicit-vectorization-in-code-and-infrastructures.html"><a href="28-implicit-vectorization-in-code-and-infrastructures.html"><i class="fa fa-check"></i><b>28</b> Implicit vectorization in code and infrastructures</a></li>
<li class="chapter" data-level="29" data-path="29-lines-traversing-behind-the-light.html"><a href="29-lines-traversing-behind-the-light.html"><i class="fa fa-check"></i><b>29</b> Lines traversing behind the light</a></li>
<li class="chapter" data-level="30" data-path="30-the-vectorised-table.html"><a href="30-the-vectorised-table.html"><i class="fa fa-check"></i><b>30</b> The vectorised table?</a></li>
<li class="chapter" data-level="31" data-path="31-machines-finding-functions.html"><a href="31-machines-finding-functions.html"><i class="fa fa-check"></i><b>31</b> Machines finding functions}</a></li>
<li class="chapter" data-level="32" data-path="32-learning-functions.html"><a href="32-learning-functions.html"><i class="fa fa-check"></i><b>32</b> Learning functions</a></li>
<li class="chapter" data-level="33" data-path="33-supervised-unsupervised-reinforcement-learning-and-functions.html"><a href="33-supervised-unsupervised-reinforcement-learning-and-functions.html"><i class="fa fa-check"></i><b>33</b> Supervised, unsupervised, reinforcement learning and functions</a></li>
<li class="chapter" data-level="34" data-path="34-which-function-operates.html"><a href="34-which-function-operates.html"><i class="fa fa-check"></i><b>34</b> Which function operates?</a></li>
<li class="chapter" data-level="35" data-path="35-what-does-a-function-learn.html"><a href="35-what-does-a-function-learn.html"><i class="fa fa-check"></i><b>35</b> What does a function learn?</a></li>
<li class="chapter" data-level="36" data-path="36-observing-with-curves-the-logistic-function.html"><a href="36-observing-with-curves-the-logistic-function.html"><i class="fa fa-check"></i><b>36</b> Observing with curves: the logistic function</a></li>
<li class="chapter" data-level="37" data-path="37-the-cost-of-curves-in-machine-learning.html"><a href="37-the-cost-of-curves-in-machine-learning.html"><i class="fa fa-check"></i><b>37</b> The cost of curves in machine learning</a></li>
<li class="chapter" data-level="38" data-path="38-curves-and-the-variation-in-models.html"><a href="38-curves-and-the-variation-in-models.html"><i class="fa fa-check"></i><b>38</b> Curves and the variation in models</a></li>
<li class="chapter" data-level="39" data-path="39-observing-costs-losses-and-objectives-through-optimisation.html"><a href="39-observing-costs-losses-and-objectives-through-optimisation.html"><i class="fa fa-check"></i><b>39</b> Observing costs, losses and objectives through optimisation</a></li>
<li class="chapter" data-level="40" data-path="40-gradients-as-partial-observers.html"><a href="40-gradients-as-partial-observers.html"><i class="fa fa-check"></i><b>40</b> Gradients as partial observers</a></li>
<li class="chapter" data-level="41" data-path="41-the-power-to-learn.html"><a href="41-the-power-to-learn.html"><i class="fa fa-check"></i><b>41</b> The power to learn</a></li>
<li class="chapter" data-level="42" data-path="42-probabilisation-and-the-taming-of-machines.html"><a href="42-probabilisation-and-the-taming-of-machines.html"><i class="fa fa-check"></i><b>42</b> Probabilisation and the Taming of Machines}</a></li>
<li class="chapter" data-level="43" data-path="43-data-reduces-uncertainty.html"><a href="43-data-reduces-uncertainty.html"><i class="fa fa-check"></i><b>43</b> Data reduces uncertainty?</a></li>
<li class="chapter" data-level="44" data-path="44-machine-learning-as-statistics-inside-out.html"><a href="44-machine-learning-as-statistics-inside-out.html"><i class="fa fa-check"></i><b>44</b> Machine learning as statistics inside out</a></li>
<li class="chapter" data-level="45" data-path="45-distributed-probabilities.html"><a href="45-distributed-probabilities.html"><i class="fa fa-check"></i><b>45</b> Distributed probabilities</a></li>
<li class="chapter" data-level="46" data-path="46-naive-bayes-and-the-distribution-of-probabilities.html"><a href="46-naive-bayes-and-the-distribution-of-probabilities.html"><i class="fa fa-check"></i><b>46</b> Naive Bayes and the distribution of probabilities</a></li>
<li class="chapter" data-level="47" data-path="47-spam-when-foralln-is-too-much.html"><a href="47-spam-when-foralln-is-too-much.html"><i class="fa fa-check"></i><b>47</b> Spam: when <span class="math inline">\(\forall{N}\)</span> is too much?</a></li>
<li class="chapter" data-level="48" data-path="48-the-improbable-success-of-the-naive-bayes-classifier.html"><a href="48-the-improbable-success-of-the-naive-bayes-classifier.html"><i class="fa fa-check"></i><b>48</b> The improbable success of the Naive Bayes classifier</a></li>
<li class="chapter" data-level="49" data-path="49-ancestral-probabilities-in-documents-inference-and-prediction.html"><a href="49-ancestral-probabilities-in-documents-inference-and-prediction.html"><i class="fa fa-check"></i><b>49</b> Ancestral probabilities in documents: inference and prediction</a></li>
<li class="chapter" data-level="50" data-path="50-statistical-decompositions-bias-variance-and-observed-errors.html"><a href="50-statistical-decompositions-bias-variance-and-observed-errors.html"><i class="fa fa-check"></i><b>50</b> Statistical decompositions: bias, variance and observed errors</a></li>
<li class="chapter" data-level="51" data-path="51-does-machine-learning-construct-a-new-statistical-reality.html"><a href="51-does-machine-learning-construct-a-new-statistical-reality.html"><i class="fa fa-check"></i><b>51</b> Does machine learning construct a new statistical reality?</a></li>
<li class="chapter" data-level="52" data-path="52-patterns-and-differences.html"><a href="52-patterns-and-differences.html"><i class="fa fa-check"></i><b>52</b> Patterns and differences</a></li>
<li class="chapter" data-level="53" data-path="53-splitting-and-the-growth-of-trees.html"><a href="53-splitting-and-the-growth-of-trees.html"><i class="fa fa-check"></i><b>53</b> Splitting and the growth of trees</a></li>
<li class="chapter" data-level="54" data-path="54-differences-in-recursive-partitioning.html"><a href="54-differences-in-recursive-partitioning.html"><i class="fa fa-check"></i><b>54</b> 1984: Differences in recursive partitioning</a></li>
<li class="chapter" data-level="55" data-path="55-limiting-differences.html"><a href="55-limiting-differences.html"><i class="fa fa-check"></i><b>55</b> Limiting differences</a></li>
<li class="chapter" data-level="56" data-path="56-the-successful-dispersion-of-the-support-vector-machine.html"><a href="56-the-successful-dispersion-of-the-support-vector-machine.html"><i class="fa fa-check"></i><b>56</b> The successful dispersion of the support vector machine</a></li>
<li class="chapter" data-level="57" data-path="57-differences-blur.html"><a href="57-differences-blur.html"><i class="fa fa-check"></i><b>57</b> Differences blur?</a></li>
<li class="chapter" data-level="58" data-path="58-bending-the-decision-boundary.html"><a href="58-bending-the-decision-boundary.html"><i class="fa fa-check"></i><b>58</b> Bending the decision boundary</a></li>
<li class="chapter" data-level="59" data-path="59-instituting-patterns.html"><a href="59-instituting-patterns.html"><i class="fa fa-check"></i><b>59</b> Instituting patterns</a></li>
<li class="chapter" data-level="60" data-path="60-regularizing-and-materializing-objects.html"><a href="60-regularizing-and-materializing-objects.html"><i class="fa fa-check"></i><b>60</b> Regularizing and materializing objects}</a></li>
<li class="chapter" data-level="61" data-path="61-genomic-referentiality-and-materiality.html"><a href="61-genomic-referentiality-and-materiality.html"><i class="fa fa-check"></i><b>61</b> Genomic referentiality and materiality</a></li>
<li class="chapter" data-level="62" data-path="62-the-genome-as-threshold-object.html"><a href="62-the-genome-as-threshold-object.html"><i class="fa fa-check"></i><b>62</b> The genome as threshold object</a></li>
<li class="chapter" data-level="63" data-path="63-genomic-knowledges-and-their-datasets.html"><a href="63-genomic-knowledges-and-their-datasets.html"><i class="fa fa-check"></i><b>63</b> Genomic knowledges and their datasets</a></li>
<li class="chapter" data-level="64" data-path="64-the-advent-of-wide-dirty-and-mixed-data.html"><a href="64-the-advent-of-wide-dirty-and-mixed-data.html"><i class="fa fa-check"></i><b>64</b> The advent of ‘wide, dirty and mixed’ data</a></li>
<li class="chapter" data-level="65" data-path="65-cross-validating-machine-learning-in-genomics.html"><a href="65-cross-validating-machine-learning-in-genomics.html"><i class="fa fa-check"></i><b>65</b> Cross-validating machine learning in genomics</a></li>
<li class="chapter" data-level="66" data-path="66-proliferation-of-discoveries.html"><a href="66-proliferation-of-discoveries.html"><i class="fa fa-check"></i><b>66</b> Proliferation of discoveries</a></li>
<li class="chapter" data-level="67" data-path="67-variations-in-the-object-or-in-the-machine-learner.html"><a href="67-variations-in-the-object-or-in-the-machine-learner.html"><i class="fa fa-check"></i><b>67</b> Variations in the object or in the machine learner?</a></li>
<li class="chapter" data-level="68" data-path="68-whole-genome-functions.html"><a href="68-whole-genome-functions.html"><i class="fa fa-check"></i><b>68</b> Whole genome functions</a></li>
<li class="chapter" data-level="69" data-path="69-propagating-subject-positions.html"><a href="69-propagating-subject-positions.html"><i class="fa fa-check"></i><b>69</b> Propagating subject positions}</a></li>
<li class="chapter" data-level="70" data-path="70-propagation-across-human-machine-boundaries.html"><a href="70-propagation-across-human-machine-boundaries.html"><i class="fa fa-check"></i><b>70</b> Propagation across human-machine boundaries</a></li>
<li class="chapter" data-level="71" data-path="71-competitive-positioning.html"><a href="71-competitive-positioning.html"><i class="fa fa-check"></i><b>71</b> Competitive positioning</a></li>
<li class="chapter" data-level="72" data-path="72-a-privileged-machine-and-its-diagrammatic-forms.html"><a href="72-a-privileged-machine-and-its-diagrammatic-forms.html"><i class="fa fa-check"></i><b>72</b> A privileged machine and its diagrammatic forms</a></li>
<li class="chapter" data-level="73" data-path="73-varying-subject-positions-in-code.html"><a href="73-varying-subject-positions-in-code.html"><i class="fa fa-check"></i><b>73</b> Varying subject positions in code</a></li>
<li class="chapter" data-level="74" data-path="74-the-subjects-of-a-hidden-operation.html"><a href="74-the-subjects-of-a-hidden-operation.html"><i class="fa fa-check"></i><b>74</b> The subjects of a hidden operation</a></li>
<li class="chapter" data-level="75" data-path="75-algorithms-that-propagate-errors.html"><a href="75-algorithms-that-propagate-errors.html"><i class="fa fa-check"></i><b>75</b> Algorithms that propagate errors</a></li>
<li class="chapter" data-level="76" data-path="76-competitions-as-examination.html"><a href="76-competitions-as-examination.html"><i class="fa fa-check"></i><b>76</b> Competitions as examination</a></li>
<li class="chapter" data-level="77" data-path="77-superimposing-power-and-knowledge.html"><a href="77-superimposing-power-and-knowledge.html"><i class="fa fa-check"></i><b>77</b> Superimposing power and knowledge</a></li>
<li class="chapter" data-level="78" data-path="78-ranked-subject-positions.html"><a href="78-ranked-subject-positions.html"><i class="fa fa-check"></i><b>78</b> Ranked subject positions</a></li>
<li class="chapter" data-level="79" data-path="79-conclusion-out-of-the-data.html"><a href="79-conclusion-out-of-the-data.html"><i class="fa fa-check"></i><b>79</b> Conclusion: Out of the Data}</a></li>
<li class="chapter" data-level="80" data-path="80-machine-learners.html"><a href="80-machine-learners.html"><i class="fa fa-check"></i><b>80</b> 250,000 machine learners</a></li>
<li class="chapter" data-level="81" data-path="81-a-summary-of-the-argument.html"><a href="81-a-summary-of-the-argument.html"><i class="fa fa-check"></i><b>81</b> A summary of the argument</a></li>
<li class="chapter" data-level="82" data-path="82-in-situ-hybridization.html"><a href="82-in-situ-hybridization.html"><i class="fa fa-check"></i><b>82</b> In-situ hybridization</a></li>
<li class="chapter" data-level="83" data-path="83-critical-operational-practice.html"><a href="83-critical-operational-practice.html"><i class="fa fa-check"></i><b>83</b> Critical operational practice?</a></li>
<li class="chapter" data-level="84" data-path="84-obstacles-to-the-work-of-freeing-machine-learning.html"><a href="84-obstacles-to-the-work-of-freeing-machine-learning.html"><i class="fa fa-check"></i><b>84</b> Obstacles to the work of freeing machine learning</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning: Archaeology of a Data Practice</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="propagation-across-human-machine-boundaries" class="section level1">
<h1><span class="header-section-number">70</span> Propagation across human-machine boundaries</h1>
<p>The concatenation of ‘one package, one mind’ does not definitively allocate agency to people or things. (A ‘package’ after all is another name for a library of code.) Mason adumbrates the outline of a subject position located at the intersection of network infrastructure, mathematics and human behaviour.<a href="#fn89" class="footnoteRef" id="fnref89"><sup>89</sup></a> Mason, herself one of <em>Fortune</em> magazines ‘Top 40 under 40’ business leaders to watch <span class="citation">[@CNN_2011]</span> and also featured in <em>Glamour</em>, a teenage fashion magazine <span class="citation">[@Mason_2012]</span>, might personify such a ‘wonderful person.’  She is not a lone example. In mid-2016 Google announced a comprehensive program to re-train its software developers as machine learners <span class="citation">[@Levy_2016]</span>.<a href="#fn90" class="footnoteRef" id="fnref90"><sup>90</sup></a></p>
<p>‘It is the privileged machine in this context that creates its marginalized human others’ writes Lucy Suchman in her account of the encounters that ‘effect “persons” and “machines” as distinct entities’ <span class="citation">[@Suchman_2006, 269]</span>.   While Mason and other relatively well-known human machine learners are not exactly marginalized (just the opposite, they achieve minor celebrity status in some cases), Suchman recommends ‘recognition of the particularities of bodies and artifacts, of the cultural-historical practices through which human-machine differences are (re-)iteratively drawn, and of the possibilities for and politics of redistribution across the human machine boundary’ (285). The intersections that machine learners currently occupy are heavily re-distributional. In almost every instance, machine learners claim to do something that humans alone, no matter how expert, could not. Does the re-distribution of engineering, mathematics, curiosity, infrastructure and ‘something that usually falls under the social sciences’ (but perhaps no longer does so?) both energise subjects (‘its a pretty exciting time to be in any of these things’) and assign them a marginal albeit still pivotal position in relation to privileged machines?</p>
<p>Machine learner subject positions are the topic of this chapter. I focus on artificial neural networks, or neural nets, in their various forms ranging from the multilayer perceptron (MLP) to the convolutional neural nets (CNN), recurrent neural nets (RNN) and deep belief networks of many recent deep learning projects (particularly in machine learning competitions, as discussed below <span class="citation">[@Dahl_2013]</span>).  in exploring the re-drawing of subject-machine positions.  Neural nets propagate between infrastructures, engineering and human behaviour (as Mason puts it), re-drawing human-machine differences, sometimes making it hard to seen what subject position they entail, where subjects are located or what they say, see and do.</p>
<p>Like other machine learners, neural nets re-draw human-machine differences. Geoffrey Hinton, Simon Osindero and Yee-Why Teh  writing in <em>Neural Computation</em> in 2006 described a ‘fast learning algorithm for deep belief nets’ <span class="citation">[@Hinton_2006a]</span>. Their description, whilst mostly couched in terms of conditional distributions, model parameters, and error rates, also contains a section entitled ‘Looking into the Mind of a Neural Network’ (1545-1546). In that section, they describe how they used their deep belief network to <em>generate</em> rather than classify images.<a href="#fn91" class="footnoteRef" id="fnref91"><sup>91</sup></a> In the process they were able to see what the ‘associative memory has in mind’ (1545). The term ‘mind,’ they comment, ‘is not intended to be metaphorical’ (1546) because the neural net in question has a distributed memory of the digits it has seen. Put slightly more formally, ‘the network has a full generative model, so it is easy to look into its mind - we simply generate an image from its high-level representations’ (1529).  ‘Looking,’ as often the case in machine learning, takes the form of diagramming a pattern, partition or strain in the data.</p>
<p>The substitution of ‘mind’ and model reproduces many aspects of the figure of artificial intelligence (which has typically relied on rule-based or symbolic reasoning), but the appearance of ‘mind’ in the form of a generative model (see chapter ) suggests a rather different subject position. Archaeologically, the description of subject positions entails more than giving voice the existential threat of artificial intelligence.. It first of all entails multiple positions linked to different groupings and statements in the operational formation. As I will suggest, neural nets are particularly interesting because they re-draw human-machine boundaries through a combination of feeding-forward of potentials and propagating backwards of differences specifically concerned with images. Similarly, the practice of machine learning shifts subject positions in a backward and forward movement. It propagates potentializing optimism even as it undercuts the very differences that give rise to that optimism.</p>
<pre><code>                     techniques year</code></pre>
<p>1 reliability 2009 2 bug prediction 2009 3 machine learning 2009 4 feature selection 2009 5 <NA> 2008 6 automatic document classification 2008 [1] 510216 3 discipline techniques year 1 statistics <NA> 2009 2 statistics <NA> 2008 3 statistics <NA> 2007 4 statistics distribution quantile 2003 5 statistics linear discriminant 2003 6 statistics monotone transformation 2003 [1] “neural network” “clustering”<br />
[3] “k mean” “feature selection”<br />
[5] “decision tree” “genetic algorithm”<br />
[7] “ensexpectation maximizationble” “pattern recognition”<br />
[9] “naive bayes” “random forest”<br />
[11] “feature extraction” “association rule”<br />
[13] “sexpectation maximizationi” “time serie”<br />
[15] “maximum likelihood” “rough set”<br />
[17] “algorithm” “knowledge discovery”<br />
[19] “sexpectation maximizationantic” “nearest neighbor”</p>

<p>Almost every machine learning class, textbook, demonstration, and in recent years, machine learning competitions at some point turns to neural nets. Neural nets display, however, some instability in the research literature. Figure  shows the most frequent keywords for technical publications across the three main disciplinary domains inhabited by machine learners. While neural nets rank very high in computer science and engineering disciplines (appearing just after support vector machines), they do not appear in the statistics literature until in the rankings. The prominence of neural nets on the engineering side of machine learning suggests a specific enunciative mapping. </p>
<p>Neural nets are often described from a deeply split perspective. At some moments, the description turns towards human subjects, or at least, the brains of human subjects. At other other moments, neural nets turn towards the vectorisation of data.  Neural nets constantly oscillate between brain and information infrastructure. In some ways, they renew long-standing cybernetic hopes of bring brains and computation together in models of computational intelligence and agency <span class="citation">[@Hayles_1999]</span>.  Although they stem from a biological inspiration (dating at least back to the work by McCulloch and Pitts in the 1940s <span class="citation">[@Halpern_2015; @Wilson_2010]</span>), they gain traction first in the 1980s and then again from mid-2000s onwards, as ways of dealing with changing computational infrastructures, and the difficulties of capitalising on infrastructure that is powerful but hard to control. In the course of fifty years, their serial re-invention – from perceptron via neural net to deep belief net – triply re-distributes subject positions amidst infrastructural re-configurations and vectorisation.  </p>
<p>For instance, writing in the 1980s, David Ackley, Geoffrey Hinton (an important figure in the inception of neural nets over several decades),  and Terrence Sejnowski link neuroscience and semiconductors: </p>
<blockquote>
<p>Evidence about the architecture of the brain and the potential of the new VLSI technology have led to a resurgence of interest in “connectionist” systems … that store their long-term knowledge as the strengths of the connections between simple neuron-like processing elements. These networks are clearly suited to tasks like vision that can be performed efficiently in parallel networks which have physical connections in just the places where processes need to communicate. … The more difficult problem is to discover parallel organizations that do not require so much problem-dependent information to be built into the architecture of the network <span class="citation">[@Ackley_1985, 147-148]</span>.</p>
</blockquote>
<p>Alignments and diagrammatic overlaps between brain and ‘new VSLI [Very Large Scale Integrated] technology’ – semiconductor chips – architectures sought to reproduce the plasticity of neuronal networks in the parallel distributed processing enabled by very densely packed semiconductor circuits.  The problem here was how to organize these connections without hardwiring domain specificity into ‘the architecture of the network.’ How could the architectures adapt to the problem in hand?</p>
<p>We saw in chapter  that the psychologist Frank Rosenblatt’s perceptron <span class="citation">[@Rosenblatt_1958]</span> first implemented McCulloch and Pitts’ cybernetic vision of neurones as models of computation <span class="citation">[@Edwards_1996]</span> .  While the computer science research on the perceptron wilted under criticism from artificial intelligence experts such as Marvin Minsky (Minsky famously showed that a perceptron cannot learn the logical exclusive OR or <code>XOR</code> function; <span class="citation">[@Minsky_1969]</span>),  cognitive psychologists such as David Rumelhart, Geoffrey Hinton and Ronald Williams persisted with perceptrons, seeking to generalize their operations by connecting them together in networks (also known as multilayer perceptrons). In the mid-1980s, they developed the back-propagation algorithm  <span class="citation">[@Rumelhart_1985; @Hinton_1989]</span>, a way of adjusting the connections – known as weights or parameters –  between nodes (neurones) in the network in response to features in the data (see Figure ).</p>
<p>The back-propagation algorithm directly addressed the problem of learning to modify network organization without reliance on problem-dependent architectures, and in without having to program them in.  Effectively, it constructs an architecture of generalization. While cognition, and the idea that machines would be cognitive (rather than say, mechanical, calculative, or rule-based) mesmerised research work in artificial intelligence for several decades, the development of the back-propagation algorithm as a way for a set of connected computational nodes to learn came with explicit infrastructural resonances.</p>
<p>The resonances between computational architectures and human cognition (centred on vision) became much more palpable from around 2006 when ‘deep belief nets’ appeared as a way of training many-layered neural nets implemented on much large computational platforms <span class="citation">[@Hinton_2006a]</span>. These resonances continue to echo today and indeed attract much attention.<a href="#fn92" class="footnoteRef" id="fnref92"><sup>92</sup></a> Like the advent of VSLI in the early 1980s, the vast concentrations of processing units in contemporary data centres (hundreds of thousands of cores as we saw in the case of Google Compute in the previous chapter ) and in the graphics cards  developed for high-end gaming and video rendering (GPUs for PC gaming now typically have a thousand and sometimes several thousand cores) pose the problem of organizing infrastructure so that processes can communicate with each other. Machine learners have become just as important as loose or mutable infrastructural orders as epistemic instruments.</p>

<p>Oscillating between cognition and infrastructures, between people and machines, neural nets suggest a way of thinking not only about how ‘long-term knowledge’ takes shape today, but about subject positions associated with machine learning.   As infrastructural reorganization takes place around learning, and around the production of statements by machine learners, both human and non-human machine learners are assigned new positions. These positions are sometimes hierarchical and sometimes dispersed. The machine learner subject position is a highly relational one rather than a single concentrated form of expertise (as we might find in a clinical oncologist, biostatistician or geologist). Because machine learners vectorize, optimize, probabilise, differentiate and refer, what counts as agency, skill, action, experience and learning shifts constantly. It is intimately bound and connected to transforms in infrastructure, variations in referentiality (such as we have seen in the construction of the vector space), and competing forms of accumulation or positivity. As Suchman suggests, examining privileged machines such as neural nets is a way to pay attention to the dispersed and somewhat disconnected sites from which subjects program, observe, design and respond to machine learners.</p>
</div>
<div class="footnotes">
<hr />
<ol start="89">
<li id="fn89"><p>In earlier work on machine learning <span class="citation">[@Mackenzie_2013]</span>, I presented programmers as agents of anticipation, suggesting that the turn to machine learning amongst programmers could be useful in understanding how predictivity was being done amidst broader shift to the regime of anticipation described by Vincenne Adams, Michelle Murphy and Adele Clarke <span class="citation">[@Adams_2009]</span>. Subsequently developments in machine learning, even just in the last three years, confirm that view, but in this chapter and in this book more generally, I focus less on transformations in programming practice and software development, and more on the asymmetries of different machine learner subjects in relation to infrastructures and knowledge.<a href="70-propagation-across-human-machine-boundaries.html#fnref89">↩</a></p></li>
<li id="fn90"><p>Other figures we might follow include Claudia Perlich, Andrew Ng, Geoffrey Hinton, Corinna Cortez, Daphne Koller, Christopher Bishop, Yann LeCun, or Jeff Hammerbacher. Although some women’s names appear here, in any such list, men’s names are much more likely to appear. This is no accident. <a href="70-propagation-across-human-machine-boundaries.html#fnref90">↩</a></p></li>
<li id="fn91"><p>In the case of this paper, and many others related to neural nets, the images are of hand-written digits. These digits have an almost constitutive role, as I discuss in this chapter.<a href="70-propagation-across-human-machine-boundaries.html#fnref91">↩</a></p></li>
<li id="fn92"><p>Although mainstream media accounts of machine learning are not the focus of my interest here, it is hard to ignore the extraordinary level of interest that deep learning projects and techniques have attracted in the last few years. Articles have appeared in all the usual places – <em>The New York Times</em> <span class="citation">[@Markoff_2012]</span>, <em>Wired</em><span class="citation">[@Garling_2015]</span>, or <em>The Guardian</em> <span class="citation">[@Arthur_2015]</span>. In many of these accounts, machine learning and neural nets in particular appear both in the guise of the existential threat of artificial intelligence and as a mundane device (for instance, speech recognition on a mobile phone). The spectacular character of deep learning could be analysed in terms like that of the genomes discussed in chapter . In both cases, the advent and transformation of these machine learners is closely linked to networked platforms (such as Google, Facebook, Yahoo and Microsoft) and their efforts to encompass within their services as many elements of experience, exchange, communication and power as possible. Deep learning machine learners currently focus mostly on images (photographs and video) and sounds (speech and music), and usually attempt to locate and label objects, words or genres. <a href="70-propagation-across-human-machine-boundaries.html#fnref92">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="69-propagating-subject-positions.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="71-competitive-positioning.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
