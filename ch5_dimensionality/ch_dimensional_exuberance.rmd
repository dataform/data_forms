# 5. Splitting, propagating and hyper-planing: patterns of movement in data 

The contemporary form of knowledge as pattern-finding 
Finding patterns in data
Movements in machine learning: patterns of generalization
Dimensional exuberance and pattern recognition in movement

```{r echo=FALSE} 
library(knitr)
opts_chunk$set(results='asis', message=FALSE, warnings=FALSE, cache=TRUE)
```
## todo

- what happens in code here -- the inner product; the recursive, the backprop algorithm ... 
- what happens to knowledge -- knowledge understood in Foucaultean sense as savoir;  the discursive practices that criss words and things
- the dispersion of techniques in domains suggests something like an enunciative function; at the same time, the techniques themselves are statements addressing a common problem around separating that which overlaps or blurs; 
- another form of dispersion -- the same techniques found in different fields under different names -- e.g. reinforcement rule learning

## structure

- decision tree, neural network and svm -- 1960 - 2000

## Introduction

> Algorithms for pattern recognition were therefore from the very beginning associated with the construction of linear decision surfaces [@Cortes_1995, 273-4]

> No statement can have a latent existence [@Foucault_1972, 16]

We have seen the emergence of the common vector space and its epistopological transformations, the multiplication of functions and their entwined partial observers, and then the re-distribution of probability in statistical devices that both populate the world and transform machine learners into populations. Across the vectors, functions, and distributions a  diagram of  machine learning weaves and knots many points of emergence, continuity and conjunction. Formidable accumulations of infrastructure, devices and expertise pivot around machine learning (as we will see in Part II). Already on several occasions problems of the density and mass of techniques, research literature, algorithms and code have appeared. What in this diagram and in the forest-like growth of techniques, projects, applications and proponents allows us to make sense of the dynamics of this accumulation? Via three highly developed and heavily used machine learners  -- decision trees, neural networks, and support vector machines  -- that more or less mesmerised the machine learning scientific literature between 1980-2000, some relatively novel and somewhat discontinuous forms of movement into and through data were initiated. These movements, which we might characterise as _splitting_, _propagating_ and _hyper-planing_ -- continue to fiercely animate machine learners in producing newer techniques (random forests, deep belief nets), intensifying their application, and perhaps more importantly, re-configuring what counts as knowledge.

These three techniques have not been chosen randomly. They appear in the machine learning scientific publications of the last three decades (1980-2010) not only as important novelties, but as, I will suggest, a new enunciative mode in which regularity and rarity, accumulation and sparsity do not exist in tension. Practically,  they  loom large in various contemporary accounts of machine learning as a way of knowing (for instance, in the popular machine learning  books such as _Machine Learning for Hackers_ [@Conway_2012] or _Doing Data Science_ [@Schutt_2013]). The machine learning academic literature published in statistics, computer science, mathematics, artificial intelligence and a swathe of related scientific fields bristles with references to decision trees, support vector machine and neural networks. (As we will see, these techniques themselves ingest substantial supplies of knowledge and practice produced by  social sciences, insurance and actuarial practice, and marketing research.) A very rough citation analysis of the research literature indicates that  the top 20 most cited publications in the field refer either to decision trees [@Quinlan_1986; @Breiman_1984], support vector machines [@Vapnik_1999; @Cortes_1995],  machine learning pedagogy [@Mitchell_1997], an early textbook written by a computer scientist; Witten, a textbook and software package on data mining using Java [@Witten_2005]; a textbook on pattern recognition dating from the 1970s [@Duda_2012], a tutorial on an error control technique (ROC - Receiver Operating Characteristics, first developed by the US military during WWII) and somewhat lower, another well-known textbook, this time on neural networks [@Bishop_2006]. While I do not place too much weight on citation counts, since they develop for different reasons over time and display discipline-specific tendencies, the spectrum of references here show a variety of investments in machine learning. They range from highly theoretical general accounts of the nature of machine learning (Mitchell, Duda) to detailed concrete implementations of techniques (Witten). Some focus on very specific techniques such as decision trees (Breiman, Quinlan) or support vector machines (Cortes) or neural networks (Bishop).

Across all of these differences, a striking _positivity_ takes shape. I understand positivity, partly in the sense of 'positivism' -- a theory widely shared amongst machine learners  that knowledge is based on observation -- and positive in the sense proposed by Michel Foucault in _The Archaeology of Knowledge_, a book that unexpectedly and anachronistically resonates with much that takes place in machine learning. Foucault in this book posits statements as elements distributed in a relatively sparse space that undergoes transformations that generate what can be thought and done within a particular discursive formation. This is their positivity:

>To describe a group of statements not as the closed, plethoric totality of a meaning, but as an incomplete, fragmented figure; to describe a group of statements not with reference to the interiority of an intention, a thought, or a subject, but in accordance with the dispersion of an exteriority; to describe a group of statements, in order to rediscover not the moment or the trace of their origin, but the specific forms of an accumulation, is certainly not to uncover an interpretation, to discover a foundation, or to free constituent acts; nor is it to decide on a rationality, or to embrace a teleology. It is to establish what I am quite willing to call a positivity [@Foucault_1972, 125].

Foucault frequently refers to positivity in those passages where he discuss the set of conditions configuring how practices occur, and how statements relating to those practice take shape. Note that the situations in which he invokes positivity involve accumulations or plethora of statements whose coherence or unity does not lie in any origin, intention, rationality, subjectivity, ideology or teleology. Positivities enable accumulations without deriving them from underlying unities. Foucault's acceptance of 'positivity,' in spite of the risk of attracting criticism, pivots on his interest in, and practical commitment to, accumulating observations. A positivity is a way of describing 'a group of statements' that may be quite dispersed, discontinuous, and unevenly distributed because they make up 'a population of events in the space of discourse in general' [@Foucault_1972, 27]. If _The Archaeology of Knowledge_ is a book concerned with the tissue of relations between things, what is said, what is done and what counts as knowledge (scientific or not) in practice, then the persistent and at times forceful reiteration of _dispersion_ and _discontinuity_ might seem strangely counterintuitive. But Foucault's insistence on discourse as something irreducible to logic, language, intention, rationality, experience, historical origin or narrative, and as the way in which 'statements' ('a graph, a growth curve, an age pyramid, a distribution cloud are all statements' (82)), strategies, concepts, objects, archives and knowledges come together in 'discursive formations' is an attempt to resist any resort to a hidden order or concealed origin that critical thought could uncover or interpret. 

This commitment to a positivity of accumulation helps, somewhat paradoxically, in a field such as machine learning whose many different forms, techniques, statements, institutions, values and associations entangle with each other. The three techniques that anchor this chapter are at once perhaps the most distinctive machine learning achievements of the late twentieth century, at least judging by the citational and implementational interest they attract, yet at the same time seem quite heterogeneous and dispersed. At certain times, they come together (for instance, in machine learning competitions discussed in chapter 8; or in certain formalizations such as machine learning  theory or in graphs of the bias-variance decomposition discussed in the previous chapter; or in the pedagogy of machine learning discussed in chapter 1 [@Ng_2008c; @Ng_2008d, @Ng_2008h]). We might understand their heterogeneity in terms of 'enunciative modalities' [@Foucault_1972, 54]   -- that is, in terms of the different ways in which they give rise to statements. Each of the  machine learners generates statements, but from different places, by somewhat different individuals, and from the different situations they that are able 'to occupy in relation to the various domains or groups of objects' [@Foucault_1972, 52]. While Foucault tends to retain a decoupled subject-object relation in the production of statements,  I tend to see these enunciative modalities as distributed across people and things. As always, machine learner is a composite term for this distribution. 

## Splitting or recursive binary partitioning and the growth of trees

> Mastering the details of tree growth and management is an excellent way to understand the activities of learning machine generally [@Malley_2011, 118].

As the quote from a biomedical textbook on statistical machine learning suggests, decision trees promise an understanding of machine learning. The enunciative modality of the decision tree partly relates to the observability and comprehensibility of machine learning. As we will see, not all machine learners readily supports observation or comprehension. The cost of comprehensibility, however, is a certain highly restricted movement through the data, a movement that has been difficult to stabilise and control. As _Elements of Statistical Learning_ puts it: 'tree-based methods partition the feature space into a set of rectangles, and then fit a simple model (like a constant) in each one. They are conceptually simple yet powerful' [@Hastie_2009, 305].

\begin{figure}
  \centering
      \includegraphics[width=0.9\textwidth]{figure/recursive_partitioning.pdf}
        \caption{Recursive partitioning of the feature space}
  \label{fig:rpart}
\end{figure}

Tree-based methods are supervised learning techniques as they require the data to either be labelled with a class or to have some outcome value. The variable types in the feature space (or common vector space of the data) can be mixed. Because the method cuts the feature space into a tiled surface, the features can be continuous or discontinuous. The 'simple models' that tree methods construct each inhabit one of the rectangular regions or partitions of the feature space. In Figure \ref{fig:rpart}, the different regions or partitions produced by a decision tree are labelled $R1, R2$ etc. 

```{r aid, echo=FALSE, results='asis', cache=TRUE, width=0.8, messages=FALSE, warning=FALSE, comment=NA , fig.cap='Early uses of the Automatic Interaction Detector'} 
    library(xtable)
    aid_df = '../ml_lit/data/morgan_sonquist_WOS'
    files <- dir(aid_df,full.names=TRUE)
    recs <-lapply(files, read.csv, sep='\t', header=TRUE, as.is=TRUE, quote='', row.names=NULL)  
    aid_df <-do.call( rbind, recs )
    colnames(aid_df)[1:52] <- colnames(aid_df)[2:53]
    colnames(aid_df)[1] <-'PT'
    aid_df$TI = tolower(aid_df$TI)
    aid_df = aid_df[order(aid_df$PY, decreasing=FALSE),]
   print(comment=FALSE, xtable(head(label="References to Morgan and Sonquist's Automatic Interaction Detector", aid_df[,c('TI', 'PY' ,'TC')],10)), label='table:aid_citation')
```

Work on classification and regression techniques using decision tree goes back to the  early 1960s when social scientists James Morgan and John Sonquist at the University of Michigan's Institute for Social Research were attempting to analyse increasingly large social survey datasets [@Morgan_1963].  As Dan Steinberg describes in his brief history of decision trees [@Steinberg_2009, 180], the  'automatic interaction detector' (AID) as it was known, sought to automate the practice of data analysts looking for interactions between different variables. The variety and sheer optimism of subsequent applications of these prototype decision tree techniques  is striking.  In the 1960s and 1970s, papers that drew on the AID paper or use AID techniques can be found, as table \ref{table:aid_citation} shows, in education, politics, economics, population control, advertising, mass media and family planning. 

A decade after initial work, the AID was the object of trenchant criticism by statisticians and others. Writing in the 1970s, statisticians in the bebavioural sciences such as Hillel Einhorn at the University of Chicago castigated the use of such techniques.   The criticisms stemmed partly from  a general distrust of 'purely empirical methods', and scepticism in relation to their positivity:

> The purely empirical approach is particularly dangerous in an age when computers and packaged programs are readily available, since there is temptation to substitute immediate empirical analysis for more analytic thought and theory building. It is also probably too much to hope that a majority of researchers will take the time to find out how and why a particular program works. The chief interest will continue to be in the output-the results-with as little delay as possible [@Einhorn_1972, 368]

Einhorn discusses AID alongside other  techniques such as factor analysis and multi-dimensional scaling (both still widely used) before concluding 'it should be clear that proceeding without a theory and with powerful data analytic techniques can lead to large numbers of Type I errors' [@Einhorn_1972, 378]. His specific objections to AID are  particularly focused on the problematic power of the technique: 'it may make sense out of "noise"' (369). Consequently, researchers easily misuse the technique: they 'overfit' the data, and do not pay enough attention to issues of validation (369-370). Most of these criticisms can be seen as expressing conventional statistical reservations. Similarly the British marketing researcher Peter Doyle, criticising the use of AID in assessing store performance and site selection by operation researchers, complained that searching for patterns in data using small data sets was bound to lead to spurious results and the decision trees, although intuitively appealing (that is, they could be easily interpreted), were afflicted with arbitrariness: 'a second variable may be almost as discriminating as the one chosen, but if the program is made to split on this, quite a different tree occurs' [@Doyle_1973, 465-466].  These objections and resistances to early decision trees echo today in discussions around pattern recognition, knowledge discovery and data-mining in science and commerce.    The problem of what computers do to the analysis of empirical data is long-standing. 

As Einhorn expected, it was too much to hope that all researchers would take time to investigate how a particular program works. Some researchers however did take time, years in fact, to investigate how decision trees work. As a result, writing in 2008, Hastie, Tibshirani and Friedman, who could hardly by accused of not understanding decision trees,  happily recommend decision trees as the best off-the-shelf classifier:  'of all the well-known learning methods, decision trees comes closest to meeting the requirements for serving as an off-the-shelf procedure for data-mining' [@Hastie_2009, 352]. We might wonder here, however, whether they damn with faint praise, since 'off-the-shelf' suggests pre-packaged, and commodified, and the term 'data-mining' itself is not without negative connotations. As for its commercial realities, in 2013, Salford Systems, the purveyors of the leading contemporary commercial decision tree software, `CART`, could claim:

 > CART is the ultimate classification tree that has revolutionized the entire field of advanced analytics and inaugurated the current era of data mining. CART, which is continually being improved, is one of the most important tools in modern data mining. Others have tried to copy CART but no one has succeeded as evidenced by unmatched accuracy, performance, feature set, built-in automation and ease of use. [Salford Systems](http://www.salford-systems.com/products/cart)

What happened between 1973 and 2013?  Decision trees somehow travelled from the statistically murky waters of the social science departments and business schools in the early 1970s to inaugurate the 'current era of datamining' (which the scientific literature suggests starts in the early 1990s). This was not only a commercial innovation. As the earlier citation from U.S. National Institutes of Health biostatisticians Malley, Malley and Pajevic indicates, decision trees now enjoy high regard even in biomedical research, a setting where statistical rigour is highly encouraged for life and death reasons. The happy situation of decision trees four decades on suggests some kind of threshold was crossed in which the epistemological, statistical, or algorithmic ('built-in automation') power of the technique altered substantially. 

## 1984: Cutbacks on recursive partitioning

The third author of _Elements of Statistical Learning_, Jerome Friedman, worked at the  U.S. Department of Energy's Stanford Linear Accelerator during the late 1970s. Friedman  was  instrumental in rescuing  decision trees from the ignominy of profligate ease of use and pure empiricism they had endured since the late  1960s. The reorganisation and statistical retrofitting of the decision tree was not a single or focused effort. During the 1980s, statisticians such  as Friedman and Leo Breiman renovated the decision tree as a statistical tool [@Breiman_1984] at the same time as  computer scientists such as  Ross Quinlan in Sydney were re-implementing decision trees guided by an  artificial intelligence-based formalisation as rule-based induction technique [@Quinlan_1986].[^5.1] This uneasy parallel effort between computer science and statistics  still characterises machine learning today. Both statisticians and computer scientists  do and use the same techniques, but often with the computer scientists focusing on optimisation and algorithmic automation and the statisticians inventing novel formalizations. The fateful embrace of statistics and computer science, the disciplinary binary that vectorizes machine learning, has been generative in the retrieval of the decision tree. 

[^5.1]: Quinlan's papers  and book on versions of the decision tree (`ID3` and `c4.5`) are both amongst the top ten the most highly cited references in the machine literature itself. Google Scholar reports over 20,00 citations of the Quinlan's book _C4.5: Programs for Machine Learning_ [@Quinlan_1993]. Several years ago,  `C4.5` was voted the top data mining algorithm [@Wu_2008]. While I don't discuss Quinlan's work in much detail here, we should note as a computer scientist, Quinlan takes a much more rule-based approach to decision tree that Breiman and co-authors. 

An initial symptom of the transformation of the technique appears in a name change. The term 'decision tree,' although still widely used in the research literature and machine learner  parlance was replaced by 'classification and regression tree' during the late 1970s and 1980s. The terms 'classification and regression tree' is sometimes contracted to 'CART,' and that term strictly speaking refers to a computer program described in [@Breiman_1984] as well as the title of that highly-cited monograph. As we have seen in previous chapters, regression and classification designate the two main sides of machine learning practice, and their concatenation with 'tree' attests to the renovation of the 1960-70's style approaches with a more statistical facade.

The implementation of machine learning techniques in `R`  offers one path into  their mode of enunciation. This path, by virtue of `R`'s statistical provenance perhaps favours the statistical side of decision tree practice, but that has certain forensic virtues not offered by commercial or closed-source software often produced by computer scientists.  In this case, the name of   one long-standing and widely-used `R` package itself attests to something: `rpart` is a contraction of 'recursive partitioning' and this term generally describes how the decision tree algorithm works to partition the common vector space into the form shown in Figure \ref{fig:rpart} [@Therneau_2015].  'Cart,' on the other hand, is a registered trademark of Salford Systems, the software company mentioned above, who sell the leading commercial implementation of classification and regression trees. Hence, the `R` package `rpart` cannot call itself the more obvious name `cart, ` and instead invokes the algorithmic process it relies on: recursive partitioning. (Other `R` packages such as `party` [@Hothorn_2006] and `tree` [@Ripley_2014] also use recursive partitioning, but with various tweaks and optimisations that I leave aside here.)

 R.A. Fisher's _iris_ dataset, which contains measurements made in the 1930s of petal and sepal lengths of _iris virginica, iris setosa_ and _iris versicolor_ is a standard example for decision trees [@Fisher_1938].[^5.12]

[^5.12]:  `Iris` is a very small dataset. It is definitely a pre-computational miniature, but that diminutive character makes it into a useful illustration. While the _iris_ dataset is quite small, it supports a rhizomatic ecosystem of examples scattered across the machine learning literature.    The usual framing of the classification problem is how to decide whether a given iris blossom is of the species _virginica_, _setosa_ or _versicolor_.   These irises don't grow in forests -- they are more often found in riverbanks and meadows --  but they do offer a variety of illustrations of how machine learning classifiers are brought to bear on classification problems. Here the classification problem is taxonomic - the  iris genus has various sub-genera, and sections within the sub-genera. _Setosa, _virginica_ and _versicolor_ all belong to the sub-genus _Limniris_. This botanical context is  routinely ignored in  machine learning applications. In machine learning textbooks and tutorials, iris typically would be used to demonstrate how cleanly a classifier can separate the different kinds of irises. 


```{r iris_tree, echo=TRUE, cache=TRUE, message=FALSE, warning=FALSE, comment=NA, size='smallsize', results='asis' } 

    data(iris)
    library(rpart)
    iris_tree =rpart(Species ~ ., iris)
```

The code shown here loads the _iris_ data (the dataset  is routinely installed with many data analysis tools), loads the `rpart` decision tree library, and builds a decision to classify the irises by species.  What has happened to the iris data in this decision tree? The `R` code that invokes the recursive partitioning algorithm is so brief `iris_tree =rpart(Species ~ ., iris)` that we can't tell much about how the data has been 'recursively partitioned.' We know that the _iris_ has `r nrow(iris)` rows, and that there are equal numbers of the three iris varieties. 

The brevity of the code indicates a great deal of formalization of practice has accrued around decision trees. Some of this formalization was described in the landmark _Classification and Regression Trees_ monograph [@Breiman_1984], and is familiar (from previous chapters)  in  its use of common vector space, functions as partial observers and the re-distribution of chance between devices and worlds. For instance, Breiman and co-authors start out by placing the technique in vector space:

> Define the measurements $(x_1, x_2, ...)$ made on a case as the _measurement vector_ $\mathbf{x}$ corresponding to the case. [@Breiman_1984, 3]

In re-configuring decision trees as classification trees, Breiman first address issues of data dimensionality. Next,  rather than simply invoking the notion of a classifier as a device (as AID did), the CART monograph defines a classifier in terms of a function: 

> A classifier or classification rule is a function $d(\mathbf{x})$ defined on $\mathcal{X}$ so that for every $\mathbf{x}$, $d(\mathbf{x})$ is equal to one of the numbers $1, 2, ..., J$. [@Breiman_1984, 4]

Note that 'rule,' the term that computer scientists working in artificial intelligence at the time were using, still figures here, but the classifier is formalized as a function.  The definition of the classifier as a function depends on the existence of a vector space $\mathcal{X}$, a set of measurement, feature or predictor vectors  $\mathbf{x}$ and a set of responses  $1, 2, ..., J$. Second, classification is understood in terms of a series of binary splits that comprise the tree structure. The problem here is that many splits are possible. 

> The first problem in tree construction is how to use $\mathcal{L}$ to determine the binary splits of $\mathcal{X}$ into smaller pieces. The fundamental idea is to select each split of a subset so that the data in each of the descendant subsets are "purer" than the data in the parent subset [@Breiman_1984, 23].

Tree construction hinges on the notion of purity or more precisely 'node impurity', a function that measures how data labelled as belonging to different classes are mixed together at a given branch or node in a decision tree: 'that is, the node impurity is largest when all classes are equally mixed together in it, and smallest when the node contains only one class' [@Breiman_1984, 24]. As Malley and co-authors note, 'the collection of purity measures is still a subject of research' [@Malley_2011, 123], but Breiman, Friedman, Olshen and Stone promoted a particular form of impurity measure for classification trees known as 'Gini index of diversity' [@Breiman_1984, 38].  Like the cost function used in optimising linear and logistic regression models, the measures of node impurity allow the process of tree construction to be understood as a kind of non-linear movement through space. Whereas in gradient descent, the intuition was 'always go down to the valley as quickly as possible', here the intuition is more like: ' split things in ways that minimize mixing'. Good splits decrease the level of impurity in the tree. In an ideal tree with maximum purity, each terminal node -- the nodes at the base of the tree -- would contain a single class. 

The results of the application of measures of node impurity can be seen below in two plots. 

```{r iris_tree_plot, echo=FALSE,  fig.cap ='Decision tree on _iris_ dataset', cache=TRUE, message=FALSE, warning=FALSE, comment=NA, results='asis' } 

    par(mfrow = c(1,2), xpd=NA)
    plot(iris_tree, main = 'tree recursive partitioning of iris')
    text(iris_tree, cex=0.8, use.n=TRUE)
    table(iris$Species) # is data.frame with 'Species' factor
     iS <- iris$Species == "setosa"
     iV <- iris$Species == "versicolor"
     op <- par(bg = "bisque")
     matplot(c(1, 8), c(0, 4.5), type =  "n", xlab = "Length", ylab = "Width",
             main = "Petal and Sepal Dimensions in Iris Blossoms")
     matpoints(iris[iS,c(1,3)], iris[iS,c(2,4)], pch = "sS", col = c(2,4))
     matpoints(iris[iV,c(1,3)], iris[iV,c(2,4)], pch = "vV", col = c(2,4))
     legend(1, 4, c("    Setosa Petals", "    Setosa Sepals",
                    "Versicolor Petals", "Versicolor Sepals"),
            pch = "sSvV", col = rep(c(2,4), 2))
```

In Figure \ref{iris_tree_plot}, the plot on the left shows the decision tree and the plot on the right shows just _setosa_ and _versicolor_  plotted by petal and sepal widths and lengths.  As the plot on the right shows, most of the measurements are well clustered. Only the _setosa_ petal lengths and widths seem to vary widely. All the other measurements are tightly bunched. This means that the decision tree shown on the left has little trouble classifying the irises. Decision trees are read from the top down, left to right. The top level of this tree can be read, for instance, as saying, if the length of petal is less 2.45, then the iris is _setosa_.

Like logistic regression models, artificial neural networks, support vector machines or any other machine learning technique, decision trees have certain qualities and logics, and they make sense of the worlds they encounter in terms of those qualities and logics. The recursive partitioning can split and sub-divide the common vector space to capture every minor difference between cases, and thereby achieve a perfect fit to the data. We have been discussed the node and tree impurity measures that guide splitting in trees. Although these splitting rules have strong statistical justifications, they do not at all eliminate the problem of instability in trees. For instance, they easily end up 'overfitting' the data. Overfitting is a problem for all machine learning techniques. Algorithms sometimes find it hard to know when to stop. During construction of a decision tree, various features in the data are split into smaller and smaller groups. ''The goodness of the split', wrote Breiman and co-authors, 'is defined to be the decrease in impurity' [@Breiman_1984, 25]. Under this definition of goodness, the terminal nodes or leaves of the tree can end up containing a single case, or a single class of cases. The decision tree respects the singularity of the individual case to such a degree that it sees differences everywhere. Driven too maximise the purity of the nodes it creates, it leans heavily on  data it has been trained on to see relevant similarities when fresh data appears. Trees that branch too much are too sensitive (or unstable), and need to be pruned. Such a model will almost always _overfit_, since slight variations in the values of variables in a fresh case are likely to yield widely differing predictions. In the terminology of machine learning, such a decision tree will have low bias but high variance. 

Much of the development of decision tree did not revolve around how to construct them, but how to limit their growth. An associated literature on _pruning_ decision trees using measures of tree complexity has also developed. As Hastie and co-authors put the problem in their account of classification and regression trees:

>How large should we grow the tree? Clearly a very large tree might overfit the data, while a small tree might not capture the important structure. Tree size is a tuning parameter governing the model’s complexity, and the optimal tree size should be adaptively chosen from the data. One approach would be to split tree nodes only if the decrease in sum-of-squares due to the split exceeds some threshold. This strategy is too short-sighted, however, since a seemingly worthless split might lead to a very good split below it. The preferred strategy is to grow a large tree $T_0$, stopping the splitting process only when some minimum node size (say 5) is reached. Then this large tree is pruned using cost-complexity pruning, which we now describe [@Hastie_2009, 307-308]

This procedure of growing a maximum decision tree, and then cutting back its branches using a cost-function is a further renovation of the decision tree device as a machine learner. The 'cost complexity pruning' reproduces the kinds of optimisation we have already discussed in relation to linear regression and logistic regression models. As in these techniques, the definition of a cost function controlling the 'complexity' of a tree -- how many branches and leaves/nodes it contains, combined with measures of how well it classifies or predicts -- opens up the possibility of iteratively testing different levels of tree growth against each other. 'We define the cost complexity criterion,' write Hastie and co-authors, as:

\begin {equation}
\label {eq:tree_model}
C_\alpha(T) = \sum_{m=1}^{|T|}N_mQ_m(T) + \alpha|T|
\end {equation}

The idea is 'to find, for each $\alpha$, the subtree $T_\alpha ⊆ T$ 0 to minimize $C_\alpha(T)$' [@Hastie_2009, 308]. For present purposes, we need only note that decision trees ($T$ in equation \ref{eq:tree_model})  are re-configured here as optimisation problems, in which the variation of a parameter ($\alpha$) allows minimization of a derived value (the cost $C_\alpha$). The shift from AID to CART moves across a discontinuity, in which the mechanism of the decision tree algorithm generates a different mode of enunciation. In that mode, machine learners stand in a different relation to classification, prediction and their situations. While the graphic form of the decision tree was, by virtue of the long-standing diagrammatic practice of tree-drawing, easy to interpret, observation of decision trees had no way of gauging the instability or variability of any given tree.  Hastie and co-authors write: 'one major problem with trees is their high variance. Often a small change in the data can result in a very different series of splits, making interpretation somewhat precarious. The major reason for this instability is the hierarchical nature of the process: the effect of an error in the top split is propagated down to all of the splits below it [@Hastie_2009, 312]. The very diagrammatic form that allows them to be observed and interpreted is also the source of their instability. 

## Hyper-planing or dimensional growth saves linear movement

'Instead of referring back to the synthesis or the unifying function of a subject,' writes Foucault, 'the various enunciative modalities manifest his dispersion. To the various statuses, the various sites, the various positions that he can occupy or be given when making a discourse. To the discontinuity of the planes from which he speaks' [@Foucault_1972, 54]. If we understand machine learners as enunciative modalities ( remembering that the concept of an enunciative modality concerns the dispersion and discontinuities that surround and actually support statements),  what does the transformation and re-modelling of the decision tree as classification and regression trees suggest? We could follow here the subsequent development of the decision tree and its more recent transmogrification into random forests [@Breiman_2001], but the dispersion of the machine learning subject can be understood from a different angle, that of the 'discontinuity of the planes from which he speaks'. Machine learners generate statements that classify, predict or estimate values of various kinds: 'a user who visited website X has a 0.75 probability of clicking on hyperlink Y.' Such statements derive, however from a quite complicated speaking position that does imply a subject position, and in particular, a subject who both makes the statement and carries out operations associated with it. These operations are more or less what I have been describing in the form of functions, optimisation techniques, probability distributions, and vectorising transforms of data. Nonetheless, already in the growing and pruning of the decision tree, and even more markedly in support vector machine and neural networks, a different kind of enunciative function can be discerned in which dispersion and discontinuity plays a much more powerful role. 

Take the case of the support vector machine. The second most highly cited reference in the last few decades of machine learning literature is a paper by Corinna Cortes and Vladimir Vapnik of AT & T Bell Labs in New Jersey, USA entitled 'Support Vector Networks' [@Cortes_1995].  Few women's names have appeared prominently in the machine learning literature. The computing science and statistics departments at Stanford and Berkeley, the laboratories at Los Alamos and AT&T Bell between the 1960s and the 1980s were, it seems, not overly popular or populated with women scientists and engineers. Some prominent machine learning researchers at the time of writing are women (I return to this in Chapter 8)l, but Cortes is perhaps the most visible, both as head of Google Research in New York (2014) and as recipient with Vapnik of an Association for Computing Machine award in 2008  for their work on the support vector machine algorithm.[^5.70]

[^5.70]: The support vector machine is distinctive in its mode of movement, and without being swamped by the technical details, we might grasp something of this from Vapnik's writings. Vapnik trained and worked of decades in the former USSR as a mathematician and statistician. His writings on the problems of pattern recognition contrast greatly with other engineers, statisticians and computer scientists  in their  robustly theoretical formalism. A highly cited 1971 publication with  Alexey Chervonenkis 'On the uniform convergence of relative frequencies of events to their probabilities' (published in Russian in 1968 ) [@Vapnik_1971] sets the tone of this work. In ensuing publications in Russian and then in English after Vapnik moved from Moscow to AT&T's New Jersey Bell Labs in 1990, Vapnik's work remains  mathematical and abstract. Although it pertains to 'learning machines,' machine here are understood in mathematically simply as 'the implementation of a set of functions' [@Vapnik_1999, 17]. The way that Vapnik develops a theory of learning owes little visible debt to actual attempts to work with data or experience in doing statistics in an particular domain. This contrasts greatly for instance with the work of statisticians like Breiman or Friedman or even computer scientists like Quinlan whose work lies much closer to fields of application.  Vapnik's work, like that of the Russian mathematician Andrey Kolmogorov he draws on, differs from many other contributions to machine learning partly by virtue of this formality and its efforts to derive insight into machine learning by theorising learning. The _Vapnik-Chervonenkis dimension_( VC dimension), a very widely used way of defining the capacity of a particular machine learning technique to recognise patterns in data dates from his work in the 1960s and underpins a general theory of 'learning.' Vapnik writes in 1995, 

> The VC dimension of the set of functions (rather than the number of parameters) is responsible for the generalization ability of learning machines. This opens remarkable opportunities to overcome the "curse of dimensionality [@Vapnik_1999, 83].


```{r pattern_recognition1, echo=FALSE, cache=TRUE, message=FALSE, warning=FALSE, comment=NA, size='smallsize', results='asis' } 
    library(xtable)
    dir = '../ml_lit/data/pattern_recognition_WOS'
    files <- dir(dir,full.names=TRUE, pattern='savedrecs.*')
    recs <-lapply(files, read.csv, sep='\t', header=TRUE, as.is=TRUE, quote='', row.names=NULL)  
    df <-do.call( rbind, recs )
    colnames(df)[1:52] <- colnames(df)[2:53]
    colnames(df)[1] <-'PT'
    df = df[order(df$TC, decreasing=TRUE),]
    res = xtable(label='table:svm_sorted', head(df[, c('TI', 'PY')], 20))
    print(res)
```

The rapid rise to popularity of the support vector machine can be seen in the machine research literature, very small slice of which appears in Table \ref{table:svm_sorted}. A substantial fraction of the overall research output is linked to this single technique. The influence of the technique  can also be seen in  adjacent and overlapping fields such as pattern recognition and data mining, where [@Cortes_1995] and similar papers rank near the top-cited papers. This kind of growth betokens high levels of interest, identification and investment on the part of the researchers, and presumably more widely. What is it about this technique?

If the shift from decision trees to classification tree (and then random forests) illustrate an increasingly dispersed enunciative modality in machine learning, one in which interpretation is replaced by optimisation (growth and pruning)  the support vector machine, demonstrates a different way of generating statements about differences. While the name is somewhat forbiddingly technical compared to more easily understood terms such as 'decision tree' or 'neural network,' the underlying intuition of the technique is much older, and can be found in the models developed by the British statistician R. A. Fisher. R.A Fisher developed the 'first pattern recognition algorithm' [@Cortes_1995, 273], the 'linear discriminator function' [@Fisher_1936],  to deal with problems of classification, and demonstrated its efficacy on the taxonomic problem of  discriminating W.E. Anderson's irises in the `iris` dataset (see above). In his 1936 article in the _Annual Review of Eugenics_, Fisher comments on similar classification work carried out in craniometry and other related settings: so-called 'disciminant functions' had been successfully used to distinguish populations. Fisher wrote: 'when two or more populations have been measured in several characters, ... special interest attaches to certain linear functions of measurements by which the populations are best disciminated' [@Fisher_1936, 179].  The route by which these long-standing disciminant functions were reconstructed during the 1990s in the form of the support vector machine is of interest, because it highlights shifts in practice that give rise to new statements about data, statements that can be glimpsed in table \ref{table:svm_} in the range of 'referentialities' running through the titles of the papers.[^5.21] These statements have a different domain associated with them, they open up a range of subject positions,  they lend themselves to use, variation and repetition. 

[^5.21]: Foucault uses the term 'referentialities' to describe what statements refer to: 

    >A statement is not confronted (face to face, as it were) by a correlate or the absence of a correlate - as a proposition has (or has not) a referent, or as a proper noun designates someone (or no one) . It is linked rather to a 'referential' that is made up not of ' things', 'facts', 'realities', or 'beings' , but of laws of possibility, rules of existence for the objects that are named, designated, or described within it, and for the relations that are affirmed or denied in it. The referential of the statement forms the place, the condition, the field of emergence, the authority to differentiate between individuals or objects, states of things and relations that are brought into play by the statement itself; it defines the possibilities of appearance and delimitation of that which gives meaning to the sentence, a value as truth to the proposition [@Foucault_1972, 91].


The support vector machine, as we will see, powerfully exemplifies a controlled form of expansion. In the abstract of their 1995 paper, Cortes and Vapnik briefly describe the support vector machine:

> The support-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed [@Cortes_1995, 273]

Note that this 'very high-dimension feature space' is explicitly made to support 'a linear decision surface,' but a decision surface that itself is the product of a non-linear mapping of the data.  The 'linear decision surface' is an old and familiar entity in statistics. It had been proposed by Fisher as a way of separating or disciminating populations (with all the resonances that the term 'discriminate' carry, especially given that Fisher was writing in 1936 in the _Annals of Eugenics_). As we have seen on several occasions, the common vector space invites a certain form of classification based on the search for the best line, the line of best fit, or the most discriminating line, the line that best divides things from each other. Linear regression is not called 'linear' for no reason. And Fisher's 'discriminant functions' were later called 'linear discriminant analysis' for the same reason: they divide the vector space into different regions ('decision regions') separated by 'decision boundaries' [@Alpaydin_2010, 53]. Clearly there is a massive idealisation of classification and prediction here, since in almost every domain, things do not lie neatly on either side of a straight lines or planes. This idealism of the perfect plane, however, is something that almost all machine learners are aware of and try to address. In Cortes and Vapnik's support vector machine, a 'linear decision surface is constructed' but in a new domain where differences exist less linearly. 

We should note a  discontinuity here: not all machine learning reduces the dimensionality of data. In certain cases, machine learners multiply dimensions in data in the name of differentiation, classification, and prediction. Many of the techniques that have accumulated or been gathered into machine learning do flatten variations and differences into lines and planes, but not always by reducing them. In fact, random forests, neural networks and support vector machines are all in their own way solid examples of a counter-movement  that maximises variety in the name of differentiation.[^5.22] The last three decades of research in machine learning, whether it has been primarily statistical, mathematical, or computational,  countenances  and addresses problems of referentiality through _dimensional expansion_.  Certain techniques, such as support vector machines, have  increased the dimensionality of the predictive  model in order to construct higher-order forms of linearity inside the ostensibly non-linear messiness of the data.   As Vapnik writes in the preface to the second edition of _The Nature of Statistical Learning Theory_ [@Vapnik_1999, vii], 'in contrast to classical methods of statistics where in order to control performance one decreases the dimensionality of a feature space, the SVM dramatically increases dimensionality and relies on the so-called large margin factor' (vii).

[^5.22]: Despite the in-principle commitment to any form of function, machine learning retains strong preferences for forms that can either be visualised on a plane (using the visual grammar of lines, dots, axes, labels, colours, shapes, etc), or can be computed in form of matrix or vectorised calculations focused on planes. Many of the techniques that grapple with complicated datasets seek to reduce their dimensionality so that lines, planes and regular curves can be applied to them: multi-dimensional scaling (MDS),  factor analysis, principal component analysis (PCA), or self-organising maps (SOM) are just a few examples of this. 


## Badly written numbers: digit recognition

> The rule of materiality that statements necessarily obey is therefore of the order of the institution rather than of the spatio-temporal localization ; it defines possibilities of reinscription and transcription (but also thresholds and limits) , rather than limited and perishable individualities [@Foucault_1972, 103]

Nearly all expositions of the support vector machine including [@Cortes_1995]  contrast clear-cut and indistinct separation. This is the problem that the support vector machine addresses -- how to model differences when differences are blurred. Since the 1930s, with linear discriminant analysis, the idea of a separating line or plane has ordered movements through data. In the form of the support vector machine, this line begins to blur.   An oft-repeated illustration of how the support vector machine traverses data appears in Cortes and Vapnik's initial publication simply entitled 'Support Vector Networks' [@Cortes_1995]. They demonstrate how the support vector machine classifies handwritten digits drawn from a dataset supplied by the US Postal Service [@LeCun_2012]. Like `iris`, the US Postal Service digits and a larger version from the US National Institute of Standard (NIST) are  standard machine learning dataset, frequently used to measure the performance of different learning algorithms. In contrast to _iris_, the US Postal Service Database is high dimensional. Each digit in the dataset is stored as 16x16 pixel element. Image classification typically treats each pixel as a feature  or variable in the input space. So each digit as represented by a 16x16 pixels amounts to a 256 dimensional input space. By comparison, `iris` has five dimensions. Unsurprisingly, there are also many more digits in the US Postal Service Database than in flowers in _iris_:10,000. The NIST dataset has around 70,000. Aside from this dimensional growth, the handwritten digits aptly convey the non-linearity of the classification problem. On the one hand, human can recognise handwritten digits fairly easily and with few errors. This is despite the many variations in handwriting that skew, morph and distort the ideal graphic forms of numbers. Differences in writing were not only an impetus for deconstructionist theory in the 1980-90s. They were also actively being modelled in machine learning. 

\begin{figure}
  \centering
      \includegraphics[width=0.9\textwidth]{figure/mnist_test6.jpg}
        \caption{MNIST Postal Digits}
  \label{fig:postal_digits}
\end{figure}

In their experiments with digit recognition (shown in figure \ref{fig:postal_digits}, Cortes and Vapnik contrast the error rates of decision trees (CART and C4.5), neural networks and the support vector machine working at various level of dimensionality. The handwritten MNIST also appear in _Elements of Statistical Learning_ , where they are used to compare the generalization error (see previous chapter) of a _k_ nearest neighbours, convolutional neural network, and a 'degree-9 polynomial' support vector machine  [@Hastie_2009, 408].

What about the handwritten digits attracts so many machine learning techniques? The logistics of the US Postal Service aside (since the MNIST datasets continue to be used by machine learners well after the problem of scrawl on letters has been sorted), the reiteration of the postal digits suggests that they become a field of objects that that become the subject of many relations, many co-existing and sometimes competing statements. The variations, the regularities, and the banal everydayness of these digits became during the 1980s and 1990s, then, furnish a kind of referential locus, whose existence as a facts, things or events in the world is less important than the relations of similarity and differences it poses. Foucault states that:

> the referential of the statement forms the place, the condition, the field of emergence, the authority to differentiate between individuals or objects, states of things and relations that are brought into play by the statement itself; it defines the possibilities of appearance and delimitation of that which gives meaning to the sentence, a value as truth to the proposition [@Foucault_1972, 91]

The field of digitals becomes a site of differentiation not only of digits -- the machine learners attempt to correctly classify the digits -- but of the authority of different machine learning techniques and approaches. They become ways of announcing and delimiting the authority, the knowledge claims or 'truth' associated with the machine. The many uses of the MNIST data documented by [@LeCun_2012] suggests something of the referential force of such datasets.

But our focus is not just on the way that a set of images of handwritten could become the site of differentiation and comparison of techniques. The techniques of the support vector machine themselves delimit the statements that can be made in relation to the data. This occurs in two different ways. On the one hand, the support vector machine develops Fisher's linear discriminant analysis since it searches for a separating hyperplane in the data. While  linear discriminant analysis constructs that hyperplane by finding the most likely boundary between classes based on all the data, the support vector machine searches for a hyperplane resting only on those cases in the data that lie near the boundary. It exploits the intuition that the best hyperplane separating different classes will run near to the cases -- the _support vectors_ --  that are most difficult to classify. Again, in contra-distinction to the $N=\forall{X}$ proposition, here the machine learner discards much of the data. As Alpayadin writes, 'we do not care about correctly estimating the densities [probability distributions of variables] inside class regions; all we care about is the correct estimation of the _boundaries_ between the class regions' [@Alpaydin_2010, 210]. 

```{r svm_margins, echo=FALSE, cache=TRUE, messages=FALSE, warnings=FALSE, fig.cap = 'Soft margins and the hyperplane'}
x = c(rnorm(50,5), rnorm(50,7))
y = c(rnorm(50,5), rnorm(50,10))
c = c(rep(TRUE, 50), rep(FALSE, 50))
df = data.frame(x,y,c)
plot(x,y)
abline(12, -0.5, lty=2)
abline(10, -0.5, lty=1)
abline(8, -0.5, lty=2)
```

Figure \ref{fig:svm_margins} can be found in many slight variations in the last two decades. In Figure \ref{fig:svm_margins}, the dotted lines represent a margin on either side of a hyperplane (the solid line).  The support vector machine finds the hyperplane for which that margin is greatest. Of all the slightly different planes that might run between the two classes shown in that figure, the maximum separating hyperplane will find the best separation even where the classes overlap. It explicitly entertains instances that lie on the wrong side of the separating hyperplane not as errors (as they would appear in most linear classifiers such as linear discriminant analysis and logistic regression), but as components of a 'soft margin' designed to accommodate inseparability and indistinctness. 

As well as cutting through accumulations of data to find only these vectors that support a maximal separating hyperplane, the support vector machine typically increases the dimensionality of the data in order to expand the range of shapes of decision surfaces. As Leo Breiman writes in his account of the development of the support vector machine, 

>In two-class data, separability by a hyperplane does not often occur. However, let us increase the dimensionality by adding as additional predictor variables all quadratic monomials in the original predictor variables; that is, all terms of the form xm1 xm2 . A hyperplane in the original variables plus quadratic monomials in the original variables is a more complex creature. The possibility of separation is greater. If no separation occurs, add cubic monomials as input features. If there are originally 30 predictor variables, then there are about 40,000 features if monomials up to the fourth degree are added. [@Breiman_2001a, 209]

This dimensional exuberance vastly expands the number of possible decision surfaces or hyperplanes that might be instituted in the vector space. The support vector machine, however, corrals and manages this massive and sometimes infinite expansion by only allowing this expansion to occur along particular lines marked out by _kernel functions_. [HERE] 

_kernelises_ the common vector space. Cortes and Vapnik describe this as a threefold achievement:

> The support-vector network combines 3 ideas: the solution technique from optimal hyperplanes (that allows for an expansion of the solution vector on support vectors), the idea of convolution of the dot-product (that extends the solution surfaces from linear to non-linear), and the notion of soft margins (to allow for errors on the training set) [@Cortes_1995, 290]

How does the support vector machine increase the dimensionality so drastically without either runnning into a computational deadlock or worse, massively overfitting the data, resulting in poor generalization?  The technique of 'mapping the input vectors into some high dimensional feature space $Z$' [@Cortes_1995, 274] has long standing roots in the field of pattern recognition.  Remember that the problem is how to deal with patterns of data that cannot be separated by a line or plane. The epistopological transformation constructs a new vector space based on a transformation of the  data, and then in that higher dimensional space, finding a (hyper)plane that best separates different classes of data (the optimal hyperplane). The synthetic dataset shown at the beginning of this chapter illustrates the visual problem. It is hard to see on the plot of that data where a line could be drawn that separates different coloured data points from each other. If they were transformed into a higher dimensional space, would a line be easier to draw? In principle it should be. This is a long-standing item of faith in pattern recognition ('the underlying justification can be found in _Cover's theorem_ on the separability of patterns; that is, a complex pattern classification problem case in a high-dimensional space is _more likely_ to be linearly separable than in a low dimensional space' [@Wu_2008, 42]). 


 They are mathematically complicated compared to decision trees or random forests. But Cortes and Vapnik highlight the growth in dimensionality introduced by the technique of the support vector in ways that I find evocative. They describe how the technique exponentially increases the dimensionality of the feature space and how the error rate on difficult to classify handwritten digits drops correspondingly. When the feature space has 256 dimensions (the given dimensions of the 16x16 pixel digits), the error rate is around 12%. As the dimensionality grows to 33,000, then a million, a billon, a trillion and so forth (up to $1 x 10^16$ dimensions), the error rate drops to just over 4%, which is close to the errors made by 'human performance' (2.5%) [@Cortes_1995, 288].


```{r svm-demo, echo=FALSE, cache=TRUE, message=FALSE, warning=FALSE, comment=NA, engine='python' } 
import numpy as np
import pylab as pl
import seaborn
from sklearn import svm, datasets

iris = datasets.load_iris()  
X = iris.data[:, :2] 
Y = iris.target

h = .02  # step size in the mesh

C = 1.0  # SVM regularization parameter
svc = svm.SVC(kernel='linear', C=C).fit(X, Y)
rbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=C).fit(X, Y)
poly_svc = svm.SVC(kernel='poly', degree=3, C=C).fit(X, Y)
lin_svc = svm.LinearSVC(C=C).fit(X, Y)

x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                     np.arange(y_min, y_max, h))

titles = ['SVC with linear kernel',
          'SVC with RBF kernel',
          'SVC with polynomial (degree 3) kernel',
          'LinearSVC (linear kernel)']


for i, clf in enumerate((svc, rbf_svc, poly_svc, lin_svc)):
    pl.subplot(2, 2, i + 1)
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    pl.contourf(xx, yy, Z, cmap=pl.cm.Paired)
    pl.axis('off')
    pl.scatter(X[:, 0], X[:, 1], c=Y, cmap=pl.cm.Paired)
pl.title(titles[i])

pl.savefig('figure/contour.pdf')
```

\begin{figure}
  \centering
      \includegraphics[width=1.0\textwidth]{figure/contour.pdf}
        \caption{SVM on iris}
  \label{fig:svm_iris}
\end{figure}

A part of me thinks that this justification for the support vector machine has to be right. It points to the importance of locality in pattern. Patterns inherently distribute differences. The form of movement engineered into various machine learning techniques grapple with the distribution of differences. But they do it in different ways. Sometimes that take for granted the possibility of separation in a pattern, as if the pattern itself was intrinsically divisible or cleavable along certain lines or contours. At other times, intrinsic inseparability is taken into account as part of the learning.  The power of support vector machine to do this is limited, but instructive. It can deal with various forms of inseparability and non-linearity by taking the difficult-to-classify boundary cases as the basis of the model. The so-called 'support vectors' lie on the decision hyperplane or surface, and effectively define its orientation and shape in the vector space of the data.  

## An illustration of a 'separable problem' and the 'maximum margin separating hyperplane.'

>The kernel trick is another commonly used technique to solve linearly inseparably problems. This issue is to define an appropriate kernel function based on the _inner product_ between the given data, as a nonlinear transformation of data from the input space to a feature space with higher (even infinite) dimension in order to make the problems linearly separable. The underlying justification can be found in _Cover's theorem_ on the separability of patterns; that is, a complex pattern classification problem case in a high-dimensional space is _more likely_ to be linearly separable than in a low dimensional space [@Wu_2008, 42].



>We can represent the optimization problem (12.9) and its solution in a special way that only involves the input features via inner products. We do this directly for the transformed feature vectors h(x i ). We then see that for particular choices of h, these inner products can be computed very cheaply [@Hastie_2009, 423]

## Propagating or forwards and backwards in the network

## Conclusion

What do we learn from decision tree and their development into random forests, or from linear discriminant analysis and its re-formalization as the support vector machine? We could also have tracked the movement between the perceptron [@Rosenblatt_1958] and the 'deep learning' convolutional neural networks [@Hinton_2006] of more recent practice. Each of these shifts attests, I have been suggesting, to the emergence of a new enunciative mode in which dispersion and regularity, accumulation and rarity are no longer opposed, or treated as contradictory. A single decision tree becomes thousands in a random forest. A relatively small number of dimensions in the common vector space becomes potentially infinite in the kernelisation practiced in support vector machines. Models that sought to encompass or fit everything in the data, including the outliers, within a single probability distribution instead dwell on the difficult-to-classify, the erroneous or borderline instances amidst the massive normalized accumulations of event.

This is a change in what counts as knowledge. The recent machine learners can be studied, I have been suggesting, as forms of discursive practice, as elements in a power-laden positivity, that changes knowledge in specific, actually quite narrow, but nevertheless far-reaching ways. The interpretability of a decision tree cascades into the statistical regularities associated with a random forest of many thousands of trees. The separating lines and planes that allow linear models to become classifiers in the 'classic' techniques such as linear discriminant analysis find themselves displaced into hyper-planes, into newly constructed and sometimes inordinately-dimensioned feature spaces that can only be traversed by virtue of the kernel functions, and their computationally tractable inner products. These transformations are not necessarily scientific, although various sciences make great use of them (as the following chapters will explore), but they do diagram  thresholds of epistemologization and formalization in practice.  We saw in the case of the MNIST postal digits that a field of objects can furnish referentials for an enunciative mode, and that these referentials construct levels on which different machine learners move transversally. Although neural networks, support vector machines and decision trees have quite different ways of partitioning, separating or propagating differences, they together generate a group space that can attract and accumulate propositions. This group space, despite its commonalities, is not an homogeneous field. It does not have the coherence of a science, it uses different systems of formalization (the cross-entropy measures of the decision tree, the gradient descent of the neural network, the dual form Lagrange multipliers of the support vector machine), and disperses in different ways across knowledge practice (the decision tree with its commercial uptake in data mining versus the support vector machine's heavy use in image recognition and classification). 

How is machine learning knowledge? Not as science? 

## References



