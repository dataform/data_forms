# 5. Splitting, propagating and hyper-planing: patterns of movement in data 

The contemporary form of knowledge as pattern-finding 
Finding patterns in data
Movements in machine learning: patterns of generalization
Dimensional exuberance and pattern recognition in movement

```{r echo=FALSE} 
library(knitr)
opts_chunk$set(results='asis', message=FALSE, warnings=FALSE, cache=TRUE)
```
## todo

- diagrams -- the rpart diagram and trees; iris version?
- what happens in code here -- the inner product; the recursive, the backprop algorithm ... 
- what happens to knowledge -- knowledge understood in Foucaultean sense as savoir;  the discursive practices that criss words and things

## structure

- decision tree, neural network and svm -- 1960 - 2000

## Introduction

We have seen the emergence of the common vector space and its epistopological transformations, the multiplication of functions and their entwined partial observers, and then the re-distribution of probability in statistical devices that both populate world and transform machine learners into populations. Across the vectors, functions, and distributions a  diagram of  machine learning weaves and knots many points of emergence, continuity and conjunction. Formidable accumulations of infrastructure, devices and expertise pivot around machine learning (as we will see in Part II). On several occasions problems of the density and mass of techniques, research literature, algorithms and code have appeared. What in this diagram and in the forest-like growth of techniques, projects, applications and proponents allows us to make sense of its dynamism? Via three highly developed and heavily used machine learners  -- decision trees, neural networks, and support vector machines  -- that more or less mesmerised the machine learning scientific literature between 1980-2000, some relatively novel and somewhat discontinuous forms of movement into and through data were initiated. These movements, which we might characterise as _splitting_, _propagating_ and _hyper-planing_ -- continue to fiercely animate machine learners in producing newer techniques (random forests, deep belief nets), intensifying their application, and perhaps more importantly, re-configuring what counts as knowledge.

These three techniques have not been chosen randomly. They appear in the machine learning scientific publications of the last three decades as the most important novelties of these decades. They also, as we will see, loom large in various contemporary enunciations of machine learning as a way of knowing (for instance, in the popular machine learning  books such as _Machine Learning for Hackers_[@Conway_2012] or _Doing Data Science_ [@Schutt_2013]). The machine learning literature bristles with references to papers in statistics, computer science, mathematics, artificial intelligence and a swathe of related scientific fields. It ingests substantial supplies of knowledge and practice coming from the social sciences, insurance and actuarial practice, and marketing research. A very rough citation analysis of the research literature indicates that  the top 20 most cited publications in the field refer either to decision trees [@Quinlan_1986; @Breiman_1984], support vector machines [@Vapnik_1999; @Cortes_1995],  machine learning pedagogy [@Mitchell_1997], an early textbook written by a computer scientist; Witten, a textbook and software package on data mining using Java [@Witten_2005]; a textbook on pattern recognition dating from the 1970s [@Duda_2012], a tutorial on an error control technique (ROC - Receiver Operating Characteristics, first developed by the US military during WWII) and somewhat lower, another well-known textbook, this time on neural networks [@Bishop_2006]. While I do not place too much weight on citation counts, since they develop for different reasons over time and display discipline-specific tendencies, the spectrum of references here show a variety of investments in machine learning. They range from highly theoretical general accounts of the nature of machine learning (Mitchell, Duda) to detailed concrete implementations of techniques (Witten). Some focus on very specific techniques such as decision trees (Breiman, Quinlan) or support vector machines (Cortes) or neural networks (Bishop).

## Splitting or recursive binary partitioning

> Mastering the details of tree growth and management is an excellent way to understand the activities of learning machine generally [@Malley_2011, 118].

```{r aid, echo=FALSE, cache=TRUE, message=FALSE, warning=FALSE, comment=NA , fig.cap='Early uses of the Automatic Interaction Detector'} 
    library(xtable)
    aid_df = '../ml_lit/data/morgan_sonquist_WOS'
    files <- dir(aid_df,full.names=TRUE)
    recs <-lapply(files, read.csv, sep='\t', header=TRUE, as.is=TRUE, quote='', row.names=NULL)  
    aid_df <-do.call( rbind, recs )
    colnames(aid_df)[1:52] <- colnames(aid_df)[2:53]
    colnames(aid_df)[1] <-'PT'
    aid_df$TI = tolower(aid_df$TI)
    aid_df = aid_df[order(aid_df$PY, decreasing=FALSE),]
   print(xtable(head(label="References to Morgan and Sonquist's Automatic Interaction Detector", aid_df[,c('TI', 'PY' ,'TC')],10)))
```

Work on classification and regression techniques using decision tree goes back to the  early 1960s when social scientists James Morgan and John Sonquist at the University of Michigan's Institute for Social Research were attempting to analyse increasingly large social survey datasets [@Morgan_1963].  As Dan Steinberg describes in his brief history of decision trees [@Steinberg_2009, 180], the  'automatic interaction detector' (AID) as it was known, sought to automate the practice of data analysts looking for interactions between different variables. The variety and sheer optimism of subsequent applications of these prototype decision tree techniques  is striking.  In the 1960s and 1970s, papers that drew on the AID paper or use AID techniques can be found, as table \ref{table:aid} shows, in education, politics, economics, population control, advertising, mass media and family planning. 

A decade after initial work, the AID was the object of trenchant criticism by statisticians and others. Writing in the 1970s, statisticians in the bebavioural sciences such as Hillel Einhorn at the University of Chicago castigated the use of such techniques.   The criticisms stemmed partly from  a general distrust of 'purely empirical methods':

> The purely empirical approach is particularly dangerous in an age when computers and packaged programs are readily available, since there is temptation to substitute immediate empirical analysis for more analytic thought and theory building. It is also probably too much to hope that a majority of researchers will take the time to find out how and why a particular program works. The chief interest will continue to be in the output-the results-with as little delay as possible [@Einhorn_1972, 368]

Einhorn discusses AID alongside other  techniques such as factor analysis and multi-dimensional scaling (both still widely used) before concluding 'it should be clear that proceeding without a theory and with powerful data analytic techniques can lead to large numbers of Type I errors' [@Einhorn_1972, 378]. His specific objections to AID are  particularly focused on the problematic power of the technique: 'it may make sense out of "noise"' (369). Consequently, researchers easily misuse the technique: they 'overfit' the data, and do not pay enough attention to issues of validation (369-370). Most of these criticisms can be seen as expressing conventional statistical reservations. Similarly the British marketing researcher Peter Doyle, criticising the use of AID in assessing store performance and site selection by operation researchers, complained that searching for patterns in data using small data sets was bound to lead to spurious results and the decision trees, although intuitively appealing (that is, they could be easily interpreted), were afflicted with arbitrariness: 'a second variable may be almost as discriminating as the one chosen, but if the program is made to split on this, quite a different tree occurs' [@Doyle_1973, 465-466].  These objections and resistances to early decision trees echo today in discussions around pattern recognition, knowledge discovery and data-mining in science and commerce.    The problem of what computers do to the analysis of empirical data is long-standing. 

As Einhorn expected, it was too much to hope that all researchers would take time to investigate how a particular program works. Some researchers however did take time, years in fact, to investigate how decision trees work. As a result, writing in 2008, Hastie, Tibshirani and Friedman, who could hardly by accused of not understanding decision trees,  happily recommend decision trees as the best off-the-shelf classifier:  'of all the well-known learning methods, decision trees comes closest to meeting the requirements for serving as an off-the-shelf procedure for data-mining' [@Hastie_2009, 352]. We might wonder here, however, whether they damn with faint praise, since 'off-the-shelf' suggests pre-packaged, and commodified, and the term 'data-mining' itself is not without negative connotations. As for its commercial realities, in 2013, Salford Systems, the purveyors of the leading contemporary commercial decision tree software, `CART`, could claim:

 > CART is the ultimate classification tree that has revolutionized the entire field of advanced analytics and inaugurated the current era of data mining. CART, which is continually being improved, is one of the most important tools in modern data mining. Others have tried to copy CART but no one has succeeded as evidenced by unmatched accuracy, performance, feature set, built-in automation and ease of use. [Salford Systems](http://www.salford-systems.com/products/cart)

What happened between 1973 and 2013?  Decision trees somehow travelled from the statistically murky waters of the social science departments and business schools in the early 1970s to inaugurate the 'current era of datamining' (which the scientific literature suggests starts in the early 1990s). This was not only a commercial innovation. As the earlier citation from U.S. National Institutes of Health biostatisticians Malley, Malley and Pajevic indicates, decision trees now enjoy high regard even in biomedical research, a setting where statistical rigour is highly encouraged for life and death reasons. The happy situation of decision trees four decades on suggests some kind of threshold was crossed in which the epistemological, statistical, or algorithmic ('built-in automation') power of the technique altered substantially. Decision trees  emblematise the emerging, evolving flow of techniques associated with machine learning.  

## 1984: Recursive partitioning re-implemented

The third author of _Elements of Statistical Learning_, Jerome Friedman, worked at the  U.S. Department of Energy's Stanford Linear Accelerator during the late 1970s. Friedman  was  instrumental in rescuing  decision trees from ignominy of instability they had entered in the late  1960s. The reorganisation and statistical retrofitting of the decision tree was not a single or focused effort. During the 1980s, statisticians such  as Friedman and Leo Breiman renovated the decision tree as a statistical tool [@Breiman_1984] at the same time as  computer scientists such as  Ross Quinlan in Sydney were re-implementing decision trees guided by an  artificial intelligence-based formalisation as rule-based induction technique [@Quinlan_1986].[^5.1] This uneasy parallel effort between computer science and statistics  still characterises machine learning today. Both statisticians and computer scientists  do and use the same techniques, but often with the computer scientists focusing on optimisation and algorithmic automation and the statisticians inventing novel formalizations. The fateful embrace of statistics and computer science, the disciplinary binary that vectorizes machine learning, has been generative in the retrieval of the decision tree. 

[^5.1]: Quinlan's papers  and book on versions of the decision tree (`ID3` and `c4.5`) are both amongst the top ten the most highly cited references in the machine literature itself. Google Scholar reports over 20,00 citations of the Quinlan's book _C4.5: Programs for Machine Learning_ [@Quinlan_1993]. Several years ago,  `C4.5` was voted the top data mining algorithm [@Wu_2008]. 

An initial symptom of the transformation of the technique appears in a name. The term 'decision tree,' although still widely used in the research literature and machine learner  parlance was replaced by 'classification and regression tree' during the late 1970s and 1980s. The terms 'classification and regression tree' is sometimes contracted to 'CART,' and that term strictly speaking refers to a computer program described in [@Breiman_1984] as well as the title of that highly-cited monograph. As we have seen in previous chapters, regression and classification designate the two main sides of machine learning practice, and their concatenation with 'tree' attests to the renovation of the 1960-70's style approaches.

As usual, the implementation of machine learning techniques in `R`  offers one path into  their wider circulation. This path, by virtue of `R`'s statistical provenance perhaps favours the statistical side of decision tree practice, but that nevertheless has certain forensic virtues not offered by commercial or closed-source software often produced by computer scientists.  In this case, the name of   one long-standing and widely-used `R` package itself attests to something: `rpart` is a contraction of 'recursive partitioning' and this term generally describes how the decision tree algorithm works [@Therneau_2015].  'Cart,' on the other hand, is a registered trademark of Salford Systems, the software company mentioned above, who sell the leading commercial implementation of classification and regression trees. Hence, the `R` package `rpart` cannot call itself the more obvious name `cart, ` and instead invokes the algorithmic process it relies on: recursive partitioning. (Other `R` packages such as `party` [@Hothorn_2006] and `tree` [@Ripley_2014] also use recursive partitioning, but with various tweaks and optimisations that I leave aside here.)


[HERE -- bring in the examples] 

## Propagating or forwards and backwards in the network

## Hyper-planing saves linear movement


