
\chapter{The problem of pattern: accretion and movement in data }
\label{ch:pattern}

The contemporary form of knowledge as pattern-finding 
Finding patterns in data
Movements in machine learning: patterns of generalization
Dimensional exuberance and pattern recognition in movement


```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=12, tidy=TRUE, fig.height=8, 
                      echo=FALSE,  results='hide', warning=FALSE, message=FALSE, dev='pdf')
library(RSQLite)
con <- dbConnect(RSQLite::SQLite(),'../ml_lit/all_refs.sqlite3')
res = dbGetQuery(con, statement ="select * from basic_refs limit 10;")
library(ggplot2)
library(stringr)
library(xtable)
options(xtable.comment = FALSE)
simpleCap <- function(x) {
    s <- strsplit(x, " |-")[[1]]
    return (paste(toupper(substring(s, 1,1)), tolower(substring(s, 2)), sep="", collapse=" "))
  }
```

> Algorithms for pattern recognition were therefore from the very beginning associated with the construction of linear decision surfaces [@Cortes_1995, 273-4]

> No statement can have a latent existence [@Foucault_1972, 16]

We have seen the emergence of the common vector space and its epistopic transformations, the multiplication of functions and their associated partial observers, and then the re-distribution of probability that transform machine learners into populations of error-sensitive learners. Across the vectors, functions and distributions, a  diagram of  machine learning weaves and knots many points of emergence, continuity and conjunction. Formidable accumulations of infrastructure, devices and expertise accrete around machine learning. Already on several occasions problems of the density and mass of techniques, research literature, algorithms and code have appeared. What in this diagram and in the forest-like growth of techniques, projects, applications and proponents allows us to make sense of the dynamics of this accumulation? Via three highly developed and heavily used machine learners  -- decision trees, neural networks, and support vector machines  -- that more or less mesmerised the machine learning scientific literature between 1980-2000, some relatively novel and somewhat discontinuous forms of movement into and through data were initiated. \index{machine learner!decision tree} \index{machine learner!support vector machine} \index{machine learner!neural net}   These diagrammatic movements, which we might characterise as _splitting_, _propagating_ and _hyper-planing_ -- animate machine learners in producing newer techniques (random forests, deep belief nets), intensifying their application, and perhaps more importantly, re-configuring what counts as pattern. \index{pattern|see{diagram}} \index{diagram!movement} 

```{r pattern}
    q = "select * from basic_refs where TI like '%pattern%' or DE like '%pattern%'"
    q2 = "select TI, PY, TC from basic_refs where TC >500"
    res2 = dbGetQuery(con, q2)
    head(res2[order(res2$TC, decreasing=TRUE),])

    res = dbGetQuery(con, q)
    dim(res)
    hist(res$PY)
    table(res$PY)
```

The decision tree, neural net and support vector machine appear in the machine learning scientific publications of the last three decades (1980-2010) as important novelties. They also, I will suggest, embody a new enunciative modality \index{enunciative modality} in  which regularity and rarity, accumulation and sparsity do not exist in tension or opposition. Practically,  they  loom large in various contemporary accounts of machine learning as a way of knowing (for instance, in the popular machine learning  books such as _Machine Learning for Hackers_ [@Conway_2012] or _Doing Data Science_ [@Schutt_2013]). The machine learning research published in statistics, computer science, mathematics, artificial intelligence and a swathe of related scientific fields bristles with references to decision trees, support vector machine and neural networks. (As we will see, these techniques themselves ingest substantial supplies of knowledge and practice produced by  social sciences, insurance and actuarial practice, and marketing research.) The top 20 most cited publications in the field include  decision trees Ross Quinlan and Leo Breiman's papers [@Quinlan_1986; @Breiman_1984], Vladimir Vapnik and Corinna Cortes' support vector machines papers [@Vapnik_1999; @Cortes_1995],  an early textbook written by a computer scientist on machine learning [@Mitchell_1997], a textbook and software package on data mining using Java [@Witten_2005]; a textbook on pattern recognition dating from the 1970s [@Duda_2012], a tutorial on an error control technique (ROC - Receiver Operating Characteristics, first developed by the US military during WWII) and somewhat lower, another well-known textbook, this time on neural networks and pattern recognition [@Bishop_2006]. 

A striking _positivity_ takes shape across these differences. \index{positivity} I understand positivity partly in the sense of 'positivism,' an epistemological stance widely shared amongst machine learners  that views knowledge as deriving from observation.[^5.01]  The re-configuration of tensions between sparsity and accumulation, between rarity and commonality can also be understood in the sense of positivity proposed by Michel Foucault in _The Archaeology of Knowledge_ [@Foucault_1972], a book whose somewhat a-subjective approach to knowledge unexpectedly resonates with much that takes place in machine learning. Foucault theorises statements as elements _posited_ or deposited in a relatively sparse space of transformations. These transformations generate what can be thought and done within a particular discursive formation. Their positivity arises from dispersion:

>To describe a group of statements not as the closed, plethoric totality of a meaning, but as an incomplete, fragmented figure; to describe a group of statements not with reference to the interiority of an intention, a thought, or a subject, but in accordance with the dispersion of an exteriority; to describe a group of statements, in order to rediscover not the moment or the trace of their origin, but the specific forms of an accumulation, is certainly not to uncover an interpretation, to discover a foundation, or to free constituent acts; nor is it to decide on a rationality, or to embrace a teleology. It is to establish what I am quite willing to call a positivity [@Foucault_1972, 125]. \index{Foucault, Michel!on positivity} \index{Foucault, Michel!on statements}  


Foucault frequently refers to positivity in those passages where he discusses  how practices develop and how statements relating to those practices take shape. The situations in which he invokes positivity involve accumulations or plethora of statements whose coherence or unity does not lie in any origin, intention, rationality, subjectivity, ideology or teleology. Like mineral strata, positivities entail accumulations without deriving them from underlying unities. Foucault's acceptance of 'positivity,' in spite of the risk of attracting criticism, pivots on his interest in, and practical commitment to, understanding accumulations and uneven distributions. A positivity is a way of describing 'a group of statements' that may be quite dispersed, discontinuous, and unevenly distributed because they make up 'a population of events in the space of discourse in general' [@Foucault_1972, 27]. If _The Archaeology of Knowledge_ is a book concerned with the tissue of relations between things, what is said, what is done and what counts as knowledge (scientific or not) in practice, then the persistent and at times forceful reiteration of _dispersion_ and _discontinuity_ might seem strangely counterintuitive. But Foucault's insistence on discourse as something irreducible to logic, language, intention, rationality, experience, historical origin or narrative, and as the way in which 'statements' ('a graph, a growth curve, an age pyramid, a distribution cloud are all statements' (82)), strategies, concepts, objects, archives and knowledges come together in 'discursive formations' is an attempt to resist any resort to a hidden order or concealed origin that critical thought could uncover or interpret. \index{positivity!of accumulation}

Positivity as accumulation helps make sense of a field such as machine learning whose many different forms, techniques, statements, institutions, values and associations often grapple with problems of much data, many variables, many possible models and a sense of an elusive but vital order or pattern discoverable amidst accumulation. Rather than seeing pattern as something discovered in data, the notion of positivity suggests we should examine the diagrammatic operations that configure the practice of machine learning, giving rise to a field of objects and subject positions.   The three machine learners that anchor this chapter are at once perhaps the most distinctive data mining, pattern recognition and predictive modelling achievements of the late twentieth century (at least judging by the citational and implementational interest they attract). They different greatly in how they move through data. At certain times, they come together (for instance, in machine learning competitions discussed in chapter \ref{ch:subjects}; or in certain formalizations such as machine learning  theory or in graphs of the bias-variance decomposition discussed in chapter \ref{ch:probability}; or in the pedagogy of machine learning discussed in chapter \ref{ch:diagram}). We might understand their differences in terms of 'enunciative modalities' [@Foucault_1972, 54] \index{enunciative modality}  -- that is, in terms of the different ways in which they give rise to statements. Every  machine learner generates statements, but from different places, by somewhat different individuals, and from the different situations they that are able 'to occupy in relation to the various domains or groups of objects' [@Foucault_1972, 52].[^5.02]

## Splitting or recursive binary partitioning and the growth of trees

> Mastering the details of tree growth and management is an excellent way to understand the activities of learning machine generally [@Malley_2011, 118].

Decision trees promise an understanding of machine learning.   The enunciative function of the decision tree partly relates to the observability and comprehensibility of machine learning. As we will see, not all machine learners readily supports observation or comprehension. The cost of comprehensibility, however, is a certain highly restricted movement through the data, a movement that has been difficult to stabilise and control. As _Elements of Statistical Learning_ puts it: 'tree-based methods partition the feature space into a set of rectangles, and then fit a simple model (like a constant) in each one. They are conceptually simple yet powerful' [@Hastie_2009, 305].

\begin{figure}
  \centering
      \includegraphics[width=0.9\textwidth]{figure/recursive_partitioning.pdf}
        \caption{Recursive partitioning of the feature space}
  \label{fig:rpart}
\end{figure}

Tree-based methods are supervised learners as they require the data to either be labelled with a class or to have some outcome value. \index{machine learning!supervised} The variable types in the feature space (or common vector space of the data) \index{feature space|see{common vector space}} can be mixed. Because the method cuts the common vector space into a tiled surface, the features or data variables can be continuous or discontinuous. The 'simple models' that tree methods construct each inhabit one of the rectangular regions or partitions of the feature space. In Figure \ref{fig:rpart}, the different regions or partitions produced by a decision tree are labelled $R1, R2$ etc. 

```{r aid, echo=FALSE, results='asis', cache=TRUE } 
    q = "select * from basic_refs where anchor = 'morgan_sonquist_WOS'"
    res = dbGetQuery(con, q)
    res$TI = sapply(res$TI, simpleCap) 
    tab  = xtable(res[1:10,c('TI', 'PY' ,'TC')], caption="References to Morgan and Sonquist's Automatic Interaction Detector", label='table:aid_citation')
    print(tab, type='latex')
   
```

Work on classification and regression techniques using decision trees goes back to the  early 1960s when social scientists James Morgan and John Sonquist at the University of Michigan's Institute for Social Research were attempting to analyse increasingly large social survey datasets [@Morgan_1963].  As Dan Steinberg describes in his brief history of decision trees [@Steinberg_2009, 180], the  'automatic interaction detector' (AID) as it was known, sought to automate the practice of data analysts looking for interactions between different variables. The variety and sheer optimism of subsequent applications of these prototype decision tree techniques  is striking.  In the 1960s and 1970s, papers that drew on the AID paper or use AID techniques can be found, as table \ref{table:aid_citation} shows, in education, politics, economics, population control, advertising, mass media and family planning. \index{machine learner!decision tree!history of} \index{machine learner!automatic interaction detector}

A decade after initial work, the AID was the object of trenchant criticism by statisticians and others. Writing in the 1970s, statisticians in the behavioural sciences such as Hillel Einhorn at the University of Chicago castigated the use of such techniques.   The criticisms stemmed from  a general distrust of 'purely empirical methods', and scepticism focused on their positivity:

> The purely empirical approach is particularly dangerous in an age when computers and packaged programs are readily available, since there is temptation to substitute immediate empirical analysis for more analytic thought and theory building. It is also probably too much to hope that a majority of researchers will take the time to find out how and why a particular program works. The chief interest will continue to be in the output-the results-with as little delay as possible [@Einhorn_1972, 368]

Einhorn discusses AID alongside other  techniques such as factor analysis and multi-dimensional scaling (both still widely used) before concluding 'it should be clear that proceeding without a theory and with powerful data analytic techniques can lead to large numbers of Type I errors' [@Einhorn_1972, 378]. His statistical objections to AID are  particularly focused on the problematic power of the technique: 'it may make sense out of "noise"' (369). Consequently, researchers easily misuse the technique: they 'overfit' the data, and do not pay enough attention to issues of validation (369-370). Similarly the British marketing researcher Peter Doyle, criticising the use of AID in assessing store performance and site selection by operations researchers, complained that searching for patterns in data using data sets was bound to lead to spurious results and the decision trees, although intuitively appealing (that is, they could be easily interpreted), were afflicted with arbitrariness: 'a second variable may be almost as discriminating as the one chosen, but if the program is made to split on this, quite a different tree occurs' [@Doyle_1973, 465-466].[^5.03] \index{operations research!use of decision trees} \index{machine learner!overfitting} Most of these criticisms can be seen as expressing conventional statistical caution in response to threats to validity. 

## 1984: Cutbacks on recursive partitioning

As Einhorn expected, it was too much to hope that all researchers would take time to investigate how a particular program works. Some researchers however did take time, years in fact, to investigate how decision trees work.  Writing around 2000, Hastie, Tibshirani and Friedman, who could hardly by accused of not understanding decision trees,  happily recommend decision trees as the best 'off-the-shelf' classifier:  'of all the well-known learning methods, decision trees comes closest to meeting the requirements for serving as an off-the-shelf procedure for data-mining' [@Hastie_2009, 352]. We might wonder here, however, whether they damn with faint praise, since 'off-the-shelf' suggests pre-packaged, and commodified, and the term 'data-mining' itself is not without negative connotations. As for its commercial realities, in 2013, Salford Systems, the purveyors of the leading contemporary commercial decision tree software, `CART`, could claim:

 > CART is the ultimate classification tree that has revolutionized the entire field of advanced analytics and inaugurated the current era of data mining. CART, which is continually being improved, is one of the most important tools in modern data mining. Others have tried to copy CART but no one has succeeded as evidenced by unmatched accuracy, performance, feature set, built-in automation and ease of use. [Salford Systems](http://www.salford-systems.com/products/cart) \index{software!CART}

What happened between 1973 and 2013?  Decision trees somehow grew out of  the statistically murky waters of social science departments and business schools in the early 1970s to inaugurate the 'current era of datamining' (which the scientific literature indicates starts in the early 1990s). This was not only a commercial innovation. As the earlier citation from U.S. National Institutes of Health biostatisticians Malley, Malley and Pajevic indicates, decision trees enjoy high regard even in biomedical research, a setting where statistical rigour is highly valued for life and death reasons. The happy situation of decision trees four decades on suggests some kind of threshold was crossed in which the epistemological, statistical, or algorithmic ('built-in automation') power of the technique altered substantially. \index{machine learner!decision tree!in medicine}


## Conclusion

What do we learn from decision tree and their development into random forests, or from linear discriminant analysis and its re-formalization as the support vector machine? We could also have tracked the movement between the perceptron [@Rosenblatt_1958] and the 'deep learning' convolutional neural networks [@Hinton_2006] of more recent practice (see chapter \ref{ch:subjects} . Each of these shifts attests, I have been suggesting, to the emergence of a new enunciative mode in which dispersion and regularity, accumulation and rarity are no longer opposed, or treated as contradictory. A single decision tree becomes thousands in a random forest. A relatively small number of dimensions in the common vector space becomes potentially infinite in the kernelisation practiced in support vector machines. Models that sought to encompass or fit everything in the data, including the outliers, within a single probability distribution instead dwell on the difficult-to-classify, the erroneous or borderline instances amidst the massive normalized accumulations of event.

What counts as knowledge today?  The recent machine learners can be studied, I have been suggesting  as elements in a power-laden positivity, \index{positivity!of knowledge} that changes knowledge in specific, actually quite narrow, but nevertheless far-reaching ways. The interpretability of a decision tree cascades into the statistical regularities associated with a random forest of many thousands of trees. The separating lines and planes that allow linear models to become classifiers in the 'classic' techniques such as linear discriminant analysis find themselves displaced into hyper-planes, into newly constructed and sometimes inordinately-dimensioned feature spaces that can only be traversed by virtue of the kernel functions, and their computationally tractable inner products. These transformations are not necessarily scientific, although various sciences make great use of them (as the following chapters will explore), but they do diagram  thresholds of epistemologization and formalization in practice.  We saw in the case of the MNIST postal digits that a field of objects can furnish referentials for an enunciative mode, and that these referentials construct levels on which different machine learners move transversally. Although neural networks, support vector machines and decision trees have quite different ways of partitioning, separating or propagating differences, they together generate a group space that can attract and accumulate propositions. This group space, despite its commonalities, is not an homogeneous field. It does not have the coherence of a science, it uses different systems of formalization (the cross-entropy measures of the decision tree, the gradient descent of the neural network, the dual form Lagrange multipliers of the support vector machine), and disperses in different ways across knowledge practice (the decision tree with its commercial uptake in data mining versus the support vector machine's heavy use in image recognition and classification). 


[^5.01]: See [@Kitchin_2014] for a survey of the major epistemological commitments in 'Big Data' discourse. These largely derived from predictive and inferential techniques of machine learners. 

[^5.02]:  While Foucault tends to retain a decoupled subject-object relation in the production of statements,  I tend to see these enunciative modalities as distributed across people and things. As always, machine learner is a composite term for this distribution. 

[^5.03]:  These objections and resistances to early decision trees echo today in discussions around pattern recognition, knowledge discovery and data-mining in science and commerce.    The problem of what computers do to the analysis of empirical data is long-standing. 
