PT	AU	BA	BE	GP	AF	BF	CA	TI	SO	SE	BS	LA	DT	CT	CY	CL	SP	HO	DE	ID	AB	C1	RP	EM	RI	OI	FU	FX	CR	NR	TC	Z9	PU	PI	PA	SN	EI	BN	J9	JI	PD	PY	VL	IS	PN	SU	SI	MA	BP	EP	AR	DI	D2	PG	WC	SC	GA	UT
B	Agarwal, D		Han, J; Wah, BW; Raghavan, V; Wu, X; Rastogi, R		Agarwal, D			An empirical Bayes approach to detect anomalies in dynamic multidimensional Arrays	Fifth IEEE International Conference on Data Mining, Proceedings			English	Proceedings Paper	5th IEEE International Conference on Data Mining	NOV 27-30, 2005	Houston, TX	IEEE Comp Soc, TCII, IEEE Comp Soc, TCPAMI, IBM Res, Knowledge & Informat Syst, Web Intelligence Consortium, Univ Louisiana Lafayette, Ctr Adv Comp Studies, Amer Discount ADS Inc, Univ Houston, Dept Comp Sci, Elder Res Inc			MULTIPLE COMPARISONS	We consider the problem of detecting anomalies in data that arise as multidimensional arrays with each dimension corresponding to the levels of a categorical variable. In typical data mining applications, the number of cells in such arrays are usually large. Our primary focus is detecting anomalies by comparing information at the current time to historical data. Naive approaches advocated in the process control literature do not work well in this scenario due to the multiple testing problem - performing multiple statistical tests on the same data produce excessive number of false positives. We use an Empirical Bayes method which works by fitting a two component gaussian mixture to deviations at current time. The approach is scalable to problems that involve monitoring massive number of cells and fast enough to be potentially useful in many streaming scenarios. We show the superiority of the method relative to a naive "per component error rate" procedure through simulation. A novel feature of our technique is the ability to suppress deviations that are merely the consequence of sharp changes in the marginal distributions. This research was motivated by the need to extract critical application information and business intelligence from the daily logs that accompany large-scale spoken dialog systems deployed by AT&T. We illustrate our method on one such system.	AT&T Labs Res, Florham Pk, NJ 07932 USA	Agarwal, D (reprint author), AT&T Labs Res, 180 Pk Ave, Florham Pk, NJ 07932 USA.						Babcock B., 2002, PODS; BENJAMINI Y, 1995, J ROY STAT SOC B MET, V57, P289; Box G., 1970, TIME SERIES ANAL; Carlin BP, 2000, BAYES EMPIRICAL BAYE; DOUGLAS S, 2004, INTERSPEECH 2004; DUMOUCHEL W, 1999, P 5 ACM SIGKDD INT C, P6, DOI 10.1145/312129.312184; DUMOUCHEL W, 1988, BAYESIAN STAT, V3; DUNCAN DB, 1965, TECHNOMETRICS, V7, P171, DOI 10.2307/1266670; Ganti V, 2002, SIGKDD EXPLORATIONS, V3, P1, DOI [10.1145/507515.507517, DOI 10.1145/507515.507517]; Genovese C., 2003, BAYESIAN STAT, V7, P145; Good P., 2000, PERMUTATION TESTS PR; Gopalan R, 1998, J AM STAT ASSOC, V93, P1130, DOI 10.2307/2669856; Kifer D., 2004, P 30 INT C VER LARG, P180, DOI 10.1016/B978-012088469-8/50019-X; SCOTT JG, 2003, EXPLORATION ASPECTS; Shaffer JP, 1999, J STAT PLAN INFER, V82, P197, DOI 10.1016/S0378-3758(99)00042-7; Yi B.-K., 2000, Proceedings of 16th International Conference on Data Engineering (Cat. No.00CB37073), DOI 10.1109/ICDE.2000.839383; Zhu Y., 2002, P 28 INT C VER LARG, P358, DOI DOI 10.1016/B978-155860869-6/50039-1	17	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA			0-7695-2278-5				2005							26	33		10.1109/ICDM.2005.22		8	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Computer Science	BDS21	WOS:000235162400004	
B	Almonayyes, A		Zhao, MS; Shi, ZZ		Almonayyes, A			Categorizing fanatic texts by integrating explanation patterns with naive Bayes classifier	PROCEEDINGS OF THE 2005 INTERNATIONAL CONFERENCE ON NEURAL NETWORKS AND BRAIN, VOLS 1-3			English	Proceedings Paper	International Conference on Neural Networks and Brain (ICNN&B 2005)	OCT 13-15, 2005	Beijing, PEOPLES R CHINA	China Neural Networks Council, IEEE Computat Intelligence Soc, Beijing Chapter, Chinese Inst Elect, Chinese Assoc Artificial Intelligence				Exploratory data analysis over foreign language text presents virtually untapped opportunity. This work incorporates Naive Bayes classifier with Case-Based Reasoning in order to classify and analyze Arabic texts related to fanaticism. The Arabic vocabularies are converted to equivalent English words using conceptual hierarchy structure. The understanding process operates at two phases. At the first phase, a discrimination network of multiple questions is used to retrieve explanatory knowledge structures each of which gives an interpretation of a text according to a particular aspect of fanaticism. Explanation structures organize past documents of fanatic content. Similar documents are retrieved to generate additional valuable information about the new document. In the second phase, the document classification process based on Naive Bayes is used to classify documents into their fanatic class. The results show that the classification accuracy is improved by incorporating the explanation patterns with the Naive Bayes classifier.	Kuwait Univ, Dept Math & Comp Sci, Kuwait 13060, Kuwait	Almonayyes, A (reprint author), Kuwait Univ, Dept Math & Comp Sci, POB 5969, Kuwait 13060, Kuwait.	sami@mcs.sci.kuniv.edu.kw					AAMODT A, 1994, AI COMMUN, V7, P39; ALMONAYYES A, 1997, 5 GERM WORKSH CAS BA; ALMONAYYES A, 2001, J EXPT THEORETICAL A; COHEN W, 1996, P NAT C ART INT, P249; Hastie T., 2001, SPRINGER SERIES STAT; Jackson P., 2002, NATURAL LANGUAGE PRO, V5; KOLODNER J, 1996, MAKING IMPLICIT EXPL, P349; Kolonder J., 1993, CASE BASED REASONING; Lewis D., 1994, 3 ANN S DOC AN INF R, P81; Manning Christopher D., 2001, FDN STAT NATURAL LAN; McCallum A, 1998, AAAI 98 WORKSH LEARN; Mitchell T.M., 1997, MACHINE LEARNING; SCHANK R, 1990, 2 NW U I LEARN SCI; XIA Y, 2005, LNCS, V3406, P718; ZELIKOVITZ K, 2000, P 17 INT C MACH LEAR, P1183; ICAIL 2003 JUN 24 28, P224	16	0	0	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA			0-7803-9422-4				2005							1279	1283				5	Computer Science, Artificial Intelligence	Computer Science	BEB63	WOS:000236575101138	
S	Andres, J; Navarro, JR; Juan, A; Casacuberta, F		Marques, JS; PerezdelaBlanca, N; Pina, P		Andres, J; Navarro, JR; Juan, A; Casacuberta, F			Word translation disambiguation using multinomial classifiers	PATTERN RECOGNITION AND IMAGE ANALYSIS, PT 2, PROCEEDINGS	LECTURE NOTES IN COMPUTER SCIENCE		English	Article; Proceedings Paper	2nd Iberian Conference on Pattern Recongnition and Image Analysis	JUN 07-09, 2005	Estoril, PORTUGAL	Fund Oriente, Fund Cienc Tecnol, HP Portugal, Inst Syst & Robotics, Int Assoc Pattern Recognit				This work focuses on a hybrid machine translation system from Spanish into Catalan called SisHiTra. In particular, we focus on its word translation disambiguation module, which has to decide on the correct translation of each ambiguous input word in accordance with its context. We propose the use of statistical pattern recognition techniques for this task and, in particular, multinomial Naive Bayes text classifiers. Extensive empirical results on the use of these classifiers are presented, in which the influence of the window (context) size and parameter smoothing are carefully studied.	Univ Politecn Valencia, Dept Sistemas Informat & Computac, Inst Tecnol Informat, Valencia, Spain	Andres, J (reprint author), Univ Politecn Valencia, Dept Sistemas Informat & Computac, Inst Tecnol Informat, Valencia, Spain.						ALFONS J, 2002, P 2 INT WORKSH PATT, P200; JOSE R, 2004, LECT NOTES ARTIF INT, P349; Och Franz Josef, 2000, ACL00, P440; ROCHE E, 1995, COMPUT LINGUIST, V21, P227; TOMAS J, 2002, P 2 INT WORKSH PATT, P213; VILAR D, 2004, P 2 INT WORKSH PATT	6	0	0	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		3-540-26154-0	LECT NOTES COMPUT SC			2005	3523						622	629				8	Computer Science, Artificial Intelligence; Computer Science, Theory & Methods	Computer Science	BCM80	WOS:000230027000076	
S	Beardslee, EA; Trafalis, TB		Zanasi, A; Brebbia, CA; Ebecken, NFF		Beardslee, EA; Trafalis, TB			Data mining methods in a metrics-deprived inventory transactions environment	Data Mining VI: Data Mining, Text Mining and Their Business Applications	WIT TRANSACTIONS ON INFORMATION AND COMMUNICATION TECHNOLOGIES		English	Proceedings Paper	6th Conference on Data Mining - Text Mining and Their Business Applications	MAY 25-27, 2005	Skiathos, GREECE	Wessex Inst Technol, Wessex Inst Technol, Transact Informat & Commun Technol		data mining; inventory; supply chain; Naive Bayes; SVM; prediction; stock shortage; transaction; time-series; rare event		Over the past decade, several data mining techniques have come into use within a variety of data intensive fields. Mining transaction data to discover interesting patterns or help forecast future rare events based upon historical records is of key interest. Among the classic problems facing any supply chain is that of determining how much of a given item to keep on the shelf of a warehouse or a store, and at what level to initiate replenishment of this supply. This problem has been well examined and extensively researched. However, applying current data mining techniques to support the determination of effective inventory stock levels and economic re-order points has yet to be explored. In this paper, Naive Bayes, Bayes Network, SVM, MLP, Logistic, and J48 decision tree data mining techniques are compared with respect to their effectiveness when tasked with predicting stock shortage conditions indicated by the reduced transaction history of a bench stock inventory. These comparisons have been made using training and test data sets drawn from a set of 1.8 million transactions. Each method is applied to the same set of transaction data that spans three years of bench stock inventory activity. This set of transactions contains orders, receipts, stock outages, and stock shortages but it does not include issue transactions from the inventory. Since the time of issue or removal of the item from the inventory is not known, determining the expected demand becomes very difficult. The effectiveness of the data mining predictive capability is measured with respect to the accuracy of the prediction and the lead-time provided to the inventory planner for the given prediction. The results show similar performance among most of the data. mining algorithms using "actual event" prediction evaluation criteria.	Univ Oklahoma, Dept Ind Engn, Norman, OK 73019 USA	Beardslee, EA (reprint author), Univ Oklahoma, Dept Ind Engn, Norman, OK 73019 USA.						Brause R., 1999, Proceedings 11th International Conference on Tools with Artificial Intelligence, DOI 10.1109/TAI.1999.809773; DHOND A, 2000, P 6 ACM SIGKDD INT C; PIRAMUTHU S, 1999, P 32 ANN HAW INT C S; Vilalta R., 2002, Proceedings 2002 IEEE International Conference on Data Mining. ICDM 2002, DOI 10.1109/ICDM.2002.1183991; Wai-Ho Au, 2003, IEEE Transactions on Evolutionary Computation, V7, DOI 10.1109/TEVC.2003.819264; WANG Z, 2002, P 2002 IEEE INT C DA, P490; WILSON AJ, 2004, P 12 ICC CER BREAD C; Witten I.H., 1999, DATA MINING PRACTICA; Yohda M., 2002, Proceedings 2002 IEEE International Conference on Data Mining. ICDM 2002, DOI 10.1109/ICDM.2002.1184052	9	0	0	WIT PRESS	SOUTHAMPTON	ASHURST LODGE, SOUTHAMPTON SO40 7AA, ASHURST, ENGLAND	1746-4463		1-84564-017-9	WIT TRANS INFO COMM			2005							513	522				10	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Computer Science	BCQ48	WOS:000230756400051	
B	Benbrahim, H; Bramer, M		Bramer, M; Coenen, F; Allen, T		Benbrahim, H; Bramer, M			Neighbourhood exploitation in hypertext categorization	RESEARCH AND DEVELOPMENT IN INTELLIGENT SYSTEMS XXI	BCS CONFERENCE SERIES		English	Proceedings Paper	24th SGAI International Conference on Innovative Techniques and Applications of Artificial Intelligence (AI-2004)	DEC 12-15, 2004	Cambridge, ENGLAND	British Comp Soc Specialist Grp Artificial Intelligence				The exponential growth of the web has led to the necessity to put some order to its content. The automatic classification of web documents into predefined classes, that is hypertext categorization, came to elevate humans from that task. The extra information available in a hypertext document poses new challenges for automatic categorization. HTML tags and linked neighbourhood all provide rich information for hypertext categorization that is not available in traditional text classification. This paper looks at (i) which extra information hidden in HTML tags and linked neighbourhood pages to take into consideration to improve the classification task, and (ii) how to deal with the high level of noise in linked pages. A hypertext dataset and four well-known learning algorithms (Naive Bayes, K-Nearest Neighbour, Support Vector Machine and C4.5) were used to exploit the enriched text representation. The results showed that the clever use of the information in linked neighbourhood and HTML tags improved the accuracy of the classification algorithms.	Univ Portsmouth, Dept Comp Sci & Software Engn, Portsmouth PO1 2UP, Hants, England	Benbrahim, H (reprint author), Univ Portsmouth, Dept Comp Sci & Software Engn, Portsmouth PO1 2UP, Hants, England.						APTE C, 1994, ACM T INFORM SYST, V12, P233, DOI 10.1145/183422.183423; BENBRAHIM H, 2004, IN PRESS IFIP WORLD; BENSAID A, 1999, INT J INTELLIGENT SY; BHARAT K, 1998, P 7 WORLD WIDE WEB C; CHAKRABARTI S, 1997, VLDB             AUG; Chakrabarti S., 1998, P ACM SIGMOD INT C M, P307, DOI 10.1145/276304.276332; Chen H., 2000, ACM INT C HUM FACT C, P145; JOACHIMS T, P ECML 98 10 EUR C M, P137; JOACHIMS T, 2001, INT C MACH LEARN ICM; Lewis D., 1992, P SPEECH NAT LANG WO, P212, DOI 10.3115/1075527.1075574; OH H, 2000, P 23 ACM SIGIR C ATH; YANG Y, 1999, J INFORMATION RETRIE; Yang YM, 2002, J INTELL INF SYST, V18, P219, DOI 10.1023/A:1013685612819	13	0	0	SPRINGER	NEW YORK	233 SPRING STREET, NEW YORK, NY 10013, UNITED STATES			1-85233-907-1	BCS CONFERENCE S			2005							258	268		10.1007/1-84628-102-4_19		11	Computer Science, Artificial Intelligence	Computer Science	BBP52	WOS:000226889800019	
S	Cano, A; Castellano, JG; Masegosa, AR; Moral, S		Godo, L		Cano, A; Castellano, JG; Masegosa, AR; Moral, S			Selective Gaussian naive Bayes model for Diffuse Large-B-Cell Lymphoma classification: Some improvements in preprocessing and variable elimination	SYMBOLIC AND QUANTITATIVE APPROACHES TO REASONING WITH UNCERTAINTY, PROCEEDINGS	LECTURE NOTES IN COMPUTER SCIENCE		English	Article; Proceedings Paper	8th European Conference on Symbolic and Quantitative Approaches to Reasoning with Uncertainty	JUL 06-08, 2005	Barcelona, SPAIN	Artificial Intelligence Res Inst, Spanish Sci Res Council, Generalitate Catalunya, Minist Educ Cienc, MusicStrands Inc			GENE-EXPRESSION DATA; CLASS PREDICTION; CANCER	In this work, we present some significant improvements for for feature selection in wrapper methods. They are two: the first of them consists in a proper preordering of the feature set; and the second one consists in the application of an irrelevant feature elimination method, where the irrelevance condition is subjected to the partial selected feature subset by the wrapper method. We validate these approaches with the Diffuse Large B-Cell Lymphoma subtype classification problem and we show that these two changes are an important improvement in the computation cost and the classification accuracy of these wrapper methods in this domain.	Univ Granada, Dept Comp Sci & Artificial Intelligence, Granada 18071, Spain	Cano, A (reprint author), Univ Granada, Dept Comp Sci & Artificial Intelligence, Granada 18071, Spain.	acu@decsai.ugr.es; jgc@decsai.ugr.es; andrew@decsai.ugr.es; smc@decsai.ugr.es	Cano Utrera, Andres/D-9971-2012; Garcia Castellano, Francisco Javier/D-9984-2012; Moral Callejon, Serafin/C-2416-2012; Masegosa, Andres R./K-5408-2012	Cano Utrera, Andres/0000-0001-7650-1221; Masegosa, Andres R./0000-0003-1333-9858			AHA DW, 1994, AAAI 94 WORKSH CAS B; Alizadeh AA, 2000, NATURE, V403, P503, DOI 10.1038/35000501; ALLMUALLIM H, 1991, 9 NAT C ART INT, P547; ANDO T, 2002, GENE INFORMATICS, V13, P278; CANO A, 2004, P 2 EUR WORKSH PROB, P33; Cowell R. G., 1999, PROBABILISTIC NETWOR; Domingos P, 1997, MACH LEARN, V29, P103, DOI 10.1023/A:1007413511361; Duda R., 1973, PATTERN CLASSIFICATI; Friedman N, 2000, J COMPUT BIOL, V7, P601, DOI 10.1089/106652700750050961; Friedman N, 1997, MACH LEARN, V29, P131, DOI 10.1023/A:1007465528199; Golub TR, 1999, SCIENCE, V286, P531, DOI 10.1126/science.286.5439.531; Hand D.J., 1981, DISCRIMINATION CLASS; Hsu C.N., 2000, P 17 INT C MACH LEAR, P399; Inza I, 2004, ARTIF INTELL MED, V31, P91, DOI 10.1016/j.artmed.2004.01.007; Inza I, 2002, J INTELL FUZZY SYST, V12, P25; John G., 1994, P 11 INT C MACH LEAR, P121; JOHN GH, 1995, P 11 C UNC ART INT, P338; Kittler J., 1978, Pattern Recognition and Signal Processing; Kohavi R, 1997, ARTIF INTELL, V97, P273, DOI 10.1016/S0004-3702(97)00043-X; Langley P., 1992, NAT C ART INT, P223; Langley P., 1994, P 10 C UNC ART INT, P399; LANGLEY P, 1994, AAAI 94 WORKSH CAS B; Li LP, 2001, BIOINFORMATICS, V17, P1131, DOI 10.1093/bioinformatics/17.12.1131; Rosenwald A, 2002, NEW ENGL J MED, V346, P1937, DOI 10.1056/NEJMoa012914; STONE M, 1997, J REAL STAT SOC, V38, P48; Wright G, 2003, P NATL ACAD SCI USA, V100, P9991, DOI 10.1073/pnas.1732008100; Zhang HP, 2003, P NATL ACAD SCI USA, V100, P4168, DOI 10.1073/pnas.0230559100	27	0	0	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		3-540-27326-3	LECT NOTES COMPUT SC			2005	3571						908	920				13	Computer Science, Artificial Intelligence	Computer Science	BCQ50	WOS:000230770500076	
S	Chen, L; Huang, J; Gong, ZH		Szczepaniak, PS; Kacprzyk, J; Niewiadomski, A		Chen, L; Huang, J; Gong, ZH			An anti-noise text categorization method based on support vector machines	ADVANCES IN WEB INTELLIGENCE, PROCEEDINGS	LECTURE NOTES IN COMPUTER SCIENCE		English	Article; Proceedings Paper	3rd International Atlantic Web Intelligence Conference (AWIC 2005)	JUN 06-09, 2005	Lodz, POLAND	Tech Univ Lodz, Inst Comp Sci, Polish Acad Sci, Syst Res Inst, Polish Cybernet Soc, Lodz Div, Polish Neural Networks Soc				Text categorization has become one of the key techniques for handling and organizing web data. Though the native features of SVM (Support Vector Machines) axe better than Naive Bayes' for text categorization in theory, the classification precision of SVM is lower than Bayesian method in real world. This paper tries to find out the mysteries by analyzing the shortages of SVM, and presents an anti-noise SVM method. The improved method has two characteristics: 1) It chooses the optimal n-dimension classifying hyperspace. 2) It separates noise samples by preprocessing, and trains the classifier using noise free samples. Compared with naive Bayes method, the classification precision of anti-noise SVM is increased about 3 to 9 percent.	Natl Univ Def Technol, Sch Comp Sci, Changsha 410073, Peoples R China	Chen, L (reprint author), Natl Univ Def Technol, Sch Comp Sci, Changsha 410073, Peoples R China.	chenlin@nudt.edu.cn; agnes_nudt@hotmail.com					ANDROUTSOPOULOS I, 2000, EVALUATION NAIVE BAY; Basu A, 2003, P 36 HAW INT C SYST; JOACHIMS T, 1997, P ECML 98 10 EUR C M; KIVEN J, 1995, C COMP LEARN THEOR; Lewis D., 1994, 3 ANN S DOC AN INF R, P81; Pearl J., 1988, PROBABILISTIC REASON; WEISS SM, 1999, IEEE INTELL SYST APP, P2; XU LF, 2003, COMPUTER SCI, V30; Zhou Z.-H., 2000, KNOWL INF SYST, V2, P115, DOI 10.1007/s101150050006	9	0	1	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		3-540-26219-9	LECT NOTES COMPUT SC			2005	3528						272	278				7	Computer Science, Artificial Intelligence	Computer Science	BCN66	WOS:000230302600043	
S	Civera, J; Cubel, E; Juan, A; Vidal, E		Marques, JS; PerezdelaBlanca, N; Pina, P		Civera, J; Cubel, E; Juan, A; Vidal, E			Different approaches to bilingual text classification based on grammatical inference techniques	PATTERN RECOGNITION AND IMAGE ANALYSIS, PT 2, PROCEEDINGS	LECTURE NOTES IN COMPUTER SCIENCE		English	Article; Proceedings Paper	2nd Iberian Conference on Pattern Recongnition and Image Analysis	JUN 07-09, 2005	Estoril, PORTUGAL	Fund Oriente, Fund Cienc Tecnol, HP Portugal, Inst Syst & Robotics, Int Assoc Pattern Recognit				Bilingual documentation has become a common phenomenon in many official institutions and private companies. In this scenario, the categorization of bilingual text is a useful tool, that can be also applied in the machine translation field. To tackle this classification task, different approaches will be proposed. On the one hand, two finite-state transducer algorithms from the grammatical inference domain will be discussed. On the other hand, the well-known naive Bayes approximation will be presented along with a possible modelization based on n-gram language models. Experiments carried out on a bilingual corpus have demonstrated the adequacy of these methods and the relevance of a second information source in text classification, as supported by classification error rates. Relative reduction of 29% with respect to the best previous results on the monolingual version of the same task has been obtained.	Univ Politecn Valencia, Dept Sistemas Informat & Computac, Valencia, Spain; Univ Politecn Valencia, Inst Tecnol Informat, Valencia, Spain	Civera, J (reprint author), Univ Politecn Valencia, Dept Sistemas Informat & Computac, Valencia, Spain.	jcivera@dsic.upv.es; ecubel@iti.upv.es; ajuan@dsic.upv.es; evidal@iti.upv.es					Brown P. F., 1993, Computational Linguistics, V19; Chen S. F., 1996, P 34 ANN M ASS COMP, P310, DOI 10.3115/981863.981904; CUBEL E, 2002, IIDSICB2301 U POL VA; GOLD EM, 1967, INFORM CONTROL, V10, P447, DOI 10.1016/S0019-9958(67)91165-5; Joachims T., 1998, P ECML 98 10 EUR C M, V1398, P137; JUAN A, 2001, WORKSH PATT REC INF; KNIGHT K, 1998, 3 C ASS MACH TRANSL, V1529, P421; Amengual J. C., 2000, Machine Translation, V15, DOI 10.1023/A:1011116115948; LLORENS D, 2000, THESIS I POLITECNICA; McCallum A, 1998, AAAI 98 WORKSH LEARN; Och Franz Josef, 2000, ACL00, P440; ONCINA J, 1993, IEEE T PATTERN ANAL, V15, P448, DOI 10.1109/34.211465; ONCINA J, 1996, ICGI BERL GERM, P301; PICO D, 2000, MACH LEARN, V44, P121; Vidal E, 1997, INT CONF ACOUST SPEE, P111, DOI 10.1109/ICASSP.1997.599563; VITERBI AJ, 1967, IEEE T INFORM THEORY, V13, P260, DOI 10.1109/TIT.1967.1054010; WITTEN IH, 1991, IEEE T INFORM THEORY, V37, P1085, DOI 10.1109/18.87000; Yang Y. M., 1999, INFORM RETRIEVAL, V1, P69, DOI 10.1023/A:1009982220290	18	0	0	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		3-540-26154-0	LECT NOTES COMPUT SC			2005	3523						630	637				8	Computer Science, Artificial Intelligence; Computer Science, Theory & Methods	Computer Science	BCM80	WOS:000230027000077	
B	Cruz, L; Perez, J; Pazos, RA; Landero, V; Alvarez, VM; Gomez, CG		Gelbukh, A; ReyesGarcia, CA		Cruz R., Laura; Perez O., Joaquin; Pazos R., Rodolfo A.; Landero N., Vanesa; Alvarez H., Victor M.; Gomez S., Claudia G.			Alternative strategies to explore the SNNB algorithm performance	MICAI 2006: FIFTH MEXICAN INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE, PROCEEDINGS			English	Proceedings Paper	5th Mexican International Conference on Artificial Intelligence (MICAI 2006)	NOV 13-17, 2006	Apizaco, MEXICO	SMIA, DGEST, INAOE, ITAM	Technol Inst Apizaco			Data mining is the process of extracting useful knowledge from large datasets. A sub-area of data mining is the classification that induces a set of models for predicting the label of the unknown class. The Naive Boyes classifier is simple, efficient and robust; its performance has been improved by some works, which focused on finding an instances subset in a conditional way and selecting the appropriate classifier with the highest probability. In this paper we propose to modify the Selective Neighborhood based Naive Bayes (SNAB) algorithm, using and combining other distance measurements, instance organization, instance space search and model selection. The proposed combinations are aimed at exploring the classifying accuracy of the SNNB algorithm. Experimental results show that the best strategy found (using 26 datasets from the UCI repository) won in 15 cases and only lost in 3 cases.	Technol Inst Cd Madero, Mexico City, DF, Mexico	Cruz, L (reprint author), Technol Inst Cd Madero, Mexico City, DF, Mexico.	lcruzreyes@prodigy.net.mx; jperez@cenidet.edu.mx; pazos@cenidet.edu.mx; landerov76@yahoo.com.mx; mantorvicuel@hotmail.com; cggs7@hotmail.com					Chawla NV, 2002, J ARTIF INTELL RES, V16, P321; Coffman Jr EG, 1997, APPROXIMATION ALGORI, P46; Friedman N, 1996, PROCEEDINGS OF THE THIRTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND THE EIGHTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE, VOLS 1 AND 2, P1277; Kohavi R., 1996, P 2 INT C KNOWL DISC, P202; NITESH C, 2003, ICML WORKSH LEARN IM; Perez J, 2004, LECT NOTES COMPUT SC, V3059, P417; XIE ZP, 2002, 6 PAC AS C KNOWL DIS; Zheng Z, 1998, LECT NOTES COMPUTER, V1398, P196, DOI [10.1007/BFb0026690, DOI 10.1007/BFB0026690]; Zheng ZJ, 2000, MACH LEARN, V41, P53, DOI 10.1023/A:1007613203719	9	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA			978-0-7695-2722-2				2005							187	196				10	Computer Science, Artificial Intelligence	Computer Science	BFT14	WOS:000244481600020	
S	Deng, ZH; Tang, SW; Zhang, M		Wang, L; Jin, Y		Deng, ZH; Tang, SW; Zhang, M			An efficient text categorization algorithm based on category memberships	FUZZY SYSTEMS AND KNOWLEDGE DISCOVERY, PT 1, PROCEEDINGS	LECTURE NOTES IN ARTIFICIAL INTELLIGENCE		English	Article; Proceedings Paper	2nd International Conference on Fuzzy Systems and Knowledge Discovery	AUG 27-29, 2005	Changsha, PEOPLES R CHINA	Xiangtan Univ, IEEE Circuits & Syst Soc, IEEE Computat IntelligenceSoc, IEEE Control Syst Soc, Int Neural Network Soc, European Neural Network Soc, Chinese Assoc Artifiical Intelligence, Japanese Neural Network Soc, Int Fuzzy Syst Assoc, Asia-Pacific Neural Network Assembly, Fuzzy Math & Syst Assoc Chine, Hunan Comp Federat				Text Categorization is the process of automatically assigning predefined categories to free text documents. Although there have existed a large number of text classification algorithms, most of them are either inefficient or too complex. In this paper, we propose the concept of category memberships, which stand for the degrees that words belonging to categories. Based on category memberships, a simple but efficient algorithm is presented. To evaluate our new algorithm, we have conducted experiments using Newsgroup_18828 text collection to compare it with Naive Bayes and k-NN. Experimental results show that our algorithm outperforms Naive Bayes and k-NN if a suitable category membership function is adopted.	Peking Univ, Sch Elect Engn & Comp Sci, Natl Lab Machine Percept, Beijing 100871, Peoples R China	Deng, ZH (reprint author), Peking Univ, Sch Elect Engn & Comp Sci, Natl Lab Machine Percept, Beijing 100871, Peoples R China.	zhdeng@cis.pku.edu.cn; tsw@pku.edu.cn; mzhang@db.pku.edu.cn					APTE C, 1988, P C AUT LEARN DISC W; Church K. W., 1989, P ACL, V27, P76; Dasarathy B., 1991, MCGRAWHILL COMPUTER; DUNNING TE, 1993, COMPUTATIONAL LINGUI, V1, P61; Fano R., 1961, TRANSMISSION INFORM; Joachims T., 1998, P 10 EUR C MACH LEAR, P137; McCallum A., 1998, AAA 98 WORKSH LEARN; MLADENIC D, 1998, WORKING NOTES LEARNI; NG HT, 1997, 20 ANN INT ACM SIGIR, P67; PORTER MF, 1980, PROGRAM-AUTOM LIBR, V14, P130, DOI 10.1108/eb046814; Ricardo B. Y., 1999, MODERN INFORM RETRIE; SCHAPIRE R, 2000, MACH LEARN, V2, P135; van Rijsbergen C. J., 1979, INFORM RETRIEVAL; Yang Y., 1999, J INFORMATION RETRIE, V1, P67; Yang Y., 1994, 17 ANN INT ACM SIGIR, P13; Yang Y., 1999, 22 ANN INT ACM SIGIR, P42; Yang Y., 1997, P 14 INT C MACH LEAR, P412	17	0	0	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		3-540-28312-9	LECT NOTES ARTIF INT			2005	3613						374	382				9	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Computer Science	BDA15	WOS:000232217900048	
B	Doan, S; Horiguchi, S		Ishikawa, M; Hashimoto, S; Paprzycki, M; Barakova, E; Yoshida, K; Koppen, M; Corne, DW; Abraham, A		Doan, S; Horiguchi, S			An efficient feature selection using multi-criteria in text categorization	HIS'04: Fourth International Conference on Hybrid Intelligent Systems, Proceedings			English	Proceedings Paper	4th International Conference on Hybrid Intelligent Systems (HIS 04)	DEC 05-08, 2004	Kitakyushu, JAPAN	IEEE Comp Soc, Computat Intelligence Soc, IEEE, Syst Man, & Cybernet Soc, BMFSA, SOFT, Int Fuzzy Syst Assoc, City Kitakyushu, World Federat Soft Comp				Text categorization is a problem of assigning a document into one or more predefined classes. One of the most interesting issues in text categorization is feature selection. This paper proposes a novel approach in feature selection based on multi-criteria ranking of features. Based on a threshold value for each criterion, a new procedure for feature selection is proposed and applied to a text categorization. Experiments dealing with the Reuters-21578 benchmark data and the naive Bayes algorithm show that the proposed approach outperforms performances in compare to conventional feature selection methods.	Japan Adv Inst Sci & Technol, Grad Sch Informat Sci, Tatsunokuchi, Ishikawa 9231292, Japan	Doan, S (reprint author), Japan Adv Inst Sci & Technol, Grad Sch Informat Sci, Asahidai 1-1, Tatsunokuchi, Ishikawa 9231292, Japan.						AMALDI E, 1998, THEORETICAL COMPUTER, P237; Baker L. D., 1998, Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, DOI 10.1145/290941.290970; Blum AL, 1997, ARTIF INTELL, V97, P245, DOI 10.1016/S0004-3702(97)00063-5; Dumais S., 1998, Proceedings of the 1998 ACM CIKM International Conference on Information and Knowledge Management, DOI 10.1145/288627.288651; Kohavi R, 1997, ARTIF INTELL, V97, P273, DOI 10.1016/S0004-3702(97)00043-X; LEWIS D, 1991, THESIS GRADUATE SCH; Liu H., 1998, FEATURE SELECTION KN; McCallum A. K., 1996, BOW TOOLKIT STAT LAN; Mladenic D., 1998, P 10 EUR C MACH LEAR, P95; SALTON G, 1975, COMMUN ACM, V18, P613, DOI 10.1145/361219.361220; Sebastiani F, 2002, ACM COMPUT SURV, V34, P1, DOI 10.1145/505282.505283; Yang Y. M., 1999, INFORM RETRIEVAL, V1, P69, DOI 10.1023/A:1009982220290; Yang Y., 1997, P 14 INT C MACH LEAR, P412	13	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA			0-7695-2291-2				2005							86	91				6	Computer Science, Artificial Intelligence	Computer Science	BBW91	WOS:000228181100014	
S	Doan, S; Horiguchi, S		Abraham, A; Dote, Y; Furuhashi, T; Koppen, M; Ohuchi, A; Ohsawa, Y		Doan, S; Horiguchi, S			Multiple concept learning - A novel approach to feature selection in text categorization	Soft Computing as Transdisciplinary Science and Technology	ADVANCES IN SOFT COMPUTING		English	Proceedings Paper	4th IEEE International Workshop on Soft Computing as Transdisciplinary Science and Technology (WSTST 05)	2005	Muroran, JAPAN	IEEEE Syst Man & Cybernet Soc, World Federat Soft Comp, European Soc Fuzzy Log & Technol, Japan Soc Promot Sci, Soc Instrumentat & Control Engineers, Transdisciplinary Federat Sci & Technol, JSPS Int Meeting Series, Life Oriented Software Lab	Muroran Inst Technol	concept learning; text categorization; feature selection; machine learning		Concept learning is an interesting problem in machine learning and has many applications in real-world problems. This paper considers the multiple concept learning which are extended from binary concept learning. Our main contribution in this paper is to propose a new framework for multiple concept learning. To this end, two sparseness and semantic measures are proposed in order to characterize the scatter and the concentration of concepts in a system. Using both of these two measures, a general strategy of multiple concept learning is given and applied to feature selection in text categorization problem. The experimental results implemented to two benchmark datasets 20Newsgroups and Reuters-21578 show that our approach improve the performances using the Rochio and naive Bayes algorithms compared to conventional methods in the system.	Japan Advance Inst Sci & Technol, Grad Sch Informat Sci, Tatsunokuchi, Ishikawa 9231292, Japan	Doan, S (reprint author), Japan Advance Inst Sci & Technol, Grad Sch Informat Sci, Asahidai 1-1, Tatsunokuchi, Ishikawa 9231292, Japan.						Baker L. D., 1998, Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, DOI 10.1145/290941.290970; MICHALSKI R, 1983, MACHINE LEARNING ART, V2; Mitchell T.M., 1997, MACHINE LEARNING; Mladenic D., 1998, P 10 EUR C MACH LEAR, P95; Sebastiani F, 2002, ACM COMPUT SURV, V34, P1, DOI 10.1145/505282.505283; SLONIM J, 2001, P 23 EUR C INF RETR; THORNTON C, 2003, CONNECTIONISM CONCEP, V2, P181; Yang Y., 1997, P 14 INT C MACH LEAR, P412	8	0	0	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	1615-3871		3-540-25055-7	ADV SOFT COMP			2005							1043	1052				10	Computer Science, Artificial Intelligence; Computer Science, Information Systems; Computer Science, Interdisciplinary Applications	Computer Science	BCV56	WOS:000231416200107	
B	Fan, W; Davidson, I; Zadrozny, B; Yu, PS		Han, J; Wah, BW; Raghavan, V; Wu, X; Rastogi, R		Fan, W; Davidson, I; Zadrozny, B; Yu, PS			An improved categorization of classifier's sensitivity on sample selection bias	Fifth IEEE International Conference on Data Mining, Proceedings			English	Proceedings Paper	5th IEEE International Conference on Data Mining	NOV 27-30, 2005	Houston, TX	IEEE Comp Soc, TCII, IEEE Comp Soc, TCPAMI, IBM Res, Knowledge & Informat Syst, Web Intelligence Consortium, Univ Louisiana Lafayette, Ctr Adv Comp Studies, Amer Discount ADS Inc, Univ Houston, Dept Comp Sci, Elder Res Inc				A recent paper categorizes classifier learning algorithms according to their sensitivity, to a common type of sample selection bias where the chance of an example being selected into the training sample depends on its feature vector x but not (directly) on its class label y. A classifier learner is categorized as "local" if it is insensitive to this type of sample selection bias, otherwise, it is considered "global". In that paper the trite model is not clearly distinguished from the model that the algorithm outputs. In their discussion of Bayesian classifiers, logistic regression and hard-margin SVMs, the true model (or the model that generates the trite class label for every example) is implicitly assumed to be contained in the model space of the learner and the trite class probabilities and model estimated class probabilities are assumed to asymptotically converge as the training data set size increases. However in the discussion of naive Bayes, decision frees and soft-margin SVMs, the model space is assumed not to contain the true model, and these three algorithms are instead argued to be "global learners". We argue that most classifier learners may or may not be affected by sample selection bias; this depends on the dataset as well as the heuristics or inductive bias implied by the learning algorithm and their appropriateness to the particular dataset.	IBM Corp, TJ Watson Res Ctr, Hawthorne, NY 10532 USA	Fan, W (reprint author), IBM Corp, TJ Watson Res Ctr, Hawthorne, NY 10532 USA.		Yu, Philip/A-2815-2012				YING S, 1999, TUTORIAL LOGISTIC RE; Zadrozny B., 2004, P 21 INT C MACH LEAR	2	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA			0-7695-2278-5				2005							605	608				4	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Computer Science	BDS21	WOS:000235162400083	
B	Feng, SL; Manmatha, R			IEEE Computer Society	Feng, SL; Manmatha, R			Classification models for historical manuscript recognition	Eighth International Conference on Document Analysis and Recognition, Vols 1 and 2, Proceedings			English	Proceedings Paper	8th International Conference on Document Analysis and Recognition (ICDAR 2005)	AUG 29-SEP 01, 2005	Seoul, SOUTH KOREA	ABBYY Software House, BK 21 Sch Informat Technol KAIST, Elect & Telecommun Res Inst, Hitachi Cent Res Lab, IBM Corp, Int Assoc Pattern Recognit, Korea Adv Inst Sci & Technol, Korea Informat Sci Soc, Korea Sci & Engn Fdn, Minist Informat & Commun, Inst Informat Assessment, Microsoft				This paper investigates different machine learning models to solve the historical handwritten manuscript recognition problem. In particular, we test and compare support vector machines, conditional maximum entropy models and Naive Bayes with kernel density estimates and explore their behaviors and properties when solving this problem. We focus on a whole word problem to avoid having to do character segmentation which is difficult with degraded handwritten documents. Our results on a publicly available standard dataset of 20 pages of George Washington's manuscripts show that Naive Bayes with Gaussian kernel density estimates significantly outperforms the other models and prior work using hidden Markov models on this heavily unbalanced dataset.	Univ Massachusetts, Ctr Intelligent Informat Retrieval, Multimedia Indexing & Retrieval Grp, Amherst, MA 01003 USA	Feng, SL (reprint author), Univ Massachusetts, Ctr Intelligent Informat Retrieval, Multimedia Indexing & Retrieval Grp, Amherst, MA 01003 USA.						Berglund B, 1996, ENVIRON INT, V22, P1, DOI 10.1016/0160-4120(95)00098-4; BUEHLER EC, 2001, WORKSH DATA MINING B; HARDING SM, 1997, P 1 EUR C RES ADV TE, P345; JAPKOWICZ N, 2002, INTELLIGENT DATA ANA; Lavrenko V., 2004, Proceedings. First Workshop on Document Image Analysis for Libraries; LI Y, 2002 P INT C MACH LE, P379; Marti UV, 2001, INT J PATTERN RECOGN, V15, P65, DOI 10.1142/S0218001401000848; RATNAPARKHI A, 1997, 9708 U PENNS I RES C; Rosenfeld R, 1996, COMPUT SPEECH LANG, V10, P187, DOI 10.1006/csla.1996.0011; VINCIARELLI A, 2003, P 7 INT C DOC AN REC, V1, P1101	10	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA			0-7695-2420-6				2005							528	532		10.1109/ICDAR.2005.73		5	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Computer Science	BCZ12	WOS:000232022600103	
S	Fu, XH; Feng, BQ		Wang, J; Liao, X; Yi, Z		Fu, XH; Feng, BQ			Content filtering of decentralized P2P search system based on heterogeneous neural networks ensemble	ADVANCES IN NEURAL NETWORKS - ISNN 2005, PT 3, PROCEEDINGS	LECTURE NOTES IN COMPUTER SCIENCE		English	Article; Proceedings Paper	2nd International Symposium on Neural Networks	MAY 30-JUN 01, 2005	Chongqing, PEOPLES R CHINA	Chongqing Univ, SW Normal Univ, Chongqing Univ, Posts& Telecommun, SW Agr Univ, Chongqing Educ Coll, Chinese Univ Hong Kong, Asia Pacific Neural Network Assembly, European Neural Network Soc, IEEE Circuits & Syst Soc, IEEE Computat Intelligence Soc, Natl Nat Sci Fdn China, K C Wong Educ Fdn Hong Kong				A Peer-to-Peer (P2P) based decentralized personalized information access system called PeerBridge for edge nodes of the Internet network is proposed to provide user-centered, content-sensitive, and high quality information search and discovery service from Web and P2P network timely. The general system architecture, user modeling and content filtering mechanism of PeerBridge are discussed in detail. Moreover in order to only find information which users are interested in, a new heterogeneous neural network ensemble (HNNE) classifier is presented for filtering irrelevant information, which combines several component neural networks to accomplish the same filtering task, and improves the generalization performance of a classification system. Performance evaluation in the experiments showed that PeerBridge is effective to search relevant information for individual users, and the filtering effect of the HNNE classifier is better than that of support vector machine, Naive Bayes, and individual neural network.	Xian Jiaotong Univ, Dept Comp Sci & Technol, Xian 710049, Peoples R China	Fu, XH (reprint author), Xian Jiaotong Univ, Dept Comp Sci & Technol, Xian 710049, Peoples R China.	csdnfxh@yahoo.com.cn					[傅向华 Fu Xianghua], 2004, [西安交通大学学报. 自然科学版, Journal of Xi'an Jiaotong University], V38, P599; [傅向华 Fu Xianghua], 2004, [西安交通大学学报. 自然科学版, Journal of Xi'an Jiaotong University], V38, P796; HANSEN LK, 1990, IEEE T PATTERN ANAL, V12, P993, DOI 10.1109/34.58871; Joachims T., 1998, 10 EUR C MACH LEARN; Sebastiani F, 2002, ACM COMPUT SURV, V34, P1, DOI 10.1145/505282.505283; SINGH A, 2003, SIGIR 2003 WORKSH DI; STOICA I, 2001, SIGCOMM ANN C DAT CO; Suel T., 2003, 6 INT WORKSH WEB DAT; ZHOU J, 2004, 10 IEEE INT WORKSH F; Zhou ZH, 2002, ARTIF INTELL, V137, P239, DOI 10.1016/S0004-3702(02)00190-X	10	0	0	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		3-540-25914-7	LECT NOTES COMPUT SC			2005	3498						349	354				6	Computer Science, Artificial Intelligence; Computer Science, Theory & Methods	Computer Science	BCN43	WOS:000230167700056	
B	Garcia, AJT; Hruschka, ER		Nedjah, N; Mourelle, LM; Vellasco, MMB; Abraham, A; Koppen, M		Garcia, AJT; Hruschka, ER			Naive Bayes as an imputation tool for classification problems	HIS 2005: 5th International Conference on Hybrid Intelligent Systems, Proceedings			English	Proceedings Paper	5th International Conference on Hybrid Intelligent Systems	NOV 06-09, 2005	Rio de Janeiro, BRAZIL	Operador Nacl Sistema Eletr, Coordenac Aperfeicoament Pessoal Nivel Super, Brazilian Comp Soc, Brazilian Soc Automat, IEEE Syst Man & Cygernet Soc, Int Fuzzy Syst Assoc, European Neural Network Soc, European Soc Fuzzy Log & Technol, World Federat Soft Comp, Pontific Univ Catol Rio deJaneiro				We investigate the use of the Naive Bayes classifier as an imputation tool for classification problems, elaborating on why the usually employed Majority Method may insert biases in a classification context. Considering Rubin's typology for the distribution of missingness, we have performed experiments that illustrate how an imputation process may influence classification tasks. Our results show that imputations performed by the Naive Bayes can be useful for other classifiers (decision trees and nearest neighbors). In this sense, interesting hybrid systems to classify datasets with missing values can be derived.	Catholic Univ Santos, Santos, Brazil	Garcia, AJT (reprint author), Catholic Univ Santos, Santos, Brazil.		Hruschka, Eduardo/E-6593-2011				Di Zio M, 2004, J ROY STAT SOC A STA, V167, P309, DOI 10.1046/j.1467-985X.2003.00736.x; Little RJA, 1987, STAT ANAL MISSING DA; Pyle D., 1999, DATA PREPARATION DAT; Schafer JL, 2002, PSYCHOL METHODS, V7, P147, DOI 10.1037//1082-989X.7.2.147; Witten I.H., 2000, DATA MINING PRACTICA	5	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA			0-7695-2457-5				2005							497	499				3	Computer Science, Artificial Intelligence	Computer Science	BDN06	WOS:000234402500081	
B	Glickman, O; Dagan, I; Koppel, M		Kaelbling, LP; Saffotti, A		Glickman, Oren; Dagan, Ido; Koppel, Moshe			A Probabilistic Lexical Approach to Textual Entailment	19TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-05)			English	Proceedings Paper	19th International Joint Conference on Artificial Intelligence (IJCAI 05)	JUL 30-AUG 05, 2005	Edinburgh, SCOTLAND	BCS, Scottish Enterprise, QinetiQ, BTO, Foresight, Microsoft Research, ERCIM, Intelligent Applications Ltd, IBM, CologNET, Intel, Google				The textual entailment problem is to determine if a given text entails a given hypothesis. This paper describes first a general generative probabilistic setting for textual entailment. We then focus on the sub-task of recognizing whether the lexical concepts present in the hypothesis are entailed from the text. This problem is recast as one of text categorization in which the classes are the vocabulary words. We make novel use of Naive Bayes to model the problem in an entirely unsupervised fashion. Empirical tests suggest that the method is effective and compares favorably with state-of-the-art heuristic scoring approaches.	[Glickman, Oren; Dagan, Ido; Koppel, Moshe] Bar Ilan Univ, Dept Comp Sci, Ramat Gan, Israel	Glickman, O (reprint author), Bar Ilan Univ, Dept Comp Sci, Ramat Gan, Israel.	glikmao@cs.biu.ac.il; dagan@cs.biu.ac.il; koppel@cs.biu.ac.il					DAGAN I, 2005, PASCAL CHALL WORKSH; McCallum A, 1998, AAAI 98 WORKSH LEARN; MONZ C, 2001, 3 WORKSH INF COMP SE; SAGGION H, 2004, SIGIR04 WORKSH INF R	4	0	0	IJCAI-INT JOINT CONF ARTIF INTELL	FREIBURG	ALBERT-LUDWIGS UNIV FREIBURG GEORGES-KOHLER-ALLEE, INST INFORMATIK, GEB 052, FREIBURG, D-79110, GERMANY							2005							1682	1683				2	Computer Science, Artificial Intelligence	Computer Science	BUS48	WOS:000290233000315	
B	Govindarajan, M; Chandrasekaran, RM; Palaniappan, B		Blair, S; Chakraborty, U; Chen, SH; Cheng, HD; Chiu, DKY; Das, S; Denker, G; Duro, R; Romay, MG; Hung, D; Kerre, EE; VaLeong, H; Lu, CT; Lu, J; Maguire, L; Ngo, CW; Sarfraz, M; Tseng, C; Tsumoto, S; Ventura, D; Wang, PP; Yao, X; Zhang, CN; Zhang, K		Govindarajan, M; Chandrasekaran, RM; Palaniappan, B			Comparative study of classification algorithms	Proceedings of the 8th Joint Conference on Information Sciences, Vols 1-3			English	Proceedings Paper	8th Joint Conference on Information Sciences (JCIS 2005)	JUL 21-26, 2005	Salt Lake City, UT	Duke Univ, Utah State Univ, San Jose State Univ, Harbin Inst Technol		data mining; classification; comparative study and time complexity		The problem of classification of records in a mushroom like database is a very difficult problem and tedious process. Study reveals that classifying the records discloses some useful patterns for selective marketing, financial forecast, and many other applications, hence it has attracted a lot of attention in recent data mining research. One of the important problems in data mining is the Classification-rule learning that involves finding rules that partition given data into predefined classes. In the data-mining domain where millions of records and a large number of attributes are involved, the execution time of existing algorithms can be exhaustive. Therefore, efficient classification algorithms in mushroom database have to be examined meticulously. A comparative study of Classification algorithms like K-Nearest Neighbors, Naive Bayes, BVDecompose, C4.5, AdaBoostM1 using mushroom database based on time complexity was done and the best algorithm is identified. Existing algorithm is modified to implement the multithreading concept and resulting in a better algorithm called as "AdaBoostMulti". It is observed that "AdaBoostMulti" outperforms the earlier algorithm.	Annamalai Univ, Dept CSE, Annamalainagar 608002, Tamil Nadu, India	Govindarajan, M (reprint author), Annamalai Univ, Dept CSE, Annamalainagar 608002, Tamil Nadu, India.						DUNHAM MH, DATA MINING INTRO AD, P164; FRIEDMAN J, 1999, ADDITIVE LOGISTIC RE; Han J., 1997, P CASCON 97 M MINDS; Han J., DATA MINING CONCEPTS; KOHAVI R, 1996, BIAS PLUS VARIANCE D	5	0	0	JOINT CONFERENCE INFORMATION SCIENCES	DURHAM	2709 MONTGOMERY ST, DURHAM, NC 27705 USA							2005							229	232				4	Computer Science, Artificial Intelligence; Computer Science, Information Systems; Computer Science, Interdisciplinary Applications	Computer Science	BDI96	WOS:000233670800055	
B	Hachey, B; Grover, C		Kaelbling, LP; Saffotti, A		Hachey, Ben; Grover, Claire			Sentence Extraction for Legal Text Summarisation	19TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-05)			English	Proceedings Paper	19th International Joint Conference on Artificial Intelligence (IJCAI 05)	JUL 30-AUG 05, 2005	Edinburgh, SCOTLAND	BCS, Scottish Enterprise, QinetiQ, BTO, Foresight, Microsoft Research, ERCIM, Intelligent Applications Ltd, IBM, CologNET, Intel, Google				We describe a system for generating extractive summaries of texts in the legal domain, focusing on the relevance classifier, which determines which sentences are abstract-worthy. We experiment with naive Bayes and maximum entropy estimation toolkits and explore methods for selecting abstract-worthy sentences in rank order. Evaluation using standard accuracy measures and using correlation confirm the utility of our approach, but suggest different optimal configurations.	[Hachey, Ben; Grover, Claire] Univ Edinburgh, Sch Informat, Edinburgh EH8 9LW, Midlothian, Scotland	Hachey, B (reprint author), Univ Edinburgh, Sch Informat, 2 Buccleuch Pl, Edinburgh EH8 9LW, Midlothian, Scotland.	bhachey@inf.ed.ac.uk; grover@inf.ed.ac.uk					FAYYAD U, 1993, IJCAI; John George H., 1995, UAI; Kupiec J., 1995, SIGIR Forum; MALOUF R, 2002, CONLL; Teufel S, 2002, COMPUT LINGUIST, V28, P409, DOI 10.1162/089120102762671936; WOLF F, 2004, ACL	6	0	0	IJCAI-INT JOINT CONF ARTIF INTELL	FREIBURG	ALBERT-LUDWIGS UNIV FREIBURG GEORGES-KOHLER-ALLEE, INST INFORMATIK, GEB 052, FREIBURG, D-79110, GERMANY							2005							1686	1687				2	Computer Science, Artificial Intelligence	Computer Science	BUS48	WOS:000290233000317	
B	How, BC; Kulathuramaiyer, N; Kiong, WT			IEEE Computer Society	How, BC; Kulathuramaiyer, N; Kiong, WT			Categorical term descriptor: A proposed term weighting scheme for feature selection	2005 IEEE/WIC/ACM International Conference on Web Intelligence, Proceedings			English	Proceedings Paper	IEEE/WIC/ACM International Conference on Web Intelligence	SEP 19-22, 2005	Compiegne, FRANCE	IEEE Comp Soc, Web Intelligence Consortium, Assoc Comp Machinery	Compiegne Univ Technol			This paper proposes a term weighting scheme, Categorical Term Descriptor (CTL), for feature selection in automated text categorization. CTD is an adaptation of the Term Frequency Inverse Document Frequency (TFIDF). We compared the performance of the proposed method against classical methods such as Correlation Coefficient, Chi-Square and Information Gain using the Multinomial Naive Bayes and the Support Vector Machine (SVM) classifiers on the Reuters(10) and Reuters(115) variants of Reuters-21578 dataset. Despite its simplicity, CTD has proven to be promising for both local and global feature selection. CTD works best for the Reuter(10) as a stable local FS method	Univ Malaysia Sarawak, Fac Comp Sci & Informat Technol, Kota Samarahan 94300, Sarawak, Malaysia	How, BC (reprint author), Univ Malaysia Sarawak, Fac Comp Sci & Informat Technol, Kota Samarahan 94300, Sarawak, Malaysia.		KULATHURAMAIYER , Narayanan /F-4586-2010	KULATHURAMAIYER , Narayanan /0000-0002-1278-8419			BOLLACKER K, 1998, AGENTS 98; BOYAN J, 1994, WS9605 AM ASS ART IN; Forman G., 2003, Journal of Machine Learning Research, V3, DOI 10.1162/153244303322753670; FRANK E, 1999, DOMAIN SPECIFIC KEYP; FUHR N, 1999, NEW EFFECTIVE APPROA; GALAVOTTI L, 1999, ECDL, P59; Joachims T., 1996, CMUCS96118; KORFHAGE R, 1997, INFORMATION STORAGE; MLADENIC D, 1998, P 13 EUR C ART INT E, P473; SALTON G, 1987, 87881 CORN U DEP COM; Sebastiani F., 1999, P ASAI 99 1 ARG S AR, P7; TOKUNAGA T, 1994, 94TR0001 TOK I TECHN; Yang Y., 1997, P 14 INT C MACH LEAR, P412; ZHENG Z, 2003, ICML 2003 WORKSH	14	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA			0-7695-2415-X				2005							313	316				4	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Computer Science	BDM68	WOS:000234321300057	
B	Jantima, P; Sumnuk, P; Chumsak, S; Rapeeporn, C; Anirut, C		Zhu, Q		Jantima, P; Sumnuk, P; Chumsak, S; Rapeeporn, C; Anirut, C			An effective pornographic web filtering system using a probabilistic classifier	Proceedings of the 11th Joint International Computer Conference			English	Proceedings Paper	11th Joint International Computer Conference (JICC 2005)	NOV 10-12, 2005	Chongqing, PEOPLES R CHINA	China Comp Federat, Hong Kong Comp Soc, Chongqing Informat Ind Bur, Chongqing Univ, Chongqing Univ Post & Telecommun		web filtering; text classification; pornographic web; probabilistic classifier		Due to the flood of pornographic web sites on the internet, an effective web filtering system is essential. Web filtering has become one of the important techniques to handle and filter inappropriate information on the web. In this paper, we introduce a web filtering system based on contents. The system uses a probabilistic text classifier to filter pornographic information on the WWW. We focus initially only on Thai and English language web sites. The first process is to parse the web sites collection to extract unique words and to reduce stop-words. Afterwards, these features are transformed into a structurized "bag of words". The next process is calculating the probabilities of each category in the naive bayes classifier (as a pornographic web filter). Finally, we have implemented and experimented on our techniques. After testing by the F-measure, the experimental results of our system show high accuracy. This demonstrates that naive bayes can provide more effectiveness for web filtering based on text content.	Mahasarakham Univ, Fac Informat, Mahasarakham 44150, Thailand	Jantima, P (reprint author), Mahasarakham Univ, Fac Informat, Mahasarakham 44150, Thailand.						Baeza-Yates R., 1999, MODERN INFORM RETRIE; BILELLO M, 2002, NATL ACAD PRESS; DU R, 2004, WEB FILTERING USING; GUIYANG S, 2004, J ZHEJANG U SCIENCE; HAN EH, 2001, P PAC AS C KNOWL DIS; Ji G., 2005, P INT C MACH LEARN I; JOACHIMS T, 1997, 23 U DORTMUND COMP S; Kim Yu-Hwan, 2000, P 23 ANN INT ACM SIG, P168, DOI 10.1145/345508.345572; Kruegel C., 2002, P 2002 ACM S APPL CO, P201; Meknavin S., 1997, P NAT LANG PROC PAC; *NETPR PROJ, 2001, REP CURR AV COST FIL; Nigam K, 2000, MACH LEARN, V39, P103, DOI 10.1023/A:1007692713085; POLPINIJ J, 2005, 4 INT C AS LANG PROC; POOVARAWAN Y, 1986, P 9 EL ENG C; PROVOST J, 1999, NAIVE BAYES RULE LEA; PUI Y, 2002, IEEE INTELLIGENT SYS; RILOFF E, 1995, P 18 ANN INT ACM SIG; SEBASTINI F, 2005, TEXT CATEGORIZATION; Yang Y., 1999, P 22 ANN INT ACM SIG; Yang YM, 2002, J INTELL INF SYST, V18, P219, DOI 10.1023/A:1013685612819	20	0	0	WORLD SCIENTIFIC PUBL CO PTE LTD	SINGAPORE	PO BOX 128 FARRER RD, SINGAPORE 9128, SINGAPORE			981-256-532-9				2005							601	607		10.1142/9789812701534_0136		7	Computer Science, Artificial Intelligence; Computer Science, Information Systems; Computer Science, Software Engineering; Imaging Science & Photographic Technology; Telecommunications	Computer Science; Imaging Science & Photographic Technology; Telecommunications	BDF35	WOS:000233230700136	
B	Jiang, LX; Zhang, H		Han, J; Wah, BW; Raghavan, V; Wu, X; Rastogi, R		Jiang, LX; Zhang, H			Learning instance greedily cloning naive Bayes for ranking	Fifth IEEE International Conference on Data Mining, Proceedings			English	Proceedings Paper	5th IEEE International Conference on Data Mining	NOV 27-30, 2005	Houston, TX	IEEE Comp Soc, TCII, IEEE Comp Soc, TCPAMI, IBM Res, Knowledge & Informat Syst, Web Intelligence Consortium, Univ Louisiana Lafayette, Ctr Adv Comp Studies, Amer Discount ADS Inc, Univ Houston, Dept Comp Sci, Elder Res Inc			ROC CURVE; AREA	Naive Bayes(simply NB)[12] has been widely used in machine learning and data mining as a simple and effective classification algorithm. Since its conditional independence assumption is rarely true, researchers have made a substantial amount of effort to improve naive Bayes. The related research work can be broadly divided into two approaches: eager learning and lazy learning, depending on when the major computation occurs. Different from eager approach, the key idea for extending naive Bayes from the lazy approach is to learn a naive Bayes for each testing example. In recent years, some lazy extensions of naive Bayes have been proposed. For example, SNNB[18], LWNB[7], and LBR[19]. All are aiming at improving the classification accuracy of naive Bayes. In many real-world machine learning and data mining applications, however an accurate ranking is more desirable than an accurate classification. Responding to this fact, we present a lazy learning algorithm called instance greedily cloning naive Bayes (simply IGCNB) in this paper Our motivation is to improve naive Bayes' ranking performance measured by AUC[4, 14]. We experimentally tested our algorithm, using the whole 36 UCI datasets recommended by Weka[1], and compared it to C4.4[16], NB[12], SNNB[18] and LWNB[7]. The experimental results show that our algorithm outperforms all the other algorithms used to compare significantly in yielding accurate ranking.	China Univ Geosci, Fac Comp Sci, Wuhan 430074, Peoples R China	Jiang, LX (reprint author), China Univ Geosci, Fac Comp Sci, Wuhan 430074, Peoples R China.		Jiang, Liangxiao /D-1237-2012				Aha D.W., 1997, LAZY LEARNING; Bennett P.N., 2000, CMUCS00155 SCH COMP; Bradley AP, 1997, PATTERN RECOGN, V30, P1145, DOI 10.1016/S0031-3203(96)00142-2; Chickering D. M., 1996, LEARNING DATA ARTIFI, VV, P121; Domingos P, 1997, MACH LEARN, V29, P103, DOI 10.1023/A:1007413511361; FRANK HMP, 2003, P C UNC ART INT M KA, P249; Friedman N, 1997, MACH LEARN, V29, P131, DOI 10.1023/A:1007465528199; Hand DJ, 2001, MACH LEARN, V45, P171, DOI 10.1023/A:1010920819831; JIANG ZHS, 2005, P 18 CAN C ART INT S, P280; Kohavi R., 1996, P 2 INT C KNOWL DISC, P202; LANGLEY P, 1994, P UNC ART INT M KAUF, P400; Langley P., 1992, P 10 NAT C ART INT, P223; Provost F, 2003, MACH LEARN, V52, P199, DOI 10.1023/A:1024099825458; Provost F., 1997, Proceedings of the Third International Conference on Knowledge Discovery and Data Mining; Provost F., 1998, P 15 INT C MACH LEAR, P445; Witten I.H., 2000, DATA MINING PRACTICA; XIE HWL, 2002, P 6 PAC AS C KDD SPR, P104; Zheng ZJ, 2000, MACH LEARN, V41, P53, DOI 10.1023/A:1007613203719	18	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA			0-7695-2278-5				2005							202	209				8	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Computer Science	BDS21	WOS:000235162400026	
S	Jiang, LX; Guo, YY		Lim, A		Jiang, LX; Guo, YY			Learning lazy naive Bayesian classifiers for ranking	ICTAI 2005: 17TH IEEE INTERNATIONAL CONFERENCE ON TOOLS WITH ARTIFICIAL INTELLIGENCE, PROCEEDINGS	Proceedings - International Conference on Tools With Artificial Intelligence		English	Proceedings Paper	17th International Conference on Tools with Artificial Intelligence	NOV 14-16, 2005	Hong Kong, PEOPLES R CHINA	IEEE Comp Soc, Informat Technol Res Inst, Wright State Univ, Hong Kong Univ Sci & Technol			ROC CURVE; AREA	Naive Bayes(simply NB) has been well-known as an effective and efficient classification algorithm. However it is based on the conditional independence assumption that it is often violated in applications. In addition, in many real-world data mining applications, however, an accurate ranking of instances is often required rather than an accurate classification. For example, a ranking of customers in terms of the likelihood that they buy one's products is useful in direct marketing. In this paper we firstly investigate the ranking performance of some lazy learning algorithms for extending naive Bayes. The ranking performance is measured by AUC[9, 5]. We observe that they can not significantly improve naive Bayes' ranking performance. Motivated by this fact and aiming at improving naive Bayes with accurate ranking, we present a new lazy learning algorithm, called lazy naive Bayes (simply LNB), to extend naive Bayes for ranking. We experimentally tested our algorithm, using the whole 36 UCI data sets[4] recommended by Weka[1], and compared it to NB and C4.4[11] measured by AUC. The experimental results show that our algorithm significantly outperforms both NB and C4.4.	China Univ Geosci, Fac Comp Sci, Wuhan 430074, Hubei, Peoples R China	Jiang, LX (reprint author), China Univ Geosci, Fac Comp Sci, Wuhan 430074, Hubei, Peoples R China.	ljiang@cug.edu.cn	Jiang, Liangxiao /D-1237-2012				Aha D.W., 1997, LAZY LEARNING; Bennett P.N., 2000, CMUCS00155 SCH COMP; BLAKE C, 2000, DEPT ICS U CALIFORNI; Bradley AP, 1997, PATTERN RECOGN, V30, P1145, DOI 10.1016/S0031-3203(96)00142-2; Chickering D. M., 1996, LEARNING DATA ARTIFI, VV, P121; CORINNA MM, 2003, ADV NEURAL INFORM PR, P1035; FRANK HMP, 2003, P C UNC ART INT M KA, P249; Hand DJ, 2001, MACH LEARN, V45, P171, DOI 10.1023/A:1010920819831; NADEAU Y, 1999, ADV NEURAL INFORM PR, V12, P307; Provost F, 2003, MACH LEARN, V52, P199, DOI 10.1023/A:1024099825458; Quinlan J.R., 1993, C4 5 PROGRAMS MACHIN; Witten I.H., 2000, DATA MINING PRACTICA; XIE HWL, 2002, P 6 PAC AS C KDD SPR, P104; Zheng ZJ, 2000, MACH LEARN, V41, P53, DOI 10.1023/A:1007613203719	14	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA	1082-3409		0-7695-2488-5	PROC INT C TOOLS ART			2005							412	416				5	Computer Science, Artificial Intelligence	Computer Science	BDO97	WOS:000234632500065	
S	Kim, H; Jang, MG; Chen, SS		Rauber, A; Christodoulakis, S; Tjoa, AM		Kim, H; Jang, MG; Chen, SS			Building semantic digital libraries: Automated ontology linking by associative naive Bayes classifier	RESEARCH AND ADVANCED TECHNOLOGY FOR DIGITAL LIBRARIES	LECTURE NOTES IN COMPUTER SCIENCE		English	Article; Proceedings Paper	9th European Conference on Research and Advanced Technology for Digital Libraries (ECDL 2005)	SEP 18-23, 2005	Vienna, AUSTRIA	Austrian Natl Lib, Austrian Comp Soc, Vienna Univ Technol				In this paper, we present a new classification method, called Associative Naive Bayes (ANB), to associate MEDLINE citations with Gene Ontology (GO) terms. We define the concept of class-support to find frequent itemsets and the concept of class-all-confidence to find interesting itemsets. Empirical test results on three MEDLINE datasets show that ANB is superior to naive Bayesian classifier. The results also show that ANB outperforms the state of the art Large Bayes classifier.	Elect & Telecommun Res Inst, Comp & Informat Sci & Engn Dept, Taejon 305700, South Korea; Univ Florida, Comp & Informat Sci & Engn Dept, Gainesville, FL 32611 USA	Kim, H (reprint author), Elect & Telecommun Res Inst, Comp & Informat Sci & Engn Dept, Taejon 305700, South Korea.	hkk@etri.re.kr; mgjang@etri.re.kr; suchen@cise.ufl.edu					KIM H, 2003, P 7 EUR C RES ADV TE, P164; MERETAKIS D, 1999, P 5 ACM SIGKDD INT C, P165, DOI 10.1145/312129.312222; Mitchell T.M., 1997, MACHINE LEARNING	3	0	0	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		3-540-28767-1	LECT NOTES COMPUT SC			2005	3652						500	501				2	Computer Science, Information Systems; Computer Science, Theory & Methods; Information Science & Library Science	Computer Science; Information Science & Library Science	BDJ77	WOS:000233890800054	
S	Li, YF; Cao, YK; Zhu, QS; Zhu, ZY		Li, X; Wang, S; Dong, ZY		Li, YF; Cao, YK; Zhu, QS; Zhu, ZY			A novel framework for web page classification using two-stage neural network	ADVANCED DATA MINING AND APPLICATIONS, PROCEEDINGS	LECTURE NOTES IN ARTIFICIAL INTELLIGENCE		English	Article; Proceedings Paper	1st International Conference on Advanced Data Mining and Applications	JUL 22-24, 2005	Wuhan, PEOPLES R CHINA					Web page classification is one of the essential techniques for Web mining. This paper presents a framework for Web page classification. It is hybrid architecture of neural network PCA (principle components analysis) and SOFM (self-organizing map). In order to perform the classification, a web page is firstly represented by a vector of features with different weights according to the term frequency and the importance of each sentence in the page. As the number of the features is big, PCA is used to select the relevant features. Finally the output of PCA is sent to SOFM for classification. To compare with the proposed framework, two conventional classifiers are used in our experiments: k-NN and Naive Bayes. Our new method makes a significant improvement in classifications on both data sets compared with the two conventional methods.	Chongqing Univ, Dept Comp Sci, Chongqing 400044, Peoples R China	Li, YF (reprint author), Chongqing Univ, Dept Comp Sci, Chongqing 400044, Peoples R China.	lyf129@126.com					Calvo R. A., 2000, Intelligent Data Analysis, V4; Calvo R. A., 1998, Proceedings of the Ninth Australian Conference on Neural Networks (ACNN'98); Gentili G. L., 2002, INT J PATTERN RECOGN, V15, P527; Haykin S., 1999, NEURAL NETWORKS COMP; Johnson R.A., 2002, APPL MULTIVARIATE ST; Ko Y, 2004, INFORM PROCESS MANAG, V40, P65, DOI 10.1016/S0306-4573(02)00056-0; Kohonen T, 1997, SPRINGER SERIES INFO, V30; LEWIN G, 1996, ANNU REV NEUROSCI, V9, P289; McCallum A., 1998, AAAI 98 WORKSH LEARN, P41; Nouali O, 2004, EXPERT SYST APPL, V26, P171, DOI 10.1016/S0957-4174(03)00118-0; Ruiz ME, 2002, INFORM RETRIEVAL, V5, P87, DOI 10.1023/A:1012782908347; Selamat A, 2004, INFORM SCIENCES, V158, P69, DOI 10.1016/j.ins.2003.03.003; Wermter S, 2000, INFORM RETRIEVAL, V3, P87, DOI 10.1023/A:1009942513170; YANG Y, 2002, J INFORM SYST, V18	14	0	0	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		3-540-27894-X	LECT NOTES ARTIF INT			2005	3584						499	506				8	Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications	Computer Science	BCR26	WOS:000230895000060	
B	Lin, SD; Han, GQ; Xu, XY			IEEE	Lin, SD; Han, GQ; Xu, XY			Information fusion of agent based heterogeneous multi-classifiers	PROCEEDINGS OF 2005 INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND CYBERNETICS, VOLS 1-9			English	Proceedings Paper	4th International Conference on Machine Learning and Cybernetics	AUG 18-21, 2005	Canton, PEOPLES R CHINA	IEEE Systems, Man & Cybernet TCC, Hong Kong Polytechn Univ, Hebei Univ, S China Univ Technol, Chongqing Univ, Sun Yatsen Univ, Harbin Inst Technol, Int Univ Germany		fusion of heterogeneous classifiers; agent; associative classification; decision tree; Naive Bayes		Traditional technology of classifier fusion can not make full use of the characteristics of heterogeneous classifiers to deal with various problems. This work suggests a new technology of information fusion using multiple agents, each of which uses a quite different classification algorithm such as decision tree algorithm, simple Naive Bayes algorithm and the newly emerging classification algorithm based on atomic association rules. Information fusion of these heterogeneous multi-classifiers is based on the classifier behavior, properties of training dataset and the instance to be classified. The proposed technology has following advantages: (1) high classification accuracy; (2) no need of fusion training, and (3) fast learning and prediction. The experimental results on 10 UCI standard datasets show that accuracy of the proposed fusion technology is noticeably higher than that of traditional voting method.	S China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510640, Peoples R China	Lin, SD (reprint author), S China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510640, Peoples R China.	czt_linsd@gd.gov.cn; csgqhan@scut.edu.cn; xiaoyxu@yahoo.com.cn					FABIO B, JADE PROGRAMMERS GUI; GIANCINTO G, 2001, PATTERN RECOGN, V34, P1879; Kamel MS, 2003, LECT NOTES COMPUT SC, V2709, P1; Kohavi R., 1996, P 2 INT C KNOWL DISC, P202; Liu B., 1998, P 4 INT C KNOWL DISC, P80; MERETAKIS D, 1999, P 5 ACM SIGKDD INT C, P165, DOI 10.1145/312129.312222; Quinlan J.R., 1993, C4 5 PROGRAMS MACHIN; Xu XY, 2004, PROCEEDINGS OF THE 2004 INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND CYBERNETICS, VOLS 1-7, P1604	8	0	0	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA			0-7803-9091-1				2005							1976	1981				6	Computer Science, Artificial Intelligence; Computer Science, Cybernetics; Computer Science, Information Systems	Computer Science	BDT94	WOS:000235325603006	
B	Liu, LZ; Li, PZ; Chen, JJ		Wen, TD		Liu, LZ; Li, PZ; Chen, JJ			The research of classification technologies based on text mining	ISTM/2005: 6th International Symposium on Test and Measurement, Vols 1-9, Conference Proceedings			English	Proceedings Paper	6th International Symposium on Test and Measurement (ISTM)	JUN 01-04, 2005	Dalian, PEOPLES R CHINA	Chinese Soc Modern Tech Equipment, Chinese Assoc Higher Educ, CSMTE, Test & Measurement Sect, Soc Instrumentat, Measurement & Control, N Univ China, Key Lab Instrumentat Sci & Dynam Measurement, Minist Educ, Natl Key Lab Elect Measurement Technol, Taiyuan Div, Candidate State Key Lab Dynam Measurement, Dalian Univ Tehnol, State Key Lab Coastal & Offshore Engn, NUC, Dept Elect Sci & Technol				With the global prevalence of the network application, there are so many resources on line that have no uniform structures and managements. They need to be processed as quickly as possible. The method of network pages automatically classification with high efficiency is the key technology, which can abstract needed information from vast network information. The paper analyzed and compared two classical methods of text classification including KNN and Naive Bayes. Some experiments further show the method of naive Bayes classification is of quickly and precise. In the end evaluated methods of classifying performance were presented in detail.	Capital Normal Univ, Informat Engn Coll, Beijing 100037, Peoples R China	Liu, LZ (reprint author), Capital Normal Univ, Informat Engn Coll, Beijing 100037, Peoples R China.						Bian Z. Q., 2000, PATTERN RECOGNITION; HAN JW, 1999, CONCEPT TECHNOLOGIES; JIE C, 2001, LECT NOTES COMPUTER; MERETAKIS D, 2000, MACHINE LEARNING ECM; Shi Zhongzhi, 2002, KNOWLEDGE DISCOVERY	5	0	0	INTERNATIONAL ACADEMIC PUBLISHERS LTD	HONG KONG	UNIT 1205, 12 FLOOR, SINO PLAZA, 255 GLOUCESTER ROAD, HONG KONG 00000, CAUSEWAY BAY, PEOPLES R CHINA			7-5062-7445-0				2005							8517	8520				4	Instruments & Instrumentation	Instruments & Instrumentation	BCZ22	WOS:000232030708090	
B	Liu, LZ; Zhang, CL; Chen, JJ		Wen, TD		Liu, LZ; Zhang, CL; Chen, JJ			Research on text classification mining based on Naive Bayes	ISTM/2005: 6th International Symposium on Test and Measurement, Vols 1-9, Conference Proceedings			English	Proceedings Paper	6th International Symposium on Test and Measurement (ISTM)	JUN 01-04, 2005	Dalian, PEOPLES R CHINA	Chinese Soc Modern Tech Equipment, Chinese Assoc Higher Educ, CSMTE, Test & Measurement Sect, Soc Instrumentat, Measurement & Control, N Univ China, Key Lab Instrumentat Sci & Dynam Measurement, Minist Educ, Natl Key Lab Elect Measurement Technol, Taiyuan Div, Candidate State Key Lab Dynam Measurement, Dalian Univ Tehnol, State Key Lab Coastal & Offshore Engn, NUC, Dept Elect Sci & Technol				In the method of text classification mining based on Naive Bayes, every attribute of eigenvector is independent relatively to confirm a class variable is supposed. But the hypothesis is always disobedient with actual things. So the model of CLIF-NB text classification learning based on Naive Bayes was proposed in the paper The method uses the theory of mutual information to calculate the maximum relative probability in feature attributes of training texts, and introduces variables sets to combine and replace line inseparable attributes to relax limit of independence hypothesis of Naive Bayes.	Capital Normal Univ, Informat Engn Coll, Beijing 100037, Peoples R China	Liu, LZ (reprint author), Capital Normal Univ, Informat Engn Coll, Beijing 100037, Peoples R China.						Blake C. L., 1998, UCI REPOSITORY MACHI; GIARRATANO J, 2000, PRINCIPLE PROGRAMMIN, P5; LANGSETH H, 2002, CLASSIFICATION USING; Linoff G. S., 2001, MINING WEB; LIU LZ, 2003, P 4 WORLD C INT CONT, P2333; LU YH, 2002, J BEIJING I TECHNOLO, V8, P166; Mena J., 1999, DATA MINING YOUR WEB; WIERING MA, 2002, HIERARCHICAL MIXTURE	8	0	0	INTERNATIONAL ACADEMIC PUBLISHERS LTD	HONG KONG	UNIT 1205, 12 FLOOR, SINO PLAZA, 255 GLOUCESTER ROAD, HONG KONG 00000, CAUSEWAY BAY, PEOPLES R CHINA			7-5062-7445-0				2005							8521	8524				4	Instruments & Instrumentation	Instruments & Instrumentation	BCZ22	WOS:000232030708091	
S	Liu, XM; Yin, JW; Dong, JX; Ghafoor, MA		Fan, W; Wu, Z; Yang, J		Liu, XM; Yin, JW; Dong, JX; Ghafoor, MA			An improved FloatBoost algorithm for Naive Bayes text classification	ADVANCES IN WEB-AGE INFORMATION MANAGEMENT, PROCEEDINGS	LECTURE NOTES IN COMPUTER SCIENCE		English	Article; Proceedings Paper	6th International Conference on Web -Age Informational Management	OCT 11-13, 2005	Hangzhou, PEOPLES R CHINA	DatabaseSoc China Comp Federat, Natl Sci Fdn China, Zhejiang Univ, Y C Tang Disciplinary Dev & Fund, Oracle China				Boosting is a method for supervised learning, which has successfully been applied to many different domains and has proven one of the best performers in text classification exercises so far. FloatBoost learning uses a backtrack mechanism after each iteration of AdaBoost learning to minimize the error rate directly, rather than minimizing an exponential function of the margin as in the traditional AdaBoost algorithm. This paper presents an improved FloatBoost boosting algorithm for boosting Naive Bayes text classification, called DifBoost, which combines Divide and Conquer Principal with the FloatBoost algorithm. Integrating FloatBoost with the Divide and Conquer principal, DifBoost divides the input space into a few sub-spaces during training process and the final classifier is formed with the weighted combination of basic classifiers, where basic classifiers are affected by different sub-spaces differently. Extensive experiments using benchmarks are conducted and the encouraging results show the effectiveness of our proposed algorithm.	Zhejiang Univ, Dept Comp Sci & Technol, Hangzhou, Peoples R China	Liu, XM (reprint author), Zhejiang Univ, Dept Comp Sci & Technol, Hangzhou, Peoples R China.	liuxiaoming@zju.edu.cn; zjuyjw@zju.edu.cn; djx@zju.edu.cn; ghafoorgem@yahoo.com					FREUND Y, 1995, P 25 EUR C COMP LEAR; Freund Yoav, 1996, INT C MACH LEARN, P148; Friedman J, 2000, ANN STAT, V28, P337, DOI 10.1214/aos/1016218223; JIANG W, 2001, P 18 INT C MACH LEAR, P234; JIMMY LJ, 2003, P CVPR, P413; Kim HJ, 2004, LECT NOTES COMPUT SC, V3129, P519; Li SZ, 2004, IEEE T PATTERN ANAL, V26, P1112, DOI 10.1109/TPAMI.2004.68; PUDIL P, 1994, PATTERN RECOGN LETT, V15, P1119, DOI 10.1016/0167-8655(94)90127-9; Sebastiani F, 2002, ACM COMPUT SURV, V34, P1, DOI 10.1145/505282.505283; Yang Y., 1999, P 22 ANN INT ACM SIG, P42, DOI 10.1145/312624.312647	10	0	0	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		3-540-29227-6	LECT NOTES COMPUT SC			2005	3739						162	171				10	Computer Science, Information Systems; Computer Science, Theory & Methods	Computer Science	BDG49	WOS:000233385300015	
B	Loganantharaj, R		Selvaraj, H; Srimani, PK		Loganantharaj, R			Recognizing transcription start site (TSS) of plant promoters	ITCC 2005: International Conference on Information Technology: Coding and Computing, Vol 1			English	Proceedings Paper	International Conference on Information Technology - Coding and Computing	APR 04-06, 2005	Las Vegas, NV	IEEE Comp Soc, IEEE			BINDING-SITES; SEQUENCES; RECOGNITION	Discovering a promoter or promoters from a given DNA sequence is an active research area in Bioinformatics. Many promoter detection algorithms use transcription binding sights and some core promoter elements such as CCAAT and TATA box, to determine the location of transcription start site. For the purpose of comparing the effectiveness of different algorithms, we consider transcription start site in isolation. We use annotated plant promoters for our experiments. We have compared the following algorithms for their effectiveness in detecting a TSS: Position Weighted Matrix (PWM), Naive Bayes, decision tree and artificial neural network.	Univ Louisiana, Ctr Adv Comp Studies, Lafayette, LA USA	Loganantharaj, R (reprint author), Univ Louisiana, Ctr Adv Comp Studies, Lafayette, LA USA.						Audic S, 1997, COMPUT CHEM, V21, P223, DOI 10.1016/S0097-8485(96)00040-X; Baldi P., 2001, BIOINFORMATICS MACHI; Balding DJ, 2003, HDB STAT GENETICS; BUCHER P, 1990, J MOL BIOL, V212, P563, DOI 10.1016/0022-2836(90)90223-9; CLAVERIE JM, 1986, NUCLEIC ACIDS RES, V14, P179, DOI 10.1093/nar/14.1.179; Durbin R, 1998, BIOL SEQUENCE ANAL P; FAISST S, 1992, NUCLEIC ACIDS RES, V20, P3, DOI 10.1093/nar/20.1.3; Fickett JW, 1997, GENOME RES, V7, P861; Hutchinson GB, 1996, COMPUT APPL BIOSCI, V12, P391; KONDRAKHIN YV, 1995, COMPUT APPL BIOSCI, V11, P477; PRESTRIDGE DS, 1995, J MOL BIOL, V249, P923, DOI 10.1006/jmbi.1995.0349; QUINLAN JR, 1987, INT J MAN MACH STUD, V27, P221, DOI 10.1016/S0020-7373(87)80053-6; Quinlan J.R., 1993, C4 5 PROGRAMS MACHIN; Reese MG, 2001, COMPUT CHEM, V26, P51, DOI 10.1016/S0097-8485(01)00099-7; Russell S., 2003, PRENTICE HALL SERIES; Shahmuradov IA, 2003, NUCLEIC ACIDS RES, V31, P114, DOI 10.1093/nar/gkg041; Solovyev V, 1997, Proc Int Conf Intell Syst Mol Biol, V5, P294; Winston P.H., 1992, ARTIFICIAL INTELLIGE	18	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA			0-7695-2315-3				2005							20	25		10.1109/ITCC.2005.240		6	Computer Science, Artificial Intelligence; Computer Science, Information Systems; Computer Science, Theory & Methods	Computer Science	BCH18	WOS:000229276500004	
B	Loganantharaj, R		Callaos, N; Lesso, W; Palesi, M		Loganantharaj, Raja			An approach to estimate the prediction accuracy of data mining: A case study with human genome	WMSCI 2005: 9th World Multi-Conference on Systemics, Cybernetics and Informatics, Vol 4			English	Proceedings Paper	9th World Multi-Conference on Systemics, Cybernetics and Informatics	JUL 10-13, 2005	Orlando, FL			naive Bayes; position weighted matrix; genomic sequences; motif detection	EUKARYOTIC PROMOTER RECOGNITION; TRANSCRIPTION FACTORS; BINDING-SITES; SEQUENCES	Data mining has been used for many applications and very little work has been done to estimate the expected accuracy of such techniques for biological sequences. The number of biological sequences in public database has been growing exponentially. In this paper we propose a technique based on positional weighted matrix to estimate the prediction accuracy of a given pattern. We will illustrate the technique to the problem of finding a transcription start site in a human genome. We will verify the result by a popular predictive data mining technique.	Univ Louisiana, Ctr Adv Comp Studies, Lafayette, LA USA	Loganantharaj, R (reprint author), Univ Louisiana, Ctr Adv Comp Studies, Lafayette, LA USA.						Audic S, 1997, COMPUT CHEM, V21, P223, DOI 10.1016/S0097-8485(96)00040-X; Baldi P., 2001, BIOINFORMATICS MACHI; Balding DJ, 2003, HDB STAT GENETICS; BUCHER P, 1990, J MOL BIOL, V212, P563, DOI 10.1016/0022-2836(90)90223-9; CLAVERIE JM, 1986, NUCLEIC ACIDS RES, V14, P179, DOI 10.1093/nar/14.1.179; Durbin R, 1998, BIOL SEQUENCE ANAL P; FAISST S, 1992, NUCLEIC ACIDS RES, V20, P3, DOI 10.1093/nar/20.1.3; Fickett JW, 1997, GENOME RES, V7, P861; Hutchinson GB, 1996, COMPUT APPL BIOSCI, V12, P391; KONDRAKHIN YV, 1995, COMPUT APPL BIOSCI, V11, P477; LGOANANTHARAJ R, 2005, INTELLIGENT TECHNOLO; PRESTRIDGE DS, 1995, J MOL BIOL, V249, P923, DOI 10.1006/jmbi.1995.0349; Reese MG, 2001, COMPUT CHEM, V26, P51, DOI 10.1016/S0097-8485(01)00099-7; Solovyev V, 1997, Proc Int Conf Intell Syst Mol Biol, V5, P294	14	0	0	INT INST INFORMATICS & SYSTEMICS	ORLANDO	14269 LORD BARCLAY DR, ORLANDO, FL 32837 USA			978-980-6560-56-7				2005							204	209				6	Computer Science, Artificial Intelligence; Computer Science, Information Systems; Computer Science, Theory & Methods	Computer Science	BFP96	WOS:000243684900037	
B	Lv, L; Liu, YS		Salem, MA; ElHadidi, MT		Lv, L; Liu, YS			Research of english text classification methods based on semantic meaning	ENABLING TECHNOLOGIES FOR THE NEW KNOWLEDGE SOCIETY			English	Proceedings Paper	ITI 3rd International Conference on Information and Communications Technology (ICICT 2005)	DEC, 2005	Cairo, EGYPT	Informat Technol Inst		WordNet; LSI; Naive Bayes; simple vector distance; semantic meaning		To overcome the limitations of traditional text classification approaches based on bag-of-words representation and to effectively incorporate linguistic knowledge and conceptual index into text vector space representation, based on WordNet thesaurus and Latent Semantic Indexing (LSI) model, combinative method of them is presented to realize Naive Bayes; text classification and simple vector distance text classification, and five groups of contrastive experiments are made respectively. The results show that the accuracy rates of the two text classification methods are both gradually advanced along with more and more in-depth semantic analysis, which indicates that semantic mining is very important and necessary to text classification. The comparative analysis of the related work is also given.	Beijing Inst Technol, Sch Comp Sci & Technol, Beijing 100081, Peoples R China	Lv, L (reprint author), Beijing Inst Technol, Sch Comp Sci & Technol, Beijing 100081, Peoples R China.						Brill E, 1995, COMPUT LINGUIST, V21, P543; CHANEN A, 2004, COMPLEX CORPUS DRIVE; Dumais S. T., 1988, P CHI 88 C HUM FACT, P281, DOI 10.1145/57167.57214; Joachims T., 1997, ICML 97, P143; KONTOSTATHIS A, 2005, IDENTIFICAION CRITIC; LIN LV, 2005, T BEIJING I TECHNOLO, V25; MILLER GA, 1993, INTRO WORDNET ON LIN, P1; Morato J, 2003, GWC 2004: SECOND INTERNATIONAL WORDNET CONFERENCE, PROCEEDINGS, P270; RODRIGUEZ MD, 1997, RANLP 97, P25; SCOTT S, 1998, P COLING ACL WORKSH; Shen HP, 2005, APPL STOCH MODEL BUS, V21, P251, DOI 10.1002/asmb.598; ZU G, 2004, T I ELECT ENG JAPA C, V124, P3	12	0	0	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA			0-7803-9270-1				2005							689	700				12	Computer Science, Hardware & Architecture; Computer Science, Information Systems; Computer Science, Software Engineering	Computer Science	BDZ47	WOS:000236349800049	
B	Lv, L; Liu, YS		Salem, MA; ElHadidi, MT		Lv, L; Liu, YS			Research and realization of Naive Bayes English text classification method based on base noun phrase identification	Enabling Technologies for the New Knowledge Society			English	Proceedings Paper	ITI 3rd International Conference on Information and Communications Technology (ICICT 2005)	DEC, 2005	Cairo, EGYPT	Informat Technol Inst		base noun phrase; phrase identification; maximum entropy model; Naive Bayes; text classification	MAXIMUM-ENTROPY	To more advance classification accuracy of English texts, Naive Bayes method based on base noun phrase (BaseNP) identification is presented. The rising maximum entropy model is applied to the identification. Firstly, use training corpus and user-defined feature templates to generate candidate features. Secondly, the feature selection algorithm computing feature gains is applied to select features. Finally, at the parameter estimation stage, the improved iterative scaling (IIS) algorithm is adopted. The experimental results show that this technique achieved precision and recall rates of roughly 93% for BaseNP identification and the classification accuracy is remarkably improved on this basis. It indicates that shallow parsing of high accuracy is very helpful to text classification.	Beijing Inst Technol, Sch Comp Sci & Technol, Beijing 100081, Peoples R China	Lv, L (reprint author), Beijing Inst Technol, Sch Comp Sci & Technol, Beijing 100081, Peoples R China.						Berger AL, 1996, COMPUT LINGUIST, V22, P39; Cardie C., 1999, Proceedings Sixteenth National Conference on Artificial Intelligence (AAI-99). Eleventh Innovative Applications of Artificial Intelligence Conference (IAAI-99); CARDIE C, 1998, P 36 ANN M ACL COLIN, P218; Duda R.O., 2003, PATTERN CLASSIFICATI; GOOD IJ, 1963, ANN MATH STAT, V34, P911, DOI 10.1214/aoms/1177704014; HUANG XQ, 2000, 2000 INT C MULT INF; JAYNES ET, 1957, PHYS REV, V106, P620, DOI 10.1103/PhysRev.106.620; Koeling R., 2000, P CONLL 2000 LLL 200, P139; PANG JF, 2001, APPL RES COMPUTERS, P18; RATNAPARKHI A, 1997, 9708 U PENNS; SANG EFT, 2000, P COLING 2000, P857	11	0	0	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA			0-7803-9270-1				2005							805	812				8	Computer Science, Hardware & Architecture; Computer Science, Information Systems; Computer Science, Software Engineering	Computer Science	BDZ47	WOS:000236349800056	
B	Marsolo, K; Parthasarathy, S; Ding, C			IEEE Computer Society	Marsolo, K; Parthasarathy, S; Ding, C			A multi-level approach to SCOP fold recognition	BIBE 2005: 5th IEEE Symposium on Bioinformatics and Bioengineering			English	Proceedings Paper	5th IEEE Symposium on Bioinformatics and Bioengineering	OCT 19-21, 2005	Minneapolis, MN	IEEE Comp Soc, Biol & Artificial Intelligence Soc, Univ Minnesota, Digital Technol Ctr, Wright State Univ, ITRI, IEEE				The classification of proteins based on their structure can play an important role in the deduction or discovery of protein function. However the relatively low number of solved protein structures and the unknown relationship between structure and sequence requires an alternative method of representation for classification to be effective. Furthermore, the large number of potential folds causes problems for many classification strategies, increasing the likelihood that the classifier will reach a local optima while trying to distinguish between all of the possible structural categories. Here we present a hierarchical strategy for structural classification that first partitions proteins based on their SCOP class before attempting to assign a protein fold. Using a well-known dataset derived from the 27 most-populated SCOP folds and several sequence-based descriptor properties as input features, we test a number of classification methods, including Naive Bayes and Boosted C4.5. Our strategy achieves an average fold recognition of 74%, which is significantly higher than the 56-60% previously reported in the literature, indicating the effectiveness of a multi-level approach.	Ohio State Univ, Dept Comp Sci & Engn, Columbus, OH 43210 USA	Marsolo, K (reprint author), Ohio State Univ, Dept Comp Sci & Engn, Columbus, OH 43210 USA.						Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324; Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.1023/A:1018054314350; BRESLOW LA, 1996, AOC96014 NCARAI; CHINNASAMY A, 2004, P PSB 2004; Ding CHQ, 2001, BIOINFORMATICS, V17, P349, DOI 10.1093/bioinformatics/17.4.349; DUBCHAK I, 1995, P NATL ACAD SCI USA, V92, P8700, DOI 10.1073/pnas.92.19.8700; John G. H., 1995, 11 C UNC ART INT, P338; Murthy SK, 1998, DATA MIN KNOWL DISC, V2, P345, DOI 10.1023/A:1009744630224; MURZIN AG, 1995, J MOL BIOL, V247, P536, DOI 10.1016/S0022-2836(05)80134-2; PAGALLO G, 1990, MACH LEARN, V5, P71, DOI 10.1023/A:1022611825350; Quinlan J.R., 1993, C4 5 PROGRAMS MACHIN; Schapire R. E., 1996, 13 INT C MACH LEARN, P148; SHI SYM, 2004, P IEEE CIBCB; Tan Aik Choon, 2003, Genome Inform, V14, P206	14	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA			0-7695-2476-1				2005							57	64				8	Biochemical Research Methods; Engineering, Biomedical	Biochemistry & Molecular Biology; Engineering	BDM77	WOS:000234335200008	
S	Morioka, C; El-Saden, S; Pope, W		Ratib, OM; Horii, SC		Morioka, C; El-Saden, S; Pope, W			Integration of HIS/RIS clinical document with PACS image studies for neuroradiology	Medical Imaging 2005: PACS and Imaging Informatics	PROCEEDINGS OF THE SOCIETY OF PHOTO-OPTICAL INSTRUMENTATION ENGINEERS (SPIE)		English	Proceedings Paper	Medical Imaging 2005 Conference	FEB 15-17, 2005	San Diego, CA	SPIE		clinical workflow; document classification; hanging protocol; inforinatics; maximum entropy; naive; Bayes	DIAGNOSTIC WORKSTATIONS; DESIGN	Poring over the medical record of brain tumor patients for pertinent history can be an overwhelming task for the neuroradiologist. The evaluation of an imaging study in a brain tumor patient involves examining the prior imaging and clinical documents for recent intervention potentially affecting the appearance of the brain and then drawing a conclusion and rendering a report based on the contextual information obtained. In complex cases, the radiologist can spend much of his/her time trying to locate the appropriate documents. The purpose of this research is to develop effective methods to review all of the pertinent information in a patient medical record incorporating HIS (Hospital Information Systems), RIS (Radiology Information Systems) and PACS (Picture Archiving and Communications Systems) information. Our research involves three areas in improving the clinical workflow for neuroradiologists: filtering the document worklist for pertinent clinical data, identification of key clusters of clinical information, and an automatic hanging protocol that displays the MR images for optimal image comparison.	UCLA Med Informat, Los Angeles, CA 90024 USA	Morioka, C (reprint author), UCLA Med Informat, 924 Westwood Blvd, Los Angeles, CA 90024 USA.						Dreyer KJ, 2000, RADIOGRAPHICS, V20, P1583; Harreld M, 1998, P SOC PHOTO-OPT INS, V3335, P316, DOI 10.1117/12.312507; JAYNES ET, 1982, P IEEE, V70, P939, DOI 10.1109/PROC.1982.12425; MANNING C, 1999, FDN STAT NATURAL LAN, P578; McCallum A. K., 1996, BOW TOOLKIT STAT LAN; Medicine Io, 2001, CROSSING QUALITY CHA; Mitchell T., 1997, MACH LEARN, P154; Morioka C A, 2001, Proc AMIA Symp, P468; MORIOKA CA, 2003, P AMIA S, P475; *NAT EL MAN ASS, 2000, DIG IM COMM MED DI 3; Ratnaparkhi A, 1999, MACH LEARN, V34, P151, DOI 10.1023/A:1007502103375; TAIRA RK, 1988, AM J ROENTGENOL, V150, P1117; VALENTINO DJ, 1995, P SOC PHOTO-OPT INS, V2435, P28, DOI 10.1117/12.208792; Valentino DJ, 1998, P SOC PHOTO-OPT INS, V3335, P600, DOI 10.1117/12.312538	14	0	0	SPIE-INT SOC OPTICAL ENGINEERING	BELLINGHAM	1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA	0277-786X		0-8194-5722-1	P SOC PHOTO-OPT INS			2005	5748						318	325		10.1117/12.595920		8	Computer Science, Interdisciplinary Applications; Engineering, Biomedical; Radiology, Nuclear Medicine & Medical Imaging	Computer Science; Engineering; Radiology, Nuclear Medicine & Medical Imaging	BCF60	WOS:000229065900037	
B	Oatley, GC; Zeleznikow, J; Ewart, BW		Macintosh, A; Ellis, R; Allen, T		Oatley, GC; Zeleznikow, J; Ewart, BW			Matching and predicting crimes	APPLICATIONS AND INNOVATIONS IN INTELLIGENT SYSTEMS XII, PROCEEDINGS	BCS CONFERENCE SERIES		English	Proceedings Paper	24th SGAI International Conference on Innovative Techniques and Applications of Artificial Intelligence (AI-2004)	DEC 12-15, 2004	Cambridge, ENGLAND	British Comp Soc Specialist Grp Artificial Intelligence			SOFTWARE	Our central aim is the development of decision support systems based on appropriate technology for such purposes as profiling single and series of crimes or offenders, and matching and predicting crimes. This paper presents research in this area for the high-volume crime of Burglary Dwelling House, with examples taken from the authors' own work a United Kingdom police force. Discussion and experimentation include exploratory techniques from spatial statistics and forensic psychology. The crime matching techniques used are case-based reasoning, logic programming and ontologies, and naive Bayes augmented with spatio-temporal features. The crime prediction techniques are survival analysis and Bayesian networks.	Univ Sunderland, Sch Comp & Technol, Sunderland SR2 7EE, Durham, England	Oatley, GC (reprint author), Univ Sunderland, Sch Comp & Technol, Sunderland SR2 7EE, Durham, England.						ALLEN JF, 1983, COMMUN ACM, V26, P123; Bishop, 1995, NEURAL NETWORKS PATT; CARLIN JB, 2000, BAYES EMPIRICAL BAYE; CHEN H, 2004, IEEE COMPUTER, V37; EWART BW, 2004, MATCHING CRIMES USIN; EWART BW, 2003, INT J POLICE SCI MAN, V5; EWART BW, 1997, FORENSIC UPDATE, V50, P4; GREEN EJ, 1976, J POLICE SCI ADMIN, V4, P382; GUPTA KM, 1997, T SYSTEMS MAN CYBE A, V27, P601; HIRSCHFIELD A, 2001, MAPPING ANAL CRIME D, P237; Jaere MD, 2002, LECT NOTES ARTIF INT, V2416, P174; LEARY RM, 2003, FORENSIC TECHNOLOGY; MOONEY RJ, 2000, DATA MINING NEXT GEN; NOY NF, 2000, 12 INT C SOFTW ENG K; Oatley G, 2002, KNOWL-BASED SYST, V15, P323, DOI 10.1016/S0950-7051(01)00170-8; Oatley GC, 2003, EXPERT SYST APPL, V25, P569, DOI 10.1016/S0957-4174(03)00097-6; OATLEY GC, 2004, INTELLIGENT COMPUTIN; Pearl J., 1988, PROBABILISTIC REASON; PEASE K, 2001, MAPPING ANAL CRIME D, P225; Pease K., 1998, REPEAT VICTIMISATION, V90; POLVI N, 1991, BRIT J CRIMINOL, V31, P411; RATCLIGGE JH, 2002, J QUANTITATIVE CRIMI, V18; SOOMRO TR, 2001, P SCI 2001 ISAS 2001; Yokota K., 2002, INT J POLICE SCI MAN, V4, P5; ZELEZNIKOW J, 2002, P 5 INT C FOR STAT I	25	0	0	SPRINGER	NEW YORK	233 SPRING STREET, NEW YORK, NY 10013, UNITED STATES			1-85233-908-X	BCS CONFERENCE S			2005							19	32		10.1007/1-84628-103-2_2		14	Computer Science, Artificial Intelligence	Computer Science	BBP51	WOS:000226887200002	
S	Pancardo-Rodriguez, A; Montes-Y-Gomez, M; Villasenor-Pineda, L; Rosso, P		Gelbukh, A		Pancardo-Rodriguez, A; Montes-Y-Gomez, M; Villasenor-Pineda, L; Rosso, P			A mapping between classifiers and training conditions for WSD	COMPUTATIONAL LINGUISTICS AND INTELLIGENT TEXT PROCESSING	Lecture Notes in Computer Science		English	Article; Proceedings Paper	6th Annual Conference on Intelligent Text Processing and Computational Linguistics	FEB 13-19, 2005	Mexico City, MEXICO	Natl Polytech Inst, Nat Language & Text Proc Lab, Ctr Comp Res				This paper studies performance of various classifiers for Word Sense Disambiguation considering different training conditions. Our preliminary results indicate that the number and distribution of training examples has a great impact on the resulting precision. The Naive Bayes method emerged as the most adequate classifier for disambiguating words having few examples.	Univ Polytecn Valencia, Valencia, Spain		aaron_cyberman@inaoep.mx; mmontes@dsic.upv.es; villasen@inaoep.mx; prosso@dsic.upv.es	Villasenor-Pineda, Luis/A-2932-2009				MIHALCEA R, 2004, 3 INT WORKSH EV SYST; PALIOURAS G, 2000, P 2 INT C NAT LANG P; SNYDER B, 2004, 3 INT WORKSH EV SYST; ZAVREL J, 2000, P 2 CEVOLE WORKSH LE	4	0	0	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		3-540-24523-5	LECT NOTES COMPUT SC			2005	3406						246	249				4	Computer Science, Artificial Intelligence; Computer Science, Theory & Methods	Computer Science	BCD56	WOS:000228725100027	
S	Pappuswamy, U; Bhembe, D; Jordan, PW; VanLehn, K		Gelbukh, A		Pappuswamy, U; Bhembe, D; Jordan, PW; VanLehn, K			A supervised clustering method for text classification	COMPUTATIONAL LINGUISTICS AND INTELLIGENT TEXT PROCESSING	LECTURE NOTES IN COMPUTER SCIENCE		English	Article; Proceedings Paper	6th Annual Conference on Intelligent Text Processing and Computational Linguistics	FEB 13-19, 2005	Mexico City, MEXICO	Natl Polytech Inst, Nat Language & Text Proc Lab, Ctr Comp Res				This paper describes a supervised three-tier clustering method for classifying students' essays of qualitative physics in the Why2-Atlas tutoring system. Our main purpose of categorizing text in our tutoring system is to map the students' essay statements into principles and misconceptions of physics. A simple `bag-of-words' representation using a naive-bayes algorithm to categorize text was unsatisfactory for our purposes of analyses as it exhibited many misclassifications because of the relatedness of the concepts themselves and its inability to handle misconceptions. Hence, we investigate the performance of the k-nearest neighborhood algorithm coupled with clusters of physics concepts on classifying students' essays. We use a three-tier tagging schemata (cluster, sub-cluster and class) for each document and found that this kind of supervised hierarchical clustering leads to a better understanding of the student's essay.	Univ Pittsburgh, Ctr Learning Res & Dev, Pittsburgh, PA 15260 USA	Pappuswamy, U (reprint author), Univ Pittsburgh, Ctr Learning Res & Dev, 3939 O Hara St, Pittsburgh, PA 15260 USA.	umarani@pitt.edu					BAKER LD, 1998, ACM SIGIR 98; CHI MTH, 1994, COGNITIVE SCI, V18, P439, DOI 10.1207/s15516709cog1803_3; DUDA RO, 1973, PATTERN CLASSIFICATI, P95; ELYANIV R, 2001, EUR C MACH LEARN ECM, P121; Fix E., 1951, 4 USAF SCH AV MED, P261; GRAESSER AC, 2000, INTERACTIVE LEARNING, V8, P29; HOTHO A, 2003, 425 AIFB I APPL INF; MARON ME, 1961, J ACM, V8, P404, DOI 10.1145/321075.321084; PERIERA F, 1993, 31 ANN M ACL, P183; ROSE CP, 2003, P HUM LANG TECHN C N; Sebastiani F, 2002, ACM COMPUT SURV, V34, P1, DOI 10.1145/505282.505283; Slonim N, 2001, P ECIR 01 23 EUR C I; SLONIM N, 2002, P SIGIR 02 25 ACM IN; SLOTTA JD, 1995, COGNITION INSTRUCT, V13, P373, DOI 10.1207/s1532690xci1303_2; VanLehn K, 2002, LECT NOTES COMPUT SC, V2363, P158	15	0	0	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		3-540-24523-5	LECT NOTES COMPUT SC			2005	3406						704	714				11	Computer Science, Artificial Intelligence; Computer Science, Theory & Methods	Computer Science	BCD56	WOS:000228725100078	
B	Peterson, MR; Raymer, ML; Lamont, GB			IEEE	Peterson, MR; Raymer, ML; Lamont, GB			Balanced accuracy for feature subset selection with genetic algorithms	2005 IEEE CONGRESS ON EVOLUTIONARY COMPUTATION, VOLS 1-3, PROCEEDINGS	IEEE Congress on Evolutionary Computation		English	Proceedings Paper	IEEE Congress on Evolutionary Computation	SEP 02-05, 2005	Edinburgh, SCOTLAND	IEEE, IEEE Computat Intelligence Soc, IEE, Evolut Programming Soc			KNOWLEDGE DISCOVERY; OPTIMIZATION	The relevance of a set of measured features describing labelled patterns within a problem domain affects classifier performance. Feature subset selection algorithms employing a wrapper approach typically assess the fitness of a feature subset simply as the accuracy of a given classifier over a set of available patterns using the candidate feature set. For datasets with many patterns for some classes and few for others, relatively high accuracy may be achieved simply by labelling unknown patterns according to the largest class. Feature selection wrappers that only emphasize high accuracy typically follow this bias. Class bias may be mitigated by emphasizing well-balanced accuracy during the optimization algorithm. This paper proposes adding selective pressure for balanced accuracy to mitigate class bias during feature set evolution. Experiments compare the selection performance of genetic algorithms using various fitness functions varying in terms of accuracy, balance, and feature parsimony. Several feature selection algorithms including greedy, genetic, filter, and hybrid filter/GA approaches are then compared using the best fitness function. The experiments employ a Naive Bayes classifier and public domain datasets. The results suggest that improvements to class balance and feature subset size can be made without compromising overall accuracy or run-time efficiency.	Wright State Univ, Dept CS & Egr, Dayton, OH 45435 USA	Peterson, MR (reprint author), Wright State Univ, Dept CS & Egr, Dayton, OH 45435 USA.	peterson.7@wright.edu; mraymer@cs.wright.edu; gary.lamont@afit.edu	Raymer, Michael/G-3398-2013	Raymer, Michael/0000-0003-2649-0792			Bala J, 1996, EVOL COMPUT, V4, P297, DOI 10.1162/evco.1996.4.3.297; BAYES T, 1763, PHILOS T ROY SOC LON, P53; Blake C. L., 1998, UCI REPOSITORY MACHI; BROTHERTON TW, 1995, EVOLUTIONARY PROGRAM, V4, P83; Cantu-Paz E, 2004, LECT NOTES COMPUT SC, V3102, P959; DUDA RO, 1973, PATTERN CLASSIFIATIO; Harik G, 1999, EVOL COMPUT, V7, P231, DOI 10.1162/evco.1999.7.3.231; Inza I, 2000, ARTIF INTELL, V123, P157, DOI 10.1016/S0004-3702(00)00052-7; Jain A, 1997, IEEE T PATTERN ANAL, V19, P153, DOI 10.1109/34.574797; Jain AK, 1982, HDB STATISTICS, V2, P835, DOI 10.1016/S0169-7161(82)02042-2; JAIN AK, 1987, IEEE T PATTERN ANAL, V9, P628; Kelly J. D. J., 1991, P 4 INT C GEN ALG TH, P377; MARRIL T, 1963, IEEE T INFORM THEORY, V9, P11; OZDEMIR M, 2001, IEEE MOUNT WORKSH SO, P53; PAPPA GL, 2004, APPL MULTIOBJECTIVE, V1, P603; Peterson MR, 2004, LECT NOTES COMPUT SC, V3102, P426; PUNCH WF, 1993, PROCEEDINGS OF THE FIFTH INTERNATIONAL CONFERENCE ON GENETIC ALGORITHMS, P557; Raymer ML, 2003, IEEE T SYST MAN CY B, V33, P802, DOI 10.1109/TSMCB.2003.816922; SIEDLECKI W, 1989, PATTERN RECOGN LETT, V10, P335, DOI 10.1016/0167-8655(89)90037-8; Somol P., 2000, P 15 INT C PATT REC, V2, P406, DOI 10.1109/ICPR.2000.906098; WHITNEY AW, 1971, IEEE T COMPUT, VC 20, P1100, DOI 10.1109/T-C.1971.223410; Yang J., 1998, FEATURE EXTRACTION C, P117	22	0	0	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA			0-7803-9363-5	IEEE C EVOL COMPUTAT			2005							2506	2513				8	Computer Science, Artificial Intelligence; Computer Science, Theory & Methods	Computer Science	BCZ86	WOS:000232173100333	
B	Qi, WM; Cai, WY; Ji, QL; Li, TZ; Chen, GD		Cheng, DH; Xu, BG		Qi, Weimin; Cai, Weiyou; Ji, Qiaoling; Li, Tianzhi; Chen, Guangda			Study of an improved Naive Bayes algorithm in data mining	Proceedings of the 24th Chinese Control Conference, Vols 1 and 2			English	Proceedings Paper	24th Chinese Control Conference	JUL 15-18, 2005	Canton, PEOPLES R CHINA	S China Univ Technol, IEEE Control Syst Soc, Soc Instrument & Control Engineer Japan, Inst Control, Automat & Syst Engineers Korea, Acad Math & Syst Sci, Chinese Acad Sci, Automat Soc Guangdong Prov, Automat Soc Guangzhou, Guilin Univ Elect Technol		data mining; rule of classify; Naive Bayes algorithm		In this paper. an improved Naive Bayes algorithm in data mining is studied. The information and knowledge gained can be used for training Naive Bayes algorithm online, which gives rise to the ability to deal with huge amounts of database quickly and continuously. The program process of the algorithm is also proposed and the experiment result indicates that the ability of on-line learning can be ensured and the algorithm possesses good stability.	Wuhan Univ, Coll Power & Mech Engn, Wuhan 430072, Peoples R China	Qi, WM (reprint author), Wuhan Univ, Coll Power & Mech Engn, Wuhan 430072, Peoples R China.						Berry RS, 1997, MICROSCALE THERM ENG, V1, P1, DOI 10.1080/108939597200386; HECKERMAN DE, 1988, EMPIRICAL COMPARISON, P158; JOHN GH, 1995, P 11 C UNC ART INT, P338; Michie D., 1994, MACHINE LEARNING NEU; Mitchell T.M., 1997, MACHINE LEARNING; MITCHELL TM, 1996, MACH LEARN, P177; Yang Y., 1999, J INFORMATION RETRIE, V1, P67	7	0	0	SOUTH CHINA UNIV TECHNOLOGY PRESS	GUANGZHOU	GUANGZHOU 510641, GUANGDONG, PEOPLES R CHINA			7-5623-2249-X				2005							1305	1307				3	Automation & Control Systems; Computer Science, Artificial Intelligence	Automation & Control Systems; Computer Science	BEL69	WOS:000238019100283	
S	Rudrapatna, M; Mai, V; Sowmya, A; Wilson, P		Christensen, GE; Sonka, M		Rudrapatna, M; Mai, V; Sowmya, A; Wilson, P			Knowledge-driven automated detection of pleural plaques and thickening in high resolution CT of the lung	INFORMATION PROCESSING IN MEDICAL IMAGING, PROCEEDINGS	LECTURE NOTES IN COMPUTER SCIENCE		English	Article; Proceedings Paper	19th International Conference on Information Processing in Medical Imaging	JUL 10-15, 2005	Glenwood Springs, CO	Whitaker Fdn, Natl Inst Biomed Imaging & Bioengn, Univ Iowa, Obermann Ctr Adv Studies, Univ Iowa, Dept Elect Engn, Univ Iowa, Coll Engn			PULMONARY NODULES; DISEASE; BENIGN	Consistent efforts are being made to build Computer-Aided Detection and Diagnosis systems for radiological images. Such systems depend on automated detection of various disease patterns, which are then combined together to obtain differential diagnosis. For diffuse lung diseases, over 12 disease patterns are of interest in High Resolution Computed Tomography (HRCT) scans of the lung. In this paper, we present an automated detection method for two such patterns, namely Pleural Plaque and Diffuse Pleural Thickening. These are characteristic features of asbestos-related benign pleural disease. The attributes used for detection are derived from anatomical knowledge and the heuristics normally used by radiologists, and are computed automatically for each scan. A probabilistic model built on the attributes using naive Bayes classifier is applied to recognise the features in new scans, and preliminary results are presented. The technique is tested on 140 images from 13 studies and validated by an experienced radiologist.	Univ New S Wales, Sch Comp Sci & Engn, Sydney, NSW 2052, Australia; MIA Network, IMED, Sydney, NSW 2000, Australia	Rudrapatna, M (reprint author), Univ New S Wales, Sch Comp Sci & Engn, Sydney, NSW 2052, Australia.	mamathar@cse.unsw.edu.au; vmma518@cse.unsw.edu.au; sowmya@cse.unsw.edu.au; pcwilson2@bigpond.com					Armato SG, 2001, MED PHYS, V28, P1552, DOI 10.1118/1.1387272; CHABAT F, 2001, IEEE T MED IMAGING, V20; Delorme S, 1997, INVEST RADIOL, V32, P566, DOI 10.1097/00004424-199709000-00009; Domingos P, 1997, MACH LEARN, V29, P103, DOI 10.1023/A:1007413511361; Kanazawa K, 1998, COMPUT MED IMAG GRAP, V22, P157, DOI 10.1016/S0895-6111(98)00017-2; Kauczor HU, 2000, AM J ROENTGENOL, V175, P1329; Matsuki Y, 2002, AM J ROENTGENOL, V178, P657; McNitt-Gray MF, 1999, MED PHYS, V26, P880, DOI 10.1118/1.598603; Misra A, 2004, Proceedings of the 2004 Intelligent Sensors, Sensor Networks & Information Processing Conference, P451; Peacock C, 2000, CLIN RADIOL, V55, P422, DOI 10.1053/crad.2000.0450; RUDRAPATNA M, 2004, P IEEE C CYB INT SYS, P768; Uppaluri R, 1999, AM J RESP CRIT CARE, V160, P648; WANG C, 2004, P 4 IND C COMP VIS G, P485; Webb W.R., 2001, HIGH RESOLUTION CT L; White CS, 1996, RADIOLOGY, V199, P109; Witten I.H., 2000, DATA MINING PRACTICA	16	0	0	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		3-540-26545-7	LECT NOTES COMPUT SC			2005	3565						270	285				16	Computer Science, Information Systems; Computer Science, Interdisciplinary Applications; Computer Science, Theory & Methods; Imaging Science & Photographic Technology; Radiology, Nuclear Medicine & Medical Imaging	Computer Science; Imaging Science & Photographic Technology; Radiology, Nuclear Medicine & Medical Imaging	BCR05	WOS:000230871900023	
S	Rus, V; Desai, K		Gelbukh, A		Rus, V; Desai, K			Assigning function tags with a simple model	COMPUTATIONAL LINGUISTICS AND INTELLIGENT TEXT PROCESSING	LECTURE NOTES IN COMPUTER SCIENCE		English	Article; Proceedings Paper	6th Annual Conference on Intelligent Text Processing and Computational Linguistics	FEB 13-19, 2005	Mexico City, MEXICO	Natl Polytech Inst, Nat Language & Text Proc Lab, Ctr Comp Res				This paper presents a method to assign function tags based on a Naive Bayes approach. The method takes as input a parse tree and labels certain constituents with a set of functional marks such as logical subject, predicate, etc. The performance reported is promising, given the simplicity of a Naive Bayes approach, when compared with similar work.	Univ Memphis, Dept Comp Sci, Inst Intelligent Syst, Fedex Inst Technol, Memphis, TN 38120 USA	Rus, V (reprint author), Univ Memphis, Dept Comp Sci, Inst Intelligent Syst, Fedex Inst Technol, Memphis, TN 38120 USA.	vrus@memphis.edu					Bies A, BRACKETING GUIDELINE; Blaheta D., 2000, P 1 ANN M N AM CHAPT, P234; COLLINS M, P 35 ANN M ASS COMP; Friedman JH, 1997, DATA MIN KNOWL DISC, V1, P55, DOI 10.1023/A:1009778005914; JIJKOUN VDM, P ACL 2004; JOHNSON M, P 40 ANN M ASS COMP; Mccallum A., 1998, WORKSH LEARN TEXT CA	7	0	0	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		3-540-24523-5	LECT NOTES COMPUT SC			2005	3406						112	115				4	Computer Science, Artificial Intelligence; Computer Science, Theory & Methods	Computer Science	BCD56	WOS:000228725100010	
S	Saito, K; Nakano, R			IEEE	Saito, K; Nakano, R			Weight sharing on naive Bayes document model	Proceedings of the International Joint Conference on Neural Networks (IJCNN), Vols 1-5	IEEE International Joint Conference on Neural Networks (IJCNN)		English	Proceedings Paper	IEEE International Joint Conference on Neural Networks (IJCNN 2005)	JUL 31-AUG 04, 2005	Montreal, CANADA	Int Neural Network Soc, IEEE Computat Intelligence Soc			NEURAL-NETWORKS; ALGORITHM	In this paper, we study weight sharing on the naive Bayes document model. Firstly we consider splitting words into a relatively small number of groups such that words in each group have the same parameter value. This problem can be regarded as a probabilistic parameter sharing task. In this task, we formalize the problem in terms of maximum likelihood estimation, and then propose an algorithm for this purpose. Secondly we focus on an adaptive hyperparameter estimation problem based on prior distributions constructed by using such word groups. This problem can be regarded as a hyperparameter sharing task. In this task, we describe a framework and algorithm, which enables to derive the unique optimal solution in the context of leave-one-out cross validation. In our experiments using a benchmark document set called webkb, we show a series of simulation results using the proposed algorithms.	NTT Commun Sci Labs, Kyoto 6190237, Japan	Saito, K (reprint author), NTT Commun Sci Labs, 2-4 Hikaridai, Kyoto 6190237, Japan.						Barabasi AL, 2004, NAT REV GENET, V5, P101, DOI 10.1038/nrg1272; Bishop C.M., 1995, NEURAL NETWORKS PATT; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1; Duda R. O., 2000, PATTERN CLASSIFICATI; Fletcher R., 1987, PRACTICAL METHODS OP; Haykin S., 1999, NEURAL NETWORKS COMP; Luenberger D, 1984, LINEAR NONLINEAR PRO; Manning C.D., 1999, FDN STAT NATURAL LAN; Nakano R, 2002, LECT NOTES ARTIF INT, V2281, P482; NIGAM K, 1999, IJCAI 1999 WORKSH MA; NOWLAN SJ, 1992, NEURAL COMPUT, V4, P473, DOI 10.1162/neco.1992.4.4.473; PORTER MF, 1980, PROGRAM-AUTOM LIBR, V14, P130, DOI 10.1108/eb046814; SAITO K, 2002, P 5 INT C DISC SCI, P206; salton G., 1988, AUTOMATIC TEXT PROCE; TOWELL GG, 1993, MACH LEARN, V13, P71, DOI 10.1023/A:1022683529158	16	0	0	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA	1098-7576		0-7803-9048-2	IEEE IJCNN			2005							576	581				6	Computer Science, Artificial Intelligence	Computer Science	BDS40	WOS:000235178000087	
B	Song, MH; Lim, SY; Kang, DJ; Park, SB; Lee, SJ		Arabnia, HR; Joshua, R		Song, MH; Lim, SY; Kang, DJ; Park, SB; Lee, SJ			E-mail classification using SVM learning algorithm	ICAI '05: Proceedings of the 2005 International Conference on Artificial Intelligence, Vols 1 and 2			English	Proceedings Paper	International Conference on Artificial Intelligence (ICAI 05)	JUN 27-30, 2005	Las Vegas, NV	CSREA, Int Technol Inst, World Acad Sci Informat Technol, HPCwire, GRIDtoday		E-mail classification; SVM; support vector machine		Due to the distribution of personal computers and the internet, E-mail has become one of the most widely used communicative means. However, a massive amount of spam mail is polluting mailboxes everyday, taking advantage of the ability to send mail to any number of random people through the internet. In this paper we will introduce an efficient method of classifying E-mails using the SVM (Support Vector Machine) learning algorithm, which is recently showing high performance in the field of classifying documents. The disposition of the words inside the E-mail documents are extracted, and the performance of classification is compared and examined through the learning based on the change of DF value which occurs to reduce the disposition space in the learning level. To assess the performance of the SVM, the SVM is compared to the Naive Bayes classifier (which uses probability methods) and a vector model classifier in order to verify that the method of using the learning algorithm of SVM shows better performance.	Kyungpook Natl Univ, Dept Comp Engn, Informat Technol Serv, Taejon 702701, South Korea	Song, MH (reprint author), Kyungpook Natl Univ, Dept Comp Engn, Informat Technol Serv, Sankyuk Dong 1370, Taejon 702701, South Korea.						Bekkerman R., 2001, P SIGIR 01 24 ACM IN, P146, DOI 10.1145/383952.383976; Cohen W.W., 1996, AAAI SPRING S MACH L, P18; CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; Drucker Harris, 1999, IEEE T NEURAL NETWOR, V10; Joachims T., 1998, EUR C MACH LEARN; Li Tao, 2003, P SIGIR 2003 26 ANN, P421; Vapnik V.N., 1995, NATURAL STAT LEARNIN	7	0	0	C S R E A PRESS	ATHENS	115 AVALON DR, ATHENS, GA 30606 USA			1-932415-68-8				2005							586	592				7	Computer Science, Artificial Intelligence	Computer Science	BDY19	WOS:000236070700086	
S	Takamura, H; Okumura, M		Su, KY; Tsujii, J; Lee, JH; Kwong, OY		Takamura, H; Okumura, M			A comparative study on the use of labeled and unlabeled data for large margin classifiers	NATURAL LANGUAGE PROCESSING - IJCNLP 2004	LECTURE NOTES IN COMPUTER SCIENCE		English	Article; Proceedings Paper	1st International Joint Conference on Natural Language Processing (IJCNLP 2004)	MAR 22-24, 2004	Hainan Isl, PEOPLES R CHINA				EM ALGORITHM	We propose to use both labeled and unlabeled data with the Expectation-Maximization (EM) algorithm in order to estimate the generative model and use this model to construct a Fisher kernel. The Naive Bayes generative probability is used to model a document. Through the experiments of text categorization, we empirically show that, (a) the Fisher kernel with labeled and unlabeled data outperforms Naive Bayes classifiers with EM and other methods for a sufficient amount of labeled data, (b) the value of additional unlabeled data diminishes when the labeled data size is large enough for estimating a reliable model, (c) the use of categories as latent variables is effective, and (d) larger unlabeled training datasets yield better results.	Tokyo Inst Technol, Precis & Intelligence Lab, Midori Ku, Yokohama, Kanagawa 2268503, Japan	Takamura, H (reprint author), Tokyo Inst Technol, Precis & Intelligence Lab, Midori Ku, 4259 Nagatsuta, Yokohama, Kanagawa 2268503, Japan.	takamura@pi.titech.ac.jp; oku@pi.titech.ac.jp					DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1; Haussler D., 1998, ADV NEURAL INFORM PR, V11, P487; HERBRICH R, 2000, ADV NEURAL INFORMATI, V12, P224; Hofmann T, 2000, ADV NEUR IN, V12, P914; HOFMANN T, 1998, AIM1625 MIT; Joachims T., 1999, P 16 INT C MACH LEAR, P200; Joachims T., 1998, P 10 EUR C MACH LEAR, P137; Kamakura K, 2000, INTERNAL MED, V39, P2, DOI 10.2169/internalmedicine.39.2; Kass R., 1997, GEOMETRICAL FDN ASYM; Kressel UHG, 1999, ADVANCES IN KERNEL METHODS, P255; KUDO T, 2001, P 2 M N AM CHAPT ASS, P192; McCallum A, 1998, P AAAI 98 WORKSH LEA, P41; Smola A. J., 2000, ADV LARGE MARGIN CLA; Tsuda K, 2002, NEURAL COMPUT, V14, P2397, DOI 10.1162/08997660260293274; TSUDA K, 2002, P INT C ART NEUR NET, P727; Ueda N, 1998, NEURAL NETWORKS, V11, P271, DOI 10.1016/S0893-6080(97)00133-0; Vapnik V., 1998, STAT LEARNING THEORY	17	0	0	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		3-540-24475-1	LECT NOTES COMPUT SC			2005	3248						456	465				10	Computer Science, Artificial Intelligence	Computer Science	BBZ57	WOS:000228359800048	
J	Thomas, CS; Howie, CA; Smith, LS				Thomas, Clifford S.; Howie, Catherine A.; Smith, Leslie S.			A New Singly Connected Network Classifier based on Mutual Information	INTELLIGENT DATA ANALYSIS			English	Article								For reasoning under uncertainty the Bayesian Network has become the representation of choice. However, except were models are considered 'simple' the tasks of construction and inference are provably NP hard. For modelling larger real-world problems this computational complexity has been addressed by methods that approximate the model. The Naive Bayes (NB) Classifier which has strong assumptions of independence among features is a common approach whilst the class of trees another less extreme example. The aim of this paper is to investigate the use of an information theory based technique as a mechanism for inference in Singly Connected Networks (SCN) or 'polytrees'. We call this variant a Mutual Information Measure (MIM) Classifier. We experimentally evaluate this new approach and compare the resulting classification performance of the MIM Classifier against (a) a Naive Bayes Classifier, (b) a General Bayesian Network (GBN) Classifier and (c) a Singly Connected Network, using benchmark problems taken from the UCI repository. With respect to (a) we show that the MIM Classifier generally performs better than the NB Classifier. For (b) and (c) we show that the MIM Classifier is comparable with both the GBN and SCN Classifiers and in most data sets used performs marginally better.	Univ Stirling, Dept Math & Comp Sci, Stirling FK9 4LA, Scotland	Thomas, CS (reprint author), Univ Stirling, Dept Math & Comp Sci, Stirling FK9 4LA, Scotland.	cst@cs.stir.ac.uk					ACID S, 2001, LECT NOTES ARTIF INT, V2143, P216; BATTITI R, 1994, IEEE T NEURAL NETWOR, V5, P537, DOI 10.1109/72.298224; BLANCO R, 2001, EUR C ART INT MED AI, P29; BOERLAGE B, 1995, THESIS U BRIT COLUMB; Buntine W, 1996, IEEE T KNOWL DATA EN, V8, P195, DOI 10.1109/69.494161; Cao J, 2003, BIOINFORMATICS, V19, P234, DOI 10.1093/bioinformatics/19.2.234; CHENG J, 2001, LEARNING BAYESIAN BE; CHENG J, 1998, POWER CONSTRUCTOR SY; CHOW CK, 1968, IEEE T INFORM THEORY, V14, P462, DOI 10.1109/TIT.1968.1054142; COOPER GF, 1990, ARTIF INTELL, V42, P393, DOI 10.1016/0004-3702(90)90060-D; Cormen T. H., 1990, INTRO ALGORITHMS; DAGUM P, 1993, ARTIF INTELL, V60, P141, DOI 10.1016/0004-3702(93)90036-B; DASGUPTA S, 1999, P UAI 99; DECAMPOS LM, 1996, DECSAI960204 U GRAND; Domingos P, 1997, MACH LEARN, V29, P103, DOI 10.1023/A:1007413511361; DRAPER D, 1995, LOCALIZED PARTIAL EV; Duda R., 1973, PATTERN CLASSIFICATI; EZAWA K, 1995, KNOWLEDGE DISCOVERY; FERRERIRA JTA, 2001, WEIGHTED NAIVE BAYES; FRIEDMAN N, 1996, LEARNING BAYESIAN NE; FUNG R, 1995, COMMUNICATION ACM, V38; GEIGER D, 1992, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P92; HECKERMAN D, 1995, MACH LEARN, V20, P197, DOI 10.1007/BF00994016; Heckerman D., 1998, LEARNING GRAPHICAL M, P01; HELLERSTEIN J, 2000, RECOGNIZING END USER, P506; JENSEN F, 2001, BAYESIAN NETWORKS DE; Jensen F. V., 1996, INTRO BAYESIAN NETWO; JITNAH N, 1997, LECT NOTES ARTIFICIA; KLEITER GD, 1996, P 6 INT C IPMU 1996; KOHAVI R, 1994, P 6 INT C TOOLS ART; Kohavi R., 1995, P 14 INT JOINT C ART, P1137; KULLBACK S, 1951, ANN MATH STAT, V22, P79, DOI 10.1214/aoms/1177729694; Kullback S., 1968, INFORM THEORY STAT; LAM W, 1994, COMPUTATIONAL INTELL, V10; LANGLEY P, 1992, AAAI-92 PROCEEDINGS : TENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE, P223; LANGLEY P, 1999, TRACTABLE AVERAGE CA, P220; LANGLEY P, 1991, INDUCTION SELECTIVE; Larranaga P, 1996, IEEE T SYST MAN CY A, V26, P487, DOI 10.1109/3468.508827; MCNAUGHT K, 2001, EUR C ART INT MED AI, P53; Murphy P. M., 1995, UCI REPOSITORY MACHI; NICHOLSON AE, 1997, USING MUTUAL INFORM; PAN HP, 2002, LEARNING BAYESIAN NE; Pearl J., 1988, PROBABILISTIC REASON; PEDERSEN T, 1998, AAAI SPR S SAT MOD, P60; REBANE G, 1987, RECOVERY CAUSAL POLY; Rish I., 2001, ANAL DATA CHARACTERI; RISH I, 2001, 17 INT JOINT C ART I; Shannon CE, 1949, MATH THEORY COMMUNIC; SINGH M, 1996, INT C MACH LEARN, P453; Singh M., 1993, Uncertainty in Artificial Intelligence. Proceedings of the Ninth Conference (1993); Spirtes P., 1991, Social Science Computer Review, V9, DOI 10.1177/089443939100900106; SPIRTES P, 1990, P ADV COMP SOC SCI W; VERMA T., 1992, P 8 C UNC ART INT, P323; Yang ZR, 2001, IEEE T PATTERN ANAL, V23, P396	54	0	0	IOS PRESS	AMSTERDAM	NIEUWE HEMWEG 6B, 1013 BG AMSTERDAM, NETHERLANDS	1088-467X			INTELL DATA ANAL	Intell. Data Anal.		2005	9	2					189	205				17	Computer Science, Artificial Intelligence	Computer Science	V43YA	WOS:000202969200005	
S	Wang, LM; Cao, CH; Li, HJ; Chen, HX; Dong, LY				Wang, LM; Cao, CH; Li, HJ; Chen, HX; Dong, LY			Orthogonally rotational transformation for naive Bayes learning	COMPUTATIONAL INTELLIGENCE AND SECURITY, PT 1, PROCEEDINGS	LECTURE NOTES IN ARTIFICIAL INTELLIGENCE		English	Article; Proceedings Paper	International Conference on Computational Intelligence and Security	DEC 15-19, 2005	Xi'an, PEOPLES R CHINA	IEEE Computat Intelligence, Hong Kong Chapter, Xidian Univ, Hong Kong Baptist Univ, Natl Nat Sci Fdn China, Guangdong Univ Technol				Naive Bayes is one of the most efficient and effective learning algorithms for machine learning, pattern recognition and data mining. But its conditional independence assumption is rarely true in real-world applications. We show that the independence assumption can be approximated by orthogonally rotational transformation of input space. During the transformation process, the continuous attributes are treated in different ways rather than simply applying discretization or assuming them to satisfy some standard probability distribution. Furthermore, the information from unlabeled instances can be naturally utilized to improve parameter estimation without considering the negative effect caused by missing class labels. The empirical results provide evidences to support our explanation.	Jilin Univ, Coll Comp Sci & Technol, Changchun 130012, Peoples R China; Northeastern Univ, Coll Informat Sci & Engn, Shenyang 110004, Peoples R China; Yantai Univ, Coll Comp Sci, Yantai 264005, Peoples R China	Wang, LM (reprint author), Jilin Univ, Coll Comp Sci & Technol, Changchun 130012, Peoples R China.	jeffreywlm@sina.com					Chun-Nan Hsu, 2003, Machine Learning, V53, DOI 10.1023/A:1026367023636; HARRY Z, 2003, LECT NOTES COMPUTER, V2671, P591; JOAO G, 2000, LECT NOTES COMPUTER, V1952, P49; LIMIN W, 2004, PROGR NATURAL SCI, V14, P541; LIMIN W, 2004, LECT NOTES COMPUTER, V3288, P1056; MARCO B, 2002, LECT NOTES COMPUTER, V2527, P1; MARCO R, 2001, ARTIF INTELL, V125, P209; STIJN V, 2002, LECT NOTES COMPUTER, V2454, P202	8	0	0	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		3-540-30818-0	LECT NOTES ARTIF INT			2005	3801						145	150				6	Computer Science, Artificial Intelligence; Computer Science, Information Systems; Computer Science, Theory & Methods	Computer Science	BDQ19	WOS:000234873700021	
B	Wang, XR; Brown, AJ; Upcroft, B			IEEE	Wang, XR; Brown, AJ; Upcroft, B			Applying incremental EM to Bayesian classifiers in the learning of hyperspectral remote sensing data	2005 7th International Conference on Information Fusion (FUSION), Vols 1 and 2			English	Proceedings Paper	7th International Conference on Information Fusion (FUSION)	JUL 25-28, 2005	Philadelphia, PA	IEEE		Bayesian networks; incremental EM; hyperspectral imaging		In this paper, we apply the incremental EM method to Bayesian Network Classifiers to learn and interpret hyperspectral sensor data in robotic planetary missions. Hyperspectral image spectroscopy is an emerging technique for geological investigations from airborne or orbital sensors. Many spacecraft carry spectroscopic equipment as wavelengths outside the visible light in the electromagnetic spectrum give much greater information about an object. The algorithm used is an extension to the standard Expectation Maximisation (EM). The incremental method allows us to learn and interpret the data as they become available. Two Bayesian network classifiers were tested: the Naive Bayes, and the Tree-Augmented-Naive Bayes structures. Our preliminary experiments show that incremental learning with unlabelled data can improve the accuracy of the classifier.	Univ Sydney, Ctr Autonomous Syst, ACFR JO4, Sydney, NSW 2006, Australia	Wang, XR (reprint author), Univ Sydney, Ctr Autonomous Syst, ACFR JO4, Sydney, NSW 2006, Australia.		Wang, X. Rosalind/B-7097-2008				ASTER Spectral Library, ASTER SPECTRAL LIB; CHOW CK, 1968, IEEE T INFORM THEORY, V14, P462, DOI 10.1109/TIT.1968.1054142; Clark RN, 2003, J GEOPHYS RES-PLANET, V108, DOI 10.1029/2002JE001847; Cocks T, 1998, 1ST EARSEL WORKSHOP ON IMAGING SPECTROSCOPY, P37; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1; Friedman N, 2003, MACH LEARN, V50, P95, DOI 10.1023/A:1020249912095; Friedman N, 1997, MACH LEARN, V29, P131, DOI 10.1023/A:1007465528199; JENSEN F, 2001, BAYESIAN NETWORKS DE; Neal R., 1998, LEARNING GRAPHICAL M; RENCZ A. N., 1999, REMOTE SENSING EARTH; THOMPSON AJB, 1999, SOC EC GEOLOGISTS NE, V39; WALTER MR, 1993, ICARUS, V101, P129, DOI 10.1006/icar.1993.1011	12	0	0	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA			0-7803-9286-8				2005							606	613				8	Computer Science, Artificial Intelligence; Computer Science, Information Systems; Imaging Science & Photographic Technology	Computer Science; Imaging Science & Photographic Technology	BDP88	WOS:000234830400082	
S	Wang, XR; Ramos, FT			IEEE	Wang, XR; Ramos, FT			Applying structural EM in autonomous planetary exploration missions using hyperspectral image spectroscopy	2005 IEEE International Conference on Robotics and Automation (ICRA), Vols 1-4	IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION		English	Proceedings Paper	IEEE International Conference on Robotics and Automation (ICRA)	APR 18-22, 2005	Barcelona, SPAIN	IEEE		Bayesian networks; structural EM; Hyperspectral Imaging; Planetary Exploration		In this paper, we use the Bayesian Structural EM algorithm as a classification method to learn and interpret hyperspectral sensor data in robotic planetary missions. Hyperspectral image spectroscopy is an emerging technique for geological investigations from airborne or orbital sensors. Many spacecraft carry spectroscopic equipment as wavelengths outside the visible light in the electromagnetic spectrum give much greater information about an object. The algorithm presented combines the standard Expectation Maximisation (EM), which optimises parameters, with structure search for model selection. We use the Bayesian Information Criterion (BIC) score to learn the network structure. The procedure only converges to a local maxima, thus requiring a good initial graph structure. Two initial structures are used: the Naive Bayes, and the Tree-Augmented-Naive Bayes structures. Our preliminary experiments show that the former results in a structure that can correctly determine the presence and types of minerals with merely 13% accuracy while the latter results in a structure that has approximately 94% accuracy.	Univ Sydney, ARC Ctr Excellence Autonomous Syst CAS, Australian Ctr Field Robot, Sydney, NSW 2006, Australia	Wang, XR (reprint author), Univ Sydney, ARC Ctr Excellence Autonomous Syst CAS, Australian Ctr Field Robot, JO4, Sydney, NSW 2006, Australia.		Wang, X. Rosalind/B-7097-2008				CHOW CK, 1968, IEEE T INFORM THEORY, V14, P462, DOI 10.1109/TIT.1968.1054142; Cocks T, 1998, 1ST EARSEL WORKSHOP ON IMAGING SPECTROSCOPY, P37; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1; FRIEDMAN N, 1997, 14 INT C MACH LEARN; Friedman N, 2003, MACH LEARN, V50, P95, DOI 10.1023/A:1020249912095; Friedman N, 1997, MACH LEARN, V29, P131, DOI 10.1023/A:1007465528199; Friedman N., 1998, 14 C UNC ART INT UAI; JENSEN F, 2001, BAYESIAN NETWORKS DE; Murphy K., 2002, THESIS UC BERKELEY; Nijman W, 1998, PRECAMBRIAN RES, V88, P25, DOI 10.1016/S0301-9268(97)00062-4; RENCZ A. N., 1999, REMOTE SENSING EARTH; THOMPSON AJB, 1999, SOC EC GEOLOGISTS NE, V39	12	0	0	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA	1050-4729		0-7803-8914-X	IEEE INT CONF ROBOT			2005							4284	4289				6	Robotics	Robotics	BDU48	WOS:000235460103119	
S	Wu, WL; Lu, RZ; Gao, F; Yuan, Y		Gelbukh, A		Wu, WL; Lu, RZ; Gao, F; Yuan, Y			Combining multiple statistical classifiers to improve the accuracy of task classification	COMPUTATIONAL LINGUISTICS AND INTELLIGENT TEXT PROCESSING	LECTURE NOTES IN COMPUTER SCIENCE		English	Article; Proceedings Paper	6th Annual Conference on Intelligent Text Processing and Computational Linguistics	FEB 13-19, 2005	Mexico City, MEXICO	Natl Polytech Inst, Nat Language & Text Proc Lab, Ctr Comp Res				Task classification is an important subproblem of Spoken Language Understanding (SLU) in automated systems providing natural language user interface, whose goal is to identify the topic of a query from the user. This paper presents a combination of multiple statistical classifiers to improve the accuracy of task classification in the context of city public transportation information inquiry domain. Three different typical types of statistical classifiers are trained on the same data to be the base classifiers of the combination system: naive bayes classifier, n-gram model, and support vector machines. The combination method of two-stage classification is emplored to yield better overall performance. Our experiments showed that support vector machines outperform excessively the other base classifiers for task classification in our domain. The comparative experimental results between two-stage classification and voting strategy indicated, under the circumstance that the best base classifier has the overwhelming performance over the other base classifiers, the strategy of two-stage classification was more effective and could produce better results than the best component classifier.	Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai 200030, Peoples R China	Wu, WL (reprint author), Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai 200030, Peoples R China.	wu-wl@cs.sjtu.edu.cn; lu-rz@cs.sjtu.edu.cn; gaofeng@cs.sjtu.edu.cn; yuanyan@sjtu.edu.cn					CETTOLO M, 1996, P ICSLP; CHELBA C, 2003, P ICASSP         APR; LARKEY LS, P 19 ANN INT C RES D, P289; Lee CH, 2000, SPEECH COMMUN, V31, P309, DOI 10.1016/S0167-6393(99)00064-3; MANNING CD, 2000, FDN STAT NATURAL LAN, P589; MARINO JB, 2000, P INT WORKSH VER LAR, P57; MARQUEZ L, 2000, LSI0045R U POL CAT D; MAST M, 1996, P ICSLP; MITCHELL TM, 1996, MACH LEARN, P52; NEY H, 1994, COMPUT SPEECH LANG, V8, P1, DOI 10.1006/csla.1994.1001; QUINLAN JR, 1996, C4 5 PROGRAMS MACHIN; Sebastiani F, 2002, ACM COMPUT SURV, V34, P1, DOI 10.1145/505282.505283; van Halteren H, 2001, COMPUT LINGUIST, V27, P199; Wang Y.-Y., 2002, P 7 INT C SPOK LANG, P609; WU WL, 2003, P INT C NAT LANG PRO, P492	15	0	0	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		3-540-24523-5	LECT NOTES COMPUT SC			2005	3406						452	462				11	Computer Science, Artificial Intelligence; Computer Science, Theory & Methods	Computer Science	BCD56	WOS:000228725100050	
B	Yang, S; Song, JP; Rajamanij, H; Cho, T; Zhang, Y; Mooney, R			IEEE	Yang, Stewart; Song, Jianping; Rajamanij, Harish; Cho, Taewon; Zhang, Yin; Mooney, Raymond			Fast and effective worm fingerprinting via machine learning	3rd International Conference on Autonomic Computing, Proceedings			English	Proceedings Paper	3rd International Conference on Autonomic Computing (ICAC 2006)	2006	Dublin, IRELAND	IEEE Comp Soc, intel, IBM, Microsoft Res, BT, hp, Fujitsu				As Internet worms become ever faster and more sophisticated, it is important to be able to extract worm signatures in an accurate and timely manner. In this paper, we apply machine learning to automatically fingerprint polymorphic worms, which are able to change their appearance across every instance. Using real Internet traces and synthetic polymorphic worms, we evaluated the performance of several advanced machine learning algorithms, including naive Bayes, decision-tree induction, rule learning (RIPPER), and support vector machines. The results are very promising. Compared with Polygraph, the state of the art in polymorphic worm fingerprinting, several machine learning algorithms are able to generate more accurate signatures, tolerate more noise in the training data, and require much shorter training time. These results open the possibility of applying machine learning to build a fast and accurate online worm fingerprinting system.	Univ Texas, Dept Comp Sci, Austin, TX 78712 USA	Yang, S (reprint author), Univ Texas, Dept Comp Sci, Austin, TX 78712 USA.						Kim H.-A., 2004, P USENIX SEC S; KOLTER JZ, 2004, KDD 04 P 2004 ACM SI, P470; Newsome J., 2005, P IEEE S SEC PRIV; Witten I.H., 1999, DATA MINING PRACTICA	4	0	0	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA			1-4244-0175-5				2005							311	313				3	Computer Science, Theory & Methods	Computer Science	BFB57	WOS:000240798700039	
S	Yeh, CY; Wu, CH; Doong, SH			IEEE	Yeh, CY; Wu, CH; Doong, SH			Effective spam classification based on meta-heuristics	INTERNATIONAL CONFERENCE ON SYSTEMS, MAN AND CYBERNETICS, VOL 1-4, PROCEEDINGS	IEEE International Conference on Systems Man and Cybernetics Conference Proceedings		English	Proceedings Paper	IEEE International Conference on Systems, Man and Cybernetics	OCT 10-12, 2005	Waikoloa, HI	IEEE Syst, Man & Cybernet Soc		machine learning; support vector machines; decision trees; Naive Bayesian; spam; classification; meta-heuristics		Using machine learning techniques such as Naive Bayes, decision trees and support vector machines to automatically filter out spam e-mails has drawn many researchers' attention. Previous methods use keywords contained in e-mails to extract binary features from the corpus. However, since keywords of e-mails change from time to time, the performance of keyword-based solution is not stable. In this study, we use behaviors of spammers as the features for classifying e-mails. Such behaviors are first described by meta-heuristics and used as features of e-mails for classification. A total of 113 new features are extracted from the given meta-heuristics. Using existing machine learning techniques, the filtering performance is much better than that using keyword-based filtering. In addition, the training time is substantially reduced because of the low dimensional feature space and sparse feature vectors.	Shu Te Univ, Dept Informat Management, Kaohsiung, Taiwan	Yeh, CY (reprint author), Shu Te Univ, Dept Informat Management, Kaohsiung, Taiwan.	jofu@ms17.hinet.net; johnw@nuk.edu.tw; tungsh@mail.stu.edu.tw					Androutsopoulos I., 2000, P 23 ANN INT ACM SIG, P160, DOI 10.1145/345508.345569; Carreras X., 2001, P 4 INT C REC ADV NA; Chang C.C., 2001, LIBSVM LIB SUPPORT V; CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; CUNNINGHAM P, 2003, ICCBR 03 WORKSH LONG; Drucker H, 1999, IEEE T NEURAL NETWOR, V10, P1048, DOI 10.1109/72.788645; JONES RWM, ANNEXIA GREAT SPAM A; KATIRAI H, 1999, FILTERING JUNK E MAI; MARTIN B, 2002, SPAM FILTERING USING; Quinlan J. R., 1986, Machine Learning, V1, DOI 10.1023/A:1022643204877; RIOS G, 2004, P 1 C EMAIL ANTI SPA; SAHAMI M, 1998, 15 NAT C AI MAD WI, P55; SAKKIS G, 2001, MEMORY BASED APPROAC; SALTON G, 1973, J DOC, V29, P351, DOI 10.1108/eb026562; TSENG LS, 2003, P 3 INT C HYBR INT S, P1024; Vapnik V., 1998, STAT LEARNING THEORY	16	0	0	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA	1062-922X		0-7803-9298-1	IEEE SYS MAN CYBERN			2005							3872	3877				6	Computer Science, Artificial Intelligence; Computer Science, Cybernetics	Computer Science	BDT16	WOS:000235210803145	
S	Yoshihara, Y; Miura, T			IEEE Comp Soc	Yoshihara, Y; Miura, T			Melody classification using EM algorithm	Proceedings of the 29th Annual International Computer Software and Applications Conference	PROCEEDINGS - INTERNATIONAL COMPUTER SOFTWARE & APPLICATIONS CONFERENCE		English	Proceedings Paper	29th Annual International Computer Software and Applications Conference	JUL 26-28, 2005	Edinburgh, SCOTLAND	IEEE Comp Soc		melody classification; melody features; naive bayes; EM algorithm		fit this investigation we propose a novel technique to V melodies by using EM algorithm. Here we generate classify classifiers based on naive Rayesian technique during FM steps. And we discuss the experimental results with several features of melodies and show we can get efficient classifiers by our approach.	Hosei Univ, Dept Elect & Elect Engn, Koganei, Tokyo, Japan	Yoshihara, Y (reprint author), Hosei Univ, Dept Elect & Elect Engn, Kajinocho 3-7-2, Koganei, Tokyo, Japan.						DOWLING WJ, 1978, PSYCHOL REV, V85, P341, DOI 10.1037//0033-295X.85.4.341; DROETTBOOM M, 2002, INT S MUS INF RETR I; DROETTBOOM M, 2001, INT S MUS INF RETR I; GROSSMAN D, 1998, INF RETRIEVAL ALGORI; IWASAKI M, 2002, FDN INCMPLETE DATA A; KIM Y, 2000, INT S MUS INF RETR I; MIURA T, 2003, ACM C INF KNOWL MAN; NIGAM K, TEXT CLASSIFICATION; SHINNOU H, 2003, IPSJ J, V44, P3211; UEJIMA H, 2004, 16 IEEE INT C TOOLS; UITDENBOGERD AL, 1998, ACM MULT C; UITDENBOGERD AL, 1999, ACM MULT C; UITDENBOGERD AL, 2000, INT S MUS INF RETR I; YANG C, 2001, INT S MUS INF RETR I	14	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA	0730-3157		0-7695-2413-3	P INT COMP SOFTW APP			2005							204	210				7	Computer Science, Software Engineering	Computer Science	BCW71	WOS:000231586500033	
S	Zhou, Y; Mulekar, MS; Nerellapalli, P		Lim, A		Zhou, Y; Mulekar, MS; Nerellapalli, P			Adaptive spam filtering using dynamic feature space	ICTAI 2005: 17TH IEEE INTERNATIONAL CONFERENCE ON TOOLS WITH ARTIFICIAL INTELLIGENCE, PROCEEDINGS	Proceedings - International Conference on Tools With Artificial Intelligence		English	Proceedings Paper	17th International Conference on Tools with Artificial Intelligence	NOV 14-16, 2005	Hong Kong, PEOPLES R CHINA	IEEE Comp Soc, Informat Technol Res Inst, Wright State Univ, Hong Kong Univ Sci & Technol			HUFFMAN	Unsolicited bulk e-mail, also known as spam, has been an increasing problem for the e-mail society. This paper presents a new spam filtering strategy that 1) uses a practical entropy coding technique, Huffman coding, to dynamically encode the feature space of e-mail collections over time and, 2) applies an online algorithm to adaptively enhance the learned spam concept as new e-mail data becomes available. The contributions of this work include a highly efficient spam filtering algorithm in which the input space is radically reduced to a single-dimension input vector and an adaptive learning technique that is robust to vocabulary change, concept drifting and skewed data distribution. We compare our technique to several existing off-line learning techniques including Support Vector Machine, Naive Bayes, k-Nearest Neighbor C4.5 decision tree, RBFNetwork, Boosted decision tree and Stacking, and demonstrate the effectiveness of our technique by presenting the experimental results on the e-mail data that is publicly available.	Univ S Alabama, Sch CIS, Mobile, AL 36688 USA	Zhou, Y (reprint author), Univ S Alabama, Sch CIS, Mobile, AL 36688 USA.	zhou@cis.usouthal.edu; mmulekar@jaguar1.usouthal.edu; prn301@jaguar1.usouthal.edu					AHA DW, 1992, INT J MAN MACH STUD, V36, P267, DOI 10.1016/0020-7373(92)90018-G; ANDERSON JA, 1979, TECHNOMETRICS, V21, P71, DOI 10.2307/1268582; Androutsopoulos I., 2000, P 23 ANN INT ACM SIG, P160, DOI 10.1145/345508.345569; Berger AL, 1996, COMPUT LINGUIST, V22, P39; Carroll S.M, 2001, LIVING REV RELATIVIT, V4, P2001; Drucker H, 1999, IEEE T NEURAL NETWOR, V10, P1048, DOI 10.1109/72.788645; Dudani S. A., 1976, IEEE Transactions on Systems, Man and Cybernetics, VSMC-6; FALLER N, 1973, 7 AS C CIRC SYST COM, P593; FAWCETT T, 2003, KDD EXPLORATIONS, V2, P140; GALLAGER RG, 1978, IEEE T INFORM THEORY, V24, P668, DOI 10.1109/TIT.1978.1055959; Hidalgo J.M.G, 2002, P 2002 ACM S APPL CO, P615; HUFFMAN DA, 1952, P IRE, V40, P1098, DOI 10.1109/JRPROC.1952.273898; KNUTH DE, 1985, J ALGORITHM, V6, P163, DOI 10.1016/0196-6774(85)90036-7; Kolcz A., 2001, P TEXTDM 01 WORKSH T; Neter J., 1996, APPL LINEAR STAT MOD; Ramsey F., 2002, STAT SLEUTH COURSE M; Sahami M, 1998, LEARNING TEXT CATEGO; Sakkis G, 2001, PROCEEDINGS OF THE 2001 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING, P44; Sakkis G, 2003, INFORM RETRIEVAL, V6, P49, DOI 10.1023/A:1022948414856; SAKKIS G, 2001, MEMORY BASED APPROAC; Salton G., 1983, INTRO MODERN INFORM; Schneider K, 2003, P 11 C EUR CHAPT ASS; Widmer G, 1996, MACH LEARN, V23, P69, DOI 10.1007/BF00116900; Witten I.H., 2000, DATA MINING PRACTICA; WOLPERT DH, 1992, NEURAL NETWORKS, V5, P241, DOI 10.1016/S0893-6080(05)80023-1; ZHAN C, 2005, ACM SIGOPS OPERATING, V39, P34; ZHANG L, 2003, P 20 INT C COMP PROC, P446	27	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA	1082-3409		0-7695-2488-5	PROC INT C TOOLS ART			2005							302	309				8	Computer Science, Artificial Intelligence	Computer Science	BDO97	WOS:000234632500047	
S	Zhu, JB; Chen, WL		Montoyo, A; Munoz, R; Metais, E		Zhu, JB; Chen, WL			Improving text categorization using domain knowledge	NATURAL LANGUAGE PROCESSING AND INFORMATION SYSTEMS, PROCEEDINGS	LECTURE NOTES IN COMPUTER SCIENCE		English	Article; Proceedings Paper	10th International Conference on Applications of Natural Language to Information Systems	JUN 15-17, 2005	Alicante, SPAIN		Univ Alicante			In this paper, we mainly study and propose an approach to improve document classification using domain knowledge. First we introduce a domain knowledge dictionary NEUKD, and propose two models which use domain knowledge as textual features for text categorization. The first one is BOTW model which uses domain associated terms and conventional words as textual features. The other one is BOF model which uses domain features as textual features. But due to limitation of size of domain knowledge dictionary, we study and use a machine learning technique to solve the problem, and propose a BOL model which could be considered as the extended version of BOF model. In the comparison experiments, we consider naive Bayes system based on BOW model as baseline system. Comparison experimental results of naive Bayes systems based on those four models (BOW, BOTW, BOF and BOL) show that domain knowledge is very useful for improving text categorization. BOTW model performs better than BOW model, and BOL and BOF models perform better than BOW model in small number of features cases. Through learning new features using machine learning technique, BOL model performs better than BOF model.	Northeastern Univ, Nat Language Proc Lab, Inst Comp Software & Theory, Shenyang 110004, Peoples R China	Zhu, JB (reprint author), Northeastern Univ, Nat Language Proc Lab, Inst Comp Software & Theory, Shenyang 110004, Peoples R China.	zhujingbo@mail.neu.edu.cn; chenw1@mail.neu.edu.cn					Baker L. D., 1998, Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, DOI 10.1145/290941.290970; CHEN WL, 2004, AUTOMATIC WORD CLUST; CHEN WL, 2003, P 7 NAT C COMP LING; *CHIN LIB CAT ED B, 1999, CHIN LIB CAT; ITTNER DJ, 1995, TEXT CATEGORIZATION; Joachims T., 1998, Machine Learning: ECML-98. 10th European Conference on Machine Learning. Proceedings; LEE SK, 2002, PASSAGE SEGMENTATION, pP305; LEWIS D, 1994, COMP 2 LEARNING ALGO; Lewis D.D., 1996, P 19 ANN INT ACM SIG, P298, DOI 10.1145/243199.243277; MCCALLUM, 1998, AAAI 98 WORKSH LEARN; Nigam K, 1999, IJCAI 99 WORKSH MACH, P61; PEREIRA F, 1993, 31ST ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P183; SALTON G, 1983, INTRO MODERN INFORMA; SCOTT S, 1998, P COLING ACL WORKSH; YANG Y, 1999, P ACM SIGIR C RES DE; YAO TS, 2002, NATURAL LANGUAGE PRO; ZHU JB, 2002, J CHINESE INFORMATIO, V16	17	0	0	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		3-540-26031-5	LECT NOTES COMPUT SC			2005	3513						103	113				11	Computer Science, Artificial Intelligence; Computer Science, Information Systems; Computer Science, Theory & Methods	Computer Science	BCO46	WOS:000230413100010	
B	Zhu, TD; Zhao, XX; Liu, YS			IEEE	Zhu, TD; Zhao, XX; Liu, YS			A new text classification model based on the sentence space	Proceedings of 2005 International Conference on Machine Learning and Cybernetics, Vols 1-9			English	Proceedings Paper	4th International Conference on Machine Learning and Cybernetics	AUG 18-21, 2005	Canton, PEOPLES R CHINA	IEEE Systems, Man & Cybernet TCC, Hong Kong Polytechn Univ, Hebei Univ, S China Univ Technol, Chongqing Univ, Sun Yatsen Univ, Harbin Inst Technol, Int Univ Germany		text classification; vector space model; naive bayes		This paper proposes a Sentence Space Model, which expresses a text by sentence units and keeps the structure of the original text. It accomplishes the TC task by the sentence-class contribution and the double-voting method. Comparing with the VSM, this model has a higher classification accuracy in many data sets.	Beijing Inst Technol, Sch Informat Sci & Technol, Dept Comp Sci & Engn, Beijing 100081, Peoples R China	Zhu, TD (reprint author), Beijing Inst Technol, Sch Informat Sci & Technol, Dept Comp Sci & Engn, Beijing 100081, Peoples R China.						Cheng J., 1999, P 15 C UNC ART INT U, P101; Domingos P, 1997, MACH LEARN, V29, P103, DOI 10.1023/A:1007413511361; Friedman N, 1997, MACH LEARN, V29, P131, DOI 10.1023/A:1007465528199; Keogh E., 1999, P 7 INT WORKSH ART I, P225; Manning Christopher D., 1999, FDN STAT NATURAL LAN; MURPHY PM, 1995, UCI REP MACH LEARN D; SALTON G, 1991, SCIENCE, V253, P974, DOI 10.1126/science.253.5023.974	7	0	0	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA			0-7803-9091-1				2005							1774	1777				4	Computer Science, Artificial Intelligence; Computer Science, Cybernetics; Computer Science, Information Systems	Computer Science	BDT94	WOS:000235325602083	
S	Zhu, WB; Lin, YP; Lin, M; Chen, ZP		Fan, W; Wu, Z; Yang, J		Zhu, WB; Lin, YP; Lin, M; Chen, ZP			Removing smoothing from naive Bayes text classifier	ADVANCES IN WEB-AGE INFORMATION MANAGEMENT, PROCEEDINGS	LECTURE NOTES IN COMPUTER SCIENCE		English	Article; Proceedings Paper	6th International Conference on Web -Age Informational Management	OCT 11-13, 2005	Hangzhou, PEOPLES R CHINA	DatabaseSoc China Comp Federat, Natl Sci Fdn China, Zhejiang Univ, Y C Tang Disciplinary Dev & Fund, Oracle China			CATEGORIZATION	Smoothing is applied in Bayes classifier when the maximum likelihood (ML) estimate can't solve the problem in the absence of some features in training data. However, smoothing doesn't have firm theoretic base to rely on as ML estimate does. In this paper, we propose two novel strategies to remove smoothing from the classifier without sacrificing classification accuracy: NB_TF and NB_TS. NB_TF adjusts the classifier by adding the test document before classification and it is suitable for online categorization. NB_TS improves the performance by adding the whole test set to the classifier in the training stage and it is more efficient for batch categorization. The experiments and analysis show that NB_TS outperforms Laplace additive smoothing and Simple Good-Turing (SGT) smoothing, and NB-TF performs better than Laplace additive smoothing.	Hunan Univ, Comp & Commun Coll, Changsha 410082, Peoples R China; Hunan Univ, Math & Econ Coll, Changsha 410082, Peoples R China	Zhu, WB (reprint author), Hunan Univ, Comp & Commun Coll, Changsha 410082, Peoples R China.	zhuwangbin@hotmail.com; yplin@hnu.cn; kevin9908@hotmail.com; jt_zpchen@hnu.cn					Chen S.F., 1998, EMPIRICAL STUDY SMOO; CHENG XZ, 2004, ACM T INFORM SYST, V2, P179; Gale W. A., 1995, J QUANT LINGUIST, V2, P217, DOI 10.1080/09296179508590051; GOOD IJ, 1953, BIOMETRIKA, V40, P237, DOI 10.2307/2333344; McCallum A., 1998, AAAI 98 WORKSH LEARN, P41; PAVLOV D, KDD 2004, P829; PENG FC, ECIR 2003, P335; Rennie Jason, 2003, ICML; Sebastiani F, 2002, ACM COMPUT SURV, V34, P1, DOI 10.1145/505282.505283; WANG Y, 2003, P 15 IEEE INT C TOOL; Yang Y. M., 1999, INFORM RETRIEVAL, V1, P69, DOI 10.1023/A:1009982220290; Zhang T, 2001, INFORM RETRIEVAL, V4, P5, DOI 10.1023/A:1011441423217	12	0	0	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		3-540-29227-6	LECT NOTES COMPUT SC			2005	3739						713	718				6	Computer Science, Information Systems; Computer Science, Theory & Methods	Computer Science	BDG49	WOS:000233385300069	
J	Robes, V; Larranaga, P; Pena, JM; Menasalvas, E; Perez, MS; Herves, V; Wasilewska, A				Robes, V; Larranaga, P; Pena, JM; Menasalvas, E; Perez, MS; Herves, V; Wasilewska, A			Bayesian network multi-classifiers for protein secondary structure prediction	ARTIFICIAL INTELLIGENCE IN MEDICINE			English	Article						multi-classifier; supervised classification; machine learning; stacked generalization; Bayesian networks; protein secondary; structure prediction; Pazzani-EDA	ACCURACY; SERVER	Successful secondary structure predictions provide a starting point for direct tertiary structure modelling, and also can significantly improve sequence analysis and sequence-structure threading for aiding in structure and function determination. Hence the improvement of predictive accuracy of the secondary structure prediction becomes essential for future development of the whole field of protein research. In this work we present several multi-classifiers that combine the predictions of the best current classifiers available on Internet. Our results prove that combining the predictions of a set of classifiers by creating composite classifiers is a fruitful one. We have created multi-classifiers that are more accurate than any of the component classifiers. The multi-classifiers are based on Bayesian networks. They are validated with 9 different datasets. Their predictive accuracy results outperform the best secondary structure predictors by 1.21% on average. Our main contributions are: (i) we improved the best know predictive accuracy by 1.21%, (ii) our best results have been obtained with a new semi naive Bayes approach named Pazzani-EDA and (iii) our multi-classifiers combine results of previously build classifiers predictions obtained through Internet, thanks to our development of a Java application. (C) 2004 Elsevier B.V. All rights reserved.	Tech Univ Madrid, Dept Comp Architecture & Technol, Madrid, Spain; Univ Basque Country, Dept Comp Sci & Artificial Intelligence, San Sebastian, Spain; SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA	Robes, V (reprint author), Tech Univ Madrid, Dept Comp Architecture & Technol, Madrid, Spain.	vrobies@fi.upm.es; ccplamup@si.ehu.es; jmpena@fi.upm.es; emenasalvas@fi.upm.es; mperez@fi.upm.es; vherves@fi.upm.es; anita@cs.sunysb.edu	Larranaga, Pedro/F-9293-2013				Altschul SF, 1997, NUCLEIC ACIDS RES, V25, P3389, DOI 10.1093/nar/25.17.3389; Baldi P, 1999, BIOINFORMATICS, V15, P937, DOI 10.1093/bioinformatics/15.11.937; BALDI P, 1999, P 16 INT JOINT ART I; BARTON G, 1988, J MOL BIOL, V195, P957; Berman HM, 2000, NUCLEIC ACIDS RES, V28, P235, DOI 10.1093/nar/28.1.235; Cuff JA, 1999, PROTEINS, V34, P508, DOI 10.1002/(SICI)1097-0134(19990301)34:4<508::AID-PROT10>3.0.CO;2-4; Cuff JA, 1998, BIOINFORMATICS, V14, P892, DOI 10.1093/bioinformatics/14.10.892; Duda R., 1973, PATTERN CLASSIFICATI; Frishman D, 1997, PROTEINS, V27, P329, DOI 10.1002/(SICI)1097-0134(199703)27:3<329::AID-PROT1>3.0.CO;2-8; GARNIER J, 1978, J MOL BIOL, V120, P97, DOI 10.1016/0022-2836(78)90297-8; GIBRAT JF, 1987, J MOL BIOL, V198, P425, DOI 10.1016/0022-2836(87)90292-0; Hand DJ, 2001, INT STAT REV, V69, P385, DOI 10.2307/1403452; HO T, 1994, IEEE T PATTERN ANAL, V11, P63; HOBOHM U, 1992, PROTEIN SCI, V1, P409; Jones DT, 1999, J MOL BIOL, V292, P195, DOI 10.1006/jmbi.1999.3091; KABSCH W, 1983, BIOPOLYMERS, V22, P2577, DOI 10.1002/bip.360221211; KARPLUS K, COMBINING LOCAL STRU; King RD, 1996, PROTEIN SCI, V5, P2298; Larranaga P., 2002, ESTIMATION DISTRIBUT; MATTHEWS FS, 1985, PROG BIOPHYS MOL BIO, V45, P1; Ouali M, 2000, PROTEIN SCI, V9, P1162; PAZZANI M, 1997, LEARNING DATA ARTIFI, V5, P239; POLLASTRI G, 2001, PROTEINS, P228; Qian N, 1988, J MOL BIOL, V202, P865, DOI 10.1016/0022-2836(88)90564-5; ROBLES V, 2003, IN PRESS LECT NOTES; ROST B, 1994, COMPUT APPL BIOSCI, V10, P53; Rost B, 2001, Proteins, VSuppl 5, P192; ROST B, 1994, J MOL BIOL, V235, P13, DOI 10.1016/S0022-2836(05)80007-5; ROST B, 1993, J MOL BIOL, V232, P584, DOI 10.1006/jmbi.1993.1413; SALAMOV AA, 1995, J MOL BIOL, V247, P11, DOI 10.1006/jmbi.1994.0116; SCHMIDLER S, 2000, J COMPUT BIOL, V2, P233; Sierra B, 2001, ARTIF INTELL MED, V22, P233, DOI 10.1016/S0933-3657(00)00111-1; WOLPERT DH, 1992, NEURAL NETWORKS, V5, P241, DOI 10.1016/S0893-6080(05)80023-1; Zemla A, 1999, PROTEINS, V34, P220, DOI 10.1002/(SICI)1097-0134(19990201)34:2<220::AID-PROT7>3.0.CO;2-K; ZVELEBIL MJ, 1987, J MOL BIOL, V195, P957, DOI 10.1016/0022-2836(87)90501-8	35	0	0	ELSEVIER SCIENCE BV	AMSTERDAM	PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS	0933-3657			ARTIF INTELL MED	Artif. Intell. Med.	JUN	2004	31	2					117	136		10.1016/j.artmed.2004.01.009		20	Computer Science, Artificial Intelligence; Engineering, Biomedical; Medical Informatics	Computer Science; Engineering; Medical Informatics	848YG	WOS:000223503500004	
J	Wang, LM; Yuan, SM				Wang, LM; Yuan, SM			Induction of hybrid decision tree based on post-discretization strategy	PROGRESS IN NATURAL SCIENCE			English	Article						machine learning; hybrid decision tree; Naive Bayes		By redefining test selection measure, we propose in this paper a new algorithm, Flexible NBTree, which induces a hybrid of decision tree and Naive Bayes. Flexible NBTree mitigates the negative effect of information loss on test selection by applying post-discretization strategy: at each internal node in the tree, we first select the test which is the most useful for improving classification accuracy, then apply discretization of continuous tests. The finial decision tree nodes contain univariate splits as regular decision trees, but the leaves contain Naive Bayesian classifiers. To evaluate the performance of Flexible NBTree, we compare it with NBTree and C4. 5, both applying pre-discretization of continuous attributes. Experimental results on a variety of natural domains indicate that the classification accuracy of Flexible NBTree is substantially improved.	Jilin Univ, Coll Comp Sci & Technol, Changchun 130012, Peoples R China	Wang, LM (reprint author), Jilin Univ, Coll Comp Sci & Technol, Changchun 130012, Peoples R China.	wangtimin-74.student@sina.com					Breiman L., 1984, CLASSIFICATION REGRE; Dougherty J., 1995, P 12 INT C MACH LEAR, P194; GEORGE H, 1995, P 11 C UNC ART INT, P338; Kohavi R., 1996, P 2 INT C KNOWL DISC, P202; McCallum A, 1998, P AAAI 98 WORKSH LEA, P41; Quinlan J.R., 1993, PROGRAMS MACHINE LEA; QUINLAN JR, 1986, INDUCTION DECISION T, P81; Quinlan JR, 1996, J ARTIF INTELL RES, V4, P77; Silverman B. W., 1986, MONOGRAPHS STAT APPL; SMYTH P, 1995, P 12 INT C MACH LEAR, P506; Zhou ZH, 2003, AI COMMUN, V16, P3	11	0	5	TAYLOR & FRANCIS LTD	ABINGDON	4 PARK SQUARE, MILTON PARK, ABINGDON OX14 4RN, OXON, ENGLAND	1002-0071			PROG NAT SCI	Prog. Nat. Sci.	JUN	2004	14	6					541	545		10.1080/10020070412331343911		5	Materials Science, Multidisciplinary; Multidisciplinary Sciences	Materials Science; Science & Technology - Other Topics	831MH	WOS:000222198000013	
B	Aksoy, S; Koperski, K; Tusk, C; Marchisio, G; Tilton, JC			ieee	Aksoy, S; Koperski, K; Tusk, C; Marchisio, G; Tilton, JC			Learning Bayesian classifiers for a visual grammar	2003 IEEE WORKSHOP ON ADVANCES IN TECHNIQUES FOR ANALYSIS OF REMOTELY SENSED DATA			English	Proceedings Paper	IEEE Workshop on Advances in Techniques for Analysis of Remotely Sensed Data held in Honor of David A Landgrebe	OCT 27-28, 2003	Greenbelt, MD	IEEE	NASA Goddard Space Flight Visitor Ctr		RETRIEVAL	A challenging problem in image content extraction and classification is building a system that automatically learns high-level semantic interpretations of images. We describe a Bayesian framework for a visual grammar that aims to reduce the gap between low-level features and user semantics. Our approach includes learning prototypes of regions and their spatial relationships for scene classification. First, naive Bayes classifiers perform automatic fusion of features and learn models for region segmentation and classification using positive and negative examples for user-defined semantic land cover labels. Then, the system automatically learns distinguishing spatial relationships of these regions from training data and builds visual grammar models. Experiments using LANDSAT scenes show that the visual grammar enables creation of higher level classes that cannot be modeled by individual pixels or regions. Furthermore, learning of the classifiers requires only a few training examples.	Insightful Corp, Seattle, WA 98109 USA	Aksoy, S (reprint author), Insightful Corp, 1700 Westlake Ave N,Suite 500, Seattle, WA 98109 USA.		Aksoy, Selim/C-3365-2008	Aksoy, Selim/0000-0003-4185-0565			AKSOY S, 2002, P IEEE INT GEOSC REM, V2, P1041; Aksoy S., 2003, FRONTIERS REMOTE SEN, P35, DOI 10.1142/9789812796752_0003; Berretti S, 2001, PATTERN ANAL APPL, V4, P83, DOI 10.1007/s100440170009; Chu WW, 1998, IEEE T KNOWL DATA EN, V10, P872, DOI 10.1109/69.738355; Haley GM, 1999, IEEE T IMAGE PROCESS, V8, P255, DOI 10.1109/83.743859; HAY SI, 2002, PHOTOGRAMMETRIC ENG, V68; Koperski K., 2002, P IGARSS TOR ON CAN, V3, P1810; NEAL PJ, 1998, P AM MED INF ASS ANN; Petrakis EGM, 1997, IEEE T KNOWL DATA EN, V9, P435, DOI 10.1109/69.599932; Smith J. R., 1996, Proceedings ACM Multimedia 96, DOI 10.1145/244130.244151; Tang LH, 1999, P SOC PHOTO-OPT INS, V3662, P360, DOI 10.1117/12.352767; TILTON JC, 2002, P IEEE INT GEOSC REM, V2, P1029	12	0	0	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA			0-7803-8350-8				2004							212	218				7	Remote Sensing	Remote Sensing	BAG84	WOS:000222142800031	
S	Alcobe, JR		LopezdeMantaras, R; Saitta, L		Alcobe, JR			Incremental augmented naive Bayes classifiers	ECAI 2004: 16TH EUROPEAN CONFERENCE ON ARTIFICIAL INTELLIGENCE, PROCEEDINGS	FRONTIERS IN ARTIFICIAL INTELLIGENCE AND APPLICATIONS		English	Proceedings Paper	16th European Conference on Artificial Intelligence	AUG 22-27, 2004	Valencia, SPAIN	European Coordinating Comm Artificial Intelligence, Asoc Espanola Inteligencia Artificial, Assoc Catalana Intelligencia Artificial, Univ Politecn Valencia, Grp Tecnol Informat				We propose two general heuristics to transform a batch Hill-climbing search into an incremental one. Our heuristics, when new data are available, study the search path to determine whether it is worth revising the current structure and if it is, they state which part of the structure must be revised. Then, we apply our heuristics to two Bayesian network structure learning algorithms in order to obtain incremental Augmented Naive Bayes classifiers. We experimentally show that our incremental approach saves a significant amount of computing time while it yields classifiers of similar quality as the ones learned with the batch approach.	Escola Univ Politecn Mataro, Mataro 08303, Catalonia, Spain	Alcobe, JR (reprint author), Escola Univ Politecn Mataro, Av Puig & Cadafalch 101-111, Mataro 08303, Catalonia, Spain.						Bouckaert R. R., 1995, THESIS UTRECHT U; BUNTINE W, 1991, P 7 UAI; CHENG J, 2001, P 14 BIENN C CAN SOC, P141; CHOW CK, 1968, IEEE T INFORM THEORY, V14, P462, DOI 10.1109/TIT.1968.1054142; DOMINGOS, 2003, J COMPUTATIONAL GRAP, V12; DUDA RO, 2001, PATTERN CLASSFICATIO; Friedman N, 1997, MACH LEARN, V29, P131, DOI 10.1023/A:1007465528199; FRIEDMAN N, 1997, P 13 UAI; KOHAVI R, 1994, PROC INT C TOOLS ART, P740, DOI 10.1109/TAI.1994.346412; Lam W, 1994, COMPUT INTELL, V10, P269, DOI 10.1111/j.1467-8640.1994.tb00166.x; Langley P., 1994, P 10 C UNC ART INT, P399; Moore A, 1998, J ARTIF INTELL RES, V8, P67; Murphy P. M., 1994, UCI REPOSITORY MACHI; SPIEGELHALTER DJ, 1990, NETWORKS, V20, P579, DOI 10.1002/net.3230200507	14	0	0	I O S PRESS	AMSTERDAM	NIEUWE HEMWEG 6B, 1013 BG AMSTERDAM, NETHERLANDS	0922-6389		1-58603-452-9	FR ART INT			2004	110						539	543				5	Computer Science, Artificial Intelligence	Computer Science	BBH24	WOS:000225505100105	
B	Almonayyes, A		Chu, HW; Savoie, M; Sanchez, B		Almonayyes, A			Case-Based driven naive Bayes classifier	International Conference on Computing, Communications and Control Technologies, Vol 1, Proceedings			English	Proceedings Paper	International Conference on Computing, Communications and Control Technologies (CCCT 2004)	AUG 14-17, 2004	Austin, TX	Univ Texas Austin, Int Inst Informat & System, IEEE Comp Soc, Venezuela Chapter, Inter Amer Org Higher Educ		case-based reasoning; naive Bayes; and explanation patterns		The knowledge of foreign language plays a big role, in intelligence and counter-terrorism. The intelligence community relies heavily on language to create finished intelligence products for decision makers. Exploratory data analysis over foreign language text presents virtually untapped opportunity. This work incorporates Naive Bayes classifier with Case-Based Reasoning in order to classify and analyze Arabic texts related to fanaticism. The Arabic vocabularies are converted. to equivalent English Words using conceptual hierarchy structure. The understanding process operates at two phases. At the first phase, a discrimination network of multiple questions is used to retrieve explanatory knowledge structures each of which gives an interpretation of a text according to a particular aspect of fanaticism. Explanation structures organize past documents of fanatic content. Similar documents are retrieved to generate additional valuable information about the new document. In the second phase, the document classification process based on Naive. Bayes is used to classify, documents into their fanatic class. The results show that the classification accuracy is improved by incorporating the explanation patterns with the Naive Bayes classifier.	Kuwait Univ, Dept Math & Comp Sci, Safat 13060, Kuwait	Almonayyes, A (reprint author), Kuwait Univ, Dept Math & Comp Sci, POB 5969, Safat 13060, Kuwait.						AAMODT A, 1994, AI COMMUN, V7, P39; ALMONAYYES A, 2001, J EXPT THEORETICAL A; COHEN W, 1996, ARTIF INTELL, P249; Hastie T., 2001, SPRINGER SERIES STAT; Jackson P., 2002, NATURAL LANGUAGE PRO, V5; KOLODNER J, 1996, MAKING IMPLICIT EXPL, P349; Kolonder J., 1993, CASE BASED REASONING; Lewis D., 1994, 3 ANN S DOC AN INF R, P81; Manning Christopher D., 2001, FDN STAT NATURAL LAN; McCallum A, 1998, AAAI 98 WORKSH LEARN; Mitchell T.M., 1997, MACHINE LEARNING; SCHANK R, 1990, 2 NW U I LEARN SCI; ZELIKOVITZ K, 2000, P 17 INT C MACH LEAR, P1183	13	0	0	INT INST INFORMATICS & SYSTEMICS	ORLANDO	14269 LORD BARCLAY DR, ORLANDO, FL 32837 USA			980-6560-17-5				2004							1	5				5	Computer Science, Artificial Intelligence; Computer Science, Information Systems; Computer Science, Software Engineering; Computer Science, Theory & Methods	Computer Science	BBV04	WOS:000227981900001	
S	Benbrahim, H; Bramer, M			IEEE	Benbrahim, H; Bramer, M			An empirical study for hypertext categorization	2004 IEEE INTERNATIONAL CONFERENCE ON SYSTEMS, MAN & CYBERNETICS, VOLS 1-7	IEEE INTERNATIONAL CONFERENCE ON SYSTEMS, MAN, AND CYBERNETICS, CONFERENCE PROCEEDINGS		English	Proceedings Paper	IEEE International Conference on Systems, Man and Cybernetics	OCT 10-13, 2004	The Hague, NETHERLANDS	IEEE		web mining; machine leaming; hypertext classification		As the web expands exponentially, the need to put some order to its content becomes apparent. Hypertext categorization, that is the automatic classification of web documents into predefined classes, came to elevate humans from that task The extra information available in a hypertext document poses new challenges for automatic categorization. HTML tags and metadata provide rich information for Hypertext categorization that is not available in traditional text classification. This paper looks at (i) what representation to use for documents and which extra information hidden in HTML pages to take into consideration to improve the classification task, and (ii) how to deal with the very high number of features of texts. A hypertext dataset and four well-known learning algorithms (Naive Bayes, K-Nearest Neighbor, Support Vector Machines and C4.5) were used to exploit the enriched text representation along with feature reduction. The results showed that enhancing the basic text content with HTML page keywords, title and anchor links improved the accuracy of the classification algorithms.	Univ Portsmouth, Dept Comp Sci & Software Engn, Portsmouth, Hants, England	Benbrahim, H (reprint author), Univ Portsmouth, Dept Comp Sci & Software Engn, Portsmouth, Hants, England.						BHARAT K, 1998, P 7 WORLD WID WEB C; CHAKRABARTI S, 1997, VLDB ATHENS GREE AUG; Dumais Susan T., 2000, P ACM SIGCHI C HUM F, P145, DOI 10.1145/332040.332418; Furnkranz J., 1999, P 3 S INT DAT AN IDA, P487; Lewis D., 1992, P SPEECH NAT LANG WO, P212, DOI 10.3115/1075527.1075574; SALTON G, 1988, INFORM PROCESS MANAG, V24, P513, DOI 10.1016/0306-4573(88)90021-0	6	0	0	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA	1062-922X		0-7803-8566-7	IEEE SYS MAN CYBERN			2004							5952	5957				6	Automation & Control Systems; Computer Science, Artificial Intelligence; Computer Science, Cybernetics; Robotics	Automation & Control Systems; Computer Science; Robotics	BBP32	WOS:000226863301002	
B	Bharatheesh, TL; Iyengar, SS		Arabnia, HR; Yang, LT		Bharatheesh, TL; Iyengar, SS			Predictive data mining for delinquency modeling	ESA'04 & VLSI'04, PROCEEDINGS			English	Proceedings Paper	Internation Conference on Embedded Systems and Applications/International Conference on VLSI	JUN 21-24, 2004	Las Vegas, NV			data mining; predictive modeling; delinquency modeling; skewed distributions; naive Bayes classifier		Predictive data mining is the process of automatically creating a classification model from a set of examples, called the training set, which belongs to a set of classes. Once a model is created, it can be used to automatically predict the class of other unclassified examples. Some datasets encountered in real life applications have skewed class distributions. Many predictive modeling systems are not prepared to induce a classifier that accurately classifies the minority class under such situation. In this work an attempt has been made to build the predictive model for delinquency in credit cards users, using the state of art methods. The success of the model is defined in different terms than the ones found in literature. Different sampling schemes are evaluated and a modified naive Bayes classifier is used as classifier. The results are encouraging and it is proposed to compare the prototype with ensemble of models.	Bigants Consulting, Bangalore, Karnataka, India	Bharatheesh, TL (reprint author), Bigants Consulting, Bangalore, Karnataka, India.						BHARATHEESH TL, 2003, THESIS MS U; EDWIN PD, 2000, RC21731 IBM; Fawcett T., 1997, DATA MINING KNOWLEDG, V1; Ling CX, 1998, P 4 INT C KNOWL DISC; MONARD MC, 2003, LEARNING SKEWED CLAS; STOLFO SJ, 1997, AAAI 97 WORKSH ALL M; WITTTEN IH, 1999, DATA MINING PRACTICA	7	0	0	C S R E A PRESS	ATHENS	115 AVALON DR, ATHENS, GA 30606 USA			1-932415-41-6				2004							99	105				7	Computer Science, Hardware & Architecture; Computer Science, Theory & Methods	Computer Science	BBK61	WOS:000225884700015	
S	Cerquides, J; de Mantaras, RL		Suzuki, E; Arikawa, S		Cerquides, J; de Mantaras, RL			Maximum a posteriori tree augmented naive Bayes classifiers	DISCOVERY SCIENCE, PROCEEDINGS	LECTURE NOTES IN COMPUTER SCIENCE		English	Article; Proceedings Paper	7th International Conference on Discovery Science	OCT 02-05, 2004	Padova, ITALY		Univ Padova	Bayesian networks; Bayesian network classifiers; naive Bayes; decomposable distributions; Bayesian model averaging	ALGORITHM	Bayesian classifiers such as Naive Bayes or Tree Augmented Naive Bayes (TAN) have shown excellent performance given their simplicity and heavy underlying independence assumptions. In this paper we prove that under suitable conditions it is possible to efficiently compute the maximum a posterior TAN model. Furthermore, we prove that it is also possible to efficiently calculate a weighted set with the k maximum a posteriori TAN models. This allows efficient TAN ensemble learning and accounting for model uncertainty. These results can be used to construct two classifiers. Both classifiers have the advantage of allowing the introduction of prior knowledge about structure or parameters into the learning process. Empirical results show that both classifiers lead to an improvement in error rate and accuracy of the predicted class probabilities over established TAN based classifiers with equivalent complexity.	Univ Barcelona, Dept Matemat Aplicada & Anal, E-08007 Barcelona, Spain; CSIC, IIIA, Bellaterra 08193, Spain	Cerquides, J (reprint author), Univ Barcelona, Dept Matemat Aplicada & Anal, Gran Via 585, E-08007 Barcelona, Spain.						Blake C. L., 1998, UCI REPOSITORY MACHI; CERQUIDES J, 1999, P INT C KNOWL DISC D; CERQUIDES J, 2003, THESIS TU CATALONIA; CERQUIDES J, 2003, IIIA200304; Cerquides J., 2003, P 20 INT C MACH LEAR, P75; Domingos P, 1997, MACH LEARN, V29, P103, DOI 10.1023/A:1007413511361; Friedman N, 1997, MACH LEARN, V29, P131, DOI 10.1023/A:1007465528199; HECKERMAN D, 1995, MACH LEARN, V20, P197, DOI 10.1007/BF00994016; KATOH N, 1981, SIAM J COMPUT, V10, P247, DOI 10.1137/0210017; KONTKANEN P, 1998, LECT NOTES ARTIF INT, V1398, P77; Langley P., 1992, P 10 NAT C ART INT, P223; Meila M, 2001, J MACH LEARN RES, V1, P1, DOI 10.1162/153244301753344605; MEILA M, 2000, CMU RI TR 00 15 TEC; MEILA M, 2000, P 16 C UNC ART INT; Pettie S, 2002, J ACM, V49, P16, DOI 10.1145/505241.505243; THEARLING K, 1998, KEYS COMMERCIAL SUCC	16	0	0	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		3-540-23357-1	LECT NOTES COMPUT SC			2004	3245						73	88				16	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Computer Science	BBB88	WOS:000224608200006	
B	Chai, XY; Deng, L; Yang, Q; Ling, CX		Rastogi, R; Morik, K; Bramer, M; Wu, X		Chai, XY; Deng, L; Yang, Q; Ling, CX			Test-cost sensitive naive Bayes classification	FOURTH IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS			English	Proceedings Paper	4th IEEE International Conference on Data Mining	NOV 01-04, 2004	Brighton, ENGLAND	IEEE Comp Soc, TCCI, IEEE Comp Soc, TCPAMI, IBM Res, StatSoft Ltd, Web Intelligence Consortium			KNOWLEDGE	Inductive learning techniques such as the naive Bayes and decision tree algorithms have been extended in the past to handle different types of costs mainly by distinguishing different costs of classification errors. However it is an equally important issue to consider how to handle the test costs associated with querying the missing values in a test case. When the value of an attribute is missing in a test case, it may or may not be worthwhile to take the effort to obtain its missing value, depending on how much the value will result in a potential gain in the classification accuracy. In this paper we show how to obtain a test-cost sensitive naive Bayes classifier (csNB) by including a test strategy which determines how unknown attributes are selected to perform test on in order to minimize the sum of the misclassification costs and test costs. We propose and evaluate several potential test strategies including one that allows several tests to be done at once. We empirically evaluate the csNB method, and show that it compares favorably with its decision tree counterpart.	Hong Kong Univ Sci & Technol, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China	Chai, XY (reprint author), Hong Kong Univ Sci & Technol, Dept Comp Sci, Clearwater Bay, Kowloon, Hong Kong, Peoples R China.						Blake C. L., 1998, UCI REPOSITORY MACHI; Domingos P., 1999, KNOWLEDGE DISCOVERY, P155; Domingos P, 1997, MACH LEARN, V29, P103, DOI 10.1023/A:1007413511361; Duda R.O., 2001, PATTERN CLASSIFICATI; Elkan C., 2001, P 17 INT JOINT C ART, P973; Greiner R, 2002, ARTIF INTELL, V139, P137, DOI 10.1016/S0004-3702(02)00209-6; Kai M., 1998, PRINCIPLES DATA MINI, P139; LING C, 2004, P ICML04; Mitchell T.M., 1997, MACHINE LEARNING; NUNEZ M, 1991, MACH LEARN, V6, P231, DOI 10.1007/BF00114778; PAPADIMITRIOU CH, 1987, MATH OPER RES, V12, P441, DOI 10.1287/moor.12.3.441; Quinlan J.R., 1993, C4 5 PROGRAMS MACHIN; TAN M, 1993, MACH LEARN, V13, P7, DOI 10.1007/BF00993101; Turney P.D., 2000, WORKSH COST SENS LEA; TURNEY PD, 1995, J ARTIFICIAL INTELLI; Zubek V.B., 2002, P 19 INT C MACH LEAR, P27	16	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA			0-7695-2142-8				2004							51	58				8	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Computer Science	BBI95	WOS:000225713000007	
B	Chen, DC; Hua, DD; Liu, ZQ; Cheng, ZF			IEEE	Chen, DC; Hua, DD; Liu, ZQ; Cheng, ZF			An integrated system for class prediction using gene expression profiling	2004 8th International Conference on Control, Automation, Robotics and Vision, Vols 1-3			English	Proceedings Paper	8th International Conference on Control, Automation, Robotics and Vision (ICARCV 2004)	DEC 06-09, 2004	Kunming, PEOPLES R CHINA	Nanyang Technol Univ, Sch Elect & Elect Engn, Republic Singapore, Nanjing Univ Sci & Technol, Peoples Republic China, IEEE Robotics & Automation Soc, IEEE Syst, Man & Cybernet Soc, IEEE Control Syst Soc, IEE, Natl Nat Sci Fdn China, Singapore Tech Engn Ltd, Lee Fdn			CANCER; CLASSIFICATION; DIAGNOSIS; TUMOR	Motivation: Gene expression profiles have been successfully applied to class prediction. Due to a large number of genes (features) and a small number of samples in gene expression data, feature selection is essential when performing the prediction task. Many methods have been proposed to select features in microarray data analysis, but there is no unique method which performs uniformly well for all the learning algorithms. It is then practical to find a feature selection method and a learning algorithm that give superior performance. Results: In this paper, we present an integrated scheme to perform the task of class prediction based on gene expression profiles. The scheme incorporates a simple novel feature selection procedure into naive Bayes models. Each selected gene has a high score of discriminatory power determined by the Brown-Forsythe test statistic. Any pair of selected genes have a low correlation. This facilitates the use of the conditional independence among genes assumed by the naive Bayes models. To demonstrate the effectiveness, the proposed scheme was applied to three commonly used expression data sets COLON, OVARIAN, and LEUKEMIA. The results show that the numbers of misclassified samples are 0, 0, and 4, respectively.	Uniformed Serv Univ Hlth Sci, Bethesda, MD 20814 USA	Chen, DC (reprint author), Uniformed Serv Univ Hlth Sci, 4301 Jones Bride Rd, Bethesda, MD 20814 USA.						Alon U, 1999, P NATL ACAD SCI USA, V96, P6745, DOI 10.1073/pnas.96.12.6745; BROWN MB, 1974, TECHNOMETRICS, V16, P129, DOI 10.2307/1267501; Chen DC, 2003, PROCEEDINGS OF THE 2003 IEEE BIOINFORMATICS CONFERENCE, P492; Dudoit S, 2002, J AM STAT ASSOC, V97, P77, DOI 10.1198/016214502753479248; Golub TR, 1999, SCIENCE, V286, P531, DOI 10.1126/science.286.5439.531; Hastie T. J., 2001, ELEMENTS STAT LEARNI; Liu Huiqing, 2002, Genome Inform, V13, P51; Neter J., 1996, APPL LINEAR STAT MOD; Ramaswamy S, 2001, P NATL ACAD SCI USA, V98, P15149, DOI 10.1073/pnas.211566398; Ripley B., 1996, PATTERN RECOGNITION; STUART A, 1999, KENDALLS ADV THEOR A, V2; Tibshirani R, 2002, P NATL ACAD SCI USA, V99, P6567, DOI 10.1073/pnas.082099299; Welsh JB, 2001, P NATL ACAD SCI USA, V98, P1176, DOI 10.1073/pnas.98.3.1176; XING EP, 2003, UNDERSTANDING USING; Xiong MM, 2001, GENOME RES, V11, P1878	15	0	0	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA			0-7803-8653-1				2004							1023	1028				6	Automation & Control Systems; Computer Science, Artificial Intelligence; Robotics	Automation & Control Systems; Computer Science; Robotics	BCP26	WOS:000230484501045	
S	Curotto, CL; Ebecken, NFF		Ebecken, NFF; Brebbia, CA; Zanasi, A		Curotto, CL; Ebecken, NFF			Evaluating the scalability of data mining provider classifiers	DATA MINING IV	MANAGEMENT INFORMATION SYSTEMS		English	Proceedings Paper	4th International Conference on Data Mining	DEC 01-03, 2003	Rio de Janeiro, BRAZIL	Wessex Inst Technol, Coppe, Fed Univ Rio de Janeiro				Two classifiers implemented as Data Mining Providers are considered. These providers runs as a stand-alone servers or aggregated with Microsoft(R) SQL Server. One of these classifiers is the Microsoft(R) Decision Trees algorithm. The other is the Simple Naive Bayes incremental classifier, that supports continuous input attributes, multiple discrete predictable attributes and incremental updating of the training data set. The performance study carried out to verify the scalability of the classifiers includes factors of cardinality (number of training cases), number of input attributes, number of states of the input attributes and number of predictable attributes.								BERNHARDT J, 1999, P ICDE 99 15 INT C D, P470; BEZERRA E, 1999, THESIS COPPEUFPR RIO; BRADLEY PS, 1999, TR9835 MICR RES MISC; CHICKERING DM, 1994, MSRTR9409 MICR CORP; CUROTTO CL, 2002, P DAT MIN 3 3 INT C, P73; CUROTTO CL, 2003, THESIS COPPE UFPR RI; Han J., 2001, DATA MINING CONCEPTS; Han J., 1996, P 2 INT C KNOWL DISC, P250; MELLI G, 1999, DATGEN DATASET GENER; *MICR CORP, 2000, OLE DB DAT MIN SPEC; *MICR CORP, 2002, OLE DB DAT MIN RES K; Netz A, 2001, PROC INT CONF DATA, P379, DOI 10.1109/ICDE.2001.914850; NETZ A, 2000, P 26 VLDB, P719; SARAWAGI S, 1998, 10107 IBM AL RES CTR; SONI S, 2002, PERFORMANCE  STUDY M; SOUSA MS, 1998, THESIS COPPE UFPR RI	16	0	0	WIT PRESS	SOUTHAMPTON	ASHURST LODGE, SOUTHAMPTON SO40 7AA, ASHURST, ENGLAND	1470-6326		1-85312-806-6	MANAG INFORMAT SYST			2004	7						651	660				10	Computer Science, Artificial Intelligence	Computer Science	BY82D	WOS:000189471200062	
S	Gilman, A; Narayanan, B; Paul, S		Zanasi, A; Ebecken, NFF; Brebbia, CA		Gilman, A; Narayanan, B; Paul, S			Mining call center dialog data	DATA MINING V: DATA MINING, TEXT MINING AND THEIR BUSINESS APPLICATIONS	MANAGEMENT INFORMATION SYSTEMS		English	Proceedings Paper	5th International Conference on Data Mining	SEP 15-17, 2004	Malaga, SPAIN	Wessex Inst Technol		text mining; classification; support vector machines; dialog mining; customer relationship management	RETRIEVAL	We consider the problem of mining conversations between customers and call center representatives for automatically classifying calls into predefined categories. We analyze the conversations for speaker dependent information content using several multi-class classification technologies. The data consists of 539 manually transcribed conversations belonging to 15 categories. Classifiers were built using Support Vector Machines, Naive Bayes, Latent Semantic Analysis, Vector Space and K-Nearest Neighbor technologies. SVM classifiers were found to perform consistently well giving an accuracy of about 74% on the entire data and about 92% when considering only the 4 largest classes. It is observed that very high weightage to either the customer part of the dialog or that of the agent results in poor accuracy. Nearly equal weightage to the customer and agent provides the best results consistently. This approach has potential to identify cross sell and up-sell opportunities in real-time.								AGRAWAL R, 2000, EXTENDING DATABASE T, P365; BERRY MW, 1995, SIAM REV, V37, P177; CARPENTER B, 1998, NATURAL LANGUAGE CAL; Chakrabarti S, 1997, PROCEEDINGS OF THE TWENTY-THIRD INTERNATIONAL CONFERENCE ON VERY LARGE DATABASES, P446; Chakrabarti S., 2002, MINING WEB DISCOVERI; CHANG C, 2001, LIBSVM LIB SUPPOORT; Chu-Carroll J, 1999, COMPUT LINGUIST, V25, P361; Cristianini N., 2000, INTRO SUPPORT VECTOR; GOLDEN J, 1999, P ICASSP, P509; Joachims T., 2002, LEARNING CLASSIFY TE; Joachims T., 1997, P 14 INT C MACH LEAR, P143; Ma J., OSU SVM CLASSIFIER M; McCallum A., 1998, AAAI 98 WORKSH LEARN, P41; MYERS K, P ICML 00 17 INT C M, P655; RAGHAVAN VV, 1986, J AM SOC INFORM SCI, V37, P279, DOI 10.1002/asi.4630370502; ROCHERY M, 2002, P ICASSP 2002; SALTON G, 1988, INFORM PROCESS MANAG, V24, P513, DOI 10.1016/0306-4573(88)90021-0; Sebastiani F, 2002, ACM COMPUTING SURVEY, V34; VAITHYANATHAN S, 2000, PRICAI 2000 INT WORK, P36	19	0	0	WIT PRESS	SOUTHAMPTON	ASHURST LODGE, SOUTHAMPTON SO40 7AA, ASHURST, ENGLAND	1470-6326		1-85312-729-9	MANAG INFORMAT SYST			2004	10						317	325				9	Computer Science, Information Systems; Computer Science, Interdisciplinary Applications	Computer Science	BBO45	WOS:000226691500030	
S	Grabczewski, K		Rutkowski, L; Siekmann, J; Tadeusiewicz, R; Zadeh, LA		Grabczewski, K			SSV criterion based discretization for naive Bayes classifiers	ARTIFICIAL INTELLIGENCE AND SOFT COMPUTING - ICAISC 2004	Lecture Notes in Artificial Intelligence		English	Article; Proceedings Paper	7th International Conference on Artificial Intelligence and Soft Computing	JUN 07-11, 2004	Zakopane, POLAND	Polish Neural Network Soc, Czestochowa Univ Technol, Dept Comp Engn				Decision tree algorithms deal with continuous variables by finding split points which provide best separation of objects belonging to different classes. Such criteria can also be used to augment methods which require or prefer symbolic data. A tool for continuous data discretization based on the SSV criterion (designed for decision trees) has been constructed. It significantly improves the performance of Naive Bayes Classifier. The combination of the two methods has been tested on 15 datasets from UCI repository and compared with similar approaches. The comparison confirms the robustness of the system.	Nicholas Copernicus Univ, Dept Informat, PL-87100 Torun, Poland	Grabczewski, K (reprint author), Nicholas Copernicus Univ, Dept Informat, Ul Grudziadzka 5, PL-87100 Torun, Poland.	kgrabcze@phys.uni.torun.pl					Dougherty J., 1995, P 12 INT C MACH LEAR, P194; DUCH W, 2003, P INT C ART NEUR NET; Fayyad U.M., 1993, P 13 INT JOINT C ART, P1022; GRABCZEWSKI K, 2000, P 5 C NEUR NETW THEI; GRLBEZEWSKI K, 2003, P ICANN 2003; Kerber R., 1992, NAT C ART INT, P123; LIU H, 1995, P 7 IEEE INT C TOOLS; Merz C. J., 1998, UCI REPOSITORY MACHI; Yang Y., 2002, P PAC RIM KNOWL ACQ, P159	9	0	0	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		3-540-22123-9	LECT NOTES ARTIF INT			2004	3070						574	579				6	Computer Science, Artificial Intelligence	Computer Science	BAH85	WOS:000222325200086	
S	Grigoryan, V; Chiarulli, D; Hauskrecht, M		Banks, D; House, L; McMorris, FR; Arabie, P; Gaul, W		Grigoryan, V; Chiarulli, D; Hauskrecht, M			Subject filtering for passive biometric monitoring	CLASSIFICATION, CLUSTERING, AND DATA MINING APPLICATIONS	STUDIES IN CLASSIFICATION, DATA ANALYSIS, AND KNOWLEDGE ORGANIZATION		English	Proceedings Paper	Meeting of the International-Federation-of-Classifications-Societies (IFCS)	JUL 15-18, 2004	Chicago, IL	Int Federat Classificat Soc	Illinois Inst Technol			Biometric data can provide useful information about the person's overall wellness. However, the invasiveness of the data collection process often prevents their wider exploitation. To alleviate this difficulty we are developing a biometric monitoring system that relies on nonintrusive biological traits such as speech and gait. We report on the development of the pattern recognition module of the system that is used to filter out nonsubject data. Our system builds upon a number of signal processing and statistical machine learning techniques to process and filter the data, including, Principal Component Analysis for feature reduction, the Naive Bayes classifier for the gait analysis, and the Mixture of Gaussian classifiers for the voice analysis. The system achieves high accuracy in filtering non-subject data, more specifically, 84% accuracy on the gait channel and 98% accuracy on the voice signal. These results allow us to generate sufficiently accurate data streams for health monitoring purposes.	Univ Pittsburgh, Pittsburgh, PA 15260 USA	Grigoryan, V (reprint author), Univ Pittsburgh, Pittsburgh, PA 15260 USA.						ATAL BS, 1976, P IEEE, V64, P460, DOI 10.1109/PROC.1976.10155; BRUNELLI R, 1995, IEEE T PATTERN ANAL, V17, P955, DOI 10.1109/34.464560; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1; Domingos P, 1997, MACH LEARN, V29, P103, DOI 10.1023/A:1007413511361; Duda R.O., 2001, PATTERN CLASSIFICATI; Gopinath R., 1998, P INT C AC SPEECH SI, V2, P661, DOI 10.1109/ICASSP.1998.675351; Haykin S., 1999, NEURAL NETWORKS COMP; Hong L., 1999, P IEEE WORKSH AUT ID, P59; Jolliffe I. T., 2002, PRINCIPAL COMPONENT; REYNOLDS DA, 1995, IEEE T SPEECH AUDI P, V3, P72, DOI 10.1109/89.365379; Stevens SS, 1940, AM J PSYCHOL, V53, P329, DOI 10.2307/1417526	11	0	0	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	1431-8814		3-540-22014-3	ST CLASS DAT ANAL			2004							485	492				8	Computer Science, Artificial Intelligence; Computer Science, Information Systems; Operations Research & Management Science; Mathematics, Applied	Computer Science; Operations Research & Management Science; Mathematics	BAS44	WOS:000223368200046	
B	Gu, ZM; Cercone, N		Callaos, N; Lesso, W; Nomura, S; Zhang, J		Gu, ZM; Cercone, N			Information extraction with the use of inter-slot relationships	8TH WORLD MULTI-CONFERENCE ON SYSTEMICS, CYBERNETICS AND INFORMATICS, VOL XIV, PROCEEDINGS: COMPUTER AND INFORMATION SYSTEMS, TECHNOLOGIES AND APPLICATIONS			English	Proceedings Paper	8th World Multi-Conference on Systemics, Cybernetics and Informatics	JUL 18-21, 2004	Orlando, FL	Int Inst Informat & System, Amer Soc Cybernet, Acad Non Linear Sci, Univ Las Palmas Gran Canaria, Telemat Engn Dept, Concurrency & Architecture Grp, CUST, Blaise Pascal Univ, Engn Sci Inst, Cybernet & Human Knowing, Int Federat Syst Res, Int Syst Inst, Int Soc Syst Sci, Italian Soc System, Univ Nacl San Luis, Lab Res Computac Intelligence, Dept Informat, Polish Syst Soc, Slovenian Artificial Intelligence Soc, Soc Appl Syst Res, Syst Soc Poland, Ctr Syst Studies, Tunisian Sci Soc, World Org System & Cybernet, IEEE Comp Soc, Venezuela Chapter, IEEE, Venezuela Chapter, Natl Res Council Canada, Steacie Inst Mol Sci		Information Extraction (IE); naive Bayes; inter-slot relationships; rule induction; slot; filler		In most Information Extraction (IE) systems, the search for the fillers of each slot in the extraction template is usually performed separately with no consideration of the relationships among these different slots. It is observed that such inter-slot relationships do exist in some information extraction tasks, which can be regarded as a potential information source to enhance the system's overall extraction performance across all the slots. We extend the traditional IE procedure by integrating steps to discover and utilize inter-slot relationships during the extraction process. Our preliminary experimental results show the potential of this approach.	Univ Waterloo, Sch Comp Sci, Waterloo, ON N2L 3B8, Canada	Gu, ZM (reprint author), Univ Waterloo, Sch Comp Sci, Waterloo, ON N2L 3B8, Canada.						AN A, 1998, LECT NOTES ARTIF INT, V1418, P426; BAGGA A, 1998, P 7 MESS UND C MUC 7, P161; Califf M. E., 1999, Proceedings Sixteenth National Conference on Artificial Intelligence (AAI-99). Eleventh Innovative Applications of Artificial Intelligence Conference (IAAI-99); FREITAG D, 1999, P AAAI 99 WORKSH MAC; FREITAG D, 1998, P 15 INT C MACH LEAR, P161; Soderland S, 1999, MACH LEARN, V34, P233, DOI 10.1023/A:1007562322031	6	0	0	INT INST INFORMATICS & SYSTEMICS	ORLANDO	14269 LORD BARCLAY DR, ORLANDO, FL 32837 USA			980-6560-13-2				2004							74	79				6	Computer Science, Artificial Intelligence; Computer Science, Information Systems; Computer Science, Theory & Methods	Computer Science	BBT43	WOS:000227691900015	
B	Janssens, D; Lan, Y; Wets, G; Chen, GQ; Brijs, T		Ruan, D; DHondt, P; DeCock, M; Nachtegael, M; Kerre, EE		Janssens, D; Lan, Y; Wets, G; Chen, GQ; Brijs, T			Empirically validating an adapted classification based on associations algorithm on UCI data	APPLIED COMPUTATIONAL INTELLIGENCE			English	Proceedings Paper	6th International Conference on Fuzzy Logic and Intelligent Technologies in Nuclear Science	SEP 01-03, 2004	Blankenberge, BELGIUM					In recent years, extensive research has been carried out by using association rules to build more accurate classifiers. The idea behind these integrated approaches is to focus on a limited subset of association rules, i.e. those rules where the consequence of the rule is restricted to the classification class attribute. This paper aims to contribute to this integrated framework by adapting the CBA (Classification Based on Associations) algorithm. More specifically, CBA was modified by coupling it with a new measurement of the quality of association rules: i.e. intensity of implication. By means of this measurement, the sequence in which the class association rules are chosen, was changed when building the classifier. The new algorithm has been implemented and empirically tested on 16 popular datasets from the UCI Machine Learning Repository. Furthermore, the results were validated with original CBA, with C4.5 (both on original and on discretized datasets), and with Naive Bayes. The adapted CBA algorithm presented in this paper, proved to generate a lowest average error rate and produced classifiers that are more compact than original CBA.	Limburgs Univ Ctr, Transportat Res Inst, B-3590 Diepenbeek, Belgium	Janssens, D (reprint author), Limburgs Univ Ctr, Transportat Res Inst, Univ Campus,Gebouw D, B-3590 Diepenbeek, Belgium.	davy.janssens@luc.ac.be; yul1@em.tsinghua.edu.cn; geert.wets@luc.ac.be; chengq@em.tsinghua.edu.cn; tom.brijsj@luc.ac.be					Gras R., 1993, MATH INFORMATIQUE SC, V120, P5; GUILLAUME S, 1998, PKDD 98, V10, P318; Liu B., 1998, P 4 INT C KNOWL DISC, P80; LIU B, 2001, DATA MINING SCI ENG; Suzuki E, 1998, LECT NOTES ARTIF INT, V1510, P10	5	0	1	WORLD SCIENTIFIC PUBL CO PTE LTD	SINGAPORE	PO BOX 128 FARRER RD, SINGAPORE 9128, SINGAPORE			981-238-873-7				2004							167	172		10.1142/9789812702661_0033		6	Computer Science, Artificial Intelligence	Computer Science	BCD87	WOS:000228784700030	
B	Kang, SS		Hamza, MH		Kang, SS			Selecting features by term importance for text categorization	Proceedings of the IASTED International Conference on Artificial Intelligence and Applications, Vols 1and 2			English	Proceedings Paper	IASTED International Conference on Artificial Intelligence and Applications	FEB 16-18, 2004	Innsbruck, AUSTRIA	Int Assoc Sci & Technol Dev		text categorization; term weighting; feature selection; document representation; term importance		A great deal of work has been done over the years in an attempt to improve the performance of the text categorization system. Statistical or probabilistic models, such as Naive Bayes and support vector machine(SVM), and feature selection techniques are explored and obtained a good result. However, the performance depends crucially on the choice of effective term weighting systems. Unfortunately, many of categorization methods are lacking in effectiveness, and more refined category representation methods are required. We applied a new technique of term weighting method for the representation of input document and the category learning; that is, the text words are ranked in accordance with how well they are able to discriminate the documents of a collection from each other. Experimental results are given showing the effectiveness of the term weighting method. We found that our term weighting system got a significant improvement over the base-line system of tf-idf weighting scheme.	Kookmin Univ, Sch Comp Sci, Seoul 136702, South Korea	Kang, SS (reprint author), Kookmin Univ, Sch Comp Sci, 861-1 Chongnung Dong, Seoul 136702, South Korea.						Cohen WW, 1999, ACM T INFORM SYST, V17, P141, DOI 10.1145/306686.306688; JOACHIMS T, 1998, P ECML 98; KANG S, 2001, P 13 C KOR LANG COMP, P196; KO Y, 2001, J KOREAN INFORMATION, P417; Lai Y. S., 2002, ACM T ASIAN LANGUAGE, V1, P34, DOI 10.1145/595576.595579; Mladenic D., 1999, P 16 INT C MACH LEAR, P258; Ponte J. M., 1998, Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, DOI 10.1145/290941.291008; Provost F, 2001, MACH LEARN, V42, P203, DOI 10.1023/A:1007601015854; Quinlan J.R., 1993, C4 5 PROGRAMS MACHIN; Sahami M., 1996, P 2 INT C KNOWL DISC, P335; SALTON G, 1975, J AM SOC INFORM SCI, V26, P33, DOI 10.1002/asi.4630260106; SALTON G, 1988, INFORM PROCESS MANAG, V24, P513, DOI 10.1016/0306-4573(88)90021-0; Sebastiani F, 2002, ACM COMPUT SURV, V34, P1, DOI 10.1145/505282.505283; SONG W, 1999, P ACM SIGIR 99, P279; WILBUR WJ, 1992, J INFORM SCI, V18, P45, DOI 10.1177/016555159201800106; Yang XH, 2001, CHINESE J ASTRON AST, V1, P200; Yang Y., 1999, P 22 ANN INT ACM SIG, P42, DOI 10.1145/312624.312647; Yang Y., 1997, P 14 INT C MACH LEAR, P412	18	0	0	ACTA PRESS	CALGARY	B6, STE 101, 2509 DIEPPE AVE SW, CALGARY, ALBERTA T3E 7J9, CANADA			0-88986-404-7				2004							342	346				5	Computer Science, Artificial Intelligence	Computer Science	BCC65	WOS:000228622100059	
S	Kanth, AS; Murthy, KN		Vicedo, JL; MartinezBarco, P; Munoz, R; Noeda, MS		Kanth, AS; Murthy, KN			Significance of syntactic features for Word Sense Disambiguation	ADVANCES IN NATURAL LANGUAGE PROCESSING	LECTURE NOTES IN ARTIFICIAL INTELLIGENCE		English	Article; Proceedings Paper	4th International Conference on Espana for Natural Language Processing (EsTAL)	OCT 20-22, 2004	Alicante, SPAIN	Univ Alicante				In this paper(1) we explore the use of syntax in improving the performance of Word Sense Disambiguation(WSD) systems. We argue that not all words in a sentence are useful for disambiguating the senses of a target word and eliminating noise is important. Syntax can be used to identify related words and eliminating other words as noise actually improves performance significantly. CMU's Link Parser has been used for syntactic analysis. Supervised learning techniques have been applied to perform word sense disambiguation on selected target words. The Naive Bayes classifier has been used in all the experiments. All the major grammatical categories of words have been covered. Experiments conducted and results obtained have been described. Ten fold cross validation has been performed in all cases. The results we have obtained are better than the published results for the same data.	Univ Hyderabad, Dept Comp & Informat Sci, Hyderabad 500134, Andhra Pradesh, India	Kanth, AS (reprint author), Univ Hyderabad, Dept Comp & Informat Sci, Hyderabad 500134, Andhra Pradesh, India.	sasi_kanth_a@yahoo.co.in; knmuh@yahoo.com					Bruce R.F., 1994, P 32 ANN M ASS COMP, P139, DOI 10.3115/981732.981752; DOLAN WB, 1994, MSRTR9418 MICR CORP; Gale William A., 1992, P 30 ANN M ASS COMP, P249, DOI 10.3115/981967.981999; Leacock C., 1998, COMPUTATIONAL LINGUI, V24; Lesk M., 1986, P 5 ANN INT C SYST D, P24, DOI 10.1145/318723.318728; MIHALCEA R, 1999, P 37 ANN M ASS COMP; MOHAMMAD S, 2004, P CONLL 2004 BOST MA, P25; NG HT, 2002, P C EMP METH NAT LAN, P41; Ng H.T., 1996, P 34 ANN M ASS COMP, P40, DOI 10.3115/981863.981869; Pederson T., 2001, P 2 M N AM CHAPT ASS, P79; Rivest R. L., 1987, Machine Learning, V2, DOI 10.1007/BF00058680; Sleator D., 1993, 3 INT WORKSH PARS TE; Sleator D.D.K., 1991, CMUCS91196; STETINA J, 1998, P COLING ACL WORKSH; WILKS Y, 1998, P ACL 36 COL, V17, P1398; Yarowsky D, 1995, M ASS COMP LING, P189; YAROWSKY D, 2000, COMPUTERS HUMANITIES, V34; Yarowsky David, 1994, P 32 ANN M ASS COMP, P88, DOI 10.3115/981732.981745; YAROWSKY JP, 2001, ACL2001EACL2001	19	0	0	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		3-540-23498-5	LECT NOTES ARTIF INT			2004	3230						340	348				9	Computer Science, Artificial Intelligence	Computer Science	BBE05	WOS:000225095200030	
S	Kowalczyk, A; Raskutti, B; Ferra, H		Dai, H; Srikant, R; Zhang, C		Kowalczyk, A; Raskutti, B; Ferra, H			Exploring potential of leave-one-out estimator for calibration of SVM in text mining	ADVANCES IN KNOWLEDGE DISCOVERY AND DATA MINING, PROCEEDINGS	LECTURE NOTES IN ARTIFICIAL INTELLIGENCE		English	Article; Proceedings Paper	8th Pacific/Asia Conference on Advances in Knowledge Discovery and Data Mining	MAY 26-28, 2004	Sydney, AUSTRALIA	NICIA, SAS, Univ Technol Sydney, Deakin Univ			CLASSIFICATION; NETWORKS	This paper investigates a number of techniques for calibration of the output of a Support Vector Machine in order to provide a posterior probability P(target class\instance). Five basic calibration techniques are combined with five ways of correcting the SVM scores on the training set. The calibration techniques used are addition of a simple ramp function, allocation of a Gaussian density, fitting of a sigmoid to the output and two binning techniques. The correction techniques include three methods that are based on recent theoretical advances in leave-one-out estimators and two that are variants of hold-out validation set. This leads us to thirty different settings (including calibration on uncorrected scores). All thirty methods are evaluated for two linear SVMs (one with linear and one with quadratic penalty) and for the ridge regression model (regularisation network) on three categories of the Reuters Newswires benchmark and the WebKB dataset. The performance of these methods are compared to both the probabilities generated by a naive Bayes classifier as well as a calibrated centroid classifier. The main conclusions of this research are: (i) simple calibrators such as ramp and sigmoids perform remarkably well, (ii) score correctors using leave-one-out techniques can perform better than those using validation sets, however, cross-validation methods allow more reliable estimation of test error from the training data.	Telstra Corp, Clayton, Vic 3168, Australia	Kowalczyk, A (reprint author), Telstra Corp, 770 Blackburn Rd, Clayton, Vic 3168, Australia.	Adam.Kowalczyk@team.telstra.com; Bhavani.Raskutti@team.telstra.com; Herman.Ferra@team.telstra.com					Bauer E, 1999, MACH LEARN, V36, P105, DOI 10.1023/A:1007515423169; CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; Cristianini N., 2000, INTRO SUPPORT VECTOR; DRISH J, OBTAINING CALIBRATED; GIROSI F, 1995, NEURAL COMPUT, V7, P219, DOI 10.1162/neco.1995.7.2.219; HAND DJ, 1982, CONSTRUCTION ASSESSM; Jaakkola TS, 1999, P 1999 C AI STAT; JOACHIMS T, 2000, 7 INT C MACH LEARN S, P431; KIMELDOR.GS, 1970, ANN MATH STAT, V41, P495, DOI 10.1214/aoms/1177697089; KOWALCZYK A, 2003, P 7 EUR C PRINC PRAC; Mitchell T., 1996, MACHINE LEARNING; Nigam K, 2000, MACH LEARN, V39, P103, DOI 10.1023/A:1007692713085; OPPER M, 2000, ADV LARGE MARGIN CLA, P301; Platt JC, 2000, ADV NEUR IN, P61; Scholkopf B., 2001, LEARNING KERNELS SUP; SMYTH P, 1995, P 12 INT C MACH LEAR, P506; Vapnik V., 1998, STAT LEARNING THEORY; Zadrozny B., 2001, P 18 INT C MACH LEAR	18	0	0	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		3-540-22064-X	LECT NOTES ARTIF INT			2004	3056						361	372				12	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Computer Science	BAF29	WOS:000221955100042	
S	Leroy, G; Rindflesch, TC		Fieschi, M; Coiera, E; Li, YCJ		Leroy, G; Rindflesch, TC			Using symbolic knowledge in the UMLS to disambiguate words in small datasets with a naive Bayes classifier	MEDINFO 2004: PROCEEDINGS OF THE 11TH WORLD CONGRESS ON MEDICAL INFORMATICS, PT 1 AND 2	STUDIES IN HEALTH TECHNOLOGY AND INFORMATICS		English	Proceedings Paper	11th World Congress on Medical Informatics	SEP 07-11, 2004	San Francisco, CA	Int Med Informat Assoc	Amer Med Informat Assoc	artificial intelligence; machine learning; naive Bayes; word sense disambiguation; Unified Medical Language System; UMLS; small datasets; symbolic knowledge	TERMS	Current approaches to word sense disambiguation use and combine various machine-learning techniques. Most refer to characteristics of the ambiguous word and surrounding words and are based on hundreds of examples. Unfortunately, developing large training sets is time-consuming. We investigate the use of symbolic knowledge to augment machine-learning techniques for small datasets. UMLS semantic types assigned to concepts found in the sentence and relationships between these semantic types form the knowledge base. A naive Bayes classifier was trained for 15 words with 100 examples for each. The most frequent sense of a word served as the baseline. The effect of increasingly accurate symbolic knowledge was evaluated in eight experimental conditions. Performance was measured by accuracy based on 10-fold cross-validation. The best condition used only the semantic types of the words in the sentence. Accuracy was then on average 10% higher than the baseline; however, it varied from 8% deterioration to 29% improvement. In a follow-up evaluation, we noted a trend that the best disambiguation was found for words that were the least troublesome to the human evaluators.	Claremont Grad Univ, Sch Informat Sci, Claremont, CA 91711 USA	Leroy, G (reprint author), Claremont Grad Univ, Sch Informat Sci, 130 E 9th St, Claremont, CA 91711 USA.						Aronson A.R., 2001, AMIA S, P17; FLORIAN R, 2002, NATURAL LANGUAGE ENG, V1, P1; HATZIVASSILOGLO.V, 2001, BIOINFORMATICS, V1, P1; Humphreys BL, 1998, J AM MED INFORM ASSN, V5, P1; Ide N, 1998, COMPUT LINGUIST, V24, P1; INKPEN DZ, 2003, 4 C INT TEXT PROC CO, P258; Liu HF, 2002, J AM MED INFORM ASSN, V9, P621, DOI 10.1097/jamia.M1101; Liu HF, 2001, J BIOMED INFORM, V34, P249, DOI 10.1006/jbin.2001.1023; McCray AT, 1993, HIGH PERFORMANCE MED, P45; MILLER GA, 1998, INTRO WORDNET LINE L; MOONEY RJ, 1996, C EMP METH NAT LANG; Pedersen T, 2001, 2ND MEETING OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P79; WEEBER M, 2001, AMIA S, P746; Witten I.H., 2000, DATA MINING PRACTICA	14	0	0	I O S PRESS	AMSTERDAM	NIEUWE HEMWEG 6B, 1013 BG AMSTERDAM, NETHERLANDS	0926-9630		1-58603-444-8	ST HEAL T			2004	107						381	385				5	Computer Science, Information Systems; Medical Informatics	Computer Science; Medical Informatics	BBO53	WOS:000226723300077	
B	Li, J; Li, HQ; Jia, XM		Kantardzic, M; Nasraoui, O; Milanova, M		Li, J; Li, HQ; Jia, XM			A Naive Bayes learning based website reconfiguration system	PROCEEDINGS OF THE 2004 INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA'04)			English	Proceedings Paper	3rd International Conference on Machine Learning and Applications	DEC 16-18, 2004	Louisville, KY	IEEE Syst, Man & Cybernet Sco, ACM SIGKDD, Assoc Machine Learning & Applicat, Univ Louisville, Dept Comp Engn & Comp Sci		Naive Bayes Classifier; web reconfiguration; machine learning; data mining		The continuous and sharp growth of web sites in terms of size and complexity has made improving the website organization to facilitate users' navigation something of an emergency. To address this problem, in this paper we propose a website reconfiguration system using the machine learning approach. First, a Naive Bayes Classifier is trained and then applied to identify each page in a web site as important oil unimportant in terms fulfilling visitors' information needs. For those important pages, we check the reason ableness of their locations, which is measured by the average number of hops needed to reach them during visitor sessions. Those important but difficult reach pages are considered for reconfiguration, which is done by either automatically moving them to some level closer to the visitors' starting point, making it easier for users to access them, or presenting webmasters with a list of suggestions. We also propose a formula to evaluate the "global structure" of a web site, and use it to examine the effect of our system on improving website design.	Univ Alberta, Dept Comp Sci, Edmonton, AB T6G 2M7, Canada	Li, J (reprint author), Univ Alberta, Dept Comp Sci, Edmonton, AB T6G 2M7, Canada.						ANDERSON CR, 2001, 17 INT JOINT C ART I; BERTHON P, 1996, J ADVERTISING RES, V36, P34; BORGELT C, NAIVE BAYES CLASSIFI; CATLEDGE LD, 1995, CHARACTERIZING BROWS; Cooley R., 1999, Knowledge and Information Systems, V1; Duda R., 1973, PATTERN CLASSIFICATI; GOOD IJ, 1965, ESSAY MODERN BAYESIA; Japkowicz N, 2000, 2000 INT C ART INT I, P111; KOHAVI R, 1998, SPECIAL ISSUE APPL M; Langley P, 1992, 10 NAT C ART INT; LI J, 2004, 5 INT C EL COMM WEB; Perkowitz M, 2000, ARTIF INTELL, V118, P245, DOI 10.1016/S0004-3702(99)00098-3; PROVOST F, 2000, LEARNING IMBALANCED, P101; Srikant R, 2001, WORLD WIDE WEB, P430; STERN MK, 1999, 7 INT C US MOD; SULLIVAN T, 1997, WEB C 97; TAKEHIRO HK, 2000, 9 INT WORLD WIDE WEB, P811; Zhu T., 2003, 9 INT C US MOD	18	0	0	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA			0-7803-8823-2				2004							18	25				8	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Computer Science	BBR87	WOS:000227493000004	
S	Li, JY; Ramamohanarao, K		Dai, H; Srikant, R; Zhang, C		Li, JY; Ramamohanarao, K			A tree-based approach to the discovery of diagnostic biomarkers for ovarian cancer	ADVANCES IN KNOWLEDGE DISCOVERY AND DATA MINING, PROCEEDINGS	LECTURE NOTES IN ARTIFICIAL INTELLIGENCE		English	Article; Proceedings Paper	8th Pacific/Asia Conference on Advances in Knowledge Discovery and Data Mining	MAY 26-28, 2004	Sydney, AUSTRALIA	NICIA, SAS, Univ Technol Sydney, Deakin Univ		decision trees; committee method; ovarian cancer; biomarkers; classification	CLASSIFICATION; PREDICTION	Computational diagnosis of cancer is a classification problem, and it has two special requirements on a learning algorithm: perfect accuracy and small number of features used in the classifier. This paper presents our results on an ovarian cancer data set. This data set is described by 15154 features, and consists of 253 samples. Each sample is referred to a woman who suffers from ovarian cancer or who does not have. In fact, the raw data is generated by the so-called mass spectrosmetry technology measuring the intensities of 15154 protein or peptide-features in a blood sample for every woman. The purpose is to identify a small subset of the features that can be used as biomarkers to separate the two classes of samples with high accuracy. Therefore, the identified features can be potentially used in routine clinical diagnosis for replacing labour-intensive and expensive conventional diagnosis methods. Our new tree-based method can achieve the perfect 100% accuracy in 10-fold cross validation on this data set. Meanwhile, this method also directly outputs a small set of biomarkers. Then we explain why support vector machines, naive bayes, and k-nearest neighbour cannot fulfill the purpose. This study is also aimed to elucidate the communication between contemporary cancer research and data mining techniques.	Inst Infocomm Res, Singapore 119613, Singapore; Univ Melbourne, Dept CSSE, Melbourne, Vic 3010, Australia	Li, JY (reprint author), Inst Infocomm Res, 21 Heng Mui Keng Terrace, Singapore 119613, Singapore.	jinyan@i2r.a-star.edu.sg; rao@cs.mu.oz.au					Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324; Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.1023/A:1018054314350; Burges CJC, 1998, DATA MIN KNOWL DISC, V2, P121, DOI 10.1023/A:1009715923555; CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; COVER TM, 1967, IEEE T INFORM THEORY, V13, P21, DOI 10.1109/TIT.1967.1053964; Dietterich TG, 2000, MACH LEARN, V40, P139, DOI 10.1023/A:1007607513941; Duda R., 1973, PATTERN CLASSIFICATI; Fayyad U.M., 1993, P 13 INT JOINT C ART, P1022; Freund Yoav, 1996, INT C MACH LEARN, P148; Golub TR, 1999, SCIENCE, V286, P531, DOI 10.1126/science.286.5439.531; JINYAN L, 2003, P ICDM, P585; JULIA D, 2001, NATURE REV CANC, V3, P267; Langley P., 1994, P 10 C UNC ART INT, P399; LI JY, 2003, BIOINFORMATICS, V19, P1193; LIU HQ, 2002, GENOME INFORMATICS, P51; Petricoin EF, 2002, LANCET, V359, P572, DOI 10.1016/S0140-6736(02)07746-2; Quinlan J. R., 1993, C45 PROGRAMS MACHINE; Yeoh EJ, 2002, CANCER CELL, V1, P133, DOI 10.1016/S1535-6108(02)00032-6	18	0	0	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		3-540-22064-X	LECT NOTES ARTIF INT			2004	3056						682	691				10	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Computer Science	BAF29	WOS:000221955100078	
B	Li, Q; Li, JH; Liu, GS; Li, SH			ieee	Li, Q; Li, JH; Liu, GS; Li, SH			A rough set-based hybrid feature selection method for topic-specific text filtering	PROCEEDINGS OF THE 2004 INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND CYBERNETICS, VOLS 1-7			English	Proceedings Paper	International Conference on Machine Learning and Cybernetics	AUG 26-29, 2004	Shanghai, PEOPLES R CHINA			text data; feature selection; rough set; x(2) statistic; information gain; mutual information		With the proliferation of harmful Internet content such as pornography, violence, and hate messages, effective content-filtering systems are essential. However, a non-trivial obstacle in good text filtering is the high dimensionality of the data. In this paper, we introduced a hybrid method to select features more accurately using some feature selection method and rough set theory. We can select features firstly using one of feature selection methods, such as x(2) statistic, mutual information, information gain, and then further select features using rough set. Thus more accurate and less features are extracted. In experiments, we used UCI machine learning dataset as our dataset. We use naive Bayes model to evaluate our feature selection method, the result shows our method has high precision and high recall, and is very effective and efficient.	Shanghai Jiao Tong Univ, Modern Commun Res Dept, Shanghai 200030, Peoples R China	Li, Q (reprint author), Shanghai Jiao Tong Univ, Modern Commun Res Dept, Shanghai 200030, Peoples R China.						Agrawal R., 2000, P 7 INT C EXT DAT TE, P365; Chouchoulas A, 2001, APPL ARTIF INTELL, V15, P843, DOI 10.1080/088395101753210773; Lee PY, 2002, IEEE INTELL SYST, V17, P48, DOI 10.1109/MIS.2002.1039832; PAWLAK Z, 1982, INT J COMPUT INF SCI, V11, P341, DOI 10.1007/BF01001956; Rogati M., 2002, CIKM 02, P659; Sahami M., 1998, AAAI 98 WORKSH LEARN	6	0	0	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA			0-7803-8403-2				2004							1464	1468				5	Computer Science, Artificial Intelligence; Computer Science, Cybernetics; Computer Science, Information Systems	Computer Science	BBF92	WOS:000225293600289	
B	Li, XL; Joshi, R; Ramachandaran, S; Leong, TY		Rastogi, R; Morik, K; Bramer, M; Wu, X		Li, XL; Joshi, R; Ramachandaran, S; Leong, TY			Classifying biomedical citations without labeled training examples	FOURTH IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS			English	Proceedings Paper	4th IEEE International Conference on Data Mining	NOV 01-04, 2004	Brighton, ENGLAND	IEEE Comp Soc, TCCI, IEEE Comp Soc, TCPAMI, IBM Res, StatSoft Ltd, Web Intelligence Consortium				In this paper we introduce a novel technique for classifying text citations without labeled training examples. We first utilize the search results of a general search engine as original training data. We then proposed a mutually reinforcing learning algorithm (MAL) to mine the classification knowledge and to "clean" the training data. With the help of a set of established domain-specific ontological terms or keywords, the MRL mining step derives the relevant classification knowledge. The MRL cleaning step then builds a Naive Bayes classifier based on the mined classification knowledge and tries to clean the training set. The MRL algorithm is iteratively applied until a clean training set is obtained. We show the effectiveness of the proposed technique in the classification of biomedical citations from a large medical literature database.	Natl Univ Singapore, Sch Comp, Comp Sci Program, MIT Alliance, Singapore 117548, Singapore	Li, XL (reprint author), Natl Univ Singapore, Sch Comp, Comp Sci Program, MIT Alliance, Singapore 117548, Singapore.		Li, Xiaoli/C-9739-2012				Agrawal R., 1994, P 20 INT C VER LARG, P487; Blum A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, DOI 10.1145/279943.279962; Laird N.M., 1977, J ROYAL STAT SOC B, V39, P1; Li X., 2003, P 18 INT JOINT C ART, P587; LIU B, P 19 INT C MACH LEAR, P387; McCallum A., 1998, AAAI 98 WORKSH LEARN, P41; Nigam K, 2000, MACH LEARN, V39, P103, DOI 10.1023/A:1007692713085; Yu H, 2002, P ACM SIGKDD INT C K, P239	8	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA			0-7695-2142-8				2004							455	458				4	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Computer Science	BBI95	WOS:000225713000075	
B	Lindsay, D; Cox, S		Rastogi, R; Morik, K; Bramer, M; Wu, X		Lindsay, D; Cox, S			Improving the reliability of decision tree and naive Bayes learners	FOURTH IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS			English	Proceedings Paper	4th IEEE International Conference on Data Mining	NOV 01-04, 2004	Brighton, ENGLAND	IEEE Comp Soc, TCCI, IEEE Comp Soc, TCPAMI, IBM Res, StatSoft Ltd, Web Intelligence Consortium				The C4.5 Decision Tree and Naive Bayes learners are known to produce unreliable probability forecasts. We have used simple Binning [11] and Laplace Transform [2] techniques to improve the reliability of these learners and compare their effectiveness with that of the newly developed Venn Probability Machine (VPM) meta-learner [9]. We assess improvements in reliability using loss functions, Receiver Operator Characteristic (ROC) curves and Empirical Reliability Curves (ERC). The VPM outperforms the simple techniques to improve reliability, although at the cost of increased computational intensity and slight increase in error rate. These trade-offs are discussed.	Univ London Royal Holloway, Comp Learning Res Ctr, Egham TW20 0EX, Surrey, England	Lindsay, D (reprint author), Univ London Royal Holloway, Comp Learning Res Ctr, Egham TW20 0EX, Surrey, England.						Atkeson CG, 1997, ARTIF INTELL REV, V11, P11, DOI 10.1023/A:1006559212014; CESTNIK B, 1990, P 9 EUR C ART INT; DAWID AP, 1985, ANN STAT, V13, P1251, DOI 10.1214/aos/1176349736; DEGROOT MH, 1983, STATISTICIAN, V32, P12, DOI 10.2307/2987588; FAYYAD U, 1992, P 10 NAT C ART INT; LINDSAY D, 2004, CLRCTR0401; Murphy A. H., 1973, Journal of Applied Meteorology, V12, DOI 10.1175/1520-0450(1973)012<0595:ANVPOT>2.0.CO;2; Provost F., 1997, P 3 INT C KNOWL DISC; VOVK V, 2003, ADV NEURAL INFORMATI; Witten I.H., 2000, DATA MINING PRACTICA; Zadrozny B., 2001, P 18 INT C MACH LEAR	11	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA			0-7695-2142-8				2004							459	462		10.1109/ICDM.2004.10037		4	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Computer Science	BBI95	WOS:000225713000076	
S	Martinez-Morales, M; Cruz-Ramirez, N; Jimenez-Andrade, JL; Garza-Dominguez, R		Monroy, R; ArroyoFigueroa, G; Sucar, LE; Sossa, H		Martinez-Morales, M; Cruz-Ramirez, N; Jimenez-Andrade, JL; Garza-Dominguez, R			Bayes-N: An algorithm for learning Bayesian networks from data using local measures of information gain applied to classification problems	MICAI 2004: ADVANCES IN ARTIFICIAL INTELLIGENCE	LECTURE NOTES IN COMPUTER SCIENCE		English	Article; Proceedings Paper	3rd Mexican International Conference on Artificial Intelligence (MICAI 2004)	APR 26-30, 2004	Mexico City, MEXICO	Mexican Soc Artificial Intelligence, Tecnol Monterrey, Ciudad Mexico, Inst Investigaciones Elect, Inst Politec Natl, Inst Technol Autonomo Mexico, Opt & Elect		Bayesian networks; data mining; classification; MDL; machine learning		Bayes-N is an algorithm for Bayesian network learning from data based on local measures of information gain, applied to problems in which there is a given dependent or class variable and a set of independent or explanatory variables from which we want to predict the class variable on new cases. Given this setting, Bayes-N induces an ancestral ordering of all the variables generating a directed acyclic graph in which the class variable is a sink variable, with a subset of the explanatory variables as its parents. It is shown that classification using this variables as predictors performs better than the naive bayes classifier, and at least as good as other algorithms that learn Bayesian networks such as K2, PC and Bayes-9. It is also shown that the MDL measure of the networks generated by Bayes-N is comparable to those obtained by these other algorithms.	Univ Veracruzana, Fac Fis & Inteligencia Artificial, Xalapa 91000, Veracruz, Mexico; LANIA, Xalapa, Veracruz, Mexico	Martinez-Morales, M (reprint author), Univ Veracruzana, Fac Fis & Inteligencia Artificial, Xalapa 91000, Veracruz, Mexico.	manumartinez@uv.mx; ncruz@lania.mx	Lerma, Sergio/A-2794-2012; Jimenez Andrade, Jose Luis/E-2262-2012				Bickel P. J., 1977, MATH STAT BASIC IDEA; BLAND JM, 1995, BRIT MED J, V310, P170; Cooper GF, 1999, COMPUTATION, CAUSATION, AND DISCOVERY, P3; COOPER GF, 1992, MACH LEARN, V9, P309, DOI 10.1007/BF00994110; Cowell R. G., 1999, PROBABILISTIC NETWOR; CROSS SS, 1998, CYTOPATHOLOGY, V8, P178; CROSS SS, 2000, ARTIF INTELL, V39, P265, DOI 10.1142/9789812792488_0007; CRUZRAMIREZ N, 1997, PRIMER ENCUENTRO NAC; CRUZRAMIREZ N, 2001, THESIS U SHEFFIELD; FEINBERG SE, 1981, ANAL CROSS CLASSIFIE; FREIDMAN N, 1997, MACH LEARN, V29, P131; Han J., 2001, DATA MINING CONCEPTS; Heckerman D., 1994, MSRTR9409; JENSEN F, 2001, BAYESIAN NETWORKS DE; Kohavi R., 1995, 14 INT JOINT C ART I; KU HH, 1971, J AM STAT ASSOC, V66, P55, DOI 10.2307/2284848; KULLBACK S, 1949, INFORMATION THEORY S; MARTINEZMORALES M, 1995, 12 REUN NAC INT ART; Pearl J., 1988, PROBABILISTIC REASON; Quinlan J.R., 1993, C4 5 PROGRAMS MACHIN; Shannon CE, 1949, MATH THEORY COMMUNIC; Spirtes P., 1991, Social Science Computer Review, V9, DOI 10.1177/089443939100900106; Spirtes P., 1993, CAUSATION PREDICTION; Whittaker J., 1990, GRAPHICAL MODELS APP	24	0	0	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		3-540-21459-3	LECT NOTES COMPUT SC			2004	2972						527	535				9	Computer Science, Artificial Intelligence	Computer Science	BAB91	WOS:000221506600054	
S	Pena, JM; Robles, V; Marban, O; Perez, MS		Menasalvas, E; Chavez, E		Pena, JM; Robles, V; Marban, O; Perez, MS			Bayesian methods to estimate future load in web farms	ADVANCES IN WEB INTELLIGENCE, PROCEEDINGS	LECTURE NOTES IN ARTIFICIAL INTELLIGENCE		English	Article; Proceedings Paper	2nd International Altantic Web Intelligence Conference (AWIC 2004)	MAY 16-19, 2004	Cancun, MEXICO	Ctr Investigac Cientif Educ Super Ensenada, Inst Nac Astrofis Opt Elect, Univ Michoacan San Nicolas Hidalgo, Univ Politean Madrid, Soc Mexicana Cienc Computac		web farms; web load estimation; naive Bayes; EDAs	ALGORITHM	Web Farms are clustered systems designed to provide high availability and high performance web services. A web farm is a group of replicated HTTP servers that reply web requests forwarded by a single point of access to the service. To deal with this task the point of access executes a load balancing algorithm to distribute web request among the group of servers. The present algorithms provides a short-term dynamic configuration for this operation, but some corrective actions (granting different session priorities or distributed WAN forwarding) cannot be achieved without a long-term estimation of the future web load. On this paper we propose a method to forecast web service work load. Our approach also includes an innovative segmentation method for the web pages using EDAs (estimation of distribution algorithms) and the application of semi-naive Bayes classifiers to predict future web load several minutes before. All our analysis has been performed using real data from a world-wide academic portal.	Univ Politecn Madrid, DATSI, Madrid, Spain; Univ Politecn Madrid, DLSIS, Madrid, Spain	Pena, JM (reprint author), Univ Politecn Madrid, DATSI, Madrid, Spain.	jmpena@fi.upm.es; vrobles@fi.upm.es; omarban@fi.upm.es; mperez@fi.upm.es					ANDRESEN D, 1996, P 10 IEEE INT S PAR, P850, DOI 10.1109/IPPS.1996.508191; BANOS R, 2003, P 14 J PAR, P245; BANOS R, 2003, 2611 LNCS, P143; Brisco T, 1995, 1794 RFC; Bui TN, 1996, IEEE T COMPUT, V45, P841; BUI TN, 1992, INFORM PROCESS LETT, V42, P153, DOI 10.1016/0020-0190(92)90140-Q; CONTI M, 1999, P WORKSH INT SERV PE; Domingos P., 1996, P 13 INT C MACH LEAR, P105; Dougherty J., 1995, P 12 INT C MACH LEAR, P194; Fayyad U.M., 1993, P 13 INT JOINT C ART, P1022; Fiduccia CM, 1982, P 19 IEEE DES AUT C, P175, DOI 10.1145/800263.809204; GHINI V, 2002, HICSS; Hand DJ, 2001, INT STAT REV, V69, P385, DOI 10.2307/1403452; Hochsztain E, 2002, LECT NOTES ARTIF INT, V2475, P479; HOLTE RC, 1993, MACH LEARN, V11, P63, DOI 10.1023/A:1022631118932; Kohavi R., 1996, P 2 INT C KNOWL DISC, P202; Larranaga P., 2002, ESTIMATION DISTRIBUT; MARTIN B, 9518 U WAIKATO; PAZZANI M, 1996, STAT INDUCTION SCI, P66; Quinlan R., 1993, C4 5 PROGRAMS MACHIN; RALF S, 1998, WEB TECHNIQUES MAGAZ, V3; Robles V, 2003, LECT NOTES ARTIF INT, V2902, P244; Simon HD, 1997, SIAM J SCI COMPUT, V18, P1436, DOI 10.1137/S1064827593255135; SRISURESH P, 1998, 2391 RFC; THOMAS T, 1995, IEEE COMPUT, P68; TING KM, 1994, 491 U SYDNEY; Walshaw C, 2000, SIAM J SCI COMPUT, V22, P63, DOI 10.1137/S1064827598337373; ZHANG WS, 1999, LINUXEXPO 1999 C; ZHANG WS, 2000, OTTAWA LINUX S; WEKA 3 DATA MINING	30	0	0	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		3-540-22009-7	LECT NOTES ARTIF INT			2004	3034						217	226				10	Computer Science, Artificial Intelligence	Computer Science	BAE37	WOS:000221807000022	
S	Phung, SL; Bouzerdoum, A; Chai, D; Watson, A			IEEE	Phung, SL; Bouzerdoum, A; Chai, D; Watson, A			Naive bayes face/nonface classifier: A study of preprocessing and feature extraction techniques	ICIP: 2004 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOLS 1- 5	IEEE International Conference on Image Processing (ICIP)		English	Proceedings Paper	International Conference on Image Processing (ICIP 2004)	OCT 24-27, 2004	Singapore, SINGAPORE	IEEE			FACE DETECTION; COLOR IMAGES	This paper presents a classifier of face and nonface patterns that is based on the naive Bayes model. Using this classifier as a tool, we analyze the effects on classification performance of preprocessing, feature extraction and classifier combination techniques. Our analysis shows that image normalization techniques that reduce the effects of different lighting conditions improve face/nonface classification significantly. In addition, techniques such as background masking and combining classifiers that use different feature vectors are shown to enhance classification performance. Over a test set of 12,000 patterns, the combined classifier using four feature vectors has correct detection rates (CDRs) of 96.2% and 99.2% at false detection rates (FDRs) of 1% and 5%, respectively.	Edith Cowan Univ, Perth, WA, Australia	Phung, SL (reprint author), Edith Cowan Univ, Perth, WA, Australia.						Duda R.O., 2001, PATTERN CLASSIFICATI; Hsu RL, 2002, IEEE T PATTERN ANAL, V24, P696; Liu CJ, 2003, IEEE T PATTERN ANAL, V25, P725; PHUNG SL, 2003, P IEEE INT S SIGN PR; Rowley HA, 1998, IEEE T PATTERN ANAL, V20, P23, DOI 10.1109/34.655647; Schneiderman H, 1998, PROC CVPR IEEE, P45, DOI 10.1109/CVPR.1998.698586; Sung KK, 1998, IEEE T PATTERN ANAL, V20, P39, DOI 10.1109/34.655648; Wu HY, 1999, IEEE T PATTERN ANAL, V21, P557; Yang MH, 2002, IEEE T PATTERN ANAL, V24, P34	9	0	0	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA	1522-4880		0-7803-8554-3	IEEE IMAGE PROC			2004							1385	1388				4	Computer Science, Software Engineering; Imaging Science & Photographic Technology	Computer Science; Imaging Science & Photographic Technology	BBV71	WOS:000228043501169	
B	Qiu, SB; Qiu, M		KhosrowPour, M		Qiu, SB; Qiu, M			Distributed data mining and its applications to intelligent textual information processing	Innovations Through Information Technology, Vols 1 and 2			English	Proceedings Paper	International Conference of the Information-Resources-Management-Association	MAY 23-26, 2004	New Orleans, LA	Informat Resources Management Assoc				Textual information processing is of fundamental importance, due to the massive amount of documents, especially online textual information that we need to process every day. In this paper, we stud), data mining techniques applied to intelligent textual information processing in distributed environments, including text classification. information extraction (IE) and topic detection and tracking (TDT). These intelligent processing techniques will improve the quality and efficiency of information resource management and utilization. Their statistical models and computational algorithms challenge the researches in data mining and distributed/parallel computing. When successfully applied, they will help enhance and benefit applications in IT, digital library, and information retrieval. Specifically, we study the distributed computing of the following algorithms: naive Bayes classifier combined with expectation-maximization (EM) for text classification, hidden Markov model for information extraction, and deterministic annealing with EM for topic detection and tracking. We also study the performances of the proposed algorithms and experiment on the improvements.	Univ New Mexico, Dept Elect & Comp Engn, Albuquerque, NM 87131 USA							Agrawal Rakesh, 1996, IEEE T KNOWLEDGE DAT; BAKER L, 1999, P 16 INT C MACH LEAR; Blei D., 2002, P 18 C UNC ART INT, P53; Fawcett T., 1999, P 5 ACM SIGKDD INT C, P53, DOI 10.1145/312129.312195; FORMAN G, 2000, KDD WORKSH DISTR PAR; FREITAG D, 1999, P AAAI 99 WORKSH MAC; JOSHI MV, 1998, P INT PAR PROC S; KOZLOV A, 1994, P 1994 C SUP WASH DC; KRUENGKRAI C, 2002, 8 ACM SIGKDD INT C K; McCallum A, 1998, P AAAI 98 WORKSH LEA, P41; McCallum A., 1998, P 15 INT C MACH LEAR, P350; MITCHELL T, 1998, P 1998 NAT C ART INT; Mitchell T.M., 1997, MACHINE LEARNING; Nigam K., 2000, MACHINE LEARNING; PATANE G, 2002, INFORM SCI, V43; RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626; Rose K, 1998, P IEEE, V86, P2210, DOI 10.1109/5.726788	17	0	0	IDEA GROUP PUBLISHING	HERSHEY	1331 E CHOCOLATE AVE, HERSHEY, PA 17033-1117 USA			1-59140-261-1				2004							366	370				5	Business; Computer Science, Information Systems; Computer Science, Interdisciplinary Applications	Business & Economics; Computer Science	BDM38	WOS:000234266200093	
S	Saito, K; Nakano, R			ieee	Saito, K; Nakano, R			Extracting characteristic words of text using neural networks	2004 IEEE INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS, VOLS 1-4, PROCEEDINGS	IEEE International Joint Conference on Neural Networks (IJCNN)		English	Proceedings Paper	IEEE International Joint Conference on Neural Networks (IJCNN)	JUL 25-29, 2004	Budapest, HUNGARY	IEEE, IEEE Neural Networks Soc, Hungarian Acad Sci, Comp & Automat Res Inst, Katholieke Univ Leuven, Republic Hungary, Natl Commun & Informat Council			ALGORITHM	In this paper, we consider models for estimating categories of documents and extracting characteristic words of such categories. To this end, we focus on three models, i.e., naive Bayes and two types of neural networks formalized as statistical models. Here suitable categories of documents are estimated based on posterior probabilities, and characteristic words are extracted based on the magnitude of resulting parameter values. In our experiments using a set of real Web pages, we compare these models in the aspect of categorization performances and extraction capabilities of characteristic words.	NTT Comm Sci Labs, Kyoto 6190237, Japan	Saito, K (reprint author), NTT Comm Sci Labs, 2-4 Hikaridai, Kyoto 6190237, Japan.						Bishop C.M., 1995, NEURAL NETWORKS PATT; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Dhillon IS, 2001, MACH LEARN, V42, P143, DOI 10.1023/A:1007612920971; DUDA RO, 2000, PATTERN CLASSIFICIAT; Luenberger D, 1984, LINEAR NONLINEAR PRO; Manning C.D., 1999, FDN STAT NATURAL LAN; NAKANO R, 2002, P DISC SCI LNAI, V2281, P482, DOI 10.1007/3-540-45884-0_36; NIGAM K, 1999, IJCAI 1999 WORKSH MA; Nigam K, 2000, MACH LEARN, V39, P103, DOI 10.1023/A:1007692713085; PORTER MF, 1980, PROGRAM-AUTOM LIBR, V14, P130, DOI 10.1108/eb046814; Saito K, 2002, NEURAL NETWORKS, V15, P1279, DOI 10.1016/S0893-6080(02)00089-8; Saito K, 2000, NEURAL COMPUT, V12, P709, DOI 10.1162/089976600300015763; Saito K, 1997, NEURAL COMPUT, V9, P123, DOI 10.1162/neco.1997.9.1.123; salton G., 1988, AUTOMATIC TEXT PROCE; Ueda N, 2002, P 8 ACM SIGKDD INT C, P626	15	0	0	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA	1098-7576		0-7803-8359-1	IEEE IJCNN			2004							1397	1402				6	Computer Science, Artificial Intelligence; Computer Science, Cybernetics	Computer Science	BBC97	WOS:000224941900243	
S	Sotoca, JM; Sanchez, JS; Pla, F		Kittler, J; Petrou, M; Nixon, M		Sotoca, JM; Sanchez, JS; Pla, F			Attribute relevance in multiclass data sets using the naive Bayes rule	PROCEEDINGS OF THE 17TH INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION, VOL 3	INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION		English	Proceedings Paper	17th International Conference on Pattern Recognition (ICPR)	AUG 23-26, 2004	Cambridge, ENGLAND	Int Assoc Pattern Recognit, Univ Surrey, UniS, IEEE Comp Soc, HP Res Labs Bristol	British Machine Vis Assoc			Feature selection using the naive Bayes rule is presented for the case of multiclass data sets. In this paper the EM algorithm is applied to each class projected over the features in order to obtain an estimation of the class probability density function. A matrix of weights per class and feature is then obtained, where it collects the level of relevance of each feature for the different classes. We show different ways to extract this information and compare the behavior of the ranking of relevance obtained applying the naive Bayes and K-NN classifiers.	Univ Jaume I, Dept Llenguatges & Sistemes Informat, E-12071 Castellon de La Plana, Spain	Sotoca, JM (reprint author), Univ Jaume I, Dept Llenguatges & Sistemes Informat, Av Sos Baynat S-N, E-12071 Castellon de La Plana, Spain.						Bressan M., 2002, Proceedings of Second IASTED International Conference Visualization, Imaging, and Image Processing; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1; Figueiredo MAT, 2002, IEEE T PATTERN ANAL, V24, P381, DOI 10.1109/34.990138; Kononenko I., 1994, P EUR C MACH LEARN, P171; Murphy P. M., 1995, UCI REPOSITORY MACHI; Novovicova J, 1996, IEEE T PATTERN ANAL, V18, P218, DOI 10.1109/34.481557	6	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA	1051-4651		0-7695-2128-2	INT C PATT RECOG			2004							426	429		10.1109/ICPR.2004.1334557		4	Computer Science, Artificial Intelligence	Computer Science	BAW25	WOS:000223879500104	
B	Su, J; Zhang, H		Rastogi, R; Morik, K; Bramer, M; Wu, X		Su, J; Zhang, H			Learning conditional independence tree for ranking	FOURTH IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS			English	Proceedings Paper	4th IEEE International Conference on Data Mining	NOV 01-04, 2004	Brighton, ENGLAND	IEEE Comp Soc, TCCI, IEEE Comp Soc, TCPAMI, IBM Res, StatSoft Ltd, Web Intelligence Consortium				Accurate ranking is desired in many real-world data mining applications. Traditional learning algorithms, however aim only at high classification accuracy. It has been observed that both traditional decision trees and naive Bayes produce good classification accuracy but poor probability estimates. In this paper we use a new model, conditional independence tree (CITree), which is a combination of decision tree and naive Bayes and more suitable for ranking and more learnable in practice. We propose a novel algorithm for learning CITree for ranking, and the experiments show that the CITree algorithm outperforms the state-of-the-art decision tree learning algorithm C4.4 and naive Bayes significantly in yielding accurate rankings. Our work provides an effective data mining algorithm for applications in which an accurate ranking is required.	Univ New Brunswick, Fac Comp Sci, Fredericton, NB E3B 5A3, Canada	Su, J (reprint author), Univ New Brunswick, Fac Comp Sci, POB 4400, Fredericton, NB E3B 5A3, Canada.						Hand DJ, 2001, MACH LEARN, V45, P171, DOI 10.1023/A:1010920819831; Kohavi R., 1996, P 2 INT C KNOWL DISC, P202; Ling C.X., 2003, P 20 INT C MACH LEAR, P480; Merz C., 1997, UCI REPOSITORY MACHI; Pazzani M., 1994, P 11 INT C MACH LEAR, P217; Provost F, 2003, MACH LEARN, V52, P199, DOI 10.1023/A:1024099825458; Provost F., 1997, Proceedings of the Third International Conference on Knowledge Discovery and Data Mining; Provost F., 1998, P 15 INT C MACH LEAR, P445; SWETS JA, 1988, SCIENCE, V240, P1285, DOI 10.1126/science.3287615; Witten I.H., 2000, DATA MINING PRACTICA; ZHANG H, 2004, IN PRESS P 15 EUR C	11	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA			0-7695-2142-8				2004							531	534				4	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Computer Science	BBI95	WOS:000225713000094	
S	Takeda, K; Tsuge, Y; Matsuyama, H		Negoita, MG; Howlett, RJ; Jain, LC		Takeda, K; Tsuge, Y; Matsuyama, H			Extraction operation know-how from historical operation data - Using characterization method of time series data and data mining method	KNOWLEDGE-BASED INTELLIGENT INFORMATION AND ENGINEERING SYSTEMS, PT 2, PROCEEDINGS	LECTURE NOTES IN COMPUTER SCIENCE		English	Article; Proceedings Paper	8th International Conference on Knowledge-Based Intelligent Information and Engineering Systems	SEP, 2004	Wellington, NEW ZEALAND	Royal Soc New Zealand, IPENZ, New Zealand Trade & Enterprise, Telecom, Allied Telesyn, Positively Wellington Business	Wellington Inst Technol			In these days, it is very difficult to hand down experts' operation know-how to beginner, because of operation technique of a large and highly complex plant and reducing operators. On the other hand, data mining methods (See5, naive bayes, k-nearest neighbor, and so on) has been proposed as knowledge discovering methods from a huge amount of data. See5 outputs decision trees or IF-THEN rules as data mining results. However, See5 cannot recognize data as time series. In this study an extraction method of experts' operation know-how from historical operation data is proposed. Furthermore efficiencies of the proposed method are demonstrated by numerical experiments using a dynamic simulator.	Kyushu Univ, Dept Chem Engn, Fukuoka 8128581, Japan; Waseda Univ, Grad Sch Informat Prod & Syst, Kitakyushu, Fukuoka 8080135, Japan	Takeda, K (reprint author), Kyushu Univ, Dept Chem Engn, Fukuoka 8128581, Japan.						BAKSHI BR, 1994, COMPUT CHEM ENG, V18, P267, DOI 10.1016/0098-1354(94)85028-3; Fayyad U. M., 1996, ADV KNOWLEDGE DISCOV, P1; Guvenir HA, 2000, KNOWL-BASED SYST, V13, P207; Inza I, 2000, ARTIF INTELL, V123, P157, DOI 10.1016/S0004-3702(00)00052-7; Mani S, 1999, ARTIF INTELL MED, V16, P51, DOI 10.1016/S0933-3657(98)00064-5; Quinlan J.R., 1993, C4 5 PROGRAMS MACHIN	6	0	0	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		3-540-23206-0	LECT NOTES COMPUT SC			2004	3214						375	381				7	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Computer Science	BBB76	WOS:000224585400048	
S	Teixeira, MA; Zaverucha, G		Pal, NR; Kasabov, N; Mudi, RK; Pal, S; Parui, SK		Teixeira, MA; Zaverucha, G			A partitioning method for fuzzy probabilistic predictors	NEURAL INFORMATION PROCESSING	LECTURE NOTES IN COMPUTER SCIENCE		English	Article; Proceedings Paper	11th International Conference on Neural Information Processing	NOV 22-25, 2004	Calcutta, INDIA	Indian Stat Inst, Jadavpur Univ			TUTORIAL	We present a new partitioning method to determinate fuzzy regions in fuzzy probabilistic predictors. Fuzzy probabilistic predictors are modifications of discrete probabilistic classifiers, as Naive Bayes Classifier and Hidden Markov Model, in order to enable them to predict continuous values. Two fuzzy probabilistic predictors, Fuzzy Markov Predictor and the Fuzzy Hidden Markov Predictor, are applied to the task of monthly electric load single-step forecasting using this new partitioning and successfully compared with two Kalman Filter Models, and two traditional forecasting methods, Box-Jenkins and Winters exponential smoothing. The employed time series present a sudden significant changing behavior at their last years, as it occurs in an energy rationing.	Fed Univ Rio Janeiro, Syst Engn & Comp Sci COPPE, BR-21745970 Rio De Janeiro, Brazil	Teixeira, MA (reprint author), Fed Univ Rio Janeiro, Syst Engn & Comp Sci COPPE, POB 68511, BR-21745970 Rio De Janeiro, Brazil.	mat@cos.ufrj.br; gerson@cos.ufrj.br	Zaverucha, Gerson /M-2290-2013				Box G. E. P., 1994, TIME SERIES ANAL; Franks MM, 2001, GERONTOLOGIST, V41, P5; FRIEDMAN N, 1998, 15 INT C MACH LEARN, P179; Ghahramani Z., 1998, LECT NOTES ARTIF INT, P168; Harvey A. C., 1994, FORECASTING STRUCTUR; Hjorth J. S. U., 1994, COMPUTER INTENSIVE S; MENDEL JM, 1995, P IEEE, V83, P345, DOI 10.1109/5.364485; Montgomery D., 1990, FORECASTING TIME SER; RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626; SCOTT DW, 1998, ENCY BIOSTATISTICS, P1134; SOUZA FJ, 2002, IFSA, V130, P189; TEIXEIRA MA, 2003, ICANN ICONIP, P374; Teixeira MA, 2002, J INTELL FUZZY SYST, V13, P155; TEIXEIRA MA, 2004, INT JOINT C NEUR NET, V1, P315; THRUN S, 1999, P 16 INT C MACH LEAR, P415; Torgo L, 1997, INTELLIGENT DATA ANA, V4, P275, DOI 10.1016/S1088-467X(97)00013-9; West M., 1997, BAYESIAN FORECASTING; ZADEH LA, 1968, J MATH ANAL APPL, V23, P421, DOI 10.1016/0022-247X(68)90078-4	18	0	0	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		3-540-23931-6	LECT NOTES COMPUT SC			2004	3316						929	934				6	Computer Science, Artificial Intelligence; Computer Science, Theory & Methods	Computer Science	BBK50	WOS:000225878300142	
S	Terra, GS; Curotto, CL; Ebecken, NFF		Zanasi, A; Ebecken, NFF; Brebbia, CA		Terra, GS; Curotto, CL; Ebecken, NFF			A data mining approach to support the development of new fuels and technology	DATA MINING V: DATA MINING, TEXT MINING AND THEIR BUSINESS APPLICATIONS	MANAGEMENT INFORMATION SYSTEMS		English	Proceedings Paper	5th International Conference on Data Mining	SEP 15-17, 2004	Malaga, SPAIN	Wessex Inst Technol				In the present work, data mining techniques are used to model the non trivial relationships between properties that characterize the fuels, engine technologies and car emissions. Using models to predict car emissions from fuel properties and technologies engines can improve their development process. To support models of relational data, using the Object Linking and Embedding Database for Data Mining technology, a Simple Naive Bayes Incremental classifier was implemented in Microsoft(R) SQL Server(TM), supporting numeric input attributes, multiple prediction attributes and incremental update of data. Computational experiments using real word data sets were made to evaluate the results obtained by this classifier.	UFRJ, COPPE, Dept Civil Engn, BR-68506 Rio De Janeiro, Brazil	Terra, GS (reprint author), UFRJ, COPPE, Dept Civil Engn, Cidade Univ,Ilha Fundao, BR-68506 Rio De Janeiro, Brazil.						BRADLEY PS, 1999, MSRTR9835; Breiman L., 1984, CLASSIFICATION REGRE; Ceci M, 2003, LECT NOTES ARTIF INT, V2838, P95; CHICKERING DM, 1994, MSRTR9409; COELHO PSS, 2002, P 3 INT C DAT MIN BO, P573; COSTA MCA, 1999, THESIS COPPE UFPR RI; CUROTTO CL, 2003, THESIS COPPE UFPR RI; Han J., 2001, DATA MINING CONCEPTS; KIM P, 2002, MICROSOFT PUBLIC SQL; *MICR CORP, 2002, OLE DB DAT MIN SAMPL; *MICR CORP, 2000, OLE DB DAT MIN SPEC; *MICR CORP, 2000, SQL SERV 2000; Netz A, 2001, PROC INT CONF DATA, P379, DOI 10.1109/ICDE.2001.914850; NETZ A, 2000, P 26 VLDB, P719	14	0	0	WIT PRESS	SOUTHAMPTON	ASHURST LODGE, SOUTHAMPTON SO40 7AA, ASHURST, ENGLAND	1470-6326		1-85312-729-9	MANAG INFORMAT SYST			2004	10						365	374				10	Computer Science, Information Systems; Computer Science, Interdisciplinary Applications	Computer Science	BBO45	WOS:000226691500035	
S	Wang, ZH; Webb, GI; Zheng, F		Dai, H; Srikant, R; Zhang, C		Wang, ZH; Webb, GI; Zheng, F			Selective augmented Bayesian network classifiers based on rough set theory	ADVANCES IN KNOWLEDGE DISCOVERY AND DATA MINING, PROCEEDINGS	LECTURE NOTES IN ARTIFICIAL INTELLIGENCE		English	Article; Proceedings Paper	8th Pacific/Asia Conference on Advances in Knowledge Discovery and Data Mining	MAY 26-28, 2004	Sydney, AUSTRALIA	NICIA, SAS, Univ Technol Sydney, Deakin Univ		naive Bayes; Bayesian network; machine learning		The naive Bayes classifier is widely used in interactive applications due to its computational efficiency, direct theoretical base, and competitive accuracy. However, its attribute independence assumption can result in sub-optimal accuracy. A number of techniques have explored simple relaxations of the attribute independence assumption in order to increase accuracy. TAN is a state-of-the-art extension of naive Bayes, that can express limited forms of inter-dependence among attributes. Rough sets theory provides tools for expressing inexact or partial dependencies within dataset. In this paper, we present a variant of TAN using rough sets theory and compare their tree classifier structures, which can be thought of as a selective restricted trees Bayesian classifier. It delivers lower error than both pre-existing TAN-based classifiers, with substantially less computation than is required by the SuperParent approach.	Beijing Jiaotong Univ, Sch Comp & Informat Technol, Beijing 100044, Peoples R China; Monash Univ, Sch Comp Sci & Software Engn, Clayton, Vic 3800, Australia	Wang, ZH (reprint author), Beijing Jiaotong Univ, Sch Comp & Informat Technol, Beijing 100044, Peoples R China.	zhhwang@center.njtu.edu.cn; webb@mail.csse.monash.edu.au; feizheng@mail.csse.monash.edu.au	Webb, Geoffrey/A-1347-2008				Domingos P., 1996, P 13 INT C MACH LEAR, P105; Fayyad U.M., 1993, P 13 INT JOINT C ART, P1022; Friedman N, 1997, MACH LEARN, V29, P131, DOI 10.1023/A:1007465528199; Friedman N, 1996, PROCEEDINGS OF THE THIRTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND THE EIGHTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE, VOLS 1 AND 2, P1277; Keogh E., 1999, P 7 INT WORKSH ART I, P225; Keogh E. J., 2002, International Journal on Artificial Intelligence Tools (Architectures, Languages, Algorithms), V11, DOI 10.1142/S0218213002001052; KOHAVI R, 1997, P ECML; Kohavi R., 1996, P 2 INT C KNOWL DISC, P202; Langley P., 1994, P 10 C UNC ART INT, P339; MINIEKA E, 1978, OPTIMIZATION ALGORIT, P26; Pawlak Z., 1991, ROUGH SETS THEORETIC; Sahami M., 1996, P 2 INT C KNOWL DISC, P335; SLEZAK D, 1998, P 1 INT C RSCTC, P52; WANG Z, 2002, P 2002 IEEE INT C DA, P490; Webb G I, 1998, P 11 AUSTR JOINT C A, P285; Webb G.I., 2002, P AUSTR DAT MIN WORK, P65; Witten I.H., 2000, DATA MINING PRACTICA; Zheng ZJ, 2000, MACH LEARN, V41, P53, DOI 10.1023/A:1007613203719	18	0	0	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		3-540-22064-X	LECT NOTES ARTIF INT			2004	3056						319	328				10	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Computer Science	BAF29	WOS:000221955100038	
S	Wu, WP; Lee, VCS; Tan, TY		Webb, GI; Yu, X		Wu, WP; Lee, VCS; Tan, TY			Contributions of domain knowledge and stacked generalization in AI-based classification models	AI 2004: ADVANCES IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS	LECTURE NOTES IN ARTIFICIAL INTELLIGENCE		English	Article; Proceedings Paper	17th Annual Australian Conference on Artificial Intelligence	DEC 04-06, 2004	Cairns, AUSTRALIA		Cent Queensland Univ		NEURAL NETWORKS	We exploit the,merits of C4.5 decision tree classifier with two stacking meta-learners: back-propagation multilayer perceptron neural network and naive-Bayes respectively. The performance of these two hybrid classification schemes have been empirically tested and compared with C4.5 decision tree using two US data sets (raw data set and new data set incorporated with domain knowledge) simultaneously to predict US bank failure. Significant improvements in prediction accuracy and training efficiency have been achieved in the schemes based on new data set. The empirical test results suggest that the proposed hybrid schemes perform marginally better in term of AUC criterion.	Monash Univ, Sch Business Syst, Clayton, Vic 3800, Australia; Monash Univ, Dept Accounting & Finance, Clayton, Vic 3800, Australia	Wu, WP (reprint author), Monash Univ, Sch Business Syst, Wellington Rd, Clayton, Vic 3800, Australia.	weiping.wu@infotech.monash.edu.au; vincent.lee@infotech.monash.edu.au; tingyean.tan@buseco.monash.edu.au					Bradley AP, 1997, PATTERN RECOGN, V30, P1145, DOI 10.1016/S0031-3203(96)00142-2; CHERKASSKY V, 1992, IEEE EXPERT, V7, P43, DOI 10.1109/64.163672; Fawcett Tom, 2004, ROC GRAPHS NOTES PRA; GEORGE HH, 1994, BANK MANAGEMENT TEXT; HIRSH H, 1994, P IEEE C AI APPL; John G. H., 1994, P 11 INT C MACH LEAR; Koller D., 1996, P 13 INT C MACH LEAR; LEDEZMA A, 2001, P 13 INT C IEEE EXPE, V7, P210; PIRAMUTHU S, 1994, DECIS SUPPORT SYST, V11, P509, DOI 10.1016/0167-9236(94)90022-1; RADCLIFFE NJ, 1995, COMPUTER SCI TODAY R; Witten I.H., 1999, DATA MINING PRACTICA; Zhou ZH, 2004, IEEE T KNOWL DATA EN, V16, P770	12	0	0	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		3-540-24059-4	LECT NOTES ARTIF INT			2004	3339						1049	1054				6	Computer Science, Artificial Intelligence	Computer Science	BBM32	WOS:000226133600100	
S	Xie, ZP; Hsu, W; Lee, ML		Khoshgoftaar, TM		Xie, ZP; Hsu, W; Lee, ML			Mode committee: A novel ensemble method by clustering and local learning	ICTAI 2004: 16TH IEEE INTERNATIONALCONFERENCE ON TOOLS WITH ARTIFICIAL INTELLIGENCE, PROCEEDINGS	PROCEEDINGS - INTERNATIONAL CONFERENCE ON TOOLS WITH ARTIFICIAL INTELLIGENCE		English	Proceedings Paper	16th IEEE International Conference on Tools with Artificial Intelligence	NOV 15-17, 2004	Boca Raton, FL	IEEE Comp Soc, Informat Technol Res Inst, Wright State Univ, Florida Atlantic Univ			CLASSIFICATION	Ensemble methods have proved effective to achieve higher accuracy. Some simple ensemble methods, such as Bagging, work well with unstable base algorithms, but fail with stable ones. The reason is that such methods achieve higher accuracy by reducing only the variance of the base algorithms. It does not touch the bias. Here, we propose a novel ensemble method, Mode Committee, intended to work for both stable and unstable base algorithms. It first derive a new algorithm, called mode competitor, from given base algorithm, with the help of k-modes clustering method and the local learning strategy. Randomness is injected into each mode competitor by the process of random seeding. The aim of deriving mode competitor is to reduce the bias with the possible increasing variance. Then, multiple mode competitors form a committee and vote on the decision of new example, with the aim to reduce the variance of mode competitors. Such an arithmetic framework has been materialized by two base algorithms, the unstable C4.5 and the stable naive Bayes. Extensive empirical results demonstrate this method's superiority, and further analysis by bias-variance decomposition reveals that it is due to the low-bias of mode competitors.	Fudan Univ, Dept CIT, Shanghai 200433, Peoples R China	Xie, ZP (reprint author), Fudan Univ, Dept CIT, Shanghai 200433, Peoples R China.						Bauer E, 1999, MACH LEARN, V36, P105, DOI 10.1023/A:1007515423169; Blake C. L., 1998, UCI REPOSITORY MACHI; Breiman L., 1996, 460 U CAL DEP STAT; Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.1023/A:1018054314350; COVER TM, 1967, IEEE T INFORM THEORY, V13, P21, DOI 10.1109/TIT.1967.1053964; Duda R., 1973, PATTERN CLASSIFICATI; Fayyad U.M., 1993, P 13 INT JOINT C ART, P1022; Ho TK, 1998, IEEE T PATTERN ANAL, V20, P832; Huang ZX, 1998, DATA MIN KNOWL DISC, V2, P283, DOI 10.1023/A:1009769707641; KOHAVI R, 1996, P 13 INT C MACH LEAR, P313; Maclin R., 1998, Proceedings Fifteenth National Conference on Artificial Intelligence (AAAI-98). Tenth Conference on Innovative Applications of Artificial Intelligence; Quinlan J.R., 1993, C4 5 PROGRAMS MACHIN; Webb GI, 2000, MACH LEARN, V40, P159, DOI 10.1023/A:1007659514849; Xie ZP, 2003, PROC INT C TOOLS ART, P522; Zheng Z., 1998, P 10 EUR C MACH LEAR, P196; Zhou ZH, 2002, KNOWL-BASED SYST, V15, P515, DOI 10.1016/S0950-7051(02)00038-2	16	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA	1082-3409		0-7695-2236-X	PROC INT C TOOLS ART			2004							628	633				6	Computer Science, Artificial Intelligence	Computer Science	BBI06	WOS:000225597000082	
S	Xie, ZP; Zhang, Q		Khoshgoftaar, TM		Xie, ZP; Zhang, Q			A study of selective neighborhood-based naive Bayes for efficient lazy learning	ICTAI 2004: 16TH IEEE INTERNATIONALCONFERENCE ON TOOLS WITH ARTIFICIAL INTELLIGENCE, PROCEEDINGS	PROCEEDINGS - INTERNATIONAL CONFERENCE ON TOOLS WITH ARTIFICIAL INTELLIGENCE		English	Proceedings Paper	16th IEEE International Conference on Tools with Artificial Intelligence	NOV 15-17, 2004	Boca Raton, FL	IEEE Comp Soc, Informat Technol Res Inst, Wright State Univ, Florida Atlantic Univ				This paper studies two accuracy estimation techniques, global accuracy estimation and local accuracy estimation, under the algorithmic framework of the selective neighborhood-based naive Bayes (SNNB) for lazy classification, resulting in two concrete learning algorithms of linear computational complexity, SNNB-G and SNNB-L. Extensive experiments show that SNNB-L is more accurate than naive Baye, C4.5, and SNNB-G.	Fudan Univ, Dept Comp & Informat Technol, Shanghai 200433, Peoples R China	Xie, ZP (reprint author), Fudan Univ, Dept Comp & Informat Technol, Shanghai 200433, Peoples R China.						Blake C. L., 1998, UCI REPOSITORY MACHI; Domingos P, 1997, MACH LEARN, V29, P103, DOI 10.1023/A:1007413511361; Fayyad U.M., 1993, P 13 INT JOINT C ART, P1022; Friedman N, 1996, PROCEEDINGS OF THE THIRTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND THE EIGHTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE, VOLS 1 AND 2, P1277; Kohavi R., 1996, P 2 INT C KNOWL DISC, P202; Pazzani M. J., 1996, Proceedings ofthe Conference, ISIS '96. Information, Statistics and Induction in Science; Quinlan J., 1993, C4 5 PROGR MACHINE L; XIE Z, 2002, LECT NOTES COMPUTER, V2336, P104; Zheng ZJ, 2000, MACH LEARN, V41, P53, DOI 10.1023/A:1007613203719	9	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA	1082-3409		0-7695-2236-X	PROC INT C TOOLS ART			2004							758	760				3	Computer Science, Artificial Intelligence	Computer Science	BBI06	WOS:000225597000103	
B	Yang, J; Xing, YK; Li, J; Wang, YY; Yang, LJ			ieee	Yang, J; Xing, YK; Li, J; Wang, YY; Yang, LJ			Belief network classifier for evaluation of DGA data of transformers	CONFERENCE RECORD OF THE 2004 IEEE INTERNATIONAL SYMPOSIUM ON ELECTRICAL INSULATION			English	Proceedings Paper	IEEE International Symposium on Electrical Insulation	SEP 19-22, 2004	Indianapolis, IN	IEEE Dielect & Elect Insulat Soc		belief network; DGA; transformer; bayes classifier	DIAGNOSIS	In this paper a method to improve the assessment capability of power transformers by using belief network classifier is proposed. Two different belief network classifiers, the Naive Bayes classifier and the Tree Augmented Naive Bayes classifier, are compared using utilities' DGA data analysis. Their respective advantages and shortcomings are also shown by the detailed comparison. More than hundreds of historical DGA data has been used to demonstrate the capability of the method. Classification results show that the two classifiers are suitable for interpretation of DGA data and for diagnosis of incipient faults in transformers.	Chongqing Univ, Coll Comp Sci, Key Lab High Voltage Enng & Elect New Technol, Minist Educ, Chongqing 400044, Peoples R China	Yang, J (reprint author), Chongqing Univ, Coll Comp Sci, Key Lab High Voltage Enng & Elect New Technol, Minist Educ, Chongqing 400044, Peoples R China.						DAGUM P, 1993, IEEE T PATTERN ANAL, V15, P246, DOI 10.1109/34.204906; DELEU J, 1988, P ANN INT C IEEE ENG, V3, P1335; Duda R., 1973, PATTERN CLASSIFICATI; Duval M., 1989, IEEE Electrical Insulation Magazine, V5, DOI 10.1109/57.44605; Gu Y., 1994, P 10 IEEE C ART INT, P305; Guardado JL, 2001, IEEE T POWER DELIVER, V16, P643, DOI 10.1109/61.956751; Islam SM, 2000, IEEE T DIELECT EL IN, V7, P177; LIAO RJ, 2001, P 2001 INT S EL INS, P809; Marefat MM, 1997, IEEE T SYST MAN CY A, V27, P705, DOI 10.1109/3468.634635; TANG YC, 2002, 2002 IEEE INT C MAN, V5, P6	10	0	0	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA			0-7803-8447-4				2004							78	80				3	Engineering, Electrical & Electronic	Engineering	BBB26	WOS:000224497200021	
S	Ye, YM; Ma, FY; Rong, HQ; Huang, JZ		Li, Q; Wang, G; Feng, L		Ye, YM; Ma, FY; Rong, HQ; Huang, JZ			Improved email classification through enriched feature space	ADVANCES IN WEB-AGE INFORMATION MANAGEMENT: PROCEEDINGS	LECTURE NOTES IN COMPUTER SCIENCE		English	Article; Proceedings Paper	5th International Conference on Web-Age Information Management	JUL 15-17, 2004	Dalian, PEOPLES R CHINA	NO Univ China, Dalian Univ Technol, Database Soc CCF				This paper presents a novel feature space enriching (FSE) technique to address the problem of sparse and noisy feature space in email classification. The (FSE) technique employs two semantic knowledge bases to enrich the original sparse feature space, which results in more semantic-richer features. From the enriched feature space, the classification algorithms can learn improved classifiers. Naive Bayes and support vector machine are selected as the classification algorithms. Experiments on an enterprise email dataset have shown that the FSE technique is effective for improving the email classification performance.	Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai 20030, Peoples R China; Univ Hong Kong, E Business Technol Inst, Hong Kong, Hong Kong, Peoples R China	Ye, YM (reprint author), Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai 20030, Peoples R China.	yym@sjtu.edu.cn; fyma@sjtu.edu.cn; hrong@eti.hku.hk; jhuang@eti.hku.hk					Baeza-Yates R., 1999, MODERN INFORMATION R; Brutlag J.D., 2000, P 17 INT C MACH LEAR; COHEN W, 1996, P 1996 AAAI SPRING S; CRAWFORD E, 2002, P 19 INT C MACH LEAR; DIAO Y, 2000, P PAKDD 2000; DONG Z, 1999, P INT C MACH TRANSL; GELBUKH A, 1999, LECT NOTES ARTIF INT, V1692, P130; LEWIS D, 1998, P ECML 98; MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748; PLATT J, 2000, ADV NEURAL INFORMATI; Provost J, 1999, NAIVE BAYES VS RULE; Rennie Jason D.M., 2000, P KDD 2000 TEXT MIN; SEGAL R, 2000, P 17 INT C MACH LEAR; SEGAL R, 1999, P 3 INT C AUT AG; Vapnik V., 1998, STAT LEARNING THEORY; Yang Y., 1997, P 14 INT C MACH LEAR	16	0	0	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		3-540-22418-1	LECT NOTES COMPUT SC			2004	3129						489	498				10	Computer Science, Information Systems; Computer Science, Theory & Methods	Computer Science	BAK53	WOS:000222635400049	
S	Ye, YM; Ma, FY; Rong, HQ; Huang, J		Meziane, F; Metais, E		Ye, YM; Ma, FY; Rong, HQ; Huang, J			Enhanced email classification based on feature space enriching	NATURAL LANGUAGE PROCESSING AND INFORMATION SYSTEMS	LECTURE NOTES IN COMPUTER SCIENCE		English	Article; Proceedings Paper	9th International Conference on the Applications of Natural Language to Information Systems	JUN 23-25, 2004	Salford, ENGLAND		Univ Salford	email classification; feature space enriching; semantic knowledge base		Email classification is challenging due to its sparse and noisy feature space. To address this problem, a novel feature space enriching (FSE) technique based on two semantic knowledge bases is proposed in this paper. The basic idea of FSE is to select the related semantic features that will increase the global information for learning algorithms from the semantic knowledge bases, and use them to enrich the original sparse feature space. The resulting feature space of FSE can provide semantic-richer features for classification algorithms to learn improved classifiers. Naive Bayes and support vector machine are selected as the classification algorithms. Experiments on a bilingual enterprise email dataset have shown that: (1) the FSE technique can improve the email classification accuracy, especially for the sparse classes, (2) the SVM classifier benefits more from FSE than the naive Bayes classifier, (3) with the support of domain knowledge, the FSE technique can be more effective.	Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai 20030, Peoples R China; Univ Hong Kong, E Business Technol Inst, Hong Kong, Hong Kong, Peoples R China	Ye, YM (reprint author), Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai 20030, Peoples R China.	yym@sjtu.edu.cn; fyma@sjtu.edu.cn; hrong@eti.hku.hk; jhuang@eti.hku.hk					BAEZAYATES R, 1999, RIBEIRO NETO MODERN; Brutlag J.D., 2000, P 17 INT C MACH LEAR; COHEN W, 1996, P 1996 AAAI SPRING S; CRAWFORD E, 2002, P 19 INT C MACH LEAR; DIAO Y, 2000, P PAKDD 2000; DONG Z, 1999, P INT C MACH TRANSL; GELBUKH A, 1999, LECT NOTES ARTIF INT, V1692, P130; MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748; Mitchell T. M., 1997, MCGRAW HILL SERIES C; PLATT J, 2000, ADV NEUROL INFORMATI; PROVOST J, 1999, NAIVE BAYES RULE LEA; Rennie Jason D.M., 2000, P KDD 2000 TEXT MIN; SEGAL R, 2000, P 17 INT C MACH LEAR; SEGAL R, 1999, P 3 INT C AUT AG; Vapnik V., 1998, STAT LEARNING THEORY; WHITTAKER S, 1996, P INT C HUM FACT COM; Yang Y., 1997, P 14 INT C MACH LEAR; ZELIKOVITZ S, 2000, P 17 INT C MACH LEAR	18	0	0	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		3-540-22564-1	LECT NOTES COMPUT SC			2004	3136						299	311				13	Computer Science, Artificial Intelligence; Computer Science, Information Systems; Computer Science, Theory & Methods	Computer Science	BAV30	WOS:000223761800026	
B	Zenebe, A; Gangopadhyay, A; Norcio, AF		KhosrowPour, M		Zenebe, A; Gangopadhyay, A; Norcio, AF			Neuro-fuzzy modeling for inferring a user's interest in a Web Page: An empirical study	Innovations Through Information Technology, Vols 1 and 2			English	Proceedings Paper	International Conference of the Information-Resources-Management-Association	MAY 23-26, 2004	New Orleans, LA	Informat Resources Management Assoc			SOFT COMPUTING FRAMEWORK; DOMAIN KNOWLEDGE	Inferring about a user's interest in a Web Page and its related pages can be considered as a part of learning or constructing a user model. Application of the standard machine learning techniques for user modeling seems appropriate and effective but they have drawbacks. The need for large labeled data, 'concept drift' concerns due to the dynamic nature of user's features. and computational complexity are the three major problems. Further, the standard machine learning techniques do not handle adequately the inherent incomplete, imprecise and uncertain knowledge about users. The objective of the study is for handling of uncertainty due to imprecision and vagueness in inferring a user's interest.. This study compares the effectiveness of the Neuro-fuzzy modeling approach to artificial neural network, decision tree induction, and the Naive Bayes. The results indicate that the Neuro-fuzzy modeling approach has potential and practical application in handling uncertainty and predicting a user interest in a Web Page and related pages.	Univ Maryland, Dept Informat Syst, Baltimore, MD 21250 USA							ABRAHAM A, 2001, 6 INT WORK C ART NAT; Chen QY, 2001, HUMAN COMPUTER INTERACTION: ISSUES AND CHALLENGES, P113; Chen QY, 1997, INT J HUM-COMPUT INT, V9, P25, DOI 10.1207/s15327590ijhc0901_2; CHIU CC, 1994, INFORM SCI-APPL, V1, P31, DOI 10.1016/1069-0115(94)90018-3; Claypool M, 2001, IEEE INTERNET COMPUT, V5, P32, DOI 10.1109/4236.968829; Lin C.-T., 1996, NEURAL FUZZY SYSTEMS; Mitra S, 2002, IEEE T NEURAL NETWOR, V13, P3, DOI 10.1109/72.977258; NAUCK D, 1997, 5 EUR C INT TECHN SO; NAUCK D, 1997, 7 INT FUZZ SYST ASS; NAUCK D, 1999, I KNOWLEDGE PROCESSI, P103; Pal SK, 2002, IEEE T NEURAL NETWOR, V13, P1163, DOI 10.1109/TNN.2002.1031947; Pedrycz W., 1998, INTRO FUZZY SETS; SBUJACE K, 2002, P 5 BIANN; Schafer J.B., 2001, J DATA MINING KNOWLE, V5, P115; STATHACOPOULOU R, 1999, P INNS IEEE INT JOIN; Webb GI, 2001, USER MODEL USER-ADAP, V11, P19, DOI 10.1023/A:1011117102175; Witten I.H., 2000, DATA MINING PRACTICA; ZADEH LA, 1965, INFORM CONTROL, V8, P338, DOI 10.1016/S0019-9958(65)90241-X	18	0	0	IDEA GROUP PUBLISHING	HERSHEY	1331 E CHOCOLATE AVE, HERSHEY, PA 17033-1117 USA			1-59140-261-1				2004							994	996				3	Business; Computer Science, Information Systems; Computer Science, Interdisciplinary Applications	Business & Economics; Computer Science	BDM38	WOS:000234266200255	
B	Zhang, J; Honavar, V		Rastogi, R; Morik, K; Bramer, M; Wu, X		Zhang, J; Honavar, V			AVT-NBL: An algorithm for learning compact and accurate Naive Bayes classifiers from attribute value taxonomies and data	FOURTH IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS			English	Proceedings Paper	4th IEEE International Conference on Data Mining	NOV 01-04, 2004	Brighton, ENGLAND	IEEE Comp Soc, TCCI, IEEE Comp Soc, TCPAMI, IBM Res, StatSoft Ltd, Web Intelligence Consortium				In many application domains, there is a need for learning algorithms that can effectively exploit attribute value taxonomies (AVT) - hierarchical groupings of attribute values - to learn compact, comprehensible, and accurate classifiers from data - including data that are partially specified. This paper describes AVT-NBL, a natural generalization of the Naive Bayes learner (NBL), for learning classifiers from AVT and data. Our experimental results show that AVT-NBL is able to generate classifiers that are substantially more compact and more accurate than those produced by NBL on a broad range of data sets with different percentages of partially specified values. We also show that AVT-NBL is more efficient in its use of training data: AVT-NBL produces classifiers that outperform those produced by NBL using substantially fewer training examples.	Iowa State Univ, Dept Comp Sci, Artificial Intelligence Res Lab, Ames, IA 50011 USA	Zhang, J (reprint author), Iowa State Univ, Dept Comp Sci, Artificial Intelligence Res Lab, Ames, IA 50011 USA.						ASHBURNER M, 2000, NAT GENET, V25; Berners-Lee T, 2001, SEMANTIC WEB SCI AM; CARAGCA D, 2004, INT J HYBRID INTELLI, V1; CARAGEA D, 2004, P 3 INT C ONT DAT AP; CHEN A, 1996, IEEE T KNOWLEDG DAT, V8; CLARE A, 2001, LECT NOTES COMPUTER, V2168; DESJARDINS M, 2000, LECT NOTES ARTIF INT, P1864; Friedman N., 1997, MACHINE LEARNING, V29; HAUSSLER D, 1988, ARTIF INTELL, P36; KANG D, 2004, IN PRESS P 4 IEEE IN; KOHAVI R, 1997, IMPROVING SIMPLE BAY; KOHAVI R, 2001, APPL DATA MINING ELE, V5; KOLLER D, 1997, P 14 INT C MACHINE L; LANGLEY P, 1992, P 10 NAT C ART INT; McCallum A., 1998, P 15 INT C MACH LEAR; MCCLEAN S, 2001, IEEE T KNOWLEDGE DAT; Mitchell T.M., 1997, MACHINE LEARNING; PAZZANI M, 1997, P 4 INT C KNOWL DISC; Pereira F., 1993, P 31 ANN M ASS COMP; RISSANEN J, 1978, AUTOMATICA, V14; SLONIM N, 2000, DOCUMENT CLUSTERING; UNDERCOFFER J, 2004, IN PRESS KNOWLEDGE E; YAMAZAKI T, 1995, P 12 INT C MACH LEAR; ZHANG J, 2002, P S ABSTR REF APPROX, P2371; Zhang J., 2003, P 20 INT C MACH LEAR	25	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA			0-7695-2142-8				2004							289	296				8	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Computer Science	BBI95	WOS:000225713000037	
S	Zhu, JB; Chen, WL; Yao, TS		Vicedo, JL; MartinezBarco, P; Munoz, R; Noeda, MS		Zhu, JB; Chen, WL; Yao, TS			Using seed words to learn to categorize Chinese text	ADVANCES IN NATURAL LANGUAGE PROCESSING	LECTURE NOTES IN ARTIFICIAL INTELLIGENCE		English	Article; Proceedings Paper	4th International Conference on Espana for Natural Language Processing (EsTAL)	OCT 20-22, 2004	Alicante, SPAIN	Univ Alicante				In this paper, we focus on text categorization model by unsupervised learning techniques that do not require labeled data. We propose a feature learning bootstrapping algorithm (FLB) using a small number of seed words, in that features for each of categories could be automatically learned from a large amount of unlabeled documents. Using these learned features we develop a new Naive Bayes classifier named NB_FLB. Experimental results show that the NB_FLB classifier performs better than other Naive Bayes classifiers by supervised learning in small number of features cases.	Northeastern Univ, Inst Comp Software & Theory, Nat Language Proc Lab, Shenyang 110004, Peoples R China	Zhu, JB (reprint author), Northeastern Univ, Inst Comp Software & Theory, Nat Language Proc Lab, Shenyang 110004, Peoples R China.	zhujingbo@mail.neu.edu.cn; chenwl@mail.neu.edu.cn; tsyao@mail.neu.edu.cn					Abney Steven, 2002, P 40 ANN M ASS COMP; Blum A, 1998, P WORKSH COMP LEARN; CARPENTER GA, 1991, NEURAL NETWORKS, V4, P565, DOI 10.1016/0893-6080(91)90012-T; CASTELLI V, 1996, IEEE T INFORMATI NOV; ITTNER DJ, 1995, S DOC AN INF RETR LA; Joachims T., 1998, Machine Learning: ECML-98. 10th European Conference on Machine Learning. Proceedings; Kohonen T, 2000, IEEE T NEURAL NETWOR, V11, P574, DOI 10.1109/72.846729; LEWIS D, 1994, COMP 2 LEARNING ALG; Lewis D.D., 1996, P 19 ANN INT ACM SIG, P298, DOI 10.1145/243199.243277; Li C., 2002, P 40 ANN M ASS COMP; McCallum A, 1998, AAAI 98 WORKSH LEARN; Nigam K, 1999, IJCAI 99 WORKSH MACH, P61; RAUBER A, 2000, J AUSTRIAN SOC ART I, V19, P17; Riloff E., 1999, P 16 NAT C ART INT; TAN AH, 1995, NEURAL NETWORKS, V8, P437, DOI 10.1016/0893-6080(94)00092-Z; Yang Y, 1997, 14 INT C MACH LEARN, P412; YAO TS, 2002, NATURAL LANGUAGE PRO; YOUNGJOONG K, 2002, AUTOMATIC TEXT CATEG	18	0	0	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		3-540-23498-5	LECT NOTES ARTIF INT			2004	3230						464	473				10	Computer Science, Artificial Intelligence	Computer Science	BBE05	WOS:000225095200041	
J	Klon, AE; Glick, M; Thoma, M; Acklin, P; Davies, JW				Klon, AE; Glick, M; Thoma, M; Acklin, P; Davies, JW			Enrichment of high-throughput docking results using Naive Bayes.	ABSTRACTS OF PAPERS OF THE AMERICAN CHEMICAL SOCIETY			English	Meeting Abstract	226th National Meeting of the American-Chemical-Society	SEP 07-11, 2003	NEW YORK, NY	Amer Chem Soc					Novartis Inst Biomed Res, Lead Discovery Ctr, Cambridge, MA 02142 USA; Novartis Inst Biomed Res, Informat & Knowledge Management, Cambridge, MA 02142 USA		anthony.klon@pharma.novartis.com						0	0	0	AMER CHEMICAL SOC	WASHINGTON	1155 16TH ST, NW, WASHINGTON, DC 20036 USA	0065-7727			ABSTR PAP AM CHEM S	Abstr. Pap. Am. Chem. Soc.	SEP	2003	226		1			146-COMP	U446	U446				1	Chemistry, Multidisciplinary	Chemistry	751JF	WOS:000187062402094	
S	Ceci, M; Appice, A; Malerba, D; Colonna, V		Cappelli, A; Turini, F		Ceci, M; Appice, A; Malerba, D; Colonna, V			Multi-relational structural Bayesian classifier	AI(ASTERISK)IA 2003: ADVANCES IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS	LECTURE NOTES IN ARTIFICIAL INTELLIGENCE		English	Article; Proceedings Paper	8th Congress of the Italian-Association-for-Artificial-Intelligence	SEP 23-26, 2003	PISA, ITALY	Italian Assoc Artificial Intelligence				In the traditional naive Bayes classification method, training data are represented as a single table (or database relation), where each row corresponds to an example and each column to a predictor variable or a target variable. In this paper we propose a multi-relational extension of the naive Bayes classification method that is characterized by three aspects: first, an integrated approach in the computation of the posterior probabilities for each class; second, the applicability to both discrete and continuous attributes, third, the consideration of knowledge on the data model embedded in the database schema during the generation of classification rules. The proposed method has been implemented in the new system Mr-SBC and tested on three benchmark tasks. Results on predictive accuracy favour our system for the most complex task. Mr-SBC also proved to be an efficient multi-relational data mining system with a tight dose integration to a relational DBMS.	Univ Bari, Dipartimento Informat, I-70126 Bari, Italy	Ceci, M (reprint author), Univ Bari, Dipartimento Informat, Via Orabona 4, I-70126 Bari, Italy.	ceci@di.uniba.it; appice@di.uniba.it; malerba@di.uniba.it; colonna_vin@yahoo.it	Malerba, Donato/H-3850-2012				Blockeel H., 1998, THESIS KATHOLIEKE U; DERAEDT L, 1998, P 8 INT C IND LOG PR, P128; Domingos P, 1997, MACH LEARN, V29, P103, DOI 10.1023/A:1007413511361; Dougherty J., 1995, P 12 INT C MACH LEAR, P194; FAYYAD UM, 1994, P 13 INT JOINT C ART, P1022; FLACH P, 2000, 1 ORD BAYES CLASS 1B; FLACH PA, 2000, P ECML2000 WORKSH DE, P33; FLACH PA, 2000, MACH LEARN, V42, P61; Friedman N., 1999, P 16 INT JOINT C ART, P1300; GETOOR L, 2002, P KDD 2002 WORKSH MU, P36; GETOOR L, 2001, P 1 WORKSH MULT DAT, P1300; HOLTE RC, 1993, MACH LEARN, V11, P63, DOI 10.1023/A:1022631118932; KROGEL MA, 2001, P 11 INT C IND LOG P, V2157; LEIVA HA, 2002, THESIS TU IOWA; Mitchell T.M., 1997, MACHINE LEARNING; MUGGLETON S, 1989, P 6 INT WORKSH MACH, P113; POMPE U, 1994, CISM LECT NOTES; Pompe Uros, 1995, P 5 INT WORKSH IND L, P417; SRINIVASAN A, 1999, ROLE BACKGROUND KNOW; Wrobel S., 2001, INDUCTIVE LOGIC PROG, P74	20	0	0	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		3-540-20119-X	LECT NOTES ARTIF INT			2003	2829						250	261				12	Computer Science, Artificial Intelligence; Robotics	Computer Science; Robotics	BY15K	WOS:000187957100021	
B	Chelba, C; Mahajan, M; Acero, A			IEEE; IEEE; IEEE	Chelba, C; Mahajan, M; Acero, A			Speech utterance classification	2003 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL I, PROCEEDINGS: SPEECH PROCESSING I			English	Proceedings Paper	IEEE International Conference on Acoustics, Speech, and Signal Processing	APR 06-10, 2003	HONG KONG, PEOPLES R CHINA	IEEE Signal Proc Soc				The paper presents a series of experiments on speech utterance classification performed on the ATIS corpus. We compare the performance of n-gram classifiers with that of Naive Bayes and maximum entropy classifiers. The n-gram classifiers have the advantage that one can use a single pass system (concurrent speech recognition and classification) whereas for Naive Bayes or maximum entropy classification we use a two-stage system: speech recognition followed by classification. Substantial relative improvements (up to 55%) in classification accuracy can be obtained using discriminative training methods that belong to the class of conditional maximum likelihood techniques.	Microsoft Corp, Res, Redmond, WA 98052 USA	Chelba, C (reprint author), Microsoft Corp, Res, 1 Microsoft Way, Redmond, WA 98052 USA.						Berger AL, 1996, COMPUT LINGUIST, V22, P39; Chen SF, 2000, IEEE T SPEECH AUDI P, V8, P37, DOI 10.1109/89.817452; Chu-Carroll J, 1999, COMPUT LINGUIST, V25, P361; Duda R.O., 2001, PATTERN CLASSIFICATI; GOPALAKRISHNAN PS, 1991, IEEE T INFORM THEORY, V37, P107, DOI 10.1109/18.61108; Gorin AL, 1997, SPEECH COMMUN, V23, P113, DOI 10.1016/S0167-6393(97)00040-X; Jelinek F., 1980, Pattern Recognition in Practice. Proceedings of an International Workshop; Manning Christopher D., 2001, FDN STAT NATURAL LAN; PALLET D, 1994, P HUM LANG TECHN WOR; YOUNG S, 1993, TR153 CAMBR U DEP EN	10	0	0	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA			0-7803-7663-3				2003							280	283				4	Acoustics; Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Acoustics; Computer Science; Engineering	BX44W	WOS:000185328700073	
B	Dhillon, IS; Guan, YQ		Wu, XD; Tuzhilin, A; Shavlik, J		Dhillon, IS; Guan, YQ			Information theoretic clustering of sparse co-occurrence data	THIRD IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS			English	Proceedings Paper	3rd IEEE International Conference on Data Mining	NOV 19-22, 2003	MELBOURNE, FL	IEEE Comp Soc TCCI, IEEE Comp Soc TCPAMI				A novel approach to clustering co-occurrence data poses it as an optimization problem in information theory which minimizes the resulting loss in mutual information. A divisive clustering algorithm that monotonically reduces this loss function was recently proposed. In this paper we show that sparse high-dimensional data presents special challenges which can result in the algorithm getting stuck at poor local minima. We propose two solutions to this problem: (a) a "prior" to overcome infinite relative entropy values as in the supervised Naive Bayes algorithm, and (b) local search to escape local minima. Finally, we combine these solutions to get a robust algorithm that is computationally efficient. We present experimental results to show that the proposed method is effective in clustering document collections and outperforms previous information-theoretic clustering approaches.	Univ Texas, Dept Comp Sci, Austin, TX 78712 USA	Dhillon, IS (reprint author), Univ Texas, Dept Comp Sci, Austin, TX 78712 USA.						Cover T. M., 1991, ELEMENTS INFORMATION; DHILLON IS, 2003, TR0339 U TEX DEPT CO; Dhillon I. S., 2003, Journal of Machine Learning Research, V3, DOI 10.1162/153244303322753661; DHILLON IS, 2002, P IEEE INT C DAT MIN; Duda R. O., 2000, PATTERN CLASSIFICATI; LANG K, 1995, P 12 INT C MACH LEAR, P331; Rose K, 1998, P IEEE, V86, P2210, DOI 10.1109/5.726788; Slonim N., 2000, ACM SIGIR, P208; SLONIM N, 2002, ACM SIGIR	9	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA			0-7695-1978-4				2003							517	520		10.1109/ICDM.2003.1250966		4	Computer Science, Artificial Intelligence	Computer Science	BY34Z	WOS:000188999400071	
S	Kim, HJ; Chang, JY		Zhong, N; Ras, ZW; Tsumoto, S; Suzuki, E		Kim, HJ; Chang, JY			Improving Naive Bayes text classifier with modified EM algorithm	FOUNDATIONS OF INTELLIGENT SYSTEMS	LECTURE NOTES IN ARTIFICIAL INTELLIGENCE		English	Article; Proceedings Paper	14th International Symposium on Methodologies for Intelligent Systems	OCT 28-31, 2003	MAEBASHI CITY, JAPAN	Maebashi Inst Technol, Japanese Soc Artificial Intelligence, Maebashi Convent Bur, Maebashi City Govt, Gunma Prefecture Govt, US AFOSR/AOARD, Web Intelligence Consortium, Gunma Informat Serv Ind Assoc, Ryomo Syst Co Ltd				This paper presents the method of significantly improving conventional Bayesian statistical text classifier by incorporating accelerated EM (Expectation Maximization) algorithm. EM algorithm experiences a slow convergence and performance degrade in its iterative process, especially when real textual documents do not follow EM's assumptions. We propose a new accelerated EM algorithm that is simple yet has a fast convergence speed and allow to estimate a more accurate classification model on Bayesian text classifier.	Seoul Natl Univ, Dept Elect & Comp Engn, Seoul, South Korea	Kim, HJ (reprint author), Seoul Natl Univ, Dept Elect & Comp Engn, Seoul, South Korea.	khj@uos.ac.kr; jychang@hansung.ac.kr					AGGRAWAL R, 2000, P 7 INT C EXT DAT TE, P365; CASTELLI V, 1995, PATTERN RECOGN LETT, V16, P105, DOI 10.1016/0167-8655(94)00074-D; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1; ENGELSON S, 1999, J ARTIFICIAL INTELLI, V11, P335; Lewis D. D., 1997, REUTERS 21578 TEXT C; Mitchell T., 1997, MACH LEARN, P154; Nigam K., 1998, Proceedings Fifteenth National Conference on Artificial Intelligence (AAAI-98). Tenth Conference on Innovative Applications of Artificial Intelligence; Yang Y., 1997, P 14 INT C MACH LEAR, P412	8	0	0	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		3-540-20256-0	LECT NOTES ARTIF INT			2003	2871						326	333				8	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Computer Science	BY15P	WOS:000187959000045	
S	Ko, SJ		Jeusfeld, MA; Pastor, O		Ko, SJ			Prediction of consumer preference through Bayesian classification and generating profile	CONCEPTUAL MODELING FOR NOVEL APPLICATION DOMAINS, PROCEEDINGS	LECTURE NOTES IN COMPUTER SCIENCE		English	Article; Proceedings Paper	22nd International Conference on Conceptual Modeling (ER 2003)	OCT 13-16, 2003	CHICAGO, ILLINOIS					Collaborative filtering system overlooks the fact that most consumers do not rate a preference; because of this oversight the consumer-product matrix shows great sparsity. A memory-based filtering system has storage problems and hence proves inefficient when applied on a large scale where tens of thousands of consumers and thousands of products are represented in the matrix. Clustering consumer into groups based on the web documents they have retrieved/fetched allows accurate recommendations of new web documents through solving the problem of sparsity. A variety of algorithms have previously been reported in the literature and their promising performance has been evaluated empirically. We identify the shortcomings of current algorithms for clustering consumer and propose the use of Naive Bayes classifier to classify consumer into group. To classify consumer into group, this paper uses the association word mining method with weighted word that reflects not only the preference rating of products but also information on them. The data expressed by the mined features are not expressed as a string of data, but as an association word vector. Then, collaborative consumer's profile is generated based on the extracted features. Naive Bayes classifier classifies consumer into group based on association words in collaborative consumer's profile. As a result, the dimension of the consumer-product matrix is decreased. We evaluate our method on database of consumer ratings for special computer study and show that it significantly outperforms previously proposed methods.	Univ Illinois, Dept Comp Sci, Urbana, IL 61801 USA	Ko, SJ (reprint author), Univ Illinois, Dept Comp Sci, 1304 W Springfield Ave, Urbana, IL 61801 USA.						Agrawal R., 1993, P 1993 ACM SIGMOD C; Agrawal R., 1994, P 20 VLDB C SANT CHI; Bansal N., 2002, Proceedings 43rd Annual IEEE Symposium on Foundations of Computer Science, DOI 10.1109/SFCS.2002.1181947; Basu C., 1998, Proceedings Fifteenth National Conference on Artificial Intelligence (AAAI-98). Tenth Conference on Innovative Applications of Artificial Intelligence; Billsus D., 1998, P INT C MACH LEARN; COHEN WW, 2000, COMPUTER NETWORKS IN, V33; DELGADO J, 1999, P INT J C ART INT IJ; *IN U, 1997, CONS FOC INT INF RET; JOHN S, 1998, P C UNC ART INT; Ko SJ, 2001, LECT NOTES COMPUT SC, V2113, P211; KO SJ, 2003, LNCS; MICHAEL T, 1997, MACHING LEARNING, P154; PAZZANI M, 1997, LEARNING REVISING US, P313; RIJSBERGEN V, 1979, INFORMATION RETRIEVA; SALTON G, 1983, INTRO MODERN INFORMA; SARWAR B, 1998, P 1998 C COMP SUPP C; UNGAR LH, 1998, AAAI WORKSH RECOMMEN	17	0	0	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		3-540-20257-9	LECT NOTES COMPUT SC			2003	2814						29	39				11	Computer Science, Artificial Intelligence; Computer Science, Information Systems; Computer Science, Theory & Methods	Computer Science	BY03U	WOS:000187325700004	
B	Kornienko, Y; Borisov, A			IEEE; IEEE	Kornienko, Y; Borisov, A			Investigation of a hybrid algorithm for decision tree generation	IDAACS'2003: PROCEEDINGS OF THE SECOND IEEE INTERNATIONAL WORKSHOP ON INTELLIGENT DATA ACQUISITION AND ADVANCED COMPUTING SYSTEMS: TECHNOLOGY AND APPLICATIONS	IEEE International Workshop on Intelligent Data Acquisition and Advanced Computing Systems-Technology and Applications-IDAACS		English	Proceedings Paper	2nd IEEE International Workshop on Intelligent Data Acquisition and Advanced Computing Systems	SEP 08-10, 2003	LVIV, UKRAINE	Ternopil Acad Natl Econ, Sci & Technol Ctr Ukraine, Minist Educ & Sci Ukraine, Post Pens Bank Aval, Inst Comp Informat Technologies, Inst Comp Technologies, Automat & Metrol, Natl Univ Lviv Polytechn		genetic algorithm; ID3; machine learning; "hill-climbing"; ensembles of classifiers		The paper describes experiments with Machine Learning. algorithms (ID3, C4.5, Bagged-C4.5, Boosted-C4.5 and Naive Bayes) and an algorithm made on the basis of a combination of genetic algorithms (GA) and ID3. To perform the experiments, the latter algorithm is implemented as an extension of the MLC++ Library of Stanford University. The behaviour of the algorithm is tested using 24 databases including the databases with a large number of attributes. It is shown that owing to "hill-climbing." problem solving, the characteristics of the classifier made with the help of the new algorithm became significantly better. The behaviour of the algorithm is examined when constructing pruned classifiers. The,ways to improve standard Machine Learning algorithms are suggested.	Riga Tech Univ, Inst Informat Technol, LV-1658 Riga, Latvia	Kornienko, Y (reprint author), Riga Tech Univ, Inst Informat Technol, 1 Kalku Str, LV-1658 Riga, Latvia.	j.kornienko@pf.lv; aborisov@egle.sc.rtu.lv					BREIMAN L, 1984, CLASSIFICATIONS REGR; KORNIENKO Y, 2000, 4 INT C APPL FUZZ SY, P287; MITCHELL TM, 1997, MACH LEARN, P405; QUINLAN JR, 1998, BAGGING BOOSTING C4; Quinlan J.R., 1993, C4 5 PROGRAMS MACHIN; WEISS SM, 1989, P 11 INT JOINT C ART, P57	6	0	0	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA			0-7803-8138-6	INT WORKSH INT DATA			2003							63	68				6	Computer Science, Artificial Intelligence; Computer Science, Theory & Methods; Engineering, Electrical & Electronic; Instruments & Instrumentation	Computer Science; Engineering; Instruments & Instrumentation	BX93T	WOS:000186891400014	
B	Nezhad, HRM; Barfourosh, AA		Callaos, N; Lesso, W; Rahimi, S; Boonjing, V; Mohamad, J; Liu, TK; Schewe, KD		Nezhad, HRM; Barfourosh, AA			Expanding reinforcement learning approaches for efficient crawling of the web	7TH WORLD MULTICONFERENCE ON SYSTEMICS, CYBERNETICS AND INFORMATICS, VOL IX, PROCEEDINGS: COMPUTER SCIENCE AND ENGINEERING: II			English	Proceedings Paper	7th World Multiconference on Systemics, Cybernetics and Informatics	JUL 27-30, 2003	ORLANDO, FL	Int Inst Informat & System		Web crawling; focused crawling; reinforcement learning; text classification		This paper introduces novel approaches for expanding the crawling methods of Web, which use reinforcement learning (RL). Previous researches (Cora crawlers) show that using reinforcement learning in crawlers improves their efficiency more than well-known focused crawling approaches and up to three times in compare to a breadth-first crawler, We have expanded the methods of calculating Q-Value in RL-based spider and introduced novel approaches for it. Our crawlers find the target pages faster and earn more rewards over the course of a crawl than Cora crawlers and Breadth-First crawlers. We have used Support Vector Machines (SVMs) classifier for the first time as a text learner in Web crawlers and compared the results with crawlers which use Naive Bayes (NB) classifier for this purpose. The results show that crawlers using SVMs Outperform crawlers which use NB in the first half of crawling a Web site and find the target pages more quickly. The test bed for evaluation of our approaches was Web sites of four computer science departments of four universities.	Amirkabir Univ Technol, Comp Eng & IT Fac, Tehran 15914, Iran	Nezhad, HRM (reprint author), Amirkabir Univ Technol, Comp Eng & IT Fac, Tehran 15914, Iran.						Aggarwal C. C., 2001, P 10 INT WORLD WID W; BARFOUROSH AA, 2002, CS4291 U MAR I ADV C; CHAKRABARTI S, 2002, B IEEE COMPUTER SOC; Chakrabarti S., 1999, P 8 INT WORLD WID WE; CHAKRABARTI S, 1999, P 25 VLDB C ED SCOTL; CHAKRABARTI S, 2002, 12 WORLD WID WEB C H; CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; DAVID DL, 1998, ECML 98; DeBra P., 1994, P 4 RIAO C NEW YORK, P481; Diligenti M., 2000, VLDB 2000, P527; EHRIG M, 2002, THESIS U KARLSRUHE T; Ji G., 2005, P INT C MACH LEARN I; JOACHIMES T, 1997, P IJCAI 97; Joachims T., 1998, P EUR C MACH LEARN E; KLEINBERG J, 1998, J ACM, V46; KLEINBERG JM, 1997, 10076 IBM RJ; MCCALLUM A, 1999, AAAI 99 SPRING S INT; MCCALLUM AK, 1999, INFORMATION RETRIEVA; Menczer F., 2001, P 24 ANN INT ACM SIG; MUKHERJEA S, 2000, P 9 INT WORLD WID WE; Najork M., 2001, P 10 INT WORLD WID W; RAGHAVAN S, 2000, CRAWLING HIDDEN WEB; RENNIE J, 1999, P INT C MACH LEARN I; Sutton R. S., 1998, REINFORCEMENT LEARNI; TORGO L, 1997, INTELLIGENT DATA ANA, V1; Vapnik Vladimir N, 1995, NATURE STAT LEARNING	26	0	0	INT INST INFORMATICS & SYSTEMICS	ORLANDO	14269 LORD BARCLAY DR, ORLANDO, FL 32837 USA			980-6560-01-9				2003							71	76				6	Computer Science, Artificial Intelligence; Computer Science, Software Engineering; Computer Science, Theory & Methods	Computer Science	BY48M	WOS:000189346900014	
S	Pernkopf, F; O'Leary, P		Tobin, KW; Meriaudeau, F		Pernkopf, F; O'Leary, P			A search-and-score structure learning algorithm for Bayesian network classifiers	SIXTH INTERNATIONAL CONFERENCE ON QUALITY CONTROL BY ARTIFICIAL VISION	PROCEEDINGS OF THE SOCIETY OF PHOTO-OPTICAL INSTRUMENTATION ENGINEERS (SPIE)		English	Proceedings Paper	6th International Conference on Quality Control by Artificial Vision	MAY 19-22, 2003	GATLINBURG, TN	UT Battelle, LLC/Oak Ridge Natl Lab, US DOE, Univ Tennessee, Univ Burgundy Le2i, IEEE Signal Proc Soc, SPIE, Soc Mfg Engineers, Machine Vis Assoc		Bayesian network classifiers; feature selection; floating search method	FEATURE-SELECTION	This paper presents a search-and-score approach for determining the network structure of Bayesian network classifiers. A selective unrestricted Bayesian network classifier is used which in combination with the search algorithm allows simultaneous feature selection and determination of the structure of the classifier. The introduced search algorithm enables conditional exclusions of previously added attributes and/or arcs from the network classifier. Hence, this algorithm is able to correct the network structure by removing attributes and/or arcs between the nodes if they become superfluous at a later stage of the search. Classification results of selective unrestricted Bayesian network classifiers axe compared to naive Bayes classifiers and tree augmented naive Bayes classifiers. Experiments on different data sets show that selective unrestricted Bayesian network classifiers achieve a better classification accuracy estimate in two domains compared to tree augmented naive Bayes classifiers, whereby in the remaining domains the performance is similar. However, the achieved network structure of selective unrestricted Bayesian network classifiers is simpler and computationally more efficient.	Graz Univ Technol, Inst Commun & Wave Propagat, A-8010 Graz, Austria	Pernkopf, F (reprint author), Graz Univ Technol, Inst Commun & Wave Propagat, A-8010 Graz, Austria.						Blum AL, 1997, ARTIF INTELL, V97, P245, DOI 10.1016/S0004-3702(97)00063-5; Dash M., 1997, INTELL DATA ANAL, V1, P131, DOI DOI 10.1016/S1088-467X(97)00008-5; Devijver P., 1982, PATTERN RECOGNITION; Duda R., 1973, PATTERN CLASSIFICATI; Fayyad U.M., 1993, P 13 INT JOINT C ART, P1022; Friedman N, 1997, MACH LEARN, V29, P131, DOI 10.1023/A:1007465528199; Friedman N, 1996, PROCEEDINGS OF THE THIRTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND THE EIGHTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE, VOLS 1 AND 2, P1277; Heckerman D., 1995, MSRTR9506; Jain A., 1982, HDB STAT, V2; Jain A, 1997, IEEE T PATTERN ANAL, V19, P153, DOI 10.1109/34.574797; JENSEN R, 1996, INTRO BAYESIAN NETWO; JOHNSONGENTILE K, 1994, J EDUC COMPUT RES, V11, P121; Keogh E., 1999, P 7 INT WORKSH ART I, P225; Kittler J., 1978, Pattern Recognition and Signal Processing; KOHAVI R, 1994, P AAAI FALL S REL, P122; KOHAVI R, 1994, PROC INT C TOOLS ART, P740, DOI 10.1109/TAI.1994.346412; Kohavi R, 1997, ARTIF INTELL, V97, P273, DOI 10.1016/S0004-3702(97)00043-X; Kononenko I., 1991, P 6 EUR WORK SESS LE, P206; Langley P., 1994, P 10 C UNC ART INT, P399; Langley P., 1992, P 10 NAT C ART INT, P223; Merz C., 1997, UCI REPOSITORY MACHI; MURPHY K, 2001, USE BAYES NET TOOLBO; Pearl J., 1988, PROBABILISTIC REASON; PUDIL P, 1994, PATTERN RECOGN LETT, V15, P1119, DOI 10.1016/0167-8655(94)90127-9; SINGH KM, 1996, INT C MACH LEARN, P453; ZONGKER D, 1996, INT C PATT REC ICPR, P18	26	0	0	SPIE-INT SOC OPTICAL ENGINEERING	BELLINGHAM	1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA	0277-786X		0-8194-4998-9	P SOC PHOTO-OPT INS			2003	5132						231	240		10.1117/12.515041		10	Computer Science, Artificial Intelligence; Imaging Science & Photographic Technology	Computer Science; Imaging Science & Photographic Technology	BX25H	WOS:000184740000026	
S	Razek, MA; Frasson, C; Kaltenbach, M		Abraham, A; Koppen, M; Franke, K		Razek, MA; Frasson, C; Kaltenbach, M			Dominant meanings classification model for web information	DESIGN AND APPLICATION OF HYBRID INTELLIGENT SYSTEMS	FRONTIERS IN ARTIFICIAL INTELLIGENCE AND APPLICATIONS		English	Proceedings Paper	3rd International Conference on Hybrid Intelligent Systems (HIS 03)	DEC 14-17, 2003	Melbourne, AUSTRALIA	IEEE, IEEE Syst, Man, & Cybernet Soc, World Federat Soft Comp, European Neural Network Soc, European Soc Fuzzy Log & Technol, Evoweb, Int Fuzzy Syst Assoc, Eunite, Australian Comp Soc, IEEE Victorian Sect, Knowledge Based Intelligent Engn Syst Ctr, IOS Press				The huge amount of information available on the Web can help in building domain knowledge of a Web-based tutoring system. Therefore, we are in need of a way to classify this information at a suitable place. To overcome this challenge, we develop a dominant meanings classification model. This model constructs domain knowledge as a hierarchy of concepts. Each concept consists of some dominant meanings, and each of those is linked with some chunks (information fragments) to define it. The dominant meanings are a set of keywords that best fit an indented meaning of a target word (concept). The more dominant meanings, the better a concept relates to its chunk context. We investigated the effect of using this model to extract features on classifying Web information. We compared the model's results with Naive Bayes classifiers. Our experiment showed that using this approach greatly improves the classification task.	Univ Montreal, Dept Informat & Rech Operat, Montreal, PQ H3C 3J7, Canada	Razek, MA (reprint author), Univ Montreal, Dept Informat & Rech Operat, CP 6128,Succ Ctr Ville Montreal, Montreal, PQ H3C 3J7, Canada.						CRAVEN M, 1998, P 15 NAT C ART INT A; De Bra P, 2002, COMMUN ACM, V45, P60; Dumais Susan, 2000, P 23 ANN INT ACM SIG, P256, DOI 10.1145/345508.345593; Friedman N, 1997, MACH LEARN, V29, P131, DOI 10.1023/A:1007465528199; Joachims T., 1997, ICML 97; Kalt T, 1996, IR78 U MASS CTR INT; Labrou Y, 1999, PROCEEDINGS OF THE EIGHTH INTERNATIONAL CONFERENCE ON INFORMATION KNOWLEDGE MANAGEMENT, CIKM'99, P180, DOI 10.1145/319950.319976; LARKEY LS, 1996, ACM SIGIR 96; LI H, 1997, P 35 ANN M ASS COMP; McCallum A, 1998, AAAI 98 WORKSH LEARN; MURRAY B, 2000, SIZING INTERNET WHIT; Nigam K, 2000, MACH LEARN, V39, P103, DOI 10.1023/A:1007692713085; RAZEK M, 2003, LECT NOTES ARTIF INT, P563; RAZEK M, 2003, 12 INT WORLD WID WEB; RAZEK M, 2002, INT C TECHN INF COMM; RAZEK MA, 2002, P INT C MACH LEARN A, P187; Sahami M., 1997, P 14 INT C MACH LEAR, P170; Sahami M., 1998, AAAI 98 WORKSH LEARN; SASAKI M, 1998, P IEEE INT C SYST MA, P2827; Sun A., 2001, P 2001 IEEE INT C DA, P521; TOM MM, 1997, MACHINE LEARING	21	0	0	I O S PRESS	AMSTERDAM	NIEUWE HEMWEG 6B, 1013 BG AMSTERDAM, NETHERLANDS	0922-6389		1-58603-394-8	FR ART INT			2003	104						1044	1053				10	Computer Science, Artificial Intelligence	Computer Science	BAC80	WOS:000221600700106	
S	Robles, V; Perez, MS; Herves, V; Pena, JM; Larranaga, P		Wyrzykowski, R; Dongarra, J; Paprzycki, M; Wasniewski, J		Robles, V; Perez, MS; Herves, V; Pena, JM; Larranaga, P			Parallel Stochastic search for protein secondary structure prediction	PARALLEL PROCESSING AND APPLIED MATHEMATICS	LECTURE NOTES IN COMPUTER SCIENCE		English	Article; Proceedings Paper	5th International Conference on Parallel Processing and Applied Mathematics	SEP 07-10, 2003	Czestochowa, POLAND	Intel Corp, IBM Corp, Optimus SA, Solidex SA				Prediction of the secondary structure of a protein from its aminoacid sequence remains an important and difficult task. Up to this moment, three generations of Protein Secondary Structure Algorithms have been defined: The first generation is based on statistical information over single aminoacids, the second generation is based on windows of aminoacids -typically 11-21 aminoacids - and the third generation is based on the usage of evolutionary information. In this paper we propose the usage of naive Bayes and Interval Estimation Naive Bayes (IENB) - a new semi naive Bayes approach - as suitable third generation methods for Protein Secondary Structure Prediction (PSSP). One of the main stages of IENB is based on a heuristic optimization, carried out by estimation of distribution algorithms (EDAs). EDAs are non-deterministic, stochastic and heuristic search strategies that belong to the evolutionary computation approaches. These algorithms under complex problems, like Protein Secondary Structure Prediction, require intensive calculation. This paper also introduces a parallel variant of IENB called PIENB (Parallel Interval Estimation Naive Bayes).	Tech Univ Madrid, Dept Comp Architecture & Technol, Madrid, Spain; Univ Basque Country, Dept Comp Sci & Artificial Intelligence, San Sebastian, Spain	Robles, V (reprint author), Tech Univ Madrid, Dept Comp Architecture & Technol, Madrid, Spain.		Larranaga, Pedro/F-9293-2013				Belding T.C., 1995, P 6 INT C GEN ALG, P114; Cantu-Paz E., 2001, EFFICIENT ACCURATE P; Cuff JA, 1999, PROTEINS, V34, P508, DOI 10.1002/(SICI)1097-0134(19990301)34:4<508::AID-PROT10>3.0.CO;2-4; Domingos P., 1996, P 13 INT C MACH LEAR, P105; Duda R., 1973, PATTERN CLASSIFICATI; FERREIRA JTA, 2001, WEIGHTED NAIVE BAYES; Hand DJ, 2001, INT STAT REV, V69, P385, DOI 10.2307/1403452; Holland J. H., 1973, SIAM Journal on Computing, V2, DOI 10.1137/0202009; Jones DT, 1999, J MOL BIOL, V292, P195, DOI 10.1006/jmbi.1999.3091; Kohavi R., 1996, P 2 INT C KNOWL DISC, P202; KONONENKO I, 1991, 6 EUR WORK SESS LEAR, P206; Langley P, 1994, INDUCTION SELECTIVE, P399; LANGLEY P, 1993, EUR C MACH LEARN, P153; Larranaga P., 2001, ESTIMATION DISTRIBUT; Levine D, 1994, THESIS ILLINOIS I TE; Message Passing Interface Forum, 1994, MPI MESS PASS INT ST; MICHAELSON G, 2000, P 12 INT WORKSH IMPL, P307; PAZZANI M, 1996, P 5 INT WORKSH ART I, P239; Pollastri G, 2002, PROTEINS, V47, P228, DOI 10.1002/prot.10082; PRZYBYLSKI D, 2001, UNPUB PROTEINS; PUNCH WF, 1998, GEN PROGR P 3 ANN C; ROBLES V, 2003, ADV WEB INTELLIGENCE, V2663, P46, DOI 10.1007/3-540-44831-4_6; ROST B, 1993, J MOL BIOL, V232, P584, DOI 10.1006/jmbi.1993.1413; Schmidler SC, 2000, J COMPUT BIOL, V7, P233, DOI 10.1089/10665270050081496; Webb G I, 1998, P 11 AUSTR JOINT C A, P285; Whitley D., 1997, Evolutionary Computing. AISB International Workshop. Selected Papers	26	0	0	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		3-540-21946-3	LECT NOTES COMPUT SC			2003	3019						1162	1169				8	Computer Science, Artificial Intelligence; Computer Science, Theory & Methods; Mathematics, Applied	Computer Science; Mathematics	BAC63	WOS:000221559200149	
B	Sanchis, A; Juan, A; Vidal, E			IEEE; IEEE; IEEE	Sanchis, A; Juan, A; Vidal, E			Improving utterance verification using a smoothed naive Bayes model	2003 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL I, PROCEEDINGS: SPEECH PROCESSING I			English	Proceedings Paper	IEEE International Conference on Acoustics, Speech, and Signal Processing	APR 06-10, 2003	HONG KONG, PEOPLES R CHINA	IEEE Signal Proc Soc				Utterance verification can be seen as a conventional pattern classification problem in which a feature vector is obtained for each hypothesized word in order to classify it as either correct or incorrect. It is unclear, however, which predictor (pattern) features and classification model should be used. Regarding the features, we have recently proposed a new feature, called Word Trellis Stability (WTS), that can be profitably used in conjunction with more or less standard features such as Acoustic Stability. This is confirmed in this paper, where a smoothed naive Bayes classification model is proposed to adequately combine predictor features. On a series of experiments with this classification model and several features, we have found that the results provided by each feature alone are outperformed by certain combinations. In particular, the combination of the two above-mentioned features has been consistently found to give the most accurate result in two verification tasks.	Univ Politecn Valencia, Inst Tecnol Informat, Dept Sistemas Informat & Computac, E-46071 Valencia, Spain	Sanchis, A (reprint author), Univ Politecn Valencia, Inst Tecnol Informat, Dept Sistemas Informat & Computac, E-46071 Valencia, Spain.	asanchis@iti.upv.es; ajuan@iti.upv.es; evidal@iti.upv.es					Chase Lin, 1997, THESIS CARNEGIE MELL; *I TECN INF FOND U, 2000, FIN REP; Kemp T., 1997, EUROSPEECH, P827; Amengual J. C., 2000, Machine Translation, V15, DOI 10.1023/A:1011116115948; Ney H, 1998, INT CONF ACOUST SPEE, P853, DOI 10.1109/ICASSP.1998.675399; Ney H, 1997, TEXT SPEECH LANG TEC, V2, P174; SANCHIS A, 2000, ICPR, V3, P278; SANCHIS A, UNPUB IBPRIA 2003; Schaaf T, 1997, INT CONF ACOUST SPEE, P875, DOI 10.1109/ICASSP.1997.596075; ZEPENFELD T, 1997, ICASSP, P1815	10	0	0	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA			0-7803-7663-3				2003							592	595				4	Acoustics; Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Acoustics; Computer Science; Engineering	BX44W	WOS:000185328700151	
S	Sauban, M; Pfahringer, B		Lavrac, N; Gamberger, D; Todorovski, L; Blockeel, H		Sauban, M; Pfahringer, B			Text categorisation using document profiling	KNOWLEDGE DISCOVERY IN DATABASES: PKDD 2003, PROCEEDINGS	LECTURE NOTES IN ARTIFICIAL INTELLIGENCE		English	Article; Proceedings Paper	7th European Conferenc on Priciples and Practice of Knowledge Discovery in Databases	SEP 22-26, 2003	CAVTAT, CROATIA	Croatian Minist Sci & Technol, Slovenian Minist Educ, Sci & Sports, Knowledge Discovery Network Excellence			RULES	This paper presents an extension of prior work by Michael D. Lee on psychologically plausible text categorisation. Our approach utilises Lee's model as a pre-processing filter to generate a dense representation for a given text document (a document profile) and passes that on to an arbitrary standard propositional learning algorithm. Similarly to standard feature selection for text classification, the dimensionality of instances is drastically reduced this way, which in turn greatly lowers the computational load for the subsequent learning algorithm. The filter itself is very fast as well, as it basically is just an interesting variant of Naive Bayes. We present different variations of the filter and conduct an evaluation against the Reuters-21578 collection that shows performance comparable to previously published results on that collection, but at a lower computational cost.	Univ Waikato, Dept Comp Sci, Hamilton, New Zealand	Sauban, M (reprint author), Univ Waikato, Dept Comp Sci, Hamilton, New Zealand.						AHA DW, 1991, MACH LEARN, V6, P37, DOI 10.1023/A:1022689900470; APTE C, 1994, ACM T INFORM SYST, V12, P233, DOI 10.1145/183422.183423; Dumais S., 1998, Proceedings of the 1998 ACM CIKM International Conference on Information and Knowledge Management, DOI 10.1145/288627.288651; Frank E., 2000, P DCC 00 IEEE DAT CO, P200; HOLTE RC, 1993, MACH LEARN, V11, P63, DOI 10.1023/A:1022631118932; Joachims T., 1998, P 10 EUR C MACH LEAR, P137; JOHN GH, 1995, P 11 C UNC ART INT, P338; LEE MD, 2002, P 14 AUSTR JOINT C A, P309; Lewis David D., 1998, P 10 EUR C MACH LEAR, P4; McCallum A, 1998, AAAI 98 WORKSH LEARN; Platt J. C., 1998, ADV KERNEL METHODS S; Quinlan R., 1993, C4 5 PROGRAMS MACHIN; Rennie J., 2003, P 20 INT C MACH LEAR; Rocchio J., 1971, RELEVANCE FEEDBACK I, P313; Roth D., 1998, Proceedings Fifteenth National Conference on Artificial Intelligence (AAAI-98). Tenth Conference on Innovative Applications of Artificial Intelligence; SCOTT S, 1998, USE WORDNET NATURAL, P38; Wiener E., 1995, P 4 ANN S DOC AN INF, P317; Witten Ian H., 1999, P ICONIP ANZIIS ANNE, P192; YANG Y, 1992, 14TH P INT C COMP LI, P447; Yang Y., 1997, P 14 INT C MACH LEAR, P412; Yang YM, 2002, J INTELL INF SYST, V18, P219, DOI 10.1023/A:1013685612819	21	0	0	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		3-540-20085-1	LECT NOTES ARTIF INT			2003	2838						411	422				12	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Computer Science	BX96Y	WOS:000187062000037	
S	Serban, G; Tatar, D		Klopotek, MA; Wierzchon, ST; Trojanowski, K		Serban, G; Tatar, D			An improved algorithm on word sense disambiguation	INTELLIGENT INFORMATION PROCESSING AND WEB MINING	ADVANCES IN SOFT COMPUTING		English	Proceedings Paper	International Intelligent Information Systems/Intelligent Information Processing and Web Mining Conference (IIS: IIPWM 03)	JUN 02-05, 2003	ZAKOPANE, POLAND	Polish Acad Sci, Inst Comp Sci		word sense disambiguation; corpus; agents; learning		The task of disambiguation is to determine which of the senses of an ambiguous word is invoked in a particular use of the word [4]. Starting from the algorithm of Yarowsky [6,5,9,10] and the Naive Bayes Classifier (NBC) algorithm, in this paper we propose an original two-steps algorithm which combines their elements. This algorithm preserves the advantage of principles of Yarowsky (one sense per discourse and one sense per collocation) with the known high performance of NBC algorithms. We design an Intelligent Agent, who learns (based on the algorithm mentioned above) to find the correct sense for an ambiguous word in some given contexts.	Univ Babes Bolyai, Dept Comp Sci, Cluj Napoca, Romania	Serban, G (reprint author), Univ Babes Bolyai, Dept Comp Sci, 1 M Kogalniceanu St, Cluj Napoca, Romania.						ALLEN J, 1995, NATURAL LANGUAGE UND; Blum A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, DOI 10.1145/279943.279962; Jurafsky D, 2000, SPEECH LANGUAGE PROC; KILGARRIFF A, GOLD STANDARD DATASE; Manning C.D., 1999, FDN STAT NATURAL LAN; RESNIK P, 1998, NATURAL LANGUAGE ENG, V1; SEBASTIANI F, 2001, TUTORIAL AUTOMATED T, P1; SERBAN G, 2001, INFORMATICA, V2, P99; TATAR D, 2001, INTELIGENTA ARTIFICI; Yarowsky D., 1995, P 33 ANN M ASS COMP, P189, DOI 10.3115/981658.981684; YAROWSKY D, 1999, HIERARCHICAL DECISIO; YAROWSKY D, WORDSENSE DISAMBIGUA	12	0	0	PHYSICA-VERLAG GMBH & CO	HEIDELBERG	TIERGARTENSTR 17, D-69121 HEIDELBERG, GERMANY	1615-3871		3-540-00843-8	ADV SOFT COMP			2003							199	208				10	Computer Science, Artificial Intelligence	Computer Science	BX13H	WOS:000184365500021	
B	Shi, H; Huang, HK; Wang, ZH			IEEE	Shi, H; Huang, HK; Wang, ZH			The stability of a restricted Bayesian network: An empirical investigation	2003 INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND CYBERNETICS, VOLS 1-5, PROCEEDINGS			English	Proceedings Paper	International Conference on Machine Learning and Cybernetics	NOV 02-05, 2003	Xian, PEOPLES R CHINA	IEEE Syst, Man & Cybernet Tech Comm Cybernet, Hebei Univ, NW Polytech Univ		Bayesian network; TAN; stability; variance		The stability is an important criterion of evaluating classification algorithms. Bayesian network classifier is one of the most popular classification methods, however, its stability is rarely studied. Tree Augmented Naive Bayes(TAN), and a restricted Bayesian network, have demonstrated stronger whole performance than the other Bayesian classification methods. The purpose of this paper is to study the stability of TAN. Bayesian network classification method and TAN model are firstly introduced, and then an empirical investigation comparing the stability of several typical classification approaches (decision tree, naive Bayes) with TAN are detailedly described. Experimental results show that Tree Augmented Naive Bayes network classifier is stable.	No Jiaotong Univ, Sch Comp & Informat Technol, Beijing 100044, Peoples R China	Shi, H (reprint author), No Jiaotong Univ, Sch Comp & Informat Technol, Beijing 100044, Peoples R China.						Bauer E, 1999, MACH LEARN, V36, P105, DOI 10.1023/A:1007515423169; Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.1023/A:1018054314350; CHENG J, 1997, P 6 ACM INT C INF KN; COHEN J, 1960, EDUC PSYCHOL MEAS, V20, P37, DOI 10.1177/001316446002000104; Friedman N, 1997, MACH LEARN, V29, P131, DOI 10.1023/A:1007465528199; Kohavi R., 1996, Machine Learning. Proceedings of the Thirteenth International Conference (ICML '96); Last M, 2002, INT J PATTERN RECOGN, V16, P145, DOI 10.1142/S0218001402001599; Margineantu D., 1997, MACH LEARN, P211; Quinlan JR, 1996, PROCEEDINGS OF THE THIRTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND THE EIGHTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE, VOLS 1 AND 2, P725; TURNEY P, 1995, MACH LEARN, V20, P23, DOI 10.1007/BF00993473; Witten I.H., 2000, DATA MINING PRACTICA	11	0	0	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA			0-7803-7865-2				2003							345	349				5	Computer Science, Artificial Intelligence; Computer Science, Cybernetics	Computer Science	BY61C	WOS:000189420700069	
S	Shi, HB; Wang, ZH; Webb, GI; Huang, HK		Whang, KY; Jeon, J; Shim, K; Srivastava, J		Shi, HB; Wang, ZH; Webb, GI; Huang, HK			A new restricted Bayesian network classifier	ADVANCES IN KNOWLEDGE DISCOVERY AND DATA MINING	LECTURE NOTES IN ARTIFICIAL INTELLIGENCE		English	Article; Proceedings Paper	7th Pacific-Asia Conference on Knowledge Discovery and Data Mining	APR 30-MAY 02, 2003	SEOUL, SOUTH KOREA	Adv Informat Technol Res Ctr, KAIST, Stat Res Ctr Complex Syst, SNU, Korea Informat Sci Soc, Korean Datamining Soc, USAF, Off Sci Res, Asian Off Aerosp Res & Dev, ACM SIGKDD		naive Bayes; Bayesian network; classification		On the basis of examining the existing restricted Bayesian network classifiers, a new Bayes-theorem-based and more strictly restricted Bayesian-network-based classification model DLBAN is proposed, which can be viewed as a double-level Bayesian network augmented naive Bayes classification. The experimental results show that the DLBAN classifier is better than the TAN classifier in the most cases.	No Jiaotong Univ, Sch Comp & Informat Technol, Beijing 100044, Peoples R China; Monash Univ, Sch Comp Sci & Software Engn, Clayton, Vic 3800, Australia	Shi, HB (reprint author), No Jiaotong Univ, Sch Comp & Informat Technol, Beijing 100044, Peoples R China.		Webb, Geoffrey/A-1347-2008				Cheng J., 1999, P 15 C UNC ART INT U, P101; CHENG J, 1997, P 6 ACM INT C INF KN; CHICKERING DM, 1994, TR9417 MICR CORP MIC; Friedman N, 1997, MACH LEARN, V29, P131, DOI 10.1023/A:1007465528199; Keogh E., 1999, P 7 INT WORKSH ART I, P225; Kononenko I., 1991, P 6 EUR WORK SESS LE, P206; PEARL J, 1988, PROB REASD INT SYST; Witten I.H., 2000, DATA MINING PRACTICA; ZHENG Z, 2000, LAZY LEARNING BAYESI, P1	9	0	0	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		3-540-04760-3	LECT NOTES ARTIF INT			2003	2637						265	270				6	Computer Science, Artificial Intelligence; Computer Science, Information Systems; Computer Science, Software Engineering	Computer Science	BX23S	WOS:000184716000026	
B	Stauffer, C			IEEE COMPUTER SOCIETY; IEEE COMPUTER SOCIETY	Stauffer, C			Minimally-supervised classification using multiple observation sets	NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS			English	Proceedings Paper	9th IEEE International Conference on Computer Vision	OCT 13-16, 2003	NICE, FRANCE	IEEE Comp Soc, TC Pattern Anal & Machine Intelligence				This paper discusses building complex classifiers from a single labeled example and vast number of unlabeled observation sets, each derived from observation of a single process or object. When data can be measured by observation, it is often plentiful and it is often possible to make more than one observation of the state of a process or object. This paper discusses how to exploit the variability across such sets of observations of the same object to estimate class labels for unlabeled examples given a minimal number of labeled examples. In contrast to similar semi-supervised classification procedures that define the likelihood that two observations share a label as a function of the embedded distance between the two observations, this method uses the Naive Bayes estimate of how often the two observations did result from the same observed process. Exploiting this additional source of information in an iterative estimation procedure can generalize complex classification models from single labeled observations. Some examples involving classification of tracked objects in a low-dimensional feature space given thousands of unlabeled observation sets are used to illustrate the effectiveness of this method.	MIT, Artificial Intelligence Lab, Cambridge, MA 02139 USA	Stauffer, C (reprint author), MIT, Artificial Intelligence Lab, Cambridge, MA 02139 USA.						Hofmann T., 1999, P 15 C UNC ART INT U; Lee DD, 1999, NATURE, V401, P788; MARINA M, 2000, ADV NEURAL INFORMATI, V13, P873; STAUFFER C, 1999, COMPUTER VISION PATT, P333; SZUMMER M, 2001, ADV NEURAL INFORMATI, V14; TISHBY N, 2000, ADV NEURAL INFORMATI, V13	6	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA			0-7695-1950-4				2003							297	304				8	Computer Science, Artificial Intelligence; Imaging Science & Photographic Technology	Computer Science; Imaging Science & Photographic Technology	BX92L	WOS:000186833000041	
B	Tsay, JJ; Chang, CF; Chen, HY; Lin, CH		Wu, XD; Tuzhilin, A; Shavlik, J		Tsay, JJ; Chang, CF; Chen, HY; Lin, CH			Enhancing techniques for efficient topic hierarchy integration	THIRD IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS			English	Proceedings Paper	3rd IEEE International Conference on Data Mining	NOV 19-22, 2003	MELBOURNE, FL	IEEE Comp Soc TCCI, IEEE Comp Soc TCPAMI				In this paper we stud), the problem of integrating documents from different sources into a comprehensive topic hierarchy. Our objective is to develop efficient techniques that improve the accuracy of traditional categorization methods by, incorporating categorization information provided by data sources into categorization process. Notice that in the World-Wide Web, categorization information is often available from information sources. We present several enhancing techniques that use categorization information to enhance traditional methods such as naive Bayes and support vector machines. Experiment on collections from Openfind and Yam, and Google and Yahoo!, well-known popular web sites in Taiwan and USA, respectively, shows that our techniques significantly improve the classification accuracy from, for example, 55% to 66% for Naive Bayes, and from 57% to 67% for SVM for the data set collected from Yam and Openfind.	Natl Chung Cheng Univ, Dept Comp Sci & Informat Engn, Chiayi 62107, Taiwan	Tsay, JJ (reprint author), Natl Chung Cheng Univ, Dept Comp Sci & Informat Engn, Chiayi 62107, Taiwan.						AGRAWAL R, 2001, INT WORLD WID WEB C; APTE C, 1998, P AUT LEARN DISC; BOYAPATI V, 2000, COMPREHENSIVE TOPIC; CHAKRABARTI S, VLDB 98; JOACHIMS T, ECML 98; LEWIS D, SIGIR 96; MCCALLUM A, ICML 98; Mitchell T.M., 1997, MACHINE LEARNING; NG HT, SIGIR 97; Schapire RE, 2000, MACH LEARN, V39, P135, DOI 10.1023/A:1007649029923; SUSAN D, SIGIR 00; TSAY JJ, 2001, J COMPUTATIONAL LING, V5, P43; TSAY JJ, 2001, NATL CHUNG CHENG U; WANG K, VLDB 99; Yang Y., 1999, J INFORMATION RETRIE, V1, P67	15	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA			0-7695-1978-4				2003							657	660		10.1109/ICDM.2003.1251001		4	Computer Science, Artificial Intelligence	Computer Science	BY34Z	WOS:000188999400106	
S	Williams, K; Calvo, RA; Bell, D		Hoppe, HU; Verdejo, F; Kay, J		Williams, K; Calvo, RA; Bell, D			Automatic categorization of questions for a mathematics education service	ARTIFICIAL INTELLIGENCE IN EDUCATION: SHAPING THE FUTURE OF LEARNING THROUGH INTELLIGENT TECHNOLOGIES	FRONTIERS IN ARTIFICIAL INTELLIGENCE AND APPLICATIONS		English	Proceedings Paper	11th International Conference on Artificial Intelligence in Education (AI-ED 2003)	JUL 20-24, 2003	Sydney, AUSTRALIA	Int Artificial Intelligence Educ Soc, Univ Sydney, Sch Informat Technologies, Smart Internet Technologies				This paper describes a new approach to managing a stream of questions about mathematics by integrating a text categorization framework into a relational database management system. The corpus studied is based on unstructured submissions to an ask-an-expert service in learning mathematics. The classification system has been tested using a Naive Bayes learner built into the framework. The performance results of the classifier are also discussed. The framework was integrated into a PostgreSQL database through the use of procedural trigger functions.	Univ Sydney, Web Engn Grp, Sydney, NSW 2006, Australia							BLAHA MR, 1988, COMMUN ACM, V31, P414, DOI 10.1145/42404.42407; CALVO RA, 2000, INTELLIGENT DATA ANA, V4; CALVO RA, 2001, 6 AUSTR DOC S DEC, P6; Fayad M., 1999, BUILDING APPL FRAMEW; Fayad ME, 1997, COMMUN ACM, V40, P32, DOI 10.1145/262793.262798; Rumbaugh J., 1991, OBJECT ORIENTED MODE; SALTON G, 1991, SCIENCE, V253, P974, DOI 10.1126/science.253.5023.974; Salton Gerard, 1989, AUTOMATIC TEXT PROCE; Sebastiani F, 2002, ACM COMPUT SURV, V34, P1, DOI 10.1145/505282.505283; WILLIAMS K, 2002, 7 AUSTR DOC COMP S; Yang Y., 1999, 22 ANN INT ACM SIGIR, P42	11	0	0	I O S PRESS	AMSTERDAM	NIEUWE HEMWEG 6B, 1013 BG AMSTERDAM, NETHERLANDS	0922-6389		1-58603-356-5	FR ART INT			2003	97						81	87				7	Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Education & Educational Research	Computer Science; Education & Educational Research	BY88W	WOS:000189487400010	
B	Wu, WL; Lu, RZ; Liu, Z		Zong, CQ		Wu, WL; Lu, RZ; Liu, Z			Comparative experiments on task classification for spoken language understanding using naive Bayes classifier	2003 INTERNATIONAL CONFERENCE ON NATURAL LANGUAGE PROCESSING AND KNOWLEDGE ENGINEERING, PROCEEDINGS			English	Proceedings Paper	International Conference on Natural Language Processing and Knowledge Engineering	OCT 26-29, 2003	BEIJING, PEOPLES R CHINA	Chinese Assoc Artificial Intelligence, Chinese Inst Elect, Chinese Informat Proc Soc China, Beijing Network & Multimedia Lab, IEEE Signal Proc Soc, IEEE Syst, Man & Cybernet Soc, IEEE Beijing Sect		task classification; Naive Bayes classifier; comparative study		This paper presents a series of comparative experiments on using statistical classifiers for task classification. Our experiments focus on three aspects: the effect of different features on the performance of the classifier, the robustness of classifiers with different features on data variability and the effect of size of training data on the performance of the classifier. For Chinese input sentences, three linguistics units can be used as the features: Chinese characters, Chinese words and semantic constituents. Both advantages and disadvantages of them are analyzed in details. A controlled study using Naive Bayes classifiers is conducted to examine the impact of different features on the performance of classifiers. The classifiers with different features are evaluated respectively on the clean and noisy test data to investigate their robustness. Learning curves of the classifiers with different features are given to show the effect of size of training data.	Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai 200030, Peoples R China	Wu, WL (reprint author), Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai 200030, Peoples R China.						CETTOLO M, 1996, P ICSLP PHIL US; MITCHELL TM, 1996, MACH LEARN, P177; NATARAJAN P, NATURAL LANGUAGE CAL; WANG YY, 2002, P ICSLP DENV COL SEP; Yang Y., 1999, J INFORMATION RETRIE, V1, P67	5	0	0	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA			0-7803-7902-0				2003							492	497				6	Computer Science, Artificial Intelligence	Computer Science	BY45P	WOS:000189300200082	
S	Yang, J; Huang, JXX; Zhang, N; Xu, ZQ		Gedeon, TD; Fung, LCC		Yang, J; Huang, JXX; Zhang, N; Xu, ZQ			C3: A new learning scheme to improve classification of rare category emails	AI 2003: ADVANCES IN ARTIFICIAL INTELLIGENCE	LECTURE NOTES IN ARTIFICIAL INTELLIGENCE		English	Article; Proceedings Paper	16th Australian Conference on Artificial Intelligence	DEC 03-05, 2003	PERTH, AUSTRALIA	Natl Comm Artificial Intellignece & Expert Syst	UNIV WESTERN AUSTRALIA			This paper(1) proposes C3, a new learning scheme to improve classification performance of rare category emails in the early stage of incremental learning. C3 consists of three components: the chief-learner, the co-learners and the combiner. The chief-learner is an ordinary learning model with an incremental learning capability. The chief-learner performs well on categories trained with sufficient samples but badly on rare categories trained with insufficient samples. The co-learners that are focused on the rare categories are used to compensate for the weakness of the chief-learner in classifying new samples of the rare categories. The combiner combines the outputs of both the chief-learner and the co-learner to make a finial classification. The chief-learner is updated incrementally with all the new samples overtime and the co-learners axe updated with new samples from rare categories only. After the chief-learner has gained sufficient knowledge about the rare categories, the co-learners become unnecessary and are removed. The experiments on customer emails from an e-commerce company have shown that the C3 model outperformed the Naive Bayes model on classifying the emails of rare categories in the early stage of incremental learning.	Peking Univ, Dept Comp Sci & Technol, Beijing 100871, Peoples R China; Univ Hong Kong, E Business Technol Inst, Hong Kong, Hong Kong, Peoples R China	Yang, J (reprint author), Peking Univ, Dept Comp Sci & Technol, Beijing 100871, Peoples R China.						Chalup Stephan K, 2002, Int J Neural Syst, V12, P447, DOI 10.1142/S0129065702001308; Dietterich TG, 1997, AI MAG, V18, P97; Fahlman S., 1989, ADV NEURAL INFORMATI, V2, P524; Giraud-Carrier C, 2000, AI COMMUN, V13, P215; McCallum A, 1998, AAAI 98 WORKSH LEARN; Rennie J., 2000, P KDD 2000 WORKSH TE; Sebastiani F, 2002, ACM COMPUT SURV, V34, P1, DOI 10.1145/505282.505283; SEGAL RB, 2000, 17 INT C MACH LEARN, P863	8	0	0	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		3-540-20646-9	LECT NOTES ARTIF INT			2003	2903						747	758				12	Computer Science, Artificial Intelligence	Computer Science	BY08N	WOS:000187551700064	
S	Zhang, H; Ling, CX		Xiang, Y; ChaibDraa, B		Zhang, H; Ling, CX			A fundamental issue of naive Bayes	ADVANCES IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS	LECTURE NOTES IN ARTIFICIAL INTELLIGENCE		English	Article; Proceedings Paper	16th Conference of the Canadian-Society-for-Computational-Studies-of-Intelligence	JUN 11-13, 2003	HALIFAX, CANADA	Canadian Soc Computat Studies Intelligence, Natl Res Council Canada				Naive Bayes is one of the most efficient and effective inductive learning algorithms for machine learning and data mining. But the conditional independence assumption on which it is based, is rarely true in real-world applications. Researchers extended naive Bayes to represent dependence explicitly, and proposed related learning algorithms based on dependence. In this paper, we argue that, from the classification point of view, dependence distribution plays a crucial role, rather than dependence. We propose a novel explanation on the superb classification performance of naive Bayes. To verify our idea, we design and conduct experiments by extending the ChowLiu algorithm to use the dependence distribution to construct TAN, instead of using mutual information that only reflects the dependencies among attributes. The empirical results provide evidences to support our new explanation.	Univ New Brunswick, Fac Comp Sci, Fredericton, NB E3B 5A3, Canada; Univ Western Ontario, Dept Comp Sci, London, ON N6A 5B7, Canada	Zhang, H (reprint author), Univ New Brunswick, Fac Comp Sci, Fredericton, NB E3B 5A3, Canada.						CHOW CK, 1968, IEEE T INFORM THEORY, V14, P462, DOI 10.1109/TIT.1968.1054142; Domingos P, 1997, MACH LEARN, V29, P103, DOI 10.1023/A:1007413511361; Friedman N, 1997, MACH LEARN, V29, P131, DOI 10.1023/A:1007465528199; Merz C., 1997, UCI REPOSITORY MACHI	4	0	0	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		3-540-40300-0	LECT NOTES ARTIF INT			2003	2671						591	595				5	Computer Science, Artificial Intelligence	Computer Science	BX29V	WOS:000184853500053	
S	Zhang, Y; Zhang, LJ; Li, ZH; Yan, JF		Zhong, N; Ras, ZW; Tsumoto, S; Suzuki, E		Zhang, Y; Zhang, LJ; Li, ZH; Yan, JF			Improving the performance of text classifiers by using association features	FOUNDATIONS OF INTELLIGENT SYSTEMS	LECTURE NOTES IN ARTIFICIAL INTELLIGENCE		English	Article; Proceedings Paper	14th International Symposium on Methodologies for Intelligent Systems	OCT 28-31, 2003	MAEBASHI CITY, JAPAN	Maebashi Inst Technol, Japanese Soc Artificial Intelligence, Maebashi Convent Bur, Maebashi City Govt, Gunma Prefecture Govt, US AFOSR/AOARD, Web Intelligence Consortium, Gunma Informat Serv Ind Assoc, Ryomo Syst Co Ltd				The co-occurrence of words can make contribution to text classification. However, current text classification technology failed to take full advantage of this information, In this paper, we use association features to describe this information and present the algorithm for creating association feature set to make the association features we selected be good discriminators. The experiment results show that the performance of Naive Bayes text classifier and decision tree text classifier could be improved by using association features.	NW Polytech Univ, Dept Comp Sci & Engn, Xian, Peoples R China	Zhang, Y (reprint author), NW Polytech Univ, Dept Comp Sci & Engn, Xian, Peoples R China.	zhangy@co-think.com; zlj@co-think.com; lzh@co-think.com; yanjf@mail.nwpu.edu.cn					DESHPANDE M, 2002, P 11 ACM INT C INF K; LESH N, 1999, P 5 ACM SIGKDD INT C; MLADENIC D, 1998, P 17 EL COMP SCI C L; Sebastiani F, 2002, ACM COMPUT SURV, V34, P1, DOI 10.1145/505282.505283; TAN CM, 2002, INFORMATION PROCESSI, V1	5	0	0	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		3-540-20256-0	LECT NOTES ARTIF INT			2003	2871						315	319				5	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Computer Science	BY15P	WOS:000187959000043	
B	Zhang, Y; Zhang, LJ; Yan, JF; Li, ZH			IEEE COMPUTER SOCIETY	Zhang, Y; Zhang, LJ; Yan, JF; Li, ZH			Using association features to enhance the performance of naive Bayes text classifier	ICCIMA 2003: FIFTH INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND MULTIMEDIA APPLICATIONS, PROCEEDINGS			English	Proceedings Paper	5th International Conference on Computational Intelligence and Multimedia Applications	SEP 27-30, 2003	XIAN, PEOPLES R CHINA	Natl Nat Sci Fdn, Xidian Univ, Griffith Univ, Computat Intelligence Res Lab, Univ Birmingham, Nat Computat Res Lab, Univ Las Vegas, EE, IEEE Comp Press				The co-occurrence of words can make contributions to automatic text classification. However, this information cannot be represented in the feature set when only using primitive features, and can only be partially, represented when using n-grams as features. In this paper, we define a novel feature, association feature, to describe this information. In order to make the association features which we selected to be good discriminators, we proposed an approach to create association feature set, including redundancy pruning algorithm and feature selection algorithm. The experiment result shows that the performance of Naive Raves text classifier could be improved by using association features, which also means that the selected set of association features can make more contributions to text classification than primitive,features, and n-grams.								Agrawal R, 1993, P ACM SIGMOD INT C M; Antonie M.-L., 2002, P IEEE INT C DAT MIN; DESHPANDE M, 2002, P 11 ACM INT C INF K; LESH N, 1999, P 5 ACM SIGKDD INT C; McCallum A, 1998, AAAI 98 WORKSH LEARN; MLADENIC D, 1998, P 17 EL COMP SCI C L; Sebastiani F, 2002, ACM COMPUT SURV, V34, P1, DOI 10.1145/505282.505283; TAN CM, 2002, INFORMATION PROCESSI, P1	8	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA			0-7695-1957-1				2003							336	341				6	Computer Science, Artificial Intelligence	Computer Science	BX71U	WOS:000186203700058	
S	Zizka, J; Sredl, M; Bourek, A		Gelbukh, A		Zizka, J; Sredl, M; Bourek, A			Searching for significant word associations in text documents using genetic algorithms	COMPUTATIONAL LINGUISTICS AND INTELLIGENT TEXT PROCESSING, PROCEEDINGS	LECTURE NOTES IN COMPUTER SCIENCE		English	Article; Proceedings Paper	4th International Conference on Computational Linguistics and Intelligent Text Processing	FEB 16-22, 2003	MEXICO CITY, MEXICO	Natl Polytech Inst, Ctr Comp Res				This paper describes some experiments that used Genetic Algorithms (GAs) for looking for important word associations (phrases) in unstructured text documents obtained from the Internet in the area of a specialized medicine. GAs can evolve sets of word associations with assigned significance weights from the document categorization point of view (here two classes: relevant and irrelevant documents). The categorization was similarly reliable like the naive Bayes method using just individual words; in addition, in this case GAs provided phrases consisting of one, two, or three words. The selected phrases were quite meaningful from the human point of view.	Masaryk Univ, Dept Informat Technol, Fac Informat, Brno 60200, Czech Republic; Masaryk Univ, Dept Biophys, Fac Med, Brno 66243, Czech Republic	Zizka, J (reprint author), Masaryk Univ, Dept Informat Technol, Fac Informat, Bot 68a, Brno 60200, Czech Republic.						Goldberg D., 1989, GENETIC ALGORITHMS S; Lewis David D., 1998, P 10 EUR C MACH LEAR, P4; McCallum A., 1998, P AAAI 98 WORKSH LEA; ZIZKA J, 2002, LECT NOTES COMPUTER, V2276, P402; ZIZKA J, 2002, LECT NOTES ARTIF INT, V2448, P99	5	0	0	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		3-540-00532-3	LECT NOTES COMPUT SC			2003	2588						584	587				4	Computer Science, Artificial Intelligence; Computer Science, Theory & Methods; Language & Linguistics	Computer Science; Linguistics	BW58E	WOS:000182492300064	
J	Pietersma, D; Lacroix, R; Lefebvre, D; Wade, KM				Pietersma, D; Lacroix, R; Lefebvre, D; Wade, KM			Machine-learning assisted knowledge acquisition to filter lactation curve data	TRANSACTIONS OF THE ASAE			English	Article						machine learning; decision-tree induction; naive-Bayes classifier; knowledge acquisition; lactation-curve analysis; ROC curves	DECISION-SUPPORT SYSTEM; DAIRY-HERD MANAGEMENT; EXPERT-SYSTEM; CLASSIFICATION; MASTITIS; ALGORITHMS; ACCURACY; RECORDS; VISION	Machine learning was employed to develop knowledge-based modules to automate the filtering of test-day data for group-average lactation curve analysis. The classification task consisted of identifying an individual cow's milk-yield test as being either outlier or non-outlier Experiments were carried out to explore the importance of deriving predictive attributes and choice of machine-learning technique. Data consisted of 1080 milk yield tests, of which 108 had been classified as outliers by a dairy nutrition specialist and 972 cases had been classified as non-outliers. Two different approaches to machine learning were applied to these data: decision-tree induction and naive-Bayes classification. Performance of the classifiers was estimated through ten-fold cross-validation, while relative operating characteristic curves were used to visualize the achieved trade-off between sensitivity and specificity. Use of an initial set of derived attributes significantly improved the performance compared to limiting attributes to those available to the domain specialist. However, adding even more complex attributes did not necessarily improve the classification performance. Overall, the naive-Bayes approach showed significantly better performance than decision-tree induction. For each machine-learning approach, three final classifiers, associated with a low, medium, and high filtering intensity, were generated from the 1080 cases. The expected true positive rate varied from 41% to 78% for false positive rates between 1.1% and 4.6%. However, due to the low prevalence of outlier tests, this performance was associated with a large number of false positives. The domain specialist considered the final classifiers of both approaches as plausible, but found the decision trees easier to understand than the naive-Bayes classifiers. Machine learning was considered to be a promising approach to assist knowledge acquisition.	McGill Univ, Dept Anim Sci, Quebec City, PQ H9C 3V9, Canada; Programme Anal Troupeaux Laiters Quebec, Dept Res & Dev, Ste Anne De Bellevue, PQ, Canada	Wade, KM (reprint author), McGill Univ, Dept Anim Sci, 21111 Lakeshore Rd,Ste Anne De Bellevue,Macdonald, Quebec City, PQ H9C 3V9, Canada.						ALLORE HG, 1995, J DAIRY SCI, V78, P1382, DOI 10.3168/jds.S0022-0302(95)76761-3; BENDAVID A, 1995, MACH LEARN, V18, P109; Bradley AP, 1997, PATTERN RECOGN, V30, P1145, DOI 10.1016/S0031-3203(96)00142-2; Breiman L., 1984, CLASSIFICATION REGRE; Brodley CE, 1997, STAT COMPUT, V7, P45, DOI 10.1023/A:1018557312521; Brodley CE, 1996, PROCEEDINGS OF THE THIRTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND THE EIGHTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE, VOLS 1 AND 2, P799; Dhar V., 1997, INTELLIGENT DECISION; Doluschitz R., 1990, Computers and Electronics in Agriculture, V5, DOI 10.1016/0168-1699(90)90045-Q; Duda R., 1973, PATTERN CLASSIFICATI; Durkin J., 1994, EXPERT SYSTEMS DESIG; GRINSPAN P, 1994, T ASAE, V37, P1647; Heald CW, 2000, J DAIRY SCI, V83, P711; HOWARTH MS, 1992, J AGR ENG RES, V53, P123, DOI 10.1016/0021-8634(92)80078-7; Jayas DS, 2000, J AGR ENG RES, V77, P119, DOI 10.1006/jaer.2000.0559; Kohavi R, 1997, ARTIF INTELL, V97, P273, DOI 10.1016/S0004-3702(97)00043-X; Kononenko I, 1998, MACHINE LEARNING DAT, P389; Kubat M, 1998, MACH LEARN, V30, P195, DOI 10.1023/A:1007452223027; LANGLEY P, 1995, COMMUN ACM, V38, P55; Langley P., 1994, P 10 C UNC ART INT, P399; MANGINA EE, 1999, P 2 EUR FED INF TECH, P99; MCQUEEN RJ, 1995, COMPUT ELECTRON AGR, V12, P275, DOI 10.1016/0168-1699(95)98601-9; Michalski R. S., 1980, International Journal of Policy Analysis and Information Systems, V4; Michie D., 1994, MACHINE LEARNING NEU; Mitchell T.M., 1997, MACHINE LEARNING; PELLERIN D, 1994, J DAIRY SCI, V77, P2308, DOI 10.3168/jds.S0022-0302(94)77174-5; Pietersma D, 2001, J DAIRY SCI, V84, P730; PIETERSMA D, 2002, IN PRESS COMPUT ELEC; Plant R. E., 1991, KNOWLEDGE BASED SYST; Provost F., 1998, P 15 INT C MACH LEAR, P445; SPAHR SL, 1988, J DAIRY SCI, V71, P879; Steinberg D., 1997, CART CLASSIFICATION; STEINMETZ V, 1994, T ASAE, V37, P1347; SWETS JA, 1988, SCIENCE, V240, P1285, DOI 10.1126/science.3287615; Verdenius F, 1997, AI APPLICATIONS, V11, P31; Weiss S. M., 1991, COMPUTER SYSTEMS LEA; Witten I.H., 2000, DATA MINING PRACTICA; Yang XZ, 1999, T ASAE, V42, P1063	37	0	0	AMER SOC AGRICULTURAL ENGINEERS	ST JOSEPH	2950 NILES RD, ST JOSEPH, MI 49085-9659 USA	0001-2351			T ASAE	Trans. ASAE	SEP-OCT	2002	45	5					1637	1650				14	Agricultural Engineering	Agriculture	630EF	WOS:000180094000044	
S	Buckinx, W; Baesens, B; Van den Poel, D; Van Kenhove, P; Vanthienen, J		Zanasi, A; Brebbia, CA; Ebecken, NFF; Melli, P		Buckinx, W; Baesens, B; Van den Poel, D; Van Kenhove, P; Vanthienen, J			Using machine learning techniques to predict defection of top clients	DATA MINING III	MANAGEMENT INFORMATION SYSTEMS		English	Proceedings Paper	3rd International Conference on Data Mining	SEP 25-27, 2002	BOLOGNA, ITALY	Wessex Inst Technol			OPERATING CHARACTERISTIC CURVES; BEHAVIOR; AREAS	Fierce competition in many industries causes switching behavior of customers. Because foregone profits of defected customers are significant, an increase of the retention rate can be very profitable. In this paper, we focus on the treatment of companies' most promising current customers in a non-contractual setting. We build a model in order to predict chum behavior of top clients who will (partially) defect in the near future. We applied the following classification techniques: logistic regression, linear discriminant analysis, quadratic discriminant analysis, C4.5, neural networks and Naive Bayes. Their performance is quantified by the classification accuracy and the area under the receiver operating characteristic curve (AUROC). The experiments were carried out on a real life data set obtained by a Belgian retailer. The article contributes in many ways. The results show that past customer behavior has predictive power to indicate future partial defection. This finding is from a companies' point of view even more important than being able to define total defectors, which was until now the traditional goal in attrition research. It was found that neural networks performed better than the other classification techniques in terms of both classification accuracy and AUROC. Although the performance benefits are sometimes small in absolute terms, they are statistically significant and relevant from a marketing perspective. Finally it was found that the number of past shop visits and the time between past shop incidences are amongst the most predictive inputs for the problem at hand.	State Univ Ghent, Dept Marketing, B-9000 Ghent, Belgium	Buckinx, W (reprint author), State Univ Ghent, Dept Marketing, B-9000 Ghent, Belgium.						Athanassopoulos AD, 2000, J BUS RES, V47, P191, DOI 10.1016/S0148-2963(98)00060-5; DELONG ER, 1988, BIOMETRICS, V44, P837, DOI 10.2307/2531595; Dietterich TG, 1998, NEURAL COMPUT, V10, P1895, DOI 10.1162/089976698300017197; Egan J., 1975, SERIES COGNITION PER; Ganesh J, 2000, J MARKETING, V64, P65, DOI 10.1509/jmkg.64.3.65.18028; HANLEY JA, 1983, RADIOLOGY, V148, P839; Jones MA, 2000, J RETAILING, V76, P259, DOI 10.1016/S0022-4359(00)00024-5; Mackay DJC., 1992, BAYESIAN METHODS ADA; MIZERSKI RW, 1982, J CONSUM RES, V9, P301, DOI 10.1086/208925; MOZER MC, 2000, IEEE T NEURAL NETWOR, V11; Quinlan J.R., 1993, C4 5 PROGRAMS MACHIN; REINARTZ WJ, 2000, J MARKETING, V64; Rust Roland, 1993, J RETAILING, V69; SWETS JA, 1979, INVEST RADIOL, V14, P109, DOI 10.1097/00004424-197903000-00002; Wu CC, 2000, EUR J OPER RES, V127, P109, DOI 10.1016/S0377-2217(99)00326-4; Zeithaml VA, 1996, J MARKETING, V60, P31, DOI 10.2307/1251929	16	0	0	WIT PRESS	SOUTHAMPTON	ASHURST LODGE, SOUTHAMPTON SO40 7AA, ASHURST, ENGLAND	1470-6326		1-85312-925-9	MANAG INFORMAT SYST			2002	6						509	517				9	Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications	Computer Science	BV88L	WOS:000180301500049	
B	Cohen, I; Sebe, N; Garg, A; Lew, MS; Huang, TS			IEEE; IEEE	Cohen, I; Sebe, N; Garg, A; Lew, MS; Huang, TS			Facial expression recognition from video sequences	IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS			English	Proceedings Paper	IEEE International Conference on Multimedia and Expo	AUG 26-29, 2002	LAUSANNE, SWITZERLAND	Ecole Polytech Fed Lausanne, IBM Res, France Telecon R&D, Interact Multimodal Informat Management	SWISS FED INST TECHNOL		OPTICAL-FLOW; NETWORK	Recognizing human facial expression and emotion by computer is an interesting and challenging problem. In this paper we propose a method for recognizing emotions through facial expressions displayed in video sequences. We introduce a Tree-Augmented-Naive Bayes (TAN) classifier that learns the dependencies between the facial features and we provide an algorithm for finding the best TAN structure. Our person-dependent and person-independent experiments show that using this TAN structure provides significantly better results than using simpler NB-classifiers.	Univ Illinois, Urbana, IL 61801 USA	Cohen, I (reprint author), Univ Illinois, Urbana, IL 61801 USA.						Chen L. S., 2000, THESIS U ILLINOIS UR; CHEN LS, 1998, IEEE WORKSH MULT SIG, P83; CHOW CK, 1968, IEEE T INFORM THEORY, V14, P462, DOI 10.1109/TIT.1968.1054142; De Silva LC, 1997, ICICS - PROCEEDINGS OF 1997 INTERNATIONAL CONFERENCE ON INFORMATION, COMMUNICATIONS AND SIGNAL PROCESSING, VOLS 1-3, P397; Ekman P., 1978, FACIAL ACTION CODING; Friedman N, 1997, MACH LEARN, V29, P131, DOI 10.1023/A:1007465528199; Goleman D., 1995, EMOTIONAL INTELLIGEN; Kanade T, 2000, INT C AUT FAC GEST R, P46; MASE K, 1991, IEICE TRANS COMMUN, V74, P3474; Rosenblum M, 1996, IEEE T NEURAL NETWOR, V7, P1121, DOI 10.1109/72.536309; SEBE N, 2002, IN PRESS ICPR; TAO H, 1998, CVPR; Yacoob Y, 1996, IEEE T PATTERN ANAL, V18, P636, DOI 10.1109/34.506414	13	0	0	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA			0-7803-7304-9				2002							A121	A124				4	Computer Science, Information Systems; Computer Science, Software Engineering; Engineering, Electrical & Electronic; Telecommunications	Computer Science; Engineering; Telecommunications	BV38W	WOS:000178771600262	
S	Curotto, CL; Ebecken, NFF		Zanasi, A; Brebbia, CA; Ebecken, NFF; Melli, P		Curotto, CL; Ebecken, NFF			Implementing data mining algorithms with Microsoft SQL server.	DATA MINING III	MANAGEMENT INFORMATION SYSTEMS		English	Proceedings Paper	3rd International Conference on Data Mining	SEP 25-27, 2002	BOLOGNA, ITALY	Wessex Inst Technol				The OLE DB for DM (Microsoft's object-based technology for sharing information and services across process and machine boundaries focused on database mining applications) specification provides an industry standard for implementation of data mining algorithms aggregated with Microsoft SQL Server 2000. The Simple Naive Bayes classifier is,, implemented using the OLE DB for DM Resource Kit. Numeric input attributes, multiple prediction trees and incremental classification are considered. All necessary steps to implement this algorithm are explained and discussed. Some results are shown to illustrate the capabilities of the implementation.								AGRAWAL R, 1995, DEV TIGHTLY COUPLED; BERNHARDT J, 1999, P 15 INT C DAT ENG S; BRADLEY PS, 1999, MSRTR9835 MICR CORP; CHICKERING DM, 1994, MSRTR9409 MICR CORP; CUROTTO CL, 2002, IMPLEMENTING NEW DAT; FREITAS AA, 1997, THESIS U ESSEX UK; HAN J, 1996, P DMKD 96 SIGMOD 96; Han J., 2001, DATA MINING CONCEPTS; John G. H., 1997, THESIS STANFORD U; KIM P, 2002, MAKING OLE DB DATA M; KIM P, 2002, MICROSOFT PUBLIC SQL; *MICR CORP, 1998, VIS STUD DEV ENT; *MICR CORP, 2001, MICR PLATF SOFTW DEV; *MICR CORP, 2000, OLE DB DAT MIN SPEC; *MICR CORP, 2000, SQL SERV 2000; *MICR CORP, 2002, OLE DB DAT MIN RES K; *MICR CORP, 1998, VIS STUD 6 0; NETZ A, 2001, P 17 INT C DAT ENG H; NETZ A, 2000, P 26 INT C VER LARG; *SANDST TECN INC, 2000, VIS PARS PLUS PLUS V; Sarawagi S., 1998, P ACM SIGMOD 1998; SEIDMAN C, 2001, DATA MINING MICROSOF; SETH PI, 2001, 3 PARTY DATA MINING; SOUSA MS, 1998, P INT C DEXA WORKSH; SOUSA MS, 1998, THESIS COPPE UFPR RI; SOUSA MS, 1998, P INT C PDPTA SPEC S; SOUSA MS, 1998, DATA MINING, P7; Witten I.H., 2000, DATA MINING PRACTICA	28	0	2	WIT PRESS	SOUTHAMPTON	ASHURST LODGE, SOUTHAMPTON SO40 7AA, ASHURST, ENGLAND	1470-6326		1-85312-925-9	MANAG INFORMAT SYST			2002	6						73	82				10	Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications	Computer Science	BV88L	WOS:000180301500008	
S	Elomaa, T; Lindgren, JT		Lange, S; Satoh, K; Smith, CH		Elomaa, T; Lindgren, JT			Experiments with projection learning	DISCOVERY SCIENCE, PROCEEDINGS	LECTURE NOTES IN COMPUTER SCIENCE		English	Article; Proceedings Paper	5th International Conference on Discovery Science (DS 2002)	NOV 24-26, 2002	LUBECK, GERMANY	CorpoBase, DFKI GmbH, JessenLenz			STACKED GENERALIZATION; WINNOW	Excessive information is known to degrade the classification performance of many machine learning algorithms. Attribute-efficient learning algorithms can tolerate irrelevant attributes without their performance being affected too much. Valiant's projection learning is a way to combine such algorithms so that this desired property is maintained. The archetype attribute-efficient learning algorithm Winnow and, especially, combinations of Winnow have turned out empirically successful in domains containing many attributes. However, projection learning as proposed by Valiant has not yet been evaluated empirically. We study how projection learning relates to using Winnow as such and with an extended set of attributes. We also compare projection learning with decision tree learning and Naive Bayes on UCI data sets. Projection learning systematically enhances the classification accuracy of Winnow, but the cost in time and space consumption can be high. Balanced Winnow seems to be a better alternative than the basic algorithm for learning the projection hypotheses. However, Balanced Winnow is not well suited for learning the second level (projective disjunction) hypothesis. The on-line approach projection learning does not fall far behind in classification accuracy from batch algorithms such as decision tree learning and Naive Bayes on the UCI data sets that we used.	Univ Helsinki, Dept Comp Sci, FIN-00014 Helsinki, Finland	Elomaa, T (reprint author), Univ Helsinki, Dept Comp Sci, POB 26,Teollisuuskatu 23, FIN-00014 Helsinki, Finland.						Blake C. L., 1998, UCI REPOSITORY MACHI; BLUM A, 1995, P 12 INT C MACH LEAR, P64; Duda R. O., 2000, PATTERN CLASSIFICATI; Fayyad U.M., 1993, P 13 INT JOINT C ART, P1022; Golding AR, 1999, MACH LEARN, V34, P107, DOI 10.1023/A:1007545901558; HAUSSLER D, 1988, ARTIF INTELL, V36, P177, DOI 10.1016/0004-3702(88)90002-1; Haykin S., 1999, NEURAL NETWORKS COMP; KHARDON R, 1999, P 16 INT JOINT C ART, P911; Kivinen J, 1997, ARTIF INTELL, V97, P325, DOI 10.1016/S0004-3702(97)00039-8; LITTLESTONE N, 1989, UCSCCRL8911; Littlestone N., 1988, Machine Learning, V2, DOI 10.1007/BF00116827; Quinlan J.R., 1993, C4 5 PROGRAMS MACHIN; ROSENBLATT F, 1958, PSYCHOL REV, V65, P386, DOI 10.1037/h0042519; Roth D, 2002, NEURAL COMPUT, V14, P1071, DOI 10.1162/089976602753633394; Servedio R. A., 1999, Proceedings of the Twelfth Annual Conference on Computational Learning Theory, DOI 10.1145/307400.307474; Servedio RA, 2000, J COMPUT SYST SCI, V60, P161, DOI 10.1006/jcss.1999.1666; Ting K. M., 1997, P 15 INT JOINT C ART, P866; Uehara R, 2000, THEOR COMPUT SCI, V230, P131, DOI 10.1016/S0304-3975(99)00154-1; Valiant LG, 1999, MACH LEARN, V37, P115, DOI 10.1023/A:1007678005361; Valiant LG, 2000, J ACM, V47, P854, DOI 10.1145/355483.355486; Valiant LG, 1994, CIRCUITS MIND; Vapnik V., 1998, STAT LEARNING THEORY; WITTEN IH, 2000, PRACTICAL MACHINE LE; WOLPERT DH, 1992, NEURAL NETWORKS, V5, P241, DOI 10.1016/S0893-6080(05)80023-1	24	0	0	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		3-540-00188-3	LECT NOTES COMPUT SC			2002	2534						127	140				14	Computer Science, Theory & Methods	Computer Science	BW68U	WOS:000182827700012	
B	Greiner, K; Zhou, W			AAAI; AAAI	Greiner, K; Zhou, W			Structural extension to logistic regression: Discriminative parameter learning of belief net classifiers	EIGHTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-02)/FOURTEENTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE (IAAI-02), PROCEEDINGS			English	Proceedings Paper	18th National Conference on Artificial Intelligence/14th Conference on Innovative Applications of Artificial Intelligence	JUL 28-AUG 01, 2002	EDMONTON, CANADA	Amer Assoc Artificial Intelligence, ACM SIGART, Alberta Informat Circle Res Excellence, DARPA, NASA Ames Res Ctr, Natl Sci Fdn, Naval Res Lab		(Bayesian) belief nets; logistic regression; classification; PAC-leaming; computational/sample complexity	PROBABILISTIC NETWORKS	Bayesian belief nets (BNs) are often used for classification tasks - typically to return the most likely "class label" for each specified instance. Many BN-learners, however, attempt to find the BN that maximizes a different objective function (viz., likelihood, rather than classification accuracy), typically by first learning an appropriate graphical structure, then finding the maximal likelihood parameters for that structure. As these parameters may not maximize the classification accuracy, "discriminative learners" follow the alternative approach of seeking the parameters that maximize conditional likelihood (CL), over the distribution of instances the BN will have to classify. This paper first formally specifies this task, and shows how it relates to logistic regression, which corresponds to finding the optimal CL parameters for a naive-bayes structure. After analyzing its inherent (sample and computational) complexity, we then present a general algorithm for this task, ELR, which applies to arbitrary BN structures and which works effectively even when given the incomplete training data. This paper presents empirical evidence that ELR works better than the standard "generative" approach in a variety of situations, especially in common situation where the BN-structure is incorrect.	Univ Alberta, Dept Comp Sci, Edmonton, AB T6G 2H1, Canada	Greiner, K (reprint author), Univ Alberta, Dept Comp Sci, Edmonton, AB T6G 2H1, Canada.						ABE N, 1991, COLT, P277; BEINLICH I, 1989, ECAI MED         AUG; Binder J, 1997, MACH LEARN, V29, P213, DOI 10.1023/A:1007421730016; Bishop C. M., 1998, NEURAL NETWORKS PATT; Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734; CHENG J, 2002, IN PRESS ARTIFICIAL; Cheng J, 1999, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P101; CHOW C, 1968, IEEE T INFORM THEORY, P462; COOPER GF, 1992, MACH LEARN, V9, P309, DOI 10.1007/BF00994110; DASGUPTA S, 1997, MACHINE LEARNING, V29; DOMINGO P, 1996, ICML; Duda R., 1973, PATTERN CLASSIFICATI; FAYYAD UM, 1993, IJCAI-93, VOLS 1 AND 2, P1022; Friedman N, 1997, MACH LEARN, V29, P131, DOI 10.1023/A:1007465528199; GREINER R, 2002, LOGISTIC REGRESSION; GREINER R, 1997, UAI; Heckerman D., 1998, LEARNING GRAPHICAL M, P01; HERSKOVITS E, 1991, METHODS INFORMATION, P362; JORDAN M, 1995, LOGISTIC FUNCTION TU; Kohavi R., 1995, IJCAI; KOHAVI R, 1997, ARTIFICIAL INTELLIGE, V97; Little J.A., 1987, STAT ANAL MISSING DA; McCullagh P., 1989, GEN LINEAR MODELS; MINKA TP, 2001, ALGORITHMS MAXIMUM L; Mitchell T.M., 1997, MACHINE LEARNING; Ng A., 2001, NIPS; Pearl J., 1988, PROBABILISTIC REASON; Ripley B., 1996, PATTERN RECOGNITION; VALIANT LG, 1984, COMMUN ACM, V27, P1134, DOI 10.1145/1968.1972	29	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA			0-262-51129-0				2002							167	173				7	Computer Science, Artificial Intelligence	Computer Science	BW91S	WOS:000183593700026	
B	Hong, SJ; Hosking, J; Natarajan, R		Kumar, V; Tsumoto, S; Zhong, N; Yu, PS; Wu, XD		Hong, SJ; Hosking, J; Natarajan, R			Ensemble modeling through multiplicative adjustment of class probability	2002 IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS			English	Proceedings Paper	2nd IEEE International Conference on Data Mining	DEC 09-12, 2002	MAEBASHI CITY, JAPAN	IEEE Comp Soc Tech Comm Pattern Anal & Machine Intelligence, IEEE Comp Soc Tech Comm Computat Intelligence, Maebashi Convent Bur, Maebashi City Govt, Gunmma Prefecture Govt, Maebashi Inst Technol, AdIn Res Inc, Japan Res Inst Ltd, Support Ctr Adv Telecommun Technol Res, US AFOSR, AOARD, AROFE, SAS Inst Japan, SPSS Japan Inc, Kankodo, Syst Alpha Inc, GCC Inc, Yamato Inc, Daiei Dream Co Ltd				We develop a new concept for aggregating items of evidence for class probability estimation. In Naive Bayes, each feature contributes an independent multiplicative factor to the estimated class probability. We modify this model to include an exponent in each factor in order to introduce feature importance. These exponents are chosen to maximize the accuracy of estimated class probabilities on the training data. For Naive Bayes, this modification accomplishes more than what feature selection can. More generally, since the individual features can be the outputs of separate probability models, this yields a new ensemble modeling approach, which we call APM (Adjusted Probability Model), along with a regularized version called APMR.	IBM Corp, Thomas J Watson Res Ctr, Yorktown Hts, NY 10598 USA	Hong, SJ (reprint author), IBM Corp, Thomas J Watson Res Ctr, POB 218, Yorktown Hts, NY 10598 USA.						Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.1023/A:1018054314350; DAWID AP, 1982, ENCY STAT SCI, V6, P210; Domingos P, 1997, MACH LEARN, V29, P103, DOI 10.1023/A:1007413511361; ELKAN C, 1997, CS97557 U CAL SAN DI; FREUND Y, 1996, P ICML 96; FREUND Y, 1999, P ICML 99; GARG A, 2001, P ECML 2001; Gartner T., 2001, P 18 INT C MACH LEAR, P154; GOOD IJ, 1952, J ROY STAT SOC B, V14, P107; HALL M, 2000, BENCHMARKING ATTRIBU; Hand DJ, 2001, INT STAT REV, V69, P385, DOI 10.2307/1403452; Hand DJ, 2000, J APPL STAT, V27, P527; Hastie T. J., 2001, ELEMENTS STAT LEARNI; HONG SJ, 2002, RC22393 IBM TJ WATS; KOHAVI R, 1997, IMPROVING SIMPLE BAY; PIATETSKYSHAPIR.G, 2000, SIGKDD EXPLORATIONS, V2, P76; QUINLAN JR, 1996, P AAAI 96; RENNIE JDM, 1999, THESIS CARNEGIE MELL; SPIEGELHALTER DJ, 1984, J ROY STAT SOC A STA, V147, P35, DOI 10.2307/2981737	19	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA			0-7695-1754-4				2002							621	624				4	Computer Science, Artificial Intelligence	Computer Science	BV87U	WOS:000180274000082	
B	Keswani, G; Hall, LO			IEEE; IEEE	Keswani, G; Hall, LO			Text classification with enhanced semi-supervised fuzzy clustering	PROCEEDINGS OF THE 2002 IEEE INTERNATIONAL CONFERENCE ON FUZZY SYSTEMS, VOL 1 & 2			English	Proceedings Paper	IEEE International Conference on Fuzzy Systems	MAY 12-17, 2002	HONOLULU, HI	IEEE, IEEE Neural Network Soc		text classification; expectation-maximization algorithm; semi supervised fuzzy c-means algorithm; naive Bayes classifier; semi supervised learning	NUMBER	Given the increasing volume of information available on the web, it is important to meaningfully organize online documents. Hence, the design of efficient and accurate text classification systems is of interest. In this paper, we explore a framework, in which we improve the performance of a base classifier, by clustering unlabeled data with labeled data using probabilistic and fuzzy approaches. We have used Expectation Maximization and Semi-Supervised Fuzzy C-Means for clustering the unlabeled data with labeled data. The Naive Bayes Classifier was the base classifier utilizing both the original labeled data and then additional data labeled through clustering. Utilizing unlabeled data from semi-supervised fuzzy clustering results in an improved classifier.	Univ S Florida, Dept Comp Sci, Tampa, FL 33620 USA	Keswani, G (reprint author), Univ S Florida, Dept Comp Sci, Tampa, FL 33620 USA.						Bensaid AM, 1996, PATTERN RECOGN, V29, P859, DOI 10.1016/0031-3203(95)00120-4; Bezdek JC, 1998, IEEE T SYST MAN CY B, V28, P301, DOI 10.1109/3477.678624; Bilmes J, 1997, ICSITR97021; Craven M., 1998, Proceedings Fifteenth National Conference on Artificial Intelligence (AAAI-98). Tenth Conference on Innovative Applications of Artificial Intelligence; Dempster NM, 1977, J R STAT SOC B, V39, P185; Duda R.O., 2001, PATTERN CLASSIFICATI; Hardy A, 1996, COMPUT STAT DATA AN, V23, P83, DOI 10.1016/S0167-9473(96)00022-9; Lewis D., 1994, P 17 ANN INT ACM SIG, P3; MILLIGAN GW, 1985, PSYCHOMETRIKA, V50, P159, DOI 10.1007/BF02294245; Mitchell T.M., 1997, MACHINE LEARNING; Nigam K, 2000, MACH LEARN, V39, P103, DOI 10.1023/A:1007692713085; Sahami M., 1998, AAAI 98 WORKSH LEARN; SEGAL RB, 1999, P 3 INT C AUT AG MAY	13	0	0	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA			0-7803-7280-8				2002							621	626				6	Computer Science, Artificial Intelligence	Computer Science	BU95M	WOS:000177476600111	
B	Lai, MY; Kuo, CH; Sung, LC			IEEE	Lai, MY; Kuo, CH; Sung, LC			An adaptation scheme for real-time video streaming	2002 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL II, PROCEEDINGS			English	Proceedings Paper	IEEE International Symposium on Circuits and Systems	MAY 26-29, 2002	PHOENIX, AZ	IEEE, IEEE Circuits & Syst Soc				The transmission of digital video on the Internet is popular recently. However, the transmission bandwidth may be limited in many situations. Therefore, a bandwidth adaptation mechanism, which is able to real-time dynamically select suitable bandwidth, is preferred for video transmission. In this paper, we make use of the Naive Bayes classifier to build such an adaptation scheme. The decision parameters of this classifier include the packet loss rate, the network instant available bandwidth, and the transmission bit rate. The objective of this designed adaptation scheme aims to reduce the transmission packet loss rate and to select the suitable bandwidth for transmission. In this paper, we describe our design and illustrate the effectiveness of the design via experimental simulations at an ADSL environment.								Carter R., 1996, MEASURING BOTTLENECK; CHUNG Y, 1998, SPIE P INT S OPT SCI; Chung YJ, 1999, IEEE T CIRCUITS-II, V46, P951; Kim J, 2000, IEEE T CIRC SYST VID, V10, P1164; LAI MY, 2001, P WORKSH 21 CENT DIG; NG JKY, 1998, REAL TIM COMP SYST A, P91; Ramanujan RS, 1997, LCN'97 - 22ND ANNUAL CONFERENCE ON LOCAL COMPUTER NETWORKS, PROCEEDINGS, P398; Rejaie R., 1999, P ACM SIGCOMM, P189, DOI 10.1145/316188.316222; SUNG LC, 2000, P INT COMP S WORKSH	9	0	0	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA			0-7803-7448-7				2002							720	723				4	Computer Science, Hardware & Architecture; Computer Science, Information Systems; Engineering, Electrical & Electronic; Telecommunications	Computer Science; Engineering; Telecommunications	BX73X	WOS:000186280700182	
B	Lawrence, RD		Bhargava, HK; Ye, N		Lawrence, RD			A machine-learning approach to optimal bid pricing	COMPUTATIONAL MODELING AND PROBLEM SOLVING IN THE NETWORKED WORLD: INTERFACES IN COMPUTER SCIENCE AND OPERATIONS RESEARCH	OPERATIONS RESEARCH/COMPUTER SCIENCE INTERFACES SERIES		English	Proceedings Paper	8th INFORMS-Computing-Society Conference	JAN 08-10, 2003	CHANDLER, AZ	INFORMS Comp Soc		classification; naive Bayes classifier; feature selection; price optimization; dynamic pricing; bid pricing		We consider the problem faced by a seller in determining an optimal price to quote in response to a Request for Quote (RFQ) from a prospective buyer. The optimal price is determined by maximizing expected profit, given the underlying seller costs of the bid items and a computed probability of winning the bid as a function of price and other bid features such as buyer characteristics and the degree of competition. An entropy-based information-gain metric is used to quantify the contribution of the extracted features to predicting the win/loss label. A naive Bayes classification model is developed to predict the bid outcome (win or loss) as a function of these features. This model naturally generates the win probability as a function of bid price required to compute the optimal price. Results obtained by applying this model to a database of bid transactions involving computer sales demonstrate statistically significant lift curves for predicting bid outcome. A method for creating additional synthetic bids to improve computation of the win probability function is demonstrated. Finally, the computed optimal prices generated via this approach are compared to the actual bid prices approved by human pricing experts.	IBM Corp, Thomas J Watson Res Ctr, Yorktown Hts, NY 10598 USA	Lawrence, RD (reprint author), IBM Corp, Thomas J Watson Res Ctr, POB 218, Yorktown Hts, NY 10598 USA.						BAKOS Y, 1997, MARKETING SCI, V43; BICHLER M, 2002, IN PRESS IBM SYSTEMS, V41; BUSSEY P, 1998, P IEEE INT C SYSTEMS, V5, P4752; Cassaigne N., 2000, Proceedings 11th International Workshop on Database and Expert Systems Applications, DOI 10.1109/DEXA.2000.875124; IYENGAR V, 2001, P 3 ACM C EL COMM, P144, DOI 10.1145/501158.501174; Kannan PK, 2001, INT J ELECTRON COMM, V5, P63; LANGLEY P, 2001, P 10 NAT C ART INT, P399; Marshall M., 2001, Proceedings Third International Workshop on Advanced Issues of E-Commerce and Web-Based Information Systems. WECWIS 2001, DOI 10.1109/WECWIS.2001.933923; McGill JI, 1999, TRANSPORT SCI, V33, P233, DOI 10.1287/trsc.33.2.233; Mitchell T.M., 1997, MACHINE LEARNING; PAPAIOANNOU V, 2000, 2000 IEEE INT C SYST, V3, P2098; PEDNAULT E, 2002, COMMUNICATION; Quinlan J.R., 1993, C4 5 PROGRAMS MACHIN; RISH I, 2001, RC21993 IBM RES; VARIAN HR, 1996, 1 MONDAY         AUG, V1; Witten I.H., 2000, DATA MINING PRACTICA	16	0	0	KLUWER ACADEMIC PUBLISHERS	NORWELL	101 PHILIP DRIVE, ASSINIPPI PARK, NORWELL, MA 02061 USA			1-4020-7295-3	OPERAT RES COMP SCI			2002	21						97	118				22	Computer Science, Artificial Intelligence; Operations Research & Management Science	Computer Science; Operations Research & Management Science	BW89R	WOS:000183538300005	
S	Lindgren, T; Bostrom, H		CesaBianchi, N; Numao, M; Reischuk, R		Lindgren, T; Bostrom, H			Classification with intersecting rules	ALGORITHMIC LEARNING THEORY, PROCEEDINGS	LECTURE NOTES IN ARTIFICIAL INTELLIGENCE		English	Article; Proceedings Paper	13th Annual International Conference on Algorithmic Learning Theory	NOV 24-26, 2002	LUBECK, GERMANY	DFKI GmbH, Corpo Base, JessenLenz				Several rule induction schemes generate hypotheses in the form of unordered rule sets. One important problem that has to be addressed when classifying examples with such hypotheses is how to deal with overlapping rules that predict different classes. Previous approaches to-this problem calculate class probabilities based on the union of examples covered by the overlapping rules (as in CN2) or assumes rule independence (using naive Bayes). It is demonstrated that a significant improvement in accuracy can be obtained if class probabilities axe calculated based on the intersection of the overlapping rules, or in case of, an-empty intersection, based on as few intersecting regions as possible.	Univ Stockholm, Dept Comp & Syst Sci, S-16440 Kista, Sweden	Lindgren, T (reprint author), Univ Stockholm, Dept Comp & Syst Sci, Forum 100, S-16440 Kista, Sweden.						BOSTROM H, 2001, VIRTUAL PREDICT USER; CLARK AJL, 1989, J MOL ENDOCRINOL, V2, P3; Clark P., 1991, P 5 EUR WORK SESS LE, P151; Quinlan J. R., 1986, Machine Learning, V1, DOI 10.1023/A:1022643204877; Rivest R. L., 1987, Machine Learning, V2, DOI 10.1007/BF00058680	5	0	0	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		3-540-00170-0	LECT NOTES ARTIF INT			2002	2533						395	402				8	Computer Science, Artificial Intelligence; Computer Science, Theory & Methods; Statistics & Probability	Computer Science; Mathematics	BW67Y	WOS:000182804400031	
B	McKnight, LK; Wilcox, A; Hripcsak, G		Kohane, IS		McKnight, LK; Wilcox, A; Hripcsak, G			The effect of sample size and outcome prevalence on supervised machine learning of narrative data	AMIA 2002 SYMPOSIUM, PROCEEDINGS: BIOMEDICAL INFORMATICS: ONE DISCIPLINE			English	Proceedings Paper	Annual Symposium of the American-Medical-Informatics-Association	NOV   09, 2002	San Antonio, TX	Amer Med Informat Assoc				This paper examines the independent effects of outcome prevalence and training sample sizes on inductive learning performance. We trained 3 inductive learning algorithms (MC4, IB, and Naive-Bayes) on 60 simulated datasets of parsed radiology text reports labeled with 6 disease states. Data sets were constructed to define positive outcome states at 4 prevalence rates (1, 5, 10, 25, and 50%) in training set sizes of 200 and 2,000 cases. We found that the effect of outcome prevalence is significant when outcome classes drop below 10% of cases. The effect appeared independent of sample size, induction algorithm used, or class label. Work is needed to identify methods of improving classifier performance when output classes are rare.	Columbia Univ, Dept Med Informat, New York, NY 10027 USA	McKnight, LK (reprint author), Columbia Univ, Dept Med Informat, New York, NY 10027 USA.						AHA D, INT J MAN MED STUDIE, V36, P267; Aronsky D, 1998, Proc AMIA Symp, P632; Chapman W W, 2001, Proc AMIA Symp, P105; Chapman W W, 1998, Proc AMIA Symp, P587; FRIEDMAN C, 1994, J AM MED INFORM ASSN, V1, P161; Heinze D T, 2001, Proc AMIA Symp, P254; HRIPCSAK G, 1995, ANN INTERN MED, V122, P681; KAHAVI R, 1996, TOOLS ARTIFICIAL INT, P234; KRAUTHAMMER M, 2001, P AMIA S, P642; Peduzzi P, 1996, J CLIN EPIDEMIOL, V49, P1373, DOI 10.1016/S0895-4356(96)00236-3; POLLACK I, 1964, PSYCHON SCI, V1, P125; Quinlan J.R., 1993, C4 5 PROGRAMS MACHIN; Srinivasan P, 2001, Proc AMIA Symp, P642; Wilcox A, 1999, Proc AMIA Symp, P455	14	0	0	HANLEY & BELFUS INC MED PUBLISHERS	PHILADELPHIA	210 S 13TH ST, PHILADELPHIA, PA 19107 USA			1-56053-600-4				2002							519	522				4	Computer Science, Information Systems; Medical Informatics	Computer Science; Medical Informatics	BY60A	WOS:000189418100105	
B	Molla, M; Andreae, P; Glasner, J; Blattner, F; Shavlik, J		Caulfield, HJ; Chen, SH; Duro, R; Honavar, V; Kerre, EE; Lu, M; Romay, MG; Shih, TK; Ventura, D; Wang, PP; Yang, YY		Molla, M; Andreae, P; Glasner, J; Blattner, F; Shavlik, J			Interpreting microarray expression data using text annotating the genes	PROCEEDINGS OF THE 6TH JOINT CONFERENCE ON INFORMATION SCIENCES			English	Proceedings Paper	6th Joint Conference on Information Sciences	MAR 08-13, 2002	RES TRIANGLE PK, NC	Assoc Intelligent Machinery, Informat Sci Journal, Duke Univ, Acad Affairs, Tamkang Univ, N Carolina Biotechnol Ctr, GalxoSmithKline, George Mason Univ				Microarray expression data is being generated by the gigabyte all over the world with undoubted exponential increases to come. Annotated genomic data is also rapidly pouring into public databases. Our goal is to develop automated ways of combining these two sources of information to produce insight into the operation of cells under various conditions. Our approach is to use machine-learning techniques to identify characteristics of genes that are up-regulated or down-regulated in a particular microarray experiment. We seek models that are (a) accurate, (b) easy to interpret, and (c) stable to small variations in the training data. This paper explores the effectiveness of two standard machine-learning algorithms for this task: Naive Bayes (based on probability) and PFOIL (based on building rules). Although we do not anticipate using our learned models to predict expression levels of genes, we cast the task in a predictive framework, and evaluate the quality of the models in terms of their predictive power on genes held out from the training. The paper reports on experiments using actual E. coli microarray data, discussing the strengths and weaknesses of the two algorithms and demonstrating the trade-offs between accuracy, comprehensibility, and stability.	Univ Wisconsin, Dept Comp Sci, Madison, WI 53706 USA	Molla, M (reprint author), Univ Wisconsin, Dept Comp Sci, 1210 W Dayton St, Madison, WI 53706 USA.						Bairoch A, 2000, NUCLEIC ACIDS RES, V28, P45, DOI 10.1093/nar/28.1.45; Brown MPS, 2000, P NATL ACAD SCI USA, V97, P262, DOI 10.1073/pnas.97.1.262; Craven M.W., 1993, P 10 INT C MACH LEAR, P73; Dudoit S., 2000, STAT METHODS IDENTIF; Hearst M., 1999, P 37 ANN M ASS COMP; Jenssen TK, 2001, NAT GENET, V28, P21, DOI 10.1038/ng0501-21; Manning C.D., 1999, FDN STAT NATURAL LAN; Masys DR, 2001, BIOINFORMATICS, V17, P319, DOI 10.1093/bioinformatics/17.4.319; Mitchell T.M., 1997, MACHINE LEARNING; PORTER MF, 1980, PROGRAM-AUTOM LIBR, V14, P130, DOI 10.1108/eb046814; QUINLAN JR, 1990, MACH LEARN, V5, P239, DOI 10.1007/BF00117105; Xing EP, 2001, P 18 INT C MACH LEAR, P601	12	0	0	ASSOC INTELLIGENT MACHINERY	DURHAM	PO BOX 90291, DURHAM, NC 27708-0291 USA			0-9707890-1-7				2002							1224	1230				7	Computer Science, Artificial Intelligence; Computer Science, Information Systems; Computer Science, Software Engineering; Engineering, Electrical & Electronic	Computer Science; Engineering	BV54W	WOS:000179331800284	
B	Sadohara, K		Kumar, V; Tsumoto, S; Zhong, N; Yu, PS; Wu, XD		Sadohara, K			On a capacity control using Boolean kernels for the learning of Boolean functions	2002 IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS			English	Proceedings Paper	2nd IEEE International Conference on Data Mining	DEC 09-12, 2002	MAEBASHI CITY, JAPAN	IEEE Comp Soc Tech Comm Pattern Anal & Machine Intelligence, IEEE Comp Soc Tech Comm Computat Intelligence, Maebashi Convent Bur, Maebashi City Govt, Gunmma Prefecture Govt, Maebashi Inst Technol, AdIn Res Inc, Japan Res Inst Ltd, Support Ctr Adv Telecommun Technol Res, US AFOSR, AOARD, AROFE, SAS Inst Japan, SPSS Japan Inc, Kankodo, Syst Alpha Inc, GCC Inc, Yamato Inc, Daiei Dream Co Ltd				This paper concerns the classification task in discrete attribute spaces, but consider the task in a more fundamental framework: the learning of Boolean functions. The purpose of this paper is to present a new learning algorithm for Boolean functions called Boolean Kernel Classifier (BKC) employing capacity control using Boolean kernels. BKC uses Support Vector Machines (SVMs) as learning engines and Boolean kernels are primarily used for running SVMs in feature spaces spanned by conjunctions of Boolean literals. However another important role of Boolean kernels is to appropriately control the size of its hypothesis space to avoid overfitting. After applying a SVM to learn a classifier f in a feature space H induced by a Boolean kernel, BKC uses another Boolean kernel to compute the projections f(k) of f onto a subspace H-k of H spanned by conjunctions with length at most k. By evaluating the accuracy of f(k) on training data for any k, BKC can determine the smallest k such that f(k) is as accurate as f and learn another f' in H-k expected to have lower error for unseen data. By an empirical study on learning of randomly generated Boolean functions, it is shown that the capacity control is effective, and BKC outperforms C4.5 and naive Bayes classifiers.	Natl Inst Adv Ind Sci & Technol, AIST, Tsukuba, Ibaraki, Japan	Sadohara, K (reprint author), Natl Inst Adv Ind Sci & Technol, AIST, AIST Tsukuba Cent 2,1-1-1 Umezono, Tsukuba, Ibaraki, Japan.						Angluin D., 1988, Machine Learning, V2, DOI 10.1007/BF00116828; BELLARE M, 1993, COMPLEXITY NUMERICAL, P16; Cristianini N., 2000, INTRO SUPPORT VECTOR; Domingos P, 1999, DATA MIN KNOWL DISC, V3, P409, DOI 10.1023/A:1009868929893; Domingos P, 1997, MACH LEARN, V29, P103, DOI 10.1023/A:1007413511361; KEARNS M, 1987, P 19 ANN ACM S THEOR, P285, DOI 10.1145/28395.28426; KHARDON R, 2001, UIUCDCSR20012233 DEP; PITT L, 1988, J ACM, V35, P965, DOI 10.1145/48014.63140; PLATT J, 1988, ADV KERNAL METHODS S, P185; Quinlan J. R., 1988, P 5 INT C MACH LEARN, P135; SADOHARA K, 2001, LECT NOTES ARTIF INT, V2225, P106; Scholkopf B, 2002, LEARNING KERNELS; VALIANT LG, 1984, COMMUN ACM, V27, P1134, DOI 10.1145/1968.1972; Vapnik V., 1995, NATURE STAT LEARNING	14	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA			0-7695-1754-4				2002							410	417		10.1109/ICDM.2002.1183934		8	Computer Science, Artificial Intelligence	Computer Science	BV87U	WOS:000180274000052	
S	Sebe, N; Lew, MS; Cohen, I; Garg, A; Huang, TS		Kasturi, R; Laurendeau, D; Suen, C		Sebe, N; Lew, MS; Cohen, I; Garg, A; Huang, TS			Emotion recognition using a Cauchy naive Bayes classifier	16TH INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION, VOL I, PROCEEDINGS	INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION		English	Proceedings Paper	16th International Conference on Pattern Recognition (ICPR)	AUG 11-15, 2002	QUEBEC CITY, CANADA	Int Assoc Pattern Recognit, Canadian Image Processing & Pattern Recognit Soc, Ctr Rech Informat Montreal, Matrox Imaging, Ind & Commerce Quebec, Rech, Sci & Technol Quebec, Microsoft Res, Bell, Lab Vis & Syst Numer, Comp Vis & Syst Lab, Ctr Pattern Recognit & Machine Intelligence, Ctr Etudes Reconnaissance Formes & Intelligence Artificielle, Scribers, Coreco Imaging, Precarn			OPTICAL-FLOW; EXPRESSION	Recognizing human facial expression and emotion by computer is an interesting and challenging problem. It this paper we propose a method for recognizing emotions through facial expressions displayed in video sequences. We introduce the Cauchy Naive Bayes classifier which uses the Cauchy distribution as the model distribution and we provide a framework for choosing the best model distribution assumption. Our person-dependent and person-independent experiments show that the Cauchy distribution assumption typically provides better results than the Gaussian distribution assumption.	Leiden Inst Adv Comp Sci, NL-2333 CA Leiden, Netherlands	Sebe, N (reprint author), Leiden Inst Adv Comp Sci, Niels Bohrweg 1, NL-2333 CA Leiden, Netherlands.						Chen L. S., 1998, P IEEE WORKSH MULT S, P83; Chen L. S., 2000, THESIS U ILLINOIS UR; De Silva L. C., 1997, Proceedings of ICICS, 1997 International Conference on Information, Communications and Signal Processing. Theme: Trends in Information Systems Engineering and Wireless Multimedia Communications (Cat. No.97TH8237), DOI 10.1109/ICICS.1997.647126; Ekman P., 1978, FACIAL ACTION CODING; HAAS G, 1970, BIOMETRIKA, V57, P403, DOI 10.2307/2334849; MASE K, 1991, IEICE TRANS COMMUN, V74, P3474; Otsuka T, 1997, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL II, P546; Rosenblum M, 1996, IEEE T NEURAL NETWOR, V7, P1121, DOI 10.1109/72.536309; Sebe N, 2000, IEEE T PATTERN ANAL, V22, P1132, DOI 10.1109/34.879793; TAO H, 1998, CVPR, P735; Yacoob Y, 1996, IEEE T PATTERN ANAL, V18, P636, DOI 10.1109/34.506414	11	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA	1051-4651		0-7695-1695-X	INT C PATT RECOG			2002							17	20				4	Computer Science, Artificial Intelligence; Engineering, Biomedical; Engineering, Electrical & Electronic; Imaging Science & Photographic Technology	Computer Science; Engineering; Imaging Science & Photographic Technology	BV11L	WOS:000177847900005	
B	Shi, HB; Wang, ZH; Huang, HK; Jing, LP		Yuan, BZ; Tang, XF		Shi, HB; Wang, ZH; Huang, HK; Jing, LP			Text claslsification based on the TAN Model	2002 IEEE REGION 10 CONFERENCE ON COMPUTERS, COMMUNICATIONS, CONTROL AND POWER ENGINEERING, VOLS I-III, PROCEEDINGS			English	Proceedings Paper	IEEE Region 10 Technical Conference on Computers, Communications, Control and Power Engineering	OCT 28-31, 2002	BEIJING, PEOPLES R CHINA	IEEE Reg 10, IEEE Comp Soc Beijing Chapter, IEEE Commun Soc, IEEE Control Syst Soc Beijing Chapter, IEEE Power Engn Soc Beijing Chapter, IEEE Power Electr Soc Beijing Chapter, IEEE Signal Proc Soc Beijing Chapter, IEEE Beijing Chapter		text classification; TAN; naive Bayes; Bayesian network; feature selection		This paper proposes a text classification method based on TAN model. Naive Bayesian classifier is the most effective and popular. text classification method, but its attribute independence assumption makes it unable to express the dependence among text terms. TAN (Tree Augmented Naive Bayes) combines the simplicity of Naive Bayesian with the ability to express the dependence among attributes in Bayesian network. This paper reviews some existing text methods, introduces TAN model, and applies TAN model to text classification. Naive Bayesian and TAN classifiers are also compared by our experiments. Experimental results show TAN classifier has better performance.	No Jiaotong Univ, Sch Comp & Informat Technol, Beijing 100044, Peoples R China	Shi, HB (reprint author), No Jiaotong Univ, Sch Comp & Informat Technol, Beijing 100044, Peoples R China.						CHICKERING DM, 1996, AI STATV; Dumais S., 1998, Proceedings of the 1998 ACM CIKM International Conference on Information and Knowledge Management, DOI 10.1145/288627.288651; EAMONN JK, 1999, LEARNING AUGMENTED B; Friedman N, 1997, MACH LEARN, V29, P131, DOI 10.1023/A:1007465528199; Han E.H, 2001, LECT NOTES COMPUTER, V2035, P53; Joachims Th., 1998, EUR C MACH LEARN ECM, P137; Lam W., 1998, P 21 ANN INT ACM SIG, P81, DOI 10.1145/290941.290961; Langley P., 1992, P 10 NAT C ART INT, P223; MCCALLUM A, COMP EVENT MODELS NA; Pearl J., PROBABILISTIC REASON; YANG Y, 1993, P 16 ANN INT ACM SIG, P281, DOI 10.1145/160688.160738; Yang Y., 1999, J INFORMATION RETRIE, V1, P67	12	0	0	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA			0-7803-7490-8				2002							43	46		10.1109/TENCON.2002.1181210		4	Automation & Control Systems; Computer Science, Artificial Intelligence; Energy & Fuels; Engineering, Electrical & Electronic	Automation & Control Systems; Computer Science; Energy & Fuels; Engineering	BW21E	WOS:000181201500011	
B	Shi, HB; Huang, HK			IEEE; IEEE	Shi, HB; Huang, HK			Learning Tree-Augmented Naive Bayesian network by reduced space requirements	2002 INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND CYBERNETICS, VOLS 1-4, PROCEEDINGS			English	Proceedings Paper	International Conference on Machine Learning and Cybernetics	NOV 04-05, 2002	BEIJING, PEOPLES R CHINA	IEEE, SMC, Hebei Univ, Machine Learning Ctr		Bayesian network; TAN; conditional mutual information		Tree-Augmented Naive Bayesian network (TAN) based on Bayes theorem is a restricted Bayesian network. Its classification performance is superior to naive Bayes's, and its time complexity is much lower than general Bayesian network's. So TAN embodies a good tradeoff between the. quality of the approximation correlation among attributes and the computational complexity in the learning stage. However, its memory requirement is quadratic in the number of attributes, which restricts its application in high dimensional data. This paper proposes a new algorithm for constructing TAN, and proves the correctness of this algorithm. The space complexity of this new algorithm is linear in the number of attributes.	No Jiaotong Univ, Sch Comp & Informat Technol, Beijing 100044, Peoples R China	Shi, HB (reprint author), No Jiaotong Univ, Sch Comp & Informat Technol, Beijing 100044, Peoples R China.						Cheng J., 1999, P 15 C UNC ART INT U, P101; CHENG J, 1997, P 6 ACM INT C INF KN; CHICKERING DM, 1996, AI STATV; EAMONN EJ, 7 INT WORKSH AI STAT, P225; Friedman N, 1997, MACH LEARN, V29, P131, DOI 10.1023/A:1007465528199; Langley P., 1992, P 10 NAT C ART INT, P223; MEILA M, ACCELERATED CHOW LIU; Pearl J., PROBABILISTIC REASON; WARSZAWA, 2001, 925 ICS PAS	9	0	0	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA			0-7803-7508-4				2002							1232	1236				5	Automation & Control Systems; Computer Science, Artificial Intelligence; Computer Science, Cybernetics; Engineering, Electrical & Electronic	Automation & Control Systems; Computer Science; Engineering	BW27J	WOS:000181396300266	
B	Soonthornphisaj, N; Kijsirikul, B		Filipe, J; Sharp, B; Miranda, P		Soonthornphisaj, N; Kijsirikul, B			The effects of different feature sets on the web page categorization problem using the Iterative Cross-Training algorithm	ENTERPRISE INFORMATION SYSTEMS III			English	Proceedings Paper	3rd International Conference on Enterprise Information Systems (ICEIS 2001)	JUL 07-10, 2001	SETUBAL, PORTUGAL			web page categorization; Iterative Cross-Training; feature sets	TEXT CATEGORIZATION	The paper presents the effects of different feature sets oil the Web page categorization problem. These features are words appearing in the content of a Web page, words appearing on the hyperlinks, which link to the page and words appearing on every headings in the page. The experiments are conducted using a new algorithm called the Iterative Cross-Training algorithm (ICT) which was successfully applied to Thai Web page identification. The main concept of ICT is to iteratively train two sub-classifiers by using unlabeled examples in crossing manner. We compare ICT against supervised naive Bayes classifier and Co-Training classifier. The experimental results show that ICT obtains the highest performance and the-heading feature is considerably succeed in helping classifiers to build the correct model used in the Web page categorization task.	Chulalongkorn Univ, Dept Comp Engn, Machine Intelligence & Knowledge Discovery Lab, Bangkok 10330, Thailand	Soonthornphisaj, N (reprint author), Chulalongkorn Univ, Dept Comp Engn, Machine Intelligence & Knowledge Discovery Lab, Bangkok 10330, Thailand.						APTE C, 1994, ACM T INFORM SYST, V12, P233, DOI 10.1145/183422.183423; Blum A., 1998, P 11 ANN C COMP LEAR; Cohen WW, 1999, ACM T INFORM SYST, V17, P141, DOI 10.1145/306686.306688; Joachims T., 1998, P 10 EUR C MACH LEAR; KIJSIRIKUL B, 2000, P PAC RIM INT C ART, P690; Mitchell T.M., 1997, MACHINE LEARNING; Nigam K, 2000, MACH LEARN, V39, P103, DOI 10.1023/A:1007692713085	7	0	0	SPRINGER	DORDRECHT	PO BOX 17, 3300 AA DORDRECHT, NETHERLANDS			1-4020-0563-6				2002							132	138				7	Computer Science, Artificial Intelligence; Computer Science, Information Systems; Engineering, Electrical & Electronic	Computer Science; Engineering	BU54N	WOS:000176288100016	
B	Wang, ZH; Webb, GI		Kumar, V; Tsumoto, S; Zhong, N; Yu, PS; Wu, XD		Wang, ZH; Webb, GI			Comparison of lazy Bayesian rule and tree-augmented Bayesian learning	2002 IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS			English	Proceedings Paper	2nd IEEE International Conference on Data Mining	DEC 09-12, 2002	MAEBASHI CITY, JAPAN	IEEE Comp Soc Tech Comm Pattern Anal & Machine Intelligence, IEEE Comp Soc Tech Comm Computat Intelligence, Maebashi Convent Bur, Maebashi City Govt, Gunmma Prefecture Govt, Maebashi Inst Technol, AdIn Res Inc, Japan Res Inst Ltd, Support Ctr Adv Telecommun Technol Res, US AFOSR, AOARD, AROFE, SAS Inst Japan, SPSS Japan Inc, Kankodo, Syst Alpha Inc, GCC Inc, Yamato Inc, Daiei Dream Co Ltd				The naive Bayes classifier is widely used in interactive applications due to its computational efficiency, direct theoretical base, and competitive accuracy. However its attribute independence assumption can result in sub-optimal accuracy. A number of techniques have explored simple relaxations of the attribute independence assumption in order to increase accuracy. Among these, the lazy Bayesian rule (LBR) and the tree-augmented naive Bayes (TAN) have demonstrated strong prediction accuracy. However their relative performance has never been evaluated. This paper compares and contrasts these two techniques, finding that they have comparable accuracy and hence should be selected according to computational profile. LBR is desirable when small numbers of objects are to be classified while TAN is desirable when large numbers of objects are to be classified.	Deakin Univ, Sch Informat Technol, Geelong, Vic 3125, Australia	Wang, ZH (reprint author), Deakin Univ, Sch Informat Technol, Geelong, Vic 3125, Australia.						AHA DW, 1997, ARTIF INTELL, P7; CERQUIDES J, 1999, P INT C KNOWL DISC D, P292, DOI 10.1145/312129.312256; CHENG J, 1999, P 15 C UNC ART INT U; Domingos P., 1996, P 13 INT C MACH LEAR, P105; Friedman N, 1997, MACH LEARN, V29, P131, DOI 10.1023/A:1007465528199; FRIEDMAN N, 1996, P 13 NAT C ART INT, P717; Keogh E., 1999, P 7 INT WORKSH ART I, P225; Kohavi R., 1996, P 2 INT C KNOWL DISC, P202; Kohavi R, 1997, ARTIF INTELL, V97, P273, DOI 10.1016/S0004-3702(97)00043-X; KONENKO I, 1991, P EUR C ART INT, P206; Langley P., 1994, P 10 C UNC ART INT, P339; Mitchell T.M., 1997, MACHINE LEARNING; PAZZANI MJ, 1996, P INF STAT IND SCI; SAHAMI M, 1996, LEARNING LTD DEPENDE, P335; WITTEN IH, 2000, LEARNING TOOLS TECHN; ZHANG H, 2001, P 18 INT C MACH LEAR; Zheng Z., 1999, P 16 INT C MACH LEAR, P493; Zheng ZJ, 2000, MACH LEARN, V41, P53, DOI 10.1023/A:1007613203719	18	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA			0-7695-1754-4				2002							490	497				8	Computer Science, Artificial Intelligence	Computer Science	BV87U	WOS:000180274000062	
B	Zhang, HW; Lu, YC		Yuan, BZ; Tang, XF		Zhang, HW; Lu, YC			Learning Bayesian network Classi$ers from data with missing values	2002 IEEE REGION 10 CONFERENCE ON COMPUTERS, COMMUNICATIONS, CONTROL AND POWER ENGINEERING, VOLS I-III, PROCEEDINGS			English	Proceedings Paper	IEEE Region 10 Technical Conference on Computers, Communications, Control and Power Engineering	OCT 28-31, 2002	BEIJING, PEOPLES R CHINA	IEEE Reg 10, IEEE Comp Soc Beijing Chapter, IEEE Commun Soc, IEEE Control Syst Soc Beijing Chapter, IEEE Power Engn Soc Beijing Chapter, IEEE Power Electr Soc Beijing Chapter, IEEE Signal Proc Soc Beijing Chapter, IEEE Beijing Chapter		Bayesian network classifier; data mining; incomplete data		Learning accurate Bayesian network (BN) classifiers from complete databases is a very active research topic in Data Mining and Machine Learning. However, in practice, databases are rarely complete. This affects their real world data mining applications. This paper investigates the methods for learning four types well-known Bayesian network classifiers from incomplete databases. These four types BN classifiers are Naive-Bayes, Tree augmented Naive-Bayes, BN augmented Naive-Bayes, and general BN, where the latter two are learned using dependency analysis based algorithms that work only on the Database Completeness assumption. In order to enable this kind algorithims to handle with missing data, this paper introduces a novel deterministic method to estimate the (Conditional) Mutual Information from incomplete databases, which can be used to do CI tests, a fundamental step in the dependency analysis based algorithms. The experimental results show that our algorithm is efficient and reliable.	Tsing Hua Univ, Dept Comp Sci & Technol, State Key Lab Intelligent Technol & Syst, Beijing 100084, Peoples R China	Zhang, HW (reprint author), Tsing Hua Univ, Dept Comp Sci & Technol, State Key Lab Intelligent Technol & Syst, Beijing 100084, Peoples R China.						Buntine W, 1996, IEEE T KNOWL DATA EN, V8, P195, DOI 10.1109/69.494161; CHENG J, 1999, 5 C UNC ART INT; CHENG J, 1998, LEARNING BAYESIAN NE; CHOW CK, 1968, IEEE T INFORM THEORY, V14, P462, DOI 10.1109/TIT.1968.1054142; FRIEDMAN N, 1998, 14 C UNC ART INT; Friedman N, 1997, MACH LEARN, V29, P131, DOI 10.1023/A:1007465528199; Langley P., 1992, P 10 NAT C ART INT, P223; MYERS J, 1999, 5 C UNC ART INT UAI; RAMONI M, 1997, 2 INT S INT DAT AN; RAMONI M, 1997, KMITR41 OP U; RAMONI M, 1997, 13 C UNC ART INT; SINGH M, 1997, 14 NAT C ART INT JUL	12	0	0	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA			0-7803-7490-8				2002							35	38				4	Automation & Control Systems; Computer Science, Artificial Intelligence; Energy & Fuels; Engineering, Electrical & Electronic	Automation & Control Systems; Computer Science; Energy & Fuels; Engineering	BW21E	WOS:000181201500009	
S	Akaho, S		Dorffner, G; Bischof, H; Hornik, K		Akaho, S			Conditionally independent component extraction for naive Bayes inference	ARTIFICIAL NEURAL NETWORKS-ICANN 2001, PROCEEDINGS	LECTURE NOTES IN COMPUTER SCIENCE		English	Article; Proceedings Paper	International Conference on Artificial Neural Networks (ICANN 2001)	AUG 21-25, 2001	VIENNA, AUSTRIA	Austrian Res Inst Artifical Intelligence	VIENNA UNIV TECHNOL			This paper extends the framework of independent component analysis (ICA) to supervised learning, The key idea is to find a conditionally independent representation of input variables for given output. The representation is useful for the naive Bayes learning which has been reported to perform as well as more sophisticated methods. The learning algorithm is derived in a similar criterion to ICA. Two dimensional entropy takes an important role, while one dimensional entropy does in ICA.	AIST, Neurosci Res Inst, Tsukuba, Ibaraki 3058568, Japan	Akaho, S (reprint author), AIST, Neurosci Res Inst, Cent 2,1-1-1 Umezono, Tsukuba, Ibaraki 3058568, Japan.						Akaho S., 1999, P 1999 INT JOINT C N, P927; BECKER S, 1996, NETWORK COMPUTATION, V7; Bell AJ, 1997, VISION RES, V37, P3327, DOI 10.1016/S0042-6989(97)00121-1; Cowell R. G., 1999, PROBABILISTIC NETWOR; Domingos P, 1997, MACH LEARN, V29, P103, DOI 10.1023/A:1007413511361; Frank E, 2000, MACH LEARN, V41, P5, DOI 10.1023/A:1007670802811; Friedman JH, 1997, DATA MIN KNOWL DISC, V1, P55, DOI 10.1023/A:1009778005914; Simonoff J.S., 1998, SMOOTHING METHODS ST; Yang HH, 1997, NEURAL COMPUT, V9, P1457, DOI 10.1162/neco.1997.9.7.1457	9	0	0	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		3-540-42486-5	LECT NOTES COMPUT SC			2001	2130						535	540				6	Computer Science, Artificial Intelligence; Computer Science, Theory & Methods	Computer Science	BT43Y	WOS:000173024600074	
B	Browning, J; Miller, DJ		Miller, DJ; Adali, T; Larsen, J; VanHulle, M; Douglas, S		Browning, J; Miller, DJ			A maximum entropy approach for collaborative filtering	NEURAL NETWORKS FOR SIGNAL PROCESSING XI			English	Proceedings Paper	11th IEEE Workshop on Neural Networks for Signal Processing (NNSP 2001)	SEP 10-12, 2001	FALMOUTH, MA	IEEE Signal Processing Soc			INFERENCE	Collaborative filtering (CF) involves predicting the preferences of a user for a set of items given partial knowledge of the user's preferences for other items, while leveraging a database of profiles for other users. CF has applications e.g. in predicting Web sites a person will visit and in automated searching of document databases. Fundamentally, CF is a pattern recognition task, but a formidable one, often involving a huge feature space, a large data set, and many missing features. Even more daunting is the fact that a CF inference engine must be capable of predicting any (user-selected) items, given any available set of partial knowledge on the user's other preferences. In other words, the model must be designed to solve any of a huge (combinatoric) set of possible inference tasks. CF techniques include memory-based, classification-based, and statistical modelling approaches. Among these, modelling approaches scale best with large data sets and are the most adept at handling missing features. The disadvantage of these methods lies in the statistical assumptions (e.g. feature independence), which may be unjustified. To address this shortcoming we propose a new model-based CF method, based on the maximum entropy principle. For the MS Web application, the new method is demonstrated to outperform a number of CF approaches, including naive Bayes and latent variable (cluster) models, support vector machines (SVMs), and the (Pearson) correlation method.	Penn State Univ, Dept Elect Engn, University Pk, PA 16802 USA	Browning, J (reprint author), Penn State Univ, Dept Elect Engn, University Pk, PA 16802 USA.						ARWAR B, IN PRESS CM E COMM 2; Berger AL, 1996, COMPUT LINGUIST, V22, P39; Breese J. S., 1998, P 14 C UNC ART INT; DellaPietra S, 1997, IEEE T PATTERN ANAL, V19, P380, DOI 10.1109/34.588021; FISHER D, SWAMI FRAMEWORK COLL; HOFFMAN T, 1999, P IJCAI 99; HOFFMAN T, 1999, P 22 ANN INT ACM C R; Jaynes E. T., 1989, PAPERS PROBABILITY S; Joachims T., 1999, ADV KERNEL METHODS S; Miller DJ, 2000, NEURAL COMPUT, V12, P2175, DOI 10.1162/089976600300015105; Nigam K, 1999, P IJCAI 99 WORKSH MA; Yan L, 2000, IEEE T NEURAL NETWOR, V11, P558, DOI 10.1109/72.846727	12	0	0	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA			0-7803-7196-8				2001							3	12		10.1109/NNSP.2001.943105		10	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic; Mathematics, Applied; Statistics & Probability	Computer Science; Engineering; Mathematics	BT15K	WOS:000172099600001	
B	Cleary, JG; Trigg, LE; Holmes, G; Hall, M		Bramer, M; Preece, A; Coenen, F		Cleary, JG; Trigg, LE; Holmes, G; Hall, M			Experiences with a weighted decision tree learner	RESEARCH AND DEVELOPMENT IN INTELLIGENT SYSTEMS XVII	B C S CONFERENCE SERIES		English	Proceedings Paper	20th SGES International Conference on Knowledge Based Systems and Applied Artificial Intelligence (ES2000)	DEC 11-13, 2000	CAMBRIDGE, ENGLAND	SGES, British Comp Soc				Machine learning algorithms for inferring decision trees typically choose a single "best" tree to describe the training data. Recent research has shown that classification performance can be significantly improved by voting predictions of multiple, independently produced decision trees. This paper describes an algorithm, OB1, that produces a weighted sum over many possible models. Model weights are determined by the prior probability of the model, as well as the performance of the model during training. We describe an implementation of OB1 that includes all possible decision trees as well as naive Bayesian models within a single option tree. Constructing all possible decision trees is very expensive? growing exponentially in the number of attributes. However it is possible to use the internal structure of the option tree to avoid recomputing values. In addition, the current implementation allows the option tree to be depth bounded. OB1 is compared with a number of other decision tree and instance based learning algorithms using a selection of data sets from the UCI repository and a maximum option tree depth of three attributes. Both information gain and percentage correct are used for the comparison. For the information gain measure OB1 performs significantly better than the other algorithms. When using percentage correct OB1 is significantly better than all the algorithms except naive Bayes and boosted C5.0(1), which perform slightly worse than OB1.	Univ Waikato, Dept Comp Sci, Hamilton, New Zealand	Cleary, JG (reprint author), Univ Waikato, Dept Comp Sci, Hamilton, New Zealand.						AHA DW, 1991, MACH LEARN, V6, P37, DOI 10.1023/A:1022689900470; AUER P, 1995, P 12 INT C MACH LEAR; BAXTER RA, 1994, 207 MON U DEP COMP S; BUNTINE W, 1991, P 1991 INT JOINT C A; Cleary J. G., 1996, Proceedings ofthe Conference, ISIS '96. Information, Statistics and Induction in Science; FAYYAD UM, 1992, MACH LEARN, V8, P87, DOI 10.1023/A:1022638503176; Holmes G., 1994, P 2 AUSTR NZ C INT I; Langley P., 1992, P 10 NAT C ART INT, P223; Merz CJ, 1996, UCI REPOSITORY MACHI; OATES WJ, 1957, STOIC EPICUREAN PHIL; Quinlan JR, 1994, C4 5 PROGRAMS MACHIN; Schapire R. E., 1997, P 14 INT C MACH LEAR, P322; VOLF PAJ, 1994, THESIS EINDHOVEN U T; WILLEMS FMJ, 1995, IEEE T INFORM THEORY, V41, P653, DOI 10.1109/18.382012	14	0	0	SPRINGER-VERLAG LONDON LTD	GODALMING	SWEETAPPLE HOUSE CATTESHALL RD FARNCOMBE, GODALMING GU7 1NH, SURREY, ENGLAND			1-85233-403-7	BCS CONF SERIES			2001							35	47				13	Computer Science, Artificial Intelligence	Computer Science	BR86G	WOS:000167857200003	
B	Gomes, FC; Matwin, S; Leitao, F		Valafar, F		Gomes, FC; Matwin, S; Leitao, F			Comparing diagnostic models for CPA tumors	METMBS'01: PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON MATHEMATICS AND ENGINEERING TECHNIQUES IN MEDICINE AND BIOLOGICAL SCIENCES			English	Proceedings Paper	International Conference on Mathematics and Engineering Techniques in Medicine and Biological Sciences	JUN 25-28, 2001	LAS VEGAS, NV	Comp Sci Res, Educ & Appl Press, Int Technol Inst, Korea Informat Proc Soc, World Acad Sci Informat Technol, PACT Corp, Comp Vis Res & Appl Tech, Java High Performance Comp Res Grp, Sundance Digital Signal Proc Inc		machine learning methods; data mining; cerebellum pontine angle tumor diagnosis		In this work we are concerned with comparing learning classification procedures generated from known medical cases. More precisely, we have obtained diagnosis models that discriminate between Cerebellum Pontine Angle (CPA) tumors and Otorhinolaryngological (ENT) disorders and compared them according to: accuracy, false negative minimization, and the interpretability of the model. These models are generated by means of five different learning methods: Support Vector Machine, Naive Bayes, C4.5, CART, and OneR. In order to differentiate CPA tumors from other disorders one must Perform a clinical-neuro logical examination together with expensive radiological imagery (CT and MRI) and/or invasive exams. This comparison shows how to obtain useful diagnostic models based only on clinical data, without the need of such expensive complementary exams.	Univ Ottawa, SITE, Ottawa, ON, Canada	Gomes, FC (reprint author), Univ Ottawa, SITE, Ottawa, ON, Canada.						Breiman L., 1984, CLASSIFICATION REGRE; Burgess C., 1998, DATA MIN KNOWL DISC, V2, P1, DOI DOI 10.1023/A:1009715923555; Domingos P, 1997, MACH LEARN, V29, P103, DOI 10.1023/A:1007413511361; HOLTE RC, 1993, MACH LEARN, V11, P63, DOI 10.1023/A:1022631118932; Joachims T., 1999, ADV KERNEL METHODS S; Lang J, 1991, CLIN ANATOMY POSTERI; Morik K., 1999, P 16 INT C MACH LEAR, P268; Quinlan J.R., 1993, C4 5 PROGRAMS MACHIN; Vapnik V., 1999, NATURE STAT LEARNING; VAPNIK VN, 1971, THEOR PROBAB APPL+, V16, P264, DOI 10.1137/1116025; Witten I.H., 2000, DATA MINING PRACTICA	11	0	0	C S R E A PRESS	ATHENS	115 AVALON DR, ATHENS, GA 30606 USA			1-892512-77-7				2001							432	438				7	Biochemical Research Methods; Computer Science, Interdisciplinary Applications; Engineering, Biomedical	Biochemistry & Molecular Biology; Computer Science; Engineering	BT48G	WOS:000173108700070	
B	Gong, XJ; Shi, ZZ		Zhong, YX; Cui, S; Wang, Y		Gong, XJ; Shi, ZZ			Web mining based on Bayes latent semantic model	2001 INTERNATIONAL CONFERENCES ON INFO-TECH AND INFO-NET PROCEEDINGS, CONFERENCE A-G: INFO-TECH & INFO-NET: A KEY TO BETTER LIFE			English	Proceedings Paper	International Conference on Info-Tech and Info-Net (ICII 2001)	OCT 29-NOV 01, 2001	BEIJING, PEOPLES R CHINA	China Assoc Sci & Technol, IEEE Beijing Ctr, ATM Forume, Beijing Internet Inst, IEEE Communicat Soc, IEEE Comp Soc, IEEE Control Soc, Global Informat Infrastruct Commiss, World Federat Engn Org, Int Federat Informat Proc, Internet Engn Task Force, Int Council Comp Commun, Chinese Inst Electr, Beijing Univ Posts & Telecommunicat		Bayes latent semantic analysis; semi-supervised learning; expectation maximization; web mining		With the increasing of information on Internet, web mining has been the focus of data mining. In this paper, we put forwards a semi-supervised learning strategy consisted of two stages. First stage labels the documents that include latent class variables by using Bayes latent semantic model; at the second stage, based on the results from first stage, we label the documents excluding latent class variables with the naive Bayes models, Experimental results show that this algorithm has a good precision and recall rate.	Chinese Acad Sci, Inst Comp, Lab Intelligent Informat Proc, Beijing 100864, Peoples R China	Gong, XJ (reprint author), Chinese Acad Sci, Inst Comp, Lab Intelligent Informat Proc, Beijing 100864, Peoples R China.						CHICKERING DM, 1996, MSRTR9608; Deerwester S., 1990, J AM SOC INFORM SCI, P41; HUANG L, SURVEY WEB INFORMATI; LI XL, 2000, COMPUTER RES DEV, V37, P1032; Nigam K., 1998, Proceedings Fifteenth National Conference on Artificial Intelligence (AAAI-98). Tenth Conference on Innovative Applications of Artificial Intelligence; SHI ZZ, 2000, KNOWLEDGE DISCOVERY; VAITHYANATHAN S, 2000, P INT WORKSH TEXT WE	7	0	0	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA			7-900081-58-5				2001							C52	C57				6	Computer Science, Artificial Intelligence; Computer Science, Information Systems; Computer Science, Software Engineering; Engineering, Electrical & Electronic; Remote Sensing; Telecommunications	Computer Science; Engineering; Remote Sensing; Telecommunications	BU94X	WOS:000177471800217	
B	Langari, Z; Tompa, FW		Cercone, N; Lin, TY; Wi, XD		Langari, Z; Tompa, FW			Subject classification in the Oxford English Dictionary	2001 IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS			English	Proceedings Paper	IEEE International Conference on Data Mining	NOV 29-DEC 02, 2001	SAN JOSE, CA	IEEE Comp Soc, TCPAMI, IEEE Comp Soc, TFVI, Insightful Corp, Microsoft Res, NARAX Inc, Springer Verlag, New York, StatSoft Inc			TEXT	The Oxford English Dictionary is a valuable source of lexical information and a rich testing ground for mining highly structured text. Each entry is organized into a hierarchy of senses, which include definitions, labels and cited quotations. Subject labels distinguish the subject classification of a sense, for example they signal how a word may be used in Anthropology, Music or Computing. Unfortunately subject labeling in the dictionary is incomplete. To overcome this incompleteness, we attempt to classify the senses (i.e., definitions) in the dictionary by their subjects, using the citations as an information guide. We report on four different approaches: k Nearest Neighbors, a standard classification technique; Term Weighting, an information retrieval method dealing with text; Naive Bayes, a probabilistic method; and Expectation Maximization, an iterative probabilistic method. Experimental performance of these methods is compared based on standard classification metrics.	Univ Waterloo, Dept Comp Sci, Waterloo, ON N2L 3G1, Canada	Langari, Z (reprint author), Univ Waterloo, Dept Comp Sci, Waterloo, ON N2L 3G1, Canada.						BERG DL, 1989, RES POTENTIAL ELECT; Berg Donna Lee, 1993, GUIDE OXFORD ENGLISH; Brin S., 1998, P WEBDB WORKSH EDBT; Cios K., 1998, DATA MINING METHODS; GALE WA, 1994, GOOD TURING SMOOTHIN; IDE N, 1994, MACHINE TRANSLATION, P19; Iwayama M., 1995, P 18 ANN INT ACM SIG, P273, DOI 10.1145/215206.215371; Joachims T., 1997, P 14 INT C MACH LEAR; KORFHAGE R, 1997, INFORMATION STORAGE; LANGARI Z, 2001, THESIS U WATERLOO; LEWIS D, 1998, EUR C MACH LEARN; Lewis D. D., 1991, P SPEECH NAT LANG WO, P312, DOI 10.3115/112405.112471; Lewis D.D., 1992, 15 ANN INT ACM SIGIR, P37; Manning C.D., 1999, FDN STAT NATURAL LAN; MCCALLUM A, 1999, ACL 99 WORKSH UNS LE; McCallum A, 1998, AAAI 98 WORKSH LEARN; Nigam K, 2000, MACH LEARN, V39, P103, DOI 10.1023/A:1007692713085; SALTON G, 1988, INFORM PROCESS MANAG, V24, P513, DOI 10.1016/0306-4573(88)90021-0; SEBASTIANI F, 1999, MACHINE LEARNING AUT; SIEDL T, 1998, P ACM SIGMOD JUN, P154; Simpson J. A., 1989, OXFORD ENGLISH DICT; Yang Y., 1999, P 22 ANN INT ACM SIG, P42, DOI 10.1145/312624.312647; YATES RB, 1999, MODERN INFORMATION R	23	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA			0-7695-1119-8				2001							329	336				8	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Computer Science; Engineering	BT50G	WOS:000173158200042	
S	Ledezma, A; Aler, R; Borrajo, D			IEEE COMPUTER SOCIETY; IEEE COMPUTER SOCIETY	Ledezma, A; Aler, R; Borrajo, D			Empirical study of a stacking state-space	ICTAI 2001: 13TH IEEE INTERNATIONAL CONFERENCE ON TOOLS WITH ARTIFICIAL INTELLIGENCE, PROCEEDINGS	Proceedings - International Conference on Tools With Artificial Intelligence		English	Proceedings Paper	13th IEEE International Conference on Tools with Artificial Intelligence (ICTAI 2001)	NOV 07-09, 2001	DALLAS, TX	IEEE Comp Soc, IEEE CS Virtual Intelligence Task Force, Informat Technol Res Inst, Wright State Univ, AIIS Inc				Nowadays, there is no doubt that machine learning techniques can be successfully applied to data mining tasks. Currently, the combination of several classifiers is one of the most active fields within inductive machine learning. Examples of such techniques are boosting, bagging and stacking. From these three techniques, stacking is perhaps the less used one. One of the main reasons for this relates to the difficulty to define and parameterize its components: selecting which combination of base classifiers to use, and which classifier to use as the meta-classifier One could use for that purpose simple search methods (e.g. hill climbing), or more complex ones (e.g. genetic algorithms). But before search is attempted, it is important to know the properties of the search space itself In this paper we study exhaustively the space of Stacking systems that can be built by using four base learning systems: C4.5, IB1, Naive Bayes, and PART The results that have been obtained in this paper will be useful for designing new Stacking-based algorithms and tools.	Univ Carlos III Madrid, Leganes 28911, Madrid, Spain	Ledezma, A (reprint author), Univ Carlos III Madrid, Avda Univ 30, Leganes 28911, Madrid, Spain.	ledezma@inf.uc3m.es; aler@inf.uc3m.es; dborrajo@ia.uc3m.es					AHA DW, 1991, MACH LEARN, V6, P37, DOI 10.1023/A:1022689900470; BENNETT KP, 2000, P 17 INT C MACH LEAR, P65; Blake C. L., 1998, UCI REPOSITORY MACHI; Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.1023/A:1018054314350; Chan PK, 1995, P 12 INT C MACH LEAR, P90; FAN D, 1999, P ICML 99 WORKSH REC, P10; FAN DW, 1996, P AAAI 96 WORKSH INT, P40; Frank E., 1998, P 15 INT C MACH LEAR, P144; Freund Y., 1995, P 2 EUR C COMP LEARN, P23; JOHN GH, 1995, P 11 C UNC ART INT, P338; KOHAVI R, 1995, P 1 INT C KNOWL DISC; LEBLANC M, 1993, 9318 U TOR DEP STAT; LEDEZMA A, 2001, IN PRESS DATA MINING; Merz CJ, 1999, MACH LEARN, V36, P33, DOI 10.1023/A:1007559205422; Quinlan JR, 1996, PROCEEDINGS OF THE THIRTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND THE EIGHTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE, VOLS 1 AND 2, P725; Quinlan J.R., 1993, C4 5 PROGRAMS MACHIN; TING K, 1997, P INT JOINT C ART IN; Witten I.H., 2000, DATA MINING PRACTICA; WOLPERT DH, 1992, NEURAL NETWORKS, V5, P241, DOI 10.1016/S0893-6080(05)80023-1	19	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA	1082-3409		0-7695-1417-0	PROC INT C TOOLS ART			2001							210	217				8	Computer Science, Artificial Intelligence	Computer Science	BU48K	WOS:000176125600026	
B	Maragoudakis, M; Kavallieratou, E; Fakotakis, N; Kokkinakis, G			IEEE COMPUTER SOCIETY; IEEE COMPUTER SOCIETY	Maragoudakis, M; Kavallieratou, E; Fakotakis, N; Kokkinakis, G			How conditional independence assumption affects handwritten character segmentation	SIXTH INTERNATIONAL CONFERENCE ON DOCUMENT ANALYSIS AND RECOGNITION, PROCEEDINGS			English	Proceedings Paper	6th International Conference on Document Analysis and Recognition (ICDAR)	SEP 10-13, 2001	SEATTLE, WA	Int Assoc Pattern Recognit				This paper deals with the rise of Bayesian Belief Networks in order to improve the accuracy and training time of character segmentation for unconstrained handwritten text. Comparative experimental results have been evaluated against Naive Bayes classification, which is hosed on the assumption of the independence of the parameters and two additional previous commonly cued methods. Results hate depicted that obtaining the inferential dependencies of the training data, could lead to the reduction of the required training time and size by a factor of 55%. Moreover, the achieved accuracy in detecting segment boundaries exceeds 86% whereas limited training data are proved to endow with very satisfactory results.	Univ Patras, Dept Elect & Comp Engn, GR-26500 Patras, Greece	Maragoudakis, M (reprint author), Univ Patras, Dept Elect & Comp Engn, GR-26500 Patras, Greece.	mmarag@wcl.e.upatras.gr; ergina@wcl.ee.upatras.gr; fakotaki@wcl.ee.upatras.gr; gkokkin@wcl.ee.upatras.gr					COOPER GF, 1992, MACH LEARN, V9, P309, DOI 10.1007/BF00994110; GLYMOUR C, 1999, COMPUTATION; Jeffreys H, 1939, THEORY PROBABILITY; Jensen F. V., 1996, INTRO BAYESIAN NETWO; Kavallieratou E., 2000, Proceedings 15th International Conference on Pattern Recognition. ICPR-2000, DOI 10.1109/ICPR.2000.906155; Kavallieratou E, 2000, PATTERN RECOGN, V33, P1261, DOI 10.1016/S0031-3203(99)00219-8; KAVALLIERATOU E, UNPUB OFF LINE UNCON; KAVALLIERATOU E, 1999, P ICECS, P1159; Kohonen T., 1987, SELF ORG ASS MEMORY; Pearl J., 1988, PROBABILISTIC REASON	10	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA			0-7695-1263-1				2001							246	250				5	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Computer Science; Engineering	BT06N	WOS:000171845000048	
S	Petridis, V; Kaburlasos, VG; Fragkou, P; Kehagias, A			IEEE; IEEE; IEEE; IEEE	Petridis, V; Kaburlasos, VG; Fragkou, P; Kehagias, A			Text classification using the sigma-FLNMAP neural network	IJCNN'01: INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS, VOLS 1-4, PROCEEDINGS	IEEE International Joint Conference on Neural Networks (IJCNN)		English	Proceedings Paper	International Joint Conference on Neural Networks (IJCNN 01)	JUL 15-19, 2001	WASHINGTON, D.C.	Int Neural Network Soc, IEEE, Neural Networks Council				A novel neural network, namely sigma Fuzzy Lattice Neural network with MAPping or sigma-FLNMAP for short, is presented and applied to classification of text (documents) from the Brown Corpus benchmark collection of documents. The sigma-FLNMAP is presented here as an enhanced extension of the fuzzy-ARTMAP neural network in the framework of fuzzy lattices. An individual sigma-FLNMAP's classification accuracy is improved by training an ensemble of sigma-FLNMAP modules on different permutations of the training data. Several different vector representations of a document are employed. The results, in a series of experiments, compare favorably with the results by other classification algorithms including K-Nearest Neighbor and Naive Bayes Classifiers.	Aristotelian Univ Thessaloniki, Fac Engn, GR-54006 Thessaloniki, Greece	Petridis, V (reprint author), Aristotelian Univ Thessaloniki, Fac Engn, GR-54006 Thessaloniki, Greece.						BREIMAN L, 1994, 421 U CAL BERK DEPT; CARPENTER KE, 1992, HARVARD LIBR BULL, V3, P5; Drucker H, 1999, IEEE T NEURAL NETWOR, V10, P1048, DOI 10.1109/72.788645; Duda R.O., 2001, PATTERN CLASSIFICATI; Fujii Atsushi, 2000, P 38 ANN M ASS COMP, P488, DOI 10.3115/1075218.1075280; Kaburlasos VG, 2000, NEURAL NETWORKS, V13, P1145, DOI 10.1016/S0893-6080(00)00074-5; Kosala R., 2000, ACM SIGKDD EXPLORATI, V2, P1, DOI DOI 10.1145/360402.360406; Kucera H., 1982, FREQUENCY ANAL ENGLI; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lewis DD, 1996, COMMUN ACM, V39, P92, DOI 10.1145/234173.234210; Manning C.D., 1999, FDN STAT NATURAL LAN; Miller George A., 1990, INT J LEXICOGR, V3, P235, DOI DOI 10.1093/IJL/3.4.235; Mitchell T.M., 1997, MACHINE LEARNING; PETRIDIS V, 2001, IEEE T KNOWLEDGE DAT, V13; SCHAEFER P, 1990, EARTH ISL J, V5, P2; Vapnik V., 1995, NATURE STAT LEARNING	16	0	0	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA	1098-7576		0-7803-7044-9	IEEE IJCNN			2001							1362	1367				6	Computer Science, Artificial Intelligence; Computer Science, Information Systems; Computer Science, Theory & Methods	Computer Science	BT36Q	WOS:000172784800242	
S	Taghva, K; Nartker, T; Borsack, J; Lumos, S; Condit, A; Young, R		Kantor, PB; Lopresti, DP; Zhou, J		Taghva, K; Nartker, T; Borsack, J; Lumos, S; Condit, A; Young, R			Evaluating text categorization in the presence of OCR errors	DOCUMENT RECOGNITION AND RETRIEVAL VIII	PROCEEDINGS OF THE SOCIETY OF PHOTO-OPTICAL INSTRUMENTATION ENGINEERS (SPIE)		English	Proceedings Paper	8th Annual Document Recognition and Retrieval Conference	JAN 24-25, 2001	SAN JOSE, CA	Soc Imaging Sci & Technol, SPIE		OCR; errors; text categorization; naive Bayes model; categorization; classification	MODEL	In this paper we describe experiments that investigate the effects of OCR errors on text categorization. In particular, we show that in our environment. OCR errors have no effect on categorization when we use a classifier based on the naive Bayes model. We also observe that dimensionality reduction techniques eliminate a large number of OCR errors and improve categorization results.	Univ Nevada, Informat Sci Res Inst, Las Vegas, NV 89154 USA	Taghva, K (reprint author), Univ Nevada, Informat Sci Res Inst, Las Vegas, NV 89154 USA.						COMMISSION NR, 1996, REGULATORDY GUIDE 3; Cover T. M., 1991, ELEMENTS INFORMATION; Hayes P. J., 1990, P CAIA 90 6 IEEE C A, P320; Ittner DJ, 1995, P SDAIR 95 4 ANN S D, P301; Joachims T., 1998, P 10 EUR C MACH LEAR, P137; Lewis D., 1994, P 3 ANN S DOC AN INF, P81; Lewis David D., 1998, P 10 EUR C MACH LEAR, P4; Lewis D. D., 1999, Proceedings of SIGIR '99. 22nd International Conference on Research and Development in Information Retrieval, DOI 10.1145/312624.313469; LEWIS K, 1992, CAREER DEV EXCEPTION, V15, P37; MARON ME, 1961, J ACM, V8, P404, DOI 10.1145/321075.321084; MARON ME, 1960, J ACM, V7, P216, DOI 10.1145/321033.321035; MCCALLUM A, 1998, AAAI98 WORKSH LEARN; McCallum A. K., 1996, BOW TOOLKIT STAT LAN; Taghva K, 1996, ACM T INFORM SYST, V14, P64, DOI 10.1145/214174.214180; Taghva K, 1996, INFORM PROCESS MANAG, V32, P317, DOI 10.1016/0306-4573(95)00058-5; Taghva K., 1994, P 17 ANN INT ACM SIG, P202	16	0	0	SPIE-INT SOC OPTICAL ENGINEERING	BELLINGHAM	1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA	0277-786X		0-8194-3985-1	P SOC PHOTO-OPT INS			2001	4307						68	74				7	Computer Science, Information Systems; Computer Science, Software Engineering	Computer Science	BR90Y	WOS:000168019700007	
B	Tomokiyo, LM; Jones, R			ACL	Tomokiyo, LM; Jones, R			You're not from 'round here, are you? Naive Bayes detection of non-native utterance text	2ND MEETING OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE			English	Proceedings Paper	2nd Meeting of the North American Chapter of the Association-for-Computational-Linguistics	JUN 02-07, 2001	Pittsburgh, PA	Assoc Computat Linguist, N Amer Chapter	Carnegie Mellon Univ			Native and non-native use of language differs, depending on the proficiency of the speaker, in clear and quantifiable ways. It has been shown that customizing the acoustic and language models of a natural language understanding system can significantly improve handling of non-native input; in order to make such a switch, however, the nativeness status of the user must be known. In this paper, we show that naive Bayes classification can be used to identify non-native utterances of English. The advantage of our method is that it relies on text, not on acoustic features, and can be used when the acoustic source is not available. We demonstrate that both read and spontaneous utterances can be classified with high accuracy, and that classification of errorful speech recognizer hypotheses is more accurate than classification of perfect transcriptions. We also characterize part-of-speech sequences that play a role in detecting non-native speech.	Carnegie Mellon Univ, Language Technol Inst, Pittsburgh, PA 15213 USA							ARGAMONENGELSON S, 1998, AAAI WORKSH LEARNING; CORDER SP, 1967, IRAL-INT REV APPL LI, V5, P161, DOI 10.1515/iral.1967.5.1-4.161; Domingos P, 1997, MACH LEARN, V29, P103, DOI 10.1023/A:1007413511361; FINKE M, 1997, P LVCSR HUB5 E WORKS; FUNG P, 1999, P ICASSP; LEE KF, 1990, P ICASSP; LEWIS D, 1998, P ECML 98; MAYFIELD L, 1999, P ACL IALL JOINT WOR; McCallum A. K., 1996, BOW TOOLKIT STAT LAN; MOSTELLER F, 1984, APPL BAYESIAN CLSSIC; Quinlan J. R., 1986, Machine Learning, V1, DOI 10.1023/A:1022643204877; RATNAPARKHI A, 1996, P EMNLP; TARONE E, 1978, UNDERSTANDING SECOND; TEIXEIRA C, 1996, P ICSLP; YANG Y, 1999, 22 ANN INT SIGIR, V42; 1987, GUIDE SPEAK PRODUCED	16	0	0	ASSOCIATION COMPUTATIONAL LINGUISTICS	SOMERSET	PO BOX 6090, SOMERSET, NJ 08875 USA			1-55860-775-7				2001							239	246				8	Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Linguistics; Statistics & Probability	Computer Science; Linguistics; Mathematics	BAP03	WOS:000223095200031	
B	Yan, L; Miller, DJ		Miller, DJ; Adali, T; Larsen, J; VanHulle, M; Douglas, S		Yan, L; Miller, DJ			Approximate maximum entropy learning for classification: Comparison with other methods	NEURAL NETWORKS FOR SIGNAL PROCESSING XI			English	Proceedings Paper	11th IEEE Workshop on Neural Networks for Signal Processing (NNSP 2001)	SEP 10-12, 2001	FALMOUTH, MA	IEEE Signal Processing Soc			INFERENCE	Recently, we proposed new methods for approximately learning the maximum entropy (ME) joint pmf for discrete feature spaces. Our approximate techniques overcome the intractability that plagues most ME learning methods when given a general set of constraints. The resulting models are useful for classification as well as more general inference. Our method has been demonstrated to yield strong performance in comparison with Bayesian networks, dependence trees, tree-augmented naive Bayes models, and multilayer perceptrons. After first reviewing our method, we provide insight into why it works and how it is related to, albeit distinct from naive Bayes classification (NBC). The connection to NBC then naturally leads us to suggest a simple method for parsimoniously choosing the set of constraints to encode when forming the model. Finally, we provide new experimental comparisons for our method, with decision trees and with support vector machines.		Yan, L (reprint author), Athene Software Inc, 2060 Broadway,Suite 300, Boulder, CO 80302 USA.						CHEESEMAN P, 1983, P 8 INT JOINT C AI, V1, P198; Friedman N., 1996, P NATL C A1, V2, P1277; HERSKOVITS E, 1991, UNCERTAINTY ARTIFICI, V6, P117; KU HH, 1969, IEEE T INFORM THEORY, V15, P444, DOI 10.1109/TIT.1969.1054336; Miller DJ, 2000, NEURAL COMPUT, V12, P2175, DOI 10.1162/089976600300015105; MILLER WJ, 1934, U CALIF PUBL MATH PH, V1, P1; Platt J. C., 1998, MSRTR9814 MICR RES; Yan L, 2000, IEEE T NEURAL NETWOR, V11, P558, DOI 10.1109/72.846727	8	0	0	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA			0-7803-7196-8				2001							243	252				10	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic; Mathematics, Applied; Statistics & Probability	Computer Science; Engineering; Mathematics	BT15K	WOS:000172099600025	
S	Yan, WZ		Dasarathy, BV		Yan, WZ			Fault detection and multi-classifier fusion for unmanned aerial vehicles (UAVs)	SENSOR FUSION: ARCHITECTURES, ALGORITHMS AND APPLICATIONS V	PROCEEDINGS OF THE SOCIETY OF PHOTO-OPTICAL INSTRUMENTATION ENGINEERS (SPIE)		English	Proceedings Paper	Conference on Sensor Fusion - Architectures, Algorithms, and Applications V	APR 18-20, 2001	ORLANDO, FL	SPIE		fault detection; classifier fusion; classifier combination; UAVs		UAVs demand more accurate fault accommodation for their mission manager and vehicle control system in order to achieve a reliability level that is comparable to that of a piloted aircraft. This paper attempts to apply multi-classifier fusion techniques to achieve the necessary performance of the fault detection function for the Lockheed Martin Skunk Works (LMSW) UAV Mission Manager. Three different classifiers that meet the design requirements of the fault detection of the UAV are employed. The binary decision outputs from the classifiers are then aggregated using three different classifier fusion schemes, namely, majority vote, weighted majority vote, and Naive Bayes combination. All of the three schemes are simple and need no retraining. The three fusion schemes (except the majority vote that gives an average performance of the three classifiers') show the classification performance that is better than or equal to that of the best individual. The unavoidable correlation between the classifiers with binary outputs is observed in this study. We conclude that it is the correlation between the classifiers that limits the fusion schemes to achieve an even better performance.	GE Corp R&D, Niskayuna, NY 12309 USA	Yan, WZ (reprint author), GE Corp R&D, 1 Res Circle, Niskayuna, NY 12309 USA.						ATKINS E, 1998, P AM CONTR C PHIL PE; BONISSONE P, 1999, P IEEE, V87; FAIRHURST M, 2000, IEE P VIS IM SIGN PR, V147; Gertler J. J., 1998, FAULT DETECTION DIAG; GOEBEL K, 2001, IN PRESS AIEDAM AI E; GOEBEL K, 2000, P 2000 ASME INT MECH, V11; Ho T. K., 1994, IEEE T PATTERN ANAL, V16; JOHNSON T, 2001, 2001 IEEE AER C BIG; Keller J. M., 1985, IEEE T SYSTEMS MAN C, V15; KITTLER J, 1997, P BRIT MACH VIS C; KUNCHEVA LI, 2000, FUZZY CLASSIFIER DES; LEI X, 1992, IEEE T SYSTEMS MAN C, V22; MAGRABI M, 2000, IEEE 2000 POS LOC NA; PETRAKOS M, 2000, P IEEE 2000 INT GEOS, V6; Quinlan J.R., 1993, C4 5 PROGRAMS MACHIN; Rago C., 1998, P 37 IEEE C DEC CONT; RAUCH H, 1993, IEEE INT S INT CONTR; SHAEFER P, 2001, 2001 IEEE AER C BIG; Sullivan J., 1992, P 1992 AIAA GUID NAV; VOS D, 1998, P AM CONTR C PHIL PA	20	0	1	SPIE-INT SOC OPTICAL ENGINEERING	BELLINGHAM	1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA	0277-786X		0-8194-4080-9	P SOC PHOTO-OPT INS			2001	4385						176	186		10.1117/12.421106		11	Engineering, Electrical & Electronic; Optics	Engineering; Optics	BS47E	WOS:000169941900018	
B	Zenko, B; Todorovski, L; Dzeroski, S		Cercone, N; Lin, TY; Wi, XD		Zenko, B; Todorovski, L; Dzeroski, S			A comparison of stacking with meta decision trees to bagging, boosting, and stacking with other methods	2001 IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS			English	Proceedings Paper	IEEE International Conference on Data Mining	NOV 29-DEC 02, 2001	SAN JOSE, CA	IEEE Comp Soc, TCPAMI, IEEE Comp Soc, TFVI, Insightful Corp, Microsoft Res, NARAX Inc, Springer Verlag, New York, StatSoft Inc				Meta decision trees (MDTs) are a method for combining multiple classifiers. We present an integration of the algorithm MLC4.5 for learning MDTs into the Weka data mining suite. We compare classifier ensembles combined with MDTs to bagged and boosted decision trees, and to classifier ensembles combined with other methods: voting and stacking with three different meta-level classifiers (ordinary decision trees, naive Bayes, and multi-response linear regression - MLR).	Jozef Stefan Inst, Dept Intelligent Syst, Ljubljana, Slovenia	Zenko, B (reprint author), Jozef Stefan Inst, Dept Intelligent Syst, Jamova 39, Ljubljana, Slovenia.						Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.1023/A:1018054314350; Freund Y., 1996, P 13 INT C MACH LEAR, P148; Quinlan J.R., 1993, C4 5 PROGRAMS MACHIN; Ting KM, 1999, J ARTIF INTELL RES, V10, P271; Todorovski L., 2000, Principles of Data Mining and Knowledge Discovery. 4th European Conference, PKDD 2000. Proceedings (Lecture Notes in Artificial Intelligence Vol.1910); Witten I.H., 1999, DATA MINING PRACTICA; WOLPERT DH, 1992, NEURAL NETWORKS, V5, P241, DOI 10.1016/S0893-6080(05)80023-1	7	0	0	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA			0-7695-1119-8				2001							669	670				2	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Computer Science; Engineering	BT50G	WOS:000173158200107	
B	Borgelt, C; Timm, H; Kruse, R			IEEE; IEEE	Borgelt, C; Timm, H; Kruse, R			Using fuzzy clustering to improve naive Bayes classifiers and probabilistic networks	NINTH IEEE INTERNATIONAL CONFERENCE ON FUZZY SYSTEMS (FUZZ-IEEE 2000), VOLS 1 AND 2			English	Proceedings Paper	9th IEEE International Conference on Fuzzy Systems (FUZZy-IEEE 2000)	MAY 07-10, 2000	SAN ANTONIO, TX	IEEE Neural Networks Council, Texas A&M Univ, Int Fuzzy Syst Assoc, Japan Soc Fuzzy Theory & Syst				Although probabilistic networks and fuzzy clustering may seem to be disparate areas of research, they can both be seen as generalizations of naive Bayes classifiers. If all descriptive attributes are numeric, naive Bayes classifiers often assume an axis-parallel multidimensional normal distribution for each class. Probabilistic networks remove the requirement that the distributions must be axis-parallel by taking covariances into account where this is necessary. Fuzzy clustering tries to And general or axis-parallel distributions to cluster the data. Although it neglects the class information, it can be used to improve the result of the abovementioned methods by removing the restriction to only one distribution per class.	Univ Magdeburg, Dept Knowledge Proc & Language Engn, D-39106 Magdeburg, Germany	Borgelt, C (reprint author), Univ Magdeburg, Dept Knowledge Proc & Language Engn, Univ Pl 2, D-39106 Magdeburg, Germany.						Andersen S. K., 1989, P 11 INT JOINT C ART, P1080; Bezdek J. C., 1981, PATTERN RECOGNITION; Bezdek J.C., 1992, FUZZY MODELS PATTERN; Dougherty J., 1995, P 12 INT C MACH LEAR, P194; Duda R., 1973, PATTERN CLASSIFICATI; Everitt B., 1981, CLUSTER ANAL; EZAWA K, 1995, P 1 INT C KNOWL DISC, P100; GATH I, 1989, IEEE T PATTERN ANAL, V11, P773, DOI 10.1109/34.192473; GEBHARDT J, 1996, FUZZY SET METHODS IN, P407; Good I, 1965, ESTIMATION PROBABILI; Heckerman D, 1991, PROBABILISTIC SIMILA; Hoppner F., 1999, FUZZY CLUSTER ANAL; Kruse R., 1994, FDN FUZZY SYSTEMS; Kruse R., 1991, UNCERTAINTY VAGUENES; Langley P., 1994, P 10 C UNC ART INT, P399; Langley P., 1992, P 10 NAT C ART INT, P223; LAURITZEN SL, 1988, J ROY STAT SOC B MET, V50, P157; NAUCK D, 1997, FDN NEURO FUZZY SYST; PEARL J, 1992, PROBABILISTIC REASON; SAFFIOTTI A, 1991, P 7 C UNC ART INT LO, P323; SHAFER G, 1988, LOCAL COMPULATIONS H; SHENOY PP, 1991, FRAMEWORK MANAGING U; Whittaker J., 1990, GRAPHICAL MODELS APP	23	0	0	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA			0-7803-5878-3				2000							53	58				6	Computer Science, Artificial Intelligence	Computer Science	BQ42Y	WOS:000088355300009	
S	El-Matouat, F; Colot, O; Vannoorenberghe, P; Labiche, J			IEEE; IEEE	El-Matouat, F; Colot, O; Vannoorenberghe, P; Labiche, J			From continuous to discrete variables for Bayesian network classifiers	SMC 2000 CONFERENCE PROCEEDINGS: 2000 IEEE INTERNATIONAL CONFERENCE ON SYSTEMS, MAN & CYBERNETICS, VOL 1-5	IEEE INTERNATIONAL CONFERENCE ON SYSTEMS, MAN, AND CYBERNETICS, CONFERENCE PROCEEDINGS		English	Proceedings Paper	IEEE International Conference on Systems, Man and Cybernetics	OCT 08-11, 2000	NASHVILLE, TN	IEEE Inc, Syst Man & Cybernet Soc		Bayesian network classifier; uncertain reasoning; medical diagnosis		Using graphical models to represent independent structure in multivariate probability model has been studied since:a few years. In this framework, Bayesian networks, have been proposed as an interesting approach for uncertain reasoning, Within the framework of pattern recognition, many methods of classification were developped based on statistical data analysis. Belief networks were not considered as classifiers until the discovery that Naive Bayes, a very simple kind of Bayesian network, is surprisingly effective. In this paper, we propose to use belief networks classifiers with optimal variables that is to say networks which have to manage discrete and continuous variables.	Univ Rouen, INSA, F-76131 Mt St Aignan, France	El-Matouat, F (reprint author), Univ Rouen, INSA, Pl Emile,BP 08, F-76131 Mt St Aignan, France.						Akaike H., 1973, P 2 INT S INF THEOR, P267; Colot O., 1994, Signal Processing VII, Theories and Applications. Proceedings of EUSIPCO-94. Seventh European Signal Processing Conference; COLOT O, 1993, THESIS U ROUEN; COOPER GF, 1992, MACH LEARN, V9, P309, DOI 10.1007/BF00994110; DODIER R, 1999, THESIS U COLORADO; Duda R., 1973, PATTERN CLASSIFICATI; Friedman N., 1996, P 13 INT C MACH LEAR, P157; Friedman N, 1997, MACH LEARN, V29, P131, DOI 10.1023/A:1007465528199; Heckerman D, 1997, DATA MIN KNOWL DISC, V1, P79, DOI 10.1023/A:1009730122752; Murphy P. M., 1995, UCI REPOSITORY MACHI; Pearl J., 1988, PROBABILISTIC REASON; SAKAMOTO Y, 1986, AKAIKE INFORMATION C; VANNOORENBERGHE P, 1999, P 10 INT C IM AN PRO; XU H, 1994, THESIS FREE U BRUSSE	14	0	0	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA	1062-922X		0-7803-6583-6	IEEE SYS MAN CYBERN			2000							2800	2805				6	Computer Science, Cybernetics; Engineering, Electrical & Electronic	Computer Science; Engineering	BR32T	WOS:000166106900487	
S	Escudero, G; Marquez, L; Rigau, G		Horn, W		Escudero, G; Marquez, L; Rigau, G			Naive Bayes and Exemplar-based approaches to word sense disambiguation revisited	ECAI 2000: 14TH EUROPEAN CONFERENCE ON ARTIFICIAL INTELLIGENCE, PROCEEDINGS	FRONTIERS IN ARTIFICIAL INTELLIGENCE AND APPLICATIONS		English	Proceedings Paper	14th European Conference on Artificial Intelligence	AUG 20-25, 2000	BERLIN, GERMANY	European Coordinating Comm Artificial Intelligence, German Informat Soc, Humboldt Univ				This paper describes an experimental comparison between two standard supervised learning methods, namely Naive Bayes and Exemplar-based classification, on the Word Sense Disambiguation (WSD) problem. The aim of the work is twofold. Firstly, it attempts to contribute to clarify some confusing information about the comparison between both methods appearing in the related literature. In doing so, several directions have been explored, including: testing several modifications of the basic learning algorithms and varying the feature space. Secondly, an improvement of both algorithms is proposed, in order to deal with large attribute sets. This modification, which basically consists in using only the positive information appearing in the examples, allows to improve greatly the efficiency of the methods, with no loss in accuracy. The experiments have been performed on the largest sense-tagged corpus available containing the most frequent and ambiguous English words. Results show that the Exemplar-based approach to WSD is generally superior to the Bayesian approach, especially when a specific metric for dealing with symbolic attributes is used.	Tech Univ Catalonia, TALP Res Ctr, Software Dept, E-08034 Catalonia, Spain	Escudero, G (reprint author), Tech Univ Catalonia, TALP Res Ctr, Software Dept, Jordi Girona Salgado 1-3, E-08034 Catalonia, Spain.						COST S, 1993, MACH LEARN, V10, P57, DOI 10.1007/BF00993481; DEMANTARAS RL, 1991, MACH LEARN, V6, P81, DOI 10.1023/A:1022694001379; Dietterich T. G., 1998, NEURAL COMPUTATION, V10; Duda R., 1973, PATTERN CLASSIFICATI; ENGELSON SP, 1996, LNAI, V1040; Fujii A, 1998, COMPUT LINGUIST, V24, P573; GALE W, 1993, COMPUT HUMANITIES, V26, P415; Ide N, 1998, COMPUT LINGUIST, V24, P1; Leach S, 1998, LOCAL GOV STUD, V24, P1; Manning C.D., 1999, FDN STAT NATURAL LAN; MIHALCEA R, 1999, P 16 NAT C ART INT; MILLER GA, 1990, SPECIAL ISSUE INT J, V3; MILLER GA, 1993, P ARPA WORKSH HUM LA; MOONEY RJ, 1996, P 1 C EMP METH NAT L; NG HT, 1996, P 34 ANN M ASS COMP; NG HT, 1997, P 2 C EMP METH NAT L; NG HT, 1997, P ACL SIGLEX WORKSH; PEDERSEN T, 1998, P 15 NAT C ART INT; Towell G, 1998, COMPUT LINGUIST, V24, P125; YAROWSKY D, 1994, P 32 ANN M ASS COMP	20	0	0	I O S PRESS	AMSTERDAM	NIEUWE HEMWEG 6B, 1013 BG AMSTERDAM, NETHERLANDS	0922-6389		1-58603-013-2	FR ART INT			2000	54						421	425				5	Computer Science, Artificial Intelligence	Computer Science	BR29N	WOS:000166049100081	
B	Hellerstein, JL; Jayram, TS; Rish, I			AAAI; AAAI	Hellerstein, JL; Jayram, TS; Rish, I			Recognizing end-user transactions in performance management	SEVENTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-2001) / TWELFTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE (IAAI-2000)			English	Proceedings Paper	17th National Conference on Artificial Intelligence (AAAI-2000)/12th Conference on Innovative Applications of Artificial Intelligence (IAAI-2000)	JUL 30-AUG 03, 2000	AUSTIN, TX	Amer Assoc Artificial Intelligence				Providing good quality of service (e.g., low response times) in distributed computer systems requires measuring end-user perceptions of performance. Unfortunately, such measures are often expensive or impossible to obtain. Herein, we propose a machine-learning approach to recognizing end-user transactions consisting of sequences of remote procedure calls (RPCs) received at a server. Two problems are addressed. The first problem is labeling an RPC sequence that corresponds to one transaction instance with the correct transaction type. This is akin to text classification. The second problem is transaction recognition, a more comprehensive task that involves segmenting RPC sequences into transaction instances and labeling those instances with transaction types. This problem is similar to segmenting sounds into words as in speech understanding. Using Naive Bayes approach, we tackle the labeling problem with four combinations of feature vectors and probability distributions: RPC occurrences with the Bernoulli distribution and RPC counts with the multinomial, geometric, and shifted geometric distributions. Our approach to transaction recognition uses a dynamic-programming Viterbi algorithm that searches for a most likely segmentation of an RPC sequence into a sequence of transactions, assuming transaction independence and using our classifiers to select a most likely transaction label for a given RPC sequence. For both problems, good accuracies are obtained, although the labeling problem achieves higher accuracies (up to 87%) than does transaction recognition (64%).	IBM Corp, Thomas J Watson Res Ctr, Hawthorne, NY USA	Hellerstein, JL (reprint author), IBM Corp, Thomas J Watson Res Ctr, Hawthorne, NY USA.						Craven M., 1998, Proceedings Fifteenth National Conference on Artificial Intelligence (AAAI-98). Tenth Conference on Innovative Applications of Artificial Intelligence; Fu K. S., 1982, SYNTACTIC PATTERN RE; Jelinek Frederick, 1998, STAT METHODS SPEECH; Joachims T, 1998, EUR C MACH LEARN ECM; Kleinrock L, 1975, QUEUEING SYSTEMS, V1; Lewis D., 1994, P 17 ANN INT ACM SIG, P3; Manning C.D., 1999, FDN STAT NATURAL LAN; McCallum A, 1998, P AAAI 98 WORKSH LEA, P41; Nigam K., 1998, Proceedings Fifteenth National Conference on Artificial Intelligence (AAAI-98). Tenth Conference on Innovative Applications of Artificial Intelligence; RAGHAVAN SV, 1995, LECT NOTES COMPUTER, V977, P314; Sahami M., 1998, P AAAI WORKSH LEARN, P55; Yang Y., 1997, P 14 INT C MACH LEAR, P412; Yang Y, 1999, J INFORMATION RETRIE, V1, P69	13	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA			0-262-51112-6				2000							596	602				7	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic; Robotics	Computer Science; Engineering; Robotics	BT26B	WOS:000172441000091	
S	Inza, I; Merino, M; Larranaga, P; Quiroga, J; Sierra, B; Girala, M		Brause, RW; Hanisch, E		Inza, I; Merino, M; Larranaga, P; Quiroga, J; Sierra, B; Girala, M			Feature subset selction using probabilistic tree structures. A case study in the survival of cirrhotic patients treated with TIPS	MEDICAL DATA ANALYSIS, PROCEEDINGS	LECTURE NOTES IN COMPUTER SCIENCE		English	Article; Proceedings Paper	1st International Symposum on Medical Data Analysis (ISMDA 2000)	SEP 29-30, 2000	FRANKFURT, GERMANY				ESOPHAGEAL-VARICES	The transjugular intrahepatic portosystemic shunt (TIPS) is an interventional treatment for cirrhotic patients with portal hypertension. In the light of our medical staff's experience, the consequences of the TIPS are not homogeneous for all the patients and a subgroup of them dies in the first six months after the TIPS placement. Actually, there is no risk indicator to identify this group, before treatment. An investigation for predicting the survival of cirrhotic patients treated with TIPS is carried out using a clinical database with 107 cases and 77 attributes. Naive-Bayes, C4.5 and CN2 supervised classifiers axe applied to identify this group, The application of several Feature Subset Selection (FSS) techniques has significantly improved the predictive accuracy of these classifiers and considerably reduced the amount of attributes in the classification models. Among FSS techniques, FSS-TREE, a new randomized algorithm inspired on the EDA (Estimation of Distribution Algorithm) paradigm, has obtained the best accuracy results.	Univ Basque Country, Dept Comp Sci & Artificial Intelligence, E-20080 San Sebastian, Spain; Basque Hlth Serv Osakidetza, E-20013 San Sebastian, Spain; Univ Navarra Clin, Fac Med, E-31080 Pamplona, Iruna, Spain	Inza, I (reprint author), Univ Basque Country, Dept Comp Sci & Artificial Intelligence, POB 649, E-20080 San Sebastian, Spain.		Larranaga, Pedro/F-9293-2013				Back T., 1996, EVOLUTIONARY ALGORIT; BORNMAN PC, 1994, LANCET, V343, P1079, DOI 10.1016/S0140-6736(94)90186-4; Cestnik B, 1990, P EUR C ART INT, P147; CHOW CK, 1968, IEEE T INFORM THEORY, V14, P462, DOI 10.1109/TIT.1968.1054142; Clark P., 1989, Machine Learning, V3, DOI 10.1007/BF00116835; CONN HO, 1981, HEPATOLOGY, V1, P1; Cooper GF, 1997, ARTIF INTELL MED, V9, P107, DOI 10.1016/S0933-3657(96)00367-3; DAMICO G, 1995, HEPATOLOGY, V22, P332, DOI 10.1016/0270-9139(95)90388-7; DRAPER D, IN PRESS J GLOBAL OP; Friedman N., 1996, P 12 C UNC ART INT, P274; Goldberg D., 1989, GENETIC ALGORITHMS S; Grefenstette J., 1986, IEEE T SYST MAN CYB, V16, P122; HOLLAND JH, 1975, ADAPTATION NATURAL A; Jelonek J, 1997, ARTIF INTELL MED, V9, P227, DOI 10.1016/S0933-3657(96)00375-2; Kittler J., 1978, Pattern Recognition and Signal Processing; Kohavi R, 1997, ARTIF INTELL, V97, P273, DOI 10.1016/S0004-3702(97)00043-X; Kohavi R., 1995, P 14 INT JOINT C ART, P1137; Kudo M, 2000, PATTERN RECOGN, V33, P25, DOI 10.1016/S0031-3203(99)00041-2; Larranaga P., 1999, EHUKZAAIK499 U BASQ; Liu H., 1998, FEATURE SELECTION KN; Malinchoc M, 2000, HEPATOLOGY, V31, P864, DOI 10.1053/he.2000.5852; MICHIE D, 1990, J STAT PLAN INFER, V25, P381, DOI 10.1016/0378-3758(90)90083-7; MUEHLENBEIN H, 1996, LECT NOTES COMPUTER, V1411, P178; Pelikan M., 1998, 98013 ILLIGAL U ILL; PUGH RNH, 1973, BRIT J SURG, V60, P646, DOI 10.1002/bjs.1800600817; QUINLAN JR, 1993, C4 5 PROGR MACH LEAR; Rossle M, 1998, LIVER, V18, P73; ROSSLE M, 1989, LANCET, V2, P153; SAUNDERS JB, 1981, BRIT MED J, V282, P263; Vafaie H., 1993, Proceedings. Fifth International Conference on Tools with Artificial Intelligence TAI '93 (Cat. No.93CH3325-8), DOI 10.1109/TAI.1993.633981	30	0	0	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		3-540-41089-9	LECT NOTES COMPUT SC			2000	1933						97	110				14	Computer Science, Information Systems; Computer Science, Theory & Methods	Computer Science	BS85C	WOS:000171225400014	
S	Kalousis, A; Hilario, M		Ebecken, N; Brebbia, CA		Kalousis, A; Hilario, M			Supervised knowledge discovery from incomplete data	DATA MINING II	MANAGEMENT INFORMATION SYSTEMS		English	Proceedings Paper	2nd International Conference on Data Mining	JUL 05-07, 2000	CAMBRIDGE, ENGLAND	Wessex Inst Technol, Fed Univ Rio De Janiero, COPPE Ctr	CAMBRIDGE UNIV			Incomplete data can raise more or less serious problems in knowledge discovery systems depending on the quantity and pattern of missing values as well as the generalization method used. For instance, some methods are inherently resilient to missing values while others have built-in methods for coping with them. Still others require that none of the values are missing; for such methods, preliminary imputation of missing values is indispensable. After a quick overview of current practice in the machine learning field, we explore the problem of missing values from a statistical perspective. In particular, we adopt the well-known distinction between three patterns of missing values-missing completely at random (MCAR), missing at random (MAR) and not missing at random (NMAR)-to focus a comparative study of eight learning algorithms from the point of view of their tolerance to incomplete data. Experiments on 47 datasets reveal a rough ranking from the most resilient (e.g., Naive Bayes) to the most sensitive (e.g., IBl and-surprisingly-C50rules). More importantly, results show that for a given amount of missing values, their dispersion among the predictive variables is at least as important as the pattern of missingness.	Univ Geneva, Dept Comp Sci, CH-1211 Geneva 4, Switzerland	Kalousis, A (reprint author), Univ Geneva, Dept Comp Sci, CH-1211 Geneva 4, Switzerland.						Blake C.L., 1998, REPOSITORY MACHINE L; BREIMAN L, 1983, CLASSIFICATIN REGRES; Cohen W., 1995, P 12 INT C MACH LEAR, P115; Gama J., 1999, Intelligent Data Analysis, V3, DOI 10.1016/S1088-467X(99)00002-5; GHAHRAMANI Z, 1994, 108 MIT CBCL; KOHAVI R, 1996, P INT C TOOLS AI; Michie D., 1994, MACHINE LEARNING NEU; QUINLAN JR, 1992, C4 5 PROGRAMMS MACHI; Schaffer J. L., 1997, ANAL INCOMPLETE MULT	9	0	0	WIT PRESS	SOUTHAMPTON	ASHURST LODGE, SOUTHAMPTON SO40 7AA, ASHURST, ENGLAND	1470-6326		1-85312-821-X	MANAG INFORMAT SYST			2000	2						269	278				10	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Computer Science	BR40U	WOS:000166319000027	
J	Lanquillon, C				Lanquillon, Carsten			Learning from Labeled and Unlabeled Documents: A Comparative Study on Semi-Supervised Text Classification	LECTURE NOTES IN COMPUTER SCIENCE <D>			English	Article								Supervised learning algorithms usually require large amounts of training data to learn reasonably accurate classifiers. Yet, for many text classification tasks, providing labeled training documents is expensive, while unlabeled documents are readily available in large quantities. Learning from both, labeled and unlabeled documents, in a semi-supervised framework is a promising approach to reduce the need for labeled training documents. This paper compares three commonly applied text classifiers in the light of semi-supervised learning, namely a linear support vector machine, a similarity-based tfidf and a Naive Bayes classifier. Results on a real-world text datasets show that these learners may substantially benefit from using a large amount of unlabeled documents in addition to some labeled documents.	DaimlerChrysler AG, Res & Technol 3, D-89013 Ulm, Germany	Lanquillon, C (reprint author), DaimlerChrysler AG, Res & Technol 3, D-89013 Ulm, Germany.	carsten.lanquillon@daimlerchrysler.com					DUMAIS S, 1998, P 17 INT C INF KNOWL; Joachims T., 1999, ADV KERNEL METHODS S; JOACHIMS T, 1998, P ECML 98; JOACHIMS T, 1997, P ICML 97; JOACHIMS T, 1999, P ICML 99; LANQUILLON C, 2000, ECML2000 IN PRESS; MacQueen J. B., 1967, P 5 BERK S MATH STAT, V1, P281, DOI DOI 10.1234/12345678; McCallum A. K., 1996, BOW TOOLKIT STAT LAN; NIGAM K, 2000, MACHINE LEA IN PRESS; ROCCHIO J., 1971, SMART RETRIEVAL SYST; Salton Gerard, 1989, AUTOMATIC TEXT PROCE; VANRIJSBERGEN CJ, 1977, J DOC, V33, P106; Yang Y., 1999, P 22 ANN INT ACM SIG, P42, DOI 10.1145/312624.312647; Yang Y., 1997, P 14 INT C MACH LEAR, P412	14	0	0	SPRINGER	NEW YORK	233 SPRING ST, NEW YORK, NY 10013 USA	0302-9743			LECT NOTES COMPUT<D>	Lect. Notes Comput. Sci.		2000	1910						490	497				8	Computer Science, Theory & Methods	Computer Science	V12XW	WOS:000207632900057	
B	Chickering, DM; Heckerman, D		Laskey, KB; Prade, H		Chickering, DM; Heckerman, D			Fast learning from sparse data	UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS			English	Proceedings Paper	15th Conference on Uncertainty in Artificial Intelligence	JUL 30-AUG 01, 1999	STOCKHOLM, SWEDEN	AT&T Labs, Fair, Isaac & Co Inc, Hewlett Packard Co, Hugin Expert A/S, Informat Extract & Transport Inc, Knowledge Ind, Microsoft Res, Norsys Software Corp, Rockwell Int	ROYAL INST TECHNOL		BAYESIAN NETWORKS; LIKELIHOOD	We describe two techniques that significantly improve the running time of several standard machine-learning algorithms when data is sparse. The first technique is an algorithm that efficiently extracts one-way and two-way counts-either real or expected-from discrete data. Extracting such counts is a fundamental step in learning algorithms for constructing a variety of models including decision trees, decision graphs, Bayesian networks, and naive-Bayes clustering models. The second technique is an algorithm that efficiently performs the E-step of the EM algorithm (i.e., inference) when applied to a naive-Bayes clustering model. Using real-world data sets, we demonstrate a dramatic decrease in running time for algorithms that incorporate these techniques.	Microsoft Res, Redmond, WA 98052 USA	Chickering, DM (reprint author), Microsoft Res, Redmond, WA 98052 USA.						Breiman L., 1984, CLASSIFICATION REGRE; Buntine W., 1991, P 7 C UNC ART INT, P52; Cheeseman P, 1995, ADV KNOWLEDGE DISCOV, P153; CHICKERING D, 1997, P 13 C UNC ART INT P; Chickering DM, 1997, MACH LEARN, V29, P181; Clogg C., 1995, HDB STAT MODELING SO, P311; Cooper G., 1991, P 7 C UNC ART INT LO, P86; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1; EDMONDS J, 1967, J RES NBS B MATH SCI, VB 71, P233, DOI 10.6028/jres.071B.032; HECKERMAN D, 1995, MACH LEARN, V20, P197, DOI 10.1007/BF00994016; Moore A, 1998, J ARTIF INTELL RES, V8, P67; SPIEGELHALTER DJ, 1993, STAT SCI, V8, P219, DOI 10.1214/ss/1177010888	12	0	0	MORGAN KAUFMANN PUB INC	SAN FRANCISCO	340 PINE STR, 6TH FLR, SAN FRANCISCO, CA 94104-3205 USA			1-55860-614-9				1999							109	115				7	Computer Science, Artificial Intelligence	Computer Science	BQ80C	WOS:000089548100013	
S	Cikhart, O; Hajic, J		Matousek, V; Mautner, P; Ocelikova, J; Sojka, P		Cikhart, O; Hajic, J			Word sense disambiguation of Czech texts	TEXT, SPEECH AND DIALOGUE	LECTURE NOTES IN ARTIFICIAL INTELLIGENCE		English	Article; Proceedings Paper	2nd International Workshop on Text, Speech and Diaglogue (TSD 99)	SEP 13-17, 1999	PLZEN, CZECH REPUBLIC	Medav GmbH, SpeechWorks, Univ Bohemia, Fac Appl Sci, Masaryk Univ				This contribution refers to the project of BYLL Software Ltd. that uses human aided WSD for the annotation of a fulltext database of the Czech law system named ASPI. We used about 3 mil, words oil annotated texts from the law system of the Czech Republic since the 60's. The annotated law corpus provides certain text regularity, but at the same time it covers wide range of subjects. The goal has been to save as much of the human intervention during text indexing as possible, measured by the number of queries posed to the human annotator, whilst retaining truly minimal error rate (similar to 0.5%) in the automatically disambiguated cases. A combination of Naive Bayes, Decision Lists and (minimal number) of manually written rules has been used. The statistical methods showed up to be appropriate for our intention. The results show that we have saved 80% of queries to the human annotator, which proved to be enough to warrant the inclusion of the software into a production system.	MFF UK, Inst Formal & Appl Linguist, CZ-11800 Prague, Czech Republic	Cikhart, O (reprint author), MFF UK, Inst Formal & Appl Linguist, Malostranske Nam 25, CZ-11800 Prague, Czech Republic.						CIKHART O, 1998, THESIS MFFUK PRAHA; FUJI A, 1998, TR980003 U LIB INF S; GALE WA, 1992, COMPUT HUMANITIES, V26, P415, DOI 10.1007/BF00136984; LACIGA Z, 1999, SB K EUR CZ 99; YAROWSKY D, 1992, P COL 92; YAROWSKY D, 1994, P 32 M ACL LAS CRUC	6	0	0	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		3-540-66494-7	LECT NOTES ARTIF INT			1999	1692						109	114				6	Computer Science, Artificial Intelligence; Computer Science, Theory & Methods	Computer Science	BQ71V	WOS:000089259200020	
S	Gama, J		Arikawa, S; Furukawa, K		Gama, J			Iterative naive Bayes	DISCOVERY SCIENCE, PROCEEDINGS	LECTURE NOTES IN ARTIFICIAL INTELLIGENCE		English	Article; Proceedings Paper	2nd International Conference on Discovery Science (DS 99)	DEC 06-08, 1999	TOKYO, JAPAN					Naive Bayes is a well known and studied algorithm both in statistics and machine learning. Bayesian learning algorithms represent each concept with a single probabilistic summary. In this paper we present an iterative approach to naive Bayes. The iterative Bayes begins with the distribution tables built by the naive Bayes. Those tables are iteratively updated in order to improve the probability class distribution associated with each training example. Experimental evaluation of Iterative Bayes on 25 benchmark datasets shows consistent gains in accuracy. An interesting side effect of our algorithm is that it shows to be robust to attribute dependencies.	Univ Porto, FEP, LIACC, P-4150 Oporto, Portugal	Gama, J (reprint author), Univ Porto, FEP, LIACC, Rua Campo Alegre 823, P-4150 Oporto, Portugal.	jgama@ncc.up.pt	Gama, Joao/A-2070-2008	Gama, Joao/0000-0003-3357-1195			Blake C., 1999, UCI REPOSITORY MACHI; BRODLEY CE, 1995, MACH LEARN, V19, P45, DOI 10.1007/BF00994660; Domingos P, 1997, MACH LEARN, V29, P103, DOI 10.1023/A:1007413511361; Dougherty J., 1995, MACH LEARN P 12 INT; Duda R., 1973, PATTERN CLASSIFICATI; John G. H., 1997, THESIS STANFORD U; KOHAVI R, 1996, MACH LEARN P 13 INT; Kohavi R., 1996, P 2 INT C KNOWL DISC; KONONENKO I, 1991, LNAI, V482; LANGLEY P, 1993, LNAI, V667; Michie D., 1994, MACHINE LEARNING NEU; Mitchell T.M., 1997, MACHINE LEARNING; Ripley B., 1996, PATTERN RECOGNITION; Webb G I, 1998, P 11 AUSTR JOINT C A, P285	14	0	0	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		3-540-66713-X	LECT NOTES ARTIF INT			1999	1721						80	91				12	Computer Science, Artificial Intelligence	Computer Science	BP88H	WOS:000086483000008	
B	Monti, S; Cooper, GF		Laskey, KB; Prade, H		Monti, S; Cooper, GF			A Bayesian network classifier that combines a finite mixture model and a naive Bayes model	UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS			English	Proceedings Paper	15th Conference on Uncertainty in Artificial Intelligence	JUL 30-AUG 01, 1999	STOCKHOLM, SWEDEN	AT&T Labs, Fair, Isaac & Co Inc, Hewlett Packard Co, Hugin Expert A/S, Informat Extract & Transport Inc, Knowledge Ind, Microsoft Res, Norsys Software Corp, Rockwell Int	ROYAL INST TECHNOL			In this paper we present a new Bayesian network model for classification that combines the naive Bayes (NB) classifier and the finite mixture (FM) classifier. The resulting classifier aims at relaxing the strong assumptions on which the two component models are based, in an attempt to improve on their classification performance, both in terms of accuracy and in terms of calibration of the estimated probabilities. The proposed classifier is obtained by superimposing a finite mixture model on the set of feature variables of a naive Bayes model, We present experimental results that compare the predictive performance on real datasets of the new classifier with the predictive performance of the NB classifier and the FM classifier.	Univ Pittsburgh, Intelligent Syst Program, Pittsburgh, PA 15260 USA	Monti, S (reprint author), Univ Pittsburgh, Intelligent Syst Program, 901M CL, Pittsburgh, PA 15260 USA.						Bernardo J. M., 1994, BAYESIAN THEORY; BIERNACKI C, 1998, 3521 INRIA; CELEUX G, 1992, COMPUT STAT DATA AN, V14, P315, DOI 10.1016/0167-9473(92)90042-E; Cheeseman P., 1996, ADV KNOWLEDGE DISCOV; Chickering DM, 1997, MACH LEARN, V29, P181; Cover T. M., 1991, ELEMENTS INFORMATION; DAY NE, 1969, BIOMETRIKA, V56, P463, DOI 10.2307/2334652; DEMPSTER A, 1977, J ROY STAT SOC, V39, P398; Domingos P, 1997, MACH LEARN, V29, P103, DOI 10.1023/A:1007413511361; Dougherty J., 1995, MACHINE LEARNING; Duda R., 1973, PATTERN CLASSIFICATI; Ezawa K.J., 1995, P 11 C UNC ART INT, P157; Fayyad U.M., 1993, P 13 INT JOINT C ART, P1022; Fine MJ, 1997, NEW ENGL J MED, V336, P243, DOI 10.1056/NEJM199701233360402; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Friedman JH, 1997, DATA MIN KNOWL DISC, V1, P55, DOI 10.1023/A:1009778005914; FRIEDMAN N, 1998, P 15 INT C MACH LEAR; FRIEDMAN N, 1997, MACHINE LEARNING; HANLEY JA, 1982, RADIOLOGY, V143, P29; Hastie T, 1996, J ROY STAT SOC B MET, V58, P155; KASS RE, 1995, J AM STAT ASSOC, V90, P773, DOI 10.2307/2291091; KOHAVI R, 1996, P 2 C KDD; KONTKANEN P, 1996, C19969 U HELS DEP CO; McLachlan G., 1997, EM ALGORITHM EXTENSI; MERZ C, 1996, MACHINE LEARNING REP; MONTI S, UNPUB BAYESIAN NETWO; Pearl J., 1988, PROBABILISTIC REASON; Provost F., 1998, P 15 INT C MACH LEAR; RIDGEWAY G, 1998, 4 INT C KDD; Salzberg SL, 1997, DATA MIN KNOWL DISC, V1, P317, DOI 10.1023/A:1009752403260; SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136; Vapnik V., 1995, NATURE STAT LEARNING; WARNER HR, 1961, JAMA-J AM MED ASSOC, V177, P177	33	0	0	MORGAN KAUFMANN PUB INC	SAN FRANCISCO	340 PINE STR, 6TH FLR, SAN FRANCISCO, CA 94104-3205 USA			1-55860-614-9				1999							447	456				10	Computer Science, Artificial Intelligence	Computer Science	BQ80C	WOS:000089548100051	
B	Rennie, J; McCallum, AK		Bratko, I; Dzeroski, S		Rennie, J; McCallum, AK			Using reinforcement learning to spider the Web efficiently	MACHINE LEARNING, PROCEEDINGS			English	Proceedings Paper	16th International Conference on Machine Learning (ICML 99)	JUN 27-30, 1999	BLED, SLOVENIA	HERMES SoftLab, DaimlerChrysler Res & Technol Ctr, JerovsekComp, Jozef Stefan Inst, Micro Res, MLNET-II ESPRIT IV Project 29288, Slovenian Minist Sci & Technol, TEMIDA, TSE TRADE				Consider the task of exploring the Web in order to find pages of a particular kind or on a particular topic. This task arises in the construction of search engines and Web knowledge bases. This paper argues that the creation of efficient web spiders is best framed and solved by reinforcement learning, a branch of machine learning that concerns itself with optimal sequential decision making. One strength of reinforcement learning is that it provides a formalism for measuring the utility of actions that give benefit only in the future. We present an algorithm for learning a value function that maps hyperlinks to future discounted reward using a naive Bayes text classifier. Experiments on two real-world spidering tasks show a threefold improvement in spidering efficiency over traditional breadth-first search, and up to a two-fold improvement over reinforcement learning with immediate reward only.	Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA	Rennie, J (reprint author), Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA.						Bellman R., 1957, DYNAMIC PROGRAMMING; Boyan J, 1996, AAAI WORKSH INT BAS; CHO JH, 1998, COMPUTER NETWORKS IS, V30; CRAVEN M, 1998, P 15 NAT C ART INT A; JOACHIMS T, 1997, P IJCAI 97; Kaelbling LP, 1996, J ARTIF INTELL RES, V4, P237; Lewis DD, 1998, ECML 98; McCallum A, 1998, AAAI 98 WORKSH LEARN; MCCALLUM A, 1999, AAAI 99 SPRING S INT; McCallum Andrew K., 1998, ICML 98, P359; MENCZER F, 1997, ICML 97; Mitchell T.M., 1997, MACHINE LEARNING; TORGO L, 1997, INTELLIGENT DATA ANA, V1	13	0	0	MORGAN KAUFMANN PUB INC	SAN FRANCISCO	340 PINE STR, 6TH FLR, SAN FRANCISCO, CA 94104-3205 USA			1-55860-612-2				1999							335	343				9	Computer Science, Artificial Intelligence	Computer Science	BV38D	WOS:000178755300036	
B	Roth, D			IJCAII; IJCAII; IJCAII	Roth, D			Learning in natural language	IJCAI-99: PROCEEDINGS OF THE SIXTEENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, VOLS 1 & 2			English	Proceedings Paper	16th International Joint Conference on Artificial Intelligence (IJCAI 99)	JUL 31-AUG 06, 1999	STOCKHOLM, SWEDEN	Int Joint Conference Artificial Intelligence Inc, Scandinavian Al Soc			MODELS	Statistics-based classifiers in natural language are developed typically by assuming a generative model for the data, estimating its parameters from training data and then using Bayes rule to obtain a classifier. For many problems the assumptions made by the generative models are evidently wrong, leaving open the question of why these approaches work. This paper presents a learning theory account of the major statistical approaches to learning in natural language. A class of Linear Statistical Queries (LSQ) hypotheses is defined and learning with it is shown to exhibit some robustness properties. Many statistical learners used in natural language, including naive Bayes, Markov Models and Maximum Entropy models are shown to be LSQ hypotheses, explaining the robustness of these predictors even when the underlying probabilistic assumptions do not hold. This coherent view of when and why learning approaches work in this context may help to develop better learning methods and an understanding of the role of learning in natural language inferences.	Univ Illinois, Dept Comp Sci, Urbana, IL 61801 USA	Roth, D (reprint author), Univ Illinois, Dept Comp Sci, 1304 W Springfield Ave, Urbana, IL 61801 USA.						Anthony M., 1993, Proceeding of the Sixth Annual ACM Conference on Computational Learning Theory, DOI 10.1145/168304.168324; Aslam J. A., 1995, Proceedings of the Eighth Annual Conference on Computational Learning Theory, DOI 10.1145/225298.225351; DARROCH JN, 1972, ANN MATH STAT, V43, P1470, DOI 10.1214/aoms/1177692379; DARROCH JN, 1972, ANN MATH STAT, V43, P1972; Decatur S. E., 1993, Proceeding of the Sixth Annual ACM Conference on Computational Learning Theory, DOI 10.1145/168304.168346; DELCHER A, 1993, NAT C ART INT, P316; Duda R., 1973, PATTERN CLASSIFICATI; GALE W, 1993, COMPUT HUMANITIES, V26, P415; GOLDING A, 1999, MACHINE LEARNING; GOLDING AR, 1995, P 3 WORKSH VER LARG; GROVE A, 1998, NEURAL INFORMATION P; HAUSSLER D, 1992, INFORM COMPUT, V100, P78, DOI 10.1016/0890-5401(92)90010-D; Hoffgen K.-U., 1992, Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, DOI 10.1145/130385.130431; JAYNES ET, 1982, P IEEE, V70, P939, DOI 10.1109/PROC.1982.12425; Kearns M., 1993, Proceedings of the Twenty-Fifth Annual ACM Symposium on the Theory of Computing, DOI 10.1145/167088.167200; Kearns M. J., 1992, Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, DOI 10.1145/130385.130424; Kupiec J., 1992, Computer Speech and Language, V6, DOI 10.1016/0885-2308(92)90019-Z; RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626; RATNAPARKHI A, 1994, ARPA; RATNAPARKHI A, 1997, EMNLP 97, P1; Roth D., 1998, Proceedings Fifteenth National Conference on Artificial Intelligence (AAAI-98). Tenth Conference on Innovative Applications of Artificial Intelligence; ROTH D, 1998, COLING ACL 98, P1136; SCHUTZE H, 1995, P 7 C EUR CHAPT ASS; VALIANT LG, 1984, COMMUN ACM, V27, P1134, DOI 10.1145/1968.1972; Vapnik V., 1995, NATURE STAT LEARNING; Vapnik VN, 1982, ESTIMATION DEPENDENC; YAMANISHI K, 1992, MACHINE LEARNING	27	0	0	MORGAN KAUFMANN PUB INC	SAN FRANCISCO	340 PINE STR, 6TH FLR, SAN FRANCISCO, CA 94104-3205 USA			1-55860-613-0				1999							898	904				7	Computer Science, Artificial Intelligence	Computer Science	BR27X	WOS:000165996800128	
B	Shanahan, JG; Baldwin, JF; Thomas, BT; Martin, TP; Campbell, NW; Mirmehdi, M		Dave, RN; Sudkamp, T		Shanahan, JG; Baldwin, JF; Thomas, BT; Martin, TP; Campbell, NW; Mirmehdi, M			Transitioning from recognition to understanding in vision using additive Cartesian granule feature models	18TH INTERNATIONAL CONFERENCE OF THE NORTH AMERICAN FUZZY INFORMATION PROCESSING SOCIETY - NAFIPS			English	Proceedings Paper	18th International Conference of the North-American-Fuzzy-Information-Processing-Society (NAFIPS 99)	JUN 10-12, 1999	NEW YORK, NY	N Amer Fuzzy Informat Proc Soc, IEEE Neural Networks Council, IEEE Syst Man & Cybernet Soc			CLASSIFICATION	Here we propose an approach to object recognition that facilitates the transition fi om recognition to understanding, The proposed approach begins by segmenting the images into regions using standard image processing approaches, which are subsequently classified using a discovered fuzzy Cartesian granule feature classifier. Understanding is made possible through the transparent and succinct nature of the discovered models. The recognition of roads in images is taken as an illustrative problem in the vision domain. The discovered fuzzy models while providing high levels of accuracy (97%), also provide understanding of the problem domain through the transparency of the learnt models. The learning step in the proposed approach is compared with other techniques such as decision trees, naive Bayes and neural networks.	Xerox Res Ctr Europe, Grenoble Ctr, F-38240 Meylan, France	Shanahan, JG (reprint author), Xerox Res Ctr Europe, Grenoble Ctr, 6 Chemin de Maupertuis, F-38240 Meylan, France.						Baldwin J F, 1995, FRIL FUZZY EVIDENTIA; BALDWIN JF, 1998, FUZZ IEEE, P960; BALDWIN JF, 1998, LECT NOTES ARTIF INT, V1566, P26; BALDWIN JF, 1997, FUZZ IEEE, P1295; Campbell NW, 1997, PATTERN RECOGN, V30, P555, DOI 10.1016/S0031-3203(96)00112-4; Campbell NW, 1997, INT J NEURAL SYST, V8, P137, DOI 10.1142/S0129065797000161; Jolliffe I. T., 1986, PRINCIPAL COMPONENT; MACKEON WPJ, 1994, IEE VISION SPEECH SI; MICHALSKI RS, 1998, MACHINE LEARNING DAT, P241; MURPHY SK, 1994, J ARTIFICIAL INTELLI, V2, P1; PENTLAND A, 1994, SPIE C STOR RETR IM; SHANAHAN JG, 1999, UNPUB TRANSITIONING; SHANAHAN JG, 1998, THESIS U BRISTOL BRI; Winston P. H., 1975, PSYCHOL COMPUTER VIS	14	0	0	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA			0-7803-5211-4				1999							710	714		10.1109/NAFIPS.1999.781786		5	Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications	Computer Science	BN34H	WOS:000081666600147	
B	Nigam, K; McCallum, A; Thrun, S; Mitchell, T			AMER ASSOC ARTIFICIAL INTELLIGENCE; AMER ASSOC ARTIFICIAL INTELLIGENCE	Nigam, K; McCallum, A; Thrun, S; Mitchell, T			Learning to classify text from labeled and unlabeled documents	FIFTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-98) AND TENTH CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICAL INTELLIGENCE (IAAI-98) - PROCEEDINGS			English	Proceedings Paper	15th National Conference on Artificial Intelligence (AAAI 98) / 10th Conference on Innovative Applications of Artificial Intelligence (IAAI 98)	JUL 26-30, 1998	MADISON, WI	Amer Assoc Artificial Intelligence, ACM, SIGART, Defense Adv Res Projects Agcy, Microsoft Corp, USN, Off Naval Res, NASA, Ames Res Ctr, Natl Sci Fdn				In many important text classification problems, acquiring class labels for training documents is costly, while gathering large quantities of unlabeled data is cheap. This paper shows that the accuracy of text classifiers trained with a small number of labeled documents can be improved by augmenting this small training set with a large pool of unlabeled documents. We present a theoretical argument showing that, under common assumptions, unlabeled data contain information about the target function. We then introduce an algorithm for learning from labeled and unlabeled text based on the combination of Expectation-Maximization with a naive Bayes classifier. The algorithm first trains a classifier using the available labeled documents, and probabilistically labels the unlabeled documents; it then trains a new classifier using the labels for all the documents, and iterates to convergence. Experimental results, obtained using text from three different real-world tasks, show that the use of unlabeled data reduces classification error by up to 33%.	Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA	Nigam, K (reprint author), Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA.							0	0	0	AMER ASSOC ARTIFICIAL INTELLIGENCE	MENLO PK	445 BURGESS DR, MENLO PK, CA 94025 USA			0-262-51098-7				1998							792	799				8	Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications	Computer Science	BL71Z	WOS:000076427500113	
B	Horton, P; Nakai, K		Gaasterland, T; Karp, P; Karplus, K; Ouzounis, C; Sander, C; Valencia, A		Horton, P; Nakai, K			Better prediction of protein cellular localization sites with the k nearest neighbors classifier	ISMB-97 - FIFTH INTERNATIONAL CONFERENCE ON INTELLIGENT SYSTEMS FOR MOLECULAR BIOLOGY, PROCEEDINGS			English	Proceedings Paper	5th International Conference on Intelligent Systems for Molecular Biology (ISMB-97)	JUN 21-25, 1997	HALKIDIKI, GREECE	Cultural Capital Europe 1997, Thessaloniki, GlaxoWellcome PLC, Int Soc Computat Biol, SmithKline Beecham Corp, Silicon Graph, Sun Microsyst, US DOE, US NIH, US Natl Sci Fdn, Univ Patras, Greece		protein localization; k nearest neighbor classifier; classification; yeast; E-coli		We have compared four classifiers on the problem of predicting the cellular localization sites of proteins in yeast and E.coli. A set of sequence derived features, such as regions of high hydrophobicity, were used for each classifier. The methods compared were a structured probabilistic model specifically designed for the localization problem, the k nearest neighbors classifier, the binary decision tree classifier, and the naive Bayes classifier. The result of tests using stratified cross validation shows the Ic nearest neighbors classifier to perform better than the other methods. In the case of yeast this difference was statistically significant using a cross-validated paired t test. The result is an accuracy of approximately 60% for 10 yeast classes and 86% for 8 E.coli classes. The best previously reported accuracies for these datasets were 55% and 81% respectively.	Univ Calif Berkeley, Div Comp Sci, Berkeley, CA 94720 USA	Horton, P (reprint author), Univ Calif Berkeley, Div Comp Sci, 387 Soda Hall, Berkeley, CA 94720 USA.		Nakai, Kenta/B-7293-2009					0	0	0	AMER ASSOC ARTIFICIAL INTELLIGENCE	MENLO PK	445 BURGESS DR, MENLO PK, CA 94025 USA			1-57735-022-7				1997							147	152				6	Biochemistry & Molecular Biology; Computer Science, Information Systems; Computer Science, Interdisciplinary Applications; Medical Informatics	Biochemistry & Molecular Biology; Computer Science; Medical Informatics	BK49D	WOS:000072320000021	
B	Mani, S; McDermott, S; Pazzani, MJ		Wu, XD; Tsai, J; Pissinou, N; Makki, K		Mani, S; McDermott, S; Pazzani, MJ			Generating models of mental retardation from data with machine learning	1997 IEEE KNOWLEDGE AND DATA ENGINEERING EXCHANGE WORKSHOP, PROCEEDINGS			English	Proceedings Paper	1997 IEEE Knowledge and Data Engineering Exchange Workshop	NOV   04, 1997	NEWPORT BEACH, CA	IEEE Comp Soc				This study focused on generating simple and expressive domain models of Mental Retardation (MR) from data using Knowledge Discovery and Datamining (KDD) methods. 2137 cases (mild or borderline MR) and 2165 controls (randomly selected) from the National Collaborative Perinatal Project (NCPP), a multicentric study involving pregnant mothers and the outcomes, constituted our sample. Twenty attributes (prenatal, perinatal and postnatal), thought to play a role in MR were utilized. The outcome variable (class), was, whether the child was retarded or not, based on the IQ score. Tree learners (C4.5, CART), rule inducers (C4.5Rules,FOCL) and a reference classifier (Naive Bayes) were the machine learning algorithms used for model building. The predictive accuracy ranged from 68.4% (FOCL) to 70.3% (Naive Bayes). CART obtained a sensitivity of 79.0% and also generated highly stable and simple trees across fifty random two-third (training), one-third (testing) partitions of the sample. The algorithms identified emotional/behavioral problem in children as a significant predictor of MR risk. Out-study shows that KDD methods hold promise in recovering useful structure from medical data.		Mani, S (reprint author), UNIV CALIF IRVINE,DEPT INFORMAT & COMP SCI,IRVINE,CA 92697, USA.							0	0	0	I E E E, COMPUTER SOC PRESS	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, LOS ALAMITOS, CA 90720			0-8186-8230-2				1997							114	119				6	Computer Science, Information Systems; Computer Science, Software Engineering; Computer Science, Theory & Methods	Computer Science	BK01W	WOS:A1997BK01W00014	
B	Peot, MA		Horvitz, E; Jensen, F		Peot, MA			Geometric implications of the Naive Bayes assumption	UNCERTAINTY IN ARTIFICIAL INTELLIGENCE			English	Proceedings Paper	12th Conference on Uncertainty in Artificial Intelligence (UAI 96)	AUG 01-04, 1996	PORTLAND, OR	Hugin Expert A S, Informat Extract & Transport, Knowledge Ind, Microsoft Res, Prevision, Ricoh Calif Res Ctr, Rockwell Sci Ctr, Thinkbank				A Naive (or Idiot) Bayes network is a network with a single hypothesis node and several observations that are conditionally independent given the hypothesis. We recently surveyed a number of members of the UAI community and discovered a general lack of understanding of the implications of the Naive Bayes assumption on the kinds of problems that can be solved by these networks. It has long been recognized [Minsky 61] that if observations are binary, the decision surfaces in these networks are hyperplanes. We extend this result (hyperplane separability) to Naive Bayes networks with m-ary observations. In addition, we illustrate the effect of observation-observation dependencies on decision surfaces. Finally, we discuss the implications of these results on knowledge acquisition and research in learning.		Peot, MA (reprint author), ROCKWELL INT SCI CTR,444 HIGH ST,SUITE 400,PALO ALTO,CA 94301, USA.							0	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	2929 CAMPUS DRIVE, SAN MATEO, CA 94403			1-55860-412-X				1996							414	419				6	Computer Science, Artificial Intelligence	Computer Science	BJ16A	WOS:A1996BJ16A00049	
J	LIU, YH; YANG, MC				LIU, YH; YANG, MC			BOUNDED RISK CONDITIONS IN SIMULTANEOUS ESTIMATION OF INDEPENDENT POISSON MEANS	JOURNAL OF STATISTICAL PLANNING AND INFERENCE			English	Article						POISSON MEANS; BOUNDED RISK; BOUNDED MINIMAX ESTIMATOR; SIMULTANEOUS ESTIMATION	DISCRETE EXPONENTIAL-FAMILIES; ENTROPY LOSS	In this paper, we consider the simultaneous estimation of Poisson means under the loss function L(m)(c)(theta, delta) = Sigma(i=1)(p) c(i) theta(-mi)(theta(i)-delta(i))(2), where m(1), ... , m(p) are known real numbers and c(1) , ... , c(p) are positive known constants. A necessary and sufficient condition for the loss functions to have estimators with bounded risk is given. In particular, we find that the naive estimator delta(0)(X) = 0 is the unique minimax admissible estimator under the loss L(m)(c) with ail m(i)'s equal to 2. The question of whether there exists any proper Bayes minimax estimator, under some loss functions, will be addressed in this paper. The estimators proposed by Ghosh et al. (Ann. Statist. 11 (1983), 351-376), etc., which dominate the usual estimator X, are shown to have unbounded risk functions, and truncated estimators having bounded risk are constructed. Further, the truncated estimators are shown to be admissible when it is known that the original estimators are admissible.	NATL CHENG KUNG UNIV,DEPT STAT,TAINAN 70101,TAIWAN; NATL CENT UNIV,GRAD INST STAT,CHUNGLI 32054,TAIWAN							Berger James O., 1985, STATISTICAL DECISION; CHOU JP, 1991, ANN STAT, V19, P314, DOI 10.1214/aos/1176347984; DEY DK, 1986, COMMUN STAT THEORY, V15, P2087, DOI 10.1080/03610928608829236; GHOSH M, 1981, J MULTIVARIATE ANAL, V11, P280, DOI 10.1016/0047-259X(81)90115-9; GHOSH M, 1988, ANN STAT, V16, P278, DOI 10.1214/aos/1176350705; GHOSH M, 1983, STAT DECISION, V1, P183; GHOSH M, 1983, ANN STAT, V11, P351, DOI 10.1214/aos/1176346143; HUDSON HM, 1978, ANN STAT, V6, P473, DOI 10.1214/aos/1176344194; HWANG JT, 1992, COMMUNICATION; HWANG JT, 1982, ANN STAT, V10, P857, DOI 10.1214/aos/1176345876; HWANG JT, 1982, ANN STAT, V10, P1137, DOI 10.1214/aos/1176345979; Kuo L., 1990, STATISTICS DECISIONS, V8, P201; Peng J.C., 1975, 78 STANF U DEP STAT; STEIN CM, 1981, ANN STAT, V9, P1135, DOI 10.1214/aos/1176345632; TSUI KW, 1982, ANN STAT, V10, P93, DOI 10.1214/aos/1176345692; Yang M.C., 1992, Statist. Dec., V10, P1; YANG MC, 1983, STATIST DECISIONS, V11, P357; YANG MC, 1990, COMMUN STAT THEORY, V19, P935, DOI 10.1080/03610929008830240; CLEVENSON ML, 1975, J AM STAT ASSOC, V70, P698, DOI 10.2307/2285958	19	0	0	ELSEVIER SCIENCE BV	AMSTERDAM	PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS	0378-3758			J STAT PLAN INFER	J. Stat. Plan. Infer.	OCT 15	1995	47	3					319	331		10.1016/0378-3758(94)00135-I		13	Statistics & Probability	Mathematics	TC800	WOS:A1995TC80000007	
B	KONONENKO, I		MORIK, K		KONONENKO, I			ID3, SEQUENTIAL BAYES, NAIVE BAYES AND BAYESIAN NEURAL NETWORKS	EWSL 89 : PROCEEDINGS OF THE FOURTH EUROPEAN WORKING SESSION ON LEARNING			English	Proceedings Paper	4TH EUROPEAN WORKING SESSION ON LEARNING ( EWSL 89 )	DEC 04-06, 1989	MONTPELLIER, FRANCE	COMMISS EUROPEAN COMMUNITIES, CNRS, INST NATL RECH INFORMAT & AUTOMAT, REG LANGUEDOC ROUSILLON, DIST MONTPELLIER													0	0	0	PITMAN PUBLISHING LTD	LONDON	LONDON			0-273-08811-4				1990							91	98				8	Psychology, Educational; Information Science & Library Science	Psychology; Information Science & Library Science	BS38F	WOS:A1990BS38F00009	
