<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Machine Learning: Archaeology of a Data Practice</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="Machine Learning: Archaeology of a Data Practice">
  <meta name="generator" content="bookdown 0.3 and GitBook 2.6.7">

  <meta property="og:title" content="Machine Learning: Archaeology of a Data Practice" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Machine Learning: Archaeology of a Data Practice" />
  
  
  

<meta name="author" content="Adrian Mackenzie">


<meta name="date" content="2016-12-10">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="31-differences-blur.html">
<link rel="next" href="33-instituting-patterns.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="1-acknowledgments.html"><a href="1-acknowledgments.html"><i class="fa fa-check"></i><b>1</b> Acknowledgments</a></li>
<li class="chapter" data-level="2" data-path="2-preface.html"><a href="2-preface.html"><i class="fa fa-check"></i><b>2</b> Preface</a></li>
<li class="chapter" data-level="3" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html"><i class="fa fa-check"></i><b>3</b> Introduction: Into the Data</a><ul>
<li class="chapter" data-level="3.1" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#three-accumulations-settings-data-and-devices"><i class="fa fa-check"></i><b>3.1</b> Three accumulations: settings, data and devices</a></li>
<li class="chapter" data-level="3.2" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#who-or-what-is-a-machine-learner"><i class="fa fa-check"></i><b>3.2</b> Who or what is a machine learner?</a></li>
<li class="chapter" data-level="3.3" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#algorithmic-control-to-the-machine-learners"><i class="fa fa-check"></i><b>3.3</b> Algorithmic control to the machine learners?</a></li>
<li class="chapter" data-level="3.4" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#the-archaeology-of-operations"><i class="fa fa-check"></i><b>3.4</b> The archaeology of operations</a></li>
<li class="chapter" data-level="3.5" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#asymmetries-in-common-knowledge"><i class="fa fa-check"></i><b>3.5</b> Asymmetries in common knowledge</a></li>
<li class="chapter" data-level="3.6" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#what-cannot-be-automated"><i class="fa fa-check"></i><b>3.6</b> What cannot be automated?</a></li>
<li class="chapter" data-level="3.7" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#different-fields-in-machine-learning"><i class="fa fa-check"></i><b>3.7</b> Different fields in machine learning?</a></li>
<li class="chapter" data-level="3.8" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#the-diagram-in-critical-thought"><i class="fa fa-check"></i><b>3.8</b> The diagram in critical thought</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html"><i class="fa fa-check"></i><b>4</b> Diagramming machines}</a><ul>
<li class="chapter" data-level="4.1" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#we-dont-have-to-write-programs"><i class="fa fa-check"></i><b>4.1</b> ‘We don’t have to write programs’?</a></li>
<li class="chapter" data-level="4.2" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#the-elements-of-machine-learning"><i class="fa fa-check"></i><b>4.2</b> The elements of machine learning</a></li>
<li class="chapter" data-level="4.3" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#who-reads-machine-learning-textbooks"><i class="fa fa-check"></i><b>4.3</b> Who reads machine learning textbooks?</a></li>
<li class="chapter" data-level="4.4" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#r-a-matrix-of-transformations"><i class="fa fa-check"></i><b>4.4</b> <code>R</code>: a matrix of transformations</a></li>
<li class="chapter" data-level="4.5" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#the-obdurate-mathematical-glint-of-machine-learning"><i class="fa fa-check"></i><b>4.5</b> The obdurate mathematical glint of machine learning</a></li>
<li class="chapter" data-level="4.6" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#cs229-2007-returning-again-and-again-to-certain-features"><i class="fa fa-check"></i><b>4.6</b> CS229, 2007: returning again and again to certain features</a></li>
<li class="chapter" data-level="4.7" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#the-visible-learning-of-machine-learning"><i class="fa fa-check"></i><b>4.7</b> The visible learning of machine learning</a></li>
<li class="chapter" data-level="4.8" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#the-diagram-of-an-operational-formation"><i class="fa fa-check"></i><b>4.8</b> The diagram of an operational formation</a></li>
<li class="chapter" data-level="4.9" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#vectorisation-and-its-consequences"><i class="fa fa-check"></i><b>4.9</b> Vectorisation and its consequences}</a></li>
<li class="chapter" data-level="4.10" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#vector-space-and-geometry"><i class="fa fa-check"></i><b>4.10</b> Vector space and geometry</a></li>
<li class="chapter" data-level="4.11" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#mixing-places"><i class="fa fa-check"></i><b>4.11</b> Mixing places</a></li>
<li class="chapter" data-level="4.12" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#truth-is-no-longer-in-the-table"><i class="fa fa-check"></i><b>4.12</b> Truth is no longer in the table?</a></li>
<li class="chapter" data-level="4.13" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#the-epistopic-fault-line-in-tables"><i class="fa fa-check"></i><b>4.13</b> The epistopic fault line in tables</a></li>
<li class="chapter" data-level="4.14" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#surface-and-depths-the-problem-of-volume-in-data"><i class="fa fa-check"></i><b>4.14</b> Surface and depths: the problem of volume in data</a></li>
<li class="chapter" data-level="4.15" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#vector-space-expansion"><i class="fa fa-check"></i><b>4.15</b> Vector space expansion</a></li>
<li class="chapter" data-level="4.16" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#drawing-lines-in-a-common-space-of-transformation"><i class="fa fa-check"></i><b>4.16</b> Drawing lines in a common space of transformation</a></li>
<li class="chapter" data-level="4.17" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#implicit-vectorization-in-code-and-infrastructures"><i class="fa fa-check"></i><b>4.17</b> Implicit vectorization in code and infrastructures</a></li>
<li class="chapter" data-level="4.18" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#lines-traversing-behind-the-light"><i class="fa fa-check"></i><b>4.18</b> Lines traversing behind the light</a></li>
<li class="chapter" data-level="4.19" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#the-vectorised-table"><i class="fa fa-check"></i><b>4.19</b> The vectorised table?</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-machines-finding-functions.html"><a href="5-machines-finding-functions.html"><i class="fa fa-check"></i><b>5</b> Machines finding functions}</a></li>
<li class="chapter" data-level="6" data-path="6-learning-functions.html"><a href="6-learning-functions.html"><i class="fa fa-check"></i><b>6</b> Learning functions</a></li>
<li class="chapter" data-level="7" data-path="7-supervised-unsupervised-reinforcement-learning-and-functions.html"><a href="7-supervised-unsupervised-reinforcement-learning-and-functions.html"><i class="fa fa-check"></i><b>7</b> Supervised, unsupervised, reinforcement learning and functions</a></li>
<li class="chapter" data-level="8" data-path="8-which-function-operates.html"><a href="8-which-function-operates.html"><i class="fa fa-check"></i><b>8</b> Which function operates?</a></li>
<li class="chapter" data-level="9" data-path="9-what-does-a-function-learn.html"><a href="9-what-does-a-function-learn.html"><i class="fa fa-check"></i><b>9</b> What does a function learn?</a></li>
<li class="chapter" data-level="10" data-path="10-observing-with-curves-the-logistic-function.html"><a href="10-observing-with-curves-the-logistic-function.html"><i class="fa fa-check"></i><b>10</b> Observing with curves: the logistic function</a></li>
<li class="chapter" data-level="11" data-path="11-the-cost-of-curves-in-machine-learning.html"><a href="11-the-cost-of-curves-in-machine-learning.html"><i class="fa fa-check"></i><b>11</b> The cost of curves in machine learning</a></li>
<li class="chapter" data-level="12" data-path="12-curves-and-the-variation-in-models.html"><a href="12-curves-and-the-variation-in-models.html"><i class="fa fa-check"></i><b>12</b> Curves and the variation in models</a></li>
<li class="chapter" data-level="13" data-path="13-observing-costs-losses-and-objectives-through-optimisation.html"><a href="13-observing-costs-losses-and-objectives-through-optimisation.html"><i class="fa fa-check"></i><b>13</b> Observing costs, losses and objectives through optimisation</a></li>
<li class="chapter" data-level="14" data-path="14-gradients-as-partial-observers.html"><a href="14-gradients-as-partial-observers.html"><i class="fa fa-check"></i><b>14</b> Gradients as partial observers</a></li>
<li class="chapter" data-level="15" data-path="15-the-power-to-learn.html"><a href="15-the-power-to-learn.html"><i class="fa fa-check"></i><b>15</b> The power to learn</a></li>
<li class="chapter" data-level="16" data-path="16-probabilisation-and-the-taming-of-machines.html"><a href="16-probabilisation-and-the-taming-of-machines.html"><i class="fa fa-check"></i><b>16</b> Probabilisation and the Taming of Machines}</a></li>
<li class="chapter" data-level="17" data-path="17-data-reduces-uncertainty.html"><a href="17-data-reduces-uncertainty.html"><i class="fa fa-check"></i><b>17</b> Data reduces uncertainty?</a></li>
<li class="chapter" data-level="18" data-path="18-machine-learning-as-statistics-inside-out.html"><a href="18-machine-learning-as-statistics-inside-out.html"><i class="fa fa-check"></i><b>18</b> Machine learning as statistics inside out</a></li>
<li class="chapter" data-level="19" data-path="19-distributed-probabilities.html"><a href="19-distributed-probabilities.html"><i class="fa fa-check"></i><b>19</b> Distributed probabilities</a></li>
<li class="chapter" data-level="20" data-path="20-naive-bayes-and-the-distribution-of-probabilities.html"><a href="20-naive-bayes-and-the-distribution-of-probabilities.html"><i class="fa fa-check"></i><b>20</b> Naive Bayes and the distribution of probabilities</a></li>
<li class="chapter" data-level="21" data-path="21-spam-when-foralln-is-too-much.html"><a href="21-spam-when-foralln-is-too-much.html"><i class="fa fa-check"></i><b>21</b> Spam: when <span class="math inline">\(\forall{N}\)</span> is too much?</a></li>
<li class="chapter" data-level="22" data-path="22-the-improbable-success-of-the-naive-bayes-classifier.html"><a href="22-the-improbable-success-of-the-naive-bayes-classifier.html"><i class="fa fa-check"></i><b>22</b> The improbable success of the Naive Bayes classifier</a></li>
<li class="chapter" data-level="23" data-path="23-ancestral-probabilities-in-documents-inference-and-prediction.html"><a href="23-ancestral-probabilities-in-documents-inference-and-prediction.html"><i class="fa fa-check"></i><b>23</b> Ancestral probabilities in documents: inference and prediction</a></li>
<li class="chapter" data-level="24" data-path="24-statistical-decompositions-bias-variance-and-observed-errors.html"><a href="24-statistical-decompositions-bias-variance-and-observed-errors.html"><i class="fa fa-check"></i><b>24</b> Statistical decompositions: bias, variance and observed errors</a></li>
<li class="chapter" data-level="25" data-path="25-does-machine-learning-construct-a-new-statistical-reality.html"><a href="25-does-machine-learning-construct-a-new-statistical-reality.html"><i class="fa fa-check"></i><b>25</b> Does machine learning construct a new statistical reality?</a></li>
<li class="chapter" data-level="26" data-path="26-patterns-and-differences.html"><a href="26-patterns-and-differences.html"><i class="fa fa-check"></i><b>26</b> Patterns and differences</a></li>
<li class="chapter" data-level="27" data-path="27-splitting-and-the-growth-of-trees.html"><a href="27-splitting-and-the-growth-of-trees.html"><i class="fa fa-check"></i><b>27</b> Splitting and the growth of trees</a></li>
<li class="chapter" data-level="28" data-path="28-differences-in-recursive-partitioning.html"><a href="28-differences-in-recursive-partitioning.html"><i class="fa fa-check"></i><b>28</b> 1984: Differences in recursive partitioning</a></li>
<li class="chapter" data-level="29" data-path="29-limiting-differences.html"><a href="29-limiting-differences.html"><i class="fa fa-check"></i><b>29</b> Limiting differences</a></li>
<li class="chapter" data-level="30" data-path="30-the-successful-dispersion-of-the-support-vector-machine.html"><a href="30-the-successful-dispersion-of-the-support-vector-machine.html"><i class="fa fa-check"></i><b>30</b> The successful dispersion of the support vector machine</a></li>
<li class="chapter" data-level="31" data-path="31-differences-blur.html"><a href="31-differences-blur.html"><i class="fa fa-check"></i><b>31</b> Differences blur?</a></li>
<li class="chapter" data-level="32" data-path="32-bending-the-decision-boundary.html"><a href="32-bending-the-decision-boundary.html"><i class="fa fa-check"></i><b>32</b> Bending the decision boundary</a></li>
<li class="chapter" data-level="33" data-path="33-instituting-patterns.html"><a href="33-instituting-patterns.html"><i class="fa fa-check"></i><b>33</b> Instituting patterns</a></li>
<li class="chapter" data-level="34" data-path="34-regularizing-and-materializing-objects.html"><a href="34-regularizing-and-materializing-objects.html"><i class="fa fa-check"></i><b>34</b> Regularizing and materializing objects}</a></li>
<li class="chapter" data-level="35" data-path="35-genomic-referentiality-and-materiality.html"><a href="35-genomic-referentiality-and-materiality.html"><i class="fa fa-check"></i><b>35</b> Genomic referentiality and materiality</a></li>
<li class="chapter" data-level="36" data-path="36-the-genome-as-threshold-object.html"><a href="36-the-genome-as-threshold-object.html"><i class="fa fa-check"></i><b>36</b> The genome as threshold object</a></li>
<li class="chapter" data-level="37" data-path="37-genomic-knowledges-and-their-datasets.html"><a href="37-genomic-knowledges-and-their-datasets.html"><i class="fa fa-check"></i><b>37</b> Genomic knowledges and their datasets</a></li>
<li class="chapter" data-level="38" data-path="38-the-advent-of-wide-dirty-and-mixed-data.html"><a href="38-the-advent-of-wide-dirty-and-mixed-data.html"><i class="fa fa-check"></i><b>38</b> The advent of ‘wide, dirty and mixed’ data</a></li>
<li class="chapter" data-level="39" data-path="39-cross-validating-machine-learning-in-genomics.html"><a href="39-cross-validating-machine-learning-in-genomics.html"><i class="fa fa-check"></i><b>39</b> Cross-validating machine learning in genomics</a></li>
<li class="chapter" data-level="40" data-path="40-proliferation-of-discoveries.html"><a href="40-proliferation-of-discoveries.html"><i class="fa fa-check"></i><b>40</b> Proliferation of discoveries</a></li>
<li class="chapter" data-level="41" data-path="41-variations-in-the-object-or-in-the-machine-learner.html"><a href="41-variations-in-the-object-or-in-the-machine-learner.html"><i class="fa fa-check"></i><b>41</b> Variations in the object or in the machine learner?</a></li>
<li class="chapter" data-level="42" data-path="42-whole-genome-functions.html"><a href="42-whole-genome-functions.html"><i class="fa fa-check"></i><b>42</b> Whole genome functions</a></li>
<li class="chapter" data-level="43" data-path="43-propagating-subject-positions.html"><a href="43-propagating-subject-positions.html"><i class="fa fa-check"></i><b>43</b> Propagating subject positions}</a></li>
<li class="chapter" data-level="44" data-path="44-propagation-across-human-machine-boundaries.html"><a href="44-propagation-across-human-machine-boundaries.html"><i class="fa fa-check"></i><b>44</b> Propagation across human-machine boundaries</a></li>
<li class="chapter" data-level="45" data-path="45-competitive-positioning.html"><a href="45-competitive-positioning.html"><i class="fa fa-check"></i><b>45</b> Competitive positioning</a></li>
<li class="chapter" data-level="46" data-path="46-a-privileged-machine-and-its-diagrammatic-forms.html"><a href="46-a-privileged-machine-and-its-diagrammatic-forms.html"><i class="fa fa-check"></i><b>46</b> A privileged machine and its diagrammatic forms</a></li>
<li class="chapter" data-level="47" data-path="47-varying-subject-positions-in-code.html"><a href="47-varying-subject-positions-in-code.html"><i class="fa fa-check"></i><b>47</b> Varying subject positions in code</a></li>
<li class="chapter" data-level="48" data-path="48-the-subjects-of-a-hidden-operation.html"><a href="48-the-subjects-of-a-hidden-operation.html"><i class="fa fa-check"></i><b>48</b> The subjects of a hidden operation</a></li>
<li class="chapter" data-level="49" data-path="49-algorithms-that-propagate-errors.html"><a href="49-algorithms-that-propagate-errors.html"><i class="fa fa-check"></i><b>49</b> Algorithms that propagate errors</a></li>
<li class="chapter" data-level="50" data-path="50-competitions-as-examination.html"><a href="50-competitions-as-examination.html"><i class="fa fa-check"></i><b>50</b> Competitions as examination</a></li>
<li class="chapter" data-level="51" data-path="51-superimposing-power-and-knowledge.html"><a href="51-superimposing-power-and-knowledge.html"><i class="fa fa-check"></i><b>51</b> Superimposing power and knowledge</a></li>
<li class="chapter" data-level="52" data-path="52-ranked-subject-positions.html"><a href="52-ranked-subject-positions.html"><i class="fa fa-check"></i><b>52</b> Ranked subject positions</a></li>
<li class="chapter" data-level="53" data-path="53-conclusion-out-of-the-data.html"><a href="53-conclusion-out-of-the-data.html"><i class="fa fa-check"></i><b>53</b> Conclusion: Out of the Data}</a></li>
<li class="chapter" data-level="54" data-path="54-machine-learners.html"><a href="54-machine-learners.html"><i class="fa fa-check"></i><b>54</b> 250,000 machine learners</a></li>
<li class="chapter" data-level="55" data-path="55-a-summary-of-the-argument.html"><a href="55-a-summary-of-the-argument.html"><i class="fa fa-check"></i><b>55</b> A summary of the argument</a></li>
<li class="chapter" data-level="56" data-path="56-in-situ-hybridization.html"><a href="56-in-situ-hybridization.html"><i class="fa fa-check"></i><b>56</b> In-situ hybridization</a></li>
<li class="chapter" data-level="57" data-path="57-critical-operational-practice.html"><a href="57-critical-operational-practice.html"><i class="fa fa-check"></i><b>57</b> Critical operational practice?</a></li>
<li class="chapter" data-level="58" data-path="58-obstacles-to-the-work-of-freeing-machine-learning.html"><a href="58-obstacles-to-the-work-of-freeing-machine-learning.html"><i class="fa fa-check"></i><b>58</b> Obstacles to the work of freeing machine learning</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning: Archaeology of a Data Practice</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="bending-the-decision-boundary" class="section level1">
<h1><span class="header-section-number">32</span> Bending the decision boundary</h1>
<p>The support vector machine reinstates a linear decision boundary as the enunciative mode of difference. Yet it transforms that boundary. In the abstract of their 1995 paper, Cortes and Vapnik briefly describe the how the support vector machine revises the linear decision surface:</p>
<blockquote>
<p>The support-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed <span class="citation">[@Cortes_1995, 273]</span> </p>
</blockquote>
<p>Another example of vector space transformation (discussed in chapter ), this ‘very high-dimension feature space’ is explicitly made to support ‘a linear decision surface,’ just as Fisher’s linear discriminant analysis had. But this linear decision surface is now located amidst a non-linear mapping of the data.<a href="#fn74" class="footnoteRef" id="fnref74"><sup>74</sup></a> Cortes and Vapnik’s support vector machine constructs a new domain - ‘a very high dimension feature space’ – where inseparable differences start to disentangle themselves. The constructed dimensions do not index new sources or kinds of data. Instead, the support vector machine transforms the vector space into a much higher dimension.</p>
<p>As Vapnik writes in the preface to the second edition of <em>The Nature of Statistical Learning Theory</em> <span class="citation">[@Vapnik_1999, vii]</span>, ‘in contrast to classical methods of statistics where in order to control performance one decreases the dimensionality of a feature space, the SVM dramatically increases dimensionality’ (vii).  From the standpoint of pattern recognition, this often vastly augmented vector space should make it harder to locate patterns. A linear or planar decision surface in high dimensional space maps onto a curving even labyrinthine decision boundary when projected back onto the original vector space (see the curving decision boundaries in figure ). In certain cases, machine learners multiply dimensions in data in the name of differentiation, classification, and prediction. Many of the techniques that have accumulated or been gathered into machine learning flatten variations and differences into lines and planes, but not always by reducing them. In fact, random forests, neural networks and support vector machines exemplify a counter-movement that maximises variety in the name of differentiation.<a href="#fn75" class="footnoteRef" id="fnref75"><sup>75</sup></a> Research in machine learning, whether it has been primarily statistical, mathematical or computational, countenances and addresses problems of non-linear classification through <em>dimensional expansion</em>. </p>
<p>The powerful augmentation characteristic of the support vector machine works through diagrammatic substitution.  Consider the expression shown below in equation :</p>

<p><span class="citation">[@Hastie_2009, 423]</span></p>
<p>In equation , a re-mapping of equation  occurs particularly through the substitution of a product <span class="math inline">\(\langle h(x_i), h(x_i^&#39;)\rangle\)</span> for <span class="math inline">\(x\)</span>. All of the data <span class="math inline">\(x\)</span> is re-mapped using some function <span class="math inline">\(h(X)\)</span> into a new higher dimensional space. What would be the value of a more complicated space? As Leo Breiman writes in his account of the development of the support vector machine:</p>
<blockquote>
<p>In two-class data, separability by a hyperplane does not often occur. However, let us increase the dimensionality by adding as additional predictor variables all quadratic monomials in the original predictor variables. … A hyperplane in the original variables plus quadratic monomials in the original variables is a more complex creature. The possibility of separation is greater. If no separation occurs, add cubic monomials as input features. If there are originally 30 predictor variables, then there are about 40,000 features if monomials up to the fourth degree are added. <span class="citation">[@Breiman_2001a, 209]</span> </p>
</blockquote>
<p>The extravagant dimensionality released in the shift from 30 to 40,000 variables vastly expands the number of possible decision surfaces or hyperplanes that might be instituted in the vector space. The support vector machine, however, corrals and manages this massive and sometimes infinite generation of differences  at the same time by only allowing this expansion to occur along particular lines marked out by <em>kernel functions</em>. While the support vector machine maintains a commitment to the separating hyperplane, a linear form albeit with soft margins, it re-constitutes that plane in newly created vector spaces constrained by certain key structural features that render them computationally tractable. On the one hand, a promise of infinite expansion and associated freedom from the rigidity of lines, and on the other hand, a mode of expansion can only countenance a limited range of movements prescribed by the kernel functions (polynomial, radial, etc.). </p>
<p><em>Elements of Statistical Learning</em> puts it this way:</p>
<blockquote>
<p>We can represent the optimization problem and its solution in a special way that only involves the input features via inner products. We do this directly for the transformed feature vectors <span class="math inline">\(h(x_i)\)</span>. We then see that for particular choices of <span class="math inline">\(h\)</span>, these inner products can be computed very cheaply <span class="citation">[@Hastie_2009, 423]</span></p>
</blockquote>
<p>The terminology here takes us back the vector space (see chapter ) that machine learning inhabits. The ‘inner product’ or ‘the convolution of the dot-product’ described by Cortes and Vapnik come from this space, in which the distances or alignments between whatever can be rendered as a vector can be calculated <em>en masse</em>. </p>
<p><em>Top 10 Algorithms for Data Mining</em> <span class="citation">[@Wu_2008]</span>, a widely cited computer science account of data mining, justifies the operation in relation to entangled differences:</p>
<blockquote>
<p>The kernel trick is another commonly used technique to solve linearly inseparably problems. This issue is to define an appropriate kernel function based on the <em>inner product</em> between the given data, as a nonlinear transformation of data from the input space to a feature space with higher (even infinite) dimension in order to make the problems linearly separable. The underlying justification can be found in <em>Cover’s theorem</em> on the separability of patterns; that is, a complex pattern classification problem case in a high-dimensional space is <em>more likely</em> to be linearly separable than in a low dimensional space <span class="citation">[@Wu_2008, 42]</span>. </p>
</blockquote>
<p>The ‘kernel trick’ that overcomes inseparability remaps an already transformed vector space – the inner product of all the vectors in the data – into a higher dimensional space defined by functions such as <span class="math inline">\(f(x) = x_i^2 + x_i^3\)</span>. The trick is no simple technical trick, since as Cortes and Vapnik point out it relies on substantial mathematical developments in the 1960s. ‘The idea of constructing support-vector networks comes from considering general forms of the dot-product in a Hilbert space (Anderson &amp; Bahadur, 1966)’, write Cortes and Vapnik <span class="citation">[@Cortes_1995, 283]</span>. It is a trick, however, in the sense that it is ‘can be computed very cheaply.’<a href="#fn76" class="footnoteRef" id="fnref76"><sup>76</sup></a></p>
<p>What does the transformed feature space combined with the computational short cut of the inner product do in practice? Describing the generalization error – the errors made when a model classifies hitherto unseen data  – Cortes and Vapnik highlight the growth in dimensionality introduced by the technique of the support vector in classifying the handwritten numbers of the <code>mnist</code> data. They recount how the technique exponentially increases the dimensionality of the feature space and how the error rate on difficult-to-classify handwritten digits drops correspondingly. When the feature space has 256 dimensions (the given dimensions of the 16x16 pixel digits), the error rate is around 12%. As the dimensionality grows to 33,000, then a million, a billion, a trillion and so forth (up to <span class="math inline">\(1 x 10^{16}\)</span> dimensions), the error rate drops to just over 4%, close to the errors made by ‘human performance’ (2.5%) <span class="citation">[@Cortes_1995, 288]</span>. </p>
</div>
<div class="footnotes">
<hr />
<ol start="74">
<li id="fn74"><p>As we have seen on several occasions, the vector space invites a certain form of classification based on the search for the best line, the line of best fit, or the most discriminating line, the line that best divides things from each other. Linear regression is not called ‘linear’ for no reason. And Fisher’s ‘discriminant functions’ were later called ‘linear discriminant analysis’ for the same reason: they divide the vector space into different regions (‘decision regions’) separated by ‘linear decision boundaries’ <span class="citation">[@Alpaydin_2010, 53]</span>. Almost all machine learners are aware of and try to address the idealism or abstraction of the line or plane. <a href="32-bending-the-decision-boundary.html#fnref74">↩</a></p></li>
<li id="fn75"><p>Despite the in-principle commitment to any form of function, machine learning strongly prefers forms that can either be visualised on a plane (using the visual grammar of lines, dots, axes, labels, colours, shapes, etc.), or can be computed in form of matrix or vectorised calculations focused on planes.  Many of the techniques that grapple with complicated datasets seek to reduce their dimensionality so that lines, planes and regular curves can be applied to them: multi-dimensional scaling (MDS), factor analysis, principal component analysis (PCA), or self-organising maps (SOM) are just a few examples of this.    <a href="32-bending-the-decision-boundary.html#fnref75">↩</a></p></li>
<li id="fn76"><p>This cheapness appeared already in the cat machine learner discussed in the introduction. Heather McArthur’s<code>kittydar</code> cat image classifier implemented a support vector machine in Javasript that runs in a browser. Cats are classified cheaply there. <a href="32-bending-the-decision-boundary.html#fnref76">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="31-differences-blur.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="33-instituting-patterns.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
