<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Machine Learning: Archaeology of a Data Practice</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="Machine Learning: Archaeology of a Data Practice">
  <meta name="generator" content="bookdown 0.3 and GitBook 2.6.7">

  <meta property="og:title" content="Machine Learning: Archaeology of a Data Practice" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Machine Learning: Archaeology of a Data Practice" />
  
  
  

<meta name="author" content="Adrian Mackenzie">


<meta name="date" content="2016-12-10">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="70-propagation-across-human-machine-boundaries.html">
<link rel="next" href="72-a-privileged-machine-and-its-diagrammatic-forms.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="1-acknowledgments.html"><a href="1-acknowledgments.html"><i class="fa fa-check"></i><b>1</b> Acknowledgments</a></li>
<li class="chapter" data-level="2" data-path="2-preface.html"><a href="2-preface.html"><i class="fa fa-check"></i><b>2</b> Preface</a></li>
<li class="chapter" data-level="3" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html"><i class="fa fa-check"></i><b>3</b> Introduction: Into the Data</a></li>
<li class="chapter" data-level="4" data-path="4-three-accumulations-settings-data-and-devices.html"><a href="4-three-accumulations-settings-data-and-devices.html"><i class="fa fa-check"></i><b>4</b> Three accumulations: settings, data and devices</a></li>
<li class="chapter" data-level="5" data-path="5-who-or-what-is-a-machine-learner.html"><a href="5-who-or-what-is-a-machine-learner.html"><i class="fa fa-check"></i><b>5</b> Who or what is a machine learner?</a></li>
<li class="chapter" data-level="6" data-path="6-algorithmic-control-to-the-machine-learners.html"><a href="6-algorithmic-control-to-the-machine-learners.html"><i class="fa fa-check"></i><b>6</b> Algorithmic control to the machine learners?</a></li>
<li class="chapter" data-level="7" data-path="7-the-archaeology-of-operations.html"><a href="7-the-archaeology-of-operations.html"><i class="fa fa-check"></i><b>7</b> The archaeology of operations</a></li>
<li class="chapter" data-level="8" data-path="8-asymmetries-in-common-knowledge.html"><a href="8-asymmetries-in-common-knowledge.html"><i class="fa fa-check"></i><b>8</b> Asymmetries in common knowledge</a></li>
<li class="chapter" data-level="9" data-path="9-what-cannot-be-automated.html"><a href="9-what-cannot-be-automated.html"><i class="fa fa-check"></i><b>9</b> What cannot be automated?</a></li>
<li class="chapter" data-level="10" data-path="10-different-fields-in-machine-learning.html"><a href="10-different-fields-in-machine-learning.html"><i class="fa fa-check"></i><b>10</b> Different fields in machine learning?</a></li>
<li class="chapter" data-level="11" data-path="11-the-diagram-in-critical-thought.html"><a href="11-the-diagram-in-critical-thought.html"><i class="fa fa-check"></i><b>11</b> The diagram in critical thought</a></li>
<li class="chapter" data-level="12" data-path="12-we-dont-have-to-write-programs.html"><a href="12-we-dont-have-to-write-programs.html"><i class="fa fa-check"></i><b>12</b> ‘We don’t have to write programs’?</a></li>
<li class="chapter" data-level="13" data-path="13-the-elements-of-machine-learning.html"><a href="13-the-elements-of-machine-learning.html"><i class="fa fa-check"></i><b>13</b> The elements of machine learning</a></li>
<li class="chapter" data-level="14" data-path="14-who-reads-machine-learning-textbooks.html"><a href="14-who-reads-machine-learning-textbooks.html"><i class="fa fa-check"></i><b>14</b> Who reads machine learning textbooks?</a></li>
<li class="chapter" data-level="15" data-path="15-r-a-matrix-of-transformations.html"><a href="15-r-a-matrix-of-transformations.html"><i class="fa fa-check"></i><b>15</b> <code>R</code>: a matrix of transformations</a></li>
<li class="chapter" data-level="16" data-path="16-the-obdurate-mathematical-glint-of-machine-learning.html"><a href="16-the-obdurate-mathematical-glint-of-machine-learning.html"><i class="fa fa-check"></i><b>16</b> The obdurate mathematical glint of machine learning</a></li>
<li class="chapter" data-level="17" data-path="17-cs229-2007-returning-again-and-again-to-certain-features.html"><a href="17-cs229-2007-returning-again-and-again-to-certain-features.html"><i class="fa fa-check"></i><b>17</b> CS229, 2007: returning again and again to certain features</a></li>
<li class="chapter" data-level="18" data-path="18-the-visible-learning-of-machine-learning.html"><a href="18-the-visible-learning-of-machine-learning.html"><i class="fa fa-check"></i><b>18</b> The visible learning of machine learning</a></li>
<li class="chapter" data-level="19" data-path="19-the-diagram-of-an-operational-formation.html"><a href="19-the-diagram-of-an-operational-formation.html"><i class="fa fa-check"></i><b>19</b> The diagram of an operational formation</a></li>
<li class="chapter" data-level="20" data-path="20-vectorisation-and-its-consequences.html"><a href="20-vectorisation-and-its-consequences.html"><i class="fa fa-check"></i><b>20</b> Vectorisation and its consequences}</a></li>
<li class="chapter" data-level="21" data-path="21-vector-space-and-geometry.html"><a href="21-vector-space-and-geometry.html"><i class="fa fa-check"></i><b>21</b> Vector space and geometry</a></li>
<li class="chapter" data-level="22" data-path="22-mixing-places.html"><a href="22-mixing-places.html"><i class="fa fa-check"></i><b>22</b> Mixing places</a></li>
<li class="chapter" data-level="23" data-path="23-truth-is-no-longer-in-the-table.html"><a href="23-truth-is-no-longer-in-the-table.html"><i class="fa fa-check"></i><b>23</b> Truth is no longer in the table?</a></li>
<li class="chapter" data-level="24" data-path="24-the-epistopic-fault-line-in-tables.html"><a href="24-the-epistopic-fault-line-in-tables.html"><i class="fa fa-check"></i><b>24</b> The epistopic fault line in tables</a></li>
<li class="chapter" data-level="25" data-path="25-surface-and-depths-the-problem-of-volume-in-data.html"><a href="25-surface-and-depths-the-problem-of-volume-in-data.html"><i class="fa fa-check"></i><b>25</b> Surface and depths: the problem of volume in data</a></li>
<li class="chapter" data-level="26" data-path="26-vector-space-expansion.html"><a href="26-vector-space-expansion.html"><i class="fa fa-check"></i><b>26</b> Vector space expansion</a></li>
<li class="chapter" data-level="27" data-path="27-drawing-lines-in-a-common-space-of-transformation.html"><a href="27-drawing-lines-in-a-common-space-of-transformation.html"><i class="fa fa-check"></i><b>27</b> Drawing lines in a common space of transformation</a></li>
<li class="chapter" data-level="28" data-path="28-implicit-vectorization-in-code-and-infrastructures.html"><a href="28-implicit-vectorization-in-code-and-infrastructures.html"><i class="fa fa-check"></i><b>28</b> Implicit vectorization in code and infrastructures</a></li>
<li class="chapter" data-level="29" data-path="29-lines-traversing-behind-the-light.html"><a href="29-lines-traversing-behind-the-light.html"><i class="fa fa-check"></i><b>29</b> Lines traversing behind the light</a></li>
<li class="chapter" data-level="30" data-path="30-the-vectorised-table.html"><a href="30-the-vectorised-table.html"><i class="fa fa-check"></i><b>30</b> The vectorised table?</a></li>
<li class="chapter" data-level="31" data-path="31-machines-finding-functions.html"><a href="31-machines-finding-functions.html"><i class="fa fa-check"></i><b>31</b> Machines finding functions}</a></li>
<li class="chapter" data-level="32" data-path="32-learning-functions.html"><a href="32-learning-functions.html"><i class="fa fa-check"></i><b>32</b> Learning functions</a></li>
<li class="chapter" data-level="33" data-path="33-supervised-unsupervised-reinforcement-learning-and-functions.html"><a href="33-supervised-unsupervised-reinforcement-learning-and-functions.html"><i class="fa fa-check"></i><b>33</b> Supervised, unsupervised, reinforcement learning and functions</a></li>
<li class="chapter" data-level="34" data-path="34-which-function-operates.html"><a href="34-which-function-operates.html"><i class="fa fa-check"></i><b>34</b> Which function operates?</a></li>
<li class="chapter" data-level="35" data-path="35-what-does-a-function-learn.html"><a href="35-what-does-a-function-learn.html"><i class="fa fa-check"></i><b>35</b> What does a function learn?</a></li>
<li class="chapter" data-level="36" data-path="36-observing-with-curves-the-logistic-function.html"><a href="36-observing-with-curves-the-logistic-function.html"><i class="fa fa-check"></i><b>36</b> Observing with curves: the logistic function</a></li>
<li class="chapter" data-level="37" data-path="37-the-cost-of-curves-in-machine-learning.html"><a href="37-the-cost-of-curves-in-machine-learning.html"><i class="fa fa-check"></i><b>37</b> The cost of curves in machine learning</a></li>
<li class="chapter" data-level="38" data-path="38-curves-and-the-variation-in-models.html"><a href="38-curves-and-the-variation-in-models.html"><i class="fa fa-check"></i><b>38</b> Curves and the variation in models</a></li>
<li class="chapter" data-level="39" data-path="39-observing-costs-losses-and-objectives-through-optimisation.html"><a href="39-observing-costs-losses-and-objectives-through-optimisation.html"><i class="fa fa-check"></i><b>39</b> Observing costs, losses and objectives through optimisation</a></li>
<li class="chapter" data-level="40" data-path="40-gradients-as-partial-observers.html"><a href="40-gradients-as-partial-observers.html"><i class="fa fa-check"></i><b>40</b> Gradients as partial observers</a></li>
<li class="chapter" data-level="41" data-path="41-the-power-to-learn.html"><a href="41-the-power-to-learn.html"><i class="fa fa-check"></i><b>41</b> The power to learn</a></li>
<li class="chapter" data-level="42" data-path="42-probabilisation-and-the-taming-of-machines.html"><a href="42-probabilisation-and-the-taming-of-machines.html"><i class="fa fa-check"></i><b>42</b> Probabilisation and the Taming of Machines}</a></li>
<li class="chapter" data-level="43" data-path="43-data-reduces-uncertainty.html"><a href="43-data-reduces-uncertainty.html"><i class="fa fa-check"></i><b>43</b> Data reduces uncertainty?</a></li>
<li class="chapter" data-level="44" data-path="44-machine-learning-as-statistics-inside-out.html"><a href="44-machine-learning-as-statistics-inside-out.html"><i class="fa fa-check"></i><b>44</b> Machine learning as statistics inside out</a></li>
<li class="chapter" data-level="45" data-path="45-distributed-probabilities.html"><a href="45-distributed-probabilities.html"><i class="fa fa-check"></i><b>45</b> Distributed probabilities</a></li>
<li class="chapter" data-level="46" data-path="46-naive-bayes-and-the-distribution-of-probabilities.html"><a href="46-naive-bayes-and-the-distribution-of-probabilities.html"><i class="fa fa-check"></i><b>46</b> Naive Bayes and the distribution of probabilities</a></li>
<li class="chapter" data-level="47" data-path="47-spam-when-foralln-is-too-much.html"><a href="47-spam-when-foralln-is-too-much.html"><i class="fa fa-check"></i><b>47</b> Spam: when <span class="math inline">\(\forall{N}\)</span> is too much?</a></li>
<li class="chapter" data-level="48" data-path="48-the-improbable-success-of-the-naive-bayes-classifier.html"><a href="48-the-improbable-success-of-the-naive-bayes-classifier.html"><i class="fa fa-check"></i><b>48</b> The improbable success of the Naive Bayes classifier</a></li>
<li class="chapter" data-level="49" data-path="49-ancestral-probabilities-in-documents-inference-and-prediction.html"><a href="49-ancestral-probabilities-in-documents-inference-and-prediction.html"><i class="fa fa-check"></i><b>49</b> Ancestral probabilities in documents: inference and prediction</a></li>
<li class="chapter" data-level="50" data-path="50-statistical-decompositions-bias-variance-and-observed-errors.html"><a href="50-statistical-decompositions-bias-variance-and-observed-errors.html"><i class="fa fa-check"></i><b>50</b> Statistical decompositions: bias, variance and observed errors</a></li>
<li class="chapter" data-level="51" data-path="51-does-machine-learning-construct-a-new-statistical-reality.html"><a href="51-does-machine-learning-construct-a-new-statistical-reality.html"><i class="fa fa-check"></i><b>51</b> Does machine learning construct a new statistical reality?</a></li>
<li class="chapter" data-level="52" data-path="52-patterns-and-differences.html"><a href="52-patterns-and-differences.html"><i class="fa fa-check"></i><b>52</b> Patterns and differences</a></li>
<li class="chapter" data-level="53" data-path="53-splitting-and-the-growth-of-trees.html"><a href="53-splitting-and-the-growth-of-trees.html"><i class="fa fa-check"></i><b>53</b> Splitting and the growth of trees</a></li>
<li class="chapter" data-level="54" data-path="54-differences-in-recursive-partitioning.html"><a href="54-differences-in-recursive-partitioning.html"><i class="fa fa-check"></i><b>54</b> 1984: Differences in recursive partitioning</a></li>
<li class="chapter" data-level="55" data-path="55-limiting-differences.html"><a href="55-limiting-differences.html"><i class="fa fa-check"></i><b>55</b> Limiting differences</a></li>
<li class="chapter" data-level="56" data-path="56-the-successful-dispersion-of-the-support-vector-machine.html"><a href="56-the-successful-dispersion-of-the-support-vector-machine.html"><i class="fa fa-check"></i><b>56</b> The successful dispersion of the support vector machine</a></li>
<li class="chapter" data-level="57" data-path="57-differences-blur.html"><a href="57-differences-blur.html"><i class="fa fa-check"></i><b>57</b> Differences blur?</a></li>
<li class="chapter" data-level="58" data-path="58-bending-the-decision-boundary.html"><a href="58-bending-the-decision-boundary.html"><i class="fa fa-check"></i><b>58</b> Bending the decision boundary</a></li>
<li class="chapter" data-level="59" data-path="59-instituting-patterns.html"><a href="59-instituting-patterns.html"><i class="fa fa-check"></i><b>59</b> Instituting patterns</a></li>
<li class="chapter" data-level="60" data-path="60-regularizing-and-materializing-objects.html"><a href="60-regularizing-and-materializing-objects.html"><i class="fa fa-check"></i><b>60</b> Regularizing and materializing objects}</a></li>
<li class="chapter" data-level="61" data-path="61-genomic-referentiality-and-materiality.html"><a href="61-genomic-referentiality-and-materiality.html"><i class="fa fa-check"></i><b>61</b> Genomic referentiality and materiality</a></li>
<li class="chapter" data-level="62" data-path="62-the-genome-as-threshold-object.html"><a href="62-the-genome-as-threshold-object.html"><i class="fa fa-check"></i><b>62</b> The genome as threshold object</a></li>
<li class="chapter" data-level="63" data-path="63-genomic-knowledges-and-their-datasets.html"><a href="63-genomic-knowledges-and-their-datasets.html"><i class="fa fa-check"></i><b>63</b> Genomic knowledges and their datasets</a></li>
<li class="chapter" data-level="64" data-path="64-the-advent-of-wide-dirty-and-mixed-data.html"><a href="64-the-advent-of-wide-dirty-and-mixed-data.html"><i class="fa fa-check"></i><b>64</b> The advent of ‘wide, dirty and mixed’ data</a></li>
<li class="chapter" data-level="65" data-path="65-cross-validating-machine-learning-in-genomics.html"><a href="65-cross-validating-machine-learning-in-genomics.html"><i class="fa fa-check"></i><b>65</b> Cross-validating machine learning in genomics</a></li>
<li class="chapter" data-level="66" data-path="66-proliferation-of-discoveries.html"><a href="66-proliferation-of-discoveries.html"><i class="fa fa-check"></i><b>66</b> Proliferation of discoveries</a></li>
<li class="chapter" data-level="67" data-path="67-variations-in-the-object-or-in-the-machine-learner.html"><a href="67-variations-in-the-object-or-in-the-machine-learner.html"><i class="fa fa-check"></i><b>67</b> Variations in the object or in the machine learner?</a></li>
<li class="chapter" data-level="68" data-path="68-whole-genome-functions.html"><a href="68-whole-genome-functions.html"><i class="fa fa-check"></i><b>68</b> Whole genome functions</a></li>
<li class="chapter" data-level="69" data-path="69-propagating-subject-positions.html"><a href="69-propagating-subject-positions.html"><i class="fa fa-check"></i><b>69</b> Propagating subject positions}</a></li>
<li class="chapter" data-level="70" data-path="70-propagation-across-human-machine-boundaries.html"><a href="70-propagation-across-human-machine-boundaries.html"><i class="fa fa-check"></i><b>70</b> Propagation across human-machine boundaries</a></li>
<li class="chapter" data-level="71" data-path="71-competitive-positioning.html"><a href="71-competitive-positioning.html"><i class="fa fa-check"></i><b>71</b> Competitive positioning</a></li>
<li class="chapter" data-level="72" data-path="72-a-privileged-machine-and-its-diagrammatic-forms.html"><a href="72-a-privileged-machine-and-its-diagrammatic-forms.html"><i class="fa fa-check"></i><b>72</b> A privileged machine and its diagrammatic forms</a></li>
<li class="chapter" data-level="73" data-path="73-varying-subject-positions-in-code.html"><a href="73-varying-subject-positions-in-code.html"><i class="fa fa-check"></i><b>73</b> Varying subject positions in code</a></li>
<li class="chapter" data-level="74" data-path="74-the-subjects-of-a-hidden-operation.html"><a href="74-the-subjects-of-a-hidden-operation.html"><i class="fa fa-check"></i><b>74</b> The subjects of a hidden operation</a></li>
<li class="chapter" data-level="75" data-path="75-algorithms-that-propagate-errors.html"><a href="75-algorithms-that-propagate-errors.html"><i class="fa fa-check"></i><b>75</b> Algorithms that propagate errors</a></li>
<li class="chapter" data-level="76" data-path="76-competitions-as-examination.html"><a href="76-competitions-as-examination.html"><i class="fa fa-check"></i><b>76</b> Competitions as examination</a></li>
<li class="chapter" data-level="77" data-path="77-superimposing-power-and-knowledge.html"><a href="77-superimposing-power-and-knowledge.html"><i class="fa fa-check"></i><b>77</b> Superimposing power and knowledge</a></li>
<li class="chapter" data-level="78" data-path="78-ranked-subject-positions.html"><a href="78-ranked-subject-positions.html"><i class="fa fa-check"></i><b>78</b> Ranked subject positions</a></li>
<li class="chapter" data-level="79" data-path="79-conclusion-out-of-the-data.html"><a href="79-conclusion-out-of-the-data.html"><i class="fa fa-check"></i><b>79</b> Conclusion: Out of the Data}</a></li>
<li class="chapter" data-level="80" data-path="80-machine-learners.html"><a href="80-machine-learners.html"><i class="fa fa-check"></i><b>80</b> 250,000 machine learners</a></li>
<li class="chapter" data-level="81" data-path="81-a-summary-of-the-argument.html"><a href="81-a-summary-of-the-argument.html"><i class="fa fa-check"></i><b>81</b> A summary of the argument</a></li>
<li class="chapter" data-level="82" data-path="82-in-situ-hybridization.html"><a href="82-in-situ-hybridization.html"><i class="fa fa-check"></i><b>82</b> In-situ hybridization</a></li>
<li class="chapter" data-level="83" data-path="83-critical-operational-practice.html"><a href="83-critical-operational-practice.html"><i class="fa fa-check"></i><b>83</b> Critical operational practice?</a></li>
<li class="chapter" data-level="84" data-path="84-obstacles-to-the-work-of-freeing-machine-learning.html"><a href="84-obstacles-to-the-work-of-freeing-machine-learning.html"><i class="fa fa-check"></i><b>84</b> Obstacles to the work of freeing machine learning</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning: Archaeology of a Data Practice</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="competitive-positioning" class="section level1">
<h1><span class="header-section-number">71</span> Competitive positioning</h1>
<p>How do neural nets come to oscillate between different subject positions? The ranking of keywords in table  suggests that machine learning as an operational knowledge formation attributes a privileged and constitutive function to neural nets. Neural nets concurrently spread into many difference disciplines: cognitive science, computer science, linguistics, adaptive control engineering, psychology, finance, operations research, etc., and particularly statistics and computer science during the 1980-1990s. This dendritic growth did not just popularise machine learning. It brought engineering and statistics together more strongly. Ethem Alpaydin, a computer scientist, writes: </p>
<blockquote>
<p>Perhaps the most important contribution of research on neural networks is this synergy that bridged various disciplines, especially statistics and engineering. It is thanks to this that the field of machine learning is now well established <span class="citation">[@Alpaydin_2010, 274]</span>.</p>
</blockquote>
<p>The forms of this field-making bridging are various.<a href="#fn93" class="footnoteRef" id="fnref93"><sup>93</sup></a> The primary meeting point of different disciplines has perhaps been the machine learning competitions of the 1990s that pitted neural nets against other machine learners such as support vector machine. Many of these competitions focused on vision-related problems such as recognising handwritten numerals. The handwritten digits used in these competitions, particularly the Neural Information Processing System workshops and KDD Cup (Knowledge Discovery and Data Mining) <span class="citation">[@KDD_2013]</span>, all come from the <code>mnist</code> dataset and during the 1990s, much effort focused on crafting neural nets to recognise these 60,000 or so handwritten digits.</p>
<p><em>Elements of Statistical Learning</em> devotes a lengthy section to the analysis of image recognition competitions that began in the early 1990s and continue today. Like Alpaydin, it affirms the coordinating effect of these competitions on the development of machine learning:</p>
<blockquote>
<p>This problem captured the attention of the machine learning and neural network community for many years, and has remained a benchmark problem in the field <span class="citation">[@Hastie_2009, 404]</span>.</p>
</blockquote>
<p>As Hastie and co-authors observe, ‘at this point the digit recognition datasets became test beds for every new learning procedure, and researchers worked hard to drive down the error rates’ <span class="citation">[@Hastie_2009, 408-409]</span>. During the 1990s, zipcodes on envelopes (the set of handwritten digits we have already seen in chapter , the <code>mnist</code> datasets <span class="citation">[@LeCun_2012]</span>) became a primary focus of learning.   The many competitions focused on the <code>mnist</code> dataset are, I suggest, a form of demonstration and testing of machines and people that propagate machine-human differences in machine learning. </p>
<p>Although they brought statistics and computer science closer, neural networks have had a somewhat problematic position in machine learning. Even in relation to the paradigmatic handwritten digit recognition problem, neural nets struggled to gain purchase precisely because a human subject position remained intimately interwoven into their operation. On the one hand, their analogies and figurations as sophisticated neuronal-style models suggested cognitive capacities surpassing the more geometrical, algebraic and statistically grounded machine learners such as linear discriminant analysis, logistic regression, or decision trees. On the other hand, the density and complexity of their architecture made them difficult to train. Neural nets could easily overfit  the data. As <em>Elements of Statistical Learning</em> puts it, it required ‘pioneering efforts to handcraft the neural network to overcome some these deficiencies…, which ultimately led to the state of the art in neural network performance’ <span class="citation">[@Hastie_2009, 404]</span>. It is rare to find the word ‘handcraft’ in machine learning literature. The operational premise of most machine learners is that machine learning works without handcrafting, or that it automates what had previously been programmed by hand.   Somewhat ironically, the competition to automate recognition of handwritten digits, the traces that epitomise movements of hands, entailed much handcrafting and recognition of variations in performances of the machine.</p>
<p>The unstable position of subjects in relation to neural nets are frequently discussed in contrasting terms by machine learners themselves.<a href="#fn94" class="footnoteRef" id="fnref94"><sup>94</sup></a> They often point to a transformation in the work of machine learning:</p>
<blockquote>
<p>Neural networks went out of fashion for a while in the 90s - 2005 because they are hard to train and other techniques like SVMs beat them on some problems. Now people have figured out better methods for training deep neural networks, requiring far fewer problem-specific tweaks. You can use the same pretraining whether you want a neural network to identify whose handwriting it is or if you want to decipher the handwriting, and the same pretraining methods work on very different problems. Neural networks are back in fashion and have been outperforming other methods, and not just in contests <span class="citation">[@Zare_2012]</span>.</p>
</blockquote>
<p>The somewhat vacillating presence of neural nets in the machine learning literature itself finds parallels in the movements of individual machine learners. Yann Le Cun’s work on optical character recognition during 1980-1990s is said to have discovered the back-propagation algorithm at the same time as Rumelhart, Hinton and Williams <span class="citation">[@Rumelhart_1986]</span>. His implementations in <code>LeNet</code> won many research machine learning competitions during the 1990s. In 2007, Andrew Ng could casually observe that neural nets <em>were</em> the best, but in 2014, Le Cun find himself working on machine learning at Facebook <span class="citation">[@Gomes_2014]</span>.  Similarly, the cognitive psychologist Geoffrey Hinton’s involvement in the early 1980s work on connectionist learning procedures in neural nets and subsequently on ‘deep learning nets’ <span class="citation">[@Hinton_2006]</span> delivers him to Google in 2013. </p>
<p>Trajectories between academic research and industry are not unusual for machine learners. Many of the techniques in machine learning have been incorporated into companies later acquired by other larger companies. Even if there is no spin-off company to be acquired, machine learners themselves have been assigned key positions in many industry settings. Corinna Cortes, co-inventor with Vladimir Vapnik of the support vector machine, heads research at Google New York . In 2011, Ng led a neural net-based project at Google that had, among other things, detected cats in millions of hours of Youtube videos.<a href="#fn95" class="footnoteRef" id="fnref95"><sup>95</sup></a> Ng himself in 2014 began work as chief scientist for the Chinese search engine, Baidu leading a team of AI researchers specializing in ‘deep learning,’ the contemporary incarnation of neural nets <span class="citation">[@Hof_2014]</span> .   In recent years, (2012-2015), work on neural nets has again intensified, most prominently in association with social media platforms, but also in the increasingly common speech and face recognition systems found in everyday services and devices. Many of these neural nets are like <code>kittydar</code> , but implemented on a much larger and more distributed scale (for instance, in classifying videos on Youtube). In contemporary machine learning competitions, as we will see, neural nets again surface as intersectional machines, re-distributing differences between humans and machines.</p>
</div>
<div class="footnotes">
<hr />
<ol start="93">
<li id="fn93"><p>We saw some use of neural nets in genomics in the previous chapter (). The initial publication of the <code>SRBCT</code> microarray dataset in <span class="citation">[@Khan_2001]</span> relied on neural nets. <a href="71-competitive-positioning.html#fnref93">↩</a></p></li>
<li id="fn94"><p>Neural nets also receive uneven attention in the machine learning literature. In Andrew Ng’s Stanford CS229 lectures from 2007, they receive somewhat short shrift: around 30 minutes of discussion in Lecture 6, in between Naive Bayes classifiers and several weeks of lectures on support vector machines <span class="citation">[@Ng_2008b]</span>. As he introduces a video of an autonomous vehicle steered by a neural net after a 20 minute training session with a human driver, Ng comments that ‘neural nets were the best for many years.’  The lectures quickly moves on to the successor, support vector machines. In <em>Elements of Statistical Learning</em>, a whole chapter appears on the topic, but prefaced by a discussion of the antecedent statistical method of ‘projection pursuit regression.’ The inception of ‘projection pursuit’ is dated to 1974, and thus precedes the 1980s work on neural nets that was to receive so much attention. In <em>An Introduction to Statistical Learning with Applications in R</em>, a book whose authors include Hastie and Tibshirani, neural nets are not discussed and indeed not mentioned <span class="citation">[@James_2013]</span>. Textbooks written by computer scientists such as Ethem Alpaydin’s <em>Introduction to Machine Learning</em> do usually include at least a chapter on them, sometimes under different titles such as ‘multi-layer perceptrons’ <span class="citation">[@Alpaydin_2010]</span>. Willi Richert and Luis Pedro Coelho’s <em>Building Machine Learning Systems with Python</em> likewise does not mention them <span class="citation">[@Richert_2013]</span>. Cathy O’Neil and Rachel Schutt’s <em>Doing Data Science</em> mentions them but does not discuss them <span class="citation">[@Schutt_2013]</span>, whereas both Brett Lantz’s <em>Machine Learning with R</em> <span class="citation">[@Lantz_2013]</span> and Matthew Kirk’s <em>Thoughtful Machine Learning</em> <span class="citation">[@Kirk_2014]</span> devote chapters to them. In the broader cannon of machine learning texts, the computer scientist Christopher Bishop’s heavily cited books on pattern recognition dwell extensively on neural nets <span class="citation">[@Bishop_1995; @Bishop_2006]</span>. Amongst statisticians, Brian Ripley’s <em>Pattern Recognition and Neural Networks</em> <span class="citation">[@Ripley_1996]</span>, also highly cited, placed a great deal of emphasis on them. But these textbooks stand out against a pointillistic background of hundreds of thousands of scientific publications mentioning or making use of neural nets since the late 1980s in the usual litany of fields – atmospheric sciences, biosensors, botany, power systems, water resource management, internal medicine, etc. This swollen publication tide attests to some kind of formation or configuration of knowledge invested in these particular techniques, perhaps more so than other I have discussed so far ( logistic regression, support vector machine, decision trees, random forests, linear discriminant analysis, etc.).<a href="71-competitive-positioning.html#fnref94">↩</a></p></li>
<li id="fn95"><p>Unlike the cats detected by <code>kittydar,</code> the software discussed in the introduction to this book, the Google experiment did not use supervised learning. The deep learning approach was unsupervised <span class="citation">[@Markoff_2012]</span>. That is, the neural nets were not trained using labelled images of cats.<a href="71-competitive-positioning.html#fnref95">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="70-propagation-across-human-machine-boundaries.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="72-a-privileged-machine-and-its-diagrammatic-forms.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
