

## Introduction to platform pragmatism

Pragmatism used here in the sense that recent pragmatism has come to use it: not just what works, but what how a general account of experience can be derived from the irreducibility of practice to the forms/ideas/concepts that usually organise it. 
Platform pragmatism: points to the kinds of experience that relate to platforms as lifted-out places on which things work: living in data; included and perhaps belonging in data;
Platform here has a relation to plane: not all platforms are planar, but planarity is a significant feature of the platforms I address
Not just a theoretical pragmatism, but a pragmatism that comes from actuality taken against itself: how to counter-effectuate in practice;
How to modify the practices of thinking so that data can be thought; 
Platforms used here to refer to two planes of reference — the recording surfaces; the sampling surfaces, which themselves are involved in construction of functions meant to actualize variations on the recording surface;
Implications for human and social sciences




## Belief in data and the invention of analytics: 1660

Supplies of random numbers, shaped by functions for almost the first time; But this problem continues today … 
Constitution of data in relation to notions of evidence, probability, error, prediction requires supplies of randomness; 
Plying numbers vs rolling numbers;
Could introduction functions here  — fits with differentials and Leibniz

Dataset: 

## Curve of curves: 1828

Role of visual forms here — density-shapes lashing out into the visual; 
Follows on from functions in D&G
John Tukey — exploratory data analysis
NYT Graphics team;
‘Data is beautiful’ vs ‘finding the signal in the data’
Tarde’s stuff on imitation useful here
Logistic curves as key example here: both the role of curves, the role of linear models;
Link between lines and curves described in terms of functions
Tension between graphics and models continues (Fisher vs Tukey?)
Matrices and hypervolumes
Dataset: iris

## 1899 — 1968: Aggregate data: more parts than elements

Has all the stuff on relationality, sets, etc; 

Expands to include different scales apart from the meso-level databases: from spreadsheets to data centres; 
The excess parts over elements as another way in which full knowledge is inhibited constantly;

Plane of reference includes enterprises, states, etc; anywhere where information retrieval counts

Dataset: twitter, mongodb. couchdb

References: Manning & Co.

## 1971: Clustering and the curse of dimensionality

Machine learning chapter
Many different ways to find the signal; as dimensions increase, more likely that points will lie near the boundaries of any sample. Also the many problems of bias and variance as the dimensions grow.
If plane of reference is a hyperplane, then many such problems will arise
This chapter goes through k-nearest neighbour, k-means, hierarchical clustering, decision trees, random forests, neural networks, etc
Pattern recognition here and here it has ramified

Dataset:

## Abyss of methods

What happens as methods become mobile: how are they recombined? 
This is a place where plane of reference is being folded onto itself
Machine learning vs data geeks, etc;
How to do deal with proliferation of methods in recombination? 

Dataset: iris — trace iris across different settings

## Elusive variation

Variation becomes the norm; but this variation is well structured?
Genomes + gwas
The problem of well-structured data — gives an explanation of certain biological forms attract so much attention. What happens to the messy ones?
So, justification for talking about this is to try and capture why certain kinds of data matter more than others. 
Dataset: 


## Epidemiology and its problems with nubers: what cannot be observed vs what can be observed

Returns to population, but now with the idea of many populations interpenetrating
Distributions of distributions
Against the idea of full knowledge, etc

Datasets: Birrell, 2011

## Conclusion
The overall argument


