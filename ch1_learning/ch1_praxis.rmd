# Chapter 1
# Writing about data

> Machine learning is not magic; it cannot get something from nothing. What it does is get more from less. Programming, like all engineering, is a lot of work: we have to build everything from scratch. Learning is more like farming, which lets nature do most of the work. Farmers combine seeds with nutrients to grow crops. Learners combine knowledge with data to grow programs. [@domingos_few_2012, 81] 

Many different tendencies shape what is happening to data today, but two of them impinge directly on  our  practical sense of the potentials of data. On the one hand, ways of working with  data have shifted substantially over the last decade or so due to the growth in open source programming languages and networked platforms. Scientists, software developers and data analysts use programming languages such as `Python` and `R` rather than commercial software packages such as Matlab, SAS or SPSS to work with data. Scientific research, which has long relied on counting and calculation, increasingly organises and processes data more comprehensively because workflows, datasets, algorithms and databases can be quickly and flexibily assembled in code. (Scientific computing languages such as FORTRAN have long underpinned scientific research in various more mathematical fields.) On the other hand, several decades of research and application of statistical and machine learning algorithms in science, government, industry and commerce have gradually developed fairly powerful ways of automating the construction of models that classify and predict events and associations between things, people, processes, etc. Programming and software development has long handled data and algorithms, and conversely, machine learning has always relied on programming. But they are powerfully coalescing in data-driven research, business and controls systems in various ways as predictive models, targeted advertising, recommendation systems, . Can we locate anything in the proliferation of machine learning that is more than plying data into a tightly woven matrix of commercial-scientific-governmental knowing/surveillance? I don't answer that question directly here, but my basic answer is affirmative. I think we can and we should  explore some of the shifts in practices of working with data that can be observed around data. These practices are both the object of analysis, and in  certain cases, provide support for a shift in the way in which we might carry out research on changes in media, publics, sciences, commerce, mobilities and  health. 

## Reading machine learning

The way that I approach data was initially influenced  by a single highly technical and compendious textbook, _Elements of Statistical Learning: Data Mining, Inference, and Prediction_ [@hastie_elements_2009], now in its third edition. Like the online machine learning courses and programming books I discuss below, _Elements of Statistical Learning_  combines statistical techniques with various algorithms to 'learn from data' [@hastie_elements_2009, 1]. The 763 pages of this often tersely written and quite mathematical book range across various kinds of data (spam email, Californian forest fires, blood pressure measurements, handwritten digit recognition), and various machine learning techniques, methods and algorithms (linear regression, k-nearest neighbours, neural networks, support vector machines, the Google Page Rank algorithm, etc). While these techniques come from many different places, in _Elements of Statistical Learning_, they are all framed by a notion of learning based on predictive models. The predictive models in _Elements of Statistical Learning_ are  dominated by a single prediction technique, linear regression models or fitting a line to points. in the course of the book, linear regression is subjected to countless variations, iterations, expansions and modifications. The authors of the book, Jeff Hastie, Rob Tibshirani and Jerome Friedman are statisticians working at Stanford and Columbia University. (Statisticians and computer scientists from Stanford University loom large in the world of machine learning, perhaps due to their promixity to Silicon Valley.)  Their own research spans several decades and a range of topics. The book collates and borrows from a panoply of scientific publications  and datasets in fields of statistics, artificial intelligence and computer science. It is not the only book of its kind. I could just have well cleaved to Alpaydin's _Introduction to Machine Learning_ [@alpaydin_introduction_2010]  (a more computer science-base account), to Christopher Bishop's  highly mathematical _Pattern recognition and machine learning_ [@bishop_pattern_2006], Brian Ripley's _Pattern Recognition and Neural Networks_ [@ripley_pattern_1996] Tom Mitchell's  earlier artificial intelligence-centred _Machine learning_ [@mitchell_machine_1997] or Peter Flach's  recent _Machine Learning: The Art and Science of Algorithms that Make Sense of Data_ [@flach_machine_2012], or further afield, the sobering _Statistical Learning for Biomedical Data_ [@malley_statistical_2011].  These and quite a few other recent machine learning textbooks display a range of emphases, ranging from the highly theoretical to the very practical, from an orientation to statistical inference to an emphasis on computational processes, from  science to commercial applications. 

While 'learning' is briefly discussed on the first page of _Elements of Statistical Learning_, the book hardly ever returns to the topic explicitly. We can read on page two that a 'learner' in machine learning is a model that predicts outcomes.  (As I discuss in the next chapter, learning is comprehensively understood in machine learning as finding a function that could have generated the data, and optimising the search for that function as much as possible.) The notion of learning in machine learning derives from the field of artificial intelligence. The broad project of artificial intelligence, at least as envisaged in its 1960s-1970s heydey, is today largely regarded as a failure. [TBA -- add Mitchell, Ripley and perhaps Bishop here] There is no general, or comprehensive artificial intelligence in existence, even though many efforts continue to develop 'deep learners' (see for instance, Google Corporation's ongoing research into 'deep learning' of its own data). But in the course of its failure, many interesting problems were generated. [TBA - references on the history of AI] The field of machine learning might be seen as one such offshoot since it treats machine learning as an algorithm process to be supervised, monitored, optimised and otherwise enhanced by any means available (including in some machine learning competitions, resort to forms of prestidigitation that amount to pulling the hat out of the data rabbit). The so-called 'learning problem' and the theory of learning machines was developed largely by researchers in the 1960-1970s, based on work already done in the 1950s on learning machines such as the perceptron, the  neural network model developed by the psychologist Frank Rosenblatt in the 1950s [@rosenblatt_perceptron_1958]. Drawing on McCulloch-Pitts model of the neurone, Rosenblatt implemented the perceptron, which today would be called a single-layer neural network on a computer at the Cornell University Aeronautical Laboratory in 1957. As Vladimir Vapnik, a leading machine learning theorist, observes: 'the perceptron was constructed to solve pattern recognition problems; in the simplest case this is the problem of constructing a rule for separating data of two different categories using given examples' [@vapnik_nature_1999, 2]. While computer scientists in artificial intelligence of the time, such as Marvin Minsky and Seymour Papert, were sceptical about the capacity of the perceptron model to distinguish  or 'learn' different patterns [@minsky_perceptron:_1969], later work showed that perceptrons could 'learn universally.' For present purposes, the key point is not that neural networks have turned out several decades later to be extremely powerful algorithms in learning to distinguish patterns, and that intense research in neural networks has led to their ongoing development and increasing sophistication in many 'real world' applications (see for instance, for their use in sciences [@hinton_reducing_2006], or in commercial applications such as drug prediction [@dahl_deep_2012]). Rather, the important point is that it began to introduce learning machines as an ongoing project in which trying to understand what machines can learn, and to predict how they will classify or predict became central concerns precisely because machines didn't seem to classify or predict things at all well. At the same time, and less visibly,  implementations and applications of techniques derived from artificial intelligence and adjacent scientific disciplines became more statistically sophisticated. That is, the theory of machine learning alongside decision theory was interwoven with a set of concepts, techniques and language drawn from statistics. Just as humans, crops, habitats, particles and economies had been previously, learning machines became entwined with statistical methods. Not only in their reliance on the linear model as a common starting point, but in theories of machine learning, statistical terms such as bias, error, likelihood and indeed an increasingly thorough-going probabilistic framing of learning machines emerged. 

In describing the entwined elements of machine learning techniques, and citing various machine learning textbooks, I'm not attempting to provide any detailed history of their development. For the most part, I leave controversies about the techniques to one side. Also, in describing these developments, I focus in the main on what happens from the 1980s onwards. My knowledge of these developments is not properly historical. It does not come out of the archives of institutions, laboratories or companies where these techniques took shape. It derives much more either from following citations back out of the highly distilled textbooks into the teeming collective labour of research on machine learning as published in hundreds of thousands of articles in science and engineering journals, or from looking at,  experimenting with and implementing techniques in code. And it means going downstream from the textbooks into actual implementations and places where people, algorithms, and machines mingle more than they do in neat formality of the textbooks. Rather than history or controversies in the field, I focus on the migratory patterns of  methods, and the many configurational shifts associated with their implementations as the same things appears in different places. 

While many of the machine learning techniques I discuss have much longer lineages (in some cases, runnning back to the 1930s), machine learning techniques begin to circulate much more widely in the 1980s as a result of personal computers, and then in the mid-1990s, the internet. The proliferation of programming languages such as FORTRAN, C, C++, Pascal, then Perl, Java, Python and `R`, and computational scripting environments such as Matlab, multiplied the paths along which  implementation of machine learning techniques could proceed. It would be impossible for anyone to read the vast machine learning research literature, or follow the the propagation of techniques across domains of science, business and goverment. For instance, a  perceptron that 'learns' the binary logical operation NAND (Not-AND) is  expressed in twenty lines of python code on the Wikipedia 'Perceptron' page [@perceptron_2013]. 

```{r perceptron, echo=TRUE, cache=TRUE, message=FALSE, warning=FALSE, comment=NA, size='smallsize', results='markup', engine='python' }
    
    threshold = 0.5
    learning_rate = 0.1
    weights = [0, 0, 0]
    training_set = [((1, 0, 0), 1), ((1, 0, 1), 1), ((1, 1, 0), 1), ((1, 1, 1), 0)]
     
    def dot_product(values):
        return sum(value * weight for value, weight in zip(values, weights))
     
    while True:
        print '-' * 60
        error_count = 0
        for input_vector, desired_output in training_set:
            print weights
            result = dot_product(input_vector) > threshold
            error = desired_output - result
            if error != 0:
                error_count += 1
                for index, value in enumerate(input_vector):
                    weights[index] += learning_rate * error * value
        if error_count == 0:
            break
```

What does this code show or say? First of all, we should note the relative novelty of code vignettes in Wikipedia pages. In citing this code, I'm not resorting to a technical publication or scientific literature as such, just to that popular yet incredibly heavily used epistemic resource, the Wikipedia page. Second, while perceptrons and neural networks are the topic of a later chapter, the typical features of this code as the implementation of a machine learning algorithm are the presence of the 'learning rate', a 'training_set,' 'weights', an 'error count', a loop function that  multiplies values ('dot_product'). Some of the terms present in the code bear the marks of the theory of learning machines that we will discuss. But much of the code here is much more familiar, generic programming. It defines variables, sets up data structures (lists of numerical values), checks conditions, loops through statements or prints results. Executing this code (by copying and pasting it into a python terminal, for instance) produces several dozen lines of numbers that are initially different to each other, but that gradually converge on the same values (see printout). These numbers are the 'weights' of the nodes of the perceptron as it iteratively learns to recognise patterns in the input values. None of the workings of the perceptron need concern us at the moment. It is a typical machine learning algorithm, almost always included in ML textbooks and usually taught in introductory ML classes.  Perhaps more strikingly than its persistence as an algorithm over half a century, the implementation of the whole algorithm in twenty lines of code on a Wikipedia page suggests the mobility of these methods. What required the resources of a major research engineering laboratory in the 1950s can be re-implemented by a cut and paste operation between wikipedia pages and a terminal window on a laptop in 2013. This is now a familiar observation, and perhaps not very striking at all. But the very visibility of this code raises some question about  who today implements perceptrons or neural networks, and where they implement them. 

## A machine learning praxiography? 

What would we learn by studying the proliferation of implementations of artificial intelligence or machine learning algorithms rather than their history or the controversies associated with them? I'm interested in how we are going to live with machine learning.  This is a largely affirmative question, albeit with a critical side to it. That is, I am looking for ways of thinking with machine learning, and this means learning to machine learn in some measure. Several general  directions of travel present themselves here. 

Science studies scholars such as Anne-Marie Mol and John Law have long urged the need to keep practice together with theories of what exists. Towards the beginning of _The Body Multiple: Ontology in Medical Practice_[@mol_body_2003], Mol writes:

> If it is not removed from the practices that sustain it, reality is multiple. This may be read as a description that beautifully fits the facts. But attending to the multiplicity of reality is also an act. It is something that may be done – or left undone [@mol_body_2003, 6]

Mol's work offers a cogent case for developing accounts of what is real steeped in the practices that make it real. While similar sounding affirmations of the underpinning role of practice can be found in many parts of social sciences and humanities, Mols insistence on this nexus of practice or doing and the existence of things in their plurality offers scope for engagement. This seemingly tautologous and even somewhat recursive project of bringing description and practice together is actually full of potential.   So for the first part, looking at techniques -- a crystallised form of practice -- and the flow of their implementations is a way of keeping practices in the picture. It is therefore, a heuristic for learning multiples. This already suggests that describing machine learning in terms of practices could be an act that attends to their multiplicity. Mol's coins the term  *praxiography*, a variant on ethnography, to refer to an act of describing practice in the name of preserving their multiple-making value. This term has particular resonance for work with data, which is itself always heavily entwined in writing and reading practices. Could we praxiographically go into data and machine learning?

A second  and related emphasis comes from the work of the early twentieth century Alfred North Whitehead. Despite his fame as co-author with Bertrand Russell of _Principia Mathematica_, a tome whose Latinate title attests to its weighty ambitions,  Whitehead's later work in books such as _The Concept of Nature_ [@TBA], _Science and the Modern World_ [@whitehead_science_1970] and _Modes of Thought_ [@whitehead_modes_1958] on how scientific and mathematical abstractions are embodied is highly relevant to thinking about machine learning [^5]. Whitehead is comprehensively critical of certain tendencies in modern science (the pervasive 'fallacy of misplaced concreteness' that treats abstractions as pre-given foundations; the reduction of space-time to discrete  exterior locations, etc), but he is very enthusiastic about the potential for better abstractions. While his affirmative account of enhanced abstractions is voiced in terminology that takes some getting used to, the broad point can be grasped. Here he writes about better abstractions as energy enhancing:

> But this enhancement of energy presupposes that the abstraction is preserved with its adequate relevance to the concrete sense of value-attainment from which it is derived. In this way, the effect of the abstraction stimulates the vividness and depth of the whole of experience. [@whitehead_modes_1958,169]

The enhancement of energy Whitehead talks about here is concerned with the 'fortunate use of abstractions' (169). Machine learning and  much work with data today can be seen as a form of abstraction (as can much of science). The crucial question for Whitehead, however, is how to abstract well. Like Mol, he advocates keeping abstractions together ('preserved with its adequate relevance') with something like practice (' to the concrete sense of value-attainment from which it is derived'). Like Mol too, the virtue of this bundling together of abstractions with their concrete sense affects the 'whole of the experience.' Abstractions can make experience more vivid or deep if they are aligned and assembled connectively. Here I pin some methodological hope on Whitehead's work as a way to experiment with abstraction as stimulant and enrichment, not as reduction or pale imitation. 

A final and again different kind of approach here comes from the Elizabeth Wilson's study, _Affect and Artificial Intelligence_ [@wilson_affect_2010]. Drawing on a combination of psychoanalytic, psychological and archival materials, Wilson discusses the work of key figures in the early history of artificial intelligence such as Alan Turing on intelligent machinery,  Warren McCulloch and Walter Pitts on neural nets, and recent examples of affective computing and robots such as the MIT robot Kismet. Her framing of the psychic nexus with machines is provocative:
    
> Sometimes machines are the very means by which we can stay alive psychically, and they can  just as readily be a means for affective expansion and amplification as for affective attenuation. This is especially the case of computational machines [@wilson_affect_2010, 30].

Under what conditions do machines and for present purposes, computational machines, become 'the very means we can stay alive psychically'? Wilson  addresses this question by positing 'some kind of intrinsic affinity, some kind of intuitive alliance between the machinic and the affective, between calculation and feeling' (31), and suggesting that the 'one of the most important challenges will be to operationalize affectivity in ways that facilitate pathways of introjection between humans and machines' (31). Introjection, the process of bringing the world within self is, according to psychoanalytic accounts of subjectivity, crucial to the formation of 'a stable subject position' (25). Wilson envisages introjection of machine processes as a good, not as a failure or attenuation of relation to the world. 

These three takes on how to think about practices, abstraction and machines work on somewhat different levels, but they broadly share an interest or even a commitment to multiple, plural or surprising conjunctions between what people do, what the world is, and the many adjustments, alignments and slippages that connect them through models, abstractions, computation and data. I regard them as  offering guidance or methodological maxims on how to write with data, with models and with machine learning somewhat differently. The question is how would one actually describe machine learning and its allied practices in ways that enhance their multiple reality (not realities!), that augment or deepen abstractions in ways that allow them to resonate.

## Mobility of statistical methods in the 1990s: the practice of `R`

Nearly all of the examples in _The Elements of Statistical Learning_ are implemented in  a single programming language, `R`. `R` is a well-known and widely used statistical programming language and environment  [@r_development_core_team_r_2010]. An open source programming language, according to surveys of business and scientific users, at the time of writing, `R` has replaced popular statistical software packages such as SPSS, SAS and Stata as the statistical and data analysis tool of choice for many people in business, government, and sciences ranging from political science to genomics, from quantitative finance to climatology [@rexer_analytics_rexer_2010]. Developed in New Zealand in the mid-1990s, and like many open source software projects, emulating `S`, a commercialised predecessor developed at AT&T Bell Labs during the 1980s, `R` is now extremely widely used across life and physical sciences, as well as quantitative social sciences. John Chambers, the designer of `S`, was awarded the Association for Computing Machinery (ACM) 'Software System Award' in 1998 for ' The S system, which has forever altered how people analyze, visualize, and manipulate data' [@acm_john_2013]. Many undergraduate and graduate students ltoday earn `R` as a basic tool for statistics. Skills in `R` are often seen as essential pre-requisite for scientific researchers, especially in the life sciences. (In engineering, `Matlab` is widely used.) Estimates of its number of users range between 250000 and 2 million. Increasingly, `R` is integrated into commercial services and products (for instance, SAS, a widely used business data analysis system now has an `R` interface; Norman Nie, one of the original developers of the SPSS package heavily used in social sciences, now leads a business, Revolution, devoted to commercialising `R`; `R` is heavily used at Google, at FaceBook, and by quantitative traders in hedge funds; in 2013 ['R usage is sky-rocketing'](http://www.r-bloggers.com/r-usage-skyrocketing-rexer-poll/); etc.).  `R` is an interestingly diffuse entity. Some ways of working with data are way more clearly focused on equations and calculation. For instance, in order to pursue number practices in physical sciences and engineering, maybe MATLAB or Mathematica would be better. In business or government, many ways of working with data are more focused on ordering and searching. For instance, in order to the look at the organisation of large aggregates of data, relational databases, query languages, data-centre architectures, and perhaps the techniques of aggregating and disaggregating data en-masse would be worth studying. 

Why then choose `R`, a statistical programming language, a language developed largely by academic statisticians and research scientists rather than computer scientists, software engineers or hackers? One pragmatic justification is that much of the recent development, demonstration, implementation and adoption of machine learning can be tracked in  `R` code. The `R` packages such as 'ElemStatLearn' [@halvorsen_elemstatlearn:_2012], for instance, encapsulates some of the main datasets and methods discussed in _The Elements of Statistical Learning_.  Research articles and texbooks in statistics commonly both use `R` to demonstrate methods and techniques, and create `R` packages to distribute the techniques and sample data. For instance, an article published in 2007 by the Princeton computer scientist David Blei on 'correlated topic models' for classification of documents [@blei_correlated_2007] leads to a package 'topicmodels' available from the 'Comprehensive `R` Archive Network (CRAN)' a few years later [@cran_comprehensive_2010]. CRAN itself is an important infrastructure in the global life of `R`. 

```{r mirrors, echo=TRUE, cache=TRUE, message=FALSE, warning=FALSE, comment=NA, size='smallsize', results='markup' } 
    
    library(utils)
    mirrors = getCRANmirrors(all = FALSE, local.only = FALSE)
    head(mirrors)

```
A set of `r nrow(mirrors)` mirror sites in `r length(unique(mirrors$Country))` countries distributes the latest `R` platform and `R` packages to many different users. These mirror sites are largely run by universities and public institutions, alongside a few commercial users (such as Revolution, a company I discuss in a later chapter). So, in terms of machine learning practice, `R` is important because it is a staple programming language in statistics research and teaching. It circulates globally. At the same time, it is stunningly specialised in manifold ways. Karl Marx wrote of the 500 different hammers made in Birmingham, 'not only is each adapted to one particular process, but several varieties often serve exclusively for the different operations in one and the same process' [@marx_capital_2007, 375]. Something similar holds in `R`: thousands of software packages in CRAN suggest that a highly specialised division of labour and possibly refined co-operative labour processes operate around data. Not only does `R` change the way people work with data, it offers the chance to see how people work with data. The processes of production of data, the building of models, the modelling of states of affairs, and many specialisations and adaptations of data techniques are written in  `R` and can be read there too.

Less pragmatically, but symptomatically, `R` has also attracted  mainstream media attention.An article in _The New York Times_ in 2009 highlighted its practical importance in data analysis [@vance_data_2009], at the time  _The New York Times_ was heavily promoting its idea of data journalism.   This is somewhat unusual, as programming languages are not normally the topic of mainstream media interest.  More broadly, `R` is an evocative object, to use the psychoanalyst Christopher Bollas' term [@bollas_evocative_2009], an object through which many different ways of thinking circulate. Standing somewhere at the intersection of statistics and computing, modelling and programming, many different disciplines, techniques, domains and actors intersect in `R`. Bollas suggests that 'our encounter, engagement with, and sometimes our employment of, actual things is a _way_ of thinking' [@bollas_evocative_2009, 92].  `R` embodies something of plurality of practices of working with measurements, numbers, text, images, models and equations, with techniques for sampling and sorting, with probability distributions and random numbers. It engages immediately, practically and widely with  words, numbers, images, symbols, signals, sensors, forms, instruments and above all abstract forms such as mathematical functions like probability distributions and many different architectural forms (vectors, matrices, arrays, etc), as it employs data. By virtue of thousands of packages that flow across boundaries between nature and culture, between aesthetic, epistemic and pragmatic domain, `R` embodies a wide-with data economies, cultures, sciences, politics and technologies. Somewhere between calculation and searching, `R` channels counting and sorting, in the estimation of likelihoods.

 It is useful, in this context, to think of things like `R` somewhat recursively. Many practices, many ways of doing things, describe themselves as part of their doing. They implicitly also think about themselves. Every practice contains elements of its own description. Descriptions arise in the process of co-operating in doing some. We tell each other what we have done so that we know what to do next. This is not to say that `R` as a collectively produced body of code is thinking (perhaps it is, at a highly collective level). It certainly does abound with descriptions of itself. It not only documents itself in the sense that a packet documents its contents with a label, but in the stronger sense that many of the basic operations and practices involved in working with R have descriptive components.   How would we describe the `R`-based practices in ways that deepen our perception of their plurality, their relevance, and potential amplification of affect? Note that Bollas distinguishes encounter, engagement and employment of things. Employment, in his terms, is less likely to lead become a way of thinking than encounter and engagement. On balance, I'd say  that the 'employment' dimension of `R` dominates.  It is easy today to find an employment-centric view of data practice. The 'chief economist' at Google, Hal Varian in 2009 made a widely quoted prediction about working with data:

> The ability to take data — to be able to understand it, to process it, to extract value from it, to visualize it, to communicate it — that’s going to be a hugely important skill in the next decades, not only at the professional level but even at the educational level for elementary school kids, for high school kids, for college kids. Because now we really do have essentially free and ubiquitous data. So the complimentary [sic] scarce factor is the ability to understand that data and extract value from it.” Hal Varian, chief economist at Google: [@mckinsey_&_company_hal_2009]

While cited here in a report prepared by the global business consultancy, McKinsey & Co, this quote, or selections from it, can be seen in many different contexts, ranging from government reports on higher education to student noticeboards in  university statistics departments.  What Varian, with all the allure of  Google as a potential employer, presents  as an 'important skill'  also has a 'scarce factor':  the 'ability to understand that data.' While Varian presents understanding data as the prelude to 'extract[ing] value from it,' understanding might take different forms, depending on the kind of encounter or engagement that precedes it. 

Encounters with `R` are by no means limited to employment. A somewhat broader framing of the value of `R` can be found voiced as a form of knowing or understanding. Some people – admittedly a small group – explicitly promote `R` in association with the wider growth of data democracy or open data. I say somewhat, because the promotion of `R` as a valuable software object is linked to individual entrepreneurship. Norman Nie, the original developer of the widely-used SPSS social science statistics software (see [uprichard_spss_2008]), is a proponent of `R`. Here is Norman Nie interviewed in _Forbes_, a leading business news magazine, evangelising for `R`:

> Everyone can, with open-source `R`, afford to know exactly the value of their house, their automobile, their spouse and their children--negative and positive  … It's a great power equalizer, a Magna Carta for the devolution of analytic rights [@hardy_power_2010].

While still  referring to `R`  in ways that lie close to employment, Nie, the founder of Revolution Analytics, a company that provides software and support services to `R` users, promotes `R` as a way of protect individual liberties to know what they own. In the context of the global financial crisis of 2007, it may be that the value of houses became much more problematic, and understandably, harder to know. And the reference to 'spouse and their children' must be flippant. Yet the 'devolution of analytic rights' that Nie speaks of here, even as he frames it in terms of entrepreneurial individualism, is important. While Nie's company  Revolution Analytics promotes the incorporation of  `R` into  powerful and pervasive business and government prediction systems (such as SAS and IBM's Pure Data [@ibm_ibm_2013]), Nie here points to the problem of 'manipulation and control' of customers associated with just that incorporation. The 'analytic rights' he advocates in the form of `R` are meant to help individuals counterbalance the power of large scale data analysis conducted by corporations and governments.  More than any particular or specific use, Nie's advocacy of `R` taps into a wider  sense of rich possibility associated with `R`. It would be possible to cite many other instances of this belief and desire in the potential of `R`. A good indication of this overflow is the aggregate blog, [R-bloggers](R-bloggers.com). `R`-bloggers brings together several hundred `R`-related blogs in one place. The range of topics discussed there on any one day – Merrill-Lynch Bond Return indices, how to run `R` on an iPhone, using US Department of Agriculture commodity prices, analysing human genetic variation using PLINK/SEQ via `R`, using the 'Rhipe' (sic!) package to program big data applications on Map-Reduce cloud architectures, doing meta-analysis of phylogenetic trees, etc., etc (R-bloggers 2011) – suggest how widely desires/beliefs in `R` range. To take just one fairly recent example, '`R` Analysis Shows How UK Health System could save £200 million' claims a recent post on the site [@smith_r_2012]. While many of the applications, experiments, or techniques reported there can be reduced to employment or to individual uses of `R`, they also, as we will see, suggest the potential for encounters and engagements. Something similar, and perhaps less bound to commercial data analytics can be found in Rachel Schutt and Cathy O'Neil's much more refreshing _Doing Data Science_ [@schutt_doing_2013] as it discusses the ethics and potentials of working with machine learning models and programming languages such as `R` and Python. Finally, and not least of all, the media theorist Ian Bogost advocates  code  as a way to make sense of the world: 'source code itself often offers inroads in alien phenomenology -- particular when carpentered to reveal the internal experience of withdrawn units' [@bogost_alien_2012, 105].

If we were to try to describe these encounters and engagements in which `R` as an actual thing is also a way of thinking, how would we go about it? The act of describing the practices of working with `R` might, at least this is my hope, be a way of staging an encounter with those aspects of data practice that exceed employment, and that instead do something like vivify experience by making it a bit less predictable and perhaps more 'alien' in the sense Bogost values. But one problem is that there are so many different ways to do things with data. In many cases, data practice begins not from data, but from books, articles and websites. This a fairly banal observation, but acquiring, generating, finding or, to use the term often used by database architects, 'discovering' data is not all that common in practice. It is perhaps far more typical to encounter data in the context of learning how to manipulate, sort, model or display it. These settings are multiple. We could turn to R-blogger's aggregation or the programmer Q&A site [Stackoverflow](http://www.stackoverflow.com), which has many questions tagged with `R`. There are also many online manuals, guides and tutorials relating to `R` [@wikibooks_r_2013]. For present purposes, I draw mainly on semi-popular books such as _R in a Nutshell_ [@adler_r_2010], _The Art of `R` Programming_ [@matloff_art_2011], or _R Cookbook_ [@teetor_r_2011]. These books are not written for academic audiences, although academics often write them and use them in their work. They are largely made up of illustrations and examples of how to do different things with different types of data, and their examples are typically oriented towards business or commercial settings, where, presumably, the bulk of the readers work, or aspire to work. Given a certain predisposition (that is, geekiness), these books and other learning materials can make for enjoyable reading. While they vary highly in quality, it is sometimes pleasing to see  the economical way in which they solve common data problems. This genre of writing about programming specialises in posing a problem and solving it directly. In these settings, learning takes place largely through following or emulating what someone else has done with either a well known dataset (Fisher's 'iris') or a toy dataset generated for demonstration purposes.  The many frictions, blockages and compromises that often affect data practice are largely occluded here in the interests of demonstrating the neat application of specific techniques. Yet are not those frictions, neat compromises and strains around data and machine learning precisely what we need to cleave to praxiographically? In order to demonstrate both the costs and benefits of approaching `R` through such materials, rather than through ethnographic observation of people using `R`, it will be necessary to stage encounters here with data that has not been completely transformed or cleaned in the interests of neatly demonstrating the power of a technique. One way to do this is by writing about code while coding. 

## Learning to predict online

> The goal of the S language ... is "to turn ideas into software, quickly and faithfully" ... it is the duty of the responsible data analysts to engage in this process ... the exercise of drafting an algorithm to the level of precision that programming requires can in itself clarify ideas and promote rigorous intellectual scrutiny. ... Turning ideas into software in this way need not be an unpleasant duty. [@venables_s_2000, 2]

Bill Venables and Brian Ripley, statisticians working on developing `R`,  wrote in the late 1990s of  the responsibility of data analysts to write not just use software in the interests of intellectual rigour. They write 'software' here not in the sense of a product, but in the sense that today would more likely be called 'code.' This sense of coding and programming as clarifying and concretising ideas with precision has thoroughly taken hold in contemporary data analysis.  Programming and writing code for data analysis has widely supplanted the use of statistical software applications, at least in statistical research. Writing code has always been  central in machine learning where algorithms are a  key focus. Let us stick with Nie's example of house prices in order to concretise what descriptive practices (of the kind that admit reality as multiple, abstractions as concretely relevant, and computation as affectively expansive not restrictive) might look like in `R` work with data. House price data is possibly a worst case example, since house prices seem so heavily webbed into employment, salaries, class position, and business as usual. Is it possible to describe  predictive modelling of house prices in a way that responds to what Mol, Whitehead and Wilson propose? Could house price data engage us in something more than working out what to buy or when to sell? House price datasets appear quite often in machine learning research and tutorials. (The most well-known is the Boston House Price dataset dating from the 1970s [Boston Housing Data](http://archive.ics.uci.edu/ml/datasets/Housing)) The sample shown below comes from San Francisco, prior to the global financial crisis 2007 and its associated sub-prime mortgages. As discussed  below, the dataset comes from an online machine learning course. The dataset is fairly small and simple and can be loaded, summarised and plotted using a few lines of `R` code. 

```{r house_price, echo=TRUE, cache=TRUE, message=FALSE, warning=FALSE, comment=NA, size='smallsize', results='markup' }

    house_prices = read.csv('data/ex1data2.txt', header=FALSE)
    names(house_prices) = c('size', 'bedrooms', 'price')
    head(house_prices)
    summary(house_prices)
    attach(house_prices)
    plot(size, price)
```

Two architectures come together here. On the one hand, around fifty houses  in a city, San Francisco, are described in terms of the number of bedrooms, their total floor area and the price they sold for. This is a very sparse description of domestic urban life, and it reduces the encounter with the psychically rich form of houses/apartments to a cognitive minimum. (The Boston House Price dataset, by comparison, has many more variables and offers a somewhat richer picture.) On the other hand, the description of this encounter has its own architectural forms -- the  pervasive form of the table, grid or, crucially for machine learning,  _matrix_ and the plot that displays values of variables in a geometric sapce. (Whatever else happens to the data in machine learning,  matrix operations and graphic plots remain ever-present. As we will see below, in modelling this data, matrix manipulations will be essential.) So Nie's recommendation of `R` as a way of learning house prices is not unusual. His mention of house price prediction as something that might engage us has many precedents. House price prediction is a surprisingly common teaching or demonstration example in machine learning, alongside other iconic datasets including R.A. Fisher's 'iris', the MNIST handwritten digits dataset [@lecun_mnist_2012]  (derived from U.S. National Institute of Standards and Technology (NIST)), or the cat photo dataset discussed in the previous chapter. 

But what happens to such readily available datasets in  learning to machine learn?  The house price dataset shown above was actually generated  for teaching Professor Andrew Ng's course 'Machine Learning' CS229 at Stanford (http://cs229.stanford.edu/). (Given CS229  was running before 2007, it is likely that this dataset does not reflect the financial crisis that began in 2007 and led to drastic reductions in house prices from 2008 onwards.) It is also heavily used in a shortened version of this course delivered under the title 'Machine Learning' on  [Coursera.org](https://class.coursera.org/ml-003/class/index), a  MOOC (Massive Open Online Course) platform. These two  courses, and especially CS229, are  typical recent pedagogical expositions of machine learning.  They usually begin with simple datasets and the simplest possible statistical models and machine learning algorithms, and then, with a greater or lesser degree of attention to issues of implementation, move through a succession of increasingly sophisticated and specialised techniques.  After sitting through 20  hours of Ng's online lectures, and attempting  some of the review questions and programming exercises, including implementing well-known algorithms using `R`, one comes to know datasets such as the house price dataset and `iris` quite well. Like the textbook problems that the historian of science Thomas Kuhn long ago described as one of the anchor points in scientific cultures [@kuhn_structure_1996], these iconic datasets provide an entry point to the 'disciplinary matrix' of machine learning. Through them, one  gains some sense of how predictive models are constructed, and what kinds of algorithmic architectures and forms of data are preferred in machine learning. 

In the course of writing this book, as well as reading the academic textbooks, popular how-to books, software manuals and blog-how-to posts,  like many other students, I attended graduate courses in Bayesian statistics, genomic data analysis, data mining, and missing data. Significantly, I also participated in the online machine learning courses and some machine learning competitions [^4]. These are all typical activities for people learning to do machine learning. Importantly, these courses, books, competitions and programming languages like `R`  are widely viewed and used. Ng's machine learning course on Coursera (of which he is co-founder, along with Daphne Koller, another machine learning scientist from Stanford), has a typical enrolment of 100,000. Youtube videos of his Stanford University lectures have cumulative viewing figures of around 500,000.  How would someone with a somewhat transdisciplinary background (undergraduate degrees in science and philosophy; postgraduate in philosophy) learn about machine learning online? I first encountered machine learning in reading recent scientific articles on genomics. Scholars interested in genomics have often discussed the growth of DNA sequence data in contemporary life sciences. They have carefully studied sequence and other biological databases, but less often attended to the ways in which data from those databases has been modelled or used predictively [^2]. Understanding predictive modelling in genomic science is difficult, since both the biology and the statistics are technically challenging. The possibility of reconstructing or implementing genomic models are more limited. Although I have attended genomic data analysis courses, even in these classes, where my classmates where either postgraduate research students or scientists, only a narrow selection of techniques for working with genomic data are covered. The online machine courses are, by contrast, quite generic and the examples are broad-ranging.  

Both the textbooks and the online university courses on machine learning have a similar topic structure. They nearly always begin with 'linear models' (fitting a line to the data), then move to logistic regression (a way of using line fitting to classify binary outcomes; for example, spam/not-spam; malignant/benign; cat/non-cat), and afterwards move to some selection of neural networks, decision trees, support vector machine and clustering algorithms. They add in some decision theory, techniques of optimization, and ways of selecting predictive models (especially the bias-variance tradeoff). Whether in the textbook or the online course, examples such as the house price dataset form part of the learning of machine learning. They differ, however, in one crucial regard. Reading the _Elements of Statistical Learning_ textbook or one of machine learning books written for programmers (for example, _Programming Collective Intelligence_ or _Machine Learning for Hackers_ [@segaran_programming_2007; conway_machine_2012]) does not directly enrol the reader in a machine learning process. By contrast, doing a Coursera course on machine learning brings with it an ineluctable sense of being machine-learned, of oneself becoming an object of machine learning. The students on Coursera are the target of machine learning. Daphne Koller and Andrew Ng are leading researchers in the field of machine learning. They are also co-founded  the online learning site [Coursera](http://coursera.org).  As experts in machine learning, it is hard to imagine how they would not treat courses as learning problems. And indeed, Daphne Koller sees things this way:

> There are some tremendous opportunities to be had from this kind of framework. The first is that it has the potential of giving us a completely unprecedented look into understanding human learning. Because the data that we can collect here is unique. You can collect every click, every homework submission, every forum post from tens of thousands of students. So you can turn the study of human learning from the hypothesis-driven mode to the data-driven mode, a transformation that, for example, has revolutionized biology. You can use these data to understand fundamental questions like, what are good learning strategies that are effective versus ones that are not? And in the context of particular courses, you can ask questions like, what are some of the misconceptions that are more common and how do we help students fix them? [@koller_daphne_2012]

Whether the turn from 'hypothesis-driven mode to the data-driven mode' has 'revolutionized biology' is debatable (I return to this in a later chapter). And whether or not the data generated by my participation in Coursera's courses on machine learning generates data supports understanding of fundamental questions about learning also seems an open question.  Nevertheless, the loopiness of this description interests and appeals to me. I learn about machine learning, a way for computer models to optimise their predictions on the basis of 'experience'/data, but at the same time, my learning is learned by machine learners. This is not something that could happen very easily with a printed text, although versions of it happen all the time as teachers work with students on reading texts.  While Coursera and other MOOCs promise something that mass education struggles to offer (individually profiled educational services), it also negatively highlights the possibility that machine learning in practice can, somewhat recursively, help us make sense of machine learning as it developes. Put more simply, and less loopily, the

## Writing code about code: scientific software and executable papers

I've been writing code for years [@mackenzie_cutting_2006]. But writing code was nearly always something  distant from writing about code.  Only recently, owing mainly to developments in ways of analysing and publishing scientific data, have I found ways to write about code while coding. This looping between writing code and writing about code brings about sometimes generative, sometimes frustrating encounters with various scientific knowledge (mathematics, statistics, computer science), with infrastructures on many scales (ranging across networks, databases here and there, hardware and platforms of various kinds, as well as interfaces) and many domains. At many points in researching the book, I digressed a long way into quite technical domains of statistical inference, probability theory, linear algebra, dynamic models as well as database design and data standards. Not all of the code I've written in implementing machine learning models or in reconstructing certain data practices is included in this text, just as not all of the words I've written in trying to construct arguments or think about data practices has been included. Much has been cut away and left on the ground (although the `git` repository of the book preserves many traces of the writing). So, like the recipe books, cookbooks, how-tos, tutorials and other documentations I have read, the code, the graphics and the prose have been tidied here. Many of the exploratory forays are lost and almost forgotten.  Nevertheless, the several years I have spent  writing about data practice has felt substantially different to any other project by virtue of a strong coupling between code in text, and text in code. Practically, this is made possible by working on code and text within the the same file, in the same text editor (not a word processing package). Switching between writing  `R` and  Python code (about which I say more below) to retrieve data, to transform it, to produce graphics, to construct models or some kind of graphic image, and within the same file be writing academic prose, might be one way to write more praxiographically about data and machine learning in particular.

The capacity to begin to mix  text, code and images depends on an ensemble of software tools that differ somewhat from the typical social scientist or humanities researchers' software toolkit of word processor, bibliographic software, image editor and web browser. In particular, it relies on software packages in `R` such as the '`knitr`' [@xie_knitr_2013; @xie_new_2012,] and in python, the `ipython` notebook environment [@perez_ipython:_2007, as well as plain text editing software. Both have been developed by scientists and statisticians in the name of 'reproducible research.' Many examples of this form of writing can be found on the web: see [IPython Notebook Viewer](http://nbviewer.ipython.org/) for a sample of these. These packages are designed to allow a combination of code written in `R`, python or other programming languages, scientific text (including mathematical formula) and images to be included, and importantly, executed together. In order to do this, they typically combine some form of text formatting or 'markup,' that ranges from very simple formatting conventions (for instance, the 'Markdown' format used in this book is much less complicated than HTML, and uses markup conventions readable as plain text and modelled on email [@gruber_markdown:_2004];) to the highly technical (LaTeX, the de-facto scientific publishing format or 'document preparation system' [@lamport_document_1986] elements of which are also used here to convey mathematical expressions). They add to that blocks of code and inline code fragments that are executed as the text is formatted in order to produce results that are shown in the text or inserted as figures in the text. 

There are a few different ways of weaving together text, computation and images together.  Each suffers from different limitations. In `ipython`, a scientific computing platform dating from 2005 [@perez_ipython:_2007] and used across a range of scientific settings, interactive visualization and plotting, as well as access to operating system functions are brought together in a `Python` programming environment. Especially in using the `ipython` notebook, where editing text and editing code is all done in the same window, and the results of changes to code can be seen immediately, practices of working with data can be directly woven together with writing about practice. By contrast, `knitr` generates documents by combining text passages and the results (graphs, calculations, tabulations of data) of code interleaved between the text into one output document. When `knitr` runs, it executes the code and inserts the results (calculations, text, images) in the flow of text. Practically, this means that the text editor used to write code and text, remains somewhat separate from the software that executes the code. By contrast, `ipython` combines text and `Python` code more continuously, but at the cost of editing and writing code and text in a browser window. Most of the conveniences and affordances of text editing software is lost.   While `ipython` focuses on interactive computation, `knitr` focuses on bringing together scientific document formatting and computation. From the perspective of praxiography, given that both can include code written in other languages (that is, python code can be processed by `knitr`, and `R` code executed in `ipython`), the differences are not crucially important [^3]. This whole book could have been written using just Python, since Python is a popular general purpose programming language, and many statistical, machine learning and data analysis libraries have been written for Python. Widely used Python modules such as NumPy, SciPy, Scikit-learn, open-cv or Pandas  allow anything  done in `R` to be done in Python. It is difficult to generalise about the differences between the two programming languages.  I have used both, sometimes to  highlight  tensions between the somewhat more research-oriented `R` and the more  practical applications typical of Python, and sometimes because code in one language is more easily understood than the other.  
 
While I have read textbooks on machine learning and statistics, how-to books on data analysis, data-mining and machine learning, as well as myriad online documents, research papers, and help files (alongside the work of other scholars, philosophers, anthropologists, sociologists, and media theorists),  I've also attempted  to bring the writing of code and writing about code into proximity in order to see if that proximity or mixture of writing code and writing words makes a practical and a praxiographic difference. If recent theories of code and software as forms of speech, expression or performative utterance are right [@cox_speaking_2012; coleman_coding_2012], it should. But how does it make a difference?   Already at various points in this text, different written materials have been juxtaposed, and that is completely normal, indeed, almost unavoidable in any writing. Any textual form is somewhat woven. The question is how weaving through  code in one domain of contemporary technical practice, machine learning, answers the methodological injunctions of keeping the practices present (Mol), maintaining the 'concrete sense of value-attainment' (Whitehead) or allowing 'affective expansion' (Wilson). How does this bring  practices more directly into awareness? 

## Aleatory encounters with data in the world

There are many different ways to encounter data. Let us imagine it happens somewhat by chance, as when walking down a city street we notice something about a building for the first time. Some feature, old or new, attracts our attention. What is happening in such situations? One possibility is that we find what we need or see what we want. From a psychoanalytic perspective, as Christopher Bollas writes, 

> the ability to move freely in the object world, to use its thing-ness as a matrix for thinking-by-action, pivots around whether the aleatory object determines - whether we move on quickly due to the dynamic of our last encounter with the object - or whether we select objects because we are unconsciously grazing: finding food for thought that only retrospectively could be seen to have a logic [@bollas_evocative_2009, 93].

Bollas' description of moving in the world has a flavour of the _flaneur_, a now classic emblem of modern urban experience, and a figure whose experience of the city was an important in understanding city life.  I'm not sure I like that as an emblem of working with data.  It seems that the gendered subject of this experience, the man who encounters data and contemporary data practices, would remain intact, more or less as they were, having found what they wanted or needed. 

This is not to say that  what Bollas describes has no resonance with the many developments in data architectures increasingly designed to invite grazing.   Something _flaneurish_ is happening to data and data techniques. There are many potentially alluringly abundant and doubtlessly  important datasets on offer, especially when we consider the increasingly commodified offering of huge realtime data streams by social media platforms via Application Programming Interfaces (APIs). Moreover, public scientific databases across the gamut of disciplines, social media datasets, government statistical data, data from various environmental sensors, financial market and other price data (so much of this!), let alone all the media forms -- images, text, video -- that can be rendered into data, congregate on the internet. Even researchers within specific fields struggle to 'discover' the data  in their field. Hundreds of databases dot scientific fields like genomics    Furthermore, the 'tool-being' of data analysis techniques, as we have already seen in the case of `R`, stocks thousands of things  in easily browsed archives. Even in the domain of machine learning itself, which we have hardly begun to discuss, we are confronted with a many coloured and seemingly interminable mosaic of different techniques.   

Our encounters with all these are necessarily somewhat 'aleatory' or chancy, even if they could retrospectively turn out to have a logic. To give just a small sample of this interminable chain of possible connections from which data might be drawn, we could turn to the World Bank databases. Via the [World Bank Data catalog](http://data.worldbank.org/developers/data-catalog-api), the World Bank publishes data on global economic activity. Much of this data can be retrieved from its Application Programmer Interface (API) using `R`  or Python code. Until relatively recently, most people, apart from economists or the occasional journalist,  would have little chance of directly encountering this data. How can its newly available thing-ness become a 'matrix for thinking-by-action'? The data can be downloaded using a web browser, and then imported into a spreadsheet, but in this case, since we are interested in how a programming language affects encounters with data, it is important to see how  whether code  encounters the object-world of World Bank data differently. The lines of code, unlike a series of mouse movements, pointing, clicking and typing into a spreadsheet, make it possible to both track the movements and gestures involved much more directly, but also to change their scale through repetition and variation. (Ironically, coding by hand slows down practice in some ways. Sometimes code is criticised for its unreadability or inaccessibility. Actually, in many ways, code makes it possible to follow practice much more closely, albeit at the cost of more reading.)

```{r worldbank_R, echo=TRUE, cache=TRUE, message=FALSE, warning=FALSE, comment=NA, size='smallsize', results='markup' } 

    library(RCurl)
    library(rjson)
    url = 'http://api.worldbank.org/countries?incomeLevel=OEC&format=json'
    high_income_countries = fromJSON(getURL(url))
    countries = unlist(sapply(high_income_countries[[2]], '[', 'name'), use.names=FALSE)[1:10]
```

Or in Python:

```{r wbdata_python, echo=TRUE, cache=TRUE, message=FALSE, warning=FALSE, comment=NA, engine='python' } 

    # a package for working with World Bank API
import  wbdata 
    # fetch the names of economically developed countries
countries = [i['id'] for i in wbdata.get_country(incomelevel="OEC", display=False)]
    # select the economic indicators we wish to view
indicators = {"IC.BUS.EASE.XQ": "doing_business", "NY.GDP.PCAP.PP.KD": "gdppc"}
    # fetch that data
df = wbdata.get_dataframe(indicators, country=countries, convert_date=True)
    # output summary of values
df.describe()
```

Now the World Bank, being a bank after all, makes this numerical data available in order to facilitate business, economic growth, investment, international trade and so forth. For instance, some of the numbers downloaded in the vignettes shown above on 'doing business'  represents measures of how easy it is do business in various countries, and the World Bank wants to liberalize trade. Given that liberalizing trade is not the issue here for us, what do we get from looking at these lines of code, or the data they pull down from the World Bank API?  Leaving aside the whole question of what these numbers do (different kinds of numbers are the main topic of  a later chapter), the code itself is strikingly brief. The lines tersely invoke whole infrastructures.  The accumulated work of collating many different observations and measurements can be marshalled and assembled in a few lines of code. I have written lines like the first  five lines of `R` code shown here many times. They superimpose internet protocols (supplied via the RCurl library), data standards (rjson), internet servers (api.worldbank.org), databases (the 'countries?incomeLevel...' is a database query), data structures and algorithms ('unlist'). In this case they request a list of high-income countries (members of the OECD) from the WorldBank databases. The World Bank database returns a nested list describing `r length(countries)` countries. The list is hardly surprising (`r paste(countries[1:7],collapse=', ')` ... ). But the point of this code is to show something of the economy of movement and reshaping of data associated with `R`.  In this code, the first line loads an `R` library (the 'RCurl' package) to make requests to internet servers. The second line loads an `R` library  (the 'rjson' package) that can process data in the JSON (Javascript Standard Object Notation) format, a data format commonly used by servers to transfer data. Lines 3-4 provide the internet address of the World Bank API, request the data, receive the response, and convert the JSON data into a typical `R` data structure, a list. The final line selects from the list, which has many sub-lists nested within it, just the names of the countries.  

Although these somewhat trivial code vignettes scarcely begin to extract value that Hal Varian propounds, in retrieving data on the production of value they do begin to point at  something important in the mode of production of value from data. Their brevity  suggests very well travelled paths. In terms of urban architecture, the packages and modules that support this brevity are like escalators that quickly move people between levels and floors in a city.  Like the how-to books with their often just-so demonstrations of problem solving, the compactness or briefness code suggests that many obstacles, detours and blockages have already been encountered by others and dealt with by them. This is not to deny any _flaneurish_ skill on my part.  I did some 'processing'  here in, for instance, choosing a path into the World Bank datasets via the selection of the query terms (`r cat(url)`). It also includes the re-formatting operations that first transform JSON data into an `R` list, and then traverse the nested list structure, selecting only the items of data labelled 'name.' These kinds of selection operations, and transformations in the format of data are vital in contemporary data. Whether in the form of well-established relational database query language SQL, in the requests made to one of the legion of APIs that render data on various internet platforms such as Youtube, WorldBank or Pubmed, modifications in the shapes and sorts of data are a constant concern.

On the other hand, the fact that the vignettes do nothing apart from retrieve  the data suggest that a series of transformations is still to  follow. For instance, numerical data might binned into categories such as high, medium, low or some other form of ranking; text might be split into keywords; the presence or absence of some value might be counted or summarised through calculations (such as the the mean or average, the median or middle value, or mode, the majority value); etc. Some of these transformations matter more than others. In the Python vignette, which uses the `wbdata` Python package written by [Oliver Sherouse](http://wbdata.readthedocs.org/en/latest/#suggested-citation),  a few  lines of code produce a similar dataset from the WorldBank data. This time, it explicitly takes the form of a dataframe, a row-column data structure. Tables, arrays and matrixes -- the rectangular form of data that typically puts one example per row, with different columns representing different measurements, attributes or properties of the examples. In both Python and `R`, such as matrixes and dataframes are heavily used for reasons that go deep in machine learning algorithms and data practices more generally. These kinds of table-making operations are often highlighted in books, documentation and tutorials on `R` since they deeply affect how code runs. In this case,   the code vignette turns the list, with its sublists, into a flat table or matrix. This is a kind of topological transformation since the nested list of countries with their economic indicators has a different relationality  than the table with its columns.  

## Vectorising data

The character of the transformations that might follow is hard to summarise. Nevertheless, at several specific points in the lines of code above  -- the  `sapply` function and  -- it demonstrates another form of compression or superimposition commonly found in software such as  `R` and other programming languages designed for data practice (Octave, Matlab, Python's NumPy, or C++ Armadillo): _vectorised_ transformations of data.  As a programming language, `R` is striking for its many  `-ply` constructs.  There are several in the core language and many to be found in packages (the popular 'plyr' package; versions of 'ply' can also be found in recent Python data analysis libraries such as Pandas).  These include `apply`, `sapply`, `tapply`, `lapply`, `mapply`. All of the `-ply` constructs have a common feature: they take some collection of things (it may be ordered in many different ways – as a list, as a table, as an array, etc), do something to it, and return something else. This hardly sounds startling. But while most programming languages in common use offer constructs to help deal with collections of things sequentially (for instance, by accessing each element of data set in turn and doing something), `R` offers ways of dealing with  them all at once. The `-ply` constructs ultimately derive from the functional logic developed by the mathematician Alonzo Church in the 1930s [@church_note_1936; @church_introduction_1996]. By virtue of these `-ply` constructs,  `R` sometimes presents difficulties for programmers trained to code using so-called procedural programming languages. In many mainstream programming languages, transformations of data are often done using loops and array constructs in which some operation is successively repeated on each element of a data structure. Moreover,  in vectorised languages such as `R`, transformations of a data structure  expressed in one line of code  simultaneously affect all the elements of the data structure. As the widely used _R Cookbook_ puts it, 'many functions [in `R`] operate on entire vectors, too, and return a vector result' [@teetor_r_2011, 38]. Or as _The Art of `R` Programming: A Tour of Statistical Software Design_ by Normal Matloff puts it, 'the fundamental data type in `R` is the _vector_' [@matloff_art_2011, 24], and indeed in `R`, all data is vector. There are no individual data types,  only varieties of vectors in `R`. Again, as _R in a Nutshell_  puts the point directly: 'in `R`, any number that you enter ... is interpreted as a vector' or as 'an ordered collection of numbers' [@adler_r_2010, 17]. The practical difference is illustrated in the code vignette above in which two 'vectors' of numbers are added together, in the first case using a classic `for`-loop construct, and in the second case using an implicitly vectorised arithmetic operation `+`. The difference in speed between adding $1 ... 10$ using a loop or vector arithmetic is completely trivial here, but when millions of numbers are handled arithmetically, these implementation differences significantly affect the work of both programmers and computing machinery.  This simultaneity is only apparent, since somehow the underlying code has to deal with all the individual elements, but vectorised programming languages take advantage of hardward optimisations or carefully-crafted low-level libraries. 
 

```{r vectorisation, echo=TRUE, cache=TRUE, message=FALSE, warning=FALSE, comment=NA, size='smallsize', results='markup' } 
 
     vector1 <- c(0,1,2,3,4,5,6,8,9,10)
     vector2 <- c(0,1,2,3,4,5,6,8,9,10)

     # procedural programming-style looped addition
     result_looped = vector()
     for (i in vector1) {
        result_looped[i] = vector1[i] + vector2[i]
     }
     result_looped

     # vectorised addition
     result_vectorised  <- vector1 + vector2
     result_vectorised

```
The functional programming style of applying functions to functions seems strangely abstract. Once you get a feel for them, these -ply constructs  and the implicit vectorisation of many operations on data structures are very convenient to write, and the code runs much faster. They  transform data in ways that utilise  increasingly parallel contemporary chip architectures, and adapt readily to the increasingly Cyclopean infrastructures of cloud computing. The `-ply` operations reduce both data and computational frictions. The real stake in -plying data, however, is not speed but transformation. -Plying makes working with data less like iteration (number, text, table, list, etc), and more like folding a pliable material. Such shifts in feeling for data are very mundane yet crucial to the flow of data.  Plying, especially with added connotation of trade and exchnage as in the 'plying goods', seems much closer to the kinds of work done on data than the figures of data flow or data deluge. 

While the examples of vectorised computation that I have shown are relatively trivial -- turning a nested list into a table, multiplying sequences of numbers -- these vectorised operations have  both concrete and abstract implications.  The concrete implication we have just seen:  the implicit and explicit use of vectorised processes feeds directly into the scaling up of computational  work on data. Not only vectorised computation, but the various styles of programming and infrastructure that lend themselves to simultaneous transformations of large matrixes or tables of data lie at the heart of not only machine learning, but in the intensive processing of data in many different places. The rendering of moving images in  computer graphics depends implicitly on vectorised  transformations of matrices of numbers. 

## Composing data forms  in vector space

The more abstract implication of vectorisation and the forms of multiplication it encourages and demands will take us longer to unravel. Statistical modelling, data-mining, pattern recognition, recommendation systems, network modelling and machine learning rely on  such transformations. 'Fitting a model' to data is often literally implemented, as we will see, by  multiplying data matrices together.  Hence,  describing vectorised transformations of   data in  `R` and other computing environments (for instance, the popular `Map-Reduce` architecture) is not only a matter of pointing to the now familiar increases in  speed or efficiency of computation. The vectoral treatment of data taps into and is interwoven with the transformation of data more generally. Later chapters will discuss various ways in which the vectoral dimensionality of data or its rendering as _vector space_ scales up and scales down in machine learning.[^1]  In fact, a major goal is to disentangle some different forms of vectorisation, or different ways of inhabiting vector space, associated with data today. But to emphasise this abstract mode of existence of data as a vector space  is not to say that the practical transformations of data in various forms of code and computing infrastructure can be ignored. Just the opposite is the case: the  abstract understanding of data as vector space generates many scaling potentials to which particular implementation in code and computing hardware respond. This concretising dynamism arising from abstraction is not new. We need only think of the way in which Marx describes the replacement of highly distributed artisanal manufacture by interconnected systems of machines in factories driven by prime-movers (Watt's steam engine above all) to see a similar process of reorganisation driven by  a numerate abstraction, in that case the cost of labour (see Chapter XV of [@marx_capital_2007]).  

The praxiographic challenge lies in attending to this highly abstracted vector space in ways that maintain and indeed augment its value-relevance, its concreteness, and attachments to lives and places. We lack good intuitions of how to do that because of the ways in which data as vector space has been constructed and described. While we now often hear about algorithms, predictions and smart devices affecting our lives, the plural actuality of what they compose largely still eludes us. Often data is  represented as if it is an homogeneous mass or a continuous flow, but  it takes many different shapes and has many different _densities_.  I loosely borrow the term 'density' from statistics, where *probability density functions* are often used to describe the hardly ever uniform distribution of probabilities of different values of a variable. Sensing density as a form of variation matters greatly both in machine learning itself, where algorithms seek purchase on unevenly distributed data, and in any broader praxiography of data.  By thinking of data density, we perhaps also gain a better sense of the heft and weave of data. Data  spaces out in many different density shapes, depending on how the data has been generated or instanced. Whatever the starting point (a measuring instrument, people clicking and typing on websites, a device like a camera, a random number generator, etc.), it is inevitable that later transformations will remap  it  to different shapes and forms. A given machine learning algorithm, data visualisation or database query will need the data to be in specific vectoral shapes (vectors,  matrices, arrays, etc), and fit within a certain volume or scale.    The process of composing data for statistical, visual, predictive or even storage purposes, maps a concrete situation, some state of affairs, onto forms imbued with various geometrical, probabilistic, decisionist abstractions often expressed in terms of functions or mathematical models. If, as I have been proposing, we seek contact with the practices, or the concrete value-situation, then the reshaping and reflowing of densities matters greatly.   This forming and reforming of data is evidence of  implicated relations. These practices attest to what Whitehead called 'strain': 'a' feeling in which the forms exemplified in the datum concern geometrical, straight, and flat loci will be called a 'strain.' In a strain, qualitative elements, other than the geometrical forms, express themselves as qualities implicated in those forms' [@whitehead_process_1960, 310]. In many machine learning models, for instance, the exemplified forms are straight or flat loci (see the discussion of line fitting, hyperplanes, decision boundaries in the next three chapters).  Yet  different practices also seek to elicit relations that strain the linear or geometrical shaping of data, that show how it does not fit.  Some  practices working in the name of linear forms effectively trace strain feelings. These feelings are the affective connectors between the abstractions and the concrete-value situations in which data arises. They are vital to the psychic life of data, and the ongoing reality-multiples it figures. 

In the machine learning literature, the composition of data is sometimes expressed quite formally. For instance, Peter Flach's _Machine Learning: The Art and Science of Algorithms that Make Sense of Data_ [@flach_machine_2012] formalises the relation between data and shaped data density like this:

> Features, also call attributes, are defined as mappings $f_i: \mathcal{I} \rightarrow \mathcal{F}_i$ from the instance space $\mathcal{i}$ to the feature domain $\mathcal{F}_i$. We can distinguish features by their domain. [@flach_machine_2012, 298]

'Features' or 'attributes' are typically columns in a table. They are sometimes already present in the data (for instance, the house price dataset features are pre-defined), but they are sometimes constructed or engineered from the data. As the prominent machine learning Pedro Domingos writes in a recent overview of the difficulties of doing machine learning: 'feature engineering is more difficult because it is domain-specific, while learners can be largely general purpose' [@domingos_few_2012, 84]. Note that Domingos uses 'domain' here to refer to a concrete situation, whereas Flach's domain refers to a set of values that can be input into a function or a machine learning algorithm. This coincidence in choice of words is actually symptomatic. People reshape data in different ways and for different purposes, sometimes in the name of a concrete situation, sometimes in view of the form of a particular mathematical function or a computational infrastructure (for instance, the amount of RAM heavily affects  modelling). At times, data is  folded together in order to contain it or reduce its dimensionality so that it fits somewhere. At other times, much work goes into expanding the dimensionality of data in order to find ways of eliciting relations that remain somewhat latent or hidden in it, that strain or contort its form in ways that are not easily seen. 

One of the stakes in following what happens to vectors, lists, matrixes, arrays, dictionaries, sets, dataframes, or series or tuples in data, is to get a sense of how such practices map a concrete situation into something more abstract. These are the practices of abstraction, and any experience of abstraction in this domain must process along these lines. More concretely, as we will see in later chapters, the predictive models, the supervised and unsupervised learners, the classifiers, the decision trees and the neural networks in machine learning, to name a few techniques, will be largely implemented as transformations that fold and refold matrices and vectors.  Amidst the many different operations and transformations associated with machine learning and in algorithmic treatments of data, perhaps the single most important practice is matrix multiplication.  It looms large in writing about machine learning, and in many articles and textbooks, matrix manipulations are taken for granted.  For instance, the first full mathematical expression in _The Elements of Statistical Learning_ is shown below along with the surrounding text:

>The linear model has been a mainstay of statistics for the past 30 years and remains one of our most important tools. Given a vector of inputs
$X^T = (X_1 , X_2, . . . , X_p)$, we predict the output $Y$ via the model
>$$\hat{Y} = \hat{\beta_0}  + \sum^p_(j=1) X_j \hat{\beta_j)}$$
>The term $\hat{\beta_0}$ is the intercept, also known as the _bias_ in machine learning [@hastie_elements_2009, 11].

Introducting the linear model, the authors of the textbook immediately resort to matrix notation. There is nothing particular mathematically elusive here, only a highly aggregated set of sums and multiplications. While such expressions are not easy to read with knowing what all the symbols mean, they offer a very direct opening to thinking about machine learning more generally.  This model is the 'simple but powerful linear model' (11) in which $Y$, the so-called 'response variable' is modelled by adding together  (the $\sum$ operator means adding them) a combination of the input variables or 'features' in  $X_j$ and a value of the intercept $\beta_0$.  Even if this does not make much sense without reading more, we can begin to see that the linear model, a mainstay of machine learning, concretely takes the form of vectors and matrices added and multiplied.  The subscripts $j=1$ refer to individual input variables.   Importantly, as Hastie and co-authors go on to say, 

> Often it is convenient to include the constant variable 1 in $X$, include $\hat{\beta_0)}$ in the vector of coefficients $\hat{\beta}$, and then write the linear model in the vector form as an inner product:
$\hat{Y} = X^T \hat{\beta}$
> where $X^T$ denotes vector or matrix transpose [@hastie_elements_2009, 12-13].

We will need further explanation to make more sense of these dense expressions, but for the moment, the only thing that needs to come across is the heavy reliance on vectors and matrices, which means that entire models can be expressed highly concisely as matrix multiplication operations. In this case, with the convenient convention of making the intercept into a constant input variable, linear models can be written simply as an 'inner product', that is,  as a matrix-matrix multiplication. Learning machine learning, and learning to implement machine learning techniques, is largely a matter of implementing series of matrix multiplications. As Andrew Ng advises his students,

> Almost any programming language you use will have great linear algebra libraries. And they will be high optimised to do that matrix-matrix multiplication very efficiently including taking advantage of any parallelism your computer. So that you can very efficiently make lots of predictions of lots of hypotheses [Ng, lecture 10.50]

In other parts of his teaching, and indeed throughout the practice exercises and assignments, Ng stresses the value of implementing machine learning techniques for both understanding them and using them properly. But this is one case where implementation does not facilitate learning. Ng advises his learners against implementing their own matrix handling code.    They should instead  use the 'great linear algebra libraries' found in 'almost any programming language.' 'Linear algebra libraries' multiply, transpose, decompose, invert and generally transform matrices and vectors. They will be 'highly optimised' not because every programming language has been prepared for the advent of machine learning on a large scale, but rather more likely because matrix operations are just so widely used in image and audio processing.  Happily, Ng observes, that means that 'you can make lots of predictions.'  It seems that generating predictions and hypotheses outweighs the value of understanding how things work on this point. 

How do we 'efficiently make lots of predictions' using matrices? Let's return to the example of house prices mentioned by Norman Nie. In this example, house price is generally treated as the response variable ($Y$), and the number of bedrooms and overall size in square feet are the input variables ($X_1$, $X_2$).  A linear model seeks to find a line (or a plane, or as we will see, a hyperplane) that best fits the points. While I will discuss in the next chapter how such a line can be found, here I want to only show one solution to this problem because it takes the form of a matrix multiplication. As Hastie and co-authors write, the 'unique solution' to the problem of fitting a linear model to a given dataset using the most popular method of 'least squares' [@hastie_elements_2009, 12] is given by:

>$\hat{\beta} = (\mathbf{X}^T\mathbf{X})^-1\mathbf{X}^T\mathbf{y}$

This is quite a tightly coiled expression of how to calculate the parameters $\hat{\beta}$ for a linear model. The two matrices involved are the $X$ input variables (house size and number of bedrooms) and a 1-dimensional matrix $y$, the house price. These matrices are multiplied, transposed (a form of rotation that swaps rows for columns) and inverted (a more complex operation that finds another matrix) Implementing it for  the San Francisco house price data only requires a few extra lines of code:

```{r house_price_predict, echo=TRUE, cache=TRUE, message=FALSE, warning=FALSE, comment=NA, size='smallsize', results='markup' } 
 
    library(MASS)
    houses = read.csv('data/ex1data2.txt', header=FALSE)
    names(houses) = c('size', 'bedrooms', 'price')
    X = as.matrix(cbind(1, houses[,1:2]))
    y = houses$price

    # calculate the coefficients of the model using the least squares algorithm
    beta_hat = ginv(t(X) %*% X) %*% t(X) %*% y
    
    #making a prediction
    rooms =3
    size = 2000
    predicted_price = beta_hat[1] + beta_hat[2]*size + beta_hat[3]*rooms
    plot(houses$size, houses$price)
    abline(beta_hat[1], beta_hat[2])
```

Having calculated the  values of $\hat{\beta}$,  the coefficients of the linear model, we can make predictions of house prices for any given number of bedrooms and house area. For instance, a house with 4 bedrooms and area of 1000 sq. feet should cost around $`'r predicted_price`. As the plot of the house price data shows, albeit without showing all the variables, the line that has been fitted passes through many of the points. Finding the geometric form of the line or surface is the predictive element of the function. 

We don't need to grasp all the details of the matrix operations working to fit this prediction surface. As with the terseness of  the code that fetches data from the World Bank databases, I want to point mainly here to the tightly coiled matrices  that produce predictions.  One line of fairly dense code that pivots on matrix multiplication operations (`%*%`) effectively makes the model `beta_hat = ginv(t(X) %*% X) %*% t(X) %*% y`. The critical question is whether by unwinding some of these operations, for instance, by seeing how  matrix multiplication ripples through different treatments of data, the we get closer to the vitality of the multiple/multiplying concrete value-situations that connect calculation and feeling.  My suggestion is that the praxiographic description of how machine learning techniques vectorise and multiply data densities  as abstractions provides a fairly direct way begin to name and unravel the processes of knowing, predicting and deciding on which many aspects of the turn to data rely.  The following three chapters enlarge on this general point about multiplication in different ways. As we will see, the matrix operations we have just been viewing are themselves organised by other layers of intuition that explore shape, movement and surfaces in much more convoluted forms.  (Without mentioning it, the same multiplication operation, the so-called _dot product_ lay at the core of the perceptron algorithm code I quoted from Wikipedia earlier.)  Over a decade ago, the cultural theorist Brian Massumi wrote in almost mathematical vein that 'the space of experience is really, literally, physically a topological hyperspace of transformation' [@massumi_parables_2002, 184]. He may have been describing the heavily vectorised operations on which much machine learning, statistical modelling and prediction depend. Experience in so many settings -- media, consumption, science, security, etc -- is embedded in the hyperspaces of matrix transformation, and matrix multiplications. 


[^1]:  In terms of multiplying matrices, dimensionality both constrains and enables many aspects of the prediction. Perhaps on the grounds of data dimensionality alone, we should attend to dimensionality practices in  `R`. A fuller discussion of dimensionality of data is the topic of a later chapter. There I discuss how machine learning re-dimensions data in various ways, sometimes reducing dimensions and at other times, multiplying dimensions.

[^2]: Machine learning in genomics is also the topic of a later chapter. Recent life sciences are not alone in their resort to computational techniques, but in contrast to commercial, industry or government data practices, it is easier to track how data is transformed, reshaped and re-dimensioned in producing biological knowledges. The scientific publications, the public databases and open source software work together to allow this. 

[^3]:In combining Markdown and LaTex, I also make use of the [Pandoc utility](http://johnmacfarlane.net/pandoc/README.html), a command line tool that converts between different document formats.

[^4]: I discuss the competitions and forms of machine learning subjectification in a later chapter. 

[^5]: Isabelle Stenger's  book on Whitehead offers a deeply philosophical introduction to his work [@stengers_penser_2002]. [TBA: English version]
    
