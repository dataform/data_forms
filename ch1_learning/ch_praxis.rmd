\chapter{Diagramming machine learning
\label{ch:diagram}

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=12, tidy=TRUE, fig.height=8, 
                      echo=FALSE,  warning=FALSE, message=FALSE, dev='pdf')
library(RSQLite)
con <- dbConnect(RSQLite::SQLite(),'../ml_lit/all_refs.sqlite3')
library(ggplot2)
library(stringr)
library(xtable)
options(xtable.comment = FALSE)
```


> Machine learning is not magic; it cannot get something from nothing. What it does is get more from less. Programming, like all engineering, is a lot of work: we have to build everything from scratch. Learning is more like farming, which lets nature do most of the work. Farmers combine seeds with nutrients to grow crops. Learners combine knowledge with data to grow programs. [@Domingos_2012, 81] \index{Domingos, Pedro}

>Take the verb 'moving': if we write 'it-moving' we are expressing a multiplicity in a diagrammatic way. It-moving indicates a complex abstract machine which can appear independent of any subjective tendency -- it could refer to an individual, an army, an insect, an object, a machine, an emotion, an idea. It allows for all the possibilities of moving, so that the verb retains it machinic character [@Guattari_1984, 139]\index{Guattari, Félix}.

Many different tendencies shape what is happening to data today, but our  sense of the potentials of data and calculation is closely linked to what Pedro Domingos terms 'a lot of work' done by machine learners. Although it sounds a bit banal, there is quite a lot at stake and many complications in this work. The problem is: who works on what? And conversely, and slightly more awkwardly, *what* works on who? In both of the epigraphs above, one from a renowned machine learning researcher and the other from the French philosopher, Félix Guattari, this flipping between who and what can be observed. In Pedro Domingo's talk of 'learners,' we might find it hard to decide what the end point is: a program or learning something? These slightly awkward formulations, with their emphasis on 'diagrammatic' combinations are worth pursuing I believe because they help us to make sense of an immense Magellanic constellation of documents, software, publications, blog pages, books, spreadsheets, databases, data centre architectures,  whiteboard and blackboard diagrams, and an inordinate amount of talk and visual media in orbit around machine learning.

The changes in calculative practice associated with machine learning work on multiple levels, ranging from infrastructures and chip architectures through to mathematical functions and categorisations with significant social and economic implications. In this chapter, however, I attend to a seemingly much more banal set of practices concerning data. On the one hand, ways of working with  data have shifted substantially over the last decade or so due to the growth in open source programming languages and as a part of the broader and well-known expansion of digital media cultures [@Couldry_2013]. The fact that scientists, software developers and data analysts use programming languages such as `python` and `R` \index{Python} \index{R programming language} just as much as commercial statistical and data software packages such as Matlab, SAS or SPSS is symptomatic of shifts in calculative culture [@Muenchen_2014]. Scientific research, which has long relied on counting and calculation, increasingly organises and processes data more comprehensively because workflows, datasets, algorithms and databases can be quickly and flexibly assembled in code. (Scientific computing languages such as FORTRAN  -- 'Formula Translator' -- have long underpinned scientific research and engineering applications in various fields [@Campbell-Kelly_2003, 34-35].\index{FORTRAN}) On the other hand, several decades of concentrated research and intensive application of statistical and machine learning algorithms in science, government, industry and commerce have gradually developed fairly powerful ways of automating the construction of models that classify and predict events and associations between things, people, processes, etc. This means that programs are being written differently (and this is why Domingos speaks of 'growing' programs), and that they implement and concentrate new layers of abstraction. In some ways, this different mode of writing code is precisely the level of abstraction at which we might need to move in order to know and come to grips with contemporary compositions of power and subjectivity. 

## A diachronic diagram of reading machine learning

In the course of writing this book, as well as reading the academic textbooks, popular how-to books, software manuals, help documents and blog-how-to posts,  I attended graduate courses in Bayesian statistics, genomic data analysis, data mining, and missing data. Significantly, I also participated in the online machine learning courses and some machine learning competitions.[^1.4] These are all typical activities for people learning to do machine learning. Importantly, these courses, books, competitions and programming languages are widely viewed and used. Andrew Ng's \index{Ng, Andrew} machine learning course on Coursera (of which he is co-founder, along with Daphne Koller, another machine learning scientist from Stanford) \index{Koller, Daphne}, has a typical enrolment of 100,000. Youtube videos of his Stanford University lectures have cumulative viewing figures of around 500,000 [@Ng_2008]. Amidst this avalanching mass, a  single highly cited and compendious textbook, _Elements of Statistical Learning: Data Mining, Inference, and Prediction_ [@Hastie_2009], currently in its second edition, can be seen from almost any point of the terrain.[^1.12] \index{\textit{_Elements of Statistical Learning_ }} The authors of the book, Jeff Hastie, Rob Tibshirani and Jerome Friedman \index{Hastie, Jeff} \index{Tibshirani, Rob} \index{Friedman, Jerome} are statisticians working at Stanford and Columbia University. (Statisticians and computer scientists from Stanford University loom large in the world of machine learning, perhaps due to their proximity to Silicon Valley.)  In terms of scientific publications, this book is like the Death Star in the _Stars Wars_ film series. It is a  massive object, densely bristling with diagrams, equations, tables, algorithms,  graphs and references to other scientific literature. It has some hyperobject-like tendencies [@Morton_2013] in the range of references, it's combinations of code, diagram, equation, scientific disciplines and computational elements, and perhaps in the somewhat viscous, interobjectively diverse referentiality that impinges on any reading of it. \index{hyperobject} Like the online machine learning courses and programming books I discuss below, _Elements of Statistical Learning_  combines statistical science with various algorithms to 'learn from data' [@Hastie_2009, 1]. The 768 pages of this often tersely written and quite mathematical book range across various kinds of problems (identifying spam email, predicting risk of heart disease, recognising handwritten digits, etc.), and various machine learning techniques, methods and algorithms (linear regression, k-nearest neighbours, neural networks, support vector machines, the Google Page Rank algorithm, etc.). This is not the only juggernaut machine learning texts.  I could just have well cleaved to Alpaydin's _Introduction to Machine Learning_ [@Alpaydin_2010]  (a more computer science-base account), to Christopher Bishop's  heavily mathematical _Pattern recognition and machine learning_ [@Bishop_2006], Brian Ripley's luminously illustrated and almost coffee-table formatted _Pattern Recognition and Neural Networks_ [@Ripley_1996] \index{Ripley, Brian}, Tom Mitchell's  earlier artificial intelligence-centred _Machine learning_ [@Mitchell_1997] \index{Mitchell, Tom}, Peter Flach's  perspicuous _Machine Learning: The Art and Science of Algorithms that Make Sense of Data_ [@Flach_2012] \index{Flach, Peter}, or further afield, the sobering and laconic _Statistical Learning for Biomedical Data_ [@Malley_2011].  These and quite a few other recent machine learning textbooks display a range of emphases, ranging from the highly theoretical to the very practical, from an orientation to statistical inference to an emphasis on computational processes, from  science to commercial applications. 

[^1.12]: The complete text of the book can be downloaded from the website [http://statweb.stanford.edu/~tibs/ElemStatLearn/](http://statweb.stanford.edu/~tibs/ElemStatLearn/). At the end of short intensive course on data minin at the Centre of Postgraduate Statistics, Lancaster University, the course convenor, Brian Francis, recommended this book as the authoritative text. Some part of me rues that day. That book is a poisoned chalice; that is, something shiny, valuable but also somewhat toxic. 

The book attracts different styles of reading. It is often cited by academic machine learning practitioners as an authoritative guide. On the other hand, students participating in new data science courses often come from different disciplinary backgrounds and find the tome unhelpful (see the comment by students during an introductory data science course documented in [@Schutt_2013]). Whether the citations are friendly or not, Google Scholar reports over 20,00 citations of the book http://scholar.google.com/scholar?hl=en&q=elements+of+statistical+machine+learning, October 2014), a huge citation count by any standards. (Michel Foucault's _The History of Sexuality: an Introduction_, one of the most highly cited book in the humanities,  receives about the same number of citations.) The citations that [@Hastie_2009] as well as its previous edition [@Hastie_2001] receive do not arrive principally from machine learning researchers. They come from a wide variety of fields. It is hard to find a field of contemporary science, engineering, natural, applied, health and indeed social science that has not cited it. A Thomson-Reuters Scientific 'Web of Science'(TM) search for references citing either the first or second edition of [@Hastie_2009] yields around 9000 results. These  publications sprawl across well over 100 different fields of research.  While computer science, mathematics and statistics dominate, a very diverse set of references comes from disciplines from archaeology, through fisheries and forestry, genetics, robotics, telecommunications and toxicology ripple out from this book since 2001. Table \ref{tab:fields_citing_hastie} shows the top 20 fields by count. One could learn something about the diagrammatic movement of machine learners  from that reference list, which itself spans biomedical, engineering, telecommunications, ecology, operations research and many other fields. While it is not surprising to see computer science, mathematics and engineering appearing at highest concentration in the literature, the molecular biology, control and automation, operation research, business and public health soon appear, suggesting something of the propagating energy of machine learning. 

```{r hastie_field, results='asis', cache=TRUE}
library(stringr)
library(xtable)
query = 'select * from basic_refs where anchor like "hastie%"'
res = dbGetQuery(con, query)
sc = res$SC
fields = str_trim(unlist(str_split(sc, ';')))
tab = xtable(as.data.frame(sort(table(fields), decreasing=TRUE)[1:20]), label = 'tab:fields_citing_hastie', caption = 'Subject categories of academic research literature citing \textit{Elements of Statistical Learning} 2001-2015. These subject categories derive from Thomson-Reuter \textit{Web of Science}.' )
print(tab, comment=FALSE)
```

```{r hastie_pages, engine='python', cache=TRUE, echo=FALSE}

import pandas as pd
import re
import os
import sys

def load_records(data_dir):
    """Return dataframe of all records,
    with new column of cited references as list"""

    # I saved all the WoS full records for the search term in this directory
    files = os.listdir(data_dir)
    wos_df = pd.concat([pd.read_table(wos_df, sep='\t', index_col=False)
                        for wos_df in [data_dir+f for f in files
                        if f.count('.txt') > 0]])
    wos_df = wos_df.drop_duplicates()

    #fix index
    index = range(0, wos_df.shape[0])
    wos_df.index = index

    #to get all cited refs
    if wos_df.columns.tolist().count('CR') > 0:
        cited_refs = [list(re.split(pattern='; ',
                                    string=str(ref).lower().lstrip().rstrip()))
                                    for ref in wos_df.CR]
        # add as column to dataframe
        wos_df['cited_refs'] = cited_refs

    # normalise authors
    if wos_df.columns.tolist().count('AF') > 0:
        wos_df.au = [str(au).lower().lstrip().rstrip() for au in wos_df.AF]

    return wos_df


def clean_fields(wos_df):

    """ Returns dataframe with a new field 'field' that lists
    the fields for each reference"""

    wos_df['fields'] = wos_df.SC.dropna().str.lower().str.split('; ')
    return wos_df


def clean_topics(wos_df):
    """Wos.DE field has a mixture of topics and techniques.
    -------------------------------------
    Returns a cleaned-up, de-pluralised list version of all the topics and techniques
    """

    wos_df['topics'] = wos_df.DE.dropna().str.lower().str.strip().str.replace('\(\w+ \)', '').str.replace('($  )|(  )', ' ')
    # wos_df['topics'] = wos_df.topics.str.replace("[\(\](\w+ ?)+[\)\]]\W*",  '')
    wos_df.topics = wos_df.topics.str.replace('svm', 'support vector machine')
    wos_df.topics = wos_df.topics.str.replace('support vector machine(\w*)',
                                            'support vector machine')
    wos_df.topics = wos_df.topics.str.replace('(artificial neural network)|(neural networks)|(neural net\b)',
                                            'neural network')
    wos_df.topics = wos_df.topics.str.replace('decision tree(.*)', 'decision tree')
    wos_df.topics = wos_df.topics.str.replace('random forest(\w+)', 'random forest')
    # Web of science topics often have  a bracket expansion
    wos_df['topics'] = wos_df.topics.str.replace("(\w+)\W*[\[\(].+[\)\]]\W*", '\\1 ')
    wos_df.topics = wos_df.topics.str.split('; ')
    return wos_df

def keyword_counts(wos_df):
    """ Returns a dictionary with keyword counts"""
    de_all = [d for de in wos_df.topics.dropna() for d in de]
    key_counts = collections.Counter(de_all)
    return key_counts


    df = load_records('data/hastie_elem_WOS/')
    print('There are %s records in the dataset' % df.shape[0])
    df = clean_topics(df)
    df = clean_fields(df)

    print('%s topic fields are null' % sum(df.topics.isnull()))
    print('%s abstract fields are null' % sum(df.AB.isnull()))
    print('%s keywords Plus fields are null' % sum(df.ID.isnull()))
    print('% s author fields are null' % sum(df.AF.isnull()))

    all_fields = sorted([e for el in df.fields.dropna() for e in el])
    fields_set = set(all_fields)
    field_counts = {e: all_fields.count(e) for e in fields_set}

    print('%s different fields appear in the literature citing Hastie (2001/2009)' % len(fields_set))
    field_counts_s = sorted(field_counts.iteritems(), key=lambda(k, v):(-v, k))
    field_counts_s[0:30]
    major_fields = {f:v for f,v in field_counts.iteritems() if v > 3 or f is not 'computer science'}
    print(len(field_counts), len(major_fields))
    kw = keyword_counts(df)
    kw.most_common()[:50]
    #find the most commonly cited pages
    hastie = df.CR.str.findall('Hastie.*?;')
    hastie = hastie.map(lambda x:  x[0] if len(x) ==1 else '')
    hastie_pages = hastie[hastie.str.contains('P')]
    pages = hastie_pages.str.findall('\d{1,3};').map(lambda x: x[0] if len(x) ==1 else '').str.replace(';', '')
    pages = pages[pages != '']
    pages = pages.astype('int')
    pages_counted = pages.value_counts()
    pages_counted_sorted = pages_counted.sort_index()
    pages_counted_sorted.to_csv('data/hastie_pages.csv')

```

```{r plot_hastie_pages, results='hide', include=FALSE, cache=TRUE}
library(ggplot2)
hastie_pages = read.csv('data/hastie_pages.csv')
colnames(hastie_pages) = c('page', 'reference_count')
plt = ggplot(hastie_pages, aes(x =page, y = reference_count))
plt + geom_bar(stat = 'identity')
```

\begin{figure}
  \centering
      \includegraphics[width=0.9\textwidth]{figure/plot_hastie_pages-1.pdf}
        \caption[Pages cited from \textit{Elements of Statistical Learning }]{Pages cited from \textit{Elements of Statistical Learning} by academic publications in all fields.}
  \label{fig:hastie_pages}
\end{figure}

So we know that _Elements of Statistical Learning_ passes into many fields. But what do people read in the book? In general, the thousands of citations of the book themselves comprise a diachronic diagram of readings of the book, and their relative concentrations and sparsities suggest there may be specific sites of engagement in the techniques, approaches and machines that the book describes. Of the around 760 pages in [@Hastie_2009] and [@Hastie_2001], around `r dim(hastie_pages)[1]` distinct pages are referenced in the citing literature. As Figure \ref{ref:hastie_pages} indicates, certain portions of the book are much more heavily cited than others. This distribution of page references in the literature that cites _Elements of Statistical Learning_ is a rough guide to how the book has been read in different settings. For instance, the most commonly cited page in the book is page `r hastie_pages$page[which.max(hastie_pages$reference_count)]`. That page begins a section called 'Non-negative Matrix Factorization', \index{Non-negative matrix factorization} a technique frequently used to process digital images to compress their visual complexity into a simpler set of visual signals [@Hastie_2009, 553]. (The underlying reference here is  the highly cited [@Lee_1999]) It is particularly powerful because it, as Hastie and co-authors write, 'learns to represent faces with a set of basis images resembling parts of faces' [@Hastie_2009, 555]. (So `kittydar`, which doesn't use NMF, might do better if it did, because it could work just with parts of the images that lie somewhere near the parts of a cat's face -- its nose, its eyes, its ears.\index{`kittydar`}) \index{face recognition}

Conversely, the semiotic machinery of the book relies on vectors and orbital trajectories that converge from many different directions. The hyperobject-like aspect of the book, its somewhat forbidding citational presence in the literature, comes from the vast semiotic weave of equations, diagrams, tables, algorithms, bibliographic apparatus, and numbers wreathed in typographic luxuriance drawn from many other sources. For instance, in terms of outgoing references, _Elements of Statistical Learning_ webs together a field of scientific and technical work with data and predictive models ranging across half a century. The reference list beginning at page 699 [@Hastie_2009, 699] runs for around 35 pages, and the five hundred or so references there point in many directions. The weave of these elements differs greatly from documents in the humanities or social sciences, and, almost before anything else, prompts attention to the problem of reading parts and fragments, and relating to an object or perhaps an apparatus in the sense that Karen Barad ascribes to experimental setups [@Barad_2007]. 

```{r hastie_references, results='hide', include=FALSE, cache=TRUE, echo=FALSE}
system('pdftotext -f 717 -l 746 ../ml_lit/hastie_elem/hastie_elements_2009.pdf hastie_references.txt')
refs_text = readLines('../ml_lit/hastie_elem/hastie_references.txt')
refs_parsed = paste(refs_text, sep=' ', collapse='\n')
refs_split = unlist(str_split(refs_parsed, '\\.\n'))
refs_clean = str_trim(str_replace_all(refs_split, '\n', ' '))
years = str_replace_all(str_extract(refs_clean, '\\([[:digit:]]{4}[[:alpha:]]{0,1}\\)'), '[()abcd]', replacement='')
yearsnum = as.numeric(years)
years_df = data.frame(years=yearsnum, refs = refs_clean)
years_df = na.omit((years_df[yearsnum<2010,]))
years_df = years_df[order(years_df$years),]
colnames(years_df) = c('Year', 'Reference')
g2 = ggplot(years_df, aes(x=as.Date(as.character(Year), '%Y'))) + geom_freqpoly()
g2 + ylab('Publications cited/year') + xlab('Year of publication') 
g2 + geom_smooth()
```

\begin{figure}
  \centering
      \includegraphics[width=0.9\textwidth]{figure/hastie_references-1.pdf}
        \caption{Publications cited by \textit{Elements of Statistical Learning}. The references range over almost 80 years, with peaks in late 1970s, late 1980s, mid-1990s and mid-2000s. These peaks relate to different mixtures of cybernetics, statistics, computer science, medicine, biology and other fields running through machine learning. The smoothing of these peaks derives from use of local regression. Regression-related publications form the main body of citation. }
  \label{fig:hastie_cited_refs}
\end{figure}


As Figure \ref{fig:hastie_cited_refs} indicates, the citational fabric of _Elements of Statistical Learning_ is woven with different threads, some reaching back into early twentieth statistics, some from post-WW2 cybernetics, many from information theory and then in the 1980s onwards, increasingly, from cognitive science and computer science. While many of these references either point to Hastie, Tibshirani or Friedman's own publications, or that of their statistical colleagues, the references show that these authors are also roving quite widely in other fields and over time. _Elements of Statistical Learning_ as a text is processing, assimilating and recombining techniques, diagrams and data from many different places and times. Both the inward and outward movements of citation suggest that the book, like much in field of  machine learning, has a wave function or matrix-like character that constantly superimposes and transposes elements across boundaries and barriers. The implication here is that machine learning as a field has a highly circulatory texture, and in this respect differs somewhat from the classical understandings of scientific disciplines as bounded by communities of practice, norms and problems (as for instance, in Thomas Kuhn's account of normal science [@Kuhn_1996]. \index{Kuhn, Thomas} This aggregate or superimposed character of machine learning should definitely figure in any sense we make of it, and will inevitably affect how we phronetically relate to it. \index{phronesis}. Hence, the different waves appearing in the references cited in [@Hastie_2009] will shape discussion in later chapters in certain ways (for instance, biology is the topic of chapter \ref{ch:genome} and optimization functions of chapter \ref{ch:function}). 

## The mathematical glint of machine learning

While references from many different places go in and out of _Elements of Statistical Learning_,  they are nearly all articulated in mathematical form. The mechanics of machine learning as a level of abstraction are mathematical. But given that mathematics is itself diverse and multi-stranded, what kind of mathematics matters most here? The predictive models in _Elements of Statistical Learning_ are  dominated by a single prediction technique, linear regression models or fitting a line to points. The linear regression model is pivotal, not just in _Elements of Statistical Learning_ but in much of the scientific and engineering literature. (We have already seen something of this model in the Introduction, in the equations for linear regression \ref{eq:linear_regression}).  The linear regression model pushes up some of the citational peaks in Figure \ref{fig:hastie_cited_refs}. Even though it is an old technique dating back to Francis Galton in the 1890s (see [@Stigler_1986, chapter 8]),  drawing sharp, straight lines through opaque clouds of data-matter is what much machine learning still seeks to do. It manoeuvres in complex ways in drawing these lines (as we will see), but lines cutting, cleaving, and shattering things are very much the central working abstraction of machine learning.  \index{linear regression model} Hastie, Tibshirani and Friedman acknowledge the statistical legacy and inheritance in machine learning:

>The linear model has been a mainstay of statistics for the past 30 years and remains one of our most important tools. Given a vector of inputs
$X^T = (X_1 , X_2, . . . , X_p)$, we predict the output $Y$ via the model
>$$\hat{Y} = \hat{\beta_0}  + \sum^p_(j=1)X_j\hat{\beta_j)}$$
>The term $\hat{\beta_0}$ is the intercept, also known as the _bias_ in machine learning [@Hastie_2009, 11].

In the course of the book, linear regression is subjected to countless variations, iterations, expansions and modifications. Their own research spans several decades and a range of topics concerning linear regression models and their variations ('ridge regression'; 'least angle regression'; etc.). But this introduction of the 'mainstay of statistics,' the linear model, already introduces a harsh form -- the mathematical equation -- that is perhaps the most prominent feature in the text. Any reading of the book has to work out a way to traverse the forms show in equation \ref{eq:linear_model}. 

\begin {equation}
\label {eq:linear_model}
\hat{Y} = \hat{\beta_0}  + \sum^p_{j=1} X_j \hat{\beta_j}
\end {equation}

In its relatively compressed typographic weave, expressions such as Equation \ref{eq:linear_model} operationalize movements through data that call for some attention. These expressions, which are not comfortable reading by and large for non-technical readers, are however worth looking at carefully if we want to move 'at the same level of abstraction as the algorithm' [@Pasquinelli_2015]. They can be found in hundreds in [@Hastie_2009], but also in many other places. While their presence distinguishes machine learning from many much computer science where mathematical equations are less common, these equations also allow the book to collate and borrow from a panoply of scientific publications  and datasets in fields of statistics, artificial intelligence and computer science. Along with the citations, the graphical plots, the algorithms (implemented in code described below), these equations are integral connective tissue in machine learning. Unless we come to grips with their diagram force, without succumbing to the obscuring dazzle of mathematical abstraction, the connectivity and mobility of these forms will be lost on us. 

I find it useful here to follow Charles Sanders Peirce account of mathematics in terms of diagrams. 'Mathematical reasoning,' he writes, 'is diagrammatic' [@Peirce_1998, 206]. \index{Peirce, C.S.} \index{diagram} That it, we should see mathematics, whether it takes an algebraic or geometrical form, whether it appears in symbols, letters, lines or curves as  diagrams. Now for Peirce, a diagram is a kind of 'icon.' The icon is a sign that resembles the object it refers to: it has a relation of likeness. What likeness appears in \ref{eq:linear_model}? As Peirce says, 'many diagrams resemble their objects not all in their looks; it is only in respect to the relations of their parts that their likeness consists' [@Peirce_1998, 13]. As we will soon see, \ref{eq:linear_model} could be expressed in statements in a programming language like `Python` or `R`, or in algorithmic pseudo-code, or perhaps most accessibly, as graphic figure (a line drawn through a cloud of points). In none of these associated diagrams can the relations between the parts be observed in the same way as they in the algebraic form. The 'very idea of the art' as Peirce puts it [@Peirce_1992, 228] of algebraic expressions is that the formulae can be manipulated. The graphic form of the expression include the various classical Greek symbols such as $\sum$ or $\prod$, as well as the letters $x, y, z$ and the indices (indexical signs) that appear in subscript or superscript, as well as the spatial arrangement of all these in lines and sometimes arrays. A variety of relations run between these different symbols and spatial arrangements. For instance, in all such expressions, the difference between the left hand side of the '=' and the right hand side is very important. By convention, the left hand side of the expression is the value that is predicted or calculated (the 'response' variable) and the right hand side are the input variables or 'features' that contribute data to the model or algorithm. This spatial arrangement fundamentally affects the design of algorithms. In the case of \ref{eq:linear_model}, the '^' over $\hat{Y}$ symbolises a predicted value rather than a value that can be known completely through deduction, derivation or calculation. This distinction between predicted and actual values organizes a panoply of different practices and imperatives (for instance, to investigate the disparities between the predicted and actual values -- machine learning practitioners spend a lot of time on that problem). 

The general point is that the whole formulae is a diagram, or an icon that '*exhibits*, by means of the algebraical signs (which are not themselves icons), the relations of the quantities concerned' [@Peirce_1998, 13]. Because such diagrams suppress so many details, they allow one to focus on a more limited range of relations between parts. The manipulation of those relations generates new diagrams or patterns. This affordance of diagrammatic forms is extremely important in the intensification of machine learning. Importantly, diagrams can diagram other diagrams. Put differently, operations can be themselves the subject of operations. Or functions can themselves for functions of functions. This nesting and coiled aspect of the diagrams is highly generative since it allows what Peirce calls 'transformations' [@Peirce_1998, 212] or the construction of 'a new general predicate'[@Peirce_1992, 303].[^1.22]  The intensive processing of data today via predictive models is largely channelled via such diagrams. These diagrams are not conspicuous in the infrastructures, and they are not directly seen by people or things they impinge upon even as they have their effect.

[^1.22]: Felix Guattari makes direct use of Peirce's account of diagrams as icons of relation in his account of 'abstract machines' [@Guattari_1984]. He writes that 'diagrammaticism brings into play more or less  de-territorialized trans-semiotic forces, systems of signs, of code, of catalysts and so on, that make it possible in various specific ways to cut across stratifications of every kind' [@Guattari_1984, 145]. Here the 'trans-semiotic forces' include mathematical formulae and operations (such as the banking system of Renaissance Venice, Pisa and Genoa). They are trans-semiotic because they are not tethered by the signifying processes that code experience or  speaking positions according to given stratifications such as class, gender, nation and so forth. While Guattari (and Deleuze in turn in their co-written works [@Guattari_1988]) is strongly critical of the way which signification territorializes (we might think of cats patrolling, marking and displaying in order to maintain their territories), he is much more affirmative of diagrammatic processes. He calls them 'a-signifying' to highlight their difference from the signifying processes that order social strata. He suggests that diagrams become the foundation for 'abstract machines' and the 'simulation of physical machinic processes.' Writing in the 1960s, Guattari powerfully  anticipates the abstract machines and their associated diagrams that have taken shape and physical form in the succeeding decades. 

While I  seek to see the equations as diagrams, and will present a selection of them (nowhere near as many as found in _Elements of Statistical Learning_) in the following pages, I am not assuming their operation is transparently obvious. Just as much as a photograph, a literary work or an ethnographic observation, their semiotic movement calls for repeated consideration. Peirce advises not to begin with examples that are too simple: 'in simple cases, the essential features are often so nearly obliterated that they can only be discerned when one knows what to look for' [@Peirce_1998, 206]. He also suggests 'it is of great importance to return again and again to certain features' [@Peirce_1998, 206]. Looking at these diagrammatic expressions repeatedly is well worth it if in consequence we can diagrammatically understand something of how transformations, generalisations or intensification flow across disciplinary boundaries, across social stratifications, and sometimes, generate potentially different ways of thinking about collectives, inclusion and belonging. 

## CS229, 2007: returning again and again to certain features

If we were to follow Peirce's injunction to 'return again and again to certain features,' how would we do that?  _Elements of Statistical Learning_ is a difficult book to read. The cost of its diagrammatic density (equations, citations, tables, datasets, plots) is a certain feeling of 'not quite understanding' for many readers. This is partly because the book largely traverses finished work from several disciplines, and partly because it covers so much terrain. Professor Andrew Ng's course 'Machine Learning' CS229 at Stanford (http://cs229.stanford.edu/) might provide a supplementary path into machine learning [@Ng_2008].[^1.61] Note that Ng is a computer scientist, not a statistician. \index{Ng, Andrew} The course description runs as follows:

>This course provides a broad introduction to machine learning and statistical pattern recognition. Topics include supervised learning, unsupervised learning, learning theory, reinforcement       learning and adaptive control.   Recent applications of machine learning, such as to robotic control, data mining, autonomous navigation, bioinformatics, speech recognition, and text and       web data processing are also discussed [@Ng_2008]

CS229 is in many ways a  typical computer science pedagogical exposition of machine learning.  Machine learning expositions  usually begin with simple datasets and the simplest possible statistical models and machine learning algorithms (usually linear regression \index{linear regression}), and then, with a greater or lesser degree of attention to issues of implementation, move through a succession of increasingly sophisticated and specialised techniques.  This pattern is found in many of the how-to books, in the online courses, and in the academic textbooks, including [@Hastie_2009]. The striking difference  of  Ng's CS229 lectures from almost all other expository materials is that we see someone writing  line after line of equations using chalk on a blackboard. Occasionally, questions come from students in the audience (not shown on the Youtube videos), but mostly Ng's transcription of equations from the paper notes he holds to blackboard continues uninterrupted.[^1.23]

[^1.23]: After sitting through 20  hours of Ng's online lectures, and attempting  some of the review questions and programming exercises, including implementing well-known algorithms using `R`, one comes to know datasets such as the San Francisco house price dataset and Fisher's `iris` [@Fisher_1936] quite well. Like the textbook problems that the historian of science Thomas Kuhn long ago described as one of the anchor points in scientific cultures [@Kuhn_1996], these iconic datasets provide an entry point to the 'disciplinary matrix' of machine learning. Through them, one  gains some sense of how predictive models are constructed, and what kinds of algorithmic architectures and forms of data are preferred in machine learning. 


\begin{figure}
  \centering
      \includegraphics[width=0.9\textwidth]{figure/ng_lecture_5_generative_discriminative.pdf}
        \caption{Class notes lecture 5, Stanford CS229, 2007}
  \label{fig:class_notes}
\end{figure}

Ng's Youtube anachronistic but popular lectures have a certain diagrammatic atmosphere that comes from the many hours of chalked writing he stages during the course. In a time when PowerPoint presentations or some other electronic textuality would very much have been the norm (2007), why is a Stanford computer science professor, teaching a fairly advanced postgraduate course, writing on a chalkboard by hand? (Figure \ref{fig:class_notes} shows a brief portion of around 100 pages of notes I made on this course.) The act of writing down these equations and copying the many hand-drawn graphs Ng produced was a deliberative descriptive experiment, but more importantly an exercise in 'returning again and again' to what is perhaps overly hardened in  _Elements of Statistical Learning_.  Like the 50,000 or so other people who had watched this video, I complied with Ng's injunction to 'copy it, write it out, cover it, and see if you can reproduce it' [@NgAndrew_2008].  While it occasions much writing and drawing, and many struggles to keep up with the diagrams that Ng narrates as he writes, it seems to me that this writing of equations, with all their substitutions and derivations, alongside the graphic sketches of intuitions about the machine learning techniques, adds something that is quite hard to finesse in _Elements of Statistical Learning_. There the diagrammatic weave between the expressions of linear algebra, calculus, statistics, and the algorithms is almost too tight to work with. In Ng's CS229 lectures, by contrast, the weave is much more open. They lack the citational tapestry of [@Hastie_2009], they are not able to wield the datasets and the panoply of graphic forms found there, and virtually no machine learning code appears on  the blackboard (although the CS229 student assignments, also to be found online, are code implementations of the algorithms and models). Ng's lectures have a useful thawing effect. As we will see, only by tracking some of the movements of code that underpin the book do we have a sense of how the book is put together. 

HERE

[^1.61]: A heavily shortened version of this course has been delivered under the title 'Machine Learning' on  [Coursera.org](https://class.coursera.org/ml-003/class/index), a  MOOC (Massive Open Online Course) platform. 


One thing that helps in any navigation of these literatures are some broadly shared topic structures. The textbooks, the how-to recipe books [@Segaran_2007; @Kirk_2014; @Russell_2011; @Conway_2012] and the online university courses on machine learning have a similar topic structure. They nearly always begin with 'linear models' (fitting a line to the data), then move to logistic regression (a way of using line fitting to classify binary outcomes; for example, spam/not-spam; malignant/benign; cat/non-cat), and afterwards move to some selection of neural networks, decision trees, support vector machine and clustering algorithms. They add in some decision theory, techniques of optimization, and ways of selecting predictive models (especially the bias-variance tradeoff).[^1.62]  The topic structures start to become increasingly familiar. 

[^1.62]: They differ, however, in several important respects.  Reading the _Elements of Statistical Learning_ textbook or one of machine learning books written for programmers (for example, _Programming Collective Intelligence_ or _Machine Learning for Hackers_ [@Segaran_2007; @Conway_2012]) does not directly enrol the reader in a machine learning process. By contrast, doing a Coursera course on machine learning brings with it an ineluctable sense of being machine-learned, of oneself becoming an object of machine learning. The students on Coursera are the target of machine learning. Daphne Koller and Andrew Ng are leading researchers in the field of machine learning, but they also co-founded  the online learning site [Coursera](http://coursera.org).  As experts in machine learning, it is hard to imagine how they would not treat teaching as a learning problem. And indeed, Daphne Koller sees things this way:

    > There are some tremendous opportunities to be had from this kind of framework. The first is that it has the potential of giving us a completely unprecedented look into understanding human learning. Because the data that we can collect here is unique. You can collect every click, every homework submission, every forum post from tens of thousands of students. So you can turn the study of human learning from the hypothesis-driven mode to the data-driven mode, a transformation that, for example, has revolutionized biology. You can use these data to understand fundamental questions like, what are good learning strategies that are effective versus ones that are not? And in the context of particular courses, you can ask questions like, what are some of the misconceptions that are more common and how do we help students fix them? [@Koller_2012]

    Whether the turn from 'hypothesis-driven mode to the data-driven mode' has 'revolutionized biology' is debatable (I return to this in a later chapter). And whether or not the data generated by my participation in Coursera's courses on machine learning generates data supports understanding of fundamental questions about learning also seems an open question.  Nevertheless, the loopiness of this description interests and appeals to me. I learn about machine learning, a way for computer models to optimise their predictions on the basis of 'experience'/data, but at the same time, my learning is learned by machine learners. This is not something that could happen very easily with a printed text, although versions of it happen all the time as teachers work with students on reading texts.  While Coursera and other MOOCs promise something that mass education struggles to offer (individually profiled educational services), it also negatively highlights the possibility that machine learning in practice can, somewhat recursively, help us make sense of machine learning as it develops. 

