\chapter{Diagramming machine learning}
\label{ch:diagram}

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=12, tidy=TRUE, fig.height=8, 
                      echo=FALSE,  warning=FALSE, message=FALSE, dev='pdf')
library(RSQLite)
con <- dbConnect(RSQLite::SQLite(),'../ml_lit/all_refs.sqlite3')
library(ggplot2)
library(stringr)
library(xtable)
options(xtable.comment = FALSE)
```


> Machine learning is not magic; it cannot get something from nothing. What it does is get more from less. Programming, like all engineering, is a lot of work: we have to build everything from scratch. Learning is more like farming, which lets nature do most of the work. Farmers combine seeds with nutrients to grow crops. Learners combine knowledge with data to grow programs. [@Domingos_2012, 81] \index{Domingos, Pedro}


>Diagram: 'a functioning, abstracted from any obstacle ... or friction' [@Deleuze_1988,34] \index{Deleuze, Gilles} \index{diagram}

Many different tendencies shape what is happening to data today, but our sense of the potentials of data and calculation is closely linked to Pedro Domingos' contrast between  'a lot of [building] work' and the 'farming' done by machine learners to 'grow programs.' Although it sounds a bit banal, there is quite a lot at stake and many complications in characterising this growing work, which Domingos presents as a shift in programming practice.  In Domingo's talk of 'learners,' we might find it hard to decide what the end point is: a program or learning something? These slightly tensions between programming and something else ('growing') are worth pursuing. They help make sense of an immense Magellanic constellation of documents, software, publications, blog pages, books, spreadsheets, databases, data centre architectures,  whiteboard and blackboard diagrams, and an inordinate amount of talk and visual media orbiting around machine learning. 

The changes in calculative practice associated with machine learning work on multiple levels, ranging from infrastructures and chip architectures through to mathematical functions *and* categorisations with significant social and economic implications. This makes it difficult to identify a single or static level of abstraction. \index{abstraction|(} Abstracting as a practice takes many forms and is distributed widely. In this chapter, therefore, I attend to the tensions between different abstractive practices intersecting in machine learning. These practices include ways of writing code for data, which have shifted substantially over the last decade or so due to the growth in open source programming languages and as a part of the broader and well-known expansion of digital media cultures [@Couldry_2012]. The fact that scientists, software developers and data analysts use programming languages such as `Python` and `R` \index{Python} \index{R} just as much as commercial statistical and data software packages such as Matlab, SAS or SPSS is symptomatic of shifts in calculative culture [@Muenchen_2014]. Scientific research, which has long relied on counting and calculation, increasingly organises and processes data more comprehensively because workflows, datasets, algorithms and databases can be quickly and flexibly assembled in code. (Scientific computing languages such as FORTRAN  -- 'Formula Translator' -- have long underpinned scientific research and engineering applications in various fields [@Campbell-Kelly_2003, 34-35].\index{FORTRAN}) On the other hand, several decades of concentrated research and intensive application of statistical and machine learning algorithms in science, government, industry and commerce have gradually developed fairly powerful ways of both theorising and automating the construction of models that classify and predict events and associations between things, people, processes, etc. This means that programs are being written differently (and this is why Domingos speaks of 'growing' programs), and that they implement and aggregate new layers of abstraction. In some ways, the different modes of writing code are precisely the faceted levels of abstraction which we might need to access and traverse in order to know and come to grips empirically with contemporary compositions of power and subjectivity. \index{abstraction|)} This constellation might be better understood as a diagram rather than as an abstraction. \index{diagram}

## A diagram of the elements of machine learning

```{r elem_stat_learn_figures, cache=TRUE, echo=FALSE, messages=FALSE}
system('pdftotext -f 10 -l 716 ../ml_lit/hastie_elements_2009.pdf hastie_main.txt')
main_text = readLines('hastie_main.txt')
figure_count = length(grep(main_text, pattern='FIGURE \\d{1,2}\\.\\d{1,3}'))
table_count = length(grep(main_text, pattern='TABLE \\d{1,2}\\.\\d{1,3}'))
algorithm_count = sum(grepl(main_text, pattern='Algorithm \\d{1,2}\\.\\d{1,3}'))
equation_count = sum(grepl(main_text, pattern='\\(\\d{1,3}\\.\\d{1,3}\\)$'))
device_count = figure_count  + table_count + algorithm_count + equation_count
```

In the course of writing this book, as well as reading the academic textbooks, popular how-to books, software manuals, help documents and blog-how-to posts,  I attended graduate courses in Bayesian statistics, genomic data analysis, data mining, and missing data. Significantly, I also participated in the online machine learning courses and some machine learning competitions.[^1.4] These are all typical activities for people learning to do machine learning. Importantly, these courses, books, competitions and programming languages are widely viewed and used. Andrew Ng's \index{Ng, Andrew} machine learning course on Coursera (of which he is co-founder, along with Daphne Koller, another machine learning scientist from Stanford) \index{Koller, Daphne}, has a typical enrolment of 100,000. Youtube videos of his Stanford University lectures have cumulative viewing figures of around 500,000 [@Ng_2008]. Amidst this avalanching learning of machine learning, a  single highly cited and compendious textbook, _Elements of Statistical Learning: Data Mining, Inference, and Prediction_ [@Hastie_2009], currently in its second edition, can be seen from almost any point of the terrain.[^1.12] \index{\textit{Elements of Statistical Learning}} At least for narrative purposes, I regard this book as a major practical assemblage, a 'mechanism of statements and visibilities' [@Deleuze_1988, 51], a display of relations between forces and some unbound points of change. The authors of the book, Jeff Hastie, Rob Tibshirani and Jerome Friedman \index{Hastie, Jeff} \index{Tibshirani, Rob} \index{Friedman, Jerome} are statisticians working at Stanford and Columbia University. (Statisticians and computer scientists from Stanford University loom large in the world of machine learning, perhaps due to their proximity to Silicon Valley.)  

In terms of scientific publications, this book is a quasar. It is a  massive textual object, densely radiant with diagrams, equations, tables, algorithms,  graphs and references to other scientific literature. From the first pages proper of the book, almost every page has a figure or a table or a formal algorithm (counting these together: equations = `r equation_count`, figures = `r figure_count`; tables = `r table_count`; and algorithms = `r algorithm_count`, giving a total of `r device_count` operational devices threaded through the book). Around `r equation_count` equations rivet the text into mathematical abstractions of varying sophistication, and construct an semiotic machinery of considerable sophistication and connectivity. Importantly,  on each page of the book we are seeing, reading, puzzling over and perhaps learning from the products of code execution. The figures are all produced by code. The tables are mostly produced by code. The algorithms specify how to implement code, and the equations diagram various operations, spaces and movements that meant to run as code. 

In the range of references, it's combinations of code, diagram, equation, scientific disciplines and computational elements, and perhaps in the somewhat viscous, inter-objectively diverse referentiality that impinges on any reading of it, it betrays some hyperobject-like tendencies [@Morton_2013].  \index{hyperobject} Like the online machine learning courses and programming books I discuss below, _Elements of Statistical Learning_  combines statistical science with various algorithms to 'learn from data' [@Hastie_2009, 1]. 768 tersely written and quite mathematical pages range across various kinds of problems (identifying spam email, predicting risk of heart disease, recognising handwritten digits, etc.), and various machine learning techniques, methods and algorithms (linear regression, k-nearest neighbours, neural networks, support vector machines, the Google Page Rank algorithm, etc.). This is not the only juggernaut machine learning text.  I could just have well cleaved to Alpaydin's _Introduction to Machine Learning_ [@Alpaydin_2010]  (a more computer science-base account), Christopher Bishop's  heavily mathematical _Pattern recognition and machine learning_ [@Bishop_2006], Brian Ripley's luminously illustrated and almost coffee-table formatted _Pattern Recognition and Neural Networks_ [@Ripley_1996] \index{Ripley, Brian}, Tom Mitchell's  earlier artificial intelligence-centred _Machine learning_ [@Mitchell_1997] \index{Mitchell, Tom}, Peter Flach's  perspicuous _Machine Learning: The Art and Science of Algorithms that Make Sense of Data_ [@Flach_2012] \index{Flach, Peter}, or further afield, the sobering and laconic _Statistical Learning for Biomedical Data_ [@Malley_2011].  These and quite a few other recent machine learning textbooks display a range of emphases, ranging from the highly theoretical to the very practical, from an orientation to statistical inference to an emphasis on computational processes, from  science to commercial applications. 

[^1.12]: The complete text of the book can be downloaded from the website [http://statweb.stanford.edu/~tibs/ElemStatLearn/](http://statweb.stanford.edu/~tibs/ElemStatLearn/). At the end of short intensive course on data mining at the Centre of Postgraduate Statistics, Lancaster University, the course convenor, Brian Francis, recommended this book as the authoritative text. Some part of me rues that day. That book is a poisoned chalice; that is, something shiny, valuable but also somewhat toxic. 

How does such a book help us access levels of abstraction and their attendant tensioning in practice? While it certainly does not comprehend everything taking place in and around machine learning, it operates as a kind of diagram of several *elementary* tendencies or traits. It has a heterogeneous texture in terms of the examples, formalisms, disciplines and domains it covers. It's readership as we will see is widespread. It starkly renders the problems of making sense of mathematical operations, diagrams and transformations carried on through calculation, simulation, deduction or analysis. It practically intersects with coding practices, particularly in the form of the `R` code it heavily but somewhat latently relies on.  Such a panoply of materials and settings suggests a multi-faceted layering of abstractive practice. My primary concern in this chapter, therefore, will be to outline the different facets of machine learning that a book such as _Elements of Statistical Learning_ brings to light. 

Who reads the book? It is often cited by academic machine learning practitioners as an authoritative guide. On the other hand, students participating in new data science courses often come from different disciplinary backgrounds and find the tome unhelpful (see the comment by students during an introductory data science course documented in [@Schutt_2013]). Whether the citations are friendly or not, Google Scholar reports over 20,00 citations of the book http://scholar.google.com/scholar?hl=en&q=elements+of+statistical+machine+learning, October 2014), a huge citation count by any standards. (Michel Foucault's _The History of Sexuality: an Introduction_, one of the most highly cited book in the humanities,  receives about the same number of citations.) The citations that [@Hastie_2009] as well as its previous edition [@Hastie_2001] receive do not arrive principally from machine learning researchers. They come from a wide variety of fields. It is hard to find a field of contemporary science, engineering, natural, applied, health and indeed social science that has not cited it. A Thomson-Reuters Scientific 'Web of Science'(TM) search for references citing either the first or second edition of [@Hastie_2009] yields around 9000 results. These  publications sprawl across well over 100 different fields of research.  While computer science, mathematics and statistics dominate, a very diverse set of references comes from disciplines from archaeology, through fisheries and forestry, genetics, robotics, telecommunications and toxicology ripple out from this book since 2001. Table \ref{tab:fields_citing_hastie} shows the top 20 fields by count. One could learn something about the diagrammatic movement of machine learners  from that reference list, which itself spans biomedical, engineering, telecommunications, ecology, operations research and many other fields. While it is not surprising to see computer science, mathematics and engineering appearing at highest concentration in the literature, the molecular biology, control and automation, operation research, business and public health soon appear, suggesting something of the propagating energy of machine learning. 

```{r hastie_field, results='asis', cache=TRUE}
library(stringr)
library(xtable)
query = 'select * from basic_refs where anchor like "hastie%"'
res = dbGetQuery(con, query)
sc = res$SC
fields = str_trim(unlist(str_split(sc, ';')))
tf = as.data.frame(sort(table(fields), decreasing=TRUE)[1:20])
colnames(tf) = c('Field', 'Citations')
tab = xtable(tf, label = 'tab:fields_citing_hastie', caption = 'Subject categories of academic research literature citing \\textit{Elements of Statistical Learning} 2001-2015. These subject categories derive from Thomson-Reuter \\textit{Web of Science}.' )
print(tab, comment=FALSE, table.placement = '!htb')
```

```{r hastie_pages, engine='python', cache=TRUE, echo=FALSE}

import pandas as pd
import re
import os
import sys
import collections

def load_records(data_dir):
    """Return dataframe of all records,
    with new column of cited references as list"""

    # I saved all the WoS full records for the search term in this directory
    files = os.listdir(data_dir)
    wos_df = pd.concat([pd.read_table(wos_df, sep='\t', index_col=False)
                        for wos_df in [data_dir+f for f in files
                        if f.count('.txt') > 0]])
    wos_df = wos_df.drop_duplicates()

    #fix index
    index = range(0, wos_df.shape[0])
    wos_df.index = index

    #to get all cited refs
    if wos_df.columns.tolist().count('CR') > 0:
        cited_refs = [list(re.split(pattern='; ',
                                    string=str(ref).lower().lstrip().rstrip()))
                                    for ref in wos_df.CR]
        # add as column to dataframe
        wos_df['cited_refs'] = cited_refs

    # normalise authors
    if wos_df.columns.tolist().count('AF') > 0:
        wos_df.au = [str(au).lower().lstrip().rstrip() for au in wos_df.AF]

    return wos_df


def clean_fields(wos_df):

    """ Returns dataframe with a new field 'field' that lists
    the fields for each reference"""

    wos_df['fields'] = wos_df.SC.dropna().str.lower().str.split('; ')
    return wos_df


def clean_topics(wos_df):
    """Wos.DE field has a mixture of topics and techniques.
    -------------------------------------
    Returns a cleaned-up, de-pluralised list version of all the topics and techniques
    """

    wos_df['topics'] = wos_df.DE.dropna().str.lower().str.strip().str.replace('\(\w+ \)', '').str.replace('($  )|(  )', ' ')
    # wos_df['topics'] = wos_df.topics.str.replace("[\(\](\w+ ?)+[\)\]]\W*",  '')
    wos_df.topics = wos_df.topics.str.replace('svm', 'support vector machine')
    wos_df.topics = wos_df.topics.str.replace('support vector machine(\w*)',
                                            'support vector machine')
    wos_df.topics = wos_df.topics.str.replace('(artificial neural network)|(neural networks)|(neural net\b)',
                                            'neural network')
    wos_df.topics = wos_df.topics.str.replace('decision tree(.*)', 'decision tree')
    wos_df.topics = wos_df.topics.str.replace('random forest(\w+)', 'random forest')
    # Web of science topics often have  a bracket expansion
    wos_df['topics'] = wos_df.topics.str.replace("(\w+)\W*[\[\(].+[\)\]]\W*", '\\1 ')
    wos_df.topics = wos_df.topics.str.split('; ')
    return wos_df

def keyword_counts(wos_df):
    """ Returns a dictionary with keyword counts"""
    de_all = [d for de in wos_df.topics.dropna() for d in de]
    key_counts = collections.Counter(de_all)
    return key_counts


df = load_records('../ml_lit/data/hastie_elem_WOS/')
df = clean_topics(df)
df = clean_fields(df)


all_fields = sorted([e for el in df.fields.dropna() for e in el])
fields_set = set(all_fields)
field_counts = {e: all_fields.count(e) for e in fields_set}

field_counts_s = sorted(field_counts.iteritems(), key=lambda(k, v):(-v, k))
field_counts_s[0:30]
major_fields = {f:v for f,v in field_counts.iteritems() if v > 3 or f is not 'computer science'}
kw = keyword_counts(df)
kw.most_common()[:50]
hastie = df.CR.str.findall('Hastie.*?;')
hastie = hastie.map(lambda x:  x[0] if len(x) ==1 else '')
hastie_pages = hastie[hastie.str.contains('P')]
pages = hastie_pages.str.findall('\d{1,3};').map(lambda x: x[0] if len(x) ==1 else '').str.replace(';', '')
pages = pages[pages != '']
pages = pages.astype('int')
pages_counted = pages.value_counts()
pages_counted_sorted = pages_counted.sort_index()
pages_counted_sorted.to_csv('data/hastie_pages.csv')

```

```{r plot_hastie_pages, results='hide', include=FALSE, cache=TRUE}
library(ggplot2)
hastie_pages = read.csv('data/hastie_pages.csv')
colnames(hastie_pages) = c('page', 'reference_count')
plt = ggplot(hastie_pages, aes(x =page, y = reference_count))
plt + geom_bar(stat = 'identity')
```

\begin{figure}
  \centering
      \includegraphics[width=0.9\textwidth]{figure/plot_hastie_pages-1.pdf}
        \caption[Pages cited from \textit{Elements of Statistical Learning }]{Pages cited from \textit{Elements of Statistical Learning }  by academic publications in all fields.}
  \label{fig:hastie_pages}
\end{figure}


So we know that _Elements of Statistical Learning_ passes into many fields. But what do people read in the book? In general, the thousands of citations of the book themselves comprise a diachronic diagram of readings of the book, and their relative concentrations and sparsities suggest there may be specific sites of engagement in the techniques, approaches and machines that the book describes. Of the around 760 pages in [@Hastie_2009] and [@Hastie_2001], around `r dim(hastie_pages)[1]` distinct pages are referenced in the citing literature. As figure \ref{ref:hastie_pages} indicates, certain portions of the book are much more heavily cited than others. This distribution of page references in the literature that cites _Elements of Statistical Learning_ is a rough guide to how the book has been read in different settings. For instance, the most commonly cited page in the book is page `r hastie_pages$page[which.max(hastie_pages$reference_count)]`. That page begins a section called 'Non-negative Matrix Factorization', \index{Non-negative matrix factorization} a technique frequently used to process digital images to compress their visual complexity into a simpler set of visual signals [@Hastie_2009, 553]. (The underlying reference here is  the highly cited [@Lee_1999]) It is particularly powerful because it, as Hastie and co-authors write, 'learns to represent faces with a set of basis images resembling parts of faces' [@Hastie_2009, 555]. (So `kittydar`, which doesn't use NMF, might do better if it did, because it could work just with parts of the images that lie somewhere near the parts of a cat's face -- its nose, its eyes, its ears.\index{`kittydar`}) \index{face recognition}

Conversely, what do the authors of _Elements of Statistical Learning_ read? The semiotic machinery of the book relies on vectors and orbital trajectories that converge from many different directions. The hyperobject-like aspect of the book comes from the thick weave of equations, diagrams, tables, algorithms, bibliographic apparatus, and numbers wreathed in typographic ornaments drawn from many other sources. For instance, in terms of outgoing references or the literature that it cites, _Elements of Statistical Learning_ webs together a field of scientific and technical work with data and predictive models ranging across half a century. The reference list beginning at page 699 [@Hastie_2009, 699] runs for around 35 pages, and the five hundred or so references there point in many directions. The weave of these elements differs greatly from documents in the humanities or social sciences, and, almost before anything else, prompts attention to the problem of reading parts and fragments, and relating to an object or perhaps an apparatus in the sense that Karen Barad ascribes to experimental setups [@Barad_2007]. 

```{r hastie_references, results='hide', include=FALSE, cache=TRUE, echo=FALSE}
system('pdftotext -f 717 -l 746 ../ml_lit/hastie_elem/hastie_elements_2009.pdf hastie_references.txt')
refs_text = readLines('../ml_lit/hastie_elem/hastie_references.txt')
refs_parsed = paste(refs_text, sep=' ', collapse='\n')
refs_split = unlist(str_split(refs_parsed, '\\.\n'))
refs_clean = str_trim(str_replace_all(refs_split, '\n', ' '))
years = str_replace_all(str_extract(refs_clean, '\\([[:digit:]]{4}[[:alpha:]]{0,1}\\)'), '[()abcd]', replacement='')
yearsnum = as.numeric(years)
years_df = data.frame(years=yearsnum, refs = refs_clean)
years_df = na.omit((years_df[yearsnum<2010,]))
years_df = years_df[order(years_df$years),]
colnames(years_df) = c('Year', 'Reference')
g2 = ggplot(years_df, aes(x=as.Date(as.character(Year), '%Y'))) + geom_freqpoly()
g2 + ylab('Publications cited/year') + xlab('Year of publication') 
```

\begin{figure}
  \centering
      \includegraphics[width=0.9\textwidth]{figure/hastie_references-1.pdf}
        \caption[Publications cited by \textit{Elements of Statistical Learning}]{Publications cited by \textit{Elements of Statistical Learning}. The references range over almost 80 years, with peaks in late 1970s, late 1980s, mid-1990s and mid-2000s. These peaks relate to different mixtures of cybernetics, statistics, computer science, medicine, biology and other fields running through machine learning. The smoothing of these peaks derives from use of local regression. Regression-related publications form the main body of citation. }
  \label{fig:hastie_cited_refs}
\end{figure}


As Figure \ref{fig:hastie_cited_refs} indicates, the citational fabric of _Elements of Statistical Learning_ is woven with different threads, some reaching back into early twentieth statistics, some from post-WW2 cybernetics, many from information theory and then in the 1980s onwards, increasingly, from cognitive science and computer science. While many of these references either point to Hastie, Tibshirani or Friedman's own publications, or that of their statistical colleagues, the references show that these authors are also roving quite widely in other fields and over time. _Elements of Statistical Learning_ as a text is processing, assimilating and recombining techniques, diagrams and data from many different places and times. Both the inward and outward movements of citation suggest that the book, like much in field of  machine learning, has a wave function or matrix-like character that constantly superimposes and transposes elements across boundaries and barriers. The implication here is that machine learning as a field has a highly interwoven texture, and in this respect differs somewhat from the classical understandings of scientific disciplines as bounded by communities of practice, norms and problems (as for instance, in Thomas Kuhn's account of normal science [@Kuhn_1996]. \index{Kuhn, Thomas} This aggregate or superimposed character of machine learning should definitely figure in any sense we make of it, and will inevitably affect how we phronetically relate to it. \index{phronesis}. Hence, the different waves appearing in the references cited in [@Hastie_2009] will shape discussion in later chapters in certain ways (for instance, biology is the topic of chapter \ref{ch:genome} and optimization functions of chapter \ref{ch:function}). 

## The obdurate mathematical glint of machine learning

While references from many different places go in and out of _Elements of Statistical Learning_,  they are nearly all articulated in mathematical form. The mechanics of machine learning as a level of abstraction are mathematical, and these mechanics soon begin to operate in the vast diagram the book presents. Given that mathematics is itself diverse and multi-stranded, what kind of mathematics matters most here? The predictive models in _Elements of Statistical Learning_ are  dominated by a single prediction technique: linear regression models or fitting a line to points. \index{linear regression model|(}  The linear regression model is pivotal, not just in _Elements of Statistical Learning_ but in much of the scientific and engineering literature. (We have already seen something of this model in the Introduction, in the equations for linear regression \ref{eq:linear_regression}).  The linear regression model pushes up some of the citational peaks in Figure \ref{fig:hastie_cited_refs}. Even though it is an old technique dating back to Francis Galton in the 1890s (see [@Stigler_1986, chapter 8]),  drawing sharp, straight lines through opaque clouds of data-matter is what much machine learning still seeks to do. It manoeuvres in complex ways in drawing these lines (as we will see), but lines cutting, cleaving, and shattering things are very much the central working abstraction of machine learning.  \index{linear regression model} Hastie, Tibshirani and Friedman acknowledge the statistical legacy and inheritance in machine learning:

>The linear model has been a mainstay of statistics for the past 30 years and remains one of our most important tools. Given a vector of inputs
$X^T = (X_1 , X_2, . . . , X_p)$, we predict the output $Y$ via the model
>$$\hat{Y} = \hat{\beta_0}  + \sum^p_(j=1)X_j\hat{\beta_j)}$$
>The term $\hat{\beta_0}$ is the intercept, also known as the _bias_ in machine learning [@Hastie_2009, 11].

In the course of the book, linear regression is subjected to countless variations, iterations, expansions and modifications. Their own research spans several decades and a range of topics concerning linear regression models and their variations ('ridge regression'; 'least angle regression'; etc.). But this introduction of the 'mainstay of statistics,' the linear model, already introduces a harsh form -- the mathematical equation -- that is perhaps the most prominent feature in the text. Any reading of the book has to work out a way to traverse the forms show in equation \ref{eq:linear_model}. 

\begin {equation}
\label {eq:linear_model}
\hat{Y} = \hat{\beta_0}  + \sum^p_{j=1} X_j \hat{\beta_j}
\end {equation}

In its relatively compressed typographic weave, expressions such as Equation \ref{eq:linear_model} operationalize movements through data that call for some attention. These expressions, which are not comfortable reading by and large for non-technical readers, are however worth looking at carefully if we want to move 'at the same level of abstraction as the algorithm' [@Pasquinelli_2015]. \index{linear regression model|)}They can be found in hundreds in [@Hastie_2009], but also in many other places. While their presence distinguishes machine learning from many much computer science where mathematical equations are less common, these equations also allow the book to collate and borrow from a panoply of scientific publications  and datasets in fields of statistics, artificial intelligence and computer science. Along with the citations, the graphical plots, the algorithms (implemented in code described below), these equations are integral connective tissue in machine learning. Unless we come to grips with their diagram force, without succumbing to the obscuring dazzle of mathematical abstraction, the connectivity and mobility of these forms will be lost on us. 

I find it useful here to follow Charles Sanders Peirce account of mathematics in terms of diagrams. 'Mathematical reasoning,' he writes, 'is diagrammatic' [@Peirce_1998, 206]. \index{Peirce, C.S.} \index{diagram|(} That it, we should see mathematics, whether it takes an algebraic or geometrical form, whether it appears in symbols, letters, lines or curves as  diagrams. Now for Peirce, a diagram is a kind of 'icon.' \index{diagram!icon} The icon is a sign that resembles the object it refers to: it has a relation of likeness. What likeness appears in \ref{eq:linear_model}? As Peirce says, 'many diagrams resemble their objects not all in their looks; it is only in respect to the relations of their parts that their likeness consists' [@Peirce_1998, 13]. As we will soon see, \ref{eq:linear_model} could be expressed in statements in a programming language like `Python` or `R`, or in algorithmic pseudo-code, or perhaps most accessibly, as graphic figure (a line drawn through a cloud of points). In none of these associated diagrams can the relations between the parts be observed in the same way as they in the algebraic form. The 'very idea of the art' as Peirce puts it [@Peirce_1992, 228] of algebraic expressions is that the formulae can be manipulated. The graphic form of the expression include the various classical Greek symbols such as $\sum$ or $\prod$, as well as the letters $x, y, z$ and the indices (indexical signs) \index{diagram!indexical sign} that appear in subscript or superscript, as well as the spatial arrangement of all these in lines and sometimes arrays. A variety of relations run between these different symbols and spatial arrangements. For instance, in all such expressions, the difference between the left hand side of the '=' and the right hand side is very important. By convention, the left hand side of the expression is the value that is predicted or calculated (the 'response' variable) and the right hand side are the input variables or 'features' that contribute data to the model or algorithm. This spatial arrangement fundamentally affects the design of algorithms. In the case of \ref{eq:linear_model}, the '^' over $\hat{Y}$ symbolises a predicted value rather than a value that can be known completely through deduction, derivation or calculation. This distinction between predicted and actual values organizes a panoply of different practices and imperatives (for instance, to investigate the disparities between the predicted and actual values -- machine learning practitioners spend a lot of time on that problem). 

The general point is that the whole formulae is a diagram, or an icon that '*exhibits*, by means of the algebraical signs (which are not themselves icons), the relations of the quantities concerned' [@Peirce_1998, 13]. Because such diagrams suppress so many details, they allow one to focus on a more limited range of relations between parts. The manipulation of those relations generates new diagrams or patterns. This affordance of diagrammatic forms is extremely important in the intensification of machine learning. Importantly, diagrams can diagram other diagrams. Put differently, operations can be themselves the subject of operations. Or functions can themselves for functions of functions. This nesting and coiled aspect of the diagrams is highly generative since it allows what Peirce calls 'transformations' [@Peirce_1998, 212] or the construction of 'a new general predicate'[@Peirce_1992, 303].[^1.22]  The intensive processing of data today via predictive models is largely channelled via such diagrams. These diagrams are not conspicuous in the infrastructures, and they are not directly seen by people or things they impinge upon even as they have their effect.\index{diagram|)}

[^1.22]: Felix Guattari makes direct use of Peirce's account of diagrams as icons of relation in his account of 'abstract machines' [@Guattari_1984]. He writes that 'diagrammaticism brings into play more or less  de-territorialized trans-semiotic forces, systems of signs, of code, of catalysts and so on, that make it possible in various specific ways to cut across stratifications of every kind' [@Guattari_1984, 145]. Here the 'trans-semiotic forces' include mathematical formulae and operations (such as the banking system of Renaissance Venice, Pisa and Genoa). They are trans-semiotic because they are not tethered by the signifying processes that code experience or  speaking positions according to given stratifications such as class, gender, nation and so forth. While Guattari (and Deleuze in turn in their co-written works [@Guattari_1988]) is strongly critical of the way which signification territorializes (we might think of cats patrolling, marking and displaying in order to maintain their territories), he is much more affirmative of diagrammatic processes. He calls them 'a-signifying' to highlight their difference from the signifying processes that order social strata. He suggests that diagrams become the foundation for 'abstract machines' and the 'simulation of physical machinic processes.' Writing in the 1960s, Guattari powerfully  anticipates the abstract machines and their associated diagrams that have taken shape and physical form in the succeeding decades. 

While I  seek to relate to the equations as diagrams, and will present a selection of them (nowhere near as many as found in _Elements of Statistical Learning_) in the following pages, I am not assuming their operation is transparently obvious. Just as much as the analysis of a photograph, a literary work or an ethnographic observation, their semiotic movement calls for repeated consideration. Peirce advises not to begin with examples that are too simple: 'in simple cases, the essential features are often so nearly obliterated that they can only be discerned when one knows what to look for' [@Peirce_1998, 206]. He also suggests 'it is of great importance to return again and again to certain features' [@Peirce_1998, 206]. Looking at these diagrammatic expressions repeatedly is well worth it if in consequence we can diagrammatically understand something of how transformations, generalisations or intensification flow across disciplinary boundaries, across social stratifications, and sometimes, generate potentially different ways of thinking about collectives, inclusion and belonging. 

## CS229, 2007: returning again and again to certain features

If we were to follow Peirce's injunction to 'return again and again to certain features,' how would we do that?  _Elements of Statistical Learning_ is a difficult book to read in isolation (although it does repay re-reading). The cost of the diagrammatic density of its 'elements' (equations, citations, tables, datasets, plots) is a certain refractory feeling of 'not quite understanding' for many readers. This feeling is inevitable because the book condenses finished work from several disciplines, and partly because it seeks to frame a great diversity of materials *statistically.* (The statistical aspects of machine learning are the main topic of chapter \ref{ch:probability}).  Professor Andrew Ng's course 'Machine Learning' CS229 at Stanford (http://cs229.stanford.edu/) might provide a supplementary path into machine learning [@Ng_2008].[^1.61] Note that Ng is a computer scientist, not a statistician. \index{Ng, Andrew} The course description runs as follows:

>This course provides a broad introduction to machine learning and statistical pattern recognition. Topics include supervised learning, unsupervised learning, learning theory, reinforcement learning and adaptive control.   Recent applications of machine learning, such as to robotic control, data mining, autonomous navigation, bioinformatics, speech recognition, and text and web data processing are also discussed [@Ng_2008]

CS229 is in many ways a  typical computer science pedagogical exposition of machine learning.  Machine learning expositions  usually begin with simple datasets and the simplest possible statistical models and machine learning algorithms (usually linear regression \index{linear regression}), and then, with a greater or lesser degree of attention to issues of implementation, move through a succession of increasingly sophisticated and specialised techniques.  This pattern is found in many of the how-to books, in the online courses, and in the academic textbooks, including [@Hastie_2009]. The striking difference  of  Ng's CS229 lectures from almost all other expository materials is that we see someone writing  line after line of equations using chalk on a blackboard. Occasionally, questions come from students in the audience (not shown on the Youtube videos), but mostly Ng's transcription of equations from the paper notes he holds to blackboard continues uninterrupted.[^1.23]

[^1.23]: After sitting through 20  hours of Ng's online lectures, and attempting  some of the review questions and programming exercises, including implementing well-known algorithms using `R`, one comes to know datasets such as the San Francisco house price dataset and Fisher's `iris` [@Fisher_1936] quite well. Like the textbook problems that the historian of science Thomas Kuhn long ago described as one of the anchor points in scientific cultures [@Kuhn_1996], these iconic datasets provide an entry point to the 'disciplinary matrix' of machine learning. Through them, one  gains some sense of how predictive models are constructed, and what kinds of algorithmic architectures and forms of data are preferred in machine learning. 


\begin{figure}
  \centering
      \includegraphics[width=0.9\textwidth]{figure/ng_lecture_5_generative_discriminative.pdf}
        \caption{Class notes lecture 5, Stanford CS229, 2007}
  \label{fig:class_notes}
\end{figure}

Ng's Youtube anachronistic but popular lectures have a certain diagrammatic atmosphere that comes from the many hours of chalked writing he stages during the course. In a time when PowerPoint presentations or some other electronic textuality would very much have been the norm (2007), why is a Stanford computer science professor, teaching a fairly advanced postgraduate course, writing on a chalkboard by hand? (Figure \ref{fig:class_notes} shows a brief portion of around 100 pages of notes I made on this course.) The act of writing down these equations and copying the many hand-drawn graphs Ng produced was a deliberative descriptive experiment, but more importantly an exercise in 'returning again and again' to what is perhaps overly hardened in  _Elements of Statistical Learning_.  Like the 50,000 or so other people who had watched this video, I complied with Ng's injunction to 'copy it, write it out, cover it, and see if you can reproduce it' [@NgAndrew_2008].  While it occasions much writing and drawing, and many struggles to keep up with the diagrams that Ng narrates as he writes, it seems to me that this writing of equations, with all their substitutions and variations, alongside the graphic sketches of intuitions about the machine learning techniques, adds something of the *derivations* that is quite hard to finesse in _Elements of Statistical Learning_. There the diagrammatic weave between the expressions of linear algebra, calculus, statistics, and the algorithms is almost too tight to work with. In Ng's CS229 lectures, by contrast, the weaving, while still complex, is much more open. They lack the citational tapestry of [@Hastie_2009], they are not able to wield the datasets and the panoply of graphic forms found there, and virtually no machine learning code appears on  the blackboard (although the CS229 student assignments, also to be found online, are code implementations of the algorithms and models), but Ng's lectures have a useful thawing effect. As we will see, only by tracking some of the movements of code that underpin machine learning do we identify levels of abstraction that we might want to think about. 

Some broadly shared topic structures help in any navigation of these pedagogical literatures. The textbooks, the how-to recipe books [@Segaran_2007; @Kirk_2014; @Russell_2011; @Conway_2012] and the online university courses on machine learning often have a similar topic structure. They nearly always begin with 'linear models' (fitting a line to the data), then move to logistic regression (a way of using the linear model to classify binary outcomes; for example, spam/not-spam; malignant/benign; cat/non-cat), and afterwards move to some selection of neural networks, decision trees, support vector machine and clustering algorithms. \index{linear regression} They add in some decision theory, techniques of optimization, and ways of selecting predictive models (especially the bias-variance tradeoff).[^1.62]  The topic structures have in recent years started to become increasingly uniform. This coagulation around certain topics,  problems and mathematical formalisms is both something worth analyzing (since, for instance, it definitely affects how machine learning is taken up in different settings), but should not be taken as obvious or given. In this respect,  Ng's step-by-step handcrafted derivations are too derivative to really track the trans-semiotic intensities of equations such as equation \ref{eq:linear_model}. How would one learn to read such diagrams less linearly (Ng's long columns of algebraic derivation),  more diagonally and in better alignment with the multi-faceted abstraction they often compact?

[^1.61]: A heavily shortened version of this course has been delivered under the title 'Machine Learning' on  [Coursera.org](https://class.coursera.org/ml-003/class/index), a  MOOC (Massive Open Online Course) platform. 

[^1.62]: They differ, however, in several important respects.  Reading the _Elements of Statistical Learning_ textbook or one of machine learning books written for programmers (for example, _Programming Collective Intelligence_ or _Machine Learning for Hackers_ [@Segaran_2007; @Conway_2012]) does not directly subject the reader to machine learning By contrast, doing a Coursera course on machine learning brings with it an ineluctable sense of being machine-learned, of oneself becoming an object of machine learning. The students on Coursera are the target of machine learning. Daphne Koller and Andrew Ng are leading researchers in the field of machine learning, but they also co-founded  the online learning site [Coursera](http://coursera.org). \index{Coursera}  As experts in machine learning, it is hard to imagine how they would not treat teaching as a learning problem. And indeed, Daphne Koller sees things this way:

    > There are some tremendous opportunities to be had from this kind of framework. The first is that it has the potential of giving us a completely unprecedented look into understanding human learning. Because the data that we can collect here is unique. You can collect every click, every homework submission, every forum post from tens of thousands of students. So you can turn the study of human learning from the hypothesis-driven mode to the data-driven mode, a transformation that, for example, has revolutionized biology. You can use these data to understand fundamental questions like, what are good learning strategies that are effective versus ones that are not? And in the context of particular courses, you can ask questions like, what are some of the misconceptions that are more common and how do we help students fix them? [@Koller_2012] \index{Koller, Daphne}

    Whether the turn from 'hypothesis-driven mode to the data-driven mode' has 'revolutionized biology' is debatable (I return to this in a later chapter). And whether or not the data generated by my participation in Coursera's courses on machine learning generates data supports understanding of fundamental questions about learning also seems an open question.  Nevertheless, the loopiness of this description interests and appeals to me. I learn about machine learning, a way for computer models to optimise their predictions on the basis of 'experience'/data, but at the same time, my learning is learned by machine learners. This is not something that could happen very easily with a printed text, although versions of it happen all the time as teachers work with students on reading texts.  While Coursera and other MOOCs promise something that mass education struggles to offer (individually profiled educational services), it also negatively highlights the possibility that machine learning in practice can, somewhat recursively, help us make sense of machine learning as it develops. 


## The learning of machine learning

For a book with 'learning' in its title, _Elements of Statistical Learning_ has only a little to say explicitly about how to learn machine learning.  \index{learning} 'Learning' is briefly discussed on the first page of _Elements of Statistical Learning_, but the book hardly ever returns to the topic or even that term explicitly. We learn on page 2 that a 'learner' in machine learning is a model that predicts outcomes.  (As I discuss in  chapter \ref{ch:function}, learning is comprehensively understood in machine learning as finding a mathematical _function_ that could have generated the data, and optimising the search for that function as much as possible.) The notion of learning in machine learning derives from the field of artificial intelligence. The broad project of artificial intelligence, at least as envisaged in its 1960s-1970s heyday as a form of symbolic reasoning, is today largely regarded as a deadend. There is no general, or comprehensive artificial intelligence in existence, even though many efforts continue to develop 'deep learners' (see for instance, Google Corporation's ongoing research into 'deep learning' of its own data; and chapter \ref{ch:subjects}). If a general intelligence did not appear, certainly a lot of learning was undertaken. The field of machine learning might be seen as one such offshoot since it paused the process of machines becoming intelligences at a developmental phase that entails much close supervision, monitoring and oversight. 

The so-called 'learning problem' and the theory of learning machines developed largely by researchers in the 1960-1970s was largely based on work already done in the 1950s on cybernetic devices such as the perceptron, the  prototypical neural network model developed by the psychologist Frank Rosenblatt in the 1950s [@Rosenblatt_1958]. \index{Rosenblatt, Frank} \index{perceptron|(}  Drawing on the McCulloch-Pitts model of the neurone, Rosenblatt implemented the perceptron, which today would be called a single-layer neural network [@Hastie_2009, 393] on a computer at the Cornell University Aeronautical Laboratory in 1957. For present purposes, it is interesting to see what diagrams Rosenblatt used and how they differ from contemporary diagrams.[^1.40] What has been *learned* in the intervening period?

[^1.40]: Elizabeth Wilson's study, _Affect and Artificial Intelligence_ [@Wilson_2010], draws on a combination of psychoanalytic, psychological and archival materials discussing the work of key figures in the early history of artificial intelligence such as Alan Turing on intelligent machinery,  Warren McCulloch and Walter Pitts on neural nets, and recent examples of affective computing and robots such as the MIT robot Kismet. Her framing of the psychic nexus with machines such as the perceptron is provocative:
    
    > Sometimes machines are the very means by which we can stay alive psychically, and they can  just as readily be a means for affective expansion and amplification as for affective attenuation. This is especially the case of computational machines [@Wilson_2010, 30].

    Under what conditions do machines and for present purposes, computational machines, become 'the very means we can stay alive psychically'? Wilson  addresses this question by positing 'some kind of intrinsic affinity, some kind of intuitive alliance between the machinic and the affective, between calculation and feeling' (31), and suggesting that the 'one of the most important challenges will be to operationalize affectivity in ways that facilitate pathways of introjection between humans and machines' (31). Introjection, the process of bringing the world within self is, according to psychoanalytic accounts of subjectivity, crucial to the formation of 'a stable subject position' (25). Wilson envisages introjection of machine processes as a good, not as a failure or attenuation of relation to the world. Note that her use of introjection differs strikingly from the sense of introjection invoked by Paolo Virno in his account of contemporary production, but both Wilson and Virno retain a commitment to the primacy of psychic processes in the human-machine nexus. While I tend to go in the same direction as Wilson in relation to affective expansion, I don't tend to see that expansion as unfolding from introjection, but rather from an intensification of diagrammatic processes.

\begin{figure}
  \centering
  \begin{subfigure}[b]{0.4\textwidth}
      \includegraphics[width=\textwidth]{figure/perceptron_rosenblatt_1958_389.pdf}
        \caption{The neurological perceptron (Rosenblatt, 1958, 389)}
          \label{fig:perceptron_1958}
\end{subfigure}
  \begin{subfigure}[b]{0.3\textwidth}
      \includegraphics[width=\textwidth]{figure/hastie_ann_2009_393.pdf}
        \caption[Neural network from Hastie (2009)]{Neural network from Hastie (2009); Single layer feedforward artificial neural network.}
  \label{fig:hastie_ann}
    \end{subfigure}
    \label{figure:perceptron_ann}
    \caption{1958 perceptron and 2001 neural net compared}
\end{figure}

If we compare the diagram of Rosenblatt's perceptron shown in Figure \ref{fig:perceptron_1958}  to the typical contemporary diagram of a neural network shown in Figure \ref{fig:hastie_ann}, the differences are not that great in many ways. The diagram of a neural network found in Rosenblatt's paper [@Rosenblatt_1958]  has no mathematical symbols on it, but the one from [@Hastie_2009] does. Rosenblatt's retains neuro-cognitive-anatomical reference points (retina, association area, projection area) whereas Hastie et. al.'s replaces them with the symbols that we have already seen in play in the expression for the linear model \ref{eq:linear_model}. What has happened between the two diagrams? As Vladimir Vapnik, a leading machine learning theorist, observes: 'the perceptron was constructed to solve pattern recognition problems; in the simplest case this is the problem of constructing a rule for separating data of two different categories using given examples' [@Vapnik_1999, 2]. \index{Vapnik, Vladimir} While computer scientists in artificial intelligence of the time, such as Marvin Minsky and Seymour Papert, were sceptical about the capacity of the perceptron model to distinguish  or 'learn' different patterns [@Minsky_1969], later work showed that perceptrons could 'learn universally'[^1.19].\index{perceptron|} For present purposes, the key point is not that neural networks have turned out several decades later to be extremely powerful algorithms in learning to distinguish patterns, and that intense research in neural networks has led to their ongoing development and increasing sophistication in many 'real world' applications (see for instance, for their use in sciences [@Hinton_2006], or in commercial applications such as drug prediction [@Dahl_2013] and above all in the current surge of interest in 'deep learning' by social media platform and search engines such as Facebook and Google). \index{neural net} For our purposes, the important point is that the notion of the learning machine began to establish an ongoing diagonalization that transforms basic diagrammatic pattern through substitutions of increasingly convoluted or nested operations. The whole claim that machines 'learn' rests on this diagrammatization that recurrently and sometimes recursively transforms the icon of relations, sometimes in the graphic forms shown above and more often in the algebraic patterns.\index{diagrammatization} 

[^1.19]: In describing the entwined elements of machine learning techniques, and citing various machine learning textbooks, I'm not attempting to provide any detailed history of their development. My account of these developments does not explore the archives of institutions, laboratories or companies where these techniques took shape. It derives much more either from following citations back out of the highly distilled textbooks into the teeming collective labour of research on machine learning as published in hundreds of thousands of articles in science and engineering journals, or from looking at,  experimenting with and implementing techniques in code. For instance,  [@Olazaran_1996] offers a history of the perceptron controversy from a science studies perspective. During the 1980s, artificial intelligence and associated approaches (expert systems, automated decision support, neural networks, etc.) were a matter of some debate in the sociology and anthropology of science. The work of Harry Collins would be one example of this [@Collins_1990], of Paul Edwards on artificial intelligence and Cold War [@Edwards_1996], or Nathan Ensmenger on chess [@Ensmenger_2012],  and the work of Lucy Suchman on plans and expert systems [@Suchman_1987] would others. Philosophers such as Hubert Dreyfus (_What Computers Can't Do_ [@Dreyfus_1972; @Dreyfus_1992] had already extensively criticised with AI.  In the 1990s, the appearance of new forms of simulation,  computational fields such as a-life and new forms of robotics such as Rodney Brook's much more insect-like robots at MIT attracted the interest of social scientists [@Helmreich_2000] amongst many others. Sometimes this interest was critical of claims to expertise (Collins), and at other times, interested in how to make sense of the claims without foreclosing their potential novelty (Helmreich). By and large, I don't attend to controversies in machine learning as a scientific field, and I don't directly contest the epistemic authority of the proponents of the techniques. I share with Lucy Suchman an interest in how the 'effect of machine-as-agent is generated'  and in how 'translations  ... render former objects as emergent subjects'[@Suchman_2006,2]. I diverge around the site of empirical attention. I'm persevering with the diagrams in each of the following chapters in order to track the movement of tendencies that are not so visible in terms of either the controversies or the assumptions of agency embodied in many AI systems of the 1980s. It's certainly not that I think these approaches are wrong, only that they don't access the diagrammatic movements of the operations we are discussing across different settings. The agency of machine learning, in short, might not reside so much in any putative predictive or classificatory power if manifests, but rather its capacity to mutate and migrate across contexts. 

##What the perceptron latches onto

I am suggesting, then, that we should follow the transformations associated with machine learning diagrammatically, provided we maintain a rich understanding of diagram, and remain open to multiple levels of abstraction. \index{diagram|(}Following Peirce, we might begin to see machine learning as a diagrammatic practice in which different semiotic forms -- lines, numbers, symbols, operators, patches of colour, words, images, marks such as dots, crosses, ticks and arrowheads --  are constantly connected, substituted, embedded or created from existing diagrams. The diagrams we have already seen from _Elements of Statistical Learning_ - algebraic formulae and network topology - don't exhaust the variations at all. Just a brief glance through this book or almost any other in the field shows not only many formulae, but tables, matrices, arrays, line graphs, contour plots, scatter plots,  dendrograms and trees, as well as algorithms expressed as pseudo-code. The connections between these diagrams are not always very tight or close. Learning to machine learning (whether you are a human learner or a learner in the sense of a machine) means dancing between diagrams. This dance is relatively silent and sometimes almost motionless as signs slide between different diagrammatic articulations. Diagrammatization offers then a way to track the ongoing project which tries treat data like farmers treat crops (see epigraph from Domingos in this chapter).  To understand what machines can learn, we need to look at how they have been drawn, designed, or formalised. But what in this work of designing and formalising predictive models is like farming? Some very divergent trajectories open up here. On the one hand, the diagrams become machines when they are implemented. On the other hand, the machines generate new diagrams when they function. We need to countenance both forms of movement in order to understand any of the preceding diagrams -- the algebraic expressions or the diagrams of models such as the perceptron or its descendants, the neural network.  This means going downstream from the textbooks into actual implementations and places where people, algorithms, and machines mingle more than they do in the relatively neat formality of the textbooks. Rather than history or controversies in the field, I focus on the migratory patterns of  methods, and the many configurational shifts associated with their implementations as the same things appears in different places. \index{diagram|)} 

One step in this diagrammatic dance moves from what we might called symbolic logical diagrams to statistical algorithmic diagrams. While many of the machine learning techniques I discuss have quite long statistical lineages (runnning back to the 1900s in the case of Karl Pearson's development of the still-heavily used Principal Component Analysis [@Pearson_1901]) \index{Pearson, Karl}, machine learning techniques also owe much to a certain dissatisfaction with the classical AI understanding of intelligence as manipulation of symbols. Symbolic intelligence, epitomised by deductive logic or predicate calculus, was very much at the centre of many AI projects during the 1950s and 1960s [@Dreyfus_1972; @Edwards_1996]. \index{artificial intelligence} In this line of diagrammatization, certain privileged symbolic-cognitive forms are subject to a kind of statistical transformation. Take for instance once of the most common operations of the Boolean logical calculus, the NOT-AND or NAND function shown in table \ref{tab:boolean}. 

X1  X2  X3  Y
--  --  --  --
0   0   0   1
0   0   1   1
0   1   1   1
1   0   0   1
1   0   1   1
1   1   0   1
1   1   1   0

Table: (the Boolean function  NOT-AND truth table) \label{tab:boolean}

For present purposes, these kinds of logical and symbolic diagrams have triple relevance. First, the spatial arrangement of the table is fundamental to not only machine learning and most contemporary data practice. (Transformations in tables are the main topic of chapter \ref{ch:vector}.) That spatial form is diagrammatic in various ways, but principally through the horizontal and vertical relationships it installs. Most datasets come as tables, or end up as tables at some point in their analysis. Second, the elements or cells of this table are numbers. These numbers, $1$ and $0$ are the binary digits as well as the 'truth' values 'True' and 'False' in classical logic. The truth table for NOT-AND is in some ways typical of data tables because it contain numbers.  These numbers are readable as symbolic logical propositions. Hence this table acts as a hinge between numbers and symbolic thought or cognition. (In [@Hastie_2009] most tables contain data in the forms of numbers rather than truth values.) Third,  the NAND  table in particular has an obvious operational importance in digital logic, since digital circuits of all kinds -- memory, processing, and communication -- comprise such logical functions knitted together in the intricate  fabrics of contemporary calculation.[^1.30]   

[^1.30]: Peirce himself had first shown that combinations of NAND operations could stand in for any logical expression whatsoever, thus paving the way for the diagrammatic weave of contemporary digital memory and computation in all their permutations. Today, NAND-based logic is norm in digital electronics. NOR -- NOT OR -- logic is also used in certain applications.  

The obviousness of the logical operation shown in the table stands out. It is hardly surprising to us at all. This looks like the kind of mechanistic calculation that computers do. How, by contrast, could such a truth table be learned by a machine, even a machine whose _modus operandi_ and indeed whose very fabric is nothing other than a massive mosaic of such operations inscribed in transistor circuits? Machine learning of such truth tables is not a typical operation today, but does illustrate something of the diagrammatic transformations that classical AI has undergone \index{artificial intelligence}. If we return to the perceptron, with its neural intuition, how could it learn this kind of logic? It knows nothing of the logical calculus, but it can be induced to take on a logical shape by training it. For instance, a  perceptron that 'learns' the binary logical operation NAND (Not-AND) is  expressed in twenty lines of python code on the Wikipedia 'Perceptron' page [@Wikipedia_2013]. The code is followed by the output  -- a series of numbers -- produced when it runs. 

```{r perceptron, echo=TRUE, cache=FALSE, message=FALSE, warning=FALSE, comment=NA, size='smallsize', results='markup', engine='python' }

threshold = 0.5
learning_rate = 0.1
weights = [0, 0, 0]
training_set = [((1, 0, 0), 1), ((1, 0, 1), 1), ((1, 1, 0), 1), ((1, 1, 1), 0)]
outfile = open('./perceptron_weights.txt', 'w+')

def dot_product(values):
    return sum(value * weight for value, weight in zip(values, weights))

out = 'weight1, weight2, weight3, error_count, iteration'
print >>outfile, out
iteration_count = 0
while True:
    error_count = 0
    iteration_count += 1
    for input_vector, desired_output in training_set:
        out = ','.join(map(str,weights)) + ',' + str(error_count)
        out = out + ',' + str(iteration_count)
        result = dot_product(input_vector) > threshold
        error = desired_output - result
        print >>outfile, out
        if error != 0:
            error_count += 1
            for index, value in enumerate(input_vector):
                weights[index] += learning_rate * error * value
    if error_count == 0:
        break

outfile.close()

```

```{r nand_weights, results='asis', include=TRUE}
library(xtable)
weights = read.csv('perceptron_weights.txt')
tab = xtable(weights, label='tab:nand_weights', caption= 'Iterative change in weights as a perceptron learns the NAND function' )
print(tab, 'latex', table.placement = '!htb')
```

What does this code show or say? First of all, we should note the relative conciseness of the code vignette. In citing this code, I'm not resorting to a technical publication or scientific literature as such, or even to a software library or package, just to that popular yet incredibly heavily used epistemic resource, the Wikipedia page, and the relatively generic and widely used programming language `python`.[^1.31] Whatever the levels of abstraction associated with machine learning, it is hardly ever hermetically opaque. \index{perceptron|(} While perceptrons and neural networks are the topic of later chapters, the typical features of this code as the implementation of a machine learning algorithm are the presence of the 'learning rate', a 'training_set,' 'weights', an 'error count', and a loop function that  multiplies values ('dot_product'). Some of the names such as `learning_rate` or `error_count`  present in the code bear the marks of the theory of learning machines that we will discuss. Much of the code here is much more familiar, generic programming. It defines variables, sets up data structures (lists of numerical values), checks conditions, loops through statements or prints results. Executing this code (by copying and pasting it into a python terminal, for instance) produces several dozen lines of numbers that are initially different to each other, but that gradually converge on the same values (see printout). These numbers are the 'weights' of the nodes of the perceptron as it iteratively learns to recognise patterns in the input values. None of the workings of the perceptron need concern us at the moment. It is a typical machine learning algorithm, almost always included in machine learning textbooks and usually taught in introductory machine learning classes.  Perhaps more strikingly than its persistence as an algorithm over half a century, the implementation of the whole algorithm in twenty lines of code on a Wikipedia page suggests the mobility of these methods. What required the resources of a major university research engineering laboratory in the 1950s can be re-implemented by a cut and paste operation between Wikipedia pages and a terminal window on a laptop in 2014. This is now a familiar observation, and perhaps not very striking at all. Again, what runs across all of these observations are the numbers that the algorithm produces. The NAND truth table has been re-drawn as a list of tuples or sets (see line 4 of the code that defines the variable `training_set`). The perceptron has learned the truth table by being given it as a set of training examples, and then adjusting its internal model -- the weights that are printed during each loop of the model as the output --  repeatedly until the model is producing the correct values of the truth table. The algorithm exits it main loops (`while True:`)  when there are no errors. The effect here is recursive: a model or algorithm implemented in digital logic has learned a basic rule of digital logic at least approximately. This transformation in learning style is symptomatic of the broader transformations that  machine learning latches onto. The learning done in machine learning has few cognitive or symbolic underpinnings. It differs from classical AI in that it takes existing symbolic and, increasingly, signifying processes (such as the cat faces that `kittydar` tries find), and latch onto them diagrammatically. The diagrammatic dance between different algebraic, geometrical, algorithmic and tabular forms of expression is where the learning occurs. \index{perceptron|)} \index{learning}

[^1.31]: There is much to discuss about programming and code in machine learning. I  return to that below. The important point for present purposes is that this is pretty much vanilla standard stand alone `python` code. There are no esoteric libraries or dependencies here. As is often the case with  machine learning, the abstract machines are quite simple, but their potential relationality is complex. 

At the same time, the perceptron algorithm produces numbers  - $0.79999$, $2.0$ -- as weights. These numbers display no direct correspondence with the symbolic categories of boolean True and False or the binary digits $1$ and $0$. There may be a relation but it is not obvious at first glance. The problem of mapping these calculated numbers -- and they truly abound in machine learning -- triggers many different diagrammatic movements in _Elements of Statistical Learning_ and like-minded texts (and these different movements will be discussed in chapters \ref{ch:probability} and \ref{ch:pattern}). These numbers engender much statistical ratiocination (see chapter \ref{ch:probability}), but here we need only note the distance between symbolically organised diagrams like the NAND truth table of Table \ref{tab:boolean}  and the diagrams of a machinic configuration printed as weights in table \ref{tab:nand_weights}.  

## Machine learning implemented as program

Pedro Domingos refers to change from building programs to growing knowledge (see the epigraph to this chapter).\index{Domingos, Pedro} Does this contrast help make sense of the perceptron's operations or _Elements of Statistical Learning_  more generally? The final part of my reading protocol for multi-faceted abstractions in machine learning  entails, I am suggesting, tracking the transformations between table \ref{tab:boolean} and table \ref{tab:nand_weights} in and by program code. What would we learn by studying and implementing machine learners as programs rather than their history or the controversies associated with them? It is not the case that code offers privileged access to abstraction. But it is highly diagrammatic and in sometimes latent ways, affectively partipatory. That is, it expresses the multi-faceted abstraction of machine learning and allows many connections running across facets to be followed. \index{machine learners!programs} Science studies scholars such as Anne-Marie Mol have urged the need to keep practice together with theories of what exists. Towards the beginning of _The Body Multiple: Ontology in Medical Practice_ [@Mol_2003], Mol writes:

>If it is not removed from the practices that sustain it, reality is multiple. This may be read as a description that beautifully fits the facts. But attending to the multiplicity of reality is also an act. It is something that may be done – or left undone [@Mol_2003, 6] \index{Mol, Anne-Marie}

Mol's work offers a cogent case for developing accounts of what is real steeped in the practices that make it real. While similar sounding affirmations of the underpinning role of practice can be found in many parts of social sciences and humanities (since who would not affirm the centrality of practice?), Mol's insistence on this nexus of practice or doing and the existence of things in their plurality offers another way forward in reading _Elements of Statistical Learning_. \index{practice} Tracking techniques -- a more stabilised form of practice -- and the flow of their implementations is a way of keeping abstractions and calculations in their faceted multiplicity. The media theorist Ian Bogost speculates and practices coding  as a way to make sense of the world: 'source code itself often offers inroads in alien phenomenology - particular when carpentered to reveal the internal experience of withdrawn units' [@Bogost_2012, 105] \index{Bogost, Ian} \index{source code}. In relation to machine learning, reading and writing code alongside scientific papers, Youtube lectures, machine learning books and competitions is not only a form of observant participation, but directly forms part of the diagrammatic multiplicity. We can see this in a number of different ways. 

\index{R|(} First,  although barely a single line of code appears in _Elements of Statistical Learning_, nearly all of the examples in _The Elements of Statistical Learning_ are implemented in  a single programming language, `R`. The authors say 'we used the R and S-PLUS programming languages in our courses' [@Hastie_2009,9] but the diagrammatic movements of the book owe much more to `R` code.[^1.91] The proliferation of programming languages such as `FORTRAN` (dating from the 1950s), `C` (1970s), `C++` (1980s),  then `Perl` (1990s), `Java` (1990s), `Python` (1990s)  and `R` (1990s), and computational scripting environments such as Matlab, multiplied the paths along which  implementation of machine learning techniques could proceed.\index{programming languages}  It would be difficult to comprehend the propagation of machine learners  across domains of science, business and government without paying attention to coding practices. Even if textbooks and research articles are not read, software packages and libraries for  machine learning are used. They have a mobility that extends the diagrammatic practices of machine learning into a variety of settings and places where the scientific reading apparatuses of equations, statistical plots, and citations of research articles would not be operative. 

[^1.91]: In a latter book by some of the same authors with the very similar sounding title _An Introduction to Statistical Machine Learning with Applications in `R`_ [@James_2013], `R` does appear in abundance. This book, however, is much shorter and  less inclusive in various ways. There are in any case many online manuals, guides and tutorials relating to `R` [@Wikibooks_2013]. For present purposes, I draw mainly on semi-popular books such as _R in a Nutshell_ [@Adler_2010], _The Art of `R` Programming_ [@Matloff_2011], _R Cookbook_ [@Teetor_2011], _Machine Learning with R_ [@Lantz_2013] or _An Introduction to Statistical Learning with Applications in R_ [@James_2013]. These books are not written for academic audiences, although academics often write them and use them in their work. They are largely made up of illustrations and examples of how to do different things with different types of data, and their examples are typically oriented towards business or commercial settings, where, presumably, the bulk of the readers work, or aspire to work. Given a certain predisposition (that is, geekiness), these books and other learning materials can make for enjoyable reading. While they vary highly in quality, it is sometimes pleasing to see  the economical way in which they solve common data problems. This genre of writing about programming specialises in posing a problem and solving it directly. In these settings, learning takes place largely through following or emulating what someone else has done with either a well known dataset (Fisher's `iris`) or a toy dataset generated for demonstration purposes.  The many frictions, blockages and compromises that often affect data practice are largely occluded here in the interests of demonstrating the neat application of specific techniques. Yet are not those frictions, neat compromises and strains around data and machine learning precisely what we need to track diagrammatically? In order to demonstrate both the costs and benefits of approaching `R` through such materials, rather than through ethnographic observation of people using `R`, it will be necessary to stage encounters here with data that has not been completely transformed or cleaned in the interests of neatly demonstrating the power of a technique. One way to do this is by writing about code while coding. 

`install.packages('ElemStatLearn', dependencies='Suggests', repos = 'http://cran.us.r-project.org')`


```{r elem_stat_learn_install, tidy=TRUE, cache=TRUE, echo=FALSE}
    list.of.packages <- c("ElemStatLearn")
    new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
    if(length(new.packages)) install.packages(new.packages, dependencies='Suggests', repos = 'http://cran.us.r-project.org')
```

```{r elem_stat_learn_dependencies, messages=FALSE, warnings=FALSE,  results='asis', cache=TRUE, echo=FALSE}
library(tools)
library(xtable)
pack = available.packages()
pack_df = as.data.frame(pack)

df = as.data.frame(head(sort(table(pack_df['Suggests']), decreasing=TRUE),20)) 
colnames(df) = c( 'How often suggested')
table1 = xtable(df, caption=c('R packages suggested by the ElemStatLearn package', 'ElemStatLearn suggest R packages'), label='tab:elemstat_learn_suggests')

df2 = as.data.frame(sort(table(pack_df['Depends']), decreasing=TRUE)) 

df2 = cbind(rownames(df2), df2)
colnames(df2) = c( 'Package', 'How often depended on')
df3 = df2[-grep(df2[,1], pattern='^R\\W?\\('),]
table2 = xtable(df3[1:20,],  caption=c('`R` packages depended on by the `ElemStatLearn` package', '`ElemStatLearn` depends on R packages'), label='tab:elemstat_learn_depends')
print(table1, type='latex', table.placement="!htb")
print(table2, type='latex', table.placement="!htb")
```
Second, particular code elements such as `R` packages relate to many others, and these relations can be used to identify how different facets and levels of abstraction come together. For instance, the line of code shown above when executed opens another way of reading _Elements of Statistical Learning_ and getting a feel for the dragnet of practice running through it. There is nothing too opaque, I would suggest, about the line of code itself, but there are some folds and code-specific convolutions associated with it that point to infrastructural undersides. Take the part of the line `dependencies = 'Suggests'`. When the line of code executes, this stipulation of `dependencies` leads to a quite wide-ranging installation event. If the installation works (and that assumes quite a lot of configuration and installation work has already taken place; for instance, installing a recent version of the `R` platform), then _Elements of Statistical Learning_ is now augmented by various pieces of code, and by various datasets that in some ways echo or mimic the book but in other ways extend it operationally (see tables \ref{tab:elemstat_learn_depends} and \ref{tab:elem_stat_learn_suggests}).[^1.92]  These code elements are often stunningly specialised. As Karl Marx wrote of the 500 different hammers made in Birmingham, 'not only is each adapted to one particular process, but several varieties often serve exclusively for the different operations in one and the same process' [@Marx_1986, 375] \index{Marx, Karl}. Something similar holds in `R`: thousands of software packages in online repositories suggest that a highly specialised division of labour and possibly refined co-operative labour processes operate around data. 


Third, `R` is an increasing well-known and widely used statistical programming language and environment  [@RDevelopmentCoreTeam_2010]. Its growth is perhaps just as important as its operation [@Mackenzie_2014]. An open source programming language, according to surveys of business and scientific users, at the time of writing, `R` has replaced popular statistical software packages such as SPSS, SAS and Stata as the statistical and data analysis tool of choice for many people in business, government, and sciences ranging from political science to genomics, from quantitative finance to climatology [@RexerAnalytics_2010]. Developed in New Zealand in the mid-1990s, and like many open source software projects, emulating `S`, a commercialised predecessor developed at AT&T Bell Labs during the 1980s, `R` is now extremely widely used across life and physical sciences, as well as quantitative social sciences. John Chambers, the designer of `S`, was awarded the Association for Computing Machinery (ACM) 'Software System Award' in 1998 for 'the S system, which has forever altered how people analyze, visualize, and manipulate data' [@ACM_2013].\index{Chambers, John}  Many undergraduate and graduate students today earn `R` as a basic tool for statistics. Skills in `R` are often seen as essential pre-requisite for scientific researchers, especially in the life sciences. (In engineering, `Matlab` is widely used.) Research articles and textbooks in statistics commonly both use `R` to demonstrate methods and techniques, and create `R` packages to distribute the techniques and sample data. Nearly all of these publication-related software packages, including quite a few from the authors of _Elements of Statistical Learning_ are soon or later available from the 'Comprehensive `R` Archive Network (CRAN)' [@CRAN_2010]. Estimates of its number of users range between 250000 and 2 million. Increasingly, `R` is integrated into commercial services and products (for instance, SAS, a widely used business data analysis system now has an `R` interface; Norman Nie, one of the original developers of the SPSS package heavily used in social sciences, now leads a business, Revolution, devoted to commercialising `R`; `R` is heavily used at Google, at FaceBook, and by quantitative traders in hedge funds; in 2013 ['R usage is sky-rocketing'](http://www.r-bloggers.com/r-usage-skyrocketing-rexer-poll/); etc.). In general terms, `R` has a kind of disciplinary polyglot currency as a form of expression, and exhibits a fine-grained relationality with many different epistemic and operational situations associated with machine learning.

Fourth, less pragmatically, but symptomatically, `R` has also attracted  mainstream media attention. An article in _The New York Times_ in 2009 highlighted its practical importance in data analysis [@Vance_2009], at the time  _The New York Times_ was heavily promoting its idea of data journalism.   This is somewhat unusual, as programming languages are not normally the topic of mainstream media interest.  It is easy today to find an employment-centric view of data practice.  [^1.53] `R` is an evocative object, to use the psychoanalyst Christopher Bollas' term [@Bollas_2008] \index{Bollas, Christopher}, an object through which many different ways of thinking circulate. Standing somewhere at the intersection of statistics and computing, modelling and programming, many different disciplines, techniques, domains and actors intersect in `R`. Bollas suggests that 'our encounter, engagement with, and sometimes our employment of, actual things is a _way_ of thinking' [@Bollas_2008, 92].\index{thinking}  `R` embodies something of plurality of practices of working with measurements, numbers, text, images, models and equations, with techniques for sampling and sorting, with probability distributions and random numbers. It engages immediately, practically and widely with  words, numbers, images, symbols, signals, sensors, forms, instruments and above all abstract forms such as mathematical functions like probability distributions and many different architectural forms (vectors, matrices, arrays, etc.), as it employs data. 

Writing code has always been  central in machine learning where algorithms and machines are the primary expressive forms that ideas take as they become diagrams. Two of the main proponents of `R` and `S` describe the motivation for the language:

> The goal of the S language ... is "to turn ideas into software, quickly and faithfully" ... it is the duty of the responsible data analysts to engage in this process ... the exercise of drafting an algorithm to the level of precision that programming requires can in itself clarify ideas and promote rigorous intellectual scrutiny. ... Turning ideas into software in this way need not be an unpleasant duty. [@Venables_2000, 2]

Bill Venables and Brian Ripley, statisticians working on developing `S`, the almost identical commercial predecessor to `R`,   wrote in the early 1990s of  the responsibility of data analysts to write not just use software. \index{Ripley, Brian} \index{Venables, Bill}   They write 'software' here not in the sense of a product, but in the sense that today would more likely be called 'code.' This sense of coding and programming as clarifying and concretising ideas with precision has thoroughly taken hold in contemporary data analysis. But not that it lies somewhat at odds with Pedro Domingos characterisation of machine learning as a shift away from building to growing. Many practices, many ways of doing things, describe themselves as part of their doing. Perhaps every practice contains elements of its own description. The contrast between `R` as a language originating in statistics and subsequently percolating through various scientific fields and `Python` or `Java`, which are  much more general purpose programming languages widely used for software development in many settings, is instructive. Implementations of machine learning algorithms in `Python` can more or less be found in a few software libraries such as [`SciPy`](http://www.scipy.org) and `ScikitLearn` [@Pedregosa_2011], and much of the instructional materials on how to use machine learning in practice rely on `Python`. But these implementations are somewhat less diagrammatically rich than those associated with `R`. They tends to be operational rather epistemic. 

## The diagram of a hyerobject

I have been suggesting that we can get a sense of the hyperobject-like character of machine learning by treating _Elements of Statistical Learning_ as a diagram of operations. These operations move across and connect facets of abstraction that include, but are not limited to, mathematical forms (especially statistical ones, but also algebra and calculus), problems from multiple scientific disciplines (especially computer science, but also biology, medicine and others), devices such as computing platforms, data formats and algorithmic operations and pieces and parts of code. Following Peirce, we can treat all of these elements as diagrammatic components. Diachronically, or in time, machine learners have patched these elements together in ways that have begun to substantially affect what counts as knowledge, action, or decision. The operational power of machine learning does not stem from a single layer of abstraction. Hence any engagement with machine learning and similar operations would be very limited if it confined its range of movement to a single level such as code, or algorithm or mathematical function or platform. The diagrammatic forms of movement we have begun to discern in the polymorphic  _Elements of Statistical Learning_ suggest key lines and paths worth following in opening up that engagement. Like the perceptron calculating weights that allow it to express the logic of the NAND function, we might first of all turn to the table, the ordering and aligning of numbers on which nearly all  machine learning depends.  

[^1.92]: Most of the packages associated with the `ElemStatLearn` \index{R packages!ElemStatLearn}implement methods or techniques developed by Hastie, Tibshirani or Friedman, but some are much more generic. `MASS` for instance is highly cited `R` package.  (Of the `r nrow(pack_df)` packages in the R CRAN system, `r sum(grepl(pack_df['Depends'], pattern='MASS'))` depend on the library `MASS`, \index{R packages!MASS} itself an adjunct to the influential and highly cited _Modern Applied Statistics with S_ [@Venables_2002], a textbook that presents many machine learning techniques using `S`\index{programming language!S}, AT & T Bell Labs commercial precursor to the open sourced `R`\index{programming language!R}). For our purposes, this hardly accidental mixing of academic or research work with a programming languages and its associated infrastructures is fortuitous. It allows us to transit between different strata of the social fields of science, engineering, health, medicine, business media and government more easily. 


[^1.53] The 'chief economist' at Google, Hal Varian in 2009 made a widely quoted prediction about working with data:

        >The ability to take data — to be able to understand it, to process it, to extract value from it, to visualize it, to communicate it — that’s going to be a hugely important skill in the next decades, not only at the professional level but even at the educational level for elementary school kids, for high school kids, for college kids. Because now we really do have essentially free and ubiquitous data. So the complimentary [sic] scarce factor is the ability to understand that data and extract value from it.” Hal Varian, chief economist at Google: [@McKinsey_2009]

    While cited here in a report prepared by the global business consultancy, McKinsey & Co, this quote, or selections from it, can be seen in many different contexts, ranging from government reports on higher education to student noticeboards in  university statistics departments.  What Varian, with all the allure of  Google as a potential employer, presents  as an 'important skill'  also has a 'scarce factor':  the 'ability to understand that data.' While Varian presents understanding data as the prelude to 'extract[ing] value from it,' understanding might take different forms, depending on the kind of encounter or engagement that precedes it. 

    A somewhat broader framing of the value of `R` can be found voiced as a form of democratised knowing or engagement with data. Norman Nie, the original developer of the widely-used SPSS social science statistics software (see [@Uprichard_2008]), is a proponent of `R`. Norman Nie interviewed in _Forbes_, a leading business news magazine, evangelises for `R` in these terms:

    > Everyone can, with open-source `R`, afford to know exactly the value of their house, their automobile, their spouse and their children--negative and positive  … It's a great power equalizer, a Magna Carta for the devolution of analytic rights [@Hardy_2010].

    While still  referring to `R`  in ways that lie close to employment, Nie, the founder of Revolution Analytics, a company that provides software and support services to `R` users, promotes `R` as a way of protect individual liberties to know what they own. In the context of the global financial crisis of 2007, it may be that the value of houses became much more problematic, and understandably, harder to know. And the reference to 'spouse and their children' must be flippant. Yet the 'devolution of analytic rights' that Nie speaks of here, even as he frames it in terms of entrepreneurial individualism, is important. While Nie's company  Revolution Analytics promotes the incorporation of  `R` into  powerful and pervasive business and government prediction systems (such as SAS and IBM's Pure Data [@IBM_2013]), Nie here points to the problem of 'manipulation and control' of customers associated with just that incorporation. The 'analytic rights' he advocates in the form of `R` are meant to help individuals counterbalance the power of large scale data analysis conducted by corporations and governments.  More than any particular or specific use, Nie's advocacy of `R` taps into a wider  sense of rich possibility associated with `R`. It would be possible to cite many other instances of this belief and desire in the potential of `R`. A good indication of this overflow is the aggregate blog, [R-bloggers](R-bloggers.com). `R`-bloggers brings together several hundred `R`-related blogs in one place. The range of topics discussed there on any one day – Merrill-Lynch Bond Return indices, how to run `R` on an iPhone, using US Department of Agriculture commodity prices, analysing human genetic variation using PLINK/SEQ via `R`, using the 'Rhipe' (sic!) package to program big data applications on Map-Reduce cloud architectures, doing meta-analysis of phylogenetic trees, etc., etc (R-bloggers 2011) – suggest how widely desires/beliefs in `R` range. To take just one fairly recent example, '`R` Analysis Shows How UK Health System could save £200 million' claims a recent post on the site [@Smith_2012]. While many of the applications, experiments, or techniques reported there can be reduced to employment or to individual uses of `R`, they also, as we will see, suggest the potential for encounters and engagements. Something similar, and perhaps less bound to commercial data analytics can be found in Rachel Schutt and Cathy O'Neil's much more lively _Doing Data Science_ [@Schutt_2013] as it discusses the ethics and potentials of working with machine learning models and programming languages such as `R` and Python.
