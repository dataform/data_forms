# Diagramming machines for data

Writing about data
How to write about data and models
Making models of data
Associating with data: a praxiography of machine learning

## from proposal

This chapter is primary a methodological discussion that addresses several different problems in working with machine learning. These problems range from quite philosophical issues through to quite practical ones. At the philosophical end, it draws on various pieces of recent and not-so-recent theoretical work (for instance, the anthropologist Paul Rabinow's work on the concept of 'equipment' [@Rabinow_2003]; the philosopher A.N. Whitehead on the spatial dimensionality of thought [@Whitehead_1956]; the philosopher Anne-Marie Mols' work praxiography -- writing about practices [@Mol_2003])  to discuss how we might think about working on highly technical or scientific areas such as machine learning in ways that allow consideration of their more general situation.  It also draws on some cultural and psychoanalytic accounts of architecture and objects [@Bollas_2008; @Wilson_2010] to suggest how the researcher him or herself relate to objects of research.  Finally in this vein, the chapter poses the methodological problem of working with large bodies of scientific and technical literature, and illustrates this by describing the growth of machine learning techniques in science and engineering research since the early 1960s, focusing on the growth of key techniques and algorithms [@Kelty_2009].

With these questions about thinking, techniques, subjectivity and literature on the table, the chapter then presents a series of vignettes that display some of the ways in which research and writing critical accounts of data cultures and data economies can make use of the tools, techniques, instruments and services of 'data science' to generate textual, diagrammatic and modelised accounts of contemporary culture.  A standard teaching example -- house-price prediction -- links these vignettes, but the real focus here is on two foundational issues: how machine learning treats data as a dimensional material that it seeks to reshape or recase  in different dimensions ('models'); how implementing machine learning techniques shifts our relation to them.  

Woven throughout this discussion of a praxiography of data, [TBC] Via a discussion of the development of `R`, the chapter analyses the  transverse, cross-disciplinary implementation of machine learning techniques in recent decades. It describes some of the transformations in software, network and scientific cultures that underpin the recent growth in data techniques and methods. These range across transformations in statistical science associated with greater computational capacity; the mutations in network,  database and digital device architectures and infrastructures that yield much greater abundance of data in various forms; and the intermeshing of knowledge economies with the media, communication, transaction, transport and logistics systems. It will trace how the lateral associations and multivalencies of data have developed through key software artefacts such as the widely used `R` programming language, and in generic programming languages such as Python. Coming from  the author's own history of working with machine learning or online accounts of machine learning,  well as the ecology of thousands of software packages  associated with the statistical programming language `R`.


## Overview

- data: its giveness, abstraction and actuality
    - observant participation -- developing Wacquant; participating by observing
    - writing recursively -- developing Kelty w.r.t subjects
    - James on feeling of transition - and practicing radical empiricism - -see Massumi
- Implementing machine learning
    - the case of `R` -- the scientific
    - the case of Python -- the business/industry
    - the case of javascript -- popular culture
- convergence and learning: for fitting a line; but then for choosing which features to use to learn
- things to include:
    - my `R` books, libraries, papers, competitions, blogs, talking, courses
    - `knitr`/`ipython` notebooks - executable - some examples from python notebooks
    - the notion of repl -- read-evaluate-print-loop
    - Add in Animation and Automation – The Liveliness and Labours of Bodies and Machines Body & Society March 2012 18: 1-46, 
    - machine learning textbooks -- hastie, williams, manning, see document archive
    - the github version of the book
    -

## Todo
- scientific papers form a substrate here; but they are entangled with software, with databases, with institutions, and with a widening circle of actors
- learning about learning --
    - the Literature - landecker and kelty on treating literature as the informant; examples such as Cover-Hart, Breiman, etc
- Michie, Spiegelhalter 1994 book has report on the Statlog project -- it tried to bring all method together; good that it mixes stats and computer scientists; managed by Daimler-Benz; shows that the problem of diversity of techniques has been discussed before; look at how they tried to bring  the methods together;
- the habit of recursive application of code to itself, or to something close by, without ever going too far -- I'll do that too -- use the algorithms to investigate the literature, to classify the examples, to see how things move; but I don't want to place too much weight on this recursion, as it is something that needs to be analsyed. See the recent Totaro article -- not recursion as an intrinsic formal property, but as a practice in certain domains that allows a form of movement.
- Cathy O'Neill writing about why big data is over -- doing one course is not enough ... 
- done? On the aridness of the textbooks and literature and what to do about that. 
- add Parisi quote about software cultures: '
>The epochal challenge of programming cultures is to venture into the infinity of incomputable probabilities (infinite discrete unities that are bigger than the totality of the whole sequence of algorithmic instructions) that lies beyond both the digital ground and interactive empiricism. 77
- the entwining of the models and the data with this text -- it can't be shown in this text, in the same way that it can't be easily shown in the world.  
- complain about all the vignettes and case studies in this area - Mayer-Schonberger, the podcasts, etcs

## Introduction

> Machine learning is not magic; it cannot get something from nothing. What it does is get more from less. Programming, like all engineering, is a lot of work: we have to build everything from scratch. Learning is more like farming, which lets nature do most of the work. Farmers combine seeds with nutrients to grow crops. Learners combine knowledge with data to grow programs. [@Domingos_2012, 81] 

>Take the verb 'moving': if we write 'it-moving' we are expressing a multiplicity in a diagrammatic way. It-moving indicates a complex abstract machine which can appear independent of any subjective tendency -- it could refer to an individual, an army, an insect, an object, a machine, an emotion, an idea. It allows for the all the possibilities of moving, so that the verb retains it machinic character. 139

Many different tendencies shape what is happening to data today (how for instance, 'it-moving'), but two of them impinge directly on  our  practical sense of the potentials of data. They will sound very banal. On the one hand, the movement of data is very much linked to a certain form of reading. On the other hand, the movement of data is very much to do with writing. Reading and writing data, nothing could be simpler. The problem is: who reads and writes what? And conversely, what reads and writes who? In both of the epigraphs above, one from a renowned machine learning researcher and the other from the French philosopher, Felix Guattari, this flipping between who and what can be observed. In Pedro Domingo's talk of 'learners,' we might find it hard to decide who is a learner. Machine learning algorithms are sometimes called 'learners.' Similarly in Guattari's 'it-moving,' the it could be individual, machine, ant or idea. These slightly gnomic formulations are worth pursuing I believe because they help us to make sense of an immense Magellanic constellation of documents, software, publications, blog pages, books, spreadsheets, databases, whiteboard and blackboard diagrams, and an inordinate amount of talk and visual media in orbit.

On the one hand, ways of working with  data have shifted substantially over the last decade or so due to the growth in open source programming languages and networked platforms. Scientists, software developers and data analysts use programming languages such as `python` and `R` rather than commercial software packages such as Matlab, SAS or SPSS to work with data. Scientific research, which has long relied on counting and calculation, increasingly organises and processes data more comprehensively because workflows, datasets, algorithms and databases can be quickly and flexibly assembled in code. (Scientific computing languages such as FORTRAN have long underpinned scientific research in various more mathematical fields.) On the other hand, several decades of research and application of statistical and machine learning algorithms in science, government, industry and commerce have gradually developed fairly powerful ways of automating the construction of models that classify and predict events and associations between things, people, processes, etc. Very extensive networks of code swirl around data in its scientific, biomedical, financial, commercial and bureaucratic incarnations.  Programming and software development has long handled data and algorithms, and conversely, machine learning has always relied on programming. But they are powerfully coalescing in data-driven research, business and controls systems in various ways as predictive models, targeted advertising, recommendation systems, . Can we locate anything in the proliferation of machine learning that is more than plying data into a tightly woven matrix of commercial-scientific-governmental knowing/surveillance? I don't answer that question directly here, but my basic answer is affirmative. I think we can and we should  explore some of the shifts in practices of working with data that can be observed around data. These practices are both the object of analysis, and in  certain cases, provide support for a shift in the way in which we might carry out research on changes in media, publics, sciences, commerce, mobilities and  health. 

## A diachronic diagram of reading machine learning

In the course of writing this book, as well as reading the academic textbooks, popular how-to books, software manuals, help documents and blog-how-to posts,  like many other students, I attended graduate courses in Bayesian statistics, genomic data analysis, data mining, and missing data. Significantly, I also participated in the online machine learning courses and some machine learning competitions.[^4] These are all typical activities for people learning to do machine learning. Importantly, these courses, books, competitions and programming languages are widely viewed and used. Andrew Ng's machine learning course on Coursera (of which he is co-founder, along with Daphne Koller, another machine learning scientist from Stanford), has a typical enrolment of 100,000. Youtube videos of his Stanford University lectures have cumulative viewing figures of around 500,000 [@Ng_2008]. Amidst this avalanching mass, a  single highly cited and compendious textbook, _Elements of Statistical Learning: Data Mining, Inference, and Prediction_ [@Hastie_2009], currently in its second edition, can be seen from almost any point of the terrain.[^12] The authors of the book, Jeff Hastie, Rob Tibshirani and Jerome Friedman are statisticians working at Stanford and Columbia University. (Statisticians and computer scientists from Stanford University loom large in the world of machine learning, perhaps due to their proximity to Silicon Valley.)  In terms of scientific publications, this book is like the Death Star in the _Stars Wars_ film series. It is a  massive orbiting object, densely bristling with diagrams, equations, tables, algorithms,  graphs and references to other scientific literature. Like the online machine learning courses and programming books I discuss below, _Elements of Statistical Learning_  combines statistical techniques with various algorithms to 'learn from data' [@Hastie_2009, 1]. The 768 pages of this often tersely written and quite mathematical book range across various kinds of problems (identifying spam email, predicting risk of heart disease, recognising handwritten digits, etc.), and various machine learning techniques, methods and algorithms (linear regression, k-nearest neighbours, neural networks, support vector machines, the Google Page Rank algorithm, etc.). This is not the only juggernaut machine learning texts.  I could just have well cleaved to Alpaydin's _Introduction to Machine Learning_ [@Alpaydin_2010]  (a more computer science-base account), to Christopher Bishop's  heavily mathematical _Pattern recognition and machine learning_ [@Bishop_2006], Brian Ripley's luminously illustrated and almost coffee-table formatted _Pattern Recognition and Neural Networks_ [@Ripley_1996], Tom Mitchell's  earlier artificial intelligence-centred _Machine learning_ [@Mitchell_1997], Peter Flach's  perspicuous _Machine Learning: The Art and Science of Algorithms that Make Sense of Data_ [@Flach_2012], or further afield, the sobering and laconic _Statistical Learning for Biomedical Data_ [@Malley_2011].  These and quite a few other recent machine learning textbooks display a range of emphases, ranging from the highly theoretical to the very practical, from an orientation to statistical inference to an emphasis on computational processes, from  science to commercial applications. 

[^12]: The complete text of the book can be downloaded from the website [http://statweb.stanford.edu/~tibs/ElemStatLearn/](http://statweb.stanford.edu/~tibs/ElemStatLearn/). At the end of short intensive course on data minin at the Centre of Postgraduate Statistics, Lancaster University, the course convenor, Brian Francis, recommended this book as the authoritative text. Some part of me rues that day. That book is a poisoned chalice; that is, something shiny, valuable but also somewhat toxic. 

The book attracts different styles of reading. It is often cited by academic machine learning practitioners as an authoritative guide. On the other hand, students participating in new data science courses often come from different disciplinary backgrounds and find the tome unhelpful (see the comment by students in [@Schutt_2013]). Whether the citations are friendly or not, Google Scholar reports over 20,00 citations of the book http://scholar.google.com/scholar?hl=en&q=elements+of+statistical+machine+learning, October 2014), a huge citation count by any standards. (Michel Foucault's _The History of Sexuality: an Introduction_, one of the most highly cited book in the humanities,  receives about the same number of citations.) The citations that [@Hastie_2009] as well as its previous edition [@Hastie_2001] receive do not arrive principally from machine learning researchers. They come from a wide variety of fields.[^13] It is hard to find a field of contemporary science, engineering, health and indeed social science that has not cited it. 

```{r hastie_pages, engine='python', echo=False}

import pandas as pd
import re
import os
import matplotlib.pyplot as plt


def load_records(data_dir):
    """Return dataframe of all records,
    with new column of cited references as list"""

    # I saved all the WoS full records for the search term in this directory
    files = os.listdir(data_dir)
    wos_df = pd.concat([pd.read_table(wos_df, sep='\t', index_col=False)
                        for wos_df in [data_dir+f for f in files
                        if f.count('.txt') > 0]])
    wos_df = wos_df.drop_duplicates()

    #fix index
    index = range(0, wos_df.shape[0])
    wos_df.index = index

    #to get all cited refs
    if wos_df.columns.tolist().count('CR') > 0:
        cited_refs = [list(re.split(pattern='; ',
                                    string=str(ref).lower().lstrip().rstrip()))
                                    for ref in wos_df.CR]
        # add as column to dataframe
        wos_df['cited_refs'] = cited_refs

    # normalise authors
    if wos_df.columns.tolist().count('AF') > 0:
        wos_df.au = [str(au).lower().lstrip().rstrip() for au in wos_df.AF]

    return wos_df


def clean_fields(wos_df):

    """ Returns dataframe with a new field 'field' that lists
    the fields for each reference"""

    wos_df['fields'] = wos_df.SC.dropna().str.lower().str.split('; ')
    return wos_df


def clean_topics(wos_df):
    """Wos.DE field has a mixture of topics and techniques.
    -------------------------------------
    Returns a cleaned-up, de-pluralised list version of all the topics and techniques
    """

    wos_df['topics'] = wos_df.DE.dropna().str.lower().str.strip().str.replace('\(\w+ \)', '').str.replace('($  )|(  )', ' ')
    # wos_df['topics'] = wos_df.topics.str.replace("[\(\](\w+ ?)+[\)\]]\W*",  '')
    wos_df.topics = wos_df.topics.str.replace('svm', 'support vector machine')
    wos_df.topics = wos_df.topics.str.replace('support vector machine(\w*)',
                                            'support vector machine')
    wos_df.topics = wos_df.topics.str.replace('(artificial neural network)|(neural networks)|(neural net\b)',
                                            'neural network')
    wos_df.topics = wos_df.topics.str.replace('decision tree(.*)', 'decision tree')
    wos_df.topics = wos_df.topics.str.replace('random forest(\w+)', 'random forest')
    # Web of science topics often have  a bracket expansion
    wos_df['topics'] = wos_df.topics.str.replace("(\w+)\W*[\[\(].+[\)\]]\W*", '\\1 ')
    wos_df.topics = wos_df.topics.str.split('; ')
    return wos_df

def keyword_counts(wos_df):
    """ Returns a dictionary with keyword counts"""
    de_all = [d for de in wos_df.topics.dropna() for d in de]
    key_counts = collections.Counter(de_all)
    return key_counts


    df = load_records('data/hastie_elem_WOS/')
    print('There are %s records in the dataset' % df.shape[0])
    df = clean_topics(df)
    df = clean_fields(df)

    print('%s topic fields are null' % sum(df.topics.isnull()))
    print('%s abstract fields are null' % sum(df.AB.isnull()))
    print('%s keywords Plus fields are null' % sum(df.ID.isnull()))
    print('% s author fields are null' % sum(df.AF.isnull()))

    all_fields = sorted([e for el in df.fields.dropna() for e in el])
    fields_set = set(all_fields)
    field_counts = {e: all_fields.count(e) for e in fields_set}

    print('%s different fields appear in the literature citing Hastie (2001/2009)' % len(fields_set))
    field_counts_s = sorted(field_counts.iteritems(), key=lambda(k, v):(-v, k))
    field_counts_s[0:30]
    major_fields = {f:v for f,v in field_counts.iteritems() if v > 3 or f is not 'computer science'}
    print(len(field_counts), len(major_fields))
    kw = keyword_counts(df)
    kw.most_common()[:50]
    #find the most commonly cited pages
    hastie = df.CR.str.findall('Hastie.*?;')
    hastie = hastie.map(lambda x:  x[0] if len(x) ==1 else '')
    hastie_pages = hastie[hastie.str.contains('P')]
    pages = hastie_pages.str.findall('\d{1,3};').map(lambda x: x[0] if len(x) ==1 else '').str.replace(';', '')
    pages = pages[pages != '']
    pages = pages.astype('int')
    pages_counted = pages.value_counts()
    pages_counted_sorted = pages_counted.sort_index()
    pages_counted_sorted.to_csv('data/hastie_pages.csv')

```

```{r plot_hastie_pages, echo=False}
library(ggplot2)
hastie_pages = read.csv('data/hastie_pages.csv')
colnames(hastie_pages) = c('page', 'reference_count')
plt = ggplot(hastie_pages, aes(x =page, y = reference_count))
plt + geom_bar(stat = 'identity')
```

So we know many people read the book. But what do they read in the book? That's harder to tell. Of the around 760 pages in [@Hastie_2009] and [@Hastie_2001], around `r dim(hastie_pages)[1]` distinct pages are referenced in the citing literature. As the figure shown above indicates, certain portions of the book are much more heavily cited than others. This distribution of page references in the literature that cites _Elements of Statistical Learning_ is a rough guide to how the book has been read in different settings. In general, the thousands of citations of the book themselves comprise a diachronic diagram of readings of the book, and their relative concentrations and sparsities suggest sites of engagement in the techniques, approaches and machines that the book describes. For instance, the most commonly cited page in the book is page `r hastie_pages$page[which.max(hastie_pages$reference_count)]`. That page begins a section called 'Non-negative Matrix Factorization', a technique frequently used to process digital images to compress their visual complexity into a simpler set of visual signals [@Hastie_2009, 553]. It is particularly powerful because it, as Hastie and co-authors write, 'learns to represent faces with a set of basis images resembling parts of faces' [@Hastie_2009, 555]. (So `kittydar`, which doesn't use NMF, might do better if it did, because it could work just with parts of the images that lie somewhere near the parts of a cat's face -- its nose, its eyes, its ears.) 

We might say that NMF with its representation of faces on the basis of a set of parts of faces, offers a reading protocol for _Elements of Statistical Learning_. We cannot read it its entirety here, not just because 768 pages is a lot to read, but because the semiotic machinery of the book generates vectors and orbital trajectories that veer and peel off in many different directions. The Death Star aspect of the book, its somewhat forbidding presence in the literature, comes from the vast semiotic weave of equations, diagrams, tables, algorithms, bibliographic apparatus, and numbers wreathed in sheer typographic luxuriance. The weave of these elements far exceeds almost anything found in the humanities or social sciences, and, almost before anything else, prompts attention to the question of who or what reads. For instance, in terms of outgoing references, _Elements of Statistical Learning_ webs together a field of scientific and technical work with data and predictive models ranging across half a century. The reference list beginning at page 699 [@Hastie_2009, 699] runs for around 35 pages, and the five hundred or so references there point in many directions. One could learn a lot from that reference list, which itself spans biomedical, engineering, telecommunications, ecology, operations research and many other fields. (We will have occasion to look at some of these fields in later chapters). 

```{r hastie_references, echo=False}
system('pdftotext -f 717 -l 746 ../ml_lit/data/hastie_elem/hastie_elements_2009.pdf hastie_references.txt')
refs = read.delim('../ml_lit/data/hastie_elem/hastie_references.txt', sep='.\n', stringsAsFactors=FALSE)
refs = sort(refs[,1])
```

[TODO: sort out these references] While many of these references either relate to Hastie, Tibshirani or Friedman's own work, or their statistical colleagues, the references show that these authors are also reading widely in other fields. _Elements of Statistical Learning_ as a text is processing, assimilating and recombining techniques, diagrams and data from many different places. 

[^13]: A Thomson Scientific 'Web of Science'(TM) search for references citiing [@Hastie_2009] yields around 9000 results. These  publications sprawl across well over 100 different fields of research.  While computer science, mathematics and statistics dominate, a very diverse set of references comes from disciplines from archaeology, through fisheries and forestry, genetics, robotics, telecommunications and and toxicology ripple out from this book since 2001. 

## The mathematical glint of machine learning

While references come and go from many different places (and take too many forms for me to fully understand or explain here) into _Elements of Statistical Learning_ they are all framed by a notion of learning based on predictive models. The predictive model is like the Death Star's planetary destroying laser: all energy feeds the predictive model.  Perhaps this weaponised metaphor is a little misleading (and risky, since I'm not suggesting that Hastie, Tibshirani and Friedman are Sith), but there is one line of alignment between the laser and the predictive models in _Elements of Statistical Learning_: cutting sharp, straight lines through opaque clouds of data-matter is what much machine learning seeks to do. It manoeuvres in complex ways in drawing these lines (as we will see), but lines cutting, cleaving, and shattering things are very much the working edge of machine learning.  The predictive models in _Elements of Statistical Learning_ are  dominated by a single prediction technique, linear regression models or fitting a line to points. Jeff Hastie, Rob Tibshirani and Jerome Friedman, the authors, definitely advocate a statistical legacy and inheritance in machine learning:

>The linear model has been a mainstay of statistics for the past 30 years and remains one of our most important tools. Given a vector of inputs
$X^T = (X_1 , X_2, . . . , X_p)$, we predict the output $Y$ via the model
>$$\hat{Y} = \hat{\beta_0}  + \sum^p_(j=1) X_j \hat{\beta_j)}$$
>The term $\hat{\beta_0}$ is the intercept, also known as the _bias_ in machine learning [@Hastie_2009, 11].

In the course of the book, linear regression is subjected to countless variations, iterations, expansions and modifications. Their own research spans several decades and a range of topics concerning linear regression models and their variations ('ridge regression'; 'least angle regression'; etc.). But this introduction of the 'mainstay of statistics,' the linear model, already introduces a harsh form -- the mathematical equation -- that is perhaps the most prominent feature in the text. Any reading of the book has to work out a way to traverse theose forms. 
\begin {equation}
\label {eq:linear_model}
$$\hat{Y} = \hat{\beta_0}  + \sum^p_{j=1} X_j \hat{\beta_j}$$
\end {equation}

In its relatively compressed typographic weave, expressions such as \ref{eq:linear_model} operationalize movements through data that we will attend to repeatedly. These expressions, which are not comfortable reading by large for non-technical readers, are however worth looking at carefully as diagrams. They can be found in hundreds in [@Hastie_2009], but also in many other places. While their presence distinguishes machine learning from many other domains of computer science where mathematical equations are much less common, these equations also allow the book to collate and borrow from a panoply of scientific publications  and datasets in fields of statistics, artificial intelligence and computer science. Along with the citations, the graphical plots, the algorithms (implemented in code described below), these equations are integral connective tissue in machine learning. Unless we come to grips with their diagram force, without succumbing to the obscuring dazzle of mathematical signs, the connectivity and mobility of these forms will be lost on us.

I find it useful here to follow Charles Sanders Peirce account of mathematics in terms of diagrams. 'Mathematical reasoning,' he writes, 'is diagrammatic' [@Peirce_1998b, 206]. That it, we should see mathematics, whether it takes an algebraic or geometrical form, whether it appears in symbols, letters, lines or curves as  diagrams. Now for Peirce, a diagram is a kind of 'icon.' The icon is a sign that resembles the object it refers to: it has a relation of likeness. What likeness appears in \ref{eq:linear_model}? As Peirce says, 'many diagrams resemble their objects not all in their looks; it is only in respect to the relations of their parts that their likeness consists' [@Peirce_1998b, 13]. As we will soon see, \ref{eq:linear_model} could be expressed in statements in a programming language like `Python` or `R`, or in algorithmic pseudo-code, or perhaps most accessibly, as graphic figure (a line drawn through a cloud of points). In none of these associated diagrams can the relations between the parts be observed in the same way as they in the algebraic form. The 'very idea of the art' as Peirce puts it [@Peirce_1992, 228] of algebraic expressions is that the formulae can be manipulated. The graphic form of the expression include the various classical Greek symbols such as $\sum$ or $\prod$, as well as the letters $x, y, z$ and the indices (indexical signs) that appear in subscript or superscript, as well as the spatial arrangement of all these in lines and sometimes arrays. A variety of relations run between these different symbols and spatial arrangements. For instance, in all such expressions, the difference between the left hand side of the '=' and the right hand side is very important. By convention, the left hand side of the expression is the value that is predicted or calculated (the 'response' variable) and the right hand side are the input variables or 'features' that contribute data to the model or algorithm. This spatial arrangement fundamentally affects the design of algorithms. In the case of \ref{eq:linear_model}, the '^' over $\hat{Y}$ symbolises a predicted value rather than a value that can be known completely through deduction, derivation or calculation. This distinction between predicted and actual values organizes a panoply of different practices and imperatives (for instance, to investigate the disparities between the predicted and actual values -- machine learning practitioners spend a lot of time on that problem).  The general point is that the whole formulae is a diagram, or an icon that '*exhibits*, by means of the algebraical signs (which are not themselves icons), the relations of the quantities concerned' [@Peirce_1998b, 13]. Because such diagrams suppress so many details, they allow one to focus on a more limited range of relations between parts. The manipulation of those relations generates new diagrams or patterns. This affordance of diagrammatic forms is extremely important in the intensification of machine learning. Importantly, diagrams can diagram other diagrams. Put differently, operations can be themselves the subject of operations. Or functions can themselves for functions of functions. This nesting and coiled aspect of the diagrams is highly generative since it allows what Peirce calls 'transformations' [@Peirce_1998b, 212] or the construction of 'a new general predicate'[@Peirce_1992, 303].[^22]  The intensive processing of data today via predictive models is largely channelled via such diagrams. These diagrams are not conspicuous in the infrastructures, and they are not directly felt or perceived by people or things they impinge upon.

While I  seek to see the equations as diagrams, and I do persist in looking at them and will present a selection of them (nowhere near as many as found in _Elements of Statistical Learning_) in the following pages, the equations can you leave you feeling that you don't quite understand. Peirce advises not to begin with examples that are too simple. ('In simple cases, the essential features are often so nearly obliterated that they can only be discerned when one knows what to look for' [@Peirce_1998b, 206].) He also suggests 'it is of great importance to return again and again to certain features' [@Peirce_1998b, 206]. That is helpful in reducing the perplexity that these diagrammatic expressions tend to engender if you are not used to them. The price of looking at these diagrammatic expressions repeatedly is well worth paying if in consequence we can understand something of how the transformations, generalisations or intensification of the diagrammatizing flows across disciplinary boundaries, across social stratifications, and sometimes, generate potentially different ways of thinking about collectives, inclusion and belonging.  


[^22]: Felix Guattari makes direct use of Peirce's account of diagrams as icons of relation in his account of 'abstract machines' [@Guattari_1984]. He writes that 'diagrammaticism brings into play more or less  de-territorialized trans-semiotic forces, systems of signs, of code, of catalysts and so on, that make it possible in various specific ways to cut across stratifications of every kind' [@Guattari_1984, 145]. Here the 'trans-semiotic forces' include mathematical formulae and operations (such as the banking system of Renaissance Venice, Pisa and Genoa). They are trans-semiotic because they are not tethered by the signifying processes that code experience or  speaking positions according to given stratifications such as class, gender, nation and so forth. While Guattari (and Deleuze in turn in their co-written works [@Guattari_1988]) is strongly critical of the way which signification territorializes (we might think of cats patrolling, marking and displaying in order to maintain their territories), he is much more affirmative of diagrammatic processes. He calls them 'a-signifying' to highlight their difference from the signifying processes that order social strata. He suggests that diagrams become the foundation for 'abstract machines' and the 'simulation of physical machinic processes.' Writing in the 1960s, Guattari powerfully  anticipates the abstract machines and their associated diagrams that have taken shape and physical form in the succeeding decades. 


## CS229, 2007: returning again and again to certain features

If we were to follow Peirce's injunction to 'return again and again to certain features,' how would we do that?  _Elements of Statistical Learning_ is a difficult book. The cost of its diagrammatic density (equations, citations, tables, datasets, plots) is a certain feeling of 'not quite understanding' for many readers. This is partly because the book largely traverses finished work, and partly because it covers so much terrain. Professor Andrew Ng's course 'Machine Learning' CS229 at Stanford (http://cs229.stanford.edu/) might provide a supplementary path into machine learning [@Ng_2008].[^61] Note that Ng is a computer scientist, not a statistician. The course description runs as follows:

>This course provides a broad introduction to machine learning and statistical pattern recognition. Topics include supervised learning, unsupervised learning, learning theory, reinforcement       learning and adaptive control.   Recent applications of machine learning, such as to robotic control, data mining, autonomous navigation, bioinformatics, speech recognition, and text and       web data processing are also discussed [@Ng_2008]

CS229 is in many ways a  typical computer science pedagogical exposition of machine learning.  Machine learning expositions  usually begin with simple datasets and the simplest possible statistical models and machine learning algorithms, and then, with a greater or lesser degree of attention to issues of implementation, move through a succession of increasingly sophisticated and specialised techniques.  This pattern is found in many of the how-to books, in the online courses, and in the academic textbooks, including [@Hastie_2009]. The strikingly distinctive difference  of  Ng's CS229 lectures from almost all other expository materials is that we see someone writing.  Line after line of equations using chalk on a blackboard. Occasionally, questions come from students in the audience (not shown on the Youtube videos), but mostly that transcription of equations from paper to blackboard continues uninterrupted.[^23]

[^23]: After sitting through 20  hours of Ng's online lectures, and attempting  some of the review questions and programming exercises, including implementing well-known algorithms using `R`, one comes to know datasets such as the San Francisco house price dataset and Fisher's `iris` [@Fisher_1936] quite well. Like the textbook problems that the historian of science Thomas Kuhn long ago described as one of the anchor points in scientific cultures [@Kuhn_1996], these iconic datasets provide an entry point to the 'disciplinary matrix' of machine learning. Through them, one  gains some sense of how predictive models are constructed, and what kinds of algorithmic architectures and forms of data are preferred in machine learning. 


\begin{figure}
  \centering
      \includegraphics[width=0.5\textwidth]{figure/ng_lecture_5_generative_discriminative.pdf}
        \caption{Class notes lecture 5, Stanford CS229, 2007}
  \label{fig:class_notes}
\end{figure}

Ng's Youtube lectures have a certain diagrammatic atmosphere that comes from the many hours of chalked writing he stages during the course. In a time when PowerPoint presentations or some other electronic textuality would very much have been the norm (2007), why is a Stanford computer science professor, teaching a fairly advanced postgraduate course, writing on a chalkboard by hand? The \ref{fig:class_notes} shows a brief portion of around 100 pages of notes I made on this course. The act of writing down these equations and copying the many hand-drawn graphs Ng produced was a deliberative descriptive experiment, but more importantly an exercise in 'returning again and again' to what is perhaps overly hardened in  _Elements of Statistical Learning_.  Like the 50,000 or so other people who had watched this video, I complied with Ng's injunction to 'copy it, write it out, cover it, and see if you can reproduce it' [@NgAndrew_2008].  While it occasions much writing and drawing, and many struggles to keep up with the diagrams that Ng narrates as he writes, it seems to me that this writing of equations, with all their substitutions and derivations, alongside the graphic sketches of intuitions about the machine learning techniques, adds something that is quite hard to finesse in _Elements of Statistical Learning_. There the diagrammatic weave between the expressions of linear algebra, calculus, statistics, and the algorithms is almost too tight to work with. In Ng's CS229 lectures, by contrast, the weave is much more open. They lack the citational fanout of [@Hastie_2009], they are not able to wield the datasets and the panoply of graphic forms found there, and virtually no machine learning ccode runs beneath the lectures (although the CS229 student assignments, also to be found online, are code implementations of the algorithms and models). So I don't think Ng's lectures provide the answers we seek overall, but they have a useful thawing effect. As we will see, only by tracking some of the movements of code that underpin the book do we have a sense of how the book is put together. 

[^61]: A heavily shortened version of this course has been delivered under the title 'Machine Learning' on  [Coursera.org](https://class.coursera.org/ml-003/class/index), a  MOOC (Massive Open Online Course) platform. 


One thing that helps in any navigation of these literatures are some broadly shared topic structures. The textbooks, the how-to recipe books [@Segaran_2007; @Kirk_2014; @Russell_2011; @Conway_2012] and the online university courses on machine learning have a similar topic structure. They nearly always begin with 'linear models' (fitting a line to the data), then move to logistic regression (a way of using line fitting to classify binary outcomes; for example, spam/not-spam; malignant/benign; cat/non-cat), and afterwards move to some selection of neural networks, decision trees, support vector machine and clustering algorithms. They add in some decision theory, techniques of optimization, and ways of selecting predictive models (especially the bias-variance tradeoff).[^62]  The topic structures start to become increasingly familiar. 

[^62]: They differ, however, in several important respects.  Reading the _Elements of Statistical Learning_ textbook or one of machine learning books written for programmers (for example, _Programming Collective Intelligence_ or _Machine Learning for Hackers_ [@Segaran_2007; @Conway_2012]) does not directly enrol the reader in a machine learning process. By contrast, doing a Coursera course on machine learning brings with it an ineluctable sense of being machine-learned, of oneself becoming an object of machine learning. The students on Coursera are the target of machine learning. Daphne Koller and Andrew Ng are leading researchers in the field of machine learning, but they also co-founded  the online learning site [Coursera](http://coursera.org).  As experts in machine learning, it is hard to imagine how they would not treat teaching as a learning problem. And indeed, Daphne Koller sees things this way:

    > There are some tremendous opportunities to be had from this kind of framework. The first is that it has the potential of giving us a completely unprecedented look into understanding human learning. Because the data that we can collect here is unique. You can collect every click, every homework submission, every forum post from tens of thousands of students. So you can turn the study of human learning from the hypothesis-driven mode to the data-driven mode, a transformation that, for example, has revolutionized biology. You can use these data to understand fundamental questions like, what are good learning strategies that are effective versus ones that are not? And in the context of particular courses, you can ask questions like, what are some of the misconceptions that are more common and how do we help students fix them? [@Koller_2012]

    Whether the turn from 'hypothesis-driven mode to the data-driven mode' has 'revolutionized biology' is debatable (I return to this in a later chapter). And whether or not the data generated by my participation in Coursera's courses on machine learning generates data supports understanding of fundamental questions about learning also seems an open question.  Nevertheless, the loopiness of this description interests and appeals to me. I learn about machine learning, a way for computer models to optimise their predictions on the basis of 'experience'/data, but at the same time, my learning is learned by machine learners. This is not something that could happen very easily with a printed text, although versions of it happen all the time as teachers work with students on reading texts.  While Coursera and other MOOCs promise something that mass education struggles to offer (individually profiled educational services), it also negatively highlights the possibility that machine learning in practice can, somewhat recursively, help us make sense of machine learning as it develops. 

## The learning of machine learning

Perhaps even Ng is not enough to really track the trans-semiotic intensities of \ref{eq:linear_model}. How would one learn to read such diagrams less linearly (Ng's long columns of algebraic derivation) and more non-linearly, more diagonally? For a book with 'learning' in its title, _Elements of Statistical Learning_ has only a little to say about how to learn machine learning.  'Learning' is briefly discussed on the first page of _Elements of Statistical Learning_, but the book hardly ever returns to the topic or even that term explicitly. We can read on page 2 that a 'learner' in machine learning is a model that predicts outcomes.  (As I discuss in the next chapter, learning is comprehensively understood in machine learning as finding a _function_ that could have generated the data, and optimising the search for that function as much as possible.) The notion of learning in machine learning derives from the field of artificial intelligence. The broad project of artificial intelligence, at least as envisaged in its 1960s-1970s heydey, is today largely regarded as a failure. [TBA -- add Mitchell, Ripley and perhaps Bishop here] There is no general, or comprehensive artificial intelligence in existence, even though many efforts continue to develop 'deep learners' (see for instance, Google Corporation's ongoing research into 'deep learning' of its own data). But in the course of its failure, many interesting problems were generated. [TBA - references on the history of AI] The field of machine learning might be seen as one such offshoot since it treats machine learning as an algorithmic process to be supervised, monitored, optimised and otherwise enhanced by any means available (including in some machine learning competitions, resort to forms of prestidigitation that amount to pulling the hat out of the data rabbit: see [@Schutt_2013] for a discussion of the problems of 'data leakage in machine learning').

The so-called 'learning problem' and the theory of learning machines that was developed largely by researchers in the 1960-1970s was largely based on work already done in the 1950s on learning machines such as the perceptron, the  neural network model developed by the psychologist Frank Rosenblatt in the 1950s [@Rosenblatt_1958]. Drawing on McCulloch-Pitts model of the neurone, Rosenblatt implemented the perceptron, which today would be called a single-layer neural network [@Hastie_2009, 393] on a computer at the Cornell University Aeronautical Laboratory in 1957. For present purposes, it is interesting to see what diagrams Rosenblatt used and how they differ from contemporary diagrams.[^40]

[^40]: Elizabeth Wilson's study, _Affect and Artificial Intelligence_ [@Wilson_2010], draws on a combination of psychoanalytic, psychological and archival materials discussing the work of key figures in the early history of artificial intelligence such as Alan Turing on intelligent machinery,  Warren McCulloch and Walter Pitts on neural nets, and recent examples of affective computing and robots such as the MIT robot Kismet. Her framing of the psychic nexus with machines such as the perceptron is provocative:
    
    > Sometimes machines are the very means by which we can stay alive psychically, and they can  just as readily be a means for affective expansion and amplification as for affective attenuation. This is especially the case of computational machines [@Wilson_2010, 30].

    Under what conditions do machines and for present purposes, computational machines, become 'the very means we can stay alive psychically'? Wilson  addresses this question by positing 'some kind of intrinsic affinity, some kind of intuitive alliance between the machinic and the affective, between calculation and feeling' (31), and suggesting that the 'one of the most important challenges will be to operationalize affectivity in ways that facilitate pathways of introjection between humans and machines' (31). Introjection, the process of bringing the world within self is, according to psychoanalytic accounts of subjectivity, crucial to the formation of 'a stable subject position' (25). Wilson envisages introjection of machine processes as a good, not as a failure or attenuation of relation to the world. Note that her use of introjection differs strikingly from the sense of introjection invoked by Paolo Virno in his account of contemporary production, but both Wilson and Virno retain a commitment to the primacy of psychic processes in the human-machine nexus. While I tend to go in the same direction as Wilson in relation to affective expansion, I don't tend to see that expansion as unfolding from introjection, but rather from an intensification of diagrammatic processes.

\begin{figure}
  \centering
      \includegraphics[width=0.5\textwidth]{figure/perceptron_rosenblatt_1958_389.pdf}
        \caption{The neurological perceptron (Rosenblatt, 1958, 389)}
  \label{fig:perceptron_1958}
\end{figure}

If we compare \ref{fig:perceptron_1958}  to a typical contemporary diagram of a neural network, the differences are not that great in many ways.

![Single layer feedforward artificial network [@Hastie_2009, 393]](figure/ann_hastie_2009_393.pdf)

The diagram of a neural network found in _Elements of Statistical Learning_ is very typical. Almost identical diagrams will be found in any number of places. The contrast between the two figures, if we leave aside the typography and the artefacts relating to the how the diagrams were produced (on a typewriter or a vector-based drawing software) relate mainly to the labelling. Figure 1 from [@Rosenblatt_1958] has no mathematical symbols on it, but the one from [@Hastie_2009] does. Rosenblatt's retains neuro-cognitive-anatomical reference points (retina, association area, projection area) whereas Hastie et. al.'s replaces them with the symbols that we have already seen in play in the expression for the linear model \ref{eq:linear_model}. What has happened between the two diagrams?

As Vladimir Vapnik, a leading machine learning theorist, observes: 'the perceptron was constructed to solve pattern recognition problems; in the simplest case this is the problem of constructing a rule for separating data of two different categories using given examples' [@Vapnik_1999, 2]. While computer scientists in artificial intelligence of the time, such as Marvin Minsky and Seymour Papert, were sceptical about the capacity of the perceptron model to distinguish  or 'learn' different patterns [@Minsky_1969], later work showed that perceptrons could 'learn universally'[^19]. For present purposes, the key point is not that neural networks have turned out several decades later to be extremely powerful algorithms in learning to distinguish patterns, and that intense research in neural networks has led to their ongoing development and increasing sophistication in many 'real world' applications (see for instance, for their use in sciences [@Hinton_2006], or in commercial applications such as drug prediction [@Dahl_2013] and above all in the current surge of interest in 'deep learning' by social media platform and search engines such as Facebook and Google). Rather, the important point is that the notion of the learning machine began to establish an ongoing diagrammatization in which transformations of that basic diagrammatic pattern through substitutions of increasingly convoluted or nested diagrams could proceed. The whole claim that machines 'learn' rests on this diagrammatization that recurrently and sometimes recursively transforms the icon of relations, sometimes in the graphic forms shown above and more often in the algebraic patterns. 

[^19]: In describing the entwined elements of machine learning techniques, and citing various machine learning textbooks, I'm not attempting to provide any detailed history of their development. My knowledge of these developments is not properly historical. It does not come out of the archives of institutions, laboratories or companies where these techniques took shape. It derives much more either from following citations back out of the highly distilled textbooks into the teeming collective labour of research on machine learning as published in hundreds of thousands of articles in science and engineering journals, or from looking at,  experimenting with and implementing techniques in code. For instance,  [@Olazaran_1996] offers a history of the perceptron controversy from a science studies perspective. During the 1980s, artificial intelligence and associated approaches (expert systems, automated decision support, neural networks, etc) were a matter of some debate in the sociology and anthropology of science. The work of Harry Collins would be one example of this [@Collins_1990], of Paul Edwards on artificial intelligence and Cold War [@Edwards_1996], or Nathan Ensmenger on chess [@Ensmenger_2012],  and the work of Lucy Suchman on plans and expert systems [@Suchman_1987] would others. Philosophers such as Hubert Dreyfus (_What Computers Can't Do_ [@Dreyfus_1972; @Dreyfus_1992] had already extensively criticised with AI.  In the 1990s, the appearance of new forms of simulation,  computational fields such as a-life and new forms of robotics such as Rodney Brook's much more insect-like robots at MIT attracted the interest of social scientists [@Helmreich_2000] amongst many others. Sometimes this interest was critical of claims to expertise (Collins), and at other times, interested in how to make sense of the claims without foreclosing their potential novelty (Helmreich). By and large, I don't attend to controversies in machine learning as a scientific field, and I don't directly contest the epistemic authority of the proponents of the techniques. I share with Lucy Suchman an interest in how the 'effect of machine-as-agent is generated'  and in how 'translations  ... render former objects as emergent subjects'[@Suchman_2006,2]. I diverge around the site of empirical attention. I'm persevering with the diagrams in each of the following chapters in order to track the movement of tendencies that are not so visible in terms of either the controversies or the assumptions of agency embodied in many AI systems of the 1980s. It's certainly not that I think these approaches are wrong, only that they don't access the diagrammatic movements of the operations we are discussing across different settings. The agency of machine learning, in short, might not reside so much in any putative predictive or classificatory power if manifests, but rather its capacity to mutate and migrate across contexts. 

## What the perceptron latches onto

I am suggesting, then, that we should follow the transformations of diagrams associated with machine learning. Following Peirce, we might begin to see machine learning as a diagrammatic practice in which different forms of diagram are constantly connected, substituted, embedded or created from existing diagrams. The diagrams we have already seen from _Elements of Statistical Learning_ - algebraic formulae and network topology - don't exhaust the variations at all. Just a brief glance through this book or almost any other in the field shows not only many formulae, but tables, matrices, arrays, line graphs, contour plots, scatter plots,  dendrograms and trees, as well as algorithms expressed as pseudo-code. The connections between these diagrams are not always very tight or close. Learning to machine learning (whether you are a human learner or a learner in the sense of a machine) means dancing between diagrams. This dance is relatively silent and sometimes almost motionless as signs slide between different diagrammatic articulations. Diagrammatization offers then a way to track the ongoing project which tries treat data like farmers treat crops (see epigraph from Domingos in this chapter).  To understand what machines can learn, we need to look at how they have been drawn, designed, or formalised. But what in this work of designing and formalising predictive models is like farming? Some very divergent trajectories open up here. On the one hand, the diagrams become machines when they are implemented. On the other hand, the machines generate new diagrams when they function. We need to countenance both forms of movement in order to understand any of the preceding diagrams -- the algebraic expressions or the diagrams of models such as the perceptron or its descendants, the neural network.  This means going downstream from the textbooks into actual implementations and places where people, algorithms, and machines mingle more than they do in the relatively neat formality of the textbooks. Rather than history or controversies in the field, I focus on the migratory patterns of  methods, and the many configurational shifts associated with their implementations as the same things appears in different places. 

One line of movement in this diagrammatic dance runs from what we might called symbolic logical diagrams to statistical algorithmic diagrams. While many of the machine learning techniques I discuss have quite long statistical lineages (runnning back to the 1900s in the case of Karl Pearson's development of the still-heavily used Principal Component Analysis [@Pearson_1901]), machine learning techniques owe much to a certain dissatisfaction with the classical AI understanding of intelligence as manipulation of symbols. Symbolic intelligence, epitomised by deductive logic or predicate calculus, was very much at the centre of many AI projects during the 1950s and 1960s [@Dreyfus_1972; @Edwards_1996].

In this line of diagrammatization, certain privileged symbolic-cognitive forms are subject to a kind of statistical transformation. Take for instance once of the most common operations of the Boolean logical calculus, the NOT-AND or NAND function.

X1  X2  X3  Y
--  --  --  --
0   0   0   1
0   0   1   1
0   1   1   1
1   0   0   1
1   0   1   1
1   1   0   1
1   1   1   0

Table: the Boolean function  NOT-AND truth table

For our present purposes, these kinds of logical and symbolic diagrams have triple relevance. First, the spatial arrangement of the table is really so fundamental to not only machine learning but to most contemporary data practices. That spatial form is diagrammatic in various ways, but principally through the horizontal and vertical relationships it installs everywhere. Most datasets come as tables, or end up as tables at some point in their analysis. Second, the elements or cells of this table are numbers. These numbers, $1$ and $0$ are the binary digitals as well as the 'truth' values 'True' and 'False' in classical logic. The truth table for NOT-AND is in some ways typical of data tables because it contain numbers. Unusually, in comparison to most data tables, these numbers are read as symbolic logical propositions. Hence this table acts as a hinge between numbers and symbolic thought or cognition. (In [@Hastie_2009] most tables contain data in the forms of numbers rather than truth values.) Third, following on directly from this, the NAND  table in particular has an obvious operational importance in digital logic, since digital circuits of all kinds -- memory, processing, and communication -- comprise such logical functions knitted together in intricate fabrics.[^30]  Running across these different kinds of relevance, the obviousness of this logical operation stands out. It is hardly surprising to us at all. This looks like the kind of mechanistic thought that computers can do. 

[^30]: Peirce himself had first shown that combinations of NAND operations could stand in for any logical expression whatsoever, thus paving the way for the diagrammatic weave of contemporary digital memory and computation in all their permutations. Today, NAND-based logic is norm in digital electronics. NOR -- NOT OR -- logic is also used in certain applications.  

How, by contrast, could such a truth table be learned by a machine, even a machine whose _modus operandi_ and indeed whose very fabric is nothing other than a massive mosaic of such operations inscribed in transistor circuits? Machine learning of such truth tables is not a typical operation today, but does illustrate something of the diagrammatic transformations that classical arAI has undergone. If we return to the perceptron, with its neural intuition, how could it learn logic? It knows nothing of the logical calculus, but it can be induced to take on a logical shape by training it. For instance, a  perceptron that 'learns' the binary logical operation NAND (Not-AND) is  expressed in twenty lines of python code on the Wikipedia 'Perceptron' page [@Wikipedia_2013]. The code is followed by the output  -- a series of numbers -- produced when it runs. 

```{r perceptron, echo=TRUE, cache=TRUE, message=FALSE, warning=FALSE, comment=NA, size='smallsize', results='markup', engine='python' }
    
threshold = 0.5
learning_rate = 0.1
weights = [0, 0, 0]
training_set = [((1, 0, 0), 1), ((1, 0, 1), 1), ((1, 1, 0), 1), ((1, 1, 1), 0)]
     
def dot_product(values):
    return sum(value * weight for value, weight in zip(values, weights))
     
while True:
    print '-' * 60
    error_count = 0
    for input_vector, desired_output in training_set:
        print weights
        result = dot_product(input_vector) > threshold
        error = desired_output - result
        if error != 0:
            error_count += 1
            for index, value in enumerate(input_vector):
                weights[index] += learning_rate * error * value
    if error_count == 0:
        break
```

What does this code show or say? First of all, we should note the relative conciseness of the code vignette. In citing this code, I'm not resorting to a technical publication or scientific literature as such, or even to a software library or package, just to that popular yet incredibly heavily used epistemic resource, the Wikipedia page, and the relatively generic and widely used programming language `python`.[^31] While perceptrons and neural networks are the topic of a later chapter, the typical features of this code as the implementation of a machine learning algorithm are the presence of the 'learning rate', a 'training_set,' 'weights', an 'error count', and a loop function that  multiplies values ('dot_product'). Some of the names such as `learning_rate` or `error_count`  present in the code bear the marks of the theory of learning machines that we will discuss. Much of the code here is much more familiar, generic programming. It defines variables, sets up data structures (lists of numerical values), checks conditions, loops through statements or prints results. Executing this code (by copying and pasting it into a python terminal, for instance) produces several dozen lines of numbers that are initially different to each other, but that gradually converge on the same values (see printout). These numbers are the 'weights' of the nodes of the perceptron as it iteratively learns to recognise patterns in the input values. None of the workings of the perceptron need concern us at the moment. It is a typical machine learning algorithm, almost always included in machine learning textbooks and usually taught in introductory machine learning classes.  Perhaps more strikingly than its persistence as an algorithm over half a century, the implementation of the whole algorithm in twenty lines of code on a Wikipedia page suggests the mobility of these methods. What required the resources of a major university research engineering laboratory in the 1950s can be re-implemented by a cut and paste operation between Wikipedia pages and a terminal window on a laptop in 2014. This is now a familiar observation, and perhaps not very striking at all. Again, what runs across all of these observations are the numbers that the algorithm produces. The NAND truth table has been re-drawn as a list of tuples or sets (see line 4 of the code that defines the variable `training_set`). The perceptron has learned the truth table by being given it as a set of training examples, and then adjusting its internal model -- the weights that are printed during each loop of the model as the output --  repeatedly until the model is producing the correct values of the truth table. The algorithm exits it main loops (`while True:`)  when there are no errors. The surprising result here is that a model or algorithm implemented in digital logic has learned a basic rule of digital logic at least approximately. This transformation in learning style is symptomatic of the broader transformations that  machine learning latches onto. The learning done in machine learning has few cognitive or symbolic underpinnings. It differs from classical AI in that it takes existing symbolic and, increasingly, signifying processes (such as the cat faces that `kittydar` tries find), and latch onto them diagrammatically. The diagrammatic dance between different algebraic, geometrical, algorithmic and tabular forms of expression is where the learning occurs. 

[^31]: There is much to discuss about programming and code in machine learning. I  return to that below. The important point for present purposes is that this is pretty much vanilla standard stand alone `python` code. There are no esoteric libraries or dependencies here. As is often the case with  machine learning, the abstract machines are quite simple, but their potential relationality is complex. 

The perceptron algorithm produces numbers  - $0.79999$, $2.0$ -- as weights. These weights display no direct correspondence with the symbolic categories of boolean True and False or the binary digits $1$ and $0$. There may be a relation but it is not obvious at first glance. The problem of mapping these numbers -- and they truly abound in machine learning -- triggers many different diagrammatic movements in _Elements of Statistical Learning_ and the like texts. The variety of more statistical diagrams that result will be discussed in later terms, but even here we should take note of the disjunction or cut between symbolically organised diagrams like the NAND truth table and the diagrams of a machinic configuration printed as weights. The figure \ref{fig:model_complexity} shows something of the density of the numbers produced by  machine learning machines in operation. The profuse lines drawn here stand for various implementations of a model with different parameters (or weights). At the moment, we are not in a good position to read this diagram. The main point is that figures like \ref{fig:model_complexity} point to a world of machinic processes moving into and through data without much reference to the privileged forms of symbolic logic or signification.

\begin{figure}
  \centering
      \includegraphics[width=0.5\textwidth]{figure/error_plot_hastie_2009_220.pdf}
        \caption{Model complexity}
  \label{fig:model_complexity}
\end{figure}

## A machine learning praxiography? Re-implementing statistical methods in `R`

What would we learn by studying the proliferation of implementations of artificial intelligence or machine learning algorithms rather than their history or the controversies associated with them? Science studies scholars such as Anne-Marie Mol urged the need to keep practice together with theories of what exists. Towards the beginning of _The Body Multiple: Ontology in Medical Practice_ [@Mol_2003], Mol writes:

> If it is not removed from the practices that sustain it, reality is multiple. This may be read as a description that beautifully fits the facts. But attending to the multiplicity of reality is also an act. It is something that may be done – or left undone [@Mol_2003, 6]

Mol's work offers a cogent case for developing accounts of what is real steeped in the practices that make it real. While similar sounding affirmations of the underpinning role of practice can be found in many parts of social sciences and humanities (since who would not affirm the centrality of practice?), Mol's insistence on this nexus of practice or doing and the existence of things in their plurality offers another way forward in reading _Elements of Statistical Learning_.  Tracking techniques -- a crystallised form of practice -- and the flow of their implementations is a way of keeping practices in their multiplicity. Describing machine learning in terms of practices could be an act that attends to their multiplicity. Mol's coins the term  *praxiography*, a variant on ethnography, to refer to an act of describing practice in the name of preserving their multiple-making value. This term has particular resonance for work with data, which is itself always heavily entwined in writing and reading practices. The practice, I have been suggesting, is principally diagrammatic. Could we praxiographically attend to data and machine learning in terms of diagrams?

On this point, programming languages and snippets of code present some useful threads to follow. Nearly all of the examples in _The Elements of Statistical Learning_ are implemented in  a single programming language, `R`. The proliferation of programming languages such as `FORTRAN` (dating from the 1950s), `C` (1970s), `C++` (1980s),  then `Perl` (1990s), `Java` (1990s), `Python` (1990s)  and `R` (1990s), and computational scripting environments such as Matlab, multiplied the paths along which  implementation of machine learning techniques could proceed. It would be impossible for anyone to follow the propagation of techniques across domains of science, business and government without paying attention to those coding practices. Even if textbooks and research articles are not read, software packages and libraries for  machine learning can be used. They have a quasi-autonomy that extends the diagrammatic practices of machine learning into a variety of settings and places where equations, statistical plots, and citations of research articles would not be common. Other book machine learning books, especially those written for programmers and software developers [@Schutt_2013; @Segaran_2007; @Russell_2013; @Kirk_2014;@Conway_2012], differ somewhat from this in that they tend to heavily emphasize code and configuration work needed to put machine learning to work. A variety of different programming languages appear in these books, but `R` and `Python` are the most common (and for this reason, almost all of the code vignettes in this book appear in either `R` or `python` code). 


    ` install.packages('ElemStatLearn', dependencies='Suggests', repos = 'http://cran.us.r-project.org')`

The single line of code shown above installs `ElemStatLearn`, a `R` programming language package [@Halvorsen_2012], as well its 'dependencies' from a software repository located at http://cran.us.r-project.org. The domains and sub-domains of the URL bear closer examination. A .org domain suggests something outside commerce, education, or government. The proper name 'r-project' suggests that this organisation is involved in an ongoing process, a project not a product or a service. The 'us,' which stands for USA, connotes geography and nations, but implies by its forward position in the URL that nation is not the prime consideration, and that there might be other nations ('uk', 'tw', 'au', 'de') somehow present.  

```{r elem_stat_learn_install, echo=TRUE}
 install.packages('ElemStatLearn', dependencies='Suggests', repos = 'http://cran.us.r-project.org')
```

This line of code affords some other ways of reading the book and the dragline extension of practice running through it a bit more deeply. There is nothing too opaque, I would suggest, about the line of code itself, but there are some folds and convolutions associated with it that open fresh perspectives. Take the part of the line `dependencies = 'Suggests'`. When the line of code executes, this stipulation of `dependencies` leads to a quite wide-ranging installation event. If the installation works (and that assumes quite a lot of configuration and installation work has already taken place; for instance, installing a recent version of the `R` platform), then the book is now augmented by various pieces of code, and by various datasets.

```{r elem_stat_learn_dependencies, echo=TRUE}
library(tools)
pack = available.packages()
pack_df = as.data.frame(pack)
pack_df['ElemStatLearn',c('Depends', 'Imports','Suggests' )]
head(sort(table(pack_df$Suggests), decreasing=TRUE),30)
```

Most of the packages associated with the `ElemStatLearn` implement methods or techniques developed by Hastie, Tibshirani or Friedman, but some are much more generic. `MASS` for instance is highly cited `R` package.  (Of the `r nrow(pack_df)` packages in the R CRAN system, `r sum(grepl(pack_df$Depends, pattern='MASS'))` depend on the library `MASS`, itself an adjunct to the influential and highly cited _Modern Applied Statistics with S_ [@Venables_2002], a textbook that presents many machine learning techniques using `S`, AT & T Bell Labs commercial precursor to the open sourced `R`). For our purposes, this hardly accidental mixing of academic or research work with a programming languages and its associated infrastructures is fortuitous. It allows us to transit between different strata of the social fields of science, engineering, health, medicine, business media and government more easily. 



## How does code work in machine learning

The media theorist Ian Bogost advocates  code  as a way to make sense of the world: 'source code itself often offers inroads in alien phenomenology - particular when carpentered to reveal the internal experience of withdrawn units' [@Bogost_2012, 105]. Not a single line of code appears in _Elements of Statistical Learning_.[^91] Yet we see many figures, many equations and many tables of numbers and statistics in the book. 

[^91]: In a latter book by some of the same authors with the very similar sounding title _An Introduction to Statistical Machine Learning with Applications in `R`_ [@James_2013], the `R` does appear in abundance. This book, however, is much shorter and lighter in various ways. 

```{r elem_stat_learn_figures, echo=FALSE}
system('pdftotext -f 10 -l 716 ../ml_lit/data/hastie_elem/hastie_elements_2009.pdf hastie_main.txt')
main_text = readLines('hastie_main.txt')
figure_count = length(grep(main_text, pattern='FIGURE \\d{1,2}\\.\\d{1,3}'))
table_count = length(grep(main_text, pattern='TABLE \\d{1,2}\\.\\d{1,3}'))
algorithm_count = sum(grepl(main_text, pattern='Algorithm \\d{1,2}\\.\\d{1,3}'))
device_count = figure_count  + table_count + algorithm_count
equation_count = sum(grepl(main_text, pattern='\\(\\d{1,3}\\.\\d{1,3}\\)$'))

```

Beginning from almost the first pages proper of the book, almost every page has a figure or a table or a formal algorithm (counting these together: figures = `r figure_count`; tables = `r table_count`; and algorithms = `r algorithm_count`, giving a total of `r device_count` operational devices threaded through the book). Adding equations here would massively increase the total. As we have already seen, around `r equation_count` equations rivet the text into mathematical abstractions of varying sophistication, and construct an semiotic machinery of considerable sophistication and connectivity.  The point is that on each page of the book we are seeing, reading, puzzling over and perhaps learning from the products of code execution. The figures are all produced by code. The tables are mostly produced by code. The algorithms specify how to implement code, and the equations diagram the various operations, spaces and movements that run through the algorithms. 

`R` is an increasing well-known and widely used statistical programming language and environment  [@RDevelopmentCoreTeam_2010]. An open source programming language, according to surveys of business and scientific users, at the time of writing, `R` has replaced popular statistical software packages such as SPSS, SAS and Stata as the statistical and data analysis tool of choice for many people in business, government, and sciences ranging from political science to genomics, from quantitative finance to climatology [@RexerAnalytics_2010]. Developed in New Zealand in the mid-1990s, and like many open source software projects, emulating `S`, a commercialised predecessor developed at AT&T Bell Labs during the 1980s, `R` is now extremely widely used across life and physical sciences, as well as quantitative social sciences. John Chambers, the designer of `S`, was awarded the Association for Computing Machinery (ACM) 'Software System Award' in 1998 for 'the S system, which has forever altered how people analyze, visualize, and manipulate data' [@ACM_2013]. Many undergraduate and graduate students today earn `R` as a basic tool for statistics. Skills in `R` are often seen as essential pre-requisite for scientific researchers, especially in the life sciences. (In engineering, `Matlab` is widely used.) Estimates of its number of users range between 250000 and 2 million. Increasingly, `R` is integrated into commercial services and products (for instance, SAS, a widely used business data analysis system now has an `R` interface; Norman Nie, one of the original developers of the SPSS package heavily used in social sciences, now leads a business, Revolution, devoted to commercialising `R`; `R` is heavily used at Google, at FaceBook, and by quantitative traders in hedge funds; in 2013 ['R usage is sky-rocketing'](http://www.r-bloggers.com/r-usage-skyrocketing-rexer-poll/); etc.). 

`R` is an interestingly diffuse entity. Some ways of working with data are way more clearly focused on equations and calculation. For instance, in order to pursue number practices in physical sciences and engineering, maybe MATLAB or Mathematica would be better. In business or government, many ways of working with data are more focused on ordering and searching. For instance, in order to the look at the organisation of large aggregates of data, relational databases, query languages, data-centre architectures, and perhaps the techniques of aggregating and disaggregating data en-masse would be worth studying. 

Why then choose `R`, a statistical programming language, a language developed largely by statisticians and research scientists rather than computer scientists, software engineers or hackers? In general terms, `R` has a kind of polyglot currency as a form of expression, it exhibits a heterogeneous variety of approaches, styles and emphases, and it has a fine-grained relationality with many different epistemic and operational situations. One more localized justification is that much of the recent development, demonstration, implementation and adoption of machine learning techniques takes the form of `R` code. Research articles and texbooks in statistics commonly both use `R` to demonstrate methods and techniques, and create `R` packages to distribute the techniques and sample data. For instance, an article published in 2007 by the Princeton computer scientist David Blei on 'correlated topic models' for classification of documents [@Blei_2007] leads to a package 'topicmodels.' Nearly all of these publication-related software packages, including quite a few from the authors of _Elements of Statistical Learning_ are soon or later available from the 'Comprehensive `R` Archive Network (CRAN)' [@CRAN_2010].[^50] CRAN itself is an important infrastructure in the global life of `R`. 

[^50]: I return to topic models in Chapter 5 in the context of the discussion of probability

```{r mirrors, echo=TRUE, cache=TRUE, message=FALSE, warning=FALSE, comment=NA, size='smallsize', results='markup' } 
    
    library(utils)
    mirrors = getCRANmirrors(all = FALSE, local.only = FALSE)
    head(mirrors[, c(1, 4, 5)]
```

A set of `r nrow(mirrors)` mirror sites in `r length(unique(mirrors$Country))` countries distributes the latest `R` platform and `R` packages to many different users. These mirror sites are largely run by universities and public institutions, alongside a few commercial users (such as Revolution, a company I discuss in a later chapter). So, in terms of machine learning practice, `R` is important because it is a staple programming language in statistics research and teaching. It circulates globally. At the same time, it is stunningly specialised in manifold ways. Karl Marx wrote of the 500 different hammers made in Birmingham, 'not only is each adapted to one particular process, but several varieties often serve exclusively for the different operations in one and the same process' [@Marx_1986, 375]. Something similar holds in `R`: thousands of software packages in CRAN suggest that a highly specialised division of labour and possibly refined co-operative labour processes operate around data. Not only does `R` change the way people work with data, it offers the chance to see how people work with data. The processes of production of data, the building of models, the modelling of states of affairs, and many specialisations and adaptations of data techniques are written in  `R` and can be read there too.[^52]

[^52]: Less pragmatically, but symptomatically, `R` has also attracted  mainstream media attention. An article in _The New York Times_ in 2009 highlighted its practical importance in data analysis [@Vance_2009], at the time  _The New York Times_ was heavily promoting its idea of data journalism.   This is somewhat unusual, as programming languages are not normally the topic of mainstream media interest.   `R` is an evocative object, to use the psychoanalyst Christopher Bollas' term [@Bollas_2008], an object through which many different ways of thinking circulate. Standing somewhere at the intersection of statistics and computing, modelling and programming, many different disciplines, techniques, domains and actors intersect in `R`. Bollas suggests that 'our encounter, engagement with, and sometimes our employment of, actual things is a _way_ of thinking' [@Bollas_2008, 92].  `R` embodies something of plurality of practices of working with measurements, numbers, text, images, models and equations, with techniques for sampling and sorting, with probability distributions and random numbers. It engages immediately, practically and widely with  words, numbers, images, symbols, signals, sensors, forms, instruments and above all abstract forms such as mathematical functions like probability distributions and many different architectural forms (vectors, matrices, arrays, etc), as it employs data. By virtue of thousands of packages that flow across boundaries between nature and culture, between aesthetic, epistemic and pragmatic domain, `R` embodies a wide-band broadcast of data techniques through  contemporary economies, cultures, sciences, politics and technologies. 

Programming and writing code for data analysis has widely supplanted the use of statistical software applications, at least in statistical research. Writing code has always been  central in machine learning where algorithms are the primary expressive forms that ideas take as they become diagrams. Finally, the two of the main proponents of `R` and `S` describe the motivation for the language:
> The goal of the S language ... is "to turn ideas into software, quickly and faithfully" ... it is the duty of the responsible data analysts to engage in this process ... the exercise of drafting an algorithm to the level of precision that programming requires can in itself clarify ideas and promote rigorous intellectual scrutiny. ... Turning ideas into software in this way need not be an unpleasant duty. [@Venables_2000, 2]

Bill Venables and Brian Ripley, statisticians working on developing `S`, the almost identical commercial predecessor to `R`,   wrote in the early 1990s of  the responsibility of data analysts to write not just use software.   They write 'software' here not in the sense of a product, but in the sense that today would more likely be called 'code.' This sense of coding and programming as clarifying and concretising ideas with precision has thoroughly taken hold in contemporary data analysis.

Many practices, many ways of doing things, describe themselves as part of their doing. Perhaps every practice contains elements of its own description. It is useful, in this context, to think of things like `R` as somewhat recursive descriptions of what they do. This is not to say that `R` as a collectively produced body of code is thinking (perhaps it is, at a highly diffuse level), but `R`  certainly does abound with descriptions of itself. It not only documents itself in the sense that a packet documents its contents with a label, but in the stronger sense that many of the basic operations and practices involved in working with R have descriptive components.   How would we describe the `R`-based practices in ways that deepen our perception of their plurality, their relevance, and potential amplification of affect? On balance, I'd say  that the 'employment' dimension of `R` dominates. Knowing `R` increases your chances of well-paid work.[^53] But other valencies can be found. 

[^53]:It is easy today to find an employment-centric view of data practice. The 'chief economist' at Google, Hal Varian in 2009 made a widely quoted prediction about working with data:

    > The ability to take data — to be able to understand it, to process it, to extract value from it, to visualize it, to communicate it — that’s going to be a hugely important skill in the next decades, not only at the professional level but even at the educational level for elementary school kids, for high school kids, for college kids. Because now we really do have essentially free and ubiquitous data. So the complimentary [sic] scarce factor is the ability to understand that data and extract value from it.” Hal Varian, chief economist at Google: [@McKinsey_2009]

    While cited here in a report prepared by the global business consultancy, McKinsey & Co, this quote, or selections from it, can be seen in many different contexts, ranging from government reports on higher education to student noticeboards in  university statistics departments.  What Varian, with all the allure of  Google as a potential employer, presents  as an 'important skill'  also has a 'scarce factor':  the 'ability to understand that data.' While Varian presents understanding data as the prelude to 'extract[ing] value from it,' understanding might take different forms, depending on the kind of encounter or engagement that precedes it. 

A somewhat broader framing of the value of `R` can be found voiced as a form of democratised knowing or engagement with data. Norman Nie, the original developer of the widely-used SPSS social science statistics software (see [@Uprichard_2008]), is a proponent of `R`. Norman Nie interviewed in _Forbes_, a leading business news magazine, evangelises for `R` in these terms:

> Everyone can, with open-source `R`, afford to know exactly the value of their house, their automobile, their spouse and their children--negative and positive  … It's a great power equalizer, a Magna Carta for the devolution of analytic rights [@Hardy_2010].

While still  referring to `R`  in ways that lie close to employment, Nie, the founder of Revolution Analytics, a company that provides software and support services to `R` users, promotes `R` as a way of protect individual liberties to know what they own. In the context of the global financial crisis of 2007, it may be that the value of houses became much more problematic, and understandably, harder to know. And the reference to 'spouse and their children' must be flippant. Yet the 'devolution of analytic rights' that Nie speaks of here, even as he frames it in terms of entrepreneurial individualism, is important. While Nie's company  Revolution Analytics promotes the incorporation of  `R` into  powerful and pervasive business and government prediction systems (such as SAS and IBM's Pure Data [@IBM_2013]), Nie here points to the problem of 'manipulation and control' of customers associated with just that incorporation. The 'analytic rights' he advocates in the form of `R` are meant to help individuals counterbalance the power of large scale data analysis conducted by corporations and governments.  More than any particular or specific use, Nie's advocacy of `R` taps into a wider  sense of rich possibility associated with `R`. It would be possible to cite many other instances of this belief and desire in the potential of `R`. A good indication of this overflow is the aggregate blog, [R-bloggers](R-bloggers.com). `R`-bloggers brings together several hundred `R`-related blogs in one place. The range of topics discussed there on any one day – Merrill-Lynch Bond Return indices, how to run `R` on an iPhone, using US Department of Agriculture commodity prices, analysing human genetic variation using PLINK/SEQ via `R`, using the 'Rhipe' (sic!) package to program big data applications on Map-Reduce cloud architectures, doing meta-analysis of phylogenetic trees, etc., etc (R-bloggers 2011) – suggest how widely desires/beliefs in `R` range. To take just one fairly recent example, '`R` Analysis Shows How UK Health System could save £200 million' claims a recent post on the site [@Smith_2012]. While many of the applications, experiments, or techniques reported there can be reduced to employment or to individual uses of `R`, they also, as we will see, suggest the potential for encounters and engagements. Something similar, and perhaps less bound to commercial data analytics can be found in Rachel Schutt and Cathy O'Neil's much more lively _Doing Data Science_ [@Schutt_2013] as it discusses the ethics and potentials of working with machine learning models and programming languages such as `R` and Python.

If we were to try to describe these encounters and engagements in which `R` as an actual thing is also a way of thinking, how would we go about it? The contrast between `R` as a language originating in statistics and widely used in various scientific fields and `Python,` a much more general purpose programming language widely used for software development in many settings is instructive. The implementations of machine learning algorithms in `Python` can more or less be found in a few software libraries such as [`SciPy`](http://www.scipy.org) and `ScikitLearn` [@Pedregosa_2011], but much of the instructional materials on how to use machine learning in practice rely on `Python`.  The act of describing the practices of working with `R` might encounter aspects of data practice that exceed employment, and vivify experience by making it a bit less predictable and perhaps more 'alien' in the sense Bogost values. It is perhaps far more typical to encounter data in the context of learning how to manipulate, sort, model or display it. These settings are multiple. We could turn to R-blogger's aggregation or the programmer Q&A site [Stackoverflow](http://www.stackoverflow.com), which has many questions tagged with `R`. There are also many online manuals, guides and tutorials relating to `R` [@Wikibooks_2013]. For present purposes, I draw mainly on semi-popular books such as _R in a Nutshell_ [@Adler_2010], _The Art of `R` Programming_ [@Matloff_2011], _R Cookbook_ [@Teetor_2011], _Machine Learning with R_ [@Lantz_2013] or _An Introduction to Statistical Learning with Applicatins in R_ [@James_2013]. These books are not written for academic audiences, although academics often write them and use them in their work. They are largely made up of illustrations and examples of how to do different things with different types of data, and their examples are typically oriented towards business or commercial settings, where, presumably, the bulk of the readers work, or aspire to work. Given a certain predisposition (that is, geekiness), these books and other learning materials can make for enjoyable reading. While they vary highly in quality, it is sometimes pleasing to see  the economical way in which they solve common data problems. This genre of writing about programming specialises in posing a problem and solving it directly. In these settings, learning takes place largely through following or emulating what someone else has done with either a well known dataset (Fisher's 'iris') or a toy dataset generated for demonstration purposes.  The many frictions, blockages and compromises that often affect data practice are largely occluded here in the interests of demonstrating the neat application of specific techniques. Yet are not those frictions, neat compromises and strains around data and machine learning precisely what we need to cleave to praxiographically? In order to demonstrate both the costs and benefits of approaching `R` through such materials, rather than through ethnographic observation of people using `R`, it will be necessary to stage encounters here with data that has not been completely transformed or cleaned in the interests of neatly demonstrating the power of a technique. One way to do this is by writing about code while coding. 

## Dataset as auratic diagrams

I sometimes have the feeling that machine learning wants to go every which way in the world, that it aims to take on any problem, that no data is too difficult for it to learn on. I won't multiply the citations, but this one from Matthew Kirk's _Thoughtful Machine Learning_ is typical:

> Machine learning is an amazing application of computation because it tackles problems that are straight out of science fiction. These algorithms can solve voice recognition, mapping, recommendations, and disease detection. The applications are endless, which is what makes machine learning so fascinating [@Kirk_2014, 11]

Certainly it is hard to find dissenting or sceptical voices (although Anand Rajaraman and David Ullman in their _Mining Massive Datasets_ do voice doubts about the power of machine learning algorithms [@Rajaraman_2010]). This epistemic imperium is not only due to the generic mathematical techniques. It might arise from the data, and in particular the indexical aura of the datasets that play across play across many pages in _Elements of Statistical Learning_.   The power of the techniques  hinges on a very specific diagrammatization of data that has already begun to appear in the NAND truth table. Machine learning literature does admittedly display a somewhat stunning variety of datasets. The plots of New Zealand fishing patterns lie next to plots of factors in South African heart disease. This seemingly diasporic variety of data as well as the variety of domains and situates the datasets index are salient components of the experience of reading these texts.  What do we make of the datasets? The datasets associated with the book can be read in various ways.  

```{r elemstatlearn_data, echo=FALSE, results = "asis"}
library(ElemStatLearn)
library(xtable)
datasets_results <-data(package='ElemStatLearn')$results[,3:4]
datasets = as.data.frame(datasets_results)
dataset_count <- nrow(datasets)
dataset_table = xtable(datasets, label = 'table:datasets_elemstatlearn')
print(dataset_table)

```

The `r dataset_count` datasets  shown in \ref{table:datasets_elemstatlearn'} typify the diaspora of domains found in [@Hastie_2009; @Hastie_2001]. As mentioned above (by myself, but also by Kirk),  their domains span the scientific, clinical, commercial and media fields. Note that they include speaking, seeing, writing and reading, as well as the broader concerns of galaxies, climate and national economies. This span of interests is not unusual. To give a another example, one of the most cited journal papers in the machine learning literature is Leo Breiman's 2001 article on random forests [@Breiman_2001]. In some ways, it simply continues what statistics as a field has always done: rove  across scattered fields ranging from astronomy to statecraft, from zoology to epidemiology, gleaning data as it goes (see [@Stigler_1986; @Hacking_1990] for samples of this trekking movement). 

In my reading of _Elements of Statistical Learning_, something more was going on with these datasets. Their diversity is almost aleatory, as if the datasets were the fruit of random derivè in the world.  Rather than random drifting, I think it more likely the field of machine learning performs one of its own statistical techniques in 'bagging' (bootstrap aggregating [@Hastie_2009, 300]) datasets in order to average its predictions out across different domains. While the datasets span many domains -- vowels, ozone, bone density, marketing, spam, and oranges -- their diversity  predicates interchangeable demonstrations of techniques. The shape and organisation of these datasets attest to a certain set of alignments or architectures even that we should pay close attention to. The tabular form, the practices of naming and labelling, and the sorting of different data types exemplified in these diverse datasets can tell us a lot about how machine learning organises and energises its movement through worlds. For instance, the different shapes and composition of the datasets could be read as providing indications of the spatial forms that  machine learning algorithms actually can grapple with. 

[^71]: Glass,  Breast cancer,  Diabetes,  Sonar,  Vowel,  Ionosphere,  Vehicle,  Soybean,  German credit,  Image,  Ecoli,  Votes,  Liver,  Letters,  Sat-images,  Zip-code,  Waveform,  Twonorm,  Threenorm,  Ringnorm,  [@Breiman_2001, 12].

[HERE] -- basically take this all out from here to conclusion?

There are many different ways to encounter data. Let us imagine it happens somewhat by chance, as when walking down a city street we notice something about a building for the first time. Some feature, old or new, attracts our attention. What is happening in such situations? One possibility is that we find what we need or see what we want. From a psychoanalytic perspective, as Christopher Bollas writes, 

> the ability to move freely in the object world, to use its thing-ness as a matrix for thinking-by-action, pivots around whether the aleatory object determines - whether we move on quickly due to the dynamic of our last encounter with the object - or whether we select objects because we are unconsciously grazing: finding food for thought that only retrospectively could be seen to have a logic [@Bollas_2008, 93].

Bollas' description of moving in the world has a flavour of the _flaneur_, a now classic emblem of modern urban experience, and a figure whose experience of the city was an important in understanding city life.  I'm not sure I like that as an emblem of working with data.  It seems that the gendered subject of this experience, the man who encounters data and contemporary data practices, would remain intact, more or less as they were, having found what they wanted or needed. 

This is not to say that  what Bollas describes has no resonance with the many developments in data architectures increasingly designed to invite grazing.   Something _flaneurish_ is happening to data and data techniques. There are many potentially alluringly abundant and doubtlessly  important datasets on offer, especially when we consider the increasingly commodified offering of huge realtime data streams by social media platforms via Application Programming Interfaces (APIs). Moreover, public scientific databases across the gamut of disciplines, social media datasets, government statistical data, data from various environmental sensors, financial market and other price data (so much of this!), let alone all the media forms -- images, text, video -- that can be rendered into data, congregate on the internet. Even researchers within specific fields struggle to 'discover' the data  in their field. Hundreds of databases dot scientific fields like genomics    Furthermore, the 'tool-being' of data analysis techniques, as we have already seen in the case of `R`, stocks thousands of things  in easily browsed archives. Even in the domain of machine learning itself, which we have hardly begun to discuss, we are confronted with a many coloured and seemingly interminable mosaic of different techniques.   

Our encounters with all these are necessarily somewhat 'aleatory' or chancy, even if they could retrospectively turn out to have a logic. To give just a small sample of this interminable chain of possible connections from which data might be drawn, we could turn to the World Bank databases. Via the [World Bank Data catalog](http://data.worldbank.org/developers/data-catalog-api), the World Bank publishes data on global economic activity. Much of this data can be retrieved from its Application Programmer Interface (API) using `R`  or Python code. Until relatively recently, most people, apart from economists or the occasional journalist,  would have little chance of directly encountering this data. How can its newly available thing-ness become a 'matrix for thinking-by-action'? The data can be downloaded using a web browser, and then imported into a spreadsheet, but in this case, since we are interested in how a programming language affects encounters with data, it is important to see how  whether code  encounters the object-world of World Bank data differently. The lines of code, unlike a series of mouse movements, pointing, clicking and typing into a spreadsheet, make it possible to both track the movements and gestures involved much more directly, but also to change their scale through repetition and variation. (Ironically, coding by hand slows down practice in some ways. Sometimes code is criticised for its unreadability or inaccessibility. Actually, in many ways, code makes it possible to follow practice much more closely, albeit at the cost of more reading.)

```{r worldbank_R, echo=TRUE, cache=TRUE, message=FALSE, warning=FALSE, comment=NA, size='smallsize', results='markup' } 

    library(RCurl)
    library(rjson)
    url = 'http://api.worldbank.org/countries?incomeLevel=OEC&format=json'
    high_income_countries = fromJSON(getURL(url))
    countries = unlist(sapply(high_income_countries[[2]], '[', 'name'), use.names=FALSE)[1:10]
```

Or in Python:

```{r wbdata_python, echo=TRUE, cache=TRUE, message=FALSE, warning=FALSE, comment=NA, engine='python' } 

    # a package for working with World Bank API
import  wbdata 
    # fetch the names of economically developed countries
countries = [i['id'] for i in wbdata.get_country(incomelevel="OEC", display=False)]
    # select the economic indicators we wish to view
indicators = {"IC.BUS.EASE.XQ": "doing_business", "NY.GDP.PCAP.PP.KD": "gdppc"}
    # fetch that data
df = wbdata.get_dataframe(indicators, country=countries, convert_date=True)
    # output summary of values
df.describe()
```

Now the World Bank, being a bank after all, makes this numerical data available in order to facilitate business, economic growth, investment, international trade and so forth. For instance, some of the numbers downloaded in the vignettes shown above on 'doing business'  represents measures of how easy it is do business in various countries, and the World Bank wants to liberalize trade. Given that liberalizing trade is not the issue here for us, what do we get from looking at these lines of code, or the data they pull down from the World Bank API?  Leaving aside the whole question of what these numbers do (different kinds of numbers are the main topic of  a later chapter), the code itself is strikingly brief. The lines tersely invoke whole infrastructures.  The accumulated work of collating many different observations and measurements can be marshalled and assembled in a few lines of code. I have written lines like the first  five lines of `R` code shown here many times. They superimpose internet protocols (supplied via the RCurl library), data standards (rjson), internet servers (api.worldbank.org), databases (the 'countries?incomeLevel...' is a database query), data structures and algorithms ('unlist'). In this case they request a list of high-income countries (members of the OECD) from the WorldBank databases. The World Bank database returns a nested list describing `r length(countries)` countries. The list is hardly surprising (`r paste(countries[1:7],collapse=', ')` ... ). But the point of this code is to show something of the economy of movement and reshaping of data associated with `R`.  In this code, the first line loads an `R` library (the 'RCurl' package) to make requests to internet servers. The second line loads an `R` library  (the 'rjson' package) that can process data in the JSON (Javascript Standard Object Notation) format, a data format commonly used by servers to transfer data. Lines 3-4 provide the internet address of the World Bank API, request the data, receive the response, and convert the JSON data into a typical `R` data structure, a list. The final line selects from the list, which has many sub-lists nested within it, just the names of the countries.  

Although these somewhat trivial code vignettes scarcely begin to extract value that Hal Varian propounds, in retrieving data on the production of value they do begin to point at  something important in the mode of production of value from data. Their brevity  suggests very well travelled paths. In terms of urban architecture, the packages and modules that support this brevity are like escalators that quickly move people between levels and floors in a city.  Like the how-to books with their often just-so demonstrations of problem solving, the compactness or briefness code suggests that many obstacles, detours and blockages have already been encountered by others and dealt with by them. This is not to deny any _flaneurish_ skill on my part.  I did some 'processing'  here in, for instance, choosing a path into the World Bank datasets via the selection of the query terms (`r cat(url)`). It also includes the re-formatting operations that first transform JSON data into an `R` list, and then traverse the nested list structure, selecting only the items of data labelled 'name.' These kinds of selection operations, and transformations in the format of data are vital in contemporary data. Whether in the form of well-established relational database query language SQL, in the requests made to one of the legion of APIs that render data on various internet platforms such as Youtube, WorldBank or Pubmed, modifications in the shapes and sorts of data are a constant concern.

On the other hand, the fact that the vignettes do nothing apart from retrieve  the data suggest that a series of transformations is still to  follow. For instance, numerical data might binned into categories such as high, medium, low or some other form of ranking; text might be split into keywords; the presence or absence of some value might be counted or summarised through calculations (such as the the mean or average, the median or middle value, or mode, the majority value); etc. Some of these transformations matter more than others. In the Python vignette, which uses the `wbdata` Python package written by [Oliver Sherouse](http://wbdata.readthedocs.org/en/latest/#suggested-citation),  a few  lines of code produce a similar dataset from the WorldBank data. This time, it explicitly takes the form of a dataframe, a row-column data structure. Tables, arrays and matrixes -- the rectangular form of data that typically puts one example per row, with different columns representing different measurements, attributes or properties of the examples. In both Python and `R`, such as matrixes and dataframes are heavily used for reasons that go deep in machine learning algorithms and data practices more generally. These kinds of table-making operations are often highlighted in books, documentation and tutorials on `R` since they deeply affect how code runs. In this case,   the code vignette turns the list, with its sublists, into a flat table or matrix. This is a kind of topological transformation since the nested list of countries with their economic indicators has a different relationality  than the table with its columns.  


## Conclusion

While I have read textbooks on machine learning and statistics, how-to books on data analysis, data-mining and machine learning, as well as myriad online documents, research papers, and help files (alongside the work of other scholars, philosophers, anthropologists, sociologists, and media theorists),  I've also attempted  to bring the writing of code and writing about code into proximity in order to see if that proximity or mixture of writing code and writing words makes a practical and a praxiographic difference. If recent theories of code and software as forms of speech, expression or performative utterance are right [@Cox_2012;@Coleman_2012], it should. But how does it make a difference?   Already at various points in this text, different written materials have been juxtaposed, and that is completely normal, indeed, almost unavoidable in any writing. Any textual form is somewhat woven. The question is how weaving through  code in one domain of contemporary technical practice, machine learning, answers the methodological injunctions of keeping the practices present (Mol), maintaining the 'concrete sense of value-attainment' (Whitehead) or allowing 'affective expansion' (Wilson). How does this bring  practices more directly into awareness? 

These three takes on how to think about practices, abstraction and machines work on somewhat different levels, but they broadly share an interest or even a commitment to multiple, plural or surprising conjunctions between what people do, what the world is, and the many adjustments, alignments and slippages that connect them through models, abstractions, computation and data. I regard them as  offering guidance or methodological maxims on how to write with data, with models and with machine learning somewhat differently. The question is how would one actually describe machine learning and its allied practices in ways that enhance their multiple reality (not realities!), that augment or deepen abstractions in ways that allow them to resonate.

and to predict how they will classify or predict became central concerns precisely because machines didn't seem to classify or predict things at all well. At the same time, and less visibly,  implementations and applications of techniques derived from artificial intelligence and adjacent scientific disciplines became more statistically sophisticated. That is, the theory of machine learning alongside decision theory was interwoven with a set of concepts, techniques and language drawn from statistics. Just as humans, crops, habitats, particles and economies had been previously, learning machines became entwined with statistical methods. Not only in their reliance on the linear model as a common starting point, but in theories of machine learning, statistical terms such as bias, error, likelihood and indeed an increasingly thorough-going probabilistic framing of learning machines emerged. 


[^1]:  In terms of multiplying matrices, dimensionality both constrains and enables many aspects of the prediction. Perhaps on the grounds of data dimensionality alone, we should attend to dimensionality practices in  `R`. A fuller discussion of dimensionality of data is the topic of a later chapter. There I discuss how machine learning re-dimensions data in various ways, sometimes reducing dimensions and at other times, multiplying dimensions.


[^3]:In combining Markdown and LaTex, I also make use of the [Pandoc utility](http://johnmacfarlane.net/pandoc/README.html), a command line tool that converts between different document formats.

[^4]:  I first encountered machine learning in reading recent scientific articles on genomics. Scholars interested in genomics have often discussed the growth of DNA sequence data in contemporary life sciences. They have carefully studied sequence and other biological databases, but less often attended to the ways in which data from those databases has been modelled or used predictively. Understanding predictive modelling in genomic science is difficult, since both the biology and the statistics are technically challenging. The possibility of reconstructing or implementing genomic models are more limited. Although I have attended genomic data analysis courses, even in these classes, where my classmates where either postgraduate research students or scientists, only a narrow selection of techniques for working with genomic data are covered. The online machine courses are, by contrast, quite generic and the examples are broad-ranging.  I discuss the competitions and forms of machine learning subjectification in a later chapter. Machine learning in genomics is also the topic of a later chapter. Recent life sciences are not alone in their resort to computational techniques, but in contrast to commercial, industry or government data practices, it is easier to track how data is transformed, reshaped and re-dimensioned in producing biological knowledges. The scientific publications, the public databases and open source software work together to allow this. 

[^5]: Isabelle Stenger's  book on Whitehead offers a deeply philosophical introduction to his work [@Stengers_2002]. [TBA: English version]
 
