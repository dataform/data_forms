#$N = all$: machine learning gets all the data
## alternative titles 

- Believing in the strength of numbers


## overview

->> what is happening to probability and why it matters?
  - much talk about N=All, and the end of sampling, but is that right?
    - not from the perspective of machine learning -- it just shifts where the probability and randomness is being done
    - quotes from Mayer-Schonberger, etc. to illustrate this
- multiplying probabilities
    - why naive bayes is important -- multiplying probabilities, assuming they are independent of each other
    - the way machine learning techniques are wrapped/undergirded by statistical thinking: Ng lectures on learning theory, empirical risk minimization, cross-validation and regularization   and especially _bias-variance tradeoff_, underfitting and overfitting-- lectures 9-10;
    - how probability affects machine learning, support vector machine, or otherwise
    - more data beats better algorithms -- why naive bayes, why association rules, etc; simple density estimation vs parametric model
-hidden probabilities
    - why EM is important -- assume that there are hidden or latent random variables that can account for how things are mixed up. This is a major site to introduce unsupervised learning. Ng lectures 13-14-15;
    - the role of tests in machine learning
- synthetic probabilities
    -why MCMC is important - assume that what we think changes the numbers that have to be calculated;
    - the interplay between personalization and probabilistic classification: the ethical-political cost of this
- Implications
    - what low probability events mean; why they matter and how to find them- long tail distributions
    - how statistical thinking re-thinks the algorithmic in terms of the probabilistic - Parisi on this
    - how probabilistic programming matters
- how probability undercuts other more sophisticated takes on the world -- Hacking's argument about Peirce

## to do
- dirichlet distribution is a distribution over distributions -- useful metapoint
- run a topic model across all the articles that cite Blei 2003; also run it across all the articles that cite Dempster, Laird, etc
- hand article describes many medical usages; seems they might have been important in shaping the technique; the other article on bayes at 50 highlights the text stuff; example of how same technique developed along different lines; and then comes together somehow
- principal component analysis and unsupervised learning from the early 20C -- Pearson; the time when probability is objective
- naive bayes, including the ipython notebook; the Lewis article - 40 years, the spam filter example -- from schutt, from segaran, from myles-white; from hastie, flach, from top10 algo, etc
- discussions of Naive bayes in malley, flach, hastie, ripley
- friedman paper -- highly cited on virtues of naive bayes
- tom mitchell on why Bayes is a good way to think about machine learning
- Rassmussen & Williams on Gaussian processes
- likelihood function Flach, 27
- references cited by Ripler -- Warner 1961, Tittington, 1981; cf Michie 1994 textbook
- sequence of references: 
	- Warner 1961, Maron, 1961,  Tittington 1981, Michie, Ripley (DONE), Bishop (DONE), Mitchell (DONE), Domingos 1997, Lewis 1998 Witten (2001/2011),Hastie (2001/2009), Jordan (2004), Alpaydin (2010)(DONE), Manning (2009)(DONE),  Flach, top10-algo, Myles-White(DONE), Schutt (DONE), Hastie/R book (2013), Thoughtful Machine Learning (Kirk 2014)
- the autonomia case
- add bayesian optimality

## from proposal


The topic in this chapter is the role of randomness, chance and number in machine learning. Machine learning techniques are suffused with probabilistic modes of thought. This chapter foregrounds probability and  the changes in probability associated with both statistical pattern recognition models such as the Naive Bayes classifier (often used to filter spam email) and the so-called 'Bayesian revolution' in statistical practice that took shape in the early 1990s in particular in the form of Markov Chain Monte Carlo simulation (MCMC). The computationally intensive techniques of Bayesian analysis treat all numbers as potentially random variables; that is, as best described by probability distributions whose interactions need to be carefully explored.  By contrast, the probabilistic machine learning models exemplified by Naive Bayes  treat numbers as if they have little relation to each other.  The chapter traces two important implications of this contrast between ways of working with probability. The popularity of MCMC is a striking example of a technique moving  across fields, and the chapter will trace some of the ramifications of the heavily-used MCMC technique in fields ranging from nuclear physics, image processing to political science and online gaming. Second, although they both treat potentially all important numbers as a matter of probability calculus, the contrast between  computationally intensive, MCMC and probabilistic models such as Naive Bayes suggest very different beliefs in the power of computation.  A broader question here will be framed by reference to notions of expectation and belief: what mode of belief in probability better sensitises us to what machine learning or pattern recognition models do in given situations?

>Since its Baroque invention [@Hacking_1975], probability has been a double-sided coin. On one side, it concerns degrees of belief (the so-called 'subjective' view), and on the other side, frequencies, or how often things happen in the world (the so-called 'objective' view). In the last few centuries, one side of this coin has come up more often -- the frequency version of probability. Yet, as many historians of statistics, and statisticians themselves recognise, probability as degree of belief has never disappeared. It has only occurred less often, and been less often the object of belief. This ineluctable entwining of belief and events, of subjective-objective faces, in probability seems quintessentially Baroque in its interweaving and folding together of inside and outside. Drawing on contemporary statistical practice, and Gilles Deleuze's understanding of monads as 'simple, inverse, distributive numbers' [@Deleuze_1993], this paper examines the resurgence of the probability as degree of belief in the face of a world seemingly teeming with data. It argues that in the last few decades of statistical practice associated especially with 'Bayesian inference' and the techniques of Markov Chain Monte Carlo (MCMC) simulation, we see a re-configured and super-imposed concept of probability taking shape. As these practices pervade diverse scientific fields, commerce, government and industry, we might be seeing a different epistemic materialisation taking shape in which beliefs and events are less separate. On the contrary, through computation, subjective belief is exteriorised in simulated events, and a certain staging of events are reshaped as updateable beliefs. 

## quotes to use

from hacking_1990


>It is pointless to debate which of the two ideas is correct. We note only that one or the other may be more dominant at different times. 97

>To return to _chance_ and _probabilite_: the fundamental distinction between 'objective' and 'subjective' in probability -- so often put in terms of frequency _vs._ belief -- is between modelling and inference. 98

stuff from Hastie on Naive Bayes

>While this assumption is generally not true, it does simplify the estimation
dramatically:

>• The individual class-conditional marginal densities $f_{jk}$ can each be estimated separately using one-dimensional kernel density estimates. This is in fact a generalization of the original naive Bayes procedures, which used univariate Gaussians to represent these marginals. • If a component $X_j$ of $X$ is discrete, then an appropriate histogram estimate can be used. This provides a seamless way of mixing variable types in a feature vector. 211
 
>Despite these rather optimistic assumptions, naive Bayes classifiers often outperform far more sophisticated alternatives. The reasons are related to Figure 6.15: although the individual class density estimates may be biased, this bias might not hurt the posterior probabilities as much, especially near the decision regions. In fact, the problem may be able to withstand considerable bias for the savings in variance such a “naive” assumption earns. 211

## Key examples:  Microsoft TrueSkill; Obama election data team
- the viagra spam example
Key techniques: Monte Carlo simulations and MCMC; Bayesian networks; 


 - probability and Bayesian inference - belief and desire in data  - belief chance, Bayes, internal proliferation of numbers; event-belief oscillation

## Introduction to $N = All$

The topic in this chapter is the role of randomness and probability in machine learning. While statistical techniques and practices have been discussed in previous chapters, this chapter foregrounds the changes in probability practices associated with machine learning, and in particular, $N = All$, the claim that with all the data, statistical thinking and the potentials of the statistical reasoning fundamentally change. Viktor Mayer-Schönberger and Kenneth Cukier's _Big Data: A Revolution That Will Transform How We Live, Work and Think_  present this shift in many different ways in the course of the vignettes and comparisons that have become typical of the data revolution genre. In a chapter entitled 'More,' they sketch the transition from data practices reliant on sampling to data practices that deal with all the data. 

>Using all the data makes it possible to spot connections and details that are otherwise cloaked in the vastness of the information. For instance, the detection of credit card fraud works by looking for anomalies, and the best way to find them is to crunch all the data rather than a sample [@Mayer-Schonberger_2013, 2013,  27]

In the several hundred pages that follow in _Big Data_, the problem of how to 'crunch all the data' is never really discussed. While they mention the role of social network theory (30), 'sophisticated computational analysis' (55),  'predictive analytics' (58) and 'correlations' (7),  and they do say that 'the revolution' is 'about applying math to huge quantities of data in order to infer probabilities' (12), any further consideration of specific techniques of data crunching or the math is largely left aside. This is not to criticize a book that sets out to describe trends affecting business and government for a general readership, but without a sense of how statistical thinking animates almost all salient features of crunching the data and particularly the predictions, it becomes hard to see how the 'revolution' takes place. In other words, the shift from $N=n$ to $N=all$ does not occur without other transpositions and rearrangements that do not simply concern choices about how much data to use.

Certain strands of social and cultural theory have taken a strong interest in algorithmic processes. For instance, the sociologist Scott Lash distinguishes  the operational rules found in  algorithms from the regulative and constitutive rules in many social settings and studied by social scientists:

>in a society of pervasive media and ubiquitous coding, at stake is a third type of rule, algorithmic, generative rules. ‘Generative’ rules are, as it were, virtuals that generate a whole variety of actuals. They are compressed and hidden and we do not encounter them in the way that we encounter constitutive and regulative rules. Yet this third type of generative rules is more and more pervasive in our social and cultural life of the post-hegemonic order. They do not merely open up opportunity for invention, however. They are also pathways through which capitalist power works, in, for example, biotechnology companies and software giants more generally [@Lash_2007, 71].

The term 'generative' is somewhat resonant in the field of machine learning as generative models, models that treat modelling as a problem of specifying the operations or dynamics that could have given rise to the observed data, are extremely important. If we consider only Andrew Ng's CS229 machine learning lectures  on Youtube [@Ng_2008], we can see that they introduce generative models in Lecture 5 and 6. Although this seems to be only a small part of the 18 lectures given in the course, later lectures on the expectation maximisation algorithm (12-13), and then on unsupervised learning techniques such as factor analysis and principal component analysis, independent component analysis, are also effectively exploring generative models.  A similar distribution of topics can be found in _Elements of Statistical Machine Learning_[@Hastie_2009].   Generative models, while perhaps slightly less common in practice than discriminative models, nevertheless capture the sense that algorithms are not just implementations of rules for filtering, sorting, or deciding, but carry within them ontological commitments that might actually challenge social theory in interesting ways. In contrast to Lash, I would suggest that the generativity of these algorithms needs to be differentiated from the algorithmic processes that implement rules more generally. Moving into the data via a generative probabilistic model is very different to moving into the data through say a database query. The models, whether generative or discriminative (models  such as decision tree,  logistic regression or even neural networks that are more limited in their probabilistic underpinnings), are more like meta-algorithms that reorganize other algorithmic processes on varying scales. 

I approach these transpositions  mainly through a contrast between the extremely simple and easily implemented Naive Bayes classifier  and in particular, a key algorithmic technique widely used in generative models, expectation maximization, especially as it is applied in the typical 'big data' technique of 'topic models' [@Blei_2011], which are more technically termed 'Latent Dirichlet Allocation' [@Blei_2003]. Across the gamut of differences between the very simple Naive Bayes approach and the sophisticated MCMC techniques at work in topic models, we can see a broad ranging shift in probability practice has been occurring. This shift is not captured by the shift from $N=n$ to  $N=all$.  The key argument here, following on from the discussions of vector space, the function of optimisation and the development of decisionistic mechanisms developed in the preceding chapters is that the ways in which data is being corralled and marshalled, and the kinds of pattern-finding, anticipation or pre-emption delivered by machine learning need to be considered alongside a _re-distribution_ of probability. This is not the first such re-distribution, since probability, as we will see, has undergone convoluted transformations before (for instance between subjective and objective senses: see [@Hacking_1975; @Hacking_1990] for standard accounts of these changes). The multiple, overlapping and often cantilevered ways in which probability practice runs through machine learning, however, suggests that another transformation is in train. This transformation, I will suggest, concerns the increasing attention paid to hidden structures and to forms of organization that belong to data rather than to the world. Accompanying this transformation, amplifying and ramifying it, certain techniques -- Naive Bayes, MCMC, but also so-called 'ensemble' methods that stage populations of models and specific techniques such as Expectation Maximization or the cross-validation -- carry this transformation wide and deep. Via a combination of these techniques, many more things become probabilistic. Words, images, faces, places, feelings, prices, and all kinds of derivative numbers display an increasingly probabilistic mode of existence that may not completely replace or suppress other ways in which they inhabit the world, but certainly induce existential oscillations in them. I would suggest too that these transformations engender many effects of animation, agency and indeed anxiety about things, objects, machines and systems that powerfully play out various registers of making, consuming, acting, happening and remembering. 

The term _distribution_ figures here in two senses -- the probabilistic sense and the circulatory sense. Put in slightly awkward technical terms, we might say that many more aspects of data, of prediction, of classification and pattern, exist more _probabilistically_. In some respects, the intensification of probabilistic distributions accompanies  of the counting of many more things, a counting that always takes place in time and is thereby connected with the temporality of risk, prediction and anticipation. There is, however, a more thoroughgoing probabilisation occurring alongside the ubiquitous counting. The computationally intensive MCMC techniques used in Bayesian analysis treat all numbers as potentially random variables, including numbers that in previous statistical processes would not themselves have been seen as probabilistic. The Naive Bayes technique treats all outcomes as conditional probabilities, not too far removed from coin tosses in certain respects. The bootstrap techniques change the way that data relates as samples to populations, and presents datasets as populations that can be intensively re-sampled in order to effectively make the dataset a bigger portion of the population it was originally cut from. These might be seen as interesting technical developments, but I think they signify something else.  In contrast to the line drawing, curve fitting, maximising and minimising of the linear models, the decision trees, and the support vector machines, the probabilistic transformations wrought through machine learning start, I would suggest, to affect how we experience collectives, how we apprehend individuality, as well as more diffuse collective entities such as populations, and perhaps markets. In a world where probability and its associated practices of randomization are no longer confined to specific centres of calculation such as laboratories or bureaus of statistics, chance, randomness and probability become part of the fabric. This is, in short, the other sense of distribution currently under intensification. 

## Probability distributions
 
Probability distributions are a common way of showing and  talking about random variables. The curves shown in Figure 2 could refer to almost anything (the chances of rain at different times of day in Lancaster, the seasonal variation in precipitation, etc.). These distributions appear in countless shapes and forms in scientific, government and popular literature of many different kinds. Statistical graphics have a rich history and semiology that I do not discuss here (see [@Bertin_1983]). Perhaps the most famous function or mapping is the normal or Gaussian distribution:

>$f(x;\mu, \sigma^2) = \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}$

This  function, whose mathematics were intensively worked over during the 18-19th centuries, has a power-laden biopolitical history closely tied with knowledges and governing of  national and other  populations. The key symbols here include $\mu$, the mean and $\sigma$, the variance. These two parameters together describe  the shape of the curve. Given $\mu$ and $\sigma$, it become possible to map different outcomes to probabilities. Given the normal distribution, it is possible, under certain circumstances, to effectively subjectify someone on the spot. For instance, if an educational psychologist indicates to someone that their intelligence lies towards the left-hand side of the normal curve peak in Figure 2 (and hence less than the population mean), they quickly render them somehow subject to the normal curve. But statistics uses dozens of different probability distributions to map continuous and discrete variations to real numbers. Other probability distributions abound — normal (Gaussian), uniform, Cauchy exponential, gamma, beta,  hypergeometric, binomial, Poisson, chi-squared, Boltzmann-Gibbs distributions, etc (see [@NIST_2012] for a gallery of distributions) — because outcomes occur in widely differing patterns. The  queuing times at airport check-ins do not, for instance, easily fit a normal distribution. Queues are usually modelled using a Poisson distribution, which unfortunately for travellers, distributes waiting times very differently.  Similarly, it might be better to think of the probability of rain today in Lancaster in terms of a Poisson distribution that models that queue of clouds in the Atlantic just waiting to land on the northwest coast of England. Rather than addressing the question of if it will rain or not, a Poisson-based model might address the question of how soon.

The diverse range of probability distributions — and we will see below some reasons why we can expect them to proliferate in certain settings — attests to the variety of ways in which events might be mapped to real numbers. Despite  the sometime forbidding mathematical equations, the term *distribution* emphasises a quite material or tangible way of thinking about how probabilities vary. The curves in both Figure 1 and Figure 2 are examples of the most common mathematical descriptions in any data analysis setting: they are  *probability density* functions (pdf). (There are also  *probability mass* functions for variables that have discrete values; for instance: 1,2,3,4,5). Pdfs such as the Gaussian function shown above are usually graphed as a curve that indicates how likely a random variable is to take on a particular value. In many cases, statistical practice seeks to estimate distribution functions such as pdfs (or their close relatives, cdfs — *cumulative distribution functions*) for the given data. Statisticians speak of 'fitting a density' to data, emphasising their assumption that events can be incorporated in the forms of probability distributions. The underlying probability distribution is in principle ‘unobservable’ as such, but a probability density function is  assumed to give rise to all the variations in data gathered through experiments and observations. The task is to estimate the shape of that curve, and its defining parameters (means, variance, etc.). Given that curve, areas under the pdf equate to the likely range of value of a variable. While the total 'probability mass' under the probability density function curve always must be equal to one (since the combined probability of all possible outcomes = 1), finding the area under particular parts of the curve is a key issue. Finding the area under probability density curves becomes the way in which many epistemic processes envisage lived states of affairs as random variables or as numbers in variation.

There is some new operating terminology in this chapter (terms such as random variable, probability distribution and likelihood).  While I attempt to be both mathematically and conceptually concise in my use of these terms, the terms themselves are somewhat troubled by the transformations I'm describing. The underpinnings of taken-for-granted and everyday statistical such as random variable or probability are not immune from change. Like all technical formalisms they took hold at a certain practical and historical conjuncture that will not and perhaps already does not hold entirely still. As the philosophy Ian Hacking suggests in his discussion of early 20th century changes in probability in _The Taming of Chance_,

>By the 1930s, however, the world teemed with frequencies, and the 'objective' notion would come to seem more important than the 'subjective' one for the rest of the century -- simply because there were so many more frequencies to be known [@Hacking_1990, 97].

I'm suggesting that we countenance a scene in which multiple different probability practices stack on top of each other in a somewhat untamed way. Amidst these, random variables and their associated probability distributions particularly concern us. As we will see, the power of the transformation in probability associated with machine learning algorithms resides in their capacity to draw in many more relations, features and components of data in support of a probabilistic outcome. When things are best described as probability distributions, they take on a different form of temporal and multiplicative existence, and this no longer easily attributed to either 'subjective' or 'objective' probability, or to a shift in balance between the long-standing poles of 'subjective' and 'objective' probability. A probabilistically generated airline seat price attracts different kinds of transactions than a fixed priced seat. Similarly, a probabilistically generated tumour classification implies different modes of responsiveness and care. Gaining some understanding of the concrete arrangements of forces in these probabilistic modes might allow us to account for the ways in which certain methods -- Bayesian inference is a striking example of transverse momentum of methods across fields  -- go on the move, or why certain problems -- automatic text classification, image recognition, etc -- suddenly hove into feasibility.

The chapter traces two important implications of probabilistic model for machine learning. First, because it is so computationally intensive, MCMC and Bayesian inference, although statistically powerful, are difficult to apply to many dimensional datasets. So Bayesian computation iconically figures the limits of contemporary data practices, with their ambitions to incorporate all available data into calculation. Second, in certain ways this technique challenges us to re-evaluate how we think about numbers. By following some of the ways numbers circulate through MCMC algorithms, we can discern to a semiotic-material faultline running through contemporary number formations. Numbers semiotically and materially embrace both events and degrees of belief. If numbers are crucial in the data economy, then instabilities in their mode of existence will affect much of what happens to data. While much of the machine learning taking place in commercial and operational settings is decidedly non-Bayesian, the popularity of MCMC and Bayesian approaches in contemporary sciences suggests a tension in what counts as number.

## Naive Bayes

The mathematical expression for one of the most ubiquitous of all machine learning classifiers is:

\begin {equation}
\label {eq:naive_bayes}
f_j(X) = \prod_{k=1}^{p}f_{jk}(X_k)
\end {equation}

[@Hastie_2009, 211]

Some machine learning techniques are so simple that they can be implemented in a few lines of code. Their simplicity, however, belies their power. The function shown above \ref{eq:naive_bayes} is about the simplest one to be found in most machine textbooks. It expresses the Naive Bayes classifier, one of the most popular machine learning algorithms, even though it is more than 50 years old [@Hand_2001]. While \ref{eq:naive_bayes} does not set out all the steps in transforming some training data into a predictive model, the lines of code needed to do this are similarly brief. In _Doing Data Science_, Rachel Schutt and Cathy O'Neill furnish a bash script (that is, command line instructions) to download a well-known email dataset and build a Naive Bayes classifier for spam:

```{r enron_nb_bash, engine='bash', echo=TRUE}

    #!/bin/bash
    # file: enron_naive_bayes.sh
    # description: trains a simple one-word naive bayes spam
    # filter using enron email data
    # usage: ./enron_naive_bayes.sh <word>
    # requirements:
    #    wget
    #
    # author: jake hofman (gmail: jhofman)
    # how to use the code
    if [ $# -eq 1 ]
    then
        word=$1
    else
        echo "usage: enron_naive_bayes.sh <word>"
        exit
    fi

     ### PART 1
    if ! [ -e enron1.tar.gz ]
    then
        wget 'http://www.aueb.gr/users/ion/data/enron-spam/preprocessed/enron1.tar.gz'
    fi

    if ! [ -d enron1 ]
    then
        tar zxvf enron1.tar.gz
    fi

    cd enron1

    ### PART 2
    Nspam=`ls -l spam/*.txt | wc -l`
    Nham=`ls -l ham/*.txt | wc -l`
    Ntot=$Nspam+$Nham
    echo $Nspam spam examples
    echo $Nham ham examples

    Nword_spam=`grep -il $word spam/*.txt | wc -l`
    Nword_ham=`grep -il $word ham/*.txt | wc -l`
    echo $Nword_spam "spam examples containing $word"
    echo $Nword_ham "ham examples containing $word"

    ### PART 3
    Pspam=`echo "scale=4; $Nspam / ($Nspam+$Nham)" | bc`
    Pham=`echo "scale=4; 1-$Pspam" | bc`
    echo
    echo "estimated P(spam) =" $Pspam
    echo "estimated P(ham) =" $Pham
    Pword_spam=`echo "scale=4; $Nword_spam / $Nspam" | bc`
    Pword_ham=`echo "scale=4; $Nword_ham / $Nham" | bc`
    echo "estimated P($word|spam) =" $Pword_spam
    echo "estimated P($word|ham) =" $Pword_ham

    ### PART 4
    Pspam_word=`echo "scale=4; $Pword_spam*$Pspam" | bc`
    Pham_word=`echo "scale=4; $Pword_ham*$Pham" | bc`
    Pword=`echo "scale=4; $Pspam_word+$Pham_word" | bc`
    Pspam_word=`echo "scale=4; $Pspam_word / $Pword" | bc`
    echo
    echo "P(spam|$word) =" $Pspam_word
    cd ..
    ```
[@Schutt_2013, 105-106]

```{r enron_nb_r, echo = FALSE}

        f_ham = list.files('enron1/ham', full.names = TRUE)
        f_spam = list.files('enron1/spam', full.names = TRUE)
        ham_count = length(f_ham) 
        spam_count = length(f_spam)

        ham = sapply(f_ham, readLines, warn=FALSE)
        spam = sapply(f_spam, readLines, warn=FALSE)
        P_ham = ham_count/(ham_count + spam_count)
        P_spam = spam_count/(ham_count + spam_count)
        word = 'gas'

        predict_spam <- function(word) {
            ## count the occurrence of the word in each
            Nword_in_ham = sum(grepl(word, ham))
            Nword_in_spam = sum(grepl(word, spam))
            #cat(Nword_in_ham, ' ham examples contain ', word,  '\n')
            #cat(Nword_in_spam, ' spam examples contain ', word,  '\n')

            #cat('estimated P(spam) = ', P_spam,  '\n')
            #cat('estimated P(ham) = ', P_ham,  '\n')

            Pword_spam = Nword_in_spam/spam_count
            Pword_ham = Nword_in_ham/ham_count
            #cat("P(spam|", word, ") = ", Pword_spam,  '\n')
            #cat("P(ham|", word, ") = ", Pword_ham,  '\n')

            Pham_word = Pword_ham * P_ham
            Pspam_word = Pword_spam * P_spam
            Pword = Pspam_word + Pham_word
            Pspam_word = Pspam_word/Pword

            #cat("P(spam|", word, ")=",  Pspam_word,  '\n')
            return(Pspam_word)
        }
```


The input to the script is a single word such as 'finance' or 'deal'. The model is so simple that it only classifies a single word as spam. The `bash` script carries out four different transformations of the data in building the model. It uses only command line tools such as `wc` (word count), `bc` (basic calculator), `grep` (text search using pattern matching) and `echo` (display a line of text). These tools or utilities are readily available in almost any UNIX-based operating system (e.g. Linux, MacOS, etc). The point of using only these utilities is to illustrate the simplicity of the algorithmic implementation of the model.  The first part of the code downloads the sample dataset of Enron emails (and I will discuss spam emails and their role in machine learning below). Note that this dataset has already been divided into two classes - 'spam' and 'ham' -- and emails of each class have been placed in separate directories or folders as individual text files.

The text of a typical spam email in the Enron datasets looks like this:

>
Subject: it ' s cheating , but it works !
can you guess how old she is ? the woman in this photograph looks like a happy teenager about to go to her high school prom , doesn ' t she ? she ' s an international , professional model whose photographs have appeared in hundreds of ads and articles whenever a client needs a photo of an attractive , teenage girl . but guess what ? this model is not a teenager ! no , she is old enough to have a 7 - year - old daughter . . and . . . the model ' s real age is in her 30 ' s . all she will say about her age to her close friends is , " i ' m dangerously close to 40 . " she also says , " if it weren ' t for this amazing new cosmetic cream called ' deception , ' i would lose hundreds of modeling assignments . . . because . . . there is no way i could pass myself off as a teenager . " learn more about this amazing new product . . . please refer all questions , opinions or additional feedback to :
service dept
9420 reseda blvd # 133
northridge , ca 91324

The text of a typical non-spam email like this:

> Subject: industrials suggestions . . . . . .
- - - - - - - - - - - - - - - - - - - - - - forwarded by kenneth seaman / hou / ect on 01 / 04 / 2000
12 : 47 pm - - - - - - - - - - - - - - - - - - - - - - - - - - -
pat clynes @ enron
01 / 04 / 2000 12 : 46 pm
to : kenneth seaman / hou / ect @ ect , robert e lloyd / hou / ect @ ect
cc :
subject : industrials
ken and robert ,
the industrials should be completely transitioned to robert as of january 1 , 2000 . please let me know if this is not complete and what else is left to transition .
thanks , pat

The second part of the code ('Part 2') counts the number of emails in each category and prints them, and the counts the number of times that the chosen word (e.g. 'finance' or 'deal') occurs in the spam and ham categories. Using these counts, the script (in what I am calling Part 3) estimates probabilities of any email being spam or ham, and then given that email is spam or ham, that the particular word occurs. To estimate a probability means, in this case, to divide the word count for the chosen word by the count of the number of spam emails, and ditto for the ham emails.  In Part 4, the final transformation of the data, these probabilities are used to calculate the probability of an email being spam given the presence of that word. Again, the mathematical operations here are no more complicated than adding, multiplying and dividing. The probability that the chosen word is a spam word is, for instance, the probability of occurrence of the word in a spam email multiplied by the overall probability that an email is spam. Finally, given that the overall probability of the chosen word occurring in the email dataset is the probability of it occurring in spam plus the probability of it occurring in ham, the overall probability that an email in the Enron data is spam given then presence of that word can be calculated. It is the probability that the chosen word is a spam word divide by the probability of that word in general. 

The point of this slightly bamboozling and mechanical description of what the script does is to show something of how \ref{eq:naive_bayes} can be translated into code. Hardly any machine learning models are so simple that they can be conveyed in 30 lines of code (including downloading the data and comments). The second feature of this script is that it shows that nothing that occurs in relation to probability is intrinsically mysterious, elusive or somehow transcendental. On the contrary, everything here is counting, adding, multiplying (that is, repeated adding) and dividing (that is, multiplying by parts or fractions). Everything is constrained by a certain limit on the results. Probabilities are always between `0` and `1`, or between 0% and 100%.  'Finance' has a  `r round(predict_spam('finance'),2)` chance of being spam, while 'sexy' has a chance of `r round(predict_spam('sexy'), 2)`, at least in the Enron emails.  Finally, the transformation of textual forms -- the emails -- into probabilistic events is relatively straightforward. It relies on counting how often things occur. This is a trivial point in some ways. Everyone knows probability and statistics concerns counting things. The quantification entailed in statistics is a counting of events that have been named or labelled in some way. The perhaps more profound point is that this counting of atomic events can be combined in a seemingly limitless variety of combinations. In the emails, individual words are events, and therefore each email is a complicated composite event. Similarly, in machine learning on images, each pixel could be treated as an event, and the image as a whole becomes an immensely complicated aggregate colour and light event. As we will see, many other machine learning techniques try to deal with this combinatory character of composite events directly. 

## The longstanding success of Naive Bayes classifiers

Hastie, Tibshirani and Friedman characterise the value of the  Naive Bayes technique in relation to its capacity to deal with high dimensional data (see Chapter 2 on this notion of data dimensionality):    

>It is especially appropriate when the dimension $p$ of the feature space is high, making density estimation unattractive. The naive Bayes model assumes that given a class $G = j$, the features $X_k$ are independent [@Hastie_2009, 211]. 

In \ref{eq:naive_bayes}, $p$ stands for the number of different dimensions or variables in the data set, and that the outcomes can be classified in $j$ different classes. In the spam classifier, the number of dimensions is probably quite large. That is, every unique word adds a new dimension to the 'feature space.' By contrast, there are are only two classes $G$, spam and non-spam.  As we see in both \ref{eq:naive_bayes} and from the `bash` script, the only operation carried out on the data is a multiplication. The $\prod$ operator multiplies probabilities in order to generate a classification. Compared to the complications of logistic regression, neural networks or support vector machines, \ref{eq:naive_bayes}  seems incredibly simple. How is it that a simple multiplication of probabilities can, as Hastie and co-authors write: 'often outperform far more sophisticated alternatives' [@Hastie_2009, 211]. Similar formulations can  be found in most of the machine learning books and instructional materials currently available. Along with linear regression, Naive Bayes might be _the_ standard introductory machine learning technique, where the Naive Bayes classifier is almost always demonstrated to the problem of filtering spam email [@Conway_2012; @Schutt_2013, 93-113; @Kirk_2014, 53; @Lantz_2013, 92-93; @Flach_2012, @Ng_2008b], and in particular dealing with the abundance of spam emails concerning Viagra (itself a byproduct of the failure of a statistical model). It is remarkable how Naive Bayes comes up in machine learning textbooks especially given how simple the technique is. Admittedly spam, and spam trying to sell Viagra in particular, has been a very familiar part of most email users lives for a decade and more. In practice, Naive Bayes classifiers and variations of them have become an integral part of managing email traffic for most people, whether they know it or not. And large sample datasets of email tagged as 'spam' or 'ham' are widely available.

I wonder, however, whether the constant reiteration of email spam filtering using Naive Bayes stands in as some kind of prototypical learning about machine learning situation, and in this learning situation, the application of probability to language and texts has a central importance, an importance that might perhaps be evaluated along the same lines followed by Michel Foucault in _The Order of Things: An Archaeology of the Human Sciences_ when he describes how understandings of differences in languages in the late 18th century were deeply re-structured by the appearance of grammar [@Foucault_1992]. After this transition, Foucault writes,


>language no longer consists only of representations and of sounds that in turn represent the representations and are ordered among them as the links of thought require; it consists also of formal elements, grouped into a a system, which impose upon the sounds, syllables, and roots an organization that is not that of representation. Thus an element has been introduced into the analysis of language that is not reducible to it [@Foucault_1992, 235].

Here Foucault describes a shift in which languages no longer function as representational systems for ideas, things, knowledge or feeling but take on a history (or historicity) of their own. The 'organization that is not of representation' is 'an interior "mechanism"' (236) that determines the individuality and resemblances of languages to each other. No doubt, the contemporary ambition to algorithmically  classify written texts responds to a very different conjunction than the late 18th century. But despite the sheer vastness and density of written communications in contemporary media environments, it does draw from it an approach to probability.  As Drew Conway and John Myles-White write in _Machine Learning for Hackers_, 

>At its core, text classification is a 20th century application of the 18th century concept of _conditional probability_. A conditional probability is the likelihood of observing some thing given some other thing we already know about [@Conway_2012, 77] 

They point here to the role of 'conditional probability,' a kind of probability that lies at the heart of many of the data transformation associated with prediction or pattern recognition. As any of the many accounts of the technique will explain, the name comes from Bayes Theorem, one of the most basic yet widely used results in probability theory (dating from the 18th century), yet Naive Bayes does not even fully embrace Bayes Theorem as the principle of its operation (hence the name 'naive' Bayes or previously 'idiots' Bayes). It simply looks for differences in terms of which words are more likely to appear in spam messages and which words are more likely to appear in non-spam or ham messages. Moreover, although it produces probabilistic results, the actual values of those probabilities matters less than their relative value (for instance, are they greater than or less than 0.5). 

For our purposes, Naive Bayes classifiers, as well some other more sophisticated statistical techniques  such as the expectation maximization (EM)  algorithm and the Monte Carlo Markov Chain technique that underpin the topic model text classifiers, attest to the deeply probabilistic character of machine learning. In contrast to the geometrical and topological concerns of dimensionality and features, or to the rule-based structures generated as decision trees, the probabilistic character of Naive Bayes entails a more unstable and perhaps in certain respects more supple relation to worlds. This suppleness comes at the cost of much greater uncertainty, and certainly it changes what we think of algorithmic processes if they start to become probabilistic in their operations. 

The transformations that machine learning bring to probability itself are not insignificant. As we will see, the growth of these techniques operationalises probabilities in new ways, or at least in ways that render more and more untenable the long-standing dichotomy between probability as objective randomness in the world and probability as measure of our own uncertain knowledge of the world. At core, both the objective and subjective treatments of probability find themselves reassembled in a new algo-probablistic assemblage. By virtue of its simplicity, the Naive Bayes classifier helps to show this. 

## More data or better algorithms?

It has been known for almost two decades that Naive Bayes can perform almost as well on classification tasks as decision trees or neural networks [@Mitchell_1997, 177]. This is surprising since statistically speaking, the Naive Bayes model is very simplistic. Every account of it immediately points out the naivete of the model:

>naive Bayes .. assumes statistical independence of all the features: at a very minimum this means the features are assumed to be all uncorrelated with each other. ... This is often a wildly implausible assumption but the method has been shown to work surprisingly well [@Malley_2011, 46]. 

The assumption that all of the features are 'statistically independent' effectively means that the model observes the world as a set of incoherent or unrelated processes. In the case of document classification, a standard application of the technique, the intuition is often expressed  in terms of an author who writes documents by first choosing a topic, and then selecting a set of words that relate to that topic. As Alpaydin puts it:

>It is as if we first pick a class $C$ at random by sampling from $P(C)$, and then having fixed $C$, we pick as $\mathbf{x}$ by sampling from $p(\mathbf{x}|C)$. ... For example, in text categorization, generating a text may be thought of as the process where an author decides to write a document on a certain topic and then chooses the set of words accordingly [@Alpaydin_2010, 398].

In this intuition, the topic of the document is its major classification, the set of words composing it are a vector of features. While such a world is a fascinating possibility, it does not resonate with many of the patterns that even the most insistent empiricism would admit largely hold sway over time. Hence the term 'naive' points to this innocence in relation to patterns of events. If the Naive Bayes classifier is so agnostic about the relation between events, how does it manage to 'learn' (in the sense that machine learning thinks about learning)? Or to put this question more practically, how is it that Naive Bayes classifiers can tell the difference between spam and non-spam email?

The conditions under which Naive Bayes perform well are rather specific it turns out. The assumption in spam classification is that all emails can be definitively classified as either spam or non-spam. Given that hard and fast classification, the presence of words such as 'sildenafil' or 'Viagra' are very much more likely to be associated with spam email than non-spam. (For people who want to get Viagra, the assumption does not hold so well. How a Naive Bayes classifier would sort the email of a Viagra user is an interesting question.) Granted this assumption,  the problem of spam classification, a setting in which machine learning approaches have long been in mundane operation, is understood by Naive Bayes models as one in which words are used in emails somewhat randomly, or with a degree of unusual degree of unrelatedness. While some words are definitely more like to be associated with spam - 'viagra', 'inheritance', 'lottery' or 'prize' -- these words have no relation to each from the Naive Bayes perspective. The kind of partial observer at the core of the model simply counts the frequency of occurrence of all words found in emails, calculates the probability of any individual word occurring, and then estimates the probability of each observed conjunction of words. As Tom Mitchell puts it:

>The naive Bayes classifier is based on the simplifying assumption that the attribute values are conditionally independent given the target value. In other words, the assumption is that given the target value of the instance, the probability of observing the conjunction $a_, a_2 ... a_n$ is just the product of the probabilities for the individual attributes: $P(a_1, a_2 ... a_n|v_j) = P(\prod_i P(a_i|v_j)$ [@Mitchell_1997, 177]

In this formulation, the 'target value' or classification is the label 'spam' or 'not-spam.' The 'attribute values' are the presence of particular words. The 'instance' is the actual email under consideration. The product of the probabilities we have already seen in \ref{eq:naive_bayes} in the form of the $\prod$ operator that multiplies the probabilities of different features to produce the probability that a specific combination of words is likely to occur.  As in all supervised machine learning, the techniques relies on a training dataset of already labelled emails in order to estimate the probability that a given email is spam or not. It bears repeating that in almost no respect are machine learning techniques. They always rely on someone's previous work or ongoing work to do classification. The only respect in which they might be seen as automatic concerns how that ongoing work produces effects of scale. While the training dataset for an email spam classifier might consist of 100,000 emails, the email classifier could in practice classify millions of emails once it has been trained on the labelled email dataset. 

## The ancestral communities of Naive Bayes

Like many of machine learning techniques, Naive Bayes has a multi-stranded geneaology, and this geneaology runs through all current demonstrations and implementations of the technique. As Lucy Suchman and Randall Trigg wrote in their study of work on artificial intelligence, 

>rather than beginning with documented instances of situated inference ... researchers begin with ... postulates and problems handed down by the ancestral communities of computer science, systems engineering, philosophical logic, and the like [@Suchman_1992,174]. 

Not only does Bayes Theorem date from the 18th century, but the first attempts to use what is now called Naive Bayes in the early 1960s already display many of the engagements with life, labour and language that Foucault saw as concertedly shifting in the late 18th century. Starting in 1960, Homer Warner, Alan Toronto and George Veasy, working at the University of Utah and Latter-day Saints Hospital in Salt Lake City, began to develop a probabilistic computer model for diagnosis of heart disease [@Warner_1961; @Warner_1964]. Their model used exactly the same 'equation of conditional probability' we see in \ref{eq:naive_bayes} but now used to 'express the logical process used by a clinician in making a diagnosis based on clinical data' [@Warner_1961, 177]. Despite the mention of logic in this description, the diagnostic model was thoroughly probabilistic in the sense that the model itself has no representation of logic included in its workings. Rather it calculates the probability of a given type of heart disease given 'statistical data on the incidence of symptoms' [@Warner_1964, 558]. Somewhat ironically, as they point out, physicians involved in preparing and submitting data to the diagnostic program improved the accuracy in their own diagnoses.   In 1964, N.J Bailey was taking the same approach to medical diagnosis [@Bailey_1965].  [TBA -- not sure about this -- I have  a paper from 1963, but not it is saying the same thing.] Similarly, working at the RAND Corporation in the early 1960s, M.E. Maron described how 'automatic indexing' of documents could become 'probabilistic automatic indexing.' The necessary statistical assumption was:

>The fundamental thesis says, in effect, that statistics on kind, frequency, location, order, etc., of selected words are adequate to make reasonably good predictions about the subject matter of documents containing those words [@Maron_1961, 406]

This fundamental thesis has since remained somewhat fundamental in text classification and information retrieval applications, as well as many other machine learning approaches. Statistics on kind, frequency, order and location are often the only chains of reference that these techniques maintain in relating to their source data. The fact that certain complexities or relationalities might be filtered out by the fundamental thesis is acknowledged explicitly, but compensated by a practical interest in automatic classification when confronted by large numbers of documents. As we will see, even in the most recent iterations of these techniques in topic modelling [@Blei_2011], with all its statistical sophistication, the fundamental thesis largely applies.  

In addition to the longevity of the technique, we might attend to the shape of the datasets. Maron's work focused on a collection of several hundred abstracts of papers published in the March and June 1959 issues of the _IRE Transactions on Electronic Computers_. As in contemporary supervised learning, these abstracts were divided into two groups, a training and a test set ('group 1' and 'group 2' in Maron's terminology [@Maron_1961, 407]), and the training set was classified according to 32 different categories that had already been in use by the Professional Group on Electronic Computers, the publishers of the _IRE Transactions_. Given these classifications, word counts for all distinct words in the abstracts were made, the most common terms ('the', 'is', 'of', 'machine', 'data', 'computer') and the most uncommon words removed, and the remaining set of around 1000 words were actually used for classification. This treatment of the abstracts as documents, then as lists of words, and then as frequencies of terms, and finally as a filtered list of most information rich terms continues in much text classification work today. Interestingly, they are rarely performed in relation to the spam classification demonstration (and I will discuss some possible reasons for this below). 

Not just words, but elements of many different datasets -- images, instrument measurements, clocktimes or timestamps, and in short almost anything that can be counted -- could be handled in the same way by a Naive Bayes classifier. Already in 1962, Warner, Toronto and Veasy were using physiological data, medical history, phonocardiogram, ECG and x-ray findings  amongst other things [@Warner_1964, 561]. Like Maron, they were working with many more categories than the average spam or document classifier in use today (35 different types of heart disease were included in the diagnostic model).  The key point is that the categories -- Maron's 32 categories drawn from _IRE Transactions_ or the two categories of 'spam' or 'ham' from the Enron dataset -- appear as 'response' or target variables, and the remainder of the dataset -- the words or other elements -- appear as 'predictors' or 'feature variables.' Everywhere, numbers are increasingly in variation, and display increasingly distributional characteristics. 

## Words distributed as random variables

Much more heavily weighted version of probabilistic modelling abound in machine learning. The celerity and lightness of Naive Bayes makes it easy to work with. Naive Bayes models can be quickly trained and retrained, and in fact, because they are so easy to retrain, lend themselves to high volume data stream analysis. Sooner or later, all predictive models drift out of alignment with the data they encounter. (Either the world changes, or the instruments change, or the kind of prediction people want from the model change, and the model needs to be retrained.) Despite its admirable simplicity and adaptability as a classifier, I'm not Naive Bayes affords a view of the range of probabilistic transformations currently underway. Where machine learning is concerned, many alternatives techniques can be discussed. It often turns out that different methods can be seen as different versions of each other (for instance, neural networks can be seen projection pursuit ridge regression; principal component analysis can be seen as k-means clustering,  etc). In this respect it does not matter too much which technique we choose to examine. Topic models, however, are in some ways the continuation of the work on probabilistic document classification begun by Maron in the early 1960s. They also neatly exemplify the role of probability distributions in machine learning techniques, a topic that I haven't really discussed until now, but seems to me a core component of the data transformations occurring in science, media, business and government. 

Naive Bayes treats combinations of words as a _joint probability distribution_. That is, it regards the combinations of words that define a topic as the key to predicting the topic of any given document. In order to classify any given document, it calculates the probability that the given combination of words would occur given each class. As we have seen, Naive Bayes assumes that although some combinations of words are more likely than others ('buy' and 'viagra' are likely to occur in the same spam email, but 'buy' and 'stochastic' are less likely to co-occur), these combinations can be ignored by the Naive Bayes model itself. All the model sees are the different probabilities of the different combinations in relation to different classes of documents ('spam' vs 'ham'). The Naive Bayes model constructs a joint distribution in which the class of the document  - $G$ in \ref{eq:naive_bayes} is a random variable alongside many other random variables  -- all of the words, or at least the subset that remain after any data cleaning, $X$ in \ref{eq:naive_bayes}. That might be hundreds of words in Maron's work, but thousands of unique terms in the case of the Enron email dataset we were working with above. 

Machine learning textbooks tend to put this use of probability distributions in Naive Bayes more formally. As Flach suggests:

>Probabilistic models view learning as a process of reducing uncertainty using data. For instance, a Bayesian classifier models the posterior distribution $P(Y|X)$ (or its counterpart, the likelihood function $P(X|Y)$) which tells me the class distribution $Y$ after observing the features values $X$ [@Flach_2012, 47]

(Note that Flach uses $Y$ instead of $G$ to designate the classes or labels for documents.) The process of reducing uncertainty differs from the logistic regression and decision tree treatments of classification because it allows the construction of a _generative_ rather than a _discriminative_ predictive model. Again, as Flach puts it:

>Such models are called 'generative' because we can sample from the joint distribution to obtain new data points together with their labels. [@Flach_2012, 263]

Not too much hinges on the distinction between generative and discriminative models for our purposes.[^1] What does matter is how Naive Bayes and other probabilistic models such as topic models  treat data. Generative models see data as generated by a process that included both observed and hidden variables. The observed variables may be measurements or they may be the kinds of counting of words done in document classification. The power and also the problem of using generative models lies in the relationship between the hidden variables -- the variable variously called the 'response' or 'target'  or just the label -- and the observed variables embodied in the joint distribution. The prediction of the hidden variable for any observed data (the words in a newly arrived email, the medical data for a given patient, etc.) depends on this joint distribution. When Flach says 'we can sample from the joint distribution to obtain new data points together with their labels,' he refers to the predictive potential of generative models. Data analysis or prediction depends on that joint distribution in that the hidden variables, the variables that we would like to see in order to classify what the thing really is, can only be computed from that joint distribution. The 'posterior distribution' or the 'likelihood function' is a probability distribution computed from the joint distribution by treating the hidden or target variable in terms of the observed data. The expression $P(Y|X)$ points to this conditioning: the probability of $Y$ is conditioned by the observed values $X$ (remembering that $X$ designates a vector of different variables, sometimes running into hundreds of thousands per observation).  

[^1]: The differences between generative and discriminative probabilistic models are not only technical. They matter in various ways in practice (see [@Ng_2002] for a general, albeit somewhat technical comparison).  The Naive Bayes classifier is somewhat unusual in that the joint probability distribution of the words and document labels ('spam' vs 'non-spam') can be calculated by simply multiplying them. That multiplication, however, assumes independence between the all the words. That assumption is inevitably realistic in almost every setting. 

## Hidden structures and themes in documents

If generative probabilistic models can so powerfully 'reduce uncertainty' or, to put in a less statistical and more machine learning oriented fashion, infer hidden structures, then why are not all machine learning models probabilistic in the way that Naive Bayes models we have been using above are? Why not always multiply all the counts of different observations, and calculate conditional probabilities of the form $P(Y|X)$ for all classification tasks? Would not that be the direct realisation of $N=all$, the far horizon of all contemporary data processing?

The problem is that events are not independent. Remember that in terms of probability theory, everything that happens -- every event -- is a kind of set comprising different outcomes. Events are understood as subsets of all the possible outcomes in a given 'sample space' ('the **sample space** $\Omega$ is the set of possible outcomes of an experiment.  ... Subsets of $\Omega$ are called **Events**' [@wasserman_all_2003,3]). In the probabilistic modelling of this text, the word 'the' at the start of this sentence is an event, an event to which a number can be assigned. The assigned number for the event 'the' will vary depending on the kind of document. For instance, a well-regarded statistics textbook written by Larry Wasserman introduces the idea of probability as event-related number in this way:

> We will assign a real number $Pr(A)$ to every event $A$, called the **probability** of A [@wasserman_all_2003,3]

Events can then be expressed as *random variables*. Probabilistic models treat response or target variables as random variables. Note that this number is 'real', meaning that it can take infinitely many values (by convention, between 0 and 1). That means, following the kind of definition that we find in a standard statistics textbook, that each variable is a function: 'a random variable is a mapping that assigns a real number to each outcome' [@Wasserman_2003,19]. If events have probabilities, random variables have a range of outcomes that are mapped to numbers. Mapping is a form of one-to-one correspondence, usually expressed as a mathematical function (as we saw in Chapter 1). A random variable links events to numbers through functions.  Again, all this remains rather formal and abstract. Given that my purpose here is not drill readers in probability theory, how does this account of probability as numbers assigned to events (the outcomes of dice roll, or the outcomes of many dice rolls, the chance a particular email is spam, the possibility that a given tissue sample is malignant, the chance that a particular visitor to a website will click on the advertisement displayed on the upper left hand side of the screen, etc.)?

At this point, we could move in several different directions. The more theoretical one would be to reconsider and perhaps re-conceptualize probability, and then use that reconstruction to reappraise the development of more recent probabilistic models such as topic models. Two lines of thought might be useful here, one stemming from philosophical work on number by Alain Badiou [@Badiou_2008] and the other pertaining to recent theoretical accounts of computation developed by Luciana Parisi [@Parisi_2013]. Although they have very different concerns, both Badiou and Paris attribute an incalculable or non-computable excess to numbers. Badiou draws on set theory to do this and Parisi draws on algorithmic information theory. To just give some flavour of the way in which they think through this excess, I provide two quote from them. Parisi proposes to: 'take Chaitin’s computational proof of Omega or infinite probabilities, which are at once computably enumerable and algorithmically random, to explain that the automation of data implies an irreversible encounter with incomputable probabilities' [@Parisi_2013,192]. Further on she writes: 'computation reveals that algorithms are actual entities imbued with discrete infinities that can also be defined as incomputable probabilities' [@Parisi_2013, 175-176]. It would take quite a long time to track down what Parisi points to here, but the salient point would be that  it has been mathematically proven by Gregory Chaitin that one can algorithmically specify thoroughly random numbers that by definition cannot be computed. This very much follows on Alan Turing's work on computability (see [@Mackenzie_2007]).

Similarly, Alain Badiou's account of number presents numbers as mundane incarnations of more expansive natural multiplicities: 'essentially, a Number is a fragment sectioned from a natural multiplicity; a multiplicity thought, as ordinal, in its being qua being' [@Badiou_2008, 211]. Badiou's idea is that by re-conceptualizing number as an 'ordinal' -- a kind of numbering introduced through set theory -- we might develop a much wider sense of number than the calculative one. Following either Parisi or Badiou's line of thought would take us far afield, but they both point to the need to re-think calculation in a much more plural ontological vein in order to avoid being swamped by the abundance of numbers and operations on numbers.  Both Parisi's account of computation in terms of algorithmic randomness and Badiou's account of the ontological excess of number open up promising leads. At this point, however, I think the strangeness of the machine learning techniques themselves offers a way to re-think what is happening via probabilistic models. A direct contrast between the two techniques of Naive Bayes and topic models might be one way to point to this shift since both techniques are used for document classification, albeit different kinds of documents.

## The allocations of topics in collections

\begin {equation}
\label {eq:lda}
p(\theta, \mathbf{z, w} | \alpha, \beta) = p(\theta | \alpha) \prod p(z_n | \theta)p(w_n | z_n , \beta),
\end {equation}
[@Blei_2003, 996]

We have seen that Naive Bayes was first used in the 1960s on both technical documents and medical data. Around 2000, the Latent Dirichlet Allocation or topic model was developed by David Blei, Andrew Ng and Michael Jordan, working at Berkeley and Stanford [@Blei_2003]. There is a stark contrast between \ref{eq:naive_bayes} and \ref{eq:lda}. If we put them side by side and view just the visual forms of expression, the much greater typographic variety of the LDA function suggest a more complicated structure or more convoluted forms of movement taking place in the data. Much of the typographic variety in \ref{eq:lda} stems from the role of probability distributions. Although Latent Dirichlet Allocation is often applied to document classification, nothing in the model essentially refers to documents or texts. It is rather more generic than that, since the only assumption built into the model is that there are collections of items that relate to one another in multiple ways. The technical term for topic model - Latent Dirichlet Allocation - already suggests that it refers to something hidden ('latent') and this hidden thing exists in the mode of 'allocation' or as a distribution (in either sense of the term -- probability and logistics). Across the line in \ref{eq:lda}, the proliferation of Greek symbols such as $\theta$ or $\beta$ informs us, according to the rather pleasingly consistent convention used in such expressions, that we are dealing with model parameters or numbers that exist only within the world of the model. As in \ref{eq:naive_bayes}, capital Greek characters act as operators. They carry multiplication or adding. Similarly, the presence of bold symbols such as $\mathbf{w}$ or $\mathbf{z}$ implies we are dealing with vectors of numbers rather than single values.[^13] Compared to \ref{eq:naive_bayes}, the difference is the preponderance of the letter 'p' in the LDA expression. They are on both sides of the expression. Why do we see so many probabilities?

[^13]: See Chapter 2 on the geometries of vectors in machine learning. 

In their initial publication in the _Journal of Machine Learning Research_, Blei, Ng and Jordan contrast their approach to document classification with the standard approaches to document classification developed during the 1980s and 1990s by the sub-field of computer science known as 'information retrieval' [@Manning_2008]. In existing information retrieval approaches, as they point out, documents are transformed into matrices of word counts. In such 'term-document matrices,' each row represents the frequency of a given term or word across all the documents in the collection. The relative position of the word and its proximity to other words does not figure at all in the so-called 'bag of words' model embodied in the term-document matrix. The transformation of documents into term-document matrices nevertheless allowed a series of dimensional reduction or compression techniques to be applied to documents. For instance, the technique of latent semantic indexing (LSI) developed in the late 1980s reduced the dimensionality of the often huge term-document matrices by finding those cross-cutting components of the matrix that best captured the variation in frequencies of different terms [@Deerwester_1990].[^12]  LSI and the latent semantic analysis (LSA) are somewhat statistical in their classification of documents since they work with variances in word frequencies across documents, but largely rely on a linear algebraic intuition to trace out the semantic features of documents. By contrast, the topic models presented by Blei, Ng and Jordan treat the documents in a much more probabilistic way by seeing words/terms and  documents of the documents as mixtures whose joint probability is '“conditionally independent and identically distributed,” where the conditioning is with respect to an underlying latent parameter of a probability distribution' [@Blei_2003, 995], the possibly multiple topics of the document. Again, as in most generative models, latent or hidden variables are quite important. In LSI and LSA, the latent variables were effectively ways of capturing most of the variation across the term-document matrix. As we saw in Naive Bayes, the latent or hidden variable there was the class of the document (spam or not). In topic models, the latent variables are the topics, but the topics now understood themselves as probability distributions. Blei, Ng and Jordan pose the model in a fairly standard way:

>Latent Dirichlet allocation (LDA) is a generative probabilistic model of a corpus. The basic idea is that documents are represented as random mixtures over latent topics, where each topic is characterized by a distribution over words [@Blei_2003, 996].

In some ways this is a subtle shift. Words in documents have been counted in many different ways by computers over the last five decades. Maron's work at the RAND Corporation on scientific abstracts was certainly doing that. Generally, however, they have been counted as if they were separate from each other, as if their order did not matter, or if did matter, only in localized ways (in so-called _n-gram_ treatments of documents, the frequencies of pairs, triples, etc of words are counted). The latent variables that topic model introduced into the generative model allows the documents to be seen as _mixtures_ of topics rather than _bags_ of words. That is, the topics themselves are now random variables in the same way that words were in the Naive Bayes spam classifier. One consequence of this shift is a change in the character of the classifications topic models produce. They are no longer the 'hard' classifications into spam or not-spam. Documents are 'random mixtures over latent topics.' A single document might have many topics, and in a collection of documents -- a corpus -- the topics may be distributed across a highly complex landscape defined by the Dirichlet distribution. 

If topics are defined by probability distributions, then everything that depends on topics takes on a slightly different sense. The many uses of topic models since 2003 (Blei, Ng and Jordan's paper had received over 2000 citations in Thomson Scientific's _Web of Science_ by 2014) suggests that this treatment of topics as distributions over collections of documents captures something of wide-ranging interest. I leave aside the question here of whether these techniques work very well or not. While Blei and co-authors demonstrate the application of the technique to scientific journal article abstracts from the _c. elegans_ literature, to Associated Press articles, to a set of Reuters newswire article (Reuters-21578 dataset [@Lewis_1997]), and to a dataset of movie recommendations (again, the science-public-media conjunction), the topics of  topic modeling have certainly taken on a mixed existence as they move into different settings (and at this point, I could begin again listing the many applications of the techniques, and this would be a set of references whose profile outlines contemporary science, government and cultures). For present purposes, and especially in countenancing what it means to have 'all the data,' the precise details of these distributions are less important than what they do in the model. Note here how the authors invoke sampling:  

>there are three levels to the LDA representation. The parameters $\alpha$ and $\beta$ are corpus-level parameters, assumed to be sampled once in the process of generating a corpus. The variables $\theta_d$ are document-level variables, sampled once per document. Finally, the variables $z_dn$ and $w_dn$ are word-level variables and are sampled once for each word in each document [@Blei_2003, 997]

The function shown in \ref{eq:lda}, as mentioned above, mixes random variables of different provenances. Some come from the model itself. Model parameters such as $\theta$, a parameter that expresses the mixture of topics in a given document, are shaped by the key probability distribution, the Dirichlet, in the model. They are not sampled from the data, but are sampled from a probability distribution whose shape is very unlikely to be the neatly symmetrical form of a Gaussian or normal distribution, but is much more like a high-dimensional alpine landscape. Indeed, evaluating these model-level parameters entails both sampling operations and intensive numerical optimisation in estimating parameters that cannot be calculated analytically (that is, by find exact solutions to equations). A further mathematical expression might make this multi-level sampling and estimation somewhat more tangible. Blei and co-authors introduce the probability of a given document containing a particular sequence of words in the following way:

\begin {equation}
\label {eq:lda_doc}
p(\mathbf{w}|\alpha, \beta) = \int p(\theta|\alpha)(\prod_{n=1]^{N}\sum_{z_n}p(z_n|\theta)p(w_n|z_n, \beta))\mathrm{d)\theta
\end {equation}
[@Blei_2003, 996]

Despite the profusion of symbols here, this expression is re-wrapped version of \ref{eq:lda}. Instead of expressing the joint probability of the topics and the words, as that equation did, it specifies how the words in a given document are generated. As always in these highly subscripted and superscripted typographic forms, the underlying operations remain relatively simple. Multiple and nested operations of adding and multiplying flow from symbols such as $\int$ and $\prod$.

[^12]: For an account of how Latent Semantic Indexing, and the transformation of documents into matrices itself mutates as it moves across disciplines, see [@Mackenzie_2013]

## Parameter estimation as incomplete data


Almost everything we have seen in relation to the more complicated generative models such as Latent Dirichlet Allocation suggests that having all the data does not do away with issues of sampling. Rather than obviating sampling, $N=all$, or having all the data (as in, for instance, having all the abstracts for published papers, or having the whole corpus of emails, or the whole collection of Twitter messages for a given period) leaves much still to be sampled. In Blei, Ng and Jordan's 2003 paper, the delineation of Latent Dirichlet Allocation as a model takes four pages but the description of how to estimate the parameters of the model fills more than a dozen pages (not including technical appendices). The model can only work if its parameters can be assigned, but these parameters cannot be computed directly. As they write: 'The key inferential problem that we need to solve in order to use LDA is that of computing the posterior distribution of the hidden variables given a document: ... Unfortunately, this distribution is intractable to compute in general' [@Blei_2003,1003]. What does it matter that these hidden variables are computationally intractable or that they are distributions? Only that the parameters themselves have become random variables with probability distributions. Remember that, as mentioned above, probability distributions only exist as ways of expressing some of the different ways that things can turn out. Now it turns out that the world is not the only place where things happen, and where events occurs as subsets and combinations. The model, that refuge or fortress of calculability, is populated by events or by forms of randomness that are difficult to order. Put more technically, a sample space, the set of possible outcomes, and its events (the subsets of $\Omega$, the sample space [@Wasserman_2003, 3]) has rent the tight fabric of the generative abstraction.

Hence, as the bulk of the paper goes on to discuss, the real issue is to construct procedures that will allow the sample space of the model itself to be sampled. 
A complicated set of technical developments appear in the aftermath. In this paper, Blei and co-authors use a variant of the highly-important expectation maximization procedure, an algorithm developed in the late 1970s  by Arthur Dempster, Nan Laird, and Donald Rubin working at Harvard University in the Educational Testing Service [@Dempster_1977]. A huge literature drawing on the EM algorithm ensued in the wake of the 1977 paper by Dempster, Laird and Rubin (over 39,000 citations according to Google Scholar, and 16,000 citations in the Thomson Scientific Web of Science, making one of most cited statistics papers ever) because EM allowed the estimation of model parameters to be conceived as a problem of incomplete observations or missing data. The remedy they proposed for estimating model parameters was to see those parameters themselves as forms of data gained through observation no longer of the world but of the behaviour of the model subjected to various experiments. As they write, 'we refer to $\mathbf{x}$  as the complete data even though in certain examples  $\mathbf{x}$ includes what are traditionally called parameters' [@Dempster_1977, 1]. The implication is that model parameters become like data, just data that cannot be observed directly. Subsequent versions of topic model soon introduced other parameter estimation techniques. Already in 2004, analyses of scientific literature using topics models appeared [@Griffiths_2004] that 'sampled from the target distribution using Markov chain Monte Carlo' [@Griffiths_2004, 5229] (a technique I discuss in the following chapter). All of these techniques, however, maintained a commitment to computational sampling of intractable model parameters. They drew on approaches developed by statisticians and approaches developed by physicists. 

## Conclusion

Mayer-Schönberger and Cukier's argument that having much data or all data ($N=all$) is a leitmotif in accounts of predictive modelling and analytics during the last decade.

>Using all the data makes it possible to spot connections and details that are otherwise cloaked in the vastness of the information. For instance, the detection of credit card fraud works by looking for anomalies, and the best way to find them is to crunch all the data rather than a sample [@Mayer-Schonberger_2013, 27]. 

Versions of this claim can be found running through various scientific and business settings throughout the 20th century.[^6] In certain settings, $N=all$ has been around for quite a while (as for instance, in many document classification settings where the whole corpus of documents have been electronically curated for decades). They rightly emphasize, it seems to me, that the huge quantities of data sluicing through some contemporary infrastructures support inferences of probabilities (12). I like the way they describe statistical sampling as a concept 'developed to solve a particular problem at a particular moment in time under specific technological constraints' [@Mayer-Schonberger_2013, 31]]. They do not, however, suggest that the very possibility of spotting connections or details that might matter deeply itself relies on probabilistic processes that the probabilistic models most fully exemplify but that run through all the predictive models in use today. Whether or not someone uses Naive Bayes, a topic model, neural networks or logistic regression, probabilistic practice, random variables and probability distributions run deep in the model. In many ways, the Mayer-Schönberger and Cukier account pays so much attention to the potentials of data accumulation that they cannot easily attend to the question of what happens to the data as people try to 'spot connections and details', or of how what is put into the data that goes beyond $N=all$.  Here sampling, estimation, likelihoods, and a whole gamut of dynamic relationships between random variables in joint probability distributions reassert themselves, as we have seen in the case of the topic models. The data may not be sampled, but the model is sampling as it tries to move through the high-dimensional feature spaces opened up by having 'all' the data. 

The contrast between the two models discussed above -- Naive Bayes and topic models -- highlights something of how models go into the data. Both models begin with transformations of documents into matrices of numbers that count the occurrence of words.  The Naive Bayes is almost purely probabilistic in that it assumes no relation between the random variables that comprise it apart from the fact that they comprise events. Topic models, by contrast, pay inordinate attention to the relations between the random variables embodied as words. It puts into the words a form of probabilistic abstraction -- the topic -- that cannot be seen directly, no matter how much data one has. The hidden or latent variables it models -- the topics -- are inferred from the data and not sampled from it. The form of 'drilling into the data' expressed by such models is not an exploration or mining or even a fishing venture.   


[^6]: Part  II of this book will track several actual instances of having all the data in the sciences, in government and in business in order to show what having all the data entails in different settings. 

## References

