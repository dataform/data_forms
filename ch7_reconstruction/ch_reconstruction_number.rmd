
# 7. Are there enough numbers in the world?

Number supplies and feature engineering 
 Reconstructing the flow of numbers in data

## to do

- joint distributions -- the argument from baroque piece -- really important to bring that out here as a post-disciplinary argument -- joint distributions and generative models
- in ch1, I said I'd return to multiplication. This is one place I want to talk about the problems of implementing multiplication
- add in Badiou style argument about numbers

## from the proposal

## Key examples: A/H1N1, Google Flu;

## Key techniques: transmission models, nested models

  - functions & supply chains; APIs; multiplication & convolution; states and functions of the lived;

A predominant narrative around data in many contemporary settings urges that more data makes all problems solveable. This narrative is usefully accompanied by an 'abundance of data' ('big data', 'data deluge', etc) narrative, in which the advent of data corresponds to a groundswell change in how we make sense of and intervene in events. Versions of these narratives surface in genomics, business analytics, and infrastructure management (e.g. in smart energy grids), as well as crisis-events such as financial collapses or epidemics. Via a case study of different data flows during the 2009 A/H1N1 'swine flu' epidemic, this chapter develops an alternative narrative of data flow in terms of number supply chain logistics. The chapter reconstructs a real-time epidemiological model that combines clinical reports, laboratory test data, web surveys, urban population mixing patterns in order to disentangle biological and social forms of contagion and infection during the 2009 epidemic in London. In reconstructing this model, a model that is typical in complicated engagement with numbers of diverse origins, the chapter will suggest that the largely  homogeneous data flows envisaged and embraced in many forms of data practice largely ignore the problem of the interactions between different agents. It specifically contrasts  the much publicised Google Flu Trends approach to 'flu prediction, which is based on search query volumes, with epidemiological models based on multiple forms of surveillance data. The chapter argues  that data practices during crises or times of great uncertainty, entail hybrid integrations of existing data practice and new forms of data.



The elements of data are often said to be numbers. But those numbers are not the same. This is something that machine learning cannot itself register. That is, the algorithms and techniques cannot deal with the way in which numbers are multiple, and and exist in different ways. We cannot learn about the differences between data without reconstructing those differences in the data. There are different ways to go about doing this. This chapter contrasts two fundamentally different approaches to number. One of them, typical of machine learning, assumes that everything is in the data, as long as machine learning techniques are careful enough in their movement through it. The other, more typical of certain sciences such as epidemiology or ecology, tends to see the relation between different kinds of data as a problem to be explored in its own right, rather than handed over to the algorithms. The chapter discusses what it might mean for machine learning to do something more like this. 


## Quotes to use

Also, machine learning is not a one-shot process of building a dataset and running a learner, but rather an iterative process of running the learner, analyzing the results, modifying the data and/or the learner, and repeating. Learning is often the quickest part of this, but that is because we have already mastered it pretty well! Feature engineering is more difficult because it is domain-specific, while learners can be largely general purpose. However, there is no sharp frontier between the two, and this is another reason the most useful learners are those that facilitate incorporating knowledge. [@Domingos_2012, 84 ] 
-- have used a bit of this quote in ch1


```{r echo=FALSE} 
opts_knit$set(dev='CairoPDF')
```


## Abstract

Contemporary data flows in science, business, government and media draw numbers from two kinds of places. In some places, numbers are assembled and assigned through observing, counting and measuring. In other places, numbers are assembled in a different way. Such places are usually defined mathematically as functions and implemented algorithmically in software. The numbers that come from them are not necessarily assigned to anything in the world as facts, keys or codes. They are functional numbers. The time-space signature of calculation that results from the convolution of  these two different supply chains of numbers has textures and traits that directly the unfolding of events.  This paper reconstructs a symptomatic crisis-event, the  2009 A/H1N1 'swine flu' influenza pandemic, from the standpoint of number flow in  and around epidemiological models. In their sometimes drastic re-shaping of lived space-times, epidemics generate and attract a plethora of numbers relating to human/non-human populations, human/non-human biology, vectors of infection, patterns of urban mobility, media use, clinical practice and laboratory tests. In disentangling social and biological numbers in infection, contemporary epidemiological models convolve relatively sparse numbers in the world through large supply chains of random numbers.  Examining some key mathematical functions, datasets, plots and computer model code, the paper reconstructs how these different kinds of numbers mix or 'convolve'. It suggests that the convolution of numbers to predict and control epidemics, or to disentangle different aspects of events more generally, might provoke us to think of how numbers multiply in multiple ways. 

## keywords

number, epidemic, contagion, calculation, statistics, event, influenza




> The first distinguishing characteristic of thinking then is facing the facts â€“ inquiry, minute and extensive scrutinizing, observation [@Dewey_1957, 140].

Is it possible to find in numbers ways of thinking about groups, co-existence and modes of togetherness? And not just in numbers in their mathematically purified forms, but in their heavily repeated, calculated and amassed modes of existence? This paper offers a kind of group or crowd theory based on practices of multiplication. Like much science and technology studies work, it seeks to closely map some terrain of contemporary science, in this case, the epidemiology of flu pandemics. Like some recent social theory, it finds mathematical practice, even heavily applied mathematics, philosophically provocative and a useful juxtaposition to over-confident assertions of the sheer power of accumulating numbers to make all processes transparent.  The analysis focuses on multiplying as a practice. Numbers multiply in both the sense of proliferating and in the sense of coming together as units from groups. There are tensions and uncontrollable slippages in the multiplication of numbers that play out in each and every setting in which numbers weave into the composition.

On page 18243 of the November 2011 issue of the _Proceedings of the National Academy of the Sciences_, the following mathematical function appears:

$$
L(\varphi) = \prod_{t=1}^{n_t}\prod_{a=1}^{n_a}
= L(w_{ta}|\varphi) \times L(x_{ta}^{CC}|\varphi) \times L(y_{ta}|m_{ta}^v,\varphi) \times L(z_{ta}|m_{ta}^{s},\varphi)
$$

[@Birrell_2011, 18243] 

A line of mathematics is a rather brutal way to open a social theory paper, but this function might begin to resonate under scrutiny. It appears in a paper entitled 'Bayesian modeling to unmask and predict influenza A/H1N1 dynamics in London', in the section 'Materials and Methods', subsection, 'Modelling Challenges'. The dynamics of the 2009 global influenza pandemic have been widely researched, and received much media attention [@Briggs_2009]. Approximately 16,000 deaths occurred, and cases were confirmed in 214 countries [@World_organisation_who_2009]. A/H1N1 was epistemically and mediatically contagious too. [Google Flu](http://www.google.com/flu), an online flu tracking tool based on web search data, attracted scientific and media attention [@Butler_2013; @Cook_2011] as a way of predicting and mapping infections in various countries (but still not at the time of writing four years later the United Kingdom or London).  Google Flu attested, it seemed, to the epistemic potency of sheer numbers of repeated, low-level individual actions -- Google searches -- to positively expose and help control crisis-events. The world had, after all, just been stunned and mesmerised just a year or two earlier by the power of massive financial number flows to avalanche and negate social and economic life.

A large scientific effort, yielding many publications, sought to track, map, predict and make sense of the A/H1N1 epidemic. Scientific articles appeared rapidly after A/H1N1, in some cases during the pandemic itself. Much of the epidemiology was understandably concerned with real-time prediction in the interests of limiting the spread of infection. The paper  I'm discussing is  an example. [@Birrell_2011] has eleven authors, some from the UK Medical Research Council Biostatistics Unit in Cambridge, some from the Health Protection Agency (HPA; a UK government agency charged with protecting 'the community from infectious diseases' [@Hpa_2012]), some from the Tropical Medical Research Unit, Mahidol University, Bangkok, others from the Centre for Outbreak Analysis and Modelling, Imperial College, as well as someone from Fu Consulting, a small data consultancy company in London. This mixture of government and university scientists, along with data consultants is somewhat unusual for a biomedical publication, but reflects the diverse range of theoretical and practical interests in controlling infectious diseases.

Preceded by a dozen other diagrams and tables of numbers, as well as five pages of text themselves liberally sprinkled with numerical data, Equation (1) is the first actual mathematical expression given in the paper. The fact that this function  expresses a *likelihood* indicates we are in the domain of statistical thinking. The expression is semiotically and materially connected to events in time and place in ways that are, as I will suggest, subtly symptomatic of the plight of number. Note that the object of analysis here is a population level, time-varying, place-specific event: the 2009 'swine flu' epidemic, which triggered government and public health action on multiple levels in the UK, lasted around nine months. Antiseptic hand-gel dispensers were attached to the doorways of schools and offices; signage about handwashing appeared above washbasins; advertising campaigns promoting the use of paper tissues for sneezes ran in public transport, and vaccination campaigns were mounted.  

Epidemics are typical of the kinds of events that attract numbers and number practices. Epidemics, infection and particularly viral pandemics, are heavily biopolitically invested crisis events, as many scholars have shown in relation to HIV/AIDS [@Martin_2009; @Mykhalovskiy_aids_2009], SARS [@Cohen_2011; @Erni_2008] and foot and mouth disease [@Law_2011]. Infection and contagion, especially in urban settings such as London, Mexico City (where A/H1N1 was first detected), Hong Kong or Toronto, touch off tightly enmeshed chain reactions in almost every register of social life, ranging from inchoate sensations of infection that trouble our sense having a body through to drastic realignments enacted through systems of governance (for instance, in the 2003 SARS outbreak, quarantine measures were imposed in Toronto). Epidemics, contagion and infection have also been significant objects of theoretical interest in media and social theory, where they have been used to conceptualise processes of copying, reproduction and mimesis, particularly in digital cultures [@Parikka_2007; @Sampson_2012]. While epidemic actors such as viruses, microbes, everyday life, urban patterns of mobility and the politics of population health have received much attention in this work, the flows, aggregations, mergers and *convolutions* of data, and particular count-based numbers, have been less often addressed. Across the social sciences and humanities engagements with epidemics and infections, numbers have both been ever-present (how many infections, how many deaths, how many animals culled, etc), yet the shaping, movement and textures of these numbers are difficult to explore, and have largely been ignored. They have been seen as largely complicit with the biomedical sciences, and therefore somewhat inimical to cultural and social theory, as if the social was a-numerate and the natural/technical was enumerable. 

Epidemiological knowledges are replete with numbers and calculations. Social scientists and other scholars such as Erika Mansnerus and Susanne Bauer [@Mansnerus_2009; @Mansnerus_2011; @Bauer_2008] emphasize the manifold social construction of the epidemiological numbers, facts and models, especially in association with simulations and modelling of the effects of vaccination and other public health strategies. Although these numbers figure in different ways, I want to propose that the flow of numbers through events have definite shapes, textures and patterns of composition that are endemic to these events. Birrell's paper offers two ways of looking at this: the diagrams or 'figures'; and the code/data used to generate those figures. Most visibly and perhaps accessibly,  sixteen diagrams in Birrell's paper (apart from Figure 2 ('Model Schematic Diagram of the Data Generating Process'), as well as a further 63 diagrams in the article's *Supporting Information,* are time series plots of the 245 days of the pandemic. The time series plots -- data points measured at successive times -- maps temporally to the course of the epidemic in different ways. Some summarise surveillance data on the epidemic, some present calculations of infections produced by the model. Figure 1 shows an example of the 'noisy' surveillance data: visits by different age groups to GPs in London. Across the seven age groups of the London population, visits to doctors during the latter half of 2009 occurred in two main waves. Peak visit numbers occurred in the week before school summer holidays. A second wave of visits occurred in the autumn 2009. 

```{r gp_consult_python,echo=FALSE, engine='python', results='asis'} 

import pandas as pn
import matplotlib.pyplot
df = pn.DataFrame.from_csv('data/Simulated_GP_Consultations_LondonAM.txt', sep='\t')
df.plot()
file_loc = 'figure/gp_consultation_simulated.pdf'
matplotlib.pyplot.savefig(file_loc, bbox_inches=0)
print '![GP Consultations](' + file_loc + ')'

```

Other graphs that appear almost identical to this one follow in the paper. Although iconically they still display the two peaks during 245 days, their topography differs subtly from the figure shown above. While this figure is produced by plotting the numbers of people of the seven different age groups visiting their doctors, many of the other figures show 'actual' numbers of A/H1N1 cases over time. These plots are the product of modelling work, and their topography is supported by much computation. These curves 'unmask' the epidemic by 'disentangling epidemic and behavioural dynamics' [@Birrell_2011, 18241]. This disentangling relies on creating entanglements, or, for reasons I will discuss, *convolutions* between different kinds of numbers.  Again, it is the shape and texture of these convolutions that strikes me as particularly important to countenance.

The small sample of data for laboratory confirmed 'seropositive' cases of A/H1N1 shown in Table 1 is typical of the flow of numbers entering the model. For each age group, on each day, a small number of cases are confirmed as A/H1N1 by virtue of the presence of antibodies specific to A/H1N1 in their blood serum. 

```{r sample_matrix, echo=FALSE, message=FALSE, warning=FALSE, results='asis'} 	
	library(xtable)
	sero <- read.delim('data/sim_seroposAM.txt')
	colnames(sero) = c('date', '<1yr','1-4yr', '5-14yr', '15-24yr', '25-44yr', '45-64yr', '65+yr')
	tab<-xtable(sero[60:70,])
	caption(tab) <- 'Sample of seropositive data for August 2009'
	print(tab, 'latex', comment=FALSE)
```
In the C++ code for the model of the pandemic, such numbers are almost all rendered in the spatial form of the matrix or array. This tabular form evenly spaces numbers on a grid. The code except shown below describes the data structure for a 'Region':

		// REGION STRUCTURE
		struct Region{
		  string name;
		  gsl_vector* population;
		  double total_population;
		  regional_model_params det_model_params;
		  gsl_matrix* GP_consultation_count_data;
		  gsl_matrix* GP_consultation_coverage_data;
		  gsl_matrix* Hospitalisation_data;
		  gsl_matrix* Death_data;
		  gsl_matrix* Seropositives_data;
		  gsl_matrix* Serosamples_data;
		  gsl_matrix* Virology_sample_size_data;
		  gsl_matrix* Virology_positive_data;
		  model_statistics region_modstats;
		};

The data from Table 1 would, for instance for stored in a matrix (a 'gsl_matrix': a [GNU Scientific Library matrix](http://www.gnu.org/software/gsl/)) in the variable 'Seropositives_data.' This matrix forms just a small starting point for the very many operations of copying, moving, adding, updating and deleting of the numbers that occurs as the model runs. We could track what happens to the Seropositives_data matrix component of the Region structure as it moves through the different parts of the epidemic model, but this would involve trekking through much more code, along with all the typographic and optical intricacies that brings. 


The likelihood expression shown above envelopes and channels such numbers into these plots in specific ways. It embodies a logistics and architecture of number flow that governs the assignment of the parameters of the model that Paul Birrell and his ten co-authors developed for the 2009 London A/H1N1 epidemic. The iconicity, the algebraic substitutability, and above all, the way that this expression *multiplies* numbers attests, as I hope to show, to the subtle kinds of re-shaping of the world wrought by contemporary number flow.  We might start to understand the potency of this multiplication by treating the function semiotically. In *Knowing Capitalism*, Nigel Thrift writes: 'number tends to cast the world reciprocally in its image as entities are increasingly made in forms that are countable. Number performs number.' [@Thrift_2005,589]. Equation (1) is an image of number. Like diagrams, all equations have an iconic mode of existence. As Charles Sanders Peirce writes,'[e]very algebraical equation is an icon, in so far as it *exhibits*, by means of the algebraical signs (which are not themselves icons), the relations of the quantities concerned' [@Peirce_1955, 107]. In Equation (1), this iconic exhibiting appears particularly strongly in the product $\prod$ and multiplication $times$ operators. The product operator, like the summation operator $\sum$, points to multiple multiplications. In this case, the first term of the equation $\prod=1}^{n_t}$ expresses multiplication over all the 273 days of the A/H1N1 epidemic in the summer of 2009. Similarly, the second term, $\prod_{a=1}^{n_a}$ expresses multiplication across data relating to people in seven different age groups. Equation (1) is not simply an equation. It is a *function*, an expression of 'the relations between quantities,' as Peirce puts it, quantities that all take the form of numbers, even if these numbers are very differently qualified. There are actually two expressions of this function here that are more or less equivalent. The first one basically says multiply all the values as they change over time with all the values as they change over age, the age of individuals in a population. The second one says the same thing, but enumarates what numbers are to be multiplied. The performance of number by number in this case comes from multiplications running across sequences of data originating from many different places -- thousands of doctor's surgeries, clinical pathology laboratories, online 'flu surveys, large-scale Europe-wide surveys of how often city dwellers encounter each other [@Mossong_2008], etc. Below we will  trace some of the number supply chains feeding this function below, but for present purposes, we need only note that while the two large product operators ($\prod$) iconically signify the time of epidemic and the age profile of the people of London, the second function, with all its parameters($w_{ta}, x_{ta}^{CC}$ etc), indexes specific number supplies that concern the epidemic in time and space. 

As I will seek to show, in accompanying the multiplication of numbers in a typical state-of-the-art, crisis-management model, we might also find ways of bringing number more directly into social theory. Less apophantic than Alain Badiou's mathematical ontology of 'Number' as 'a fragment sectioned from a natural multiplicity' [@Badiou_2008, 211], and less monolithic than Google Flu's claim that the pattern of web searches tells all, the convolutionary character of A/H1N1 number chains in epidemiological models might be seen as an architectural diagram of contemporary computational number flows.

## Epidemiological variables and the transmission of infection

What relation is expressed in all the multiplication going on Equation (1)? While Peirce says a function is a relation between quantities, no numbers actually appear there. Instead we see a dozen _variables_ that stand in for particular kinds of numbers, _parameters_.  Furthermore, as mentioned above, Equation (1) is a 'likelihood function.' The function $L(\varphi)$ expresses a *likelihood,* and in particular, the likelihood that the parameters of the epidemiological model ('we denote the collection of all model parameters by the vector $\varphi$' [@Birrell_2011, 18243]) take on particular values given the available data about lab tests, doctors visits, etc. Akin to the notion of probability (an immensely important category of contemporary numbers about which I say little more here), a likelihood is an estimate of how likely it is that the set of parameters chosen for a model are the right ones given the data. The higher the likelihood, the more likely the model is to fit the data. A well-chosen set of parameters for a model have a high likelihood. The likelihood function, as we have seen, is expressed as the product or multiplication of sequences of data relating to such things as 'the virological positivity at time $t$ in age group $a$, based on a sample of size $m^v_{ta}$' or '$y_{ta}$ is a realization of $Y(t,a)$, the number of GP consultations at time $t$ in age group $a$' [@Birrell_2011, 18243]. Observe that in Equation (1), the epidemic does not appear as such. The model of the actual epidemic takes another form entirely, as we will see. But the point here is that the likelihood function connects two different semiotic processes. The iconic process expresses multiplicative relations. This is the algebraic form of the function. In turn, the iconic expression of multiplication is coupled with an indexical weave densely signified in subscripts and superscripts that connect to age groups, institutions, biological processes, laboratory testing, and indeed, the complicated rhythms of life in a big city.

Where then is the epidemic in this paper? In the *Supporting Information* section of the article, we find a set of equations that look different. Rather than a likelihood function telling us how likely it is that the model parameters could generate the actual observed data, we find this system of equations: 

$$S(t_n, a) = S(t_{n-1},a)(1-p_\lambda(t_{n-1},a))$$ 
$$E_1(t_n,a)= E_1(t_{n-1},a)(1-\sigma\partial t) + S(t_{n-1},a)p_\lambda(t_{n-1},a)$$
$$E_2(t_n,a)= E_2(t_{n-1},a)(1-\sigma\partial t) + E_1(t_{n-1},a)\sigma\partial t$$
$$I_1(t_n,a) = I_1(t_{n-1},a)(1-\gamma\partial t) + E_2((t_{n-1},a))\sigma\partial t$$
$$I_2(t_n,a) = I_2(t_{n-1},a)(1-\gamma\partial t) + I_1((t_{n-1},a))\gamma\partial t$$
$$R(t_n,a) = R(t_{n-1},a) + I_2((t_{n-1},a))\gamma\partial t$$

>where $t_n = n\partial t$ and $p_\lambda(t_n,a)$ is the proportion of susceptibles infected in the interval $[t_{n-1},t_n)$ [@Birrell_2011,S2].

We do not have space to discursively reconstruct the architecture of the model laid out in  the semiotic and typographic matrix of these six difference equations (S1).  This series of expressions has many more compartments than the likelihood function. This system of equations describes a 'transmission model' (S1) for the epidemic. The transmission model is the standard predictive model in epidemiology [@Keeling_2008]. Transmission models focus on how people transit between infection-related states, and they describe what happens to a population in terms of rates of changes in state. This approach dates back almost a century to studies of infectious diseases carried out by British epidemiologists such as Ronald Ross in India (see [@Hardy_2002] for a review of this work). In epidemiology, populations are understood as 'a number $P$ of living things affected by something' [@Ross_1916,208]. In an epidemic, a population hosts an infection by transmitting it internally (or sometimes via another organism). During the epidemic, living things are in different states of infection, and they move between states over time. The underlying conception of populations here is 'compartment models.' Compartment models are probably the most widely used model in biology and medicine [@Ellner_2006, 7] in everything from pharmacokinetics to fisheries management. Members of a population are treated as if they are in one of several states or compartments, and the model describes how they move from one compartment to another. The number of compartments and the possible moves between them vary from model to model. Infection occurs at different rates, so at any given time, the affected population will be made up of individuals in different states or compartments. There will be a proportion of the population susceptible to infection (S), there will people exposed to infection (E), there will be an infectious group (I), and a proportion of either recovered or dead people (R). The difference equations shown above describe the rate of movement between these states. These different groups or proportion of the population are brought together in a series of equations that comprise a transmission model. The predictive power of so-called *SEIR* 'transmission' models pivots on the calculation of the transmission rates between S, E, I and R states. But these rates depend on the values of parameters such as $\gamma$, $\sigma$ and $p_\lambda$ that themselves need to be measured or estimated on the basis of biological, medical and social evidence.

Transmission models, because they are based on shifts between pre-defined compartments, might seem rather mechanistic. The difference equations describe, however, a *non-linear dynamic system* . Rather than multiplying numbers as we saw in the likelihood function, these equations express the change in various quantities both as a function of each other, and as a function of the previous state of the model. As [@Keeling_2008, 25] observe, 'it is typically not possible to derive an exact equation that predicts the evolution of model variables through time' for a non-linear dynamic system. Widely used in engineering, biology, physics, economics and many other settings, difference equations use discrete time steps to simulate how quantities change in relation to other each in time. 

Difference equations and differential equations have, we should note, have significantly influenced certain strands of  philosophical theory in its account of relationality and becomes. Much of Gilles Deleuze and Felix Guattari's influential account of becoming [@Deleuze_1994], as well as Deleuze's _Difference and Repetition_ [@Deleuze_2001] for instance, predicates change as a differential process in which processes continually flow and feed back through each other. As Manuel DeLanda has documented, their conceptualisation of intensities and differentiation is deeply indebted to differential calculus, and indeed to dynamic systems theory [@Delanda_2002]. Looking at the equations, we can see that apart from S (initial susceptibility to infection), each of the main states in the model - _E, I, R_ - is expressed in terms of the others.  We should note however, that unlike the continuous variation expressed in differential equations, difference equations step through time in discrete steps (245 days in this case; time steps are built into these equations in the form of $t_{n-1}$ and in the many $\partial t$ expressions)). These discrete time steps make it possible to write computer code to iterate through the shifting values of _S, E, I, R_ over time (in the Birrell model, the values of _S, E, I, R_ are updated twice each day of the pandemic).

	
There is one further feature of the dynamic model we should note. The dynamics of transmission lie at the heart of the pandemic and perhaps many other crisis events. With different parameters, more or less the same model could be used for HIV/AIDS, measles, chlamydia or, perhaps, a ponzi scheme. More widely, dynamic models are used to control many kinds of flow-related processes in production, logistics and deep within communication infrastructures. Yet the numbers of  people exposed, infected or  recovered are perhaps less important in the event than the questions of how fast the infection is spreading, and how many people could be affected by it. For disease and process control, the numbers that shape rates of change are more important than the numbers that count how many members of the population are infected, exposed, susceptible or recovered. No doubt the former depend on the latter, but the rate parameters describe the dynamics of transmission. These rate parameters are inconspicuous yet pivotal in the model of the epidemic, and indeed in any setting where growth or change is of interest. As Birrell writes, 'the transmission model ... is governed by a time and age-varying force of infection, $\lambda(t,a)$, and transition rate parameters $\sigma$ and $\gamma$' [@birrell_bayesian_2011,S2]. In a typical epidemiological model, modellers focus on assigning values to these governing numbers. The basic reproduction ratio, $R_{0}$ and the force of infection $\lambda$, 'the per capita rate at which susceptible individuals contract the infection' [@keeling_modeling_2008,17]. To just focus on $R_0$, this number depends on how fast the infection is transmitted (the transmission rate) and how long people remain infectious (the infectious period). Different infections have widely different basic reproductive ratios. While seasonal influenza (the common flu) is somewhere around 3-4, measles in humans is 16-18 [@keeling_modeling_2008,21]. Depending on how many infections there are to start with, these numbers shape what happens in the epidemic. So for instance, in the UK foot and mouth epidemic of 2001, in the global SARS pandemic of 2003, in the bovine TB badger culling trials currently in process in parts of England or in the extensive modelling that was done in the USA after September 2001 of possible bioterrorist releases of smallpox, these numbers --- the basic reproduction ratio and the force of infection --- guided culling, quarantining and vaccinating policies. In Birrell's  second difference equation, we can see that the number of exposed individuals at time $t$ refers to both $\sigma$ (which itself indexes the latent period of the virus in the human body) and to $p_\lambda(t_{n-1},a)$, a densely synthetic number that varies over time, according to individual age, the replication rate of the virus as expressed in $R_0$, the 'basic reproduction number,'' and time-dependent contact patterns of the population. This compaction of numbers into even a single parameter such as $p_\lambda$ is I would suggest worth thinking about. 


## Connecting flows of numbers

A gap separates the model parameter likelihood function Equation (1),  and the transmission model (S1) we have just been discussing. On the one side, stands the statistical likelihood function, which defines the likelihood of the model parameters   given the data. On the other side, stands the dynamical system difference equations that  express at a given point in time what proportions of the population will be in different infection-related states (S,E,I,R).	 How do they relate? We have glimpsed already in the closely interwoven functions and equations a matrix of semiotic processes indicative of different kinds of number flow. The equations themselves diagram in their typographic arrangement certain relations. They index, particularly in the weave of subscripts and superscripts, flows of numbers of diverse provenance. And what if the encounter between different kinds of numbers constituted a mode of togetherness of contemporary collectives? As Whitehead suggests, if all quantity presupposes an underlying pattern ('apart from a presupposed pattern, quantity determines nothing. Indeed quantity itself is nothing other than analogy of functions within analogous patterns' [@Whitehead_1956, 195]), the encounter between different kinds of numbers is a compositional event. It makes patterns. 

But as yet, we have not seen how or where these two apparatuses, the statistical and the dynamic models, meet. 
Indeed, this is the very problem Birrell and co-author themselves wish to address. They begin by pointing to this  difference between numbers:

>The tracking and projection of emerging epidemics is hindered by the disconnect between apparent epidemic dynamics, discernible from noisy and incomplete surveillance data, and the underlying, imperfectly observed, system. Behavior changes compound this, altering both true dynamics and reporting patterns, particularly for diseases with nonspecific symptoms, such as influenza. [@Birrell_2011, 18238]

The disconnect they talk about here between apparent dynamics as presented in the observational data and the 'underlying, imperfectly observed system' (the transmission model) is their topic of interest. The 'true dynamics' of the disease implicitly refers to the transmission of viral infections as it moves around in a population. What the virus is doing and what is happening to it cannot be seen directly. Even when a London doctor in September 2009 sees a patient who presents with flu-like symptoms, not all flu is A/H1N1. It might be ILI (influenza-like illness). Conversely, some people infected by the A/H1N1 virus  show no symptoms. If every body could be periodically tested for presence of the virus, then we could observe 'the system' (I will have more to say about the system below). But that is quite simply impossible. Only some bodies can be tested for the virus, and not very often. There are national and regional surveillance systems in the UK as in many other countries, but they are incomplete and in many ways necessarily slow. This is the first problem that Birrell et. al. face as they seek to 'unmask' the epidemic. 

Numbers relating to epidemics face a second problem. Even if we could count virus-affected individuals, how people are affected by awareness of infection would be hard to enumerate. For instance, public health authorities such as the UK HPA publicise the infections as part of their management strategy. There is the public sensitivity to swine flu that leads people to, for instance, visit a doctor because they think they have it or are worried about having it. To the extent they are affected by awareness of their own susceptibility, exposure or infection, they start to do something different. The population's _mixing patterns_ -- the everyday flows of contact that characterise collective life and shape the waves of infection -- might shift, and this would affect the basic numbers shaping the course of pandemic (e.g. the force of infection $\lambda$, the basic reproduction number of the virus $R_0$) as expressed in the transmission model. This shift,  it has been observed, heavily distorted Google Flu predictions in 2009 and again in 2012 [@Butler_2013].  Much of the Birrell paper discusses the way in which they sought to bridge the gap between what people were doing as they visited GPs, and the dynamics of the infection as discerned through various 'noisy and incomplete' data flows from medical practices, and clinical pathology laboratories. While people might have been going to see their doctors, having blood tests, contacting the National Pandemic Flu Service, seeking vaccinations, staying away from work, filling out the web forms on FluSurvey, their bodies, through contact with others, through complicated immune system orchestrations, were collectively doing something else. Put slightly more abstractly,  the collective life of the biological object -- the influenza virus -- is directly shaped by collective (as in population) and individual (as in organism) feeling and thinking about it. The way in which people respond to news about the pandemic shapes the rates of transmission and infection. The way in which people modify their relations to intimates and strangers has a measurable biological effect on the transmission of infection. Moreover, the data on each of these things has its own problems, its own qualities, density, ambiguities and noisiness. 

The forms of calculation Birrell and co-authors report in the paper all attempt to isolate the  simulacral epidemic, the one  in which a population acts as though it is  infected, even though it is not. They seek to nullify the 'behaviour changes' that affect both the apparent and real dynamics of the epidemic by bringing different sorts of data together. The kinds of data they bring together include GP reports on cases, serum samples taken by the Health Protection Agency (HPA), tests for the flu virus carried out by Royal College of GPs (RCGP) and the HPA Regional Microbiology Network, cases handled by telephone by the National Pandemic Flu Service (NPFS), data from the web-based FluSurvey, general population data from the Office of National Statistics, as well as statistics on the mixing of different age groups in the UK population over time. Their model of the infection is shaped by other models, including a model of how people come into contact with each other (as known as the 'mixing pattern') in London, and model of the reporting of the disease that predicts on the basis of virological tests how many of the people who present with flu-like symptoms have A/H1N1. It includes a model of GP consultations (again calender-time and age-structured) that reflects something of what people do during an epidemic in relation to NHS medical services. Do they tend to visit their GPs in ever greater numbers? Or do public health announcements and new medical services such as NPFS -- National Pandemic Flu Service -- change what people do in significant ways? Put simply, do more people visit doctors because they are ill or because they are worried about getting the flu? 

## The faultline in actual numbers and its multiplying consequences

This means that there is an uneven fault-line running through Birrell's paper where the different kinds of numbers fuse. The numbers that dynamic models produce  usually refer to actual physical, chemical, biological or sometimes social processes. The parameters they seek to calculate -- the actual $R_0$ and the force of infection -- are not statistical variables. They shape the dynamics of the epidemic or pandemic and hence, the authors tend to present these numbers as *actual* yet _unobservable_ numbers. Many of the other numbers they synthesise -- propensity to consult to a GP, proportion of reported cases confirmed as A/H1N1 -- are observable in the sense that they can be enumerated by observing the population, and modelled by probability distributions as random variables. As we have seen, these fluxing statistical numbers impinge upon the dynamic system of transmission in ways the authors seek to render visible in the model. The irregular, multi-scale flow of statistical numbers from the reporting, lab-tests, surveys and population mixing patterns percolates down into the difference equations via the transmission model parameters. But this flow is not a simple inputting or tabulating of numbers. On the contrary, most of the paper, including the 14 time-series figures included in the paper address the problem of folding these different kinds of numbers together in time. Applying the 'surveillance data' to shape the parameters of the transmission model, the authors can begin to 'unmask' the dynamics of A/H1N1 in London 2009, and at the same time, predict 'in real time' the most likely course of the pandemic. 

The unmasking of the epidemic occurs in a kind of percolation performed in the computer code for the model.  The code for computer models are perhaps the most literal rendering of the multiplicative practices we can obtain in this domain. Eight lines of code are shown below:

	 // UPDATING THE MODEL:
    for (register int t = 0; t < time_points - 1; ++t)// 
    {
        for (register int a = 0; a < NUM_AGE_GROUPS; ++a)
        {
            gsl_matrix_set(l_S, t + 1, a, gsl_matrix_get(l_S, t, a) * 
            	(1 - gsl_matrix_get(prob_infection, t, a)));
            
            gsl_matrix_set(l_E_1, t + 1, a, gsl_matrix_get(l_E_1, t, a) 
            	* (1 - (2 / (gsl_matrix_get(in_dmp.l_latent_period, t, a) * timestepsperday))) 
            	* + gsl_matrix_get(prob_infection, t, a) * gsl_matrix_get(l_S, t, a));

            gsl_matrix_set(l_E_2, t + 1, a, gsl_matrix_get(l_E_2, t, a) 
            * (1 - (2 / (gsl_matrix_get(in_dmp.l_latent_period, t, a) * timestepsperday))) 
            + (2 / (gsl_matrix_get(in_dmp.l_latent_period, t, a) * timestepsperday)) * gsl_matrix_get(l_E_1, t, a));

            gsl_matrix_set(l_I_1, t + 1, a, gsl_matrix_get(l_I_1, t, a) * 
            (1 - (2 / (gsl_matrix_get(in_dmp.l_average_infectious_period, t, a) * timestepsperday))) 
            +  (2 / (gsl_matrix_get(in_dmp.l_latent_period, t, a) * timestepsperday)) * gsl_matrix_get(l_E_2, t, a));

            gsl_matrix_set(l_I_2, t + 1, a, gsl_matrix_get(l_I_2, t, a) 
            * (1 - (2 / (gsl_matrix_get(in_dmp.l_average_infectious_period, t, a) * timestepsperday))) 
            + (2 / (gsl_matrix_get(in_dmp.l_average_infectious_period, t, a) * timestepsperday)) 
            * gsl_matrix_get(l_I_1, t, a));

            gsl_matrix_set(l_R, t + 1, a, gsl_matrix_get(l_R, t, a) 
            + (2 / (gsl_matrix_get(in_dmp.l_average_infectious_period, t, a) * timestepsperday)) 
            * gsl_matrix_get(l_I_2, t, a));

            }
        } 

Again, as when observing the mathematical formalisms, the precise choreography of numbers on the move in this code need not concern us. We see variables such as `t`, `a` and `l_E` that map directly onto variables in the difference equations. We also see `*`  or multiply operators proliferating. The `get` and `set` operations on larges matrices of numbers held in memory (`gsl_matrix_set`) abound. And, finally, nested `for` loops traverse the time of the pandemic and the age groups of the London population. (Not coincidentally the instruction `register` that specifies that important numbers such as the current time period and current age-group should be held in hardware storage on the central processing unit of the computer in order to maximise the speed of the code.)

What happens in this plying  and percolating of different kinds of numbers across different matrices and registers? Thrift observes that 'few accounts have tended to work out in any detail what the space-time signatures of a lifeworld that was heavily calculated (or, as I would have it, qualculated) would look like, even though it could be argued that this is the world that we are increasingly living in' [@Thrift_2004, 596]. He advocates  'sometimes fevered projection'  coupled with 'an attention to the basic basics _[sic]_ of everyday life' in order to 'obtain some measure of what is going on and what is falling away as new kinds of subjectivity are forced into existence by spaces and times that ... exceed and transform existing spaces and times as they apply a new set of arts of distribution' [@Thrift_2004, 601]. The phrase 'new set of arts of distribution' is doubly evocative in this case, since it has both logistic and statistical connotations. Particular cases like Birrell's and co-authors show different numbers weaving together. In their threads, we glimpse something of the 'heavily calculated' space-time signatures, and in particular, we might begin to grasp the extent of the  mixing of numbers going on here. But the multiplication goes much deeper than this code sample suggests.


## Convoluting distributions

I am not sure whether the move that comes next is metaphysically marvelous or depressing. It is this: more than a dozen key parameters in the model are modelled as random variables, or as numbers whose values take the form of probability distributions rather than single, fixed values. That is, the model of the epidemic is itself defined and controlled by parameters that have no single fixed value because they themselves as random variables effectively act as populations. This is a very loopy development when you think about it. Populations are modelled by populations. For instance, the modellers assumed that the proportion of infected individuals who actually will show symptoms such as a fever can be described by a *Beta* distribution, which resembles the $\theta$ curve in Figure 2. Similarly, the 'mean infection period' ($d_i$), an important parameter in the transmission model is modelled as a *Gamma* distribution ($\Gamma$):


```{r distributions_py, engine='python', echo=FALSE, fig.cap = 'Generating numbers', results='asis'} 
import numpy as np
import matplotlib.pyplot as mp
n=100000

theta = np.random.beta( 32.5, 18.5, size=n)
d_i = np.random.gamma(shape=518, scale=1.0/357.0, size=n)

mp.subplot(2,2,1)
mp.plot(d_i)
mp.xlabel(r'$d_i$')
mp.subplot(2,2,2)
mp.hist(d_i, bins=1000,facecolor='red')
mp.xlabel(r'$d_i$')
mp.subplot(2,2,3)
mp.plot(theta)
mp.xlabel(r'$\theta$')
mp.subplot(2,2,4)
mp.hist(theta, bins=1000,facecolor='g')
mp.xlabel(r'$\theta$')
file_loc = 'figure/generated_numbers.pdf'
mp.savefig(file_loc, bbox_inches=0)
print '![Generated numbers](' + file_loc + ')'
```

The case shown above addresses the problem of individuals who are infected, but perhaps happily for them display no symptoms. Birrell et. al.  use a *Beta* distribution to express an initial or 'prior' belief in the proportion of individuals who show symptoms (the parameter $\theta$ in the model). The values 32.5 and 18.5 initially shape the distribution of values for the proportion of symptomatic infections. The median value of that distribution is around 0.63 (63% of infected individuals display symptoms). After running the model, the prior distribution is reshaped and had a new median value: 0.328 (~33%), which is significantly different. Similar modifications of other parameters key to the unmasking of the epidemic can be tracked: the propensity to consult a GP, the proportion of cases confirmed by the pathology laboratory, the delay time between a visit to a GP and the GP reporting the visit, etc, etc. All of the numbers that help unmask the epidemic can onlyenter as probability distributions, distributions of possible values that cannot be measured as such. Here then another kind of movement adds onto the likelihood function (Equation 1) and the difference equations of the transmission model. The 'probability density functions' embodying the model parameters, as Deleuze and Guattari suggest, produce 'a fantastic slowing down' [@Deleuze_1994, 118] of the changing shapes and rates of infection, the propensity of people to visit their doctors, the speed of laboratory work (itself affected by the volume of cases involved in a pandemic, etc. Slowing-down the dynamics of 'behaviour' or 'public sensitivity' depends on numbers derived from variously shaped probability distributions, themselves described by functions such as Beta, Gamma, Normal and Uniform probability density functions. 

The random variables describing the London population during 2009 are themselves populations, but now largely generated by the model itself as it processes the various datastreams. The model, like the city, is a place where populations mix with each other, even as it itself seeks to describe the mixing of population in the course of a pandemic. In the model, the population of London becomes a population of biological, social, and probabilistic populations interacting or mixing with each other. This interaction is precisely what was expressed in Equation (1), the first function we observed, the likelihood function. It was saying that at each time point, and for each age group, the interactions between a population of values for each of the model parameters should be multiplied.[^2]

[^2]: We still, however, do not know how this slowing down of the intense interactions between the social and biological, between public sensitivity and viral contagion can be obtained from the surveillance data. Key unmasking numbers are modelled as probability distributions, and exist as populations within the model. Instantiating  key variables as probability distributions generates many more numbers. Rather than the thousands or tens of thousands of data points from the various data sources (GP visit reports, virological reports, serological reports, the web-based FluSurvey), data points need to be generated as 'realisations' or simulations of random variables shaped by probability functions.  Here the supply of data from observations is enveloped in a torrent of numbers generated randomly by the computer and shaped by probability density functions. These shaped flows of random numbers become the sample on which the model depends in estimating the parameters that describe the rates of actual infection in London, 2009. In the model, the population of London becomes a population of populations, all of which are interacting with each other, like a crowds of people walking at cross purposes in a busy public place, constantly deviating, swerving and flowing around each other. At the heart of the C++ code that Birrell and co-authors wrote, stands a set of calculations concerned with  a form of statistical inference known as 'Bayesian statistics'. Without elaborating on the 'Bayesian revolution' in statistics of the last two decades (see [@Mcgrayne_2011] for a popular account), we can intuit the slowing-down that occurs as Equation (1) is folded through the transmission model as an intricate multiplication. 

The distributions instantiating important model parameters  have to be multiplied by each other as they change over time. But what appear as multiplication in Equation (1) is no ordinary multiplication. It is actually *convolution* (also known as a 'composition product;' previously called *Faltung*). In convolution, one function is applied to another in a moving time window to produce a special kind of product, or many-sided addition.[^1] This strange kind of multiplication does more than multiply. Effectively it multiplies all the values over time by all the other values over time, and then adds the results together to produce a new time-varying pattern. A more typical product simply takes each value and multiplies it by some other corresponding value. In terms of a geometrical intuition, multiplying two values produces an area or a surface enclosed by lines (for instance, we multiply the lengths of the sides of a square to find the area). The intuition for convolution is more like plotting the  shadow cast by an object as it moves in front of a light source. The shadow changes  depending on where the object stands in relation to the light. The convolution integral, then, is an anamorphic  shadow. The convolutions in Birrell's model are one place where we see how numbers can generate space-times that re-configure existing planes and surfaces of life. 

[^1]:A textbook definition of convolution as a mathematical operation describes it in terms of integration: 'Convolution integrals arise in various applications in which the behaviour of the system at time _t_ depends not only on its state at time _t_, but on its past history as well' [@William_1969, 255].

Two lines of code from the heart of the model, the central loop where all the model parameters, the $\varphi$, are being updated, delineate the convolutional process:

	
	// Central Loop //
	for(; int_iter < simulation_parameters.num_iterations; int_iter++) {
	    	...

		log_accep += random_walk_proposal(*(gsl_vector_ptr(theta_i->proposal_value, a_component)), 
			    *(gsl_vector_ptr(theta_i->param_value, a_component)),
			    (distribution_type) gsl_vector_int_get(theta_i->prior_distribution, a_component),
			    gsl_vector_get(theta_i->proposal_variances, a_component),
			    r);
    }

These lines relate to Equation (1) in a straightforward  way. The first line, the 'Central Loop' of the model,  is the typical code construct we have already encountered: software is full of loops, or repeated operations. Loops are the basis of responsiveness, liveness, and repetition in nearly all software. In this case, the number of loops in the model is governed by a parameter, `simulation_parameters.num_iterations`. As Birrell et.al., report, they ran their model many times: 'two separate chains, each consisting of 450,000 iterations, were run in parallel' [@Birrell_2011, 18243]. In the name of what were these chains of iteration done? As the model has nineteen parameters, each parameter was updated for each day of the epidemic, and often with respect to each the seven age groups, 450,000 times in 'two separate chains.' 

$$Iterations = {19 parameters } \times {249 days} \times {twice daily} \times {7 age groups} \times {450,000 iterations} \times {2 chains}$$

More than twenty billion iterations of the model parameters are fed through the SEIR transmission as it moves through the 249 day time steps of the difference equations in order to estimate the numbers of infections, the number of recovered cases, etc. This estimate of twenty billion updates of the model parameters does not take into account the many additions, subtractions, divisions and multiplications occurring at lower levels within components of the iterations (for instance, in the random number generating functions). This is just a small sign of the number supply running underneath and drawing together the  numbers  the researchers gather from various sources. It is this undergirding of numbers in the world by number supplies internal to the model that lends contemporary epidemiology and similar processes their predictive purchase on the world.[^partial]

[^partial]: The second line of code also bears scrutiny: it describes what happens to each model parameter (`theta_i`) amidst this relentless iteration. Every parameter has an initial value (`param_value`) and an existing or prior distribution type (`prior_distribution`; that is, it has a normal, beta, gamma, Poisson or uniform probability density function defined by some shaping parameters, as discussed above). The function this line of code invokes -- `random_walk_proposal` -- is itself engaged in generating yet more numbers (random values), in order to see whether the overall direction or 'drift' refines the value of the parameter in question  This update of parameters through iteration chains based on 'random walks' is too complicated to describe here, but could be readily understood in terms of Deleuze and Guattari's notion of 'partial observers.' In their account of functions, partial observers are situated in functions, as points of view on directions, gradients, turning point, peaks and troughs of the function: 'the role of the partial observer is to perceive and to experience ... These partial observers belong to the neighbourhood of the singularities of a curve, of a physical system, of an organism' [@Deleuze_1994, 130]. The `random_walk_proposal` against which each parameter value is tested many times in the central loop of the model performs such partial observations. As the value of a model parameter is displaced by small random amounts 450,000 times, the proposed new value is accepted or rejected depending on whether it moves towards the neighbourhood of a singularity. By virtue of the partial observer who randomly walks and decides at each step whether they are moving closer to  the peak or most likely value for that parameter, the values tend to settle into a stable distribution that approximates the probability distribution of the parameter under 'observation.' 

## Reconstructing numbers

Convolution or time-shifting multiplication of many different numbers together in order to track entangled forms of social, clinical, mediatic and viral contagion during an epidemic. This mixing of  numbers in the interests of differentiating patterns of transmission has several broader implications.  Epidemics and pandemics collapse existing spatio-temporal orderings. The virality of viruses and other infectious agents has largely oriented social and culture theories of contagion. The figures of infection, contagion and viruses more generally have become significant tropes in social and cultural theory [@Cohen_2009], and especially in understandings of network media and media cultures, in recent years [@Sampson_2012; @Parikka_2007]. In both the actual epidemiologies and in the theoretical uptake of epidemiology as a way of thinking about cultural processes, the multiplication of numbers, and the different ways in which numbers flow in and out of *refer* to states of affairs has been little remarked, even though, I would suggest, it is a primary semiotic-material process in the  collapsing-proliferating dynamics of infection and other cascade-events.  Epidemiologies of infection and contagion typically entail much calculation in the pursuit of prediction or control. 

The reconstruction that Birrell and co-authors undertake is instructive in this regard. Their reconstruction, as we have seen, goes to great lengths to bring numbers of widely different origins (observed, generated) together in order to slow down the mixing of different layers of pandemic events. They include institutional practices, biological activity of various kinds, ranging from the way the virus propagates through to the ways in which different human immune systems respond differently, urban patterns of mobility (especially the infectiousness of children), actions of the state authorities, and a variety of media practice (including web surveys of flu), and the changing of the seasons (the summer peak, the autumn peak). Their model, and its convolution of these patterns with each other, aims  to 'unmask' (a slightly odd choice of term on reflection) the epidemic. Different forms of contagiousness, different forms of influence or relationality, become traceable through the construction of synthetic variables (the parameters such as propensity to consult a GP, etc).

Their reconstruction of the epidemic differs substantially from some other construals of the power of network-media to generate numerical data that renders all social processes visible. Around the time of the 2009 A/H1N1 pandemic, for instance, search engines queries relating to influenza on Google were being used by Google and others to predict the spread of infection [@Pelat_2009; @Ginsberg_2008; @Tilston_2010]. In the seasonal flu of 2012, it turns out that Google Flu Trends had 'drastically overestimated peak flu levels' [@Butler_2013]. Even during the 2009 A/H1N1 epidemic, the Google Flu Trends had gone awry in underestimating the numbers of A/H1N1 infections due to a change in people's web search behaviour [@Cook_2011]. By contrast, the equations, the tables and plots of time series data, the many model parameters, the superimposition of different models (reporting, background and transmission), and particularly, the extensive recourse to chains of random numbers as ways of exploring the likely distribution of model parameters, suggest that the time-space signature of contemporary epidemiological calculations are more convoluted. Millions of people were affected by the 2009 A/H1N1 pandemic in mundane ways (handwashing, visit to doctors, vaccination), but this effect could not be profiled simply through the putatively 'social practices' of web searching. The calculations  in the Birrell paper and others such as [@Baguelin_2010; @Baguelin_2011] are less flat and less homogeneous than the search engine query approach, for all its much-discussed perspicacity and speed. 

A fluxing dimensionality characterises the convolutional integration of the different number supply chains in the multilayered Bayesian model. I find it remarkable that the diversity of numbers derived from laboratories, surveys, clinics, and websites is shot through, to varying extents, by fluxes of random numbers shaped by different probability distributions. All of this speeding up is done in order to slow down the turbulent confluence of biological, social and mediatic strands of contagions in a teeming city. The widely-shared view that we are in a data deluge suggests that willingly or not we swim in a rising tide of numbers. But there are very different ways of swimming this tide. A transmission or epidemic model of numbers suggests that numbers in contemporary science, business, government and media multiply and figure in different ways. There are places in the world where numbers are assembled and assigned through counting and measuring. Such places, as science studies has extensively documented, include sensors, instruments, recording devices, screens, records, databases, laboratories, clinics, addresses and other forms of assigning and indexing. Such numbers are governed by 'functions of the lived.' There are other places where numbers are synthesised differently. Such places are usually processed through algorithmic implementations of mathematical functions (such as differential equations or probability distributions). The numerous numbers realised from them are not necessarily assigned to anything in the world as facts, keys, codes or labels. They are functional numbers, heavily dependent on computation. The two different chains of numbers come into contact in the mixing matrices of  operational and control systems, in digital media forms, in statistical models and simulations. What happens in these entanglements? What kind of transmission event or changes in state does the entanglement of different kinds of number occasion? 

Where different numbers convolve, we might develop a more epidemiological sense of the flow of numbers. My treatment of the A/H1N1 modelling leaves many trails unfollowed, both in directions of the public health surveillance systems that generate various streams of numbers, and in the statistical and computational underpinnings and scaffolding of the model, as well as the paths leading into government policies, and operational management of epidemics. The convolutions of the model bootstrap indexical chains of reference that link the model to various bodies, things and states of affairs. All of this remains somewhat arbitrarily cropped from view here. Nonetheless, the difference equations (S1), likelihood functions (Equation 1), graphs, tables, data structures and C++ `for` loops call our attention to the intricate, cantilevered architecture through which numbers multiply. In this architecture, with its convolutional turns, social, biological, institutional, mediatic and computational processes are heavily entwined. Indeed, this entanglement is the whole point of  the model: the massive slowing-down embodied in the Bayesian model of the epidemic is meant to show how numbers affect each other.  Alfred North Whitehead suggests that 'the very notion of number refers to the process from the individual units to the compound group. The final number belongs to no one of the units; it characterizes the way in which the group unity has been attained' [@Whitehead_1956, 127], and his simple example  $2 \times 3 = 6$  illustrates how multiplication fuses groups in the process of realizing forms. My reconstruction of an epidemiological model is more convoluted than Whitehead's example, but the presence of convoluted calculations suggests that 'group unity' can be attained in very different ways.   


## Acknowledgements

The first author of the A/H1N1 paper, Paul Birrell (Medical Research Council Biostatistics Unit, Cambridge UK), kindly provided sample data sets as well as the full  computer code in which the model was implemented.
 

## References
