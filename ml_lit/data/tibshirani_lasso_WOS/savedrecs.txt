PT	AU	BA	BE	GP	AF	BF	CA	TI	SO	SE	BS	LA	DT	CT	CY	CL	SP	HO	DE	ID	AB	C1	RP	EM	RI	OI	FU	FX	CR	NR	TC	Z9	U1	U2	PU	PI	PA	SN	EI	BN	J9	JI	PD	PY	VL	IS	PN	SU	SI	MA	BP	EP	AR	DI	D2	PG	WC	SC	GA	UT	PM
J	Marx, BD				Marx, BD			Iteratively reweighted partial least squares estimation for generalized linear regression	TECHNOMETRICS			English	Article						biased estimation; cross-validation; ill-conditioned information; latent variables; principal components		I extend the concept of partial least squares (PLS) into the framework of generalized linear models. A spectroscopy example in a logistic regression framework illustrates the developments. These models form a sequence of rank 1 approximations useful for predicting the response variable when the explanatory information is severely ill-conditioned. Iteratively reweighted PLS algorithms are presented with various theoretical properties. Connections to principal-component and maximum likelihood estimation are made, as well as suggestions for rules to choose the proper rank of the final model.		Marx, BD (reprint author), LOUISIANA STATE UNIV,DEPT EXPT STAT,BATON ROUGE,LA 70803, USA.						WOLD S, 1984, SIAM J SCI STAT COMP, V5, P735, DOI 10.1137/0905052; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; HELLAND IS, 1988, COMMUN STAT SIMULAT, V17, P581, DOI 10.1080/03610918808812681; HESTENES MR, 1952, J RES NAT BUR STAND, V49, P409, DOI 10.6028/jres.049.044; FRANK IE, 1993, TECHNOMETRICS, V35, P109, DOI 10.2307/1269656; MARX BD, 1990, BIOMETRIKA, V77, P23, DOI 10.1093/biomet/77.1.23; LECESSIE S, 1992, APPL STAT-J ROY ST C, V41, P191; DOBSON AJ, 1990, INTRO GENERALIZED LI; HELLAND IS, 1990, SCAND J STAT, V17, P97; LAND SR, 1994, P STAT COMP SECT AM, P100; MARTENS H, 1985, THESIS TU NORWAY TRO; Martens H., 1989, MULTIVARIATE CALIBRA; MARX BD, 1992, STAT MODEL, P227; McCullagh P., 1989, GENERALIZED LINEAR M, VSecond; NAES T, 1993, SCAND J STAT, V20, P239; STONE M, 1974, J R STAT SOC B, V36, P111; Wold H., 1975, PERSPECTIVES PROBABI	17	40	41	1	5	AMER STATIST ASSN	ALEXANDRIA	1429 DUKE ST, ALEXANDRIA, VA 22314	0040-1706			TECHNOMETRICS	Technometrics	NOV	1996	38	4					374	381		10.2307/1271308		8	Statistics & Probability	Mathematics	VP434	WOS:A1996VP43400012		
J	Lange, N; Zeger, SL				Lange, N; Zeger, SL			Non-linear Fourier time series analysis for human brain mapping by functional magnetic resonance imaging - Reply	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C-APPLIED STATISTICS			English	Editorial Material							CAT STRIATE CORTEX; VISUAL-CORTEX; FMRI; DISCRIMINATION; DISCHARGE; DIAGNOSIS; NEOPLASMS									ALEXANDER ME, 1995, P 3 SCI M SOC MAGN R, V1, P126; GAWNE TJ, 1993, J NEUROSCI, V13, P2758; ZOHARY E, 1994, NATURE, V370, P140, DOI 10.1038/370140a0; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; TSO DY, 1986, J NEUROSCI, V6, P1160; KWONG KK, 1992, P NATL ACAD SCI USA, V89, P5675, DOI 10.1073/pnas.89.12.5675; DEAN AF, 1981, EXP BRAIN RES, V44, P437; VANKAN PLE, 1985, EXP BRAIN RES, V60, P559; FORMAN SD, 1995, MAGNET RESON MED, V33, P636, DOI 10.1002/mrm.1910330508; Just MA, 1996, SCIENCE, V274, P114, DOI 10.1126/science.274.5284.114; BEASLEY LD, 1995, CLIN EXPT PHARM PHYS, V22, P550; Caviness VS, 1996, J COGNITIVE NEUROSCI, V8, P566, DOI 10.1162/jocn.1996.8.6.566; *DAN ALL BRAIN IN, 1995, DEL RES PROGR REP BR; DAS A, 1995, NATURE, V375, P780, DOI 10.1038/375780a0; Dunlop SA, 1996, EXP NEUROL, V137, P294, DOI 10.1006/exnr.1996.0028; Eddy WF, 1996, P COMP STAT, P39; Eddy WF, 1996, MAGNET RESON MED, V36, P923, DOI 10.1002/mrm.1910360615; ENGEL SA, 1994, NATURE, V369, P525, DOI 10.1038/369525a0; Friston K., 1994, HUMAN BRAIN MAPPING, V1, P153, DOI 10.1002/hbm.460010207; FRISTON KJ, 1995, NEUROIMAGE, V2, P157, DOI 10.1006/nimg.1995.1018; GENOVESE CR, 1996, IN PRESS ESTIMATING, V1; GENOVESE CR, 1996, IN PRESS STAT INFERE; HANNAN EJ, 1971, J APPL PROBAB, V8, P767, DOI 10.2307/3212240; HU X, 1996, IN PRESS EVALUATION; LANGE N, 1996, NEUROIMAGE, V3, pS75, DOI 10.1016/S1053-8119(96)80077-2; LEAN CL, 1995, J CLIN ENDOCR METAB, V80, P1306, DOI 10.1210/jc.80.4.1306; LEE AT, 1995, MAGNET RESON MED, V33, P745, DOI 10.1002/mrm.1910330602; Mason C., 1991, PRINCIPLES NEURAL SC, P420; MOELLER JR, 1987, J CEREBR BLOOD F MET, V7, P649; NIREL R, 1997, IN PRESS MODELLING A; NOLL DC, 1996, IN PRESS ESTIMATING; NOLL DC, 1996, P 4 A MEET INT SOC M, P343; RENSHAW E, 1983, APPL STAT-J ROY ST C, V32, P51, DOI 10.2307/2348042; RUSSELL P, 1994, AM J MED, V96, P383, DOI 10.1016/0002-9343(94)90071-X; SCARTH G, 1996, 4 M INT SOC MAG RES; SIEGMUND DO, 1995, ANN STAT, V23, P608, DOI 10.1214/aos/1176324539; SINGER W, 1994, LARGE SCALE NEURONAL; SOMORJAI RL, 1995, MAGNET RESON MED, V33, P257, DOI 10.1002/mrm.1910330217; Stoffer DS, 1987, J TIME SER ANAL, V8, P449, DOI 10.1111/j.1467-9892.1987.tb00008.x; Tuckwell HC, 1988, INTRO THEORETICAL NE, V2; VANLIESHOUT MNM, 1994, ADV APPL PROBAB, V26, P281; Worsley K., 1996, CHANCE, V9, P27; WORSLEY KJ, 1995, NEUROIMAGE, V2, P173, DOI 10.1006/nimg.1995.1023; WORSLEY KJ, 1995, ADV APPL PROBAB, V27, P943, DOI 10.2307/1427930	44	0	0	1	2	WILEY-BLACKWELL	HOBOKEN	111 RIVER ST, HOBOKEN 07030-5774, NJ USA	0035-9254	1467-9876		J R STAT SOC C-APPL	J. R. Stat. Soc. Ser. C-Appl. Stat.		1997	46	1					26	29				4	Statistics & Probability	Mathematics	WP958	WOS:A1997WP95800011		
J	Portnoy, S; Koenker, R				Portnoy, S; Koenker, R			The Gaussian hare and the Laplacian tortoise: Computability of squared-error versus absolute-error estimators	STATISTICAL SCIENCE			English	Article						l(1); L-1; least absolute deviations; median; regression quantiles; interior point; statistical preprocessing; linear programming; simplex method; simultaneous confidence bands	POLYNOMIAL-TIME ALGORITHM; REGRESSION QUANTILES; WAGE STRUCTURE; PATH	Since the time of Gauss, it has been generally accepted that l(2)-methods of combining observations by minimizing sums of squared errors have significant computational advantages over earlier el-methods based on minimization of absolute errors advocated by Boscovich, Laplace and others. However, l(2)-methods are known to have significant robustness advantages over l(2)-methods in many applications, and related quantile regression methods provide a useful, complementary approach to classical least-squares estimation of statistical models. Combining recent advances in interior point methods for solving linear programs with a new statistical preprocessing approach for l(1)-type problems, we obtain a 10- to 100-fold improvement in computational speeds over current (simplex-based) l(1)-algorithms in large problems, demonstrating that l(1)-methods can be made competitive with l(2)-methods in terms of computational speed throughout the entire range of problem sizes. Formal complexity results suggest that l(1)-regression can be made faster than least-squares regression for n sufficiently large and p modest.	Univ Illinois, Dept Stat, Champaign, IL 61820 USA; Univ Illinois, Dept Econ, Champaign, IL 61820 USA	Portnoy, S (reprint author), Univ Illinois, Dept Stat, 101 Iilini Hall,725 S Wright St, Champaign, IL 61820 USA.	portnoy@stat.uiuc.edu; roger@ysidro.econ.uiuc.edu					Anderson E., 1995, LAPACK USERS GUIDE, V2nd; KOENKER R, 1978, ECONOMETRICA, V46, P33, DOI 10.2307/1913643; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; KARMARKAR N, 1984, COMBINATORICA, V4, P373, DOI 10.1007/BF02579150; Charnes A, 1955, MANAGE SCI, V1, P138, DOI 10.1287/mnsc.1.2.138; Vanderbei RJ, 1986, ALGORITHMICA, V1, P395, DOI 10.1007/BF01840454; WAGNER HM, 1959, J AM STAT ASSOC, V54, P206, DOI 10.2307/2282146; BARTELS RH, 1980, ACM T MATH SOFTWARE, V6, P594, DOI 10.1145/355921.355930; Bloomfield P., 1983, LEAST ABSOLUTE DEVIA; BUCHINSKY M, 1994, ECONOMETRICA, V62, P405, DOI 10.2307/2951618; BUCHINSKY M, 1995, J ECONOMETRICS, V65, P109, DOI 10.1016/0304-4076(94)01599-U; Chamberlain G., 1994, ADV ECONOMETRICS; Chambers J. M., 1992, STAT MODELS S, P95; CHAUDHURI P, 1992, P 2 C DAT AN BAS L1, P169; CHEN S, 1995, IN PRESS SIAM J SCI; DIKIN I., 1967, SOV MATH DOKL, V8, P674; Edgeworth F., 1888, PHILOS MAG, V25, P184; Edgeworth F.Y., 1887, HERMATHENA, V6, P279; FIACCO A. V., 1968, NONLINEAR PROGRAMMIN; FLOYD RW, 1975, COMMUN ACM, V18, P165, DOI 10.1145/360680.360691; FRISCH R., 1956, CAHIERS SEMINAIRE EC, V4, P7; GAUSS CF, 1821, THEORIA COMBINATIONI; Gijbels I., 1996, LOCAL POLYNOMIAL MOD; GILL PE, 1986, MATH PROGRAM, V36, P183, DOI 10.1007/BF02592025; GONZAGA CC, 1992, SIAM REV, V34, P167, DOI 10.1137/1034048; Green P, 1994, NONPARAMETRIC REGRES; GUTENBRUNNER C, 1992, ANN STAT, V20, P305, DOI 10.1214/aos/1176348524; Gutenbrunner CJ, 1993, J NONPARAMETR STAT, V2, P307, DOI 10.1080/10485259308832561; HALL P, 1988, J ROY STAT SOC B MET, V50, P381; KOENKER RW, 1987, APPL STAT-J ROY ST C, V36, P383, DOI 10.2307/2347802; KOENKER R, 1994, BIOMETRIKA, V81, P673; KOENKER R., 1993, J ROY STAT SOC C-APP, V43, P410; Koenker Roger, 1994, ASYMPTOTIC STAT, P349; LAPLACE PS, 1789, MEMOIRES ACAD SCI PA, V11, P475; Lustig I. J., 1994, ORSA Journal on Computing, V6; Lustig IJ, 1992, SIAM J OPTIMIZ, V2, P435, DOI 10.1137/0802022; MANNING WG, 1995, J HEALTH ECON, V14, P123, DOI 10.1016/0167-6296(94)00042-3; Mehrotra S, 1992, SIAM J OPTIMIZ, V2, P575, DOI 10.1137/0802028; Meketon M, 1986, LEAST ABSOLUTE VALUE; MIZUNO S, 1993, MATH OPER RES, V18, P964, DOI 10.1287/moor.18.4.964; Oja H., 1983, STATISTICS PROBABILI, V1, P327, DOI 10.1016/0167-7152(83)90054-8; PORTNOY S, 1991, SIAM J SCI STAT COMP, V12, P867, DOI 10.1137/0912047; POWELL JL, 1986, J ECONOMETRICS, V32, P143, DOI 10.1016/0304-4076(86)90016-3; RENEGAR J, 1988, MATH PROGRAM, V40, P59, DOI 10.1007/BF01580724; BARRODAL.I, 1974, COMMUN ACM, V17, P319, DOI 10.1145/355616.361024; SHAMIR R, 1993, STAT SCI, V8, P57, DOI 10.1214/ss/1177011084; SIDDIQUI MM, 1960, J RES NBS B MATH SCI, V64, P145, DOI 10.6028/jres.064B.017; SONNEVEND G, 1991, MATH PROGRAM, V52, P527, DOI 10.1007/BF01582904; Stigler S. M., 1986, HIST STAT MEASUREMEN; STIGLER SM, 1984, BIOMETRIKA, V71, P615; Welsh AH, 1996, STAT SINICA, V6, P347; Wright M.H., 1992, ACTA NUMERICA, V1, P341, DOI DOI 10.1017/S0962492900002300; ZHANG Y, 1993, J OPTIMIZ THEORY APP, V77, P323, DOI 10.1007/BF00940715	53	161	176	4	9	INST MATHEMATICAL STATISTICS	HAYWARD	IMS BUSINESS OFFICE-SUITE 7, 3401 INVESTMENT BLVD, HAYWARD, CA 94545 USA	0883-4237			STAT SCI	Stat. Sci.	NOV	1997	12	4					279	296				18	Statistics & Probability	Mathematics	YQ352	WOS:000071377400005		
B	Osborne, MR; Presnell, B; Turlach, BA		Weisberg, S		Osborne, MR; Presnell, B; Turlach, BA			Knot selection for regression splines via the LASSO	DIMENSION REDUCTION, COMPUTATIONAL COMPLEXITY AND INFORMATION	COMPUTING SCIENCE AND STATISTICS (SERIES)		English	Proceedings Paper	30th Symposium on Interface Between Computing Science and Statistics	MAY 13-16, 1998	MINNEAPOLIS, MN	Nat Security Agcy, Univ Minnesota, Coll Liberal Arts, SAS Inst Inc		convex programming; dual problem; knot selection; regression splines	WAVELET SHRINKAGE	Tibshirani (1996) proposes the "Least Absolute Shrinkage and Selection Operator" (lasso) as a method for regression estimation which combines features of shrinkage and variable selection. In this paper we present an algorithm that allows efficient calculation of the lasso estimator. In particular our algorithm can also be used when the number of variables exceeds the number of observations. This algorithm is then applied to the problem of knot selection for regression splines.	Australian Natl Univ, Ctr Math & Applicat, Canberra, ACT 0200, Australia	Osborne, MR (reprint author), Australian Natl Univ, Ctr Math & Applicat, GPO Box 4, Canberra, ACT 0200, Australia.		Turlach, Berwin/A-4995-2008	Turlach, Berwin/0000-0001-8795-471X			Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Denison DGT, 1998, J ROY STAT SOC B, V60, P333, DOI 10.1111/1467-9868.00128; DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425; SILVERMAN BW, 1985, J R STAT SOC B, V47, P1; BAKIN S, 1997, COMPUTATIONAL TECHNI, P75; Brent RP, 1973, ALGORITHMS MINIMIZAT; Donoho DL, 1995, J AM STAT ASSOC, V90, P1200, DOI 10.2307/2291512; EUBANK RL, 1988, SMOOTHING SPLINES NO; Hjorth J. S. U., 1994, COMPUTER INTENSIVE S; HOCKING RR, 1977, MATH METHODS DIGITAL, V3, P39; Miller A.J., 1990, MONOGRAPHS STAT APPL, V40; OSBORNE MR, 1998, UNPUB NOTE LEAST ABS; Ripley B. D., 1994, MODERN APPL STAT S P; Teukolsky SA, 1992, NUMERICAL RECIPES C; WAND MP, 1997, UNPUB COMP REGRESSIO	15	19	19	0	0	INTERFACE FOUNDATION NORTH AMERICA	FAIRFAX	PO BOX 7460, FAIRFAX, VA 22039-7460 USA			1-886658-05-6	COMP SCI STAT			1998	30						44	49				6	Computer Science, Interdisciplinary Applications; Computer Science, Theory & Methods; Statistics & Probability	Computer Science; Mathematics	BN90B	WOS:000083408300007		
J	Harrell, FE; Margolis, PA; Gove, S; Mason, KE; Mulholland, EK; Lehmann, D; Muhe, L; Gatchalian, S; Eichenwald, HF				Harrell, FE; Margolis, PA; Gove, S; Mason, KE; Mulholland, EK; Lehmann, D; Muhe, L; Gatchalian, S; Eichenwald, HF		WHO ARI Young Infant Multicentre Study Grp	Development of a clinical prediction model for an ordinal outcome: The World Health Organization Multicentre Study of clinical signs and etiological agents of pneumonia, sepsis and meningitis in young infants	STATISTICS IN MEDICINE			English	Article							ORDERED CATEGORICAL RESPONSE; REGRESSION-MODELS; LOGISTIC-REGRESSION; PROGNOSTIC PREDICTION; COX REGRESSION; ODDS MODELS; LIKELIHOOD; VARIABLES; RESIDUALS; SELECTION	This paper describes the methodologies used to develop a prediction model to assist health workers in developing countries in facing one of the most difficult health problems in all parts of the world: the presentation of an acutely ill young infant. Statistical approaches for developing the clinical prediction model faced at least two major difficulties. First, the number of predictor variables, especially clinical signs and symptoms, is very large, necessitating the use of data reduction techniques that are blinded to the outcome. Second, there is no uniquely accepted continuous outcome measure or final binary diagnostic criterion. For example, the diagnosis of neonatal sepsis is ill-defined. Clinical decision makers must identify infants likely to have positive cultures as well as to grade the severity of illness. In the WHO/ARI Young Infant Multicentre Study we have found an ordinal outcome scale made up of a mixture of laboratory and diagnostic markers to have several clinical advantages as well as to increase the power of tests for risk factors. Such a mixed ordinal scale does present statistical challenges because it may violate constant slope assumptions of ordinal regression models. In this paper we develop and validate an ordinal predictive model after choosing a data reduction technique. We show how ordinality of the outcome is checked against each predictor. We describe new but simple techniques for graphically examining residuals from ordinal logistic models to detect problems with variable transformations as well as to detect non-proportional odds and other lack of fit. We examine an alternative type of ordinal logistic model, the continuation ratio model, to determine if it provides a better fit. We find that it does not but that this model is easily modified to allow the regression coefficients to vary with cut-offs of the response variable. Complex terms in this extended model are penalized to allow only as much complexity as the data will support. We approximate the extended continuation ratio model with a model with fewer terms to allow us to draw a nomogram for obtaining various predictions. The model is validated for calibration and discrimination using the bootstrap. We apply much of the modelling strategy described in Harrell, Lee and Mark (Statist. Med. 15, 361-387 (1998)) for survival analysis, adapting it to ordinal logistic regression and further emphasizing penalized maximum likelihood estimation and data reduction. (C) 1998 John Wiley & Sons, Ltd.	Univ Virginia, Hlth Sci Ctr, Dept Hlth Evaluat Sci, Div Biostat & Epidemiol, Charlottesville, VA 22908 USA; Univ N Carolina, Div Community Pediat, Chapel Hill, NC 27515 USA; World Hlth Org, Programme Control Acute Resp Infect ARI, Geneva, Switzerland; Papua New Guinea Inst Med Res, Goroka, Papua N Guinea; Univ Addis Ababa, Dept Paediat & Child Hlth, Addis Ababa, Ethiopia; Res Inst Trop Med, Alabang, Philippines; Univ Texas, SW Med Ctr, Dept Pediat, Dallas, TX USA	Harrell, FE (reprint author), Univ Virginia, Hlth Sci Ctr, Dept Hlth Evaluat Sci, Div Biostat & Epidemiol, Box 600, Charlottesville, VA 22908 USA.	fharrell@virginia.edu					AGRESTI A, 1989, STAT MED, V8, P1209, DOI 10.1002/sim.4780081005; ALTMAN DG, 1989, STAT MED, V8, P771, DOI 10.1002/sim.4780080702; Anderson J. A., 1981, Applied Statistics, V30, DOI 10.2307/2346654; ANDERSON JA, 1984, J ROY STAT SOC B MET, V46, P1; HARRELL FE, 1988, J NATL CANCER I, V80, P1198, DOI 10.1093/jnci/80.15.1198; MCCULLAGH P, 1980, J ROY STAT SOC B MET, V42, P109; ARMSTRONG BG, 1989, AM J EPIDEMIOL, V129, P191; GRAMBSCH PM, 1994, BIOMETRIKA, V81, P515; HOEFFDING W, 1948, ANN MATH STAT, V19, P546, DOI 10.1214/aoms/1177730150; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; HERNDON JE, 1990, COMMUN STAT THEORY, V19, P639, DOI 10.1080/03610929008830224; NAGELKERKE NJD, 1991, BIOMETRIKA, V78, P691, DOI 10.1093/biomet/78.3.691; CLEVELAND WS, 1979, J AM STAT ASSOC, V74, P829, DOI 10.2307/2286407; SCHOENFELD D, 1982, BIOMETRIKA, V69, P239, DOI 10.1093/biomet/69.1.239; HURVICH CM, 1989, BIOMETRIKA, V76, P297, DOI 10.1093/biomet/76.2.297; SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136; Harrell FE, 1996, STAT MED, V15, P361, DOI 10.1002/(SICI)1097-0258(19960229)15:4<361::AID-SIM168>3.0.CO;2-4; ASHBY D, 1989, STAT MED, V8, P1317, DOI 10.1002/sim.4780081104; ATKINSON AC, 1980, BIOMETRIKA, V67, P413, DOI 10.1093/biomet/67.2.413; BERRIDGE DM, 1991, STAT MED, V10, P1703, DOI 10.1002/sim.4780101108; BRAZER SR, 1991, J CLIN EPIDEMIOL, V44, P1263, DOI 10.1016/0895-4356(91)90159-7; Brieman L, 1984, CLASSIFICATION REGRE; Brier G. W., 1950, MONTHLY WEATHER REV, V75, P1, DOI DOI 10.1175/1520-0493(1950)078<0001:V0FEIT>2.0.C0;2; LECESSIE S, 1992, APPL STAT-J ROY ST C, V41, P191; COLE TJ, 1991, J ROY STAT SOC A STA, V154, P287, DOI 10.2307/2983042; Collett D., 1991, MODELLING BINARY DAT; COX C, 1995, STAT MED, V14, P1191, DOI 10.1002/sim.4780141105; COX DR, 1972, J R STAT SOC B, V34, P187; Cureton EE, 1983, FACTOR ANAL APPL APP; DAGOSTINO RB, 1995, STAT MED, V14, P1757, DOI 10.1002/sim.4780141605; Devlin TF, 1986, P 11 ANN SAS US GROU, P646; Efron B, 1993, INTRO BOOTSTRAP; FARAWAY JJ, 1992, J COMPUTATIONAL GRAP, V1, P213, DOI 10.2307/1390717; Fienberg SE, 1980, ANAL CROSS CLASSIFIE, V2nd; FOLLMANN D, 1995, STAT MED, V14, P1163, DOI 10.1002/sim.4780141103; GRAMBSCH PM, 1995, BIOMETRIKA, V82, P668, DOI 10.2307/2337547; GRAY RJ, 1992, J AM STAT ASSOC, V87, P942, DOI 10.2307/2290630; GREENLAND S, 1994, STAT MED, V13, P1665, DOI 10.1002/sim.4780131607; HARRELL FE, 1985, CANCER TREAT REP, V69, P1071; HARRELL FE, 1997, DESIGN S PLUS FUNCTI; HARRELL FE, 1984, STAT MED, V3, P143, DOI 10.1002/sim.4780030207; HASTIE TJ, 1989, STAT MED, V8, P785, DOI 10.1002/sim.4780080703; Jackson J. E., 1991, USERS GUIDE PRINCIPA; Jolliffe I. T., 1986, PRINCIPAL COMPONENT; KOCH GG, 1985, BIOSTATISTICS STAT B; KUHFELD WF, 1990, SAS STAT USERS GUIDE, V2, P1265; Lamport L., 1994, LATEX DOCUMENT PREPA, V2nd; LANDWEHR JM, 1984, J AM STAT ASSOC, V79, P61, DOI 10.2307/2288334; MARSHALL G, 1994, STAT MED, V13, P1501, DOI 10.1002/sim.4780131502; *MATHS INC, 1995, S PLUS US MAN VERS 2; PETERSON B, 1990, APPL STAT-J ROY ST C, V39, P205, DOI 10.2307/2347760; PHILLIPS AN, 1990, STAT MED, V9, P1189, DOI 10.1002/sim.4780091008; Ripley B. D., 1994, MODERN APPL STAT S P; Sarle WS, 1990, SAS STAT USERS GUIDE, V2, P1641; SAS Institute, 1990, SAS STAT US GUID, V2; SPIEGELHALTER DJ, 1986, STAT MED, V5, P421, DOI 10.1002/sim.4780050506; STONE CJ, 1995, P STAT COMP SECT ASA, P45; VANHOUWELINGEN JC, 1990, STAT MED, V9, P1303, DOI 10.1002/sim.4780091109; VERWEIJ PJM, 1994, STAT MED, V13, P2427, DOI 10.1002/sim.4780132307; WALKER SH, 1967, BIOMETRIKA, V54, P167, DOI 10.2307/2333860; WHITEHEAD J, 1993, STAT MED, V12, P2257, DOI 10.1002/sim.4780122404; *WHO ARI STUD GROU, 1997, UNPUB CLIN SIGNS ET; Yee TW, 1996, J ROY STAT SOC B MET, V58, P481; ZHOU XH, 1994, STAT MED, V13, P1737, DOI 10.1002/sim.4780131705	64	80	83	4	8	JOHN WILEY & SONS LTD	W SUSSEX	BAFFINS LANE CHICHESTER, W SUSSEX PO19 1UD, ENGLAND	0277-6715			STAT MED	Stat. Med.	APR 30	1998	17	8					909	944		10.1002/(SICI)1097-0258(19980430)17:8<909::AID-SIM753>3.3.CO;2-F		36	Mathematical & Computational Biology; Public, Environmental & Occupational Health; Medical Informatics; Medicine, Research & Experimental; Statistics & Probability	Mathematical & Computational Biology; Public, Environmental & Occupational Health; Medical Informatics; Research & Experimental Medicine; Mathematics	ZJ496	WOS:000073221800008	9595619	
J	Fu, WJJ				Fu, WJJ			Penalized regressions: The bridge versus the lasso	JOURNAL OF COMPUTATIONAL AND GRAPHICAL STATISTICS			English	Article						Bayesian prior; bridge regressions; GCV; Newton-Raphson; shrinkage; shooting method	NONORTHOGONAL PROBLEMS; RIDGE REGRESSION	Bridge regression, a special family of penalized regressions of a penalty function Sigma \beta(j)\(gamma) with gamma greater than or equal to 1, is considered. A general approach to solve for the bridge estimator is developed. A new algorithm for the lasso (gamma = 1) is obtained by studying the structure of the bridge estimators. The shrinkage parameter gamma and the tuning parameter lambda are selected via generalized cross-validation (GCV). Comparison between the bridge model (gamma greater than or equal to 1) and several other shrinkage models, namely the ordinary least squares regression (lambda = 0), the lasso (gamma = 1) and ridge regression (gamma = 2), is made through a simulation study. It is shown that the bridge regression performs well compared to the lasso and ridge regression. These methods are demonstrated through an analysis of a prostate cancer data. Some computational advantages and limitations are discussed.	Michigan State Univ, Dept Epidemiol, E Lansing, MI 48823 USA	Fu, WJJ (reprint author), Michigan State Univ, Dept Epidemiol, E Lansing, MI 48823 USA.						Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; FURNIVAL GM, 1974, TECHNOMETRICS, V16, P499, DOI 10.2307/1267601; HOERL AE, 1970, TECHNOMETRICS, V12, P55; FRANK IE, 1993, TECHNOMETRICS, V35, P109, DOI 10.2307/1269656; CRAVEN P, 1979, NUMER MATH, V31, P377; Efron B, 1993, INTRO BOOTSTRAP; Gill P. E., 1981, PRACTICAL OPTIMIZATI; HOERL AE, 1970, TECHNOMETRICS, V12, P69, DOI 10.2307/1267352; Lawson C. L., 1974, SOLVING LEAST SQUARE; SEBER GAF, 1977, LINEAR REGRESSION AN; Sen A. K., 1990, REGRESSION ANAL THEO; Stamey T., 1989, J UROLOGY, V16, P1076	12	280	296	3	15	AMER STATISTICAL ASSOC	ALEXANDRIA	1429 DUKE ST, ALEXANDRIA, VA 22314 USA	1061-8600			J COMPUT GRAPH STAT	J. Comput. Graph. Stat.	SEP	1998	7	3					397	416		10.2307/1390712		20	Statistics & Probability	Mathematics	121HY	WOS:000076008800010		
J	LeBlanc, M; Tibshirani, R				LeBlanc, M; Tibshirani, R			Monotone shrinkage of trees	JOURNAL OF COMPUTATIONAL AND GRAPHICAL STATISTICS			English	Article						CART; classification and regression trees; LASSO		We investigate a new method for regression trees which obtains estimates and predictions subject to constraints on the coefficients representing the effects of splits in the tree. The procedure leads to both shrinking of the node estimates and pruning of branches in the tree and for some problems gives better predictions than cost-complexity pruning used in the classification and regression tree (CART) algorithm. The new method is based on the least absolute shrinkage and selection operator (LASSO) method developed by Tibshirani.	Fred Hutchinson Canc Res Ctr, Seattle, WA 98109 USA; Univ Toronto, Dept Publ Hlth Sci, Toronto, ON M5S 1A8, Canada; Univ Toronto, Dept Stat, Toronto, ON M5S 1A8, Canada	LeBlanc, M (reprint author), Fred Hutchinson Canc Res Ctr, 1100 Fairview Ave N, Seattle, WA 98109 USA.						Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; HARRISON D, 1978, J ENVIRON ECON MANAG, V5, P81, DOI 10.1016/0095-0696(78)90006-2; Becker R. A., 1988, NEW S LANGUAGE; Breiman L., 1984, CLASSIFICATION REGRE; HASTIE TJ, 1991, SHRINKING TREES	5	11	11	0	1	AMER STATISTICAL ASSOC	ALEXANDRIA	1429 DUKE ST, ALEXANDRIA, VA 22314 USA	1061-8600			J COMPUT GRAPH STAT	J. Comput. Graph. Stat.	DEC	1998	7	4					417	433		10.2307/1390674		17	Statistics & Probability	Mathematics	145GF	WOS:000077362100001		
J	Sauerbrei, W; Royston, P				Sauerbrei, W; Royston, P			Building multivariable prognostic and diagnostic models: transformation of the predictors by using fractional polynomials	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES A-STATISTICS IN SOCIETY			English	Article						breast cancer; model stability; nominal p-value; resampling; variable selection; variable transformation	CROSS-VALIDATION; REGRESSION-MODEL; BREAST-CANCER; BOOTSTRAP; SELECTION; SURVIVAL	To be useful to clinicians, prognostic and diagnostic indices must be derived from accurate models developed by using appropriate data sets. We show that fractional polynomials, which extend ordinary polynomials by including non-positive and fractional powers, may be used as the basis of such models. We describe how to fit fractional polynomials in several continuous covariates simultaneously, and we propose ways of ensuring that the resulting models are par-simonious and consistent with basic medical knowledge. The methods are applied to two breast cancer data sets, one from a prognostic factors study in patients with positive lymph nodes and the other from a study to diagnose malignant or benign tumours by using colour Doppler blood flow mapping. We investigate the problems of biased parameter estimates in the final model and overfitting using cross-validation calibration to estimate shrinkage factors. We adopt bootstrap resampling to assess model stability. We compare our new approach with conventional modelling methods which apply stepwise variables selection to categorized covariates. We conclude that fractional polynomial methodology can be very successful in generating simple and appropriate models.	Univ Freiburg, Inst Med Biometry & Informat, D-79104 Freiburg, Germany; Univ London Imperial Coll Sci Technol & Med, Sch Med, London, England	Sauerbrei, W (reprint author), Univ Freiburg, Inst Med Biometry & Informat, Stefan Meier Str 26, D-79104 Freiburg, Germany.		Jepsen, Peter/A-2593-2010	Jepsen, Peter/0000-0002-6641-1430			ROYSTON P, 1994, APPL STAT-J ROY ST C, V43, P429, DOI 10.2307/2986270; Ambler G., 1998, STATA TECH B, V43, P24; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; EFRON B, 1983, J AM STAT ASSOC, V78, P316, DOI 10.2307/2288636; BOX GEP, 1962, TECHNOMETRICS, V4, P531, DOI 10.2307/1266288; ALTMAN DG, 1994, J NATL CANCER I, V86, P829, DOI 10.1093/jnci/86.11.829; GALEA MH, 1992, BREAST CANCER RES TR, V22, P207, DOI 10.1007/BF01840834; SIMON R, 1994, BRIT J CANCER, V69, P979, DOI 10.1038/bjc.1994.192; HANLEY JA, 1982, RADIOLOGY, V143, P29; MIZON GE, 1986, ECONOMETRICA, V54, P657, DOI 10.2307/1911313; AKAIKE H, 1969, ANN I STAT MATH, V21, P243, DOI 10.1007/BF02532251; MALLOWS CL, 1973, TECHNOMETRICS, V15, P661, DOI 10.2307/1267380; SAUERBREI W, 1992, STAT MED, V11, P2093, DOI 10.1002/sim.4780111607; Harrell FE, 1996, STAT MED, V15, P361, DOI 10.1002/(SICI)1097-0258(19960229)15:4<361::AID-SIM168>3.0.CO;2-4; BLETTNER M, 1993, STAT MED, V12, P1325, DOI 10.1002/sim.4780121405; Byar DP, 1984, CANC CLIN TRIALS MET, P423; CHEN CH, 1985, STAT MED, V4, P39, DOI 10.1002/sim.4780040107; Count Earl W., 1942, HUMAN BIOL, V14, P143; Eubank R.L., 1988, SPLINE SMOOTHING NON; GONG G, 1986, J AM STAT ASSOC, V81, P108, DOI 10.2307/2287975; Green P, 1994, NONPARAMETRIC REGRES; HASTIE T, 1992, BREAST CANCER RES TR, V22, P241, DOI 10.1007/BF01840837; Hastie TJ, 1990, GEN ADDITIVE MODELS; LAGAKOS SW, 1988, STAT MED, V7, P257, DOI 10.1002/sim.4780070126; MANTEL N, 1970, TECHNOMETRICS, V12, P621, DOI 10.2307/1267207; Miller A. J., 1990, SUBSET SELECTION REG; MORGAN TM, 1986, J AM STAT ASSOC, V81, P917, DOI 10.2307/2289060; PHILLIPS AN, 1990, STAT MED, V9, P1189, DOI 10.1002/sim.4780091008; Ramsay J. O., 1988, STAT SCI, V3, P425, DOI DOI 10.1214/SS/1177012761; SAUERBREI W, 1999, IN PRESS APPL STAT, V48; SAUERBREI W, 1992, THESIS U DORTMUND DO; Sauerbrei W, 1998, METHOD INFORM MED, V37, P226; Schmoor C, 1996, STAT MED, V15, P263, DOI 10.1002/(SICI)1097-0258(19960215)15:3<263::AID-SIM165>3.0.CO;2-K; SCHUMACHER M, 1994, J CLIN ONCOL, V12, P2086; *STAT, 1996, STAT REF MAN VERS 5; TERASVIRTA T, 1986, SCAND J STAT, V13, P159; VANHOUWELINGEN HC, 1995, STAT MED, V14, P1999, DOI 10.1002/sim.4780141806; VANHOUWELINGEN JC, 1990, STAT MED, V9, P1303, DOI 10.1002/sim.4780091109; VERWEIJ PJM, 1993, STAT MED, V12, P2305, DOI 10.1002/sim.4780122407; WINGERD J, 1970, HUM BIOL, V42, P105	40	184	185	3	22	BLACKWELL PUBL LTD	OXFORD	108 COWLEY RD, OXFORD OX4 1JF, OXON, ENGLAND	0964-1998			J ROY STAT SOC A STA	J. R. Stat. Soc. Ser. A-Stat. Soc.		1999	162		1				71	94		10.1111/1467-985X.00122		24	Social Sciences, Mathematical Methods; Statistics & Probability	Mathematical Methods In Social Sciences; Mathematics	165TQ	WOS:000078538300006		
J	Sauerbrei, W				Sauerbrei, W			The use of resampling methods to simplify regression models in medical statistics	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES C-APPLIED STATISTICS			English	Article						backward elimination; bootstrap; cross-validation; model complexity; prediction; selection bias; selection level	PREDICTION ERROR; CROSS-VALIDATION; SELECTION; BOOTSTRAP; STRATEGIES; INFERENCE; SHRINKAGE; CRITERIA	The number of variables in a regression model is often too large and a more parsimonious model may be preferred. Selection strategies (e.g. all-subset selection with various penalties for model complexity, or stepwise procedures) are widely used, but there are few analytical results about their properties. The problems of replication stability, model complexity, selection bias and an overoptimistic estimate of the predictive value of a model are discussed together with several proposals based on resampling methods. The methods are applied to data from a case-control study on atopic dermatitis and a clinical trial to compare two chemotherapy regimes by using a logistic regression and a Cox model. A recent proposal to use shrinkage factors to reduce the bias of parameter estimates caused by model building is extended to parameterwise shrinkage factors and is discussed as a further possibility to illustrate problems of models which are too complex. The results from the resampling approaches favour greater simplicity of the final regression model.	Univ Freiburg, Inst Med Biometry & Med Informat, D-79104 Freiburg, Germany	Sauerbrei, W (reprint author), Univ Freiburg, Inst Med Biometry & Med Informat, Stefan Meier Str 26, D-79104 Freiburg, Germany.						BREIMAN L, 1995, TECHNOMETRICS, V37, P373, DOI 10.2307/1269730; ROECKER E, 1991, TECHNOMETRICS, V33, P459, DOI 10.2307/1269417; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; BREIMAN L, 1992, J AM STAT ASSOC, V87, P738, DOI 10.2307/2290212; Efron B, 1997, J AM STAT ASSOC, V92, P548, DOI 10.2307/2965703; GAIL MH, 1984, BIOMETRIKA, V71, P431; Buckland ST, 1997, BIOMETRICS, V53, P603, DOI 10.2307/2533961; SAUERBREI W, 1992, STAT MED, V11, P2093, DOI 10.1002/sim.4780111607; COPAS JB, 1983, J R STAT SOC B, V45, P311; Harrell FE, 1996, STAT MED, V15, P361, DOI 10.1002/(SICI)1097-0258(19960229)15:4<361::AID-SIM168>3.0.CO;2-4; BLETTNER M, 1993, STAT MED, V12, P1325, DOI 10.1002/sim.4780121405; CHATFIELD C, 1995, J ROY STAT SOC A STA, V158, P419, DOI 10.2307/2983440; CHEN CH, 1985, STAT MED, V4, P39, DOI 10.1002/sim.4780040107; COPAS JB, 1991, STATISTICIAN, V40, P51, DOI 10.2307/2348223; COX DR, 1972, J R STAT SOC B, V34, P187; Diepgen TL, 1996, J CLIN EPIDEMIOL, V49, P1031, DOI 10.1016/0895-4356(96)00119-9; Efron B, 1993, INTRO BOOTSTRAP; HARRELL FE, 1984, STAT MED, V3, P143, DOI 10.1002/sim.4780030207; HJORTH U, 1989, J STAT PLAN INFER, V23, P101, DOI 10.1016/0378-3758(89)90043-8; Hjorth U, 1994, COMPUTER INTENSIVE S; LePage R., 1992, EXPLORING LIMITS BOO; MILLER AJ, 1984, J ROY STAT SOC A STA, V147, P389, DOI 10.2307/2981576; Miller A. J., 1990, SUBSET SELECTION REG; PICARD RR, 1990, AM STAT, V44, P140, DOI 10.2307/2684155; Sauerbrei W., 1993, EUROPAISCHE PERSPEKT, P108; SAUERBREI W, 1992, THESIS U DORTMUND DO; TERASVIRTA T, 1986, SCAND J STAT, V13, P159; ULM K, 1989, BIOMETRIE INFORMATIK, V20, P171; VANHOUWELINGEN JC, 1990, STAT MED, V9, P1303, DOI 10.1002/sim.4780091109; VERWEIJ PJM, 1993, STAT MED, V12, P2305, DOI 10.1002/sim.4780122407	30	72	73	1	14	BLACKWELL PUBL LTD	OXFORD	108 COWLEY RD, OXFORD OX4 1JF, OXON, ENGLAND	0035-9254			J ROY STAT SOC C-APP	J. R. Stat. Soc. Ser. C-Appl. Stat.		1999	48		3				313	329		10.1111/1467-9876.00155		17	Statistics & Probability	Mathematics	204NA	WOS:000080769400003		
J	Bruce, AG; Gao, HY; Stuetzle, W				Bruce, AG; Gao, HY; Stuetzle, W			Subset-selection and ensemble methods for wavelet de-noising	STATISTICA SINICA			English	Article						cycle spinning; model combination; nonparametric regression; stepwise regression; wavelet shrinkage	SHRINKAGE; APPROXIMATION	Many nonparametric regression procedures are based on "subset selection": they choose a subset of carriers from a large or even infinite set, and then determine the coefficients of the chosen carriers by least squares. Procedures which can be cast in this framework include Projection Pursuit, Turbo, Mars, and Matching Pursuit. Recently, considerable attention has been given to "ensemble estimators" which combine least squares estimates obtained from multiple subsets of carriers. In the parametric regression setting, such ensemble estimators have been shown to improve on the accuracy of subset selection procedures in some situations. In this paper we compare subset selection estimators and ensemble estimators in the context of wavelet de-noising. We present simulation results demonstrating that a certain class of ensemble wavelet estimators, based on the concept of "cycle spinning", are significantly more accurate than subset selection methods. These advantages hold even when the subset selection procedures can rely on an oracle to select the optimal number of carriers. We compute ideal thresholds for translation invariant wavelet shrinkage and investigate other approaches to ensemble wavelet estimation.	MathSoft Inc, Data Anal Prod Div, Seattle, WA 98109 USA; TeraLog Inc, Mt View, CA 94041 USA; Univ Washington, Dept Stat, Seattle, WA 98195 USA	Bruce, AG (reprint author), MathSoft Inc, Data Anal Prod Div, 1700 Westlake Ave N,Suite 500, Seattle, WA 98109 USA.						BREIMAN L, 1995, TECHNOMETRICS, V37, P373, DOI 10.2307/1269730; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; MALLAT SG, 1993, IEEE T SIGNAL PROCES, V41, P3397, DOI 10.1109/78.258082; FRANK IE, 1993, TECHNOMETRICS, V35, P109, DOI 10.2307/1269656; DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425; FRIEDMAN JH, 1989, TECHNOMETRICS, V31, P3, DOI 10.2307/1270359; BREIMAN L, 1994, BAGGING PREDICATORS; Breiman L, 1994, HEURISTICS INSTABILI; BRUCE A, 1994, SPLUS WAVELETS USERS; Bruce AG, 1996, BIOMETRIKA, V83, P727, DOI 10.1093/biomet/83.4.727; BRUCE AG, 1995, P 29 AS C SIGN SYST; BRUCE AG, 1996, 45 MATHS INC DAT AN; CHEN S, 1991, IEEE T NEURAL NETWOR, V2, P302, DOI 10.1109/72.80341; CHEN S, 1996, ATOMIC DECOMPOSITION; CHIPMAN NA, 1996, 421 U CHIC; CLYDE M, 1995, 9537 DUK U; Coifman R. R., 1995, WAVELETS STAT, P125; Daubechies I., 1992, 10 LECT WAVELETS; Donoho DL, 1995, J AM STAT ASSOC, V90, P1200, DOI 10.2307/2291512; FAN JQ, 1995, J ROY STAT SOC B MET, V57, P371; FRIEDMAN JH, 1991, ANN STAT, V19, P1, DOI 10.1214/aos/1176347963; FRIEDMAN JH, 1983, SIAM J SCI STAT COMP, V4, P291, DOI 10.1137/0904023; Nason G.P., 1995, WAVELETS STAT, P281; PATI YC, 1993, C REC 27 AS C SIGN S; QIAN S, 1994, SIGNAL PROCESS, V36, P1, DOI 10.1016/0165-1684(94)90174-0; SHENSA MJ, 1992, IEEE T SIGNAL PROCES, V40, P2464, DOI 10.1109/78.157290; *STAT SCI, 1993, S PLUS US MAN VERS 3; TIBSHIRANI R, 1995, MODEL SEARCH INFEREN; VIDAKOVIC B, 1994, 24 DUKE U; WANG LX, 1992, IEEE T NEURAL NETWOR, V3, P807, DOI 10.1109/72.159070	30	3	3	0	0	STATISTICA SINICA	TAIPEI	C/O DR H C HO, INST STATISTICAL SCIENCE, ACADEMIA SINICA, TAIPEI 115, TAIWAN	1017-0405			STAT SINICA	Stat. Sin.	JAN	1999	9	1					167	182				16	Statistics & Probability	Mathematics	162RF	WOS:000078360000009		
J	Pan, W				Pan, W			Extending the iterative convex minorant algorithm to the Cox model for interval-censored data	JOURNAL OF COMPUTATIONAL AND GRAPHICAL STATISTICS			English	Article						cross-validation; EM; generalized gradient projection; ICM; interval censoring; isotonic regression; Lasso; NPMLE	PROPORTIONAL HAZARDS MODEL; FAILURE TIME DATA; TRUNCATED DATA; SELECTION; LASSO	The iterative convex minorant (ICM) algorithm proposed by Groeneboom and Wellner is fast in computing the NPMLE of the distribution function for interval censored data without covariates. We reformulate the ICM as a generalized gradient projection method (GGP), which leads to a natural extension to the Cox model. It is also easily extended to support Tibshirani's Lasso method. Some simulation results are also shown. For illustration we reanalyze two real datasets.	Univ Minnesota, Div Biostat, Minneapolis, MN 55455 USA	Pan, W (reprint author), Univ Minnesota, Div Biostat, A460 Mayo Bldg,Box 303, Minneapolis, MN 55455 USA.	weip@biostat.umn.edu					Alioum A, 1996, BIOMETRICS, V52, P512, DOI 10.2307/2532891; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; BERTSEKAS DP, 1976, IEEE T AUTOMAT CONTR, V21, P174, DOI 10.1109/TAC.1976.1101194; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1; BERTSEKAS DP, 1982, SIAM J CONTROL OPTIM, V20, P221, DOI 10.1137/0320018; FINKELSTEIN DM, 1986, BIOMETRICS, V42, P845, DOI 10.2307/2530698; Aragon J., 1992, J COMPUT GRAPH STAT, V1, P129, DOI 10.2307/1390837; BURR D, 1994, J AM STAT ASSOC, V89, P1290, DOI 10.2307/2290992; Efron B, 1993, INTRO BOOTSTRAP; EFRON B, 1978, BIOMETRIKA, V65, P457, DOI 10.2307/2335893; FINKELSTEIN DM, 1985, BIOMETRICS, V41, P933, DOI 10.2307/2530965; GROENEBOOM P, 1992, INFORMATION BOUNDS N; Hastie TJ, 1990, GEN ADDITIVE MODELS; Huang J, 1996, ANN STAT, V24, P540; Huang J., 1995, EFFICIENT ESTIMATION; Huangs J., 1997, P 1 SEATTL S BIOST S, P123; MANGASARIAN OL, 1996, CS730 U WISC COMP SC; PAN W, 1998, COMPUTATIONAL STAT D, V17, P33; POLAK E., 1971, COMPUTATIONAL METHOD; Robertson T, 1988, ORDER RESTRICTED STA; Silverman B. M., 1986, DENSITY ESTIMATION S; Thisted R. A., 1988, ELEMENTS STAT COMPUT; Tibshirani R, 1997, STAT MED, V16, P385, DOI 10.1002/(SICI)1097-0258(19970228)16:4<385::AID-SIM380>3.0.CO;2-3; Tibshirani R., 1990, GEN ADDITIVE MODELS; TURNBULL BW, 1976, J ROY STAT SOC B MET, V38, P290; ZHAN Y, 1995, DOUBLE CENSORING CHA	26	35	35	0	3	AMER STATISTICAL ASSOC	ALEXANDRIA	1429 DUKE ST, ALEXANDRIA, VA 22314 USA	1061-8600			J COMPUT GRAPH STAT	J. Comput. Graph. Stat.	MAR	1999	8	1					109	120		10.2307/1390923		12	Statistics & Probability	Mathematics	176QL	WOS:000079163200007		
J	Steyerberg, EW; Eijkemans, MJC; Habbema, JDF				Steyerberg, EW; Eijkemans, MJC; Habbema, JDF			Stepwise selection in small data sets: A simulation study of bias in logistic regression analysis	JOURNAL OF CLINICAL EPIDEMIOLOGY			English	Article						regression analysis; logistic models; bias; variable selection	ACUTE MYOCARDIAL-INFARCTION; CLINICAL-TRIALS; COX REGRESSION; PROGNOSTIC PREDICTION; VARIABLE SELECTION; MODEL SELECTION; INFERENCE; BOOTSTRAP; MORTALITY; STRATIFICATION	Stepwise selection methods are widely applied to identify covariables for inclusion in regression models. One of the problems of stepwise selection is biased estimation of the regression coefficients. We illustrate this "selection bias" with logistic regression in the GUSTO-I trial (40,830 patients with an acute myocardial infarction). Random samples were drawn that included 3, 5, 10, 20, or 40 events per variable (EPV). Backward stepwise selection was applied in models containing 8 or 16 pre-specified predictors of 30-day mortality. We found a considerable overestimation of regression coefficients of selected covariables. The selection bias decreased with increasing EPV. For EPV 3, 10, or 40, the bias exceeded 25% fur 7, 3, and 1 in the 8-predictor model respectively, when a conventional selection criterion was used (alpha = 0.05). For these EPV values, the bias was less than 20% for all covariables when no selection was applied. We conclude that stepwise selection may result in a substantial bias of estimated regression coefficients. (C) 1999 Elsevier Science Inc.	Erasmus Univ, Dept Publ Hlth, Ctr Clin Decis Sci, NL-3000 DR Rotterdam, Netherlands	Steyerberg, EW (reprint author), Erasmus Univ, Dept Publ Hlth, Ctr Clin Decis Sci, EE2091,POB 1738, NL-3000 DR Rotterdam, Netherlands.		Eijkemans, Marinus/D-8214-2013; 	Steyerberg, Ewout/0000-0002-7787-0122			ALTMAN DG, 1990, LANCET, V335, P149, DOI 10.1016/0140-6736(90)90014-V; ALTMAN DG, 1989, STAT MED, V8, P771, DOI 10.1002/sim.4780080702; TITTERINGTON DM, 1981, J ROY STAT SOC A STA, V144, P145, DOI 10.2307/2981918; HURVICH CM, 1990, AM STAT, V44, P214, DOI 10.2307/2685338; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; BANCROFT TA, 1977, INT STAT REV, V45, P117; MALDONADO G, 1993, AM J EPIDEMIOL, V138, P923; MAGGIONI AP, 1993, NEW ENGL J MED, V329, P1442, DOI 10.1056/NEJM199311113292002; SIMON R, 1994, BRIT J CANCER, V69, P979, DOI 10.1038/bjc.1994.192; LEE KL, 1995, CIRCULATION, V91, P1659; Concato J, 1995, J CLIN EPIDEMIOL, V48, P1495, DOI 10.1016/0895-4356(95)00510-2; SENN S, 1994, STAT MED, V13, P1715, DOI 10.1002/sim.4780131703; Buckland ST, 1997, BIOMETRICS, V53, P603, DOI 10.2307/2533961; SAUERBREI W, 1992, STAT MED, V11, P2093, DOI 10.1002/sim.4780111607; Harrell FE, 1996, STAT MED, V15, P361, DOI 10.1002/(SICI)1097-0258(19960229)15:4<361::AID-SIM168>3.0.CO;2-4; ATKINSON AC, 1980, BIOMETRIKA, V67, P413, DOI 10.1093/biomet/67.2.413; CHATFIELD C, 1995, J ROY STAT SOC A STA, V158, P419, DOI 10.2307/2983440; CHEN CH, 1985, STAT MED, V4, P39, DOI 10.1002/sim.4780040107; DALES LG, 1978, INT J EPIDEMIOL, V7, P373, DOI 10.1093/ije/7.4.373; DERKSEN S, 1992, BRIT J MATH STAT PSY, V45, P265; DUBOIS C, 1988, AM J CARDIOL, V61, P216, DOI 10.1016/0002-9149(88)90918-6; Feinstein AR, 1996, MULTIVARIABLE ANAL; GRAY RJ, 1992, J AM STAT ASSOC, V87, P942, DOI 10.2307/2290630; GREENLAND S, 1993, STAT MED, V12, P717, DOI 10.1002/sim.4780120802; GREENLAND S, 1991, EPIDEMIOLOGY, V1, P43; *GUSTO INV, 1993, NEW ENGL J MED, V329, P673; HARRELL FE, 1985, CANCER TREAT REP, V69, P1071; HARRELL FE, 1996, DESIGN S PLUS FUNCTI; HARRELL FE, 1984, STAT MED, V3, P143, DOI 10.1002/sim.4780030207; Kleinbaum DG, 1982, EPIDEMIOLOGIC RES PR; MIETTINEN OS, 1976, AM J EPIDEMIOL, V104, P609; Miller A. J., 1990, SUBSET SELECTION REG; MUELLER HS, 1992, CIRCULATION, V85, P1254; Peduzzi P, 1996, J CLIN EPIDEMIOL, V49, P1373, DOI 10.1016/S0895-4356(96)00236-3; Rothman KJ, 1998, MODERN EPIDEMIOLOGY; SPIEGELHALTER DJ, 1986, STAT MED, V5, P421, DOI 10.1002/sim.4780050506; Tibshirani R, 1997, STAT MED, V16, P385, DOI 10.1002/(SICI)1097-0258(19970228)16:4<385::AID-SIM380>3.0.CO;2-3; VANHOUWELINGEN JC, 1990, STAT MED, V9, P1303, DOI 10.1002/sim.4780091109; VERWEIJ PJM, 1994, STAT MED, V13, P2427, DOI 10.1002/sim.4780132307; WEISS RE, 1995, J AM STAT ASSOC, V90, P619, DOI 10.2307/2291074	40	163	164	5	15	PERGAMON-ELSEVIER SCIENCE LTD	OXFORD	THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND	0895-4356			J CLIN EPIDEMIOL	J. Clin. Epidemiol.	OCT	1999	52	10					935	942		10.1016/S0895-4356(99)00103-1		8	Health Care Sciences & Services; Public, Environmental & Occupational Health	Health Care Sciences & Services; Public, Environmental & Occupational Health	239LN	WOS:000082769800007	10513756	
J	Wand, MP				Wand, MP			A comparison of regression spline smoothing procedures	COMPUTATIONAL STATISTICS			English	Article						Bayesian variable selection; B-spline; Gibbs sampling; nonparametric regression; polynomial spline; roughness penalty; stepwise regression	COMPUTATION; SELECTION	Regression spline smoothing involves modelling a regression function as a piecewise polynomial with a high number of pieces relative to the sample size. Because the number of possible models is so large, efficient strategies for choosing among them are required. In this paper we review approaches to this problem and compare them through a simulation study. For simplicity and conciseness we restrict attention to the univariate smoothing setting with Gaussian noise and the truncated polynomial regression spline basis.	Harvard Univ, Sch Publ Hlth, Dept Biostat, Boston, MA 02115 USA	Wand, MP (reprint author), Harvard Univ, Sch Publ Hlth, Dept Biostat, 665 Huntington Ave, Boston, MA 02115 USA.						AGARWAL GG, 1978, 7815 PURD U; AGARWAL GG, 1980, ANN STAT, V8, P1307, DOI 10.1214/aos/1176345203; GELFAND AE, 1990, J AM STAT ASSOC, V85, P398, DOI 10.2307/2289776; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; SMITH PL, 1979, AM STAT, V33, P57, DOI 10.2307/2683222; FURNIVAL GM, 1974, TECHNOMETRICS, V16, P499, DOI 10.2307/1267601; Denison DGT, 1998, J ROY STAT SOC B, V60, P333, DOI 10.1111/1467-9868.00128; Eilers PHC, 1996, STAT SCI, V11, P89, DOI 10.1214/ss/1038425655; FRIEDMAN JH, 1989, TECHNOMETRICS, V31, P3, DOI 10.2307/1270359; de Boor C., 1978, PRACTICAL GUIDE SPLI; FRIEDMAN JH, 1991, ANN STAT, V19, P1, DOI 10.1214/aos/1176347963; FULLER WA, 1969, AUST J AGR ECON, V13, P35; GALLANT AR, 1973, J AM STAT ASSOC, V68, P144, DOI 10.2307/2284158; Green PJ, 1995, BIOMETRIKA, V82, P711, DOI 10.1093/biomet/82.4.711; Green P, 1994, NONPARAMETRIC REGRES; HE X, 1996, UNPUB COBS CONSTRAIN; HUDSON DJ, 1966, J AM STAT ASSOC, V61, P1097, DOI 10.2307/2283203; Luo Z, 1997, J AM STAT ASSOC, V92, P107, DOI 10.2307/2291454; OSULLIVAN F, 1988, SIAM J SCI STAT COMP, V9, P363, DOI 10.1137/0909024; O'Sullivan F., 1986, STAT SCI, V4, P505; REINSCH CH, 1967, NUMER MATH, V10, P177, DOI 10.1007/BF02162161; RUPPERT D, 1998, UNPUB PENALIZED REGR; Smith M, 1996, J ECONOMETRICS, V75, P317, DOI 10.1016/0304-4076(95)01763-1; Smith P.L., 1982, 166034 NASA LANGL RE; Stone CJ, 1997, ANN STAT, V25, P1371; STUDDEN WJ, 1969, ANN MATH STAT, V40, P1557, DOI 10.1214/aoms/1177697373; Weisberg S., 1985, APPL LINEAR REGRESSI; WOLD S, 1974, TECHNOMETRICS, V16, P1, DOI 10.2307/1267485; WOLD S, 1971, CHEM SCRIPTA, V1, P97	29	35	36	2	3	PHYSICA-VERLAG GMBH & CO	HEIDELBERG	TIERGARTENSTRASSE 17, 69121 HEIDELBERG, GERMANY	0943-4062			COMPUTATION STAT	Comput. Stat.		2000	15	4					443	462		10.1007/s001800000047		20	Statistics & Probability	Mathematics	393RZ	WOS:000166484600001		
S	Sauerbrei, W; Schumacher, M		Brause, RW; Hanisch, E		Sauerbrei, W; Schumacher, M			Bootstrap and cross-validation to assess complexity of data-driven regression models	MEDICAL DATA ANALYSIS, PROCEEDINGS	LECTURE NOTES IN COMPUTER SCIENCE		English	Article; Proceedings Paper	1st International Symposum on Medical Data Analysis (ISMDA 2000)	SEP 29-30, 2000	FRANKFURT, GERMANY				SELECTION; INFERENCE	The number of potential variables included into a regression model is often too large and a more parsimonious model may be preferable. Selection strategies are widely used, but there are few analytical results about their properties. To investigate problems as replication stability, model complexity and selection bias we use bootstrap and cross-validation methods. For stepwise strategies, we discuss the importance of the predefined selection level. The methods are illustrated by investigating prognostic factors for survival time of patients with malignant glioma in the framework of a Cox regression model.	Univ Freiburg Klinikum, Inst Med Biometrie & Med Informatik, D-79104 Freiburg, Germany	Sauerbrei, W (reprint author), Univ Freiburg Klinikum, Inst Med Biometrie & Med Informatik, Stefan Meier Str 26, D-79104 Freiburg, Germany.						BREIMAN L, 1995, TECHNOMETRICS, V37, P373, DOI 10.2307/1269730; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Hoeting JA, 1999, STAT SCI, V14, P382; Buckland ST, 1997, BIOMETRICS, V53, P603, DOI 10.2307/2533961; SAUERBREI W, 1992, STAT MED, V11, P2093, DOI 10.1002/sim.4780111607; CHATFIELD C, 1995, J ROY STAT SOC A STA, V158, P419, DOI 10.2307/2983440; CHEN CH, 1985, STAT MED, V4, P39, DOI 10.1002/sim.4780040107; Efron B, 1993, INTRO BOOTSTRAP; MARUBINI E, 1994, ANAL SURVIVAL DATA C; Miller A. J., 1990, SUBSET SELECTION REG; Sauerbrei W., 1993, EUROPAISCHE PERSPEKT, P108; Sauerbrei W, 1999, APPL STAT, V48, P313; Schumacher M, 1997, STAT MED, V16, P2813, DOI 10.1002/(SICI)1097-0258(19971230)16:24<2813::AID-SIM701>3.0.CO;2-Z; TERASVIRTA T, 1986, SCAND J STAT, V13, P159; ULM K, 1989, BIOMETRIE INFORMATIK, V20, P171; VANHOUWELINGEN JC, 1990, STAT MED, V9, P1303, DOI 10.1002/sim.4780091109; VERWEIJ PJM, 1993, STAT MED, V9, P487	17	1	1	2	2	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		3-540-41089-9	LECT NOTES COMPUT SC			2000	1933						234	241				8	Computer Science, Information Systems; Computer Science, Theory & Methods	Computer Science	BS85C	WOS:000171225400029		
J	Osborne, MR; Presnell, B; Turlach, BA				Osborne, MR; Presnell, B; Turlach, BA			On the LASSO and its dual	JOURNAL OF COMPUTATIONAL AND GRAPHICAL STATISTICS			English	Article						convex programming; dual problem; partial least squares; penalized regression; quadratic programming; regression; shrinkage; subset selection; variable selection	REGRESSION; SELECTION	Proposed by Tibshirani, the least absolute shrinkage and selection operator (LASSO) estimates a vector of regression coefficients by minimizing the residual sum of squares subject to a constraint on the l(1)-norm of the coefficient vector. The LASSO estimator typically has one or more zero elements and thus shares characteristics of both shrinkage estimation and variable selection. In this article we treat the LASSO as a convex programming problem and derive its dual. Consideration of the primal and dual problems together leads to important new insights into the characteristics of the LASSO estimator and to an improved method for estimating its covariance matrix. Using these results we also develop an efficient algorithm for computing LASSO estimates which is usable even in cases where the number of regressors exceeds the number of observations. An S-Plus library based on this algorithm is available from StatLib.	Australian Natl Univ, Ctr Math & Applicat, Canberra, ACT 0200, Australia; Univ Florida, Dept Stat, Gainesville, FL 32611 USA; Univ Western Australia, Dept Math & Stat, Nedlands, WA 6907, Australia	Osborne, MR (reprint author), Australian Natl Univ, Ctr Math & Applicat, Canberra, ACT 0200, Australia.		Turlach, Berwin/A-4995-2008	Turlach, Berwin/0000-0001-8795-471X			Fu WJJ, 1998, J COMPUT GRAPH STAT, V7, P397, DOI 10.2307/1390712; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; FRANK IE, 1993, TECHNOMETRICS, V35, P109, DOI 10.2307/1269656; Brown P. J., 1993, MEASUREMENT REGRESSI; Chen S. S., 1999, SIAM J SCI COMPUT, V20, P33; CLARK DI, 1988, IMA J NUMER ANAL, V8, P23, DOI 10.1093/imanum/8.1.23; Gallant A. R., 1987, NONLINEAR STAT MODEL; HAAGEN K, 1993, STAT MODELLING LATEN; Koch I, 1996, ANN STAT, V24, P1648; Meyer MC, 1999, J STAT PLAN INFER, V81, P13, DOI 10.1016/S0378-3758(99)00025-7; Miller A.J., 1990, MONOGRAPHS STAT APPL, V40; Osborne M R, 1985, WILEY SERIES PROBABI; Osborne MR, 1998, COMP SCI STAT, V30, P44; OSBORNE MR, IN PRESS IMA J NUMER; Rao C. R., 1973, LINEAR STAT INFERENC; Rockafellar R.T., 1970, PRINCETON MATH SERIE, V28; Sardy S, 2000, J COMPUT GRAPH STAT, V9, P361, DOI 10.2307/1390659; Sofer A, 1996, LINEAR NONLINEAR PRO; STAMEY TA, 1989, J UROLOGY, V141, P1076	19	220	222	6	15	AMER STATISTICAL ASSOC	ALEXANDRIA	1429 DUKE ST, ALEXANDRIA, VA 22314 USA	1061-8600			J COMPUT GRAPH STAT	J. Comput. Graph. Stat.	JUN	2000	9	2					319	337		10.2307/1390657		19	Statistics & Probability	Mathematics	332AC	WOS:000088050000007		
J	Sardy, S; Bruce, AG; Tseng, P				Sardy, S; Bruce, AG; Tseng, P			Block coordinate relaxation methods for nonparametric wavelet denoising	JOURNAL OF COMPUTATIONAL AND GRAPHICAL STATISTICS			English	Article						basis pursuit; block coordinate relaxation; function estimation; interior-point; optimization	SELECTION; LASSO; MINIMIZATION; SHRINKAGE	An important class of nonparametric signal processing methods entails forming a set of predictors from an overcomplete set of basis functions associated with a fast transform (e.g., wavelet packets). In these methods, the number of basis functions can far exceed the number of sample values in the signal, leading to an ill-posed prediction problem. The "basis pursuit" denoising method of Chen, Donoho, and Saunders regularizes the prediction problem by adding an l(1) penalty term on the coefficients for the basis functions. Use of an l(1) penalty instead of l(2) has significant benefits, including higher resolution of signals close in time/frequency and a more parsimonious representation. The l(1) penalty, however, poses a challenging optimization problem that was solved by Chen, Donoho and Saunders using a novel application of interior-point algorithms (IP). This article investigates an alternative optimization approach based on block coordinate relaxation (BCR) for sets of basis functions that are the finite union of sets of orthonormal basis functions (e.g., wavelet packets). We show that the BCR algorithm is globally convergent, and empirically, the BCR algorithm is faster than the IP algorithm for a variety of signal denoising problems.	Swiss Fed Inst Technol, EPFL DMA, CH-1015 Lausanne, Switzerland; Mathsoft Inc, Seattle, WA 98109 USA; Univ Washington, Dept Math, Seattle, WA 98195 USA	Sardy, S (reprint author), Swiss Fed Inst Technol, EPFL DMA, CH-1015 Lausanne, Switzerland.						ALLINEY S, 1994, IEEE T SIGNAL PROCES, V42, P618, DOI 10.1109/78.277854; Fu WJJ, 1998, J COMPUT GRAPH STAT, V7, P397, DOI 10.2307/1390712; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425; Nesterov YE, 1997, MATH OPER RES, V22, P1, DOI 10.1287/moor.22.1.1; Bishop Y.V.V., 1975, DISCRETE MULTIVARIAT; BRUCE AG, 1994, S PLUS WAVELETS USER; Chen S. S., 1999, SIAM J SCI COMPUT, V20, P33; Clarke F. H., 1990, OPTIMIZATION NONSMOO; Coifman R. R., 1995, LECT NOTES STAT, V103, P125; COIFMAN RR, 1992, IEEE T INFORM THEORY, V38, P713, DOI 10.1109/18.119732; Daubechies I., 1992, 10 LECT WAVELETS; Golub G., 1989, MATRIX COMPUTATIONS; Hastie TJ, 1990, GEN ADDITIVE MODELS; Herman GT, 1980, IMAGE RECONSTRUCTION; Kojima M., 1993, MATH PROGRAM, V61, P261; LINA JM, 1995, APPL COMPUT HARMON A, V2, P219, DOI 10.1006/acha.1995.1015; MANN S, 1992, OPT ENG, V31, P1243, DOI 10.1117/12.57676; Meyer FG, 1997, APPL COMPUT HARMON A, V4, P147, DOI 10.1006/acha.1997.0208; Nason G.P., 1995, WAVELETS STAT, P261; OSBORNE M, 1995, FINITE ALGORITHMS OP; Osborne MR, 1998, COMP SCI STAT, V30, P44; Sardy S, 2000, IEEE T SIGNAL PROCES, V48, P1023, DOI 10.1109/78.827536; Shiskin J., 1967, 15 US BUR CENS; TSENG P, 1993, MATH PROGRAM, V59, P231, DOI 10.1007/BF01581245	25	90	98	1	6	AMER STATISTICAL ASSOC	ALEXANDRIA	1429 DUKE ST, ALEXANDRIA, VA 22314 USA	1061-8600			J COMPUT GRAPH STAT	J. Comput. Graph. Stat.	JUN	2000	9	2					361	379		10.2307/1390659		19	Statistics & Probability	Mathematics	332AC	WOS:000088050000009		
J	Osborne, MR; Presnell, B; Turlach, BA				Osborne, MR; Presnell, B; Turlach, BA			A new approach to variable selection in least squares problems	IMA JOURNAL OF NUMERICAL ANALYSIS			English	Article							REGRESSION	The title Lasso has been suggested by Tibshirani (1996) as a colourful name for a technique of variable selection which requires the minimization of a sum of squares subject to an l(1) bound kappa on the solution. This forces zero components in the minimizing solution for small values of kappa. Thus this bound can function as a selection parameter. This paper makes two contributions to computational problems associated with implementing the Lasso: (1) a compact descent method for solving the constrained problem for a particular value of kappa is formulated, and (2) a homotopy method, in which the constraint bound kappa becomes the homotopy parameter, is developed to completely describe the possible selection regimes. Both algorithms have a finite termination property. It is suggested that modified Gram-Schmidt orthogonalization applied to an augmented design matrix provides an effective basis for implementing the algorithms.	Australian Natl Univ, Sch Math Sci, Canberra, ACT, Australia; Univ Florida, Dept Stat, Gainesville, FL 32611 USA; Univ Western Australia, Dept Math & Stat, Perth, WA 6009, Australia	Osborne, MR (reprint author), Australian Natl Univ, Sch Math Sci, Canberra, ACT, Australia.		Turlach, Berwin/A-4995-2008	Turlach, Berwin/0000-0001-8795-471X			Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; CLARK DI, 1988, IMA J NUMER ANAL, V8, P23, DOI 10.1093/imanum/8.1.23; DRAPER N. R., 1966, APPL REGRESSION ANAL; Fletcher R., 1993, ANN OPER RES, V47, P307; More J. J., 1977, LECT NOTES MATH, V630, P105; OSBORNE M. R., 1985, FINITE ALGORITHMS OP; OSBORNE MR, 1992, IMA J NUMER ANAL, V12, P151, DOI 10.1093/imanum/12.2.151	7	271	274	8	14	OXFORD UNIV PRESS	OXFORD	GREAT CLARENDON ST, OXFORD OX2 6DP, ENGLAND	0272-4979			IMA J NUMER ANAL	IMA J. Numer. Anal.	JUL	2000	20	3					389	403		10.1093/imanum/20.3.389		15	Mathematics, Applied	Mathematics	340TL	WOS:000088551000003		
J	Bengio, Y				Bengio, Y			Gradient-based optimization of hyperparameters	NEURAL COMPUTATION			English	Article								Many machine learning algorithms can be formulated as the minimization of a training criterion that involves a hyperparameter. This hyperparameter is usually chosen by trial and error with a model selection criterion. In this article we present a methodology to optimize several hyperparameters, based on the computation of the gradient of a model selection criterion with respect to the hyperparameters. In the case of a quadratic training criterion, the gradient of the selection criterion with respect to the hyperparameters is efficiently computed by backpropagating through a Cholesky decomposition. In the more general case, we show that the implicit function theorem can be used to derive a formula for the hyperparameter gradient involving second derivatives of the training criterion.	Univ Montreal, Dept Informat & Rech Operat, Montreal, PQ H3C 3J7, Canada	Bengio, Y (reprint author), Univ Montreal, Dept Informat & Rech Operat, Montreal, PQ H3C 3J7, Canada.						AKAIKE H, 1974, IEEE T AUTOMAT CONTR, VAC19, P716, DOI 10.1109/TAC.1974.1100705; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; HOERL AE, 1970, TECHNOMETRICS, V12, P55; MACKAY DJC, 1995, NETWORK-COMP NEURAL, V6, P469, DOI 10.1088/0954-898X/6/3/011; CRAVEN P, 1979, NUMER MATH, V31, P377; Becker S., 1989, P 1988 CONN MOD SUMM, P29; BENGIO Y, 1999, UNPUB LEARNING SIMPL; BISHOP C, 1992, NEURAL COMPUT, V4, P494, DOI 10.1162/neco.1992.4.4.494; Bottou L., 1998, ONLINE LEARNING NEUR; Breiman L, 1996, ANN STAT, V24, P2350; Grandvalet Y., 1998, ICANN 98. Proceedings of the 8th International Conference on Artificial Neural Networks; GUYON I, 1992, ADV NEUR IN, V4, P471; Hinton G. E., 1987, PARLE. Parallel Architectures and Languages Europe. Proceedings; Larsen J., 1998, Neural networks: tricks of the trade; LATENDRESSE S, 1999, THESIS U MONTREAL DE; Le Cun Y., 1990, ADV NEURAL INFORMATI, V2, P598; Neal R. M., 1998, Neural Networks and Machine Learning. Proceedings; POGGIO T, 1985, NATURE, V317, P314, DOI 10.1038/317314a0; RISSANEN J, 1990, STOCHASTIC COMPLEXIT; Tikhonov AN, 1977, SOLUTIONS ILL POSED; Vapnik VN, 1982, ESTIMATION DEPENDENC	21	48	53	3	5	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	0899-7667			NEURAL COMPUT	Neural Comput.	AUG	2000	12	8					1889	1900		10.1162/089976600300015187		12	Computer Science, Artificial Intelligence	Computer Science	345GV	WOS:000088809000007	10953243	
J	Blaker, H				Blaker, H			Minimax estimation in linear regression under restrictions	JOURNAL OF STATISTICAL PLANNING AND INFERENCE			English	Article						linear regression; minimax estimation; ridge regression; shrinkage; Mallow's C-L; spline smoothing	GENERALIZED CROSS-VALIDATION; NONPARAMETRIC REGRESSION; RIDGE-REGRESSION; PARAMETER; MODELS; SHRINKAGE; BAYES	We consider the problem of estimating the regression coefficients in a linear regression model under ellipsoid constraints on the parameter space. The problem is closely connected to spline smoothing. A minimax estimator under weighted squared error is derived. Special cases include ridge regression, Stein's estimator and principal component regression. The asymptotic risk ratio for ridge regression versus the minimax estimator is computed for a special case and seen to be infinite in some situations. Adaptive estimators based on Mallows' C-L-statistic are suggested when the size of the parameter space is unknown and shown to have the same asymptotic risk as the estimator based on known size. The minimax estimator is compared to ridge regression and principal component: regression on real data sets. (C) 2000 Elsevier Science B.V. All rights reserved. MSC: primary 62J05; secondary 62J07; 62F12.	Univ Oslo, Dept Math, N-0316 Oslo, Norway							BREIMAN L, 1995, TECHNOMETRICS, V37, P373, DOI 10.2307/1269730; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; MASSY WF, 1965, J AM STAT ASSOC, V60, P234, DOI 10.2307/2283149; HOERL AE, 1970, TECHNOMETRICS, V12, P55; FRANK IE, 1993, TECHNOMETRICS, V35, P109, DOI 10.2307/1269656; GOLUB GH, 1979, TECHNOMETRICS, V21, P215, DOI 10.1080/00401706.1979.10489751; LI KC, 1986, ANN STAT, V14, P1101, DOI 10.1214/aos/1176350052; MALLOWS CL, 1973, TECHNOMETRICS, V15, P661, DOI 10.2307/1267380; Becker R. A., 1988, NEW S LANGUAGE; BERAN R, 1995, STAT SINICA, V5, P109; CARTER CK, 1992, BIOMETRIKA, V79, P81, DOI 10.1093/biomet/79.1.81; George EI, 1996, STATISTICIAN, V45, P111, DOI 10.2307/2348417; Hald A, 1952, STAT THEORY ENG APPL; HECKMAN NE, 1991, ANN STAT, V19, P2003, DOI 10.1214/aos/1176348383; HEILIGERS B, 1993, J STAT PLAN INFER, V36, P175, DOI 10.1016/0378-3758(93)90122-M; Hoffmann K., 1979, Mathematische Operationsforschung und Statistik, Series Statistics, V10; James W., 1961, 4TH P BERK S MATH ST, V1, P361; KNEIP A, 1994, ANN STAT, V22, P835, DOI 10.1214/aos/1176325498; LI KC, 1982, ANN STAT, V10, P937, DOI 10.1214/aos/1176345883; NUSSBAUM M, 1985, ANN STAT, V13, P984, DOI 10.1214/aos/1176349651; PILZ J, 1986, J STAT PLAN INFER, V13, P297, DOI 10.1016/0378-3758(86)90141-2; Pinsker M. S., 1980, Problems of Information Transmission, V16; SCLOVE SL, 1968, J AM STAT ASSOC, V63, P596, DOI 10.2307/2284030; SPECKMAN P, 1985, ANN STAT, V13, P970, DOI 10.1214/aos/1176349650	24	9	9	0	0	ELSEVIER SCIENCE BV	AMSTERDAM	PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS	0378-3758			J STAT PLAN INFER	J. Stat. Plan. Infer.	SEP 1	2000	90	1					35	55		10.1016/S0378-3758(00)00101-4		21	Statistics & Probability	Mathematics	345CJ	WOS:000088797600003		
J	Knight, K; Fu, WJ				Knight, K; Fu, WJ			Asymptotics for Lasso-type estimators	ANNALS OF STATISTICS			English	Article						penalized regression; Lasso; shrinkage estimation; epi-convergence in distribution	REGRESSION	We consider the asymptotic behavior of regression estimators that minimize the residual sum of squares plus a penalty proportional to Sigma\beta (j)\(gamma) for some gamma > 0. These estimators include the Lasso as a special case when gamma = 1. Under appropriate conditions, we show that the limiting distributions can have positive probability mass at 0 when the true value of the parameter is 0. We also consider asymptotics for "nearly singular" designs.	Univ Toronto, Dept Stat, Toronto, ON M5S 3G3, Canada; Michigan State Univ, Dept Epidemiol, E Lansing, MI 48823 USA	Knight, K (reprint author), Univ Toronto, Dept Stat, 100 St George St, Toronto, ON M5S 3G3, Canada.						ANDERSEN PK, 1982, ANN STAT, V10, P1100, DOI 10.1214/aos/1176345976; Fu WJJ, 1998, J COMPUT GRAPH STAT, V7, P397, DOI 10.2307/1390712; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; FRANK IE, 1993, TECHNOMETRICS, V35, P109, DOI 10.2307/1269656; KIM JY, 1990, ANN STAT, V18, P191, DOI 10.1214/aos/1176347498; BESAG J, 1986, J ROY STAT SOC B MET, V48, P259; Chen S. S., 1999, SIAM J SCI COMPUT, V20, P33; FAN J, 1999, UNPUB VARIABLE SELEC; Fiacco AV, 1990, NONLINEAR PROGRAMMIN; FU WJ, 1999, UNPUB ESTIMATING EFF; GEYER CJ, 1996, UNPUB ASYMPTOTICS CO; GEYER CJ, 1994, ANN STAT, V22, P1993, DOI 10.1214/aos/1176325768; KUPPER LL, 1985, J CHRON DIS, V38, P811, DOI 10.1016/0021-9681(85)90105-5; Linhart H., 1986, MODEL SELECTION; OSBORNE MR, 1998, 981 U AD DEP STAT; Pflug GC, 1995, MATH OPER RES, V20, P769, DOI 10.1287/moor.20.4.769; POLLARD D, 1991, ECONOMET THEOR, V7, P186; SASAKI K., 1963, Phytopathologische Zeitschrift, V46, P343; SRIVASTA.MS, 1971, ANN MATH STAT, V42, P1403, DOI 10.1214/aoms/1177693251; Van der Vaart A., 1996, WEAK CONVERGENCE EMP	20	338	352	1	11	INST MATHEMATICAL STATISTICS	HAYWARD	IMS BUSINESS OFFICE-SUITE 7, 3401 INVESTMENT BLVD, HAYWARD, CA 94545 USA	0090-5364			ANN STAT	Ann. Stat.	OCT	2000	28	5					1356	1378				23	Statistics & Probability	Mathematics	408HR	WOS:000167321200005		
J	George, EI				George, EI			The variable selection problem	JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION			English	Article							LINEAR-MODEL SELECTION; CROSS-VALIDATION; INFLATION CRITERION; MULTIPLE-REGRESSION; WAVELET SHRINKAGE; SCHWARZ CRITERION; BAYES FACTORS; UNCERTAINTY; PREDICTION; CHOICE	The problem of variable selection is one of the most pervasive model selection problems in statistical applications. Often referred to as the problem of subset selection, it arises when one wants to model the relationship between a variable of interest and a subset of potential explanatory variables or predictors, but there is uncertainty about which subset to use. This vignette reviews some of the key developments that have led to the wide variety of approaches for this problem.	Univ Texas, Dept MSIS, Austin, TX 78712 USA	George, EI (reprint author), Univ Texas, Dept MSIS, Austin, TX 78712 USA.						Akaike H., 1973, 2 INT S INF THEOR, P267; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; EFRON B, 1983, J AM STAT ASSOC, V78, P316, DOI 10.2307/2288636; Hoeting JA, 1999, STAT SCI, V14, P382; FURNIVAL GM, 1974, TECHNOMETRICS, V16, P499, DOI 10.2307/1267601; STONE M, 1977, J R STAT SOC B, V39, P44; GEISSER S, 1979, J AM STAT ASSOC, V74, P153, DOI 10.2307/2286745; SHIBATA R, 1981, BIOMETRIKA, V68, P45; KASS RE, 1995, J AM STAT ASSOC, V90, P928, DOI 10.2307/2291327; HURVICH CM, 1989, BIOMETRIKA, V76, P297, DOI 10.1093/biomet/76.2.297; RISSANEN J, 1978, AUTOMATICA, V14, P465, DOI 10.1016/0005-1098(78)90005-5; GEORGE EI, 1993, J AM STAT ASSOC, V88, P881, DOI 10.2307/2290777; SHAO J, 1993, J AM STAT ASSOC, V88, P486, DOI 10.2307/2290328; BENJAMINI Y, 1995, J ROY STAT SOC B MET, V57, P289; DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425; SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136; Berger JO, 1996, J AM STAT ASSOC, V91, P109, DOI 10.2307/2291387; MALLOWS CL, 1973, TECHNOMETRICS, V15, P661, DOI 10.2307/1267380; Berger J. O., 1996, BAYESIAN STAT, V5, P25; BRIEMAN L, 1992, J AM STAT ASSOC, V87, P738; BRIEMAN L, 1995, TECHNOMETRICS, V37, P373; Clyde M, 1998, BIOMETRIKA, V85, P391, DOI 10.1093/biomet/85.2.391; Clyde M., 1999, BAYESIAN INFERENCE W, P309; Clyde M., 1999, BAYESIAN STAT, V6; Clyde M, 2000, J ROY STAT SOC B, V62, P681, DOI 10.1111/1467-9868.00257; Dash M, 1997, INTELL DATA ANAL, V1, P131, DOI DOI 10.1016/S1088-467X(97)00008-5; Donoho DL, 1995, J AM STAT ASSOC, V90, P1200, DOI 10.2307/2291512; DRAPER D, 1995, J R STAT SOC B, V57, P45; Efroymson M. A., 1960, MATH METHODS DIGITAL, P191; FOSTER DP, 1994, ANN STAT, V22, P1947, DOI 10.1214/aos/1176325766; Garthwaite PH, 1996, CHEMOMETR INTELL LAB, V35, P1, DOI 10.1016/S0169-7439(96)00035-4; George E. I., 2006, ENCY STAT SCI UPDATE, V3, P39; GEORGE EI, 2000, IN PRESS BIOMETRIKA, V87; George EI, 1997, STAT SINICA, V7, P339; GONG G, 1986, J AM STAT ASSOC, V81, P108, DOI 10.2307/2287975; Green PJ, 1995, BIOMETRIKA, V82, P711, DOI 10.1093/biomet/82.4.711; HAUGHTON DMA, 1988, ANN STAT, V16, P342, DOI 10.1214/aos/1176350709; HOCKING RR, 1976, BIOMETRICS, V32, P1, DOI 10.2307/2529336; Hurvich CM, 1998, BIOMETRIKA, V85, P701, DOI 10.1093/biomet/85.3.701; JOHNSTONE IM, 1998, EMPIRICIAL BAYES APP; LAUD PW, 1995, J ROY STAT SOC B MET, V57, P247; LINDLEY DV, 1968, J R STAT SOC B, V30, P31; MALLOWS CL, 1995, TECHNOMETRICS, V37, P362, DOI 10.2307/1269729; Miller A. J., 1990, SUBSET SELECTION REG; OHAGAN A, 1995, J ROY STAT SOC B MET, V57, P99; Pauler DK, 1998, BIOMETRIKA, V85, P13, DOI 10.1093/biomet/85.1.13; Raftery AE, 1996, BIOMETRIKA, V83, P251, DOI 10.1093/biomet/83.2.251; RAO CR, 1989, BIOMETRIKA, V76, P369, DOI 10.2307/2336671; San Martini A., 1984, J ROYAL STAT SOC B, V46, P296; Shao J, 1997, STAT SINICA, V7, P221; STONE M, 1979, J ROY STAT SOC B MET, V41, P276; THOMPSON ML, 1978, INT STAT REV, V46, P1, DOI 10.2307/1402505; Tibshirani R, 1999, J ROY STAT SOC B, V61, P529, DOI 10.1111/1467-9868.00191; WEI CZ, 1992, ANN STAT, V29, P1; Ye JM, 1998, J AM STAT ASSOC, V93, P120, DOI 10.2307/2669609; ZHANG P, 1993, ANN STAT, V21, P299, DOI 10.1214/aos/1176349027; ZHANG P, 1992, BIOMETRIKA, V79, P741, DOI 10.1093/biomet/79.4.741; Zheng XD, 1997, STAT SINICA, V7, P311	58	111	111	4	16	AMER STATISTICAL ASSOC	ALEXANDRIA	1429 DUKE ST, ALEXANDRIA, VA 22314 USA	0162-1459			J AM STAT ASSOC	J. Am. Stat. Assoc.	DEC	2000	95	452					1304	1308		10.2307/2669776		5	Statistics & Probability	Mathematics	376RJ	WOS:000165470300038		
J	Solo, V				Solo, V			The end of time series	JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION			English	Article							SELECTION; MODEL; REGRESSION; SHRINKAGE; ERROR; ORDER		Univ New S Wales, Sydney, NSW 2052, Australia	Solo, V (reprint author), Univ New S Wales, Sydney, NSW 2052, Australia.						Ait-Sahalia Y, 1996, REV FINANC STUD, V9, P385; AKAIKE H, 1974, IEEE T AUTOMAT CONTR, VAC19, P667, DOI 10.1109/TAC.1974.1100707; Aki K., 1980, QUANTITATIVE SEISMOL; ALLINEY S, 1994, IEEE T SIGNAL PROCES, V42, P618, DOI 10.1109/78.277854; Almasri A., 2003, MULTIVARIATE ANAL; Anderson B., 1984, J TIME SER ANAL, V5, P1, DOI 10.1111/j.1467-9892.1984.tb00374.x; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; BREIMAN L, 1992, J AM STAT ASSOC, V87, P738, DOI 10.2307/2290212; RISSANEN J, 1978, AUTOMATICA, V14, P465, DOI 10.1016/0005-1098(78)90005-5; ENGLE RF, 1987, ECONOMETRICA, V55, P251, DOI 10.2307/1913236; AKAIKE H, 1969, ANN I STAT MATH, V21, P243, DOI 10.1007/BF02532251; SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136; Bartlett M. S., 1955, INTRO STOCHASTIC PRO; Beran J., 1994, STAT LONG MEMORY PRO; BESSE P, 1986, PSYCHOMETRIKA, V51, P285, DOI 10.1007/BF02293986; Bosq D., 1998, NONPARAMETRIC STAT S; Box G., 1970, TIME SERIES ANAL; Brillinger D. R., 1981, TIME SERIES DATA ANA; Brillinger D.R., 1980, HDB STATISTICS, VI, P237, DOI 10.1016/S0169-7161(80)01010-3; Bryson A.E., 1969, APPL OPTIMAL CONTROL; CAINES P. E., 1988, LINEAR STOCHASTIC SY; Campbell JY, 1997, ECONOMETRICS FINANCI; CHEN S, 1996, ATOMIC DECOMPOSITION; CHIB S, 1998, MARKOV CHAIN MONTE C; Daley R., 1991, ATMOSPHERIC DATA ANA; DEISTLER M, 1989, J ECONOMETRICS, V41, P39, DOI 10.1016/0304-4076(89)90042-0; Donoho DL, 1995, J AM STAT ASSOC, V90, P1200, DOI 10.2307/2291512; Durbin J., 1960, REV INT STATIST I, V28, P233, DOI DOI 10.2307/1401322; Fuller W.A., 1976, INTRO STAT TIME SERI; Gallant AR, 1998, J AM STAT ASSOC, V93, P10, DOI 10.2307/2669598; GORDON NJ, 1993, IEE PROC-F, V140, P107; Granger C.W.J, 1969, ECONOMETRICA, V37; HALL P, 1980, MARTINGALE LIMIT THE; Hamilton J, 1994, TIME SERIES ANAL; Hannan E. J., 1988, STAT THEORY LINEAR S; Hannan EJ, 1970, MULTIPLE TIME SERIES; Harvey A. C., 1989, FORECASTING STRUCTUR; Haykin S, 1983, COMMUNICATION SYSTEM; JASZWINSKI AH, 1970, STOCHASTIC PROCESSES; Jenkins G.M., 1969, SPECTRAL ANAL ITS AP; KAILATH J, 1980, LINEAR SYSTEMS; Kalman R. E., 1960, J BASIC ENG, V82, P35, DOI DOI 10.1115/1.3662552; Kay S., 1988, MODERN SPECTRAL ESTI; KITAGAWA G, 1996, SMOOTHNESS PRIORS AN; Kitagawa G., 1993, P 2 US JAP JOINT SEM, P110; Ljung L., 1987, SYSTEM IDENTIFICATIO; PAPOULIS A, 1968, SYSTEMS TRANSFORMS W; Ramsay J. O., 1997, FUNCTIONAL DATA ANAL; RISSANEN J, 1986, ANN STAT, V14, P1080, DOI 10.1214/aos/1176350051; Robinson E., 1967, MULTICHANNEL TIME SE; SHIBATA R, 1980, ANN STAT, V8, P147, DOI 10.1214/aos/1176344897; SHIBATA R, 1976, BIOMETRIKA, V63, P117; SHUMWAY R, 1987, APPL STAT TIME SERIE; Shumway R. H., 1970, J AM STAT ASSOC, V65, P1527, DOI 10.2307/2284334; STEWARD E, 1987, INTRO FOURIER OPTICS; Stock J, 1998, DIFFUSION INDEXES; Tong H., 1990, NONLINEAR TIME SERIE; Wahba G., 1990, SPLINE MODELS OBSERV; Whittle P., 1963, PREDICTION REGULATIO; WHITTLE P, 1963, BIOMETRIKA, V50, P129; WIENER M, 1949, EXTRAPOLATION INTERP	61	1	1	1	2	AMER STATISTICAL ASSOC	ALEXANDRIA	1429 DUKE ST, ALEXANDRIA, VA 22314 USA	0162-1459			J AM STAT ASSOC	J. Am. Stat. Assoc.	DEC	2000	95	452					1346	1349		10.2307/2669785		4	Statistics & Probability	Mathematics	376RJ	WOS:000165470300047		
S	Figueiredo, MAT; Jain, AK		Jacobs, A; Baldwin, T		Figueiredo, MAT; Jain, AK			Bayesian learning of sparse classifiers	2001 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, VOL 1, PROCEEDINGS	IEEE Conference on Computer Vision and Pattern Recognition		English	Proceedings Paper	Conference on Computer Vision and Pattern Recognition	DEC 08-14, 2001	KAUAI, HI	IEEE Comp Soc			GAUSSIAN-PROCESSES	Bayesian approaches to supervised learning use priors on the classifier parameters. However, few priors aim at achieving "sparse" classifiers, where irrelevant/redundant parameters are automatically set to zero. Two well-known ways of obtaining sparse classifiers are: use a zero-mean Laplacian prior on the parameters, and the "support vector machine" (SVM). Whether one uses a Laplacian prior or an SVM, one still needs to specify/estimate the parameters that control the degree of sparseness of the resulting classifiers. We propose a Bayesian approach to learning sparse classifiers which does not involve any parameters controlling the degree of sparseness. This is achieved by a hierarchical-Bayes interpretation of the Laplacian prior, followed by the adoption of a Jeffreys' non-informative hyper-prior Implementation is carried out by an EM algorithm. Experimental evaluation of the proposed method shows that it performs competitively with (often better than) the best classification techniques available.	Univ Tecn Lisboa, Inst Telecommun, Inst Super Tecn, P-1049 Lisbon, Portugal	Figueiredo, MAT (reprint author), Univ Tecn Lisboa, Inst Telecommun, Inst Super Tecn, P-1049 Lisbon, Portugal.	mtf@lx.it.pt; jain@cse.msu.edu	Figueiredo, Mario/C-5428-2008	Figueiredo, Mario/0000-0002-0970-7745			ALBERT JH, 1993, J AM STAT ASSOC, V88, P669, DOI 10.2307/2290350; WILLIAMS PM, 1995, NEURAL COMPUT, V7, P117, DOI 10.1162/neco.1995.7.1.117; Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010; Jain AK, 2000, IEEE T PATTERN ANAL, V22, P4, DOI 10.1109/34.824819; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1; POGGIO T, 1990, P IEEE, V78, P1481, DOI 10.1109/5.58326; Williams CKI, 1998, IEEE T PATTERN ANAL, V20, P1342, DOI 10.1109/34.735807; Lewicki MS, 2000, NEURAL COMPUT, V12, P337, DOI 10.1162/089976600300015826; Berger J. O., 1980, STAT DECISION THEORY; Bishop C., 2000, P 16 C UNC ART INT, P46; Bishop C. M., 1995, NEURAL NETWORKS PATT; Breiman L, 1983, CLASSIFICATION REGRE; Cherkassky V. S., 1998, LEARNING DATA CONCEP; Cristianini N., 2000, SUPPORT VECTOR MACHI, V1st; DANRUBINSTEIN Y, 1997, P 3 INT C KNOWL DISC, P49; Fahrmeir L, 1994, MULTIVARIATE STAT MO; Figueiredo MAT, 2001, IEEE T IMAGE PROCESS, V10, P1322, DOI 10.1109/83.941856; KIMELDOR.GS, 1970, ANN MATH STAT, V41, P495, DOI 10.1214/aoms/1177697089; Lange K. L., 1993, J COMPUT GRAPH STAT, V2, P175, DOI 10.2307/1390698; MACKAY DJC, 1993, MAXIMUM ENTROPY BAYE, P221; McCullagh P., 1989, GEN LINEAR MODELS; Neal R. M., 1996, BAYESIAN LEARNING NE; Ripley B. D., 1996, PATTERN RECOGNITION; Seeger M, 2000, ADV NEUR IN, V12, P603; TibShirani R., 1996, J ROYAL STAT SOC B, V58; Tipping ME, 2000, ADV NEUR IN, V12, P652; Vapnik V., 1998, STAT LEARNING THEORY; Wahba G., 1990, SPLINE MODELS OBSERV; WILLIAMS C, 2001, NIPS 13; Williams CKI, 1998, LEARNING INFERENCE G	30	8	11	1	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA	1063-6919		0-7695-1272-0	PROC CVPR IEEE			2001							35	41				7	Computer Science, Artificial Intelligence; Computer Science, Information Systems; Computer Science, Software Engineering; Imaging Science & Photographic Technology	Computer Science; Imaging Science & Photographic Technology	BX22J	WOS:000184694200005		
S	Roth, V		Dorffner, G; Bischof, H; Hornik, K		Roth, V			Sparse kernel regressors	ARTIFICIAL NEURAL NETWORKS-ICANN 2001, PROCEEDINGS	LECTURE NOTES IN COMPUTER SCIENCE		English	Article; Proceedings Paper	International Conference on Artificial Neural Networks (ICANN 2001)	AUG 21-25, 2001	VIENNA, AUSTRIA	Austrian Res Inst Artifical Intelligence	VIENNA UNIV TECHNOL		SELECTION	Sparse kernel regressors have become popular by applying the support vector method to regression problems. Although this approach has been shown to exhibit excellent generalization properties in many experiments, it suffers from several drawbacks: the absence of probabilistic outputs, the restriction to Mercer kernels, and the steep growth of the number of support vectors with increasing size of the training set. In this paper we present a new class of kernel regressors that effectively overcome the above problems. We call this now approach generalized LASSO regression. It has a clear probabilistic interpretation, produces extremely sparse solutions, can handle learning sets that are corrupted by outliers, and is capable of dealing with large-scale problems.	Univ Bonn, Dept Comp Sci 3, D-53117 Bonn, Germany	Roth, V (reprint author), Univ Bonn, Dept Comp Sci 3, Roemerstr 164, D-53117 Bonn, Germany.						Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Osborne MR, 2000, IMA J NUMER ANAL, V20, P389, DOI 10.1093/imanum/20.3.389; CLARK DI, 1988, IMA J NUMER ANAL, V8, P23, DOI 10.1093/imanum/8.1.23; COLLOBERT R, 2000, IDIAPRR0017 IDIAP; Fessler JA, 1997, P SOC PHOTO-OPT INS, V3170, P184, DOI 10.1117/12.279713; FRIEDMAN JH, 1991, ANN STAT, V19, P1, DOI 10.1214/aos/1176347963; Grandvalet Y., 1998, ICANN 98. Proceedings of the 8th International Conference on Artificial Neural Networks; TIPPING ME, 1999, NEURAL INFORMATION P, V12, P652	8	5	5	2	2	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		3-540-42486-5	LECT NOTES COMPUT SC			2001	2130						339	346				8	Computer Science, Artificial Intelligence; Computer Science, Theory & Methods	Computer Science	BT43Y	WOS:000173024600047		
J	Klinger, A				Klinger, A			Inference in high dimensional generalized linear models based on soft thresholding	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES B-STATISTICAL METHODOLOGY			English	Article						generalized additive models; generalized linear models; penalized likelihood; shrinkage; splines; thresholding	ADAPTIVE REGRESSION SPLINES; WAVELET SHRINKAGE; LASSO	We further develop and analyse penalized likelihood estimators for generalized linear models with a large number of coefficients. The methodology proposed leads to an adaptive selection of model terms without substantial variance inflation. Our proposal extends the soft thresholding strategy of Donoho and Johnstone and the lasso of Tibshirani to generalized linear models and multiple predictor variables. In addition, we develop an estimator for the covariance matrix of the estimated coefficients, which can even be used for terms dropped from the model. Used in connection with basis functions, the methodology proposed provides an alternative to other generalized function estimators. It leads to an adaptive economical description of the results in terms of basis functions. Specifically, it is shown how adaptive regression splines and qualitative restrictions can be incorporated. Our approach is demonstrated by applications to a prognosis of solvency and rental guides.	Univ Munich, D-80539 Munich, Germany	Klinger, A (reprint author), Bayer Ruck Holding AG, Sederanger 4-6, D-80538 Munich, Germany.						Fu WJJ, 1998, J COMPUT GRAPH STAT, V7, P397, DOI 10.2307/1390712; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; SMITH PL, 1979, AM STAT, V33, P57, DOI 10.2307/2683222; DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425; HASTIE T, 1993, J ROY STAT SOC B MET, V55, P757; BACCHETTI P, 1989, J AM STAT ASSOC, V84, P289, DOI 10.2307/2289876; Bruce AG, 1996, BIOMETRIKA, V83, P727, DOI 10.1093/biomet/83.4.727; Cox D., 1974, THEORETICAL STAT; Cox DD, 1996, J MULTIVARIATE ANAL, V56, P185, DOI 10.1006/jmva.1996.0010; DEMMLER A, 1975, NUMER MATH, V24, P375, DOI 10.1007/BF01437406; Donoho DL, 1995, J AM STAT ASSOC, V90, P1200, DOI 10.2307/2291512; FAHRMEIR L, 1994, GUTACHTEN ERSTELLUNG; FAHRMEIR L, 1996, MULTIVARIATE STAT VE, V2; FAHRMEIR L, 1998, ECONOMETRICS THEORY, P241; Fahrmeir L., 1990, STATISTICS, V21, P487, DOI 10.1080/02331889008802259; FAHRMEIR L, 1996, ULTIVARIATE STAT VER, V2, P357; FAN JQ, 1995, J AM STAT ASSOC, V90, P141, DOI 10.2307/2291137; FRIEDMAN JH, 1991, ANN STAT, V19, P1, DOI 10.1214/aos/1176347963; Gill P. E., 1981, PRACTICAL OPTIMIZATI; Hastie T, 1996, J ROY STAT SOC B MET, V58, P379; Hastie TJ, 1990, GEN ADDITIVE MODELS; KLINGER A, 1998, THESIS U MUNCHEN MUN; KLINKE S, 1996, 9692 HUMB U BERL BER; Mammen E, 1997, ANN STAT, V25, P387; MARX BD, 1992, STAT MODEL, P227; McCullagh P., 1989, GEN LINEAR MODELS; MCDONALD JW, 1990, BIOMETRICS, V46, P201, DOI 10.2307/2531643; Michie D., 1994, MACHINE LEARNING NEU; NYQUIST H, 1991, APPL STAT-J ROY ST C, V40, P133, DOI 10.2307/2347912; Ramsay J. O., 1988, STAT SCI, V3, P425, DOI DOI 10.1214/SS/1177012761; TISHLER A, 1982, OPTIMIZATION STAT	31	5	5	0	1	BLACKWELL PUBL LTD	OXFORD	108 COWLEY RD, OXFORD OX4 1JF, OXON, ENGLAND	1369-7412			J ROY STAT SOC B	J. R. Stat. Soc. Ser. B-Stat. Methodol.		2001	63		2				377	392		10.1111/1467-9868.00291		16	Statistics & Probability	Mathematics	434VU	WOS:000168837200011		
J	Steyerberg, EW; Eijkemans, MJC; Harrell, FE; Habbema, JDF				Steyerberg, EW; Eijkemans, MJC; Harrell, FE; Habbema, JDF			Prognostic modeling with logistic regression analysis: In search of a sensible strategy in small data sets	MEDICAL DECISION MAKING			English	Article; Proceedings Paper	21st Annual Meeting of the Society-for-Medical-Decision Making	OCT 03-05, 1999	RENO, NEVADA	Soc Med Decis Making		regression analysis; logistic models; bias; variable selection; prediction	ACUTE MYOCARDIAL-INFARCTION; GERM-CELL TUMOR; NEURAL NETWORKS; COX REGRESSION; THROMBOLYTIC THERAPY; PREDICTIVE VALUE; DECISION-MAKING; MORTALITY; SELECTION; LIKELIHOOD	Clinical decision making often requires estimates of the likelihood of a dichotomous outcome in individual patents. When empirical data are available, these estimates may well be obtained from a logistic regression model. Several strategies may be followed in the development of such a model. In this study, the authors compare alternative strategies in 23 small subsamples from a large data set of patients with an acute myocardial infarction, where they developed predictive models for 30-day mortality. Evaluations were performed in an independent part of the data set. Specifically, the authors studied the effect of coding of covariables and stepwise selection on discriminative ability of the resulting model, and the effect of statistical "shrinkage" techniques on calibration. As expected, dichotomization of continuous covariables implied a loss of information. Remarkably, stepwise selection resulted in less discriminating models compared to full models including all available covariables, even when more than half of these were randomly associated with the outcome. Using qualitative information on the sign of the effect of predictors slightly improved the predictive ability. Calibration improved when shrinkage was applied on the standard maximum likelihood estimates of the regression coefficients. In conclusion, a sensible strategy in small data sets is to apply shrinkage methods in full models that include well-coded predictors that are selected based on external information.	Erasmus Univ, Ctr Clin Decis Sci, Dept Publ Hlth, Rotterdam, Netherlands; Univ Virginia, Div Biostat & Epidemiol, Dept Hlth Evaluat Sci, Charlottesville, VA 22903 USA	Steyerberg, EW (reprint author), Erasmus Univ, Ctr Clin Decis Sci, Dept Publ Hlth, EE2091, Rotterdam, Netherlands.		Eijkemans, Marinus/D-8214-2013; 	Steyerberg, Ewout/0000-0002-7787-0122			ALTMAN DG, 1989, STAT MED, V8, P771, DOI 10.1002/sim.4780080702; HARRELL FE, 1988, J NATL CANCER I, V80, P1198, DOI 10.1093/jnci/80.15.1198; BREIMAN L, 1995, TECHNOMETRICS, V37, P373, DOI 10.2307/1269730; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; EFRON B, 1983, J AM STAT ASSOC, V78, P316, DOI 10.2307/2288636; MAGGIONI AP, 1993, NEW ENGL J MED, V329, P1442, DOI 10.1056/NEJM199311113292002; SIMON R, 1994, BRIT J CANCER, V69, P979, DOI 10.1038/bjc.1994.192; PAUKER SG, 1975, NEW ENGL J MED, V293, P229, DOI 10.1056/NEJM197507312930505; LEE KL, 1995, CIRCULATION, V91, P1659; PAUKER SG, 1980, NEW ENGL J MED, V302, P1109, DOI 10.1056/NEJM198005153022003; COX DR, 1958, BIOMETRIKA, V45, P562, DOI 10.1093/biomet/45.3-4.562; Buckland ST, 1997, BIOMETRICS, V53, P603, DOI 10.2307/2533961; Steyerberg EW, 1999, J CLIN EPIDEMIOL, V52, P935, DOI 10.1016/S0895-4356(99)00103-1; SAUERBREI W, 1992, STAT MED, V11, P2093, DOI 10.1002/sim.4780111607; COPAS JB, 1983, J R STAT SOC B, V45, P311; Harrell FE, 1996, STAT MED, V15, P361, DOI 10.1002/(SICI)1097-0258(19960229)15:4<361::AID-SIM168>3.0.CO;2-4; Brenner H, 1997, EPIDEMIOLOGY, V8, P429, DOI 10.1097/00001648-199707000-00014; Califf RM, 1997, AM HEART J, V133, P630, DOI 10.1016/S0002-8703(97)70164-9; CHATFIELD C, 1995, J ROY STAT SOC A STA, V158, P419, DOI 10.2307/2983440; CHEN CH, 1985, STAT MED, V4, P39, DOI 10.1002/sim.4780040107; DERKSEN S, 1992, BRIT J MATH STAT PSY, V45, P265; DRAPER D, 1995, J R STAT SOC B, V57, P45; DUBOIS C, 1988, AM J CARDIOL, V61, P216, DOI 10.1016/0002-9149(88)90918-6; Efron B., 1993, INTRO BOOSTRAP; Ennis M, 1998, STAT MED, V17, P2501; GLASZIOU PP, 1995, BRIT MED J, V311, P1356; GOODMAN PH, NEVPROP SOFTWARE VER; GRAY RJ, 1992, J AM STAT ASSOC, V87, P942, DOI 10.2307/2290630; GREENLAND S, 1993, STAT MED, V12, P717, DOI 10.1002/sim.4780120802; *GUSTO 1 INV, 1993, NEW ENGL J MED, V306, P673; Harrell F., DESIGN S PLUS FUNCTI; Harrell FE, 1998, STAT MED, V17, P909, DOI 10.1002/(SICI)1097-0258(19980430)17:8<909::AID-SIM753>3.3.CO;2-F; HARRELL FE, 1982, JAMA-J AM MED ASSOC, V247, P2543, DOI 10.1001/jama.247.18.2543; HARRELL FE, 1984, STAT MED, V3, P143, DOI 10.1002/sim.4780030207; Hastie T, 1995, Stat Methods Med Res, V4, P187, DOI 10.1177/096228029500400302; HILDEN J, 1978, METHOD INFORM MED, V17, P227; Lapuerta P, 1998, MED DECIS MAKING, V18, P70, DOI 10.1177/0272989X9801800114; Laupacis A, 1997, JAMA-J AM MED ASSOC, V277, P488, DOI 10.1001/jama.277.6.488; LUBSEN J, 1978, METHOD INFORM MED, V17, P127; MAYNARD C, 1993, AM J CARDIOL, V72, P877, DOI 10.1016/0002-9149(93)91099-4; MILLER ME, 1991, STAT MED, V10, P1213, DOI 10.1002/sim.4780100805; MUELLER HS, 1992, CIRCULATION, V85, P1254; Orr RK, 1997, MED DECIS MAKING, V17, P178, DOI 10.1177/0272989X9701700208; Peduzzi P, 1996, J CLIN EPIDEMIOL, V49, P1373, DOI 10.1016/S0895-4356(96)00236-3; Penny W, 1996, MED DECIS MAKING, V16, P386, DOI 10.1177/0272989X9601600409; PRYOR DB, 1983, AM J MED, V75, P771, DOI 10.1016/0002-9343(83)90406-0; Selker HP, 1997, ANN INTERN MED, V127, P538; SPANOS A, 1989, JAMA-J AM MED ASSOC, V262, P2700, DOI 10.1001/jama.262.19.2700; SPIEGELHALTER DJ, 1986, STAT MED, V5, P421, DOI 10.1002/sim.4780050506; STEYERBERG EW, 2000, IN PRESS STAT MED; Steyerberg EW, 1997, CANCER, V79, P345; STEYERBERG EW, 1995, J CLIN ONCOL, V13, P1177; STEYERBERG EW, SAMPLE4 ZIP SPLUS PR; Steyerberg EW, 1998, MED DECIS MAKING, V18, P349, DOI 10.1177/0272989X9801800314; Tibshirani R, 1997, STAT MED, V16, P385, DOI 10.1002/(SICI)1097-0258(19970228)16:4<385::AID-SIM380>3.0.CO;2-3; Tu JV, 1998, MED DECIS MAKING, V18, P229; VANHOUWELINGEN JC, 1990, STAT MED, V9, P1303, DOI 10.1002/sim.4780091109; VERWEIJ PJM, 1994, STAT MED, V13, P2427, DOI 10.1002/sim.4780132307	58	208	211	1	13	SAGE PUBLICATIONS INC	THOUSAND OAKS	2455 TELLER RD, THOUSAND OAKS, CA 91320 USA	0272-989X			MED DECIS MAKING	Med. Decis. Mak.	JAN-FEB	2001	21	1					45	56				12	Health Care Sciences & Services; Medical Informatics	Health Care Sciences & Services; Medical Informatics	469EL	WOS:000170801600006	11206946	
S	Eilers, PHC; Boer, JM; van Ommen, GJ; van Houwelingen, HC		Bittner, ML; Chen, YD; Dorsel, AN; Dougherty, ER		Eilers, PHC; Boer, JM; van Ommen, GJ; van Houwelingen, HC			Classification of microarray data with penalized logistic regression	MICROARRAYS: OPTICAL TECHNOLOGIES AND INFORMATICS	PROCEEDINGS OF THE SOCIETY OF PHOTO-OPTICAL INSTRUMENTATION ENGINEERS (SPIE)		English	Proceedings Paper	Conference on Microarrays - Optical Technologies and Informatics	JAN 21-22, 2001	SAN JOSE, CA	SPIE		AIC; genetic expression; cross-validation; generalized linear models; multicollinearity; multivariate calibration; ridge regression; singular value decomposition		Classification of microarray data needs a firm statistical basis. In principle, logistic regression can provide it, modeling the probability of membership of a class with (transforms of) linear combinations of explanatory variables. However, classical logistic regression does not work for microarrays, because generally there will be far more variables than observations. One problem is multicollinearity: estimating equations become singular and have no unique and stable solution. A second problem is over-fitting: a model may fit well to a data set, but perform badly when used to classify new data. We propose penalized likelihood as a solution to both problems. The values of the regression coefficients are constrained in a similar way as in ridge regression. All variables play an equal role., there is no ad-hoc selection of "most relevant" or "most expressed" genes. The dimension of the resulting systems of equations is equal to the number of variables, and generally will be too large for most computers, but it can dramatically be reduced with the singular value decomposition of some matrices. The penalty is optimized with AIC (Akaike's Information Criterion), which essentially is a measure of prediction performance. We find that penalized logistic regression performs well on a public data set (the MIT ALL/AML data).	Leiden Univ, Med Ctr, Dept Med Stat, Leiden, Netherlands	Eilers, PHC (reprint author), POB 9604, NL-2300 RC Leiden, Netherlands.						Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Golub TR, 1999, SCIENCE, V286, P531, DOI 10.1126/science.286.5439.531; FRANK IE, 1993, TECHNOMETRICS, V35, P109, DOI 10.2307/1269656; Alter O, 2000, P NATL ACAD SCI USA, V97, P10101, DOI 10.1073/pnas.97.18.10101; Burnham K. P., 1998, MODEL SELECTION INFE; LECESSIE S, 1992, APPL STAT-J ROY ST C, V41, P191; DUDOIT S, 2000, 576 U CAL DEP STAT B; FEARN T, 1983, APPL STAT, V32, P73, DOI 10.2307/2348045; HASTIE T, 2000, GENOME BIOL, V1, P3; Hastie TJ, 1990, GEN ADDITIVE MODELS; HOERL AE, 1970, TECHNOMETRICS, V12, P69, DOI 10.2307/1267352; HOERL AE, 1985, APPL STAT-J ROY ST C, V34, P114, DOI 10.2307/2347363; Martens H., 1989, MULTIVARIATE CALIBRA; MARX BD, 1996, P 11 INT WORKSH STAT, P259; Marx BD, 1999, TECHNOMETRICS, V41, P1, DOI 10.2307/1270990; McCullagh P., 1989, GEN LINEAR MODELS; Myers R, 1986, CLASSICAL MODERN REG; Peduzzi P, 1996, J CLIN EPIDEMIOL, V49, P1373, DOI 10.1016/S0895-4356(96)00236-3; THOMAS EV, 1994, ANAL CHEM, V66, pA795, DOI 10.1021/ac00087a002; VANHOUWELINGEN JC, 1990, STAT MED, V9, P1303, DOI 10.1002/sim.4780091109; Watkins D. S., 1991, FUNDAMENTALS MATRIX; WEST M, 2000, 0015 DUK U I STAT DE	22	8	8	0	0	SPIE-INT SOC OPTICAL ENGINEERING	BELLINGHAM	1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA	0277-786X		0-8194-3944-4	P SOC PHOTO-OPT INS			2001	2	23					187	198				12	Engineering, Biomedical; Optics	Engineering; Optics	BS86U	WOS:000171277300020		
J	Golan, A				Golan, A			A simultaneous estimation and variable selection rule	JOURNAL OF ECONOMETRICS			English	Article						shrinkage estimator; maximum entropy; extraneous variables; squared error loss; data weighted prior; subset selection	EMPIRICAL LIKELIHOOD; MAXIMUM-ENTROPY; MODEL SELECTION; LINEAR-REGRESSION; PRIOR INFORMATION; BAYES FACTORS; INFERENCE; CRITERIA	A new data-based method of estimation and variable selection in linear statistical models is proposed. This method is based on a generalized maximum entropy formalism, and makes use of both sample and non-sample information in determining a basis for coefficient shrinkage and extraneous variable identification. In contrast to tradition, shrinkage and variable selection are achieved on a coordinate-by-coordinate basis, and the procedure works well for both ill- and well-posed statistical models. Analytical asymptotic results are presented and sampling experiments are used as a basis for determining finite sample behavior and comparing the sampling performance of the new estimation rule with traditional competitors. Solution algorithms for the non-linear inversion problem that results are simple to implement. (C) 2001 Elsevier Science S.A. All rights reserved. JEL classification: C13; C14; C5.	American Univ, Dept Econ, Washington, DC 20016 USA	Golan, A (reprint author), American Univ, Dept Econ, Roper 200,4400 Massachusetts Ave NW, Washington, DC 20016 USA.						AKAIKE H, 1974, IEEE T AUTOMAT CONTR, VAC19, P716, DOI 10.1109/TAC.1974.1100705; BREIMAN L, 1995, TECHNOMETRICS, V37, P373, DOI 10.2307/1269730; KASS RE, 1995, J AM STAT ASSOC, V90, P773, DOI 10.1080/01621459.1995.10476572; JAYNES ET, 1957, PHYS REV, V108, P171, DOI 10.1103/PhysRev.108.171; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; GOOD IJ, 1963, ANN MATH STAT, V34, P911, DOI 10.1214/aoms/1177704014; SHIBATA R, 1981, BIOMETRIKA, V68, P45; HOERL AE, 1970, TECHNOMETRICS, V12, P55; CSISZAR I, 1991, ANN STAT, V19, P2032, DOI 10.1214/aos/1176348385; Imbens GW, 1998, ECONOMETRICA, V66, P333, DOI 10.2307/2998561; GEORGE EI, 1993, J AM STAT ASSOC, V88, P881, DOI 10.2307/2290777; JAYNES ET, 1957, PHYS REV, V106, P620, DOI 10.1103/PhysRev.106.620; OWEN A, 1990, ANN STAT, V18, P90, DOI 10.1214/aos/1176347494; SHORE JE, 1980, IEEE T INFORM THEORY, V26, P26, DOI 10.1109/TIT.1980.1056144; SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136; MALLOWS CL, 1973, TECHNOMETRICS, V15, P661, DOI 10.2307/1267380; AMEMIYA T, 1980, INT ECON REV, V21, P331, DOI 10.2307/2526185; Sin CY, 1996, J ECONOMETRICS, V71, P207, DOI 10.1016/0304-4076(94)01701-8; BERCHER JF, 1996, MAXIMUM ENTROPY BAYE; Bock M.E., 1988, STAT DECISION THEORY, P281; BOHRER R, 1984, J ECONOMETRICS, V25, P235, DOI 10.1016/0304-4076(84)90049-6; BREIMAN L, 1992, INT STAT REV, V60, P291, DOI 10.2307/1403680; BROWN LD, 1966, ANN MATH STAT, V37, P1087, DOI 10.1214/aoms/1177699259; DONOHO DL, 1992, J ROY STAT SOC B MET, V54, P41; GEWEKE J, 1994, 539 FED RES BANK MIN; GOKHALE DV, 1978, INFORMATION CONTINGE; GOLAN A, 1996, 1997 SUMM EC SOC M; Golan A., 1996, MAXIMUM ENTROPY ECON; Hellerstein JK, 1999, REV ECON STAT, V81, P1, DOI 10.1162/003465399557860; HOCKING RR, 1976, BIOMETRICS, V32, P1, DOI 10.2307/2529336; JAMES W, 1960, P 4 BERK S MATH STAT, P361; Jaynes E. T., 1984, INVERSE PROBL, V14, P151; JUDGE G, 1987, J ECONOMETRICS, V35, P375, DOI 10.1016/0304-4076(87)90034-0; Judge G., 1978, STAT IMPLICATIONS PR; KEMPTHORNE PJ, 1987, ANN STAT, V15, P1389, DOI 10.1214/aos/1176350600; KULLBACK J, 1959, INFORMATION THEORY S; LaFrance JT, 1999, AM J AGR ECON, V81, P728, DOI 10.2307/1244042; LAUD PW, 1995, J ROY STAT SOC B MET, V57, P247; Lavergne P., 1998, ECONOMET REV, V17, P227, DOI 10.1080/07474939808800415; LEVINE RD, 1980, J PHYS A-MATH GEN, V13, P91, DOI 10.1088/0305-4470/13/1/011; LONGLEY JW, 1967, J AM STAT ASSOC, V62, P819, DOI 10.2307/2283673; Miller A. J., 1990, SUBSET SELECTION REG; MITCHELL TJ, 1988, J AM STAT ASSOC, V83, P1023, DOI 10.2307/2290129; MITTELHAMMER R, 1996, UNPUB CONSISTENCY AS; Berger JO, 1999, J AM STAT ASSOC, V94, P542, DOI 10.2307/2670175; O'Sullivan F., 1986, STAT SCI, V1, P502, DOI 10.1214/ss/1177013525; OWEN A, 1991, ANN STAT, V19, P1725, DOI 10.1214/aos/1176348368; PERLOFF J, 1999, UNPUB MAXIMUM ENTROP; QIN J, 1994, ANN STAT, V22, P300, DOI 10.1214/aos/1176325370; SHANNON CE, 1948, AT&T TECH J, V27, P379; Skilling J., 1989, MAXIMUM ENTROPY BAYE, P173; Soofi E. S., 1996, BAYESIAN ANAL STAT E, P179; Stein Charles, 1955, P 3 BERK S MATH STAT, P197; STEIN CM, 1981, ANN STAT, V9, P1135, DOI 10.1214/aos/1176345632; STONE M, 1974, J R STAT SOC B, V36, P111; TITTERINGTON DM, 1985, INT STAT REV, V53, P141, DOI 10.2307/1402932; TOBIAS J, IN PRESS INT EC REV; ZELLNER A, 1997, ADV ECONOMETRICS, V12, P85, DOI 10.1108/S0731-9053(1997)0000012005; ZELLNER A, 1988, AM STAT, V42, P278, DOI 10.2307/2685143; Zellner A, 1999, AM J AGR ECON, V81, P742, DOI 10.2307/1244044; ZELLNER A, 1996, PREDICTION MODELLING; ZELLNER A, 1975, STUDIES BAYESIAN ECO; Zellner A, 1996, J ECONOMETRICS, V75, P51, DOI 10.1016/0304-4076(95)01768-2; ZHANG P, 1992, J AM STAT ASSOC, V87, P732, DOI 10.2307/2290211; ZHENG XD, 1995, J AM STAT ASSOC, V90, P151	65	8	8	0	5	ELSEVIER SCIENCE SA	LAUSANNE	PO BOX 564, 1001 LAUSANNE, SWITZERLAND	0304-4076			J ECONOMETRICS	J. Econom.	MAR	2001	101	1					165	193		10.1016/S0304-4076(00)00078-6		29	Economics; Mathematics, Interdisciplinary Applications; Social Sciences, Mathematical Methods	Business & Economics; Mathematics; Mathematical Methods In Social Sciences	394XA	WOS:000166548600007		
J	van Houwelingen, JC				van Houwelingen, JC			Shrinkage and penalized likelihood as methods to improve predictive accuracy	STATISTICA NEERLANDICA			English	Article; Proceedings Paper	Workshop on Prediction in Medical Statistics	FEB 04-05, 1999	ODENSE, DENMARK		ODENSE UNIV, DEPT STATIST & DEMOG	Pre-Test Estimation; Ridge Regression; LASSO; Garotte	COX REGRESSION; ESTIMATORS; MODELS; SPLINES; TIME	A review is given of shrinkage and penalization as tools to improve predictive accuracy of regression models. The James-Stein estimator is taken as starting point. Procedures covered are Pre-test Estimation, the Ridge Regression of Hoerl and Kennard, the Shrinkage Estimators of Copas and Van Houwelingen and Le Cessie, the LASSO of Tibshirani and the Garotte of Breiman. An attempt is made to place all these procedures in a unifying framework of semi-Bayesian methodology. Applications are briefly mentioned, but not amply discussed.	Dept Med Stat, NL-2300 RC Leiden, Netherlands			van houwelingen, hans/C-1872-2008				Akaike H, 1973, 2 INT S INF THEOR, P268; BREIMAN L, 1995, TECHNOMETRICS, V37, P373, DOI 10.2307/1269730; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; HOERL AE, 1970, TECHNOMETRICS, V12, P55; Eilers PHC, 1996, STAT SCI, V11, P89, DOI 10.1214/ss/1038425655; Verbyla AP, 1999, J ROY STAT SOC C-APP, V48, P269, DOI 10.1111/1467-9876.00154; SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136; MALLOWS CL, 1973, TECHNOMETRICS, V15, P661, DOI 10.2307/1267380; HASTIE T, 1993, J ROY STAT SOC B MET, V55, P757; COPAS JB, 1983, J R STAT SOC B, V45, P311; Harrell FE, 1996, STAT MED, V15, P361, DOI 10.1002/(SICI)1097-0258(19960229)15:4<361::AID-SIM168>3.0.CO;2-4; BANCROFT TA, 1964, BIOMETRICS, V20, P427, DOI 10.2307/2528486; LECESSIE S, 1992, APPL STAT-J ROY ST C, V41, P191; Efron B, 1986, STAT SCI, V1, P54, DOI DOI 10.1214/SS/1177013815; EFRON B, 1972, J AM STAT ASSOC, V67, P130, DOI 10.2307/2284711; HARRELL FE, 1998, STAT MED, V17, P939; Hastie T, 1986, STAT SCI, V1, P297, DOI DOI 10.1214/SS/1177013604; Heisterkamp SH, 1999, BIOMETRICAL J, V41, P385, DOI 10.1002/(SICI)1521-4036(199907)41:4<385::AID-BIMJ385>3.0.CO;2-Z; JAMES W, 1962, P 4 BERK S, V1, P361; LIANG KY, 1992, J ROY STAT SOC B MET, V54, P3; Marx BD, 1999, TECHNOMETRICS, V41, P1, DOI 10.2307/1270990; STEIN CM, 1981, ANN STAT, V9, P1135, DOI 10.1214/aos/1176345632; VANHOUWELINGEN HC, 1995, BIOCYBERNETICS BIOME, V17, P127; VANHOUWELINGEN JC, 1990, STAT MED, V9, P1303, DOI 10.1002/sim.4780091109; Verweij PJM, 1995, BIOMETRICS, V51, P1550, DOI 10.2307/2533286; VERWEIJ PJM, 1994, STAT MED, V13, P2427, DOI 10.1002/sim.4780132307	26	21	21	2	6	BLACKWELL PUBL LTD	OXFORD	108 COWLEY RD, OXFORD OX4 1JF, OXON, ENGLAND	0039-0402			STAT NEERL	Stat. Neerl.	MAR	2001	55	1					17	34		10.1111/1467-9574.00154		18	Statistics & Probability	Mathematics	403JH	WOS:000167038400003		
J	Vach, W				Vach, W			Cross validation calibration based shrinkage - revisited for the construction of prognostic indices	STATISTICA NEERLANDICA			English	Article; Proceedings Paper	Workshop on Prediction in Medical Statistics	FEB 04-05, 1999	ODENSE, DENMARK		ODENSE UNIV, DEPT STATIST & DEMOG	average prediction error; generalized linear models; logistic regression; prediction; regression to the mean	REGRESSION; MODELS	The use of shrinkage methods for the construction of prognostic indices has been paid increasing attention in the literature on medical statistics in the last years, One approach for the construction of a shrinkage factor is cross validation calibration as suggested by VAN HOUWELINGEN and LE CESSIE (1990). We investigate this approach in more detail. First we try to clarify why shrinkage factors constructed by cross validation calibration tend to be smaller than 1. Second we explain why use of this shrinkage factor can result in an improvement of the average prediction error. Third we investigate the possible gain for constellations relevant in medical research by means of a simulation study, focusing on the dilemma, that the improvement on average has to be paid by distinct deteriorations for some patients. Finally we conclude that it is necessary to rethink the choice of loss functions in constructing prognostic indices before recommendations about the use of shrinkage methods can be made.	Odense Univ, Univ So Denmark, Dept Stat & Demog, DK-5230 Odense M, Denmark			Vach, Werner/F-2512-2011				BREIMAN L, 1995, TECHNOMETRICS, V37, P373, DOI 10.2307/1269730; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; SIMON R, 1994, BRIT J CANCER, V69, P979, DOI 10.1038/bjc.1994.192; COPAS JB, 1983, J R STAT SOC B, V45, P311; Harrell FE, 1996, STAT MED, V15, P361, DOI 10.1002/(SICI)1097-0258(19960229)15:4<361::AID-SIM168>3.0.CO;2-4; Cox D. R., 1970, ANAL BINARY DATA; Das P, 1983, STAT NEERL, V37, P15, DOI 10.1111/j.1467-9574.1983.tb00794.x; Fuller W. A., 1987, MEASUREMENT ERROR MO; Galton F., 1886, J ANTHR I, V15, P246, DOI 10.2307/2841583; McCullagh P., 1989, GEN LINEAR MODELS; PREGIBON D, 1981, ANN STAT, V9, P705, DOI 10.1214/aos/1176345513; ROSNER B, 1992, AM J EPIDEMIOL, V136, P1400; Sauerbrei W, 1999, J ROY STAT SOC C-APP, V48, P313, DOI 10.1111/1467-9876.00155; Schumacher M, 1997, STAT MED, V16, P2813, DOI 10.1002/(SICI)1097-0258(19971230)16:24<2813::AID-SIM701>3.0.CO;2-Z; STONE M, 1974, J R STAT SOC B, V36, P111; Vach W, 1997, COMPUTATION STAT, V12, P279; VANHOUWELINGEN JC, 1990, STAT MED, V9, P1303, DOI 10.1002/sim.4780091109; VERWEIJ PJM, 1993, STAT MED, V12, P2305, DOI 10.1002/sim.4780122407	18	0	0	0	1	BLACKWELL PUBL LTD	OXFORD	108 COWLEY RD, OXFORD OX4 1JF, OXON, ENGLAND	0039-0402			STAT NEERL	Stat. Neerl.	MAR	2001	55	1					35	52		10.1111/1467-9574.00155		18	Statistics & Probability	Mathematics	403JH	WOS:000167038400004		
J	Vach, K; Sauerbrei, W; Schumacher, M				Vach, K; Sauerbrei, W; Schumacher, M			Variable selection and shrinkage: comparison of some approaches	STATISTICA NEERLANDICA			English	Article; Proceedings Paper	Workshop on Prediction in Medical Statistics	FEB 04-05, 1999	ODENSE, DENMARK		ODENSE UNIV, DEPT STATIST & DEMOG	selection bias; linear regression; prediction error	COX REGRESSION-MODEL; BOOTSTRAP	A common strategy within the framework of regression models is the selection of variables with possible predictive value, which are incorporated in the regression model. Two recently proposed methods, Breiman's Garotte (BREIMAN, 1995) and Tibshirani's Lasso (TIBSHIRANI, 1996) try to combine variable selection and shrinkage. We compare these with pure variable selection and shrinkage procedures. We consider the backward elimination procedure as a typical variable selection procedure and as an example of a shrinkage procedure an approach of VAN HOUWELINGEN and LE CESSIE (1990). Additionally an extension of van Houwelingens and le Cessies approach proposed by SAUERBREI (1999) is considered. The ordinary least squares method is used as a reference. With the help of a simulation study we compare these approaches with respect to the distribution of the complexity of the selected model, the distribution of the shrinkage factors, selection bias, the bias and variance of the effect estimates and the average prediction error.	Univ Freiburg, Inst Med Biometry & Informat, Freiburg, Germany; Univ Freiburg, Ctr Data Anal & Model Bldg, Freiburg, Germany							ALTMAN DG, 1989, STAT MED, V8, P771, DOI 10.1002/sim.4780080702; BREIMAN L, 1995, TECHNOMETRICS, V37, P373, DOI 10.2307/1269730; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; SAUERBREI W, 1992, STAT MED, V11, P2093, DOI 10.1002/sim.4780111607; Harrell FE, 1996, STAT MED, V15, P361, DOI 10.1002/(SICI)1097-0258(19960229)15:4<361::AID-SIM168>3.0.CO;2-4; CHEN CH, 1985, STAT MED, V4, P39, DOI 10.1002/sim.4780040107; Cook R.D., 1982, RESIDUALS INFLUENCE; COX DR, 1972, J R STAT SOC B, V34, P187; Gruber M. H., 1990, REGRESSION ESTIMATOR; Lawson C. L., 1974, SOLVING LEAST SQUARE; McCullagh P., 1989, GEN LINEAR MODELS; Miller A. J., 1990, SUBSET SELECTION REG; PREGIBON D, 1981, ANN STAT, V9, P705, DOI 10.1214/aos/1176345513; Sauerbrei W, 1999, J ROY STAT SOC C-APP, V48, P313, DOI 10.1111/1467-9876.00155; STONE M, 1976, J KROYAL STAT SOC B, V39, P44; TERASVIRTA T, 1986, SCAND J STAT, V13, P159; VACH K, 2000, 70 FDM U FREIB CTR D; VANHOUWELINGEN JC, 1990, STAT MED, V9, P1303, DOI 10.1002/sim.4780091109; VERWEIJ PJM, 1993, STAT MED, V12, P2305, DOI 10.1002/sim.4780122407	19	6	7	1	5	BLACKWELL PUBL LTD	OXFORD	108 COWLEY RD, OXFORD OX4 1JF, OXON, ENGLAND	0039-0402			STAT NEERL	Stat. Neerl.	MAR	2001	55	1					53	75		10.1111/1467-9574.00156		23	Statistics & Probability	Mathematics	403JH	WOS:000167038400005		
J	Steyerberg, EW; Eijkemans, MJC; Habbema, JDF				Steyerberg, EW; Eijkemans, MJC; Habbema, JDF			Application of shrinkage techniques in logistic regression analysis: a case study	STATISTICA NEERLANDICA			English	Article; Proceedings Paper	Workshop on Prediction in Medical Statistics	FEB 04-05, 1999	ODENSE, DENMARK		ODENSE UNIV, DEPT STATIST & DEMOG	regression analysis; logistic models; bias; variable selection; prediction	ACUTE MYOCARDIAL-INFARCTION; MODEL SELECTION; PREDICTORS; INFERENCE; MORTALITY; TRIAL	Logistic regression analysis may well be used to develop a predictive model for a dichotomous medical outcome, such as short-term mortality. When the data set is small compared to the number of covariables studied, shrinkage techniques may improve predictions. We compared the performance of three variants of shrinkage techniques: 1) a linear shrinkage factor, which shrinks all coefficients with the same factor; 2) penalized maximum likelihood (or ridge regression), where a penalty factor is added to the likelihood function such that coefficients are shrunk individually according to the variance of each covariable; 3) the Lasso, which shrinks some coefficients to zero by setting a constraint on the sum of the absolute values of the coefficients of standardized covariables. Logistic regression models were constructed to predict 30-day mortality after acute myocardial infarction. Small data sets were created from a large randomized controlled trial, half of which provided independent validation data. We found that all three shrinkage techniques improved the calibration of predictions compared to the standard maximum likelihood estimates. This study illustrates that shrinkage is a valuable tool to overcome some of the problems of overfitting in medical data.	Erasmus Univ, Dept Publ Hlth, Ctr Clin Decis Sci, NL-3000 DR Rotterdam, Netherlands			Eijkemans, Marinus/D-8214-2013; 	Steyerberg, Ewout/0000-0002-7787-0122			BREIMAN L, 1995, TECHNOMETRICS, V37, P373, DOI 10.2307/1269730; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; HOERL AE, 1970, TECHNOMETRICS, V12, P55; LEE KL, 1995, CIRCULATION, V91, P1659; Buckland ST, 1997, BIOMETRICS, V53, P603, DOI 10.2307/2533961; Steyerberg EW, 1999, J CLIN EPIDEMIOL, V52, P935, DOI 10.1016/S0895-4356(99)00103-1; COPAS JB, 1983, J R STAT SOC B, V45, P311; Harrell FE, 1996, STAT MED, V15, P361, DOI 10.1002/(SICI)1097-0258(19960229)15:4<361::AID-SIM168>3.0.CO;2-4; LECESSIE S, 1992, APPL STAT-J ROY ST C, V41, P191; CHATFIELD C, 1995, J ROY STAT SOC A STA, V158, P419, DOI 10.2307/2983440; Efron B, 1993, MONOGRAPHS STAT APPL, V57; Ennis M, 1998, STAT MED, V17, P2501; GRAY RJ, 1992, J AM STAT ASSOC, V87, P942, DOI 10.2307/2290630; HILDEN J, 1978, METHOD INFORM MED, V17, P227; MILLER ME, 1991, STAT MED, V10, P1213, DOI 10.1002/sim.4780100805; MUELLER HS, 1992, CIRCULATION, V85, P1254; Steyerberg EW, 2000, STAT MED, V19, P1059, DOI 10.1002/(SICI)1097-0258(20000430)19:8<1059::AID-SIM412>3.3.CO;2-S; van Houwelingen JC, 2001, STAT NEERL, V55, P17, DOI 10.1111/1467-9574.00154; VANHOUWELINGEN JC, 1990, STAT MED, V9, P1303, DOI 10.1002/sim.4780091109; VERWEIJ PJM, 1994, STAT MED, V13, P2427, DOI 10.1002/sim.4780132307; Ye JM, 1998, J AM STAT ASSOC, V93, P120, DOI 10.2307/2669609	21	38	40	0	2	BLACKWELL PUBL LTD	OXFORD	108 COWLEY RD, OXFORD OX4 1JF, OXON, ENGLAND	0039-0402			STAT NEERL	Stat. Neerl.	MAR	2001	55	1					76	88		10.1111/1467-9574.00157		13	Statistics & Probability	Mathematics	403JH	WOS:000167038400006		
J	Hansen, MH; Yu, B				Hansen, MH; Yu, B			Model selection and the principle of minimum description length	JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION			English	Review						AIC; Bayesian methods; Bayes information criterion; cluster analysis; code length; coding redundancy; information theory; model selection; pointwise and minimax lower bounds; regression; time series	BAYESIAN VARIABLE SELECTION; STOCHASTIC COMPLEXITY; DISCRIMINANT-ANALYSIS; RECURSIVE ESTIMATION; DATA-COMPRESSION; REGRESSION; ORDER; INFORMATION; PREDICTION; SHRINKAGE	This article reviews the principle of minimum description length (MDL) for problems of model selection. By viewing statistical modeling as a means of generating descriptions of observed data, the MDL framework discriminates between competing models based on the complexity of each description. This approach began with Kolmogorov's theory of algorithmic complexity, matured in the literature on information theory, and has recently received renewed attention within the statistics community. Here we review both the practical and the theoretical aspects of MDL as a tool for model selection, emphasizing the rich connections between information theory and statistics. At the boundary between these two disciplines we find many interesting interpretations of popular frequentist and Bayesian procedures. As we show, MDL provides an objective umbrella under which rather disparate approaches to statistical modeling can coexist and be compared. We illustrate the MDL principle by considering problems in regression, nonparametric curve estimation, cluster analysis, and time series analysis. Because model selection in linear regression is an extremely common problem that arises in many applications, we present detailed derivations of several MDL criteria in this context and discuss their properties through a number of examples. Our emphasis is on the practical application of MDL, and hence we make extensive use of real datasets. in writing this review, we tried to make the descriptive philosophy of MDL natural to a statistics audience by examining classical problems in model selection. In the engineering literature, however, MDL is being applied to ever more exotic modeling situations. As a principle for statistical modeling in general, one strength of MDL is that it can be intuitively extended to provide useful toots for new problems.	Univ Calif Berkeley, Berkeley, CA 94720 USA							AKAIKE H, 1977, ANN I STAT MATH, V29, P9, DOI 10.1007/BF02532770; AKAIKE H, 1974, IEEE T AUTOMAT CONTR, VAC19, P716, DOI 10.1109/TAC.1974.1100705; AN HZ, 1985, ACTA MATH APPL SIN-E, V2, P27; SMITH AFM, 1980, J ROY STAT SOC B MET, V42, P213; KASS RE, 1995, J AM STAT ASSOC, V90, P773, DOI 10.1080/01621459.1995.10476572; DAVISSON LD, 1980, IEEE T INFORM THEORY, V26, P166, DOI 10.1109/TIT.1980.1056167; HASTIE T, 1994, J AM STAT ASSOC, V89, P1255, DOI 10.2307/2290989; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; FURNIVAL GM, 1974, TECHNOMETRICS, V16, P499, DOI 10.2307/1267601; HANNAN EJ, 1982, BIOMETRIKA, V69, P81, DOI 10.1093/biomet/69.1.81; Raftery AE, 1997, J AM STAT ASSOC, V92, P179, DOI 10.2307/2291462; SHIBATA R, 1981, BIOMETRIKA, V68, P45; CARLIN BP, 1992, J AM STAT ASSOC, V87, P493, DOI 10.2307/2290282; LINDLEY DV, 1956, ANN MATH STAT, V27, P986, DOI 10.1214/aoms/1177728069; MERHAV N, 1995, IEEE T INFORM THEORY, V41, P714, DOI 10.1109/18.382017; ELIAS P, 1975, IEEE T INFORM THEORY, V21, P194, DOI 10.1109/TIT.1975.1055349; HURVICH CM, 1989, BIOMETRIKA, V76, P297, DOI 10.1093/biomet/76.2.297; RISSANEN J, 1978, AUTOMATICA, V14, P465, DOI 10.1016/0005-1098(78)90005-5; GEORGE EI, 1993, J AM STAT ASSOC, V88, P881, DOI 10.2307/2290777; SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136; Berger JO, 1996, J AM STAT ASSOC, V91, P109, DOI 10.2307/2291387; MALLOWS CL, 1973, TECHNOMETRICS, V15, P661, DOI 10.2307/1267380; Barron A, 1998, IEEE T INFORM THEORY, V44, P2743, DOI 10.1109/18.720554; BAXTER R, 1995, UNPUB MDL MML SIMI 3; BERNARDO JM, 1994, BAYESIAN TEHORY; Brockwell P. J., 1991, TIME SERIES THEORY M; Broman K. W., 1997, THESIS U CALIFORNIA; HASTIE T, 1995, ANN STAT, V23, P73, DOI 10.1214/aos/1176324456; CLARKE BS, 1990, IEEE T INFORM THEORY, V36, P453, DOI 10.1109/18.54897; Clyde M, 1998, BIOMETRIKA, V85, P391, DOI 10.1093/biomet/85.2.391; Clyde M, 1996, J AM STAT ASSOC, V91, P1197, DOI 10.2307/2291738; Cover T. M., 1991, ELEMENTS INFORMATION; Cowen NM, 1989, DEV APPL MOL MARKERS, P113; CSISZAR I, 1990, INFORMATION THEORETI; DAVISSON LD, 1973, IEEE T INFORM THEORY, V19, P783, DOI 10.1109/TIT.1973.1055092; DAWID AP, 1984, J ROYAL STATISTICA A, V147, P178; DAWID AP, 1991, 4 VAL INT M BAYES ST; DOERGE RW, 1996, GENETICS, V134, P585; Edwards AWF, 1972, LIKELIHOOD; FINDLEY DF, 1991, ANN I STAT MATH, V43, P505, DOI 10.1007/BF00053369; FINDLEY DF, 1999, ENCY STAT SCI UPDATE, V3; FURBY S, 1990, P 5 AUSTR REM SENS C, P175; GALLAGER RG, 1976, UNPUB SOURCE CODING; GEORGE E, 1999, IN PRESS BIOMETRIKA; George EI, 1997, STAT SINICA, V7, P339; GERENCSER L, 1994, J STAT PLAN INFER, V41, P303, DOI 10.1016/0378-3758(94)90026-4; GERENCSER L, 1987, ORDER ESTIMATION STA; HANNAN EJ, 1979, J ROY STAT SOC B MET, V41, P190; HANNAN EJ, 1989, J ROY STAT SOC B MET, V51, P217; HANNAN EJ, 1984, BIOMETRIKA, V71, P273; HANSEN M, 1999, UNPUB STAT SCI; HANSEN M, 1999, P IT WORKSH DET EST; Haussler D, 1997, ANN STAT, V25, P2451; HAUSSLER D, 1994, MACH LEARN, V14, P83, DOI 10.1007/BF00993163; Haussler D, 1997, IEEE T INFORM THEORY, V43, P1276, DOI 10.1109/18.605594; HEMERLY EM, 1989, ANN STAT, V17, P941, DOI 10.1214/aos/1176347154; Hertz J., 1991, INTRO THEORY NEURAL; HUANG D, 1990, J TIME SER ANAL, V11, P107, DOI 10.1111/j.1467-9892.1990.tb00045.x; Hurvich CM, 1998, J ROY STAT SOC B, V60, P271, DOI 10.1111/1467-9868.00125; IBRAGIMOV IA, 1973, P 2 INT S INF THEOR; Jobson J. D., 1992, APPL MULTIVARIATE DA, VII; KOLMOGOR.AN, 1968, IEEE T INFORM THEORY, V14, P662, DOI 10.1109/TIT.1968.1054210; Kolmogorov A.N., 1965, Problems of Information Transmission, V1; Lai TL, 1997, STAT SINICA, V7, P285; Le Cam L., 1986, ASYMPTOTIC METHODS S; LECLERC YG, 1989, INT J COMPUT VISION, V3, P73, DOI 10.1007/BF00054839; LI M, 1996, INTRO KOLMOGOROV COM; LONG AD, 1995, GENETICS, V139, P1273; Luo Z, 1997, J AM STAT ASSOC, V92, P107, DOI 10.2307/2291454; MALLOWS CL, 1995, TECHNOMETRICS, V37, P362, DOI 10.2307/1269729; MERHAV N, 1989, IEEE T INFORM THEORY, V35, P1014, DOI 10.1109/18.42210; MERHAV N, 1989, IEEE T INFORM THEORY, V35, P1109, DOI 10.1109/18.42231; Moulin P, 1996, PROCEEDINGS OF THE IEEE-SP INTERNATIONAL SYMPOSIUM ON TIME-FREQUENCY AND TIME-SCALE ANALYSIS, P141, DOI 10.1109/TFSA.1996.546706; O'Hagan A., 1994, KENDALLS ADV THEORY, V2nd; OHAGAN A, 1995, J ROY STAT SOC B MET, V57, P99; PAN HP, 1994, INT ARCH PHOTOGRAMME, V30, P648; PETERSON JJ, 1986, STAT PROBABIL LETT, V4, P227, DOI 10.1016/0167-7152(86)90093-3; Qian GQ, 1996, BIOMETRIKA, V83, P41, DOI 10.1093/biomet/83.1.41; Rissanen J., 1996, IEEE T INFORMATION T, V42, P48; RISSANEN J, 1992, IEEE T INFORM THEORY, V38, P315, DOI 10.1109/18.119689; RISSANEN J, 1986, ANN STAT, V14, P1080, DOI 10.1214/aos/1176350051; RISSANEN J, 1987, J ROY STAT SOC B MET, V49, P223; RISSANEN J, 1983, ANN STAT, V11, P416, DOI 10.1214/aos/1176346150; Rissanen J, 1986, IMA J MATH CONTROL I, V3, P211, DOI 10.1093/imamci/3.2-3.211; Rissanen Jorma, 1989, STOCHASTIC COMPLEXIT; Saito N., 1994, WAVELETS GEOPHYSICS, P299; SAKAMOTO Y, 1985, AKAIKE INFORMATION S; Schumaker L.L., 1993, SPLINE FUNCTIONS BAS; SCLOVE SL, 1972, ANN MATH STAT, V43, P1481, DOI 10.1214/aoms/1177692380; SCLOVE SL, 1968, J AM STAT ASSOC, V63, P596, DOI 10.2307/2284030; Shtarkov YM, 1987, PROBL INFORM TRANSM, V23, P3; Smith M, 1996, J ECONOMETRICS, V75, P317, DOI 10.1016/0304-4076(95)01763-1; SMITH MS, 1996, THESIS U NEW S WALES; SPEED TP, 1994, ANN I STAT MATH, V45, P35; *SSDC, SOC SCI DAT COLL U C; STINE RA, 1999, UNPUB COMPETITIVE CO; SUGIURA N, 1978, COMMUN STAT A-THEOR, V7, P13, DOI 10.1080/03610927808827599; Wahba G., 1990, SPLINE MODELS OBSERV; Wallace C. S., 1994, Proceedings of the 7th Australian Joint Conference on Artificial Intelligence. Artificial Intelligence. AI'94. Sowing the Seeds for the Future; WALLACE CS, 1968, COMPUT J, V11, P185; WALLACE CS, 1987, J ROY STAT SOC B MET, V49, P240; WEI CZ, 1992, ANN STAT, V36, P581; WONG F, 1997, FOCUSED SAMPLING ITS; YU B, 1992, PROBAB THEORY REL, V92, P195, DOI 10.1007/BF01194921; Zellner A, 1986, BAYESIAN INFERENCE D, P233	105	247	259	5	16	AMER STATISTICAL ASSOC	ALEXANDRIA	1429 DUKE ST, ALEXANDRIA, VA 22314 USA	0162-1459			J AM STAT ASSOC	J. Am. Stat. Assoc.	JUN	2001	96	454					746	774		10.1198/016214501753168398		29	Statistics & Probability	Mathematics	437HN	WOS:000168986400037		
J	Edler, L; Grassmann, J; Suhai, S				Edler, L; Grassmann, J; Suhai, S			Role and results of statistical methods in protein fold class prediction	MATHEMATICAL AND COMPUTER MODELLING			English	Article						protein fold classes; regression; discrimination	DISCRIMINANT-ANALYSIS; NEURAL NETWORKS; CLASSIFICATION; REGRESSION	Statistical methods of discrimination and classification are used for the prediction of protein structure from amino acid sequence data. This provides information for the establishment of new paradigms of carcinogenesis modeling on the basis of gene expression. Feed forward neural networks and standard statistical classification procedures are used to classify proteins into fold classes. Logistic regression, additive models, and projection pursuit regression from the family of methods based on a posterior probabilities; linear, quadratic, and a flexible discriminant analysis from the class of methods based on class conditional probabilities, and the nearest-neighbors classification rule are applied to a data set of 268 sequences. From analyzing the prediction error obtained with a test sample (n = 125) and with a cross validation procedure, we conclude that the standard linear discriminant analysis and nearest-neighbor methods are at the same time statistically feasible and potent competitors to the more flexible tools of feed forward neural networks. Further research is needed to explore the gain obtainable from statistical methods by the application to larger sets of protein sequence data, and to compare the results with those from biophysical approaches. (C) 2001 Elsevier Science Ltd. All rights reserved.	German Canc Res Ctr, Biostat Unit R0700, D-69120 Heidelberg, Germany; German Canc Res Ctr, Dept Mol Biophys, D-69120 Heidelberg, Germany	Edler, L (reprint author), German Canc Res Ctr, Biostat Unit R0700, POB 10 19 49, D-69120 Heidelberg, Germany.						FRIEDMAN JH, 1981, J AM STAT ASSOC, V76, P817, DOI 10.2307/2287576; HASTIE T, 1994, J AM STAT ASSOC, V89, P1255, DOI 10.2307/2290989; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; ANFINSEN CB, 1961, P NATL ACAD SCI USA, V47, P1309, DOI 10.1073/pnas.47.9.1309; KULLBACK S, 1951, ANN MATH STAT, V22, P79, DOI 10.1214/aoms/1177729694; HASTIE T, 1995, ANN STAT, V23, P73, DOI 10.1214/aos/1176324456; EDLER L, 1999, IMS LECT NOTES MONOG, V33, P288; Efron B, 1993, INTRO BOOTSTRAP; Finkelstein AV, 1997, CURR OPIN STRUC BIOL, V7, P60, DOI 10.1016/S0959-440X(97)80008-5; FRIEDMAN JH, 1989, J AM STAT ASSOC, V84, P165, DOI 10.2307/2289860; GRASSMANN J, 1996, ADV STAT SOFTWARE, V51, P399; GRASSMANN J, 1966, COMPSTAT, P277; Grassmann J, 1999, Proc Int Conf Intell Syst Mol Biol, P106; HASTIE T, 1989, TECHNOMETRICS, V31, P3; Hastie T, 1996, IEEE T PATTERN ANAL, V18, P607, DOI 10.1109/34.506411; Hastie TJ, 1990, GEN ADDITIVE MODELS; KARLIN S, 1992, SCIENCE, V257, P39, DOI 10.1126/science.1621093; Lesk A. M., 1991, PROTEIN ARCHITECTURE; Neumaier A, 1997, SIAM REV, V39, P407, DOI 10.1137/S0036144594278060; PASCARELLA S, 1992, PROTEIN ENG, V5, P121, DOI 10.1093/protein/5.2.121; RECZKO M, 1994, NUCLEIC ACIDS RES, V22, P3616; RECZKO M, 1994, PROTEIN STRUCTURE BY DISTANCE ANALYSIS, P277; Ripley B. D., 1996, PATTERN RECOGNITION; RIPLEY BD, 1994, J ROY STAT SOC B MET, V56, P409; RITTGEN W, 1997, ADAM HDB PROGRAM PAC; Rost B, 1997, COMPUT APPL BIOSCI, V13, P345; ROST B, 1994, PROTEINS, V19, P55, DOI 10.1002/prot.340190108; Schumacher M, 1996, COMPUT STAT DATA AN, V21, P661, DOI 10.1016/0167-9473(95)00032-1	28	15	15	0	2	PERGAMON-ELSEVIER SCIENCE LTD	OXFORD	THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND	0895-7177			MATH COMPUT MODEL	Math. Comput. Model.	JUN	2001	33	12-13					1401	1417		10.1016/S0895-7177(01)80022-4		17	Computer Science, Interdisciplinary Applications; Computer Science, Software Engineering; Mathematics, Applied	Computer Science; Mathematics	431DE	WOS:000168615900017		
J	Ojelund, H; Madsen, H; Thyregod, P				Ojelund, H; Madsen, H; Thyregod, P			Calibration with absolute shrinkage	JOURNAL OF CHEMOMETRICS			English	Article						lasso; variable selection; wavelength selection; NIR spectroscopy; linear regression	WAVELENGTH SELECTION; GENETIC ALGORITHMS; REGRESSION; CHEMOMETRICS; LASSO	In this paper, penalized regression using the L-1 norm on the estimated parameters is proposed for chemometric je calibration. The algorithm is of the lasso type, introduced by Tibshirani in 1996 as a linear regression method with bound on the absolute length of the parameters, but a modification is suggested to cope with the singular design matrix most often seen in chemometric calibration. Furthermore, the proposed algorithm may be generalized to all convex norms like Sigma/beta (j)/(gamma) where gamma greater than or equal to 1, i.e. a method that continuously varies from ridge regression to the lasso. The lasso is applied both directly as a calibration method and as a method to select important variables/wave lengths. It is demonstrated that the lasso algorithm, in general, leads to parameter estimates of which some are zero while others are quite large (compared to e.g. the traditional PLS or RR estimates). By using several benchmark data sets, it is shown that both the direct lasso method and the regression where the lasso acts as a wavelength selection method most often outperform the PLS and RR methods. Copyright (C) 2001 John Wiley & Sons, Ltd.	Tech Univ Denmark, Dept Math Modeling, DK-2800 Lyngby, Denmark	Ojelund, H (reprint author), Tech Univ Denmark, Dept Math Modeling, DK-2800 Lyngby, Denmark.						Fu WJJ, 1998, J COMPUT GRAPH STAT, V7, P397, DOI 10.2307/1390712; WILLIAMS PM, 1995, NEURAL COMPUT, V7, P117, DOI 10.1162/neco.1995.7.1.117; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; FRANK IE, 1993, TECHNOMETRICS, V35, P109, DOI 10.2307/1269656; SHAO J, 1993, J AM STAT ASSOC, V88, P486, DOI 10.2307/2290328; Kalivas JH, 1997, CHEMOMETR INTELL LAB, V37, P255, DOI 10.1016/S0169-7439(97)00038-5; BREIMAN L, 1992, INT STAT REV, V60, P291, DOI 10.2307/1403680; Brown P. J., 1993, MEASUREMENT REGRESSI; Brown PJ, 1998, J CHEMOMETR, V12, P173, DOI 10.1002/(SICI)1099-128X(199805/06)12:3<173::AID-CEM505>3.3.CO;2-S; HORCHNER U, 1995, J CHEMOMETR, V9, P283, DOI 10.1002/cem.1180090404; JOUANRIMBAUD D, 1995, ANAL CHEM, V67, P4295, DOI 10.1021/ac00119a015; LEARDI R, 1992, J CHEMOMETR, V6, P267, DOI 10.1002/cem.1180060506; OSBORNE MR, IN PRESS J COMPUT GR; OWEN T, 1996, 1259655123E HEWL PAC; Wold S., 1983, MATRIX PENCILS, P286; Wold S, 1995, CHEMOMETR INTELL LAB, V30, P109, DOI 10.1016/0169-7439(95)00042-9	16	16	16	1	2	JOHN WILEY & SONS LTD	W SUSSEX	BAFFINS LANE CHICHESTER, W SUSSEX PO19 1UD, ENGLAND	0886-9383			J CHEMOMETR	J. Chemometr.	JUL	2001	15	6					497	509				13	Automation & Control Systems; Chemistry, Analytical; Computer Science, Artificial Intelligence; Instruments & Instrumentation; Mathematics, Interdisciplinary Applications; Statistics & Probability	Automation & Control Systems; Chemistry; Computer Science; Instruments & Instrumentation; Mathematics	456WF	WOS:000170105200001		
J	Coull, BA; Hobert, JP; Ryan, LM; Holmes, LB				Coull, BA; Hobert, JP; Ryan, LM; Holmes, LB			Crossed random effect models for multiple outcomes in a study of teratogenesis	JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION			English	Article; Proceedings Paper	160th Annual Meeting of the American-Statistical-Association	FEB, 2000	BOSTON, MASSACHUSETTS	Amer Statist Assoc		generalized linear mixed model; lasso; logistic regression; Markov chain Monte Carlo; Monte Carlo EM algorithm; Monte Carlo Newton-Raphson	LINEAR MIXED MODELS; LATENT VARIABLE MODELS; MONTE-CARLO EM; BINARY OUTCOMES; ANTICONVULSANT TERATOGENESIS; CLINICAL-TRIALS; BIRTH-DEFECTS; REGRESSION; ALGORITHM; DISCRETE	Human teratogens often manifest themselves through a broad spectrum of adverse effects. Although often not serious when considered individually, such outcomes taken together may represent a syndrome that can lead to serious developmental problems. Accordingly, studies that investigate the effect of human teratogens on fetal development typically record the presence or absence of a multitude of abnormalities, resulting in the data of multivariate binary form for each infant. Such studies typically have three objectives: (1) estimate an overall effect of exposure across outcomes, (2) identify subjects having the syndrome, and (3) identify those outcomes that constitute the syndrome so that doctors know what to look for when diagnosing the syndrome in other exposed newborns. This article proposes the use of a logistic regression model with crossed random effect structure to address all three questions simultaneously. We use the proposed models to analyze data from a study investigating the effects of in utero antiepileptic drug exposure on fetal development.	Harvard Univ, Sch Publ Hlth, Dept Biostat, Boston, MA 02115 USA; Univ Florida, Dept Stat, Gainesville, FL 32611 USA; Dana Farber Canc Inst, Boston, MA 02115 USA; Massachusetts Gen Hosp, Genet & Teratol Unit, Serv Pediat, Boston, MA 02115 USA; Harvard Univ, Sch Med, Boston, MA 02115 USA	Coull, BA (reprint author), Harvard Univ, Sch Publ Hlth, Dept Biostat, 665 Huntington Ave, Boston, MA 02115 USA.		Ryan, Louise/A-4562-2009	Ryan, Louise/0000-0001-5957-2490			Agresti A, 2000, STAT MED, V19, P1115, DOI 10.1002/(SICI)1097-0258(20000430)19:8<1115::AID-SIM408>3.3.CO;2-O; Andersen EB, 1980, DISCRETE STAT MODELS; BRESLOW NE, 1993, J AM STAT ASSOC, V88, P9, DOI 10.2307/2290687; Bandeen-Roche K, 1997, J AM STAT ASSOC, V92, P1375, DOI 10.2307/2965407; LIANG KY, 1986, BIOMETRIKA, V73, P13, DOI 10.1093/biomet/73.1.13; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; RIPLEY BD, 1977, J ROY STAT SOC B MET, V39, P172; BRESLOW NE, 1995, BIOMETRIKA, V82, P81; Booth JG, 1999, J ROY STAT SOC B, V61, P265, DOI 10.1111/1467-9868.00176; Sammel MD, 1997, J ROY STAT SOC B MET, V59, P667, DOI 10.1111/1467-9868.00090; McCulloch CE, 1997, J AM STAT ASSOC, V92, P162, DOI 10.2307/2291460; BLACKWELL B, 1999, RANDOM EFFECTS LATEN; CHANG CK, 1994, COMPUTING SCIENCE AND STATISTICS, VOL 26, P182; Cohen MM, 1982, CHILD MULTIPLE BIRTH; DONNER A, 1988, J CLIN EPIDEMIOL, V41, P899, DOI 10.1016/0895-4356(88)90107-2; Fischer G. H., 1995, RASCH MODELS FDN REC; Gelman A., 1995, BAYESIAN DATA ANAL; GREEN PJ, 1987, INT STAT REV, V55, P245, DOI 10.2307/1403404; Greenland S, 2000, BIOMETRICS, V56, P915, DOI 10.1111/j.0006-341X.2000.00915.x; Hartzel J, 2001, COMPUT STAT DATA AN, V35, P429, DOI 10.1016/S0167-9473(00)00020-7; Heinen T., 1996, LATENT CLASS DISCRET; HOLMES LB, 1987, TERATOLOGY, V36, P291, DOI 10.1002/tera.1420360304; HOLMES LB, 1994, TERATOLOGY, V49, P202, DOI 10.1002/tera.1420490316; KHOURY MJ, 1987, TERATOLOGY, V36, P345, DOI 10.1002/tera.1420360311; KHOURY MJ, 1991, AM J MED GENET, V40, P500, DOI 10.1002/ajmg.1320400426; LEFKOPOULOU M, 1989, J AM STAT ASSOC, V84, P810, DOI 10.2307/2289671; LEFKOPOULOU M, 1993, BIOMETRICS, V49, P975, DOI 10.2307/2532240; LEGLER JM, 1995, J AM STAT ASSOC, V90, P680; Legler JM, 1997, J AM STAT ASSOC, V92, P13, DOI 10.2307/2291445; LEGLER JM, 1994, TERATOLOGY, V50, P74, DOI 10.1002/tera.1420500110; LIU Q, 1993, BIOMETRIKA, V80, P543, DOI 10.1093/biomet/80.3.543; LOUIS TA, 1982, J ROY STAT SOC B MET, V44, P226; McCullagh P., 1989, GEN LINEAR MODELS; MENG CYK, 1987, BIOMETRICS, V43, P301, DOI 10.2307/2531814; Natarajan R, 1999, BIOMETRICS, V55, P553, DOI 10.1111/j.0006-341X.1999.00553.x; Natarajan R, 2000, J AM STAT ASSOC, V95, P227, DOI 10.2307/2669540; NEUHAUS JM, 1997, LECT NOTES STAT, V122; Pocock SJ, 1997, CONTROL CLIN TRIALS, V18, P530, DOI 10.1016/S0197-2456(97)00008-1; Quintana FA, 1999, COMPUT STAT DATA AN, V29, P429, DOI 10.1016/S0167-9473(98)00075-9; RAI SN, 1993, BIOMETRICS, V49, P587, DOI 10.2307/2532570; ROM DM, 1992, STAT MED, V11, P511, DOI 10.1002/sim.4780110413; Rothman KJ, 1998, MODERN EPIDEMIOLOGY; Sammel M, 1999, STAT MED, V18, P2479, DOI 10.1002/(SICI)1097-0258(19990915/30)18:17/18<2479::AID-SIM270>3.3.CO;2-6; Sammel MD, 1996, BIOMETRICS, V52, P650, DOI 10.2307/2532903; Tanner M. A., 1996, TOOLS STAT INFERENCE; Thall PF, 1998, STAT MED, V17, P1563, DOI 10.1002/(SICI)1097-0258(19980730)17:14<1563::AID-SIM873>3.3.CO;2-C; WILLIAMS PL, 1996, J AGR BIOL ENVIR ST, V1, P250, DOI 10.2307/1400368; ZASLAVSKY AM, 1992, ASA P SECTION SURVEY, P279	48	13	13	2	7	AMER STATISTICAL ASSOC	ALEXANDRIA	1429 DUKE ST, ALEXANDRIA, VA 22314 USA	0162-1459			J AM STAT ASSOC	J. Am. Stat. Assoc.	DEC	2001	96	456					1194	1204		10.1198/016214501753381841		11	Statistics & Probability	Mathematics	502DG	WOS:000172728000006		
J	Fan, JQ; Li, RZ				Fan, JQ; Li, RZ			Variable selection via nonconcave penalized likelihood and its oracle properties	JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION			English	Article; Proceedings Paper	160th Annual Meeting of the American-Statistical-Association	FEB, 2000	BOSTON, MASSACHUSETTS	Amer Statist Assoc		hard thresholding; LASSO; nonnegative garrote; penalized likelihood; oracle estimator; SCAD; soft thresholding	REGRESSION; SHRINKAGE; MODEL; LASSO; RISK	Variable selection is fundamental to high-dimensional statistical modeling, including nonparametric regression. Many approaches in use are stepwise selection procedures, which can be computationally expensive and ignore stochastic errors in the variable selection process. In this article, penalized likelihood approaches are proposed to handle these kinds of problems. The proposed methods select variables and estimate coefficients simultaneously. Hence they enable us to construct confidence intervals for estimated parameters. The proposed approaches are distinguished from others in that the penalty functions are symmetric, nonconcave on (0, infinity), and have singularities at the origin to produce sparse solutions. Furthermore, the penalty functions should be bounded by a constant to reduce bias and satisfy certain conditions to yield continuous solutions. A new algorithm is proposed for optimizing penalized likelihood functions. The proposed ideas are widely applicable. They are readily applied to a variety of parametric models such as generalized linear models and robust regression models. They can also be applied easily to nonparametric modeling by using wavelets and splines. Rates of convergence of the proposed penalized likelihood estimators are established. Furthermore, with proper choice of regularization parameters, we show that the proposed estimators perform as well as the oracle procedure in variable selection; namely, they work as well as if the correct submodel were known. Our simulation shows that the newly proposed methods compare favorably with other variable selection techniques. Furthermore. the standard error formulas are tested to be accurate enough for practical applications.	Chinese Univ Hong Kong, Dept Stat, Sha Tin 100083, Peoples R China; Univ Calif Los Angeles, Dept Stat, Los Angeles, CA 90095 USA; Penn State Univ, Dept Stat, University Pk, PA 16802 USA	Fan, JQ (reprint author), Chinese Univ Hong Kong, Dept Stat, Sha Tin 100083, Peoples R China.		Li, Runze/C-5444-2013	Li, Runze/0000-0002-0154-2202			BREIMAN L, 1995, TECHNOMETRICS, V37, P373, DOI 10.2307/1269730; Fu WJJ, 1998, J COMPUT GRAPH STAT, V7, P397, DOI 10.2307/1390712; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Antoniadis A, 2001, J AM STAT ASSOC, V96, P939, DOI 10.1198/016214501753208942; FRANK IE, 1993, TECHNOMETRICS, V35, P109, DOI 10.2307/1269656; DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425; CRAVEN P, 1979, NUMER MATH, V31, P377; Antoniadis A., 1997, J ITAL STAT SOC, V6, P97, DOI DOI 10.1007/BF03178905; BICKEL PJ, 1975, J AM STAT ASSOC, V70, P428, DOI 10.2307/2285834; Breiman L, 1996, ANN STAT, V24, P2350; DONOHO DL, 1994, PROBAB THEORY REL, V99, P277, DOI 10.1007/BF01199026; Fan J., 1997, J ITALIAN STAT ASS, V6, P131; Gao HY, 1997, STAT SINICA, V7, P855; Green P, 1994, NONPARAMETRIC REGRES; Huber P., 1981, ROBUST ESTIMATION; Lehmann E. L., 1983, THEORY POINT ESTIMAT; LI R, 2000, THESIS U N CAROLINA; Marron JS, 1998, J COMPUT GRAPH STAT, V7, P278; McCullagh P., 1989, GEN LINEAR MODELS; ROBINSON PM, 1988, ECONOMETRICA, V56, P531, DOI 10.2307/1911699; Stone CJ, 1997, ANN STAT, V25, P1371; Tibshirani R, 1997, STAT MED, V16, P385, DOI 10.1002/(SICI)1097-0258(19970228)16:4<385::AID-SIM380>3.0.CO;2-3; Wahba G., 1990, SPLINE MODELS OBSERV	23	1361	1443	16	55	AMER STATISTICAL ASSOC	ALEXANDRIA	1429 DUKE ST, ALEXANDRIA, VA 22314 USA	0162-1459			J AM STAT ASSOC	J. Am. Stat. Assoc.	DEC	2001	96	456					1348	1360		10.1198/016214501753382273		13	Statistics & Probability	Mathematics	502DG	WOS:000172728000028		
S	Figueiredo, MAT		Dietterich, TG; Becker, S; Ghahramani, Z		Figueiredo, MAT			Adaptive sparseness using Jeffreys Prior	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				GAUSSIAN-PROCESSES; APPROXIMATION; SELECTION	In this paper we introduce a new sparseness inducing prior which does not involve any (hyper)parameters that need to be adjusted or estimated. Although other applications are possible, we focus here on supervised learning problems: regression and classification. Experiments with several publicly available benchmark data sets show that the proposed approach yields state-of-the-art performance. In particular, our method outperforms support vector machines and performs competitively with the best alternative techniques, both in terms of error rates and sparseness, although it involves no tuning or adjusting of sparseness-controlling hyper-parameters.	Univ Tecn Lisboa, Inst Telecommun, Inst Super Tecn, P-1049001 Lisbon, Portugal	Figueiredo, MAT (reprint author), Univ Tecn Lisboa, Inst Telecommun, Inst Super Tecn, P-1049001 Lisbon, Portugal.	mtf@lx.it.pt	Figueiredo, Mario/C-5428-2008	Figueiredo, Mario/0000-0002-0970-7745			ALBERT JH, 1993, J AM STAT ASSOC, V88, P669, DOI 10.2307/2290350; WILLIAMS PM, 1995, NEURAL COMPUT, V7, P117, DOI 10.1162/neco.1995.7.1.117; Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010; POGGIO T, 1990, P IEEE, V78, P1481, DOI 10.1109/5.58326; Williams CKI, 1998, IEEE T PATTERN ANAL, V20, P1342, DOI 10.1109/34.735807; Osborne MR, 2000, IMA J NUMER ANAL, V20, P389, DOI 10.1093/imanum/20.3.389; DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425; Berger J. O., 1980, STAT DECISION THEORY; Bishop C., 2000, P 16 C UNC ART INT, P46; Bishop C. M., 1995, NEURAL NETWORKS PATT; Cherkassky V. S., 1998, LEARNING DATA CONCEP; Cristianini N., 2000, SUPPORT VECTOR MACHI, V1st; Figueiredo MAT, 2001, IEEE T IMAGE PROCESS, V10, P1322, DOI 10.1109/83.941856; HOERL AE, 1970, TECHNOMETRICS, V12, P55, DOI DOI 10.1080/00401706.1970.10488634; KIMELDOR.GS, 1970, ANN MATH STAT, V41, P495, DOI 10.1214/aoms/1177697089; Lange K. L., 1993, J COMPUT GRAPH STAT, V2, P175, DOI 10.2307/1390698; MacKay DJC, 1996, FUND THEOR, V62, P221; McCullagh P., 1989, GENERALIZED LINEAR M, VSecond; Neal R. M., 1996, BAYESIAN LEARNING NE; Poggio T, 1998, NEURAL COMPUT, V10, P1445, DOI 10.1162/089976698300017250; Ripley B. D., 1996, PATTERN RECOGNITION; Seeger M, 2000, ADV NEUR IN, V12, P603; Tibshirani R, 1996, J ROYAL STAT SOC B, V58; Tipping ME, 2000, ADV NEUR IN, V12, P652; Tresp V., 2001, NIPS, V13; Vapnik V., 1998, STAT LEARNING THEORY; Williams CKI, 1998, LEARNING INFERENCE G	27	12	12	0	1	MIT PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						697	704				8	Computer Science, Artificial Intelligence	Computer Science	BV95T	WOS:000180520100087		
S	Fickel, N		Gaul, W; Ritter, G		Fickel, N			Regression analysis of extremely multicollinear data	CLASSIFICATION, AUTOMATION, AND NEW MEDIA	STUDIES IN CLASSIFICATION, DATA ANALYSIS, AND KNOWLEDGE ORGANIZATION		English	Proceedings Paper	24th Annual Conference of the German-Classification-Society	MAR 15-17, 2000	PASSAU, GERMANY	German Classificat Soc	UNIV PASSAU			Regression analysis tries to measure the influences of independent variables on a dependent variable. This can be achieved by partial coefficients if there is not too much multicollinearity. A new method provides alternative coefficients which can be interpreted for every degree of multicollinearity.	Univ Erlangen Nurnberg, Wirtschafts & Sozialwissenschaftliche Fak, D-90403 Nurnberg, Germany	Fickel, N (reprint author), Univ Erlangen Nurnberg, Wirtschafts & Sozialwissenschaftliche Fak, D-90403 Nurnberg, Germany.						BREIMAN L, 1995, TECHNOMETRICS, V37, P373, DOI 10.2307/1269730; CHEVAN A, 1991, AM STAT, V45, P90, DOI 10.2307/2684366; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; BRING J, 1994, AM STAT, V48, P209, DOI 10.2307/2684719; HOERL AE, 1970, TECHNOMETRICS, V12, P55; Bowerman BL, 1993, FORECASTING TIME SER; Darlington RB, 1990, REGRESSION LINEAR MO; Draper N. R., 1998, APPL REGRESSION ANAL; EZEKIEL, 1959, METHODS CORRELATION; FICKEL N, 1999, THESIS U ERLANGEN NE; Fickel N, 2000, OPERATIONS RESEARCH PROCEEDINGS 1999, P154; Hawkins D. M., 1973, Applied Statistics, V22, DOI 10.2307/2346776; KRUSKAL W, 1987, AM STAT, V41, P6, DOI 10.2307/2684310; Miller A. J., 1990, SUBSET SELECTION REG; MORROWHOWELL N, 1994, SOC WORK RES, V18, P247; SEN A, 1994, REGRESSION ANAL THEO; Tukey J. W., 1977, DATA ANAL REGRESSION; WOLD H, 1966, RES PAPERS STAT, P411	18	1	1	0	0	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	1431-8814		3-540-43233-7	ST CLASS DAT ANAL			2002							67	74				8	Computer Science, Artificial Intelligence; Mathematics, Interdisciplinary Applications; Social Sciences, Mathematical Methods	Computer Science; Mathematics; Mathematical Methods In Social Sciences	BU58A	WOS:000176420600007		
B	Hastie, T; Tibshirani, R; Narasimhan, B; Chu, G		Hardle, W; Ronz, B		Hastie, T; Tibshirani, R; Narasimhan, B; Chu, G			Supervised learning from microarray data	COMPSTAT 2002: PROCEEDINGS IN COMPUTATIONAL STATISTICS			English	Proceedings Paper	15th Biannual Conference on Computational Statistics (COMPSTAT)	AUG 24-28, 2002	BERLIN, GERMANY	Humboldt Univ Berlin, Ctr Appl Stat & Econ, Frie Univ Berlin, Univ Potsdam		microarrays; expression; classification; discriminant analysis	SHRINKAGE	Gene expression arrays pose challenging problems for most traditional supervised learning techniques. We present a discussion of some of the issues involved. We then propose a simple approach to class prediction for DNA microarrays, based on a enhancement of the nearest centroid classifier. Our technique uses soft-thresholded class centroids as prototypes for each class. The shrinkage improves significantly prediction performance, and identifies a subset of the genes most responsible for class separation. The method performs as well or better than competitors from the literature, and is easy to understand and interpret. We illustrate the technique on data from three studies: small round blue cell tumors, leukemia and breast cancer.	Stanford Univ, Dept Stat, Stanford, CA 94305 USA	Hastie, T (reprint author), Stanford Univ, Dept Stat, Stanford, CA 94305 USA.						Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Khan J, 2001, NAT MED, V7, P673, DOI 10.1038/89044; Tusher VG, 2001, P NATL ACAD SCI USA, V98, P5116, DOI 10.1073/pnas.091062498; Eisen MB, 1998, P NATL ACAD SCI USA, V95, P14863, DOI 10.1073/pnas.95.25.14863; DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425; Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.1023/A:1018054314350; FRIEDMAN JH, 1989, J AM STAT ASSOC, V84, P165, DOI 10.2307/2289860; Hastie T., 2001, ELEMENTS STAT LEARNI; TIBSHIRANI R, 2002, P NAT AC SCI	9	0	0	1	3	PHYSICA-VERLAG GMBH & CO	HEIDELBERG	TIERGARTENSTR 17, D-69121 HEIDELBERG, GERMANY			3-7908-1517-9				2002							67	77				11	Statistics & Probability	Mathematics	BV74V	WOS:000179942900007		
J	He, XM; Kim, MO				He, XM; Kim, MO			On marginal estimation in a semiparametric model for longitudinal data with time-independent covariates	METRIKA			English	Article; Proceedings Paper	International Conference on Robust Statistics	JUL 23-27, 2001	VORAU, AUSTRIA			efficiency; longitudinal data; M-estimator; semiparametric models	PARTLY LINEAR-MODEL; SPLINES; SELECTION	We consider M-estimators for a class of semiparametric mixed-effect models without time-dependent covariates and show that the simple marginal estimation method is generally better than the same M-estimator applied to the de-correlated response based on a known or estimated covariance matrix for each subject.	Univ Illinois, Dept Stat, Champaign, IL 61820 USA							Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; He XM, 1998, J ROY STAT SOC B, V60, P537, DOI 10.1111/1467-9868.00138; CHEN H, 1988, ANN STAT, V16, P136, DOI 10.1214/aos/1176350695; HE X, 2001, MEDIAN REGRESSION LO; HE X, 2001, IN PRESS BIOMETRIKA; He X., 1994, J NONPARAMETR STAT, V3, P299, DOI DOI 10.1080/10485259408832589; He XM, 2001, IEEE SIGNAL PROC LET, V8, P137; He XM, 1996, J MULTIVARIATE ANAL, V58, P162, DOI 10.1006/jmva.1996.0045; HUGGINS RM, 1993, BIOMETRICS, V49, P715, DOI 10.2307/2532192; Jung SH, 1996, J AM STAT ASSOC, V91, P251, DOI 10.2307/2291402; Portnoy S, 1997, ANN STAT, V25, P414; Richardson AM, 1997, J AM STAT ASSOC, V92, P154, DOI 10.2307/2291459; Ruppert D, 2000, AUST NZ J STAT, V42, P205, DOI 10.1111/1467-842X.00119; Schumaker L.L., 1981, SPLINE FUNCTIONS; SPECKMAN P, 1988, J ROY STAT SOC B MET, V50, P413; ZEGER SL, 1994, BIOMETRICS, V50, P689, DOI 10.2307/2532783; Zhang D, 1998, J AM STAT ASSOC, V93, P710	17	3	3	0	9	PHYSICA-VERLAG GMBH & CO	HEIDELBERG	TIERGARTENSTRASSE 17, 69121 HEIDELBERG, GERMANY	0026-1335			METRIKA	Metrika		2002	55	1-2					67	74		10.1007/s001840200187		8	Statistics & Probability	Mathematics	553XK	WOS:000175702200007		
J	Fan, JQ; Li, RZ				Fan, JQ; Li, RZ			Variable selection for Cox's proportional hazards model and frailty model	ANNALS OF STATISTICS			English	Article						Cox's regression model; frailty model; LASSO; penalized likelihood; partial likelihood; profile likelihood	GENERALIZED CROSS-VALIDATION; SURVIVAL-DATA; LIKELIHOOD; SHRINKAGE; LASSO	A class of variable selection procedures for parametric models via nonconcave penalized likelihood was proposed in Fan and Li (2001a). It has been shown there that the resulting procedures per-form as well as if the subset of significant variables were known in advance. Such a property is called an oracle property. The proposed procedures were illustrated in the context of linear regression, robust linear regression and generalized linear models. In this paper, the nonconcave penalized likelihood approach is extended further to the Cox proportional hazards model and the Cox proportional hazards frailty model, two commonly used semi-parametric models in survival analysis. As a result, new variable selection procedures for these two commonly-used models are proposed. It is demonstrated how the rates of convergence depend on the regularization parameter in the penalty function. Further. with a proper choice of the regularization parameter and the penalty function, the proposed estimators possess an oracle property. Standard error formulae are derived and their accuracies are empirically tested. Simulation studies show that the proposed procedures are more stable in prediction and more effective in computation than the best subset variable selection. and they reduce model complexity as effectively as the best subset variable selection. Compared with the LASSO, which is the penalized likelihood method with the L-1 -penalty, proposed by Tibshirani, the newly proposed approaches have better theoretic properties and finite sample performance.	Chinese Univ Hong Kong, Dept Stat, Shatin, Hong Kong, Peoples R China; Penn State Univ, Dept Stat, University Pk, PA 16802 USA	Fan, JQ (reprint author), Chinese Univ Hong Kong, Dept Stat, Shatin, Hong Kong, Peoples R China.		Li, Runze/C-5444-2013	Li, Runze/0000-0002-0154-2202			AN J, 2001, I STAT MIMEO SERIES, V2372; AN J, 2001, J AM STAT ASSOC, V96, P1348; Andersen P., 1993, STAT MODELS BASED CO; ANDERSEN PK, 1982, ANN STAT, V10, P1100, DOI 10.1214/aos/1176345976; Knight K, 2000, ANN STAT, V28, P1356; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Antoniadis A, 2001, J AM STAT ASSOC, V96, P939, DOI 10.1198/016214501753208942; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; COX DR, 1975, BIOMETRIKA, V62, P269, DOI 10.1093/biomet/62.2.269; DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425; CRAVEN P, 1979, NUMER MATH, V31, P377; Antoniadis A., 1997, J ITAL STAT SOC, V6, P97, DOI DOI 10.1007/BF03178905; BICKEL PJ, 1975, J AM STAT ASSOC, V70, P428, DOI 10.2307/2285834; Breiman L, 1996, ANN STAT, V24, P2350; FAN J, 2001, I STAT MIMEO SERIES, V2372; Fan J., 1997, J ITALIAN STAT ASS, V6, P131; Faraggi D, 1998, BIOMETRICS, V54, P1475, DOI 10.2307/2533672; Lehmann E. L., 1983, THEORY POINT ESTIMAT; LI KC, 1987, ANN STAT, V15, P958, DOI 10.1214/aos/1176350486; LINDLEY DV, 1968, J R STAT SOC B, V30, P31; Morris C.N., 1994, WILEY S PRO, P231; Murphy SA, 2000, J AM STAT ASSOC, V95, P449, DOI 10.2307/2669386; Murphy SA, 1999, BERNOULLI, V5, P381, DOI 10.2307/3318710; NIELSEN GG, 1992, SCAND J STAT, V19, P25; Parner E, 1998, ANN STAT, V26, P183; ROBINSON PM, 1988, ECONOMETRICA, V56, P531, DOI 10.2307/1911699; Sinha D, 1998, BIOMETRICS, V54, P1463, DOI 10.2307/2533671; Tibshirani R, 1997, STAT MED, V16, P385, DOI 10.1002/(SICI)1097-0258(19970228)16:4<385::AID-SIM380>3.0.CO;2-3; WAHBA G, 1985, ANN STAT, V13, P1378, DOI 10.1214/aos/1176349743	29	155	158	3	15	INST MATHEMATICAL STATISTICS	BEACHWOOD	PO BOX 22718, BEACHWOOD, OH 44122 USA	0090-5364			ANN STAT	Ann. Stat.	FEB	2002	30	1					74	99				26	Statistics & Probability	Mathematics	532GW	WOS:000174466800003		
J	Chipman, HA; George, EI; McCulloch, RE; Holmes, CC; Kass, RE; Wallstrom, G; Koenker, R; Mizera, I; Lindstrom, MJ; Wahba, G; Lin, Y; Leng, CL; Ruppert, D; Hansen, MH; Kooperberg, C				Chipman, HA; George, EI; McCulloch, RE; Holmes, CC; Kass, RE; Wallstrom, G; Koenker, R; Mizera, I; Lindstrom, MJ; Wahba, G; Lin, Y; Leng, CL; Ruppert, D; Hansen, MH; Kooperberg, C			Spline adaptation in extended linear models - Comments and rejoinder	STATISTICAL SCIENCE			English	Editorial Material							QUANTILE SMOOTHING SPLINES; MINIMUM DESCRIPTION LENGTH; VARIABLE SELECTION; POSTERIOR DISTRIBUTIONS; SCHWARZ CRITERION; ADDITIVE-MODELS; BAYES FACTORS; MIXED MODELS; REGRESSION; ESTIMATORS		Univ Waterloo, Dept Stat & Actuarial Sci, Waterloo, ON N2L 3G1, Canada; Univ Penn, Wharton Sch, Dept Stat, Philadelphia, PA 19104 USA; Univ Chicago, Grad Sch Business, Chicago, IL 60637 USA; Univ London Imperial Coll Sci Technol & Med, Dept Math, London SW7 2BZ, England; Carnegie Mellon Univ, Dept Stat, Pittsburgh, PA 15213 USA; Univ Illinois, Dept Econ, Champaign, IL 61820 USA; Univ Illinois, Dept Stat, Champaign, IL 61820 USA; Univ Alberta, Dept Math Sci, Edmonton, AB T6G 2G1, Canada; Univ Wisconsin, Dept Biostat & Med Informat, Madison, WI 53792 USA; Univ Wisconsin, Dept Stat, Madison, WI 53792 USA; Cornell Univ, Dept Ind Engn & Operat Res, Ithaca, NY 14853 USA	Chipman, HA (reprint author), Univ Waterloo, Dept Stat & Actuarial Sci, Waterloo, ON N2L 3G1, Canada.						SMITH AFM, 1980, J ROY STAT SOC B MET, V42, P213; Lin XH, 1999, J ROY STAT SOC B, V61, P381, DOI 10.1111/1467-9868.00183; Novak E, 1996, NUMER MATH, V75, P79, DOI 10.1007/s002110050231; KASS RE, 1995, J AM STAT ASSOC, V90, P773, DOI 10.1080/01621459.1995.10476572; DiMatteo I, 2001, BIOMETRIKA, V88, P1055, DOI 10.1093/biomet/88.4.1055; LIU JS, 1994, BIOMETRIKA, V81, P27; Hansen MH, 2001, J AM STAT ASSOC, V96, P746, DOI 10.1198/016214501753168398; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; He XM, 1998, J ROY STAT SOC B, V60, P537, DOI 10.1111/1467-9868.00138; Shively TS, 1999, J AM STAT ASSOC, V94, P777, DOI 10.2307/2669990; Denison DGT, 1998, STAT COMPUT, V8, P337, DOI 10.1023/A:1008824606259; KASS RE, 1995, J AM STAT ASSOC, V90, P928, DOI 10.2307/2291327; Eilers PHC, 1996, STAT SCI, V11, P89, DOI 10.1214/ss/1038425655; Carroll RJ, 1997, J AM STAT ASSOC, V92, P477, DOI 10.1080/01621459.1997.10474001; Barron A, 1998, IEEE T INFORM THEORY, V44, P2743, DOI 10.1109/18.720554; Barron A, 1999, ANN STAT, V27, P536; Bartlett M, 1957, BIOMETRIKA, V44, P533, DOI DOI 10.1093/BIOMET/44.3-4.533; Berry SM, 2002, J AM STAT ASSOC, V97, P160, DOI 10.1198/016214502753479301; BREIMAN L, 1993, COMPUT STAT DATA AN, V15, P13, DOI 10.1016/0167-9473(93)90217-H; Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.1023/A:1018054314350; Brumback BA, 1999, J AM STAT ASSOC, V94, P794, DOI 10.2307/2669991; Brumback BA, 1998, J AM STAT ASSOC, V93, P961, DOI 10.2307/2669837; Chipman HA, 2002, MACH LEARN, V48, P299, DOI 10.1023/A:1013916107446; Chipman HA, 1998, J AM STAT ASSOC, V93, P935, DOI 10.2307/2669832; Chipman HA, 1998, COMP SCI STAT, V30, P84; CLEVELAND WS, 1996, 961 BELL LAB LUC TEC; Coull BA, 2001, BIOMETRICS, V57, P539, DOI 10.1111/j.0006-341X.2001.00539.x; DARROCH JN, 1980, ANN STAT, V8, P522, DOI 10.1214/aos/1176345006; Denison D. G. T., 2002, BAYESIAN METHODS NON; Genovese CR, 2000, J AM STAT ASSOC, V95, P691, DOI 10.2307/2669445; George EI, 2000, BIOMETRIKA, V87, P731, DOI 10.1093/biomet/87.4.731; GU C, 1993, J AM STAT ASSOC, V88, P495, DOI 10.2307/2290329; Hastie T., 2001, ELEMENTS STAT LEARNI; Holmes CC, 1998, NEURAL COMPUT, V10, P1217, DOI 10.1162/089976698300017421; HOLMES CC, 1999, BAYESIAN STAT, V6, P769; HOLMES CC, 2002, IN PRESS MACHINE LEA; Jordan M. I., 1998, LEARNING GRAPHICAL M; KOENKER R, 1994, BIOMETRIKA, V81, P673; KOHN R, 1991, J AM STAT ASSOC, V86, P1042, DOI 10.2307/2290523; Lin XW, 2000, ANN STAT, V28, P1570; LINDLEY DV, 1957, BIOMETRIKA, V44, P187; LINDSTROM MJ, 1995, STAT MED, V14, P2009, DOI 10.1002/sim.4780141807; Luo Z, 1997, J AM STAT ASSOC, V92, P107, DOI 10.2307/2291454; NISHII R, 1988, J MULTIVARIATE ANAL, V27, P392, DOI 10.1016/0047-259X(88)90137-6; OSULLIVAN F, 1988, SIAM J SCI STAT COMP, V9, P363, DOI 10.1137/0909024; Pauler DK, 1998, BIOMETRIKA, V85, P13, DOI 10.1093/biomet/85.1.13; Rippa S., 1990, Computer-Aided Geometric Design, V7, DOI 10.1016/0167-8396(90)90011-F; RIPPA S, 1992, SIAM J SCI STAT COMP, V13, P1123, DOI 10.1137/0913065; RUPPERT D, 2002, IN PRESS J COMPUT GR; Ruppert D, 2000, AUST NZ J STAT, V42, P205, DOI 10.1111/1467-842X.00119; Shen XT, 2001, ANN STAT, V29, P687; Shi MG, 1996, APPL STAT-J ROY ST C, V45, P151, DOI 10.2307/2986151; SILVERMAN BW, 1982, ANN STAT, V10, P795, DOI 10.1214/aos/1176345872; Tibshirani R, 1999, J COMPUT GRAPH STAT, V8, P671; WAHBA G, 1978, J ROY STAT SOC B MET, V40, P364; Wahba G., 1995, J R STAT SOC B, V57, P360; WAHBA G, 1978, 523 U WISC DEP STAT; Wahba G, 1995, ANN STAT, V23, P1865; WAHBA G, 1985, ANN STAT, V13, P1378, DOI 10.1214/aos/1176349743; WALLSTROM GL, 2002, IN PRESS CASE STUDIE, V6; WANG YD, 1995, J STAT COMPUT SIM, V51, P263, DOI 10.1080/00949659508811637; Whittaker J, 1990, GRAPHICAL MODELS APP; YU Y, 2002, IN PRESS J AM STAT A, V97; Zellner A, 1986, BAYESIAN INFERENCE D, P233; Zhang D, 1998, J AM STAT ASSOC, V93, P710; ZHAO L, 1998, HIERARCHICAL BAYESIA; ZHAO LH, 1993, THESIS CORNELL U; Zhou SG, 2001, J AM STAT ASSOC, V96, P247, DOI 10.1198/016214501750332820	68	0	0	0	3	INST MATHEMATICAL STATISTICS	BEACHWOOD	PO BOX 22718, BEACHWOOD, OH 44122 USA	0883-4237			STAT SCI	Stat. Sci.	FEB	2002	17	1					20	51				32	Statistics & Probability	Mathematics	567FL	WOS:000176474700003		
J	Jolliffe, IT; Uddin, M; Vines, SK				Jolliffe, IT; Uddin, M; Vines, SK			Simplified EOFs - three alternatives to rotation	CLIMATE RESEARCH			English	Article						EOFs; simplification; interpretation; rotation; LASSO; Mediterranean; SST	NONLINEAR DIMENSIONALITY REDUCTION; PRINCIPAL COMPONENTS; SELECTION	Principal component analysis (PCA) is widely used in atmospheric science, and the resulting empirical orthogonal functions (EOFs) are often rotated to aid interpretation. In this paper 3 methods are described which provide alternatives to the standard 2-stage procedure of PCA followed by rotation. The techniques are illustrated on a small example involving sea-surface temperatures in the Mediterranean, Each method is shown to give different simplified interpretations for the major sources of variation in the data set. All 3 techniques have advantages over standard rotation.	Univ Aberdeen, Dept Math Sci, Kings Coll, Aberdeen AB24 3UE, Scotland; Univ Karachi, Dept Stat, Karachi 75270, Pakistan; Open Univ, Dept Stat, Fac Math & Comp, Milton Keynes MK7 6AA, Bucks, England	Jolliffe, IT (reprint author), Univ Aberdeen, Dept Math Sci, Kings Coll, Meston Bldg, Aberdeen AB24 3UE, Scotland.						Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319; GAY DM, 1983, ACM T MATH SOFTWARE, V9, P503, DOI 10.1145/356056.356066; Cadima JFCL, 2001, J AGRIC BIOL ENVIR S, V6, P62, DOI 10.1198/108571101300325256; BARTZOKAS A, 1994, INT J CLIMATOL, V14, P201, DOI 10.1002/joc.3370140206; GAY DM, 1984, NUMERICAL ANAL, P171; GOFFE WL, 1994, J ECONOMETRICS, V60, P65, DOI 10.1016/0304-4076(94)90038-8; JOLLIFFE IT, 1987, J CLIMATOL, V7, P507; JOLLIFFE IT, 1989, APPL STAT-J ROY ST C, V38, P139, DOI 10.2307/2347688; Jolliffe IT, 2000, J COMPUT GRAPH STAT, V9, P689, DOI 10.2307/1391088; JOLLIFFE IT, 1995, J APPL STAT, V22, P29, DOI 10.1080/757584395; JOLLIFFE IT, 1995, 6 INT M STAT CLIM, P127; Krzanowski W.J., 1995, MULTIVARIATE ANAL 2; Mestas-Nunez AM, 2000, INT J CLIMATOL, V20, P1509, DOI 10.1002/1097-0088(200010)20:12<1509::AID-JOC553>3.0.CO;2-Q; Monahan AH, 2000, J CLIMATE, V13, P821, DOI 10.1175/1520-0442(2000)013<0821:NPCABN>2.0.CO;2; RICHMAN MB, 1987, J CLIMATOL, V7, P511; RICHMAN MB, 1986, J CLIMATOL, V6, P293; van den Dool HM, 2000, J CLIMATE, V13, P1421, DOI 10.1175/1520-0442(2000)013<1421:EOT>2.0.CO;2; Vines SK, 2000, J ROY STAT SOC C-APP, V49, P441, DOI 10.1111/1467-9876.00204; ZWEIRS FW, 1999, ANTHROPOGENIC CLIMAT, P161	21	9	9	3	12	INTER-RESEARCH	OLDENDORF LUHE	NORDBUNTE 23, D-21385 OLDENDORF LUHE, GERMANY	0936-577X			CLIMATE RES	Clim. Res.	APR 26	2002	20	3					271	279		10.3354/cr020271		9	Environmental Sciences; Meteorology & Atmospheric Sciences	Environmental Sciences & Ecology; Meteorology & Atmospheric Sciences	563CD	WOS:000176235900009		
J	Tibshirani, R; Hastie, T; Narasimhan, B; Chu, G				Tibshirani, R; Hastie, T; Narasimhan, B; Chu, G			Diagnosis of multiple cancer types by shrunken centroids of gene expression	PROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES OF THE UNITED STATES OF AMERICA			English	Article							ROUND-CELL TUMORS; NEUROECTODERMAL TUMOR; CLASSIFICATION; PREDICTION; SHRINKAGE	We have devised an approach to cancer class prediction from gene expression profiling, based on an enhancement of the simple nearest prototype (centroid) classifier. We shrink the prototypes and hence obtain a classifier that is often more accurate than competing methods. Our method of "nearest shrunken centroids" identifies subsets of genes that best characterize each class. The technique is general and can be used in many other classification problems. To demonstrate its effectiveness, we show that the method was highly efficient in finding genes for classifying small round blue cell tumors and leukemias.	Stanford Univ, Dept Hlth Res & Policy & Stat, Stanford, CA 94305 USA; Stanford Univ, Dept Stat & Hlth Res & Policy, Stanford, CA 94305 USA; Stanford Univ, Dept Med & Biochem, Stanford, CA 94305 USA	Tibshirani, R (reprint author), Stanford Univ, Dept Hlth Res & Policy & Stat, Stanford, CA 94305 USA.		Moorthy, Kohbalan/B-2470-2015	Moorthy, Kohbalan/0000-0002-6184-0359			ALTMANNSBERGER M, 1985, AM J PATHOL, V118, P85; Anderson J, 2001, BRIT J CANCER, V85, P831, DOI 10.1054/bjoc.2001.2008; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Khan J, 2001, NAT MED, V7, P673, DOI 10.1038/89044; Tusher VG, 2001, P NATL ACAD SCI USA, V98, P5116, DOI 10.1073/pnas.091062498; Golub TR, 1999, SCIENCE, V286, P531, DOI 10.1126/science.286.5439.531; Folpe AL, 2000, AM J SURG PATHOL, V24, P1657, DOI 10.1097/00000478-200012000-00010; Eisen MB, 1998, P NATL ACAD SCI USA, V95, P14863, DOI 10.1073/pnas.95.25.14863; DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425; Boon K, 2001, EMBO J, V20, P1383, DOI 10.1093/emboj/20.6.1383; Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.1023/A:1018054314350; Hastie T, 2001, GENOME BIOL, V2; Hastie T., 2001, ELEMENTS STAT LEARNI; Hedenfalk I, 2001, NEW ENGL J MED, V344, P539, DOI 10.1056/NEJM200102223440801; KOVAR H, 1990, ONCOGENE, V5, P1067; Pagani A, 1998, DIAGN MOL PATHOL, V7, P36, DOI 10.1097/00019606-199802000-00007; WANG NP, 1995, AM J PATHOL, V147, P1799	17	1370	1412	4	32	NATL ACAD SCIENCES	WASHINGTON	2101 CONSTITUTION AVE NW, WASHINGTON, DC 20418 USA	0027-8424			P NATL ACAD SCI USA	Proc. Natl. Acad. Sci. U. S. A.	MAY 14	2002	99	10					6567	6572		10.1073/pnas.082099299		6	Multidisciplinary Sciences	Science & Technology - Other Topics	552TZ	WOS:000175637300012	12011421	
J	Li, RZ; Lin, DKJ				Li, RZ; Lin, DKJ			Data analysis in supersaturated designs	STATISTICS & PROBABILITY LETTERS			English	Article						AIC; BIC; penalized least squares; SCAD; stepwise regression	LASSO; SELECTION	Supersaturated designs (SSDs) can save considerable cost in industrial experimentation when many potential factors are introduced in preliminary studies. Analyzing data in SSDs is challenging because the number of experiments is less than the number of candidate factors. In this paper, we introduce a variable selection approach to identifying the active effects in SSD via nonconvex penalized least squares. An iterative ridge regression is employed to find the solution of the penalized least squares. We provide both theoretical and empirical justifications for the proposed approach. Some related issues are also discussed. (C) 2002 Elsevier Science B.V. All rights reserved.	Penn State Univ, Dept Stat, University Pk, PA 16802 USA; Penn State Univ, Dept Management Sci & Informat Syst, University Pk, PA 16802 USA			Li, Runze/C-5444-2013	Li, Runze/0000-0002-0154-2202			Fu WJJ, 1998, J COMPUT GRAPH STAT, V7, P397, DOI 10.2307/1390712; Knight K, 2000, ANN STAT, V28, P1356; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; FRANK IE, 1993, TECHNOMETRICS, V35, P109, DOI 10.2307/1269656; CRAVEN P, 1979, NUMER MATH, V31, P377; Fan J., 1997, J ITALIAN STAT ASS, V6, P131; Fang KT, 2000, J STAT PLAN INFER, V86, P239, DOI 10.1016/S0378-3758(99)00163-9; LIN DKJ, 1995, TECHNOMETRICS, V37, P213, DOI 10.2307/1269622; LIN DKJ, 2000, STAT PROCESS MONITOR, pCH18; LIN DKJ, 1991, 264 U TENN DEP STAT; LIN DKJ, 1993, TECHNOMETRICS, V35, P28, DOI 10.2307/1269286; Westfall PH, 1998, STAT SINICA, V8, P101; WILLIAMS KR, 1968, RUBBER AGE, V100, P65	14	33	34	1	2	ELSEVIER SCIENCE BV	AMSTERDAM	PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS	0167-7152			STAT PROBABIL LETT	Stat. Probab. Lett.	SEP 1	2002	59	2					135	144	PII S0167-7152(02)00140-2	10.1016/S0167-7152(02)00140-2		10	Statistics & Probability	Mathematics	592AY	WOS:000177915600003		
J	Urmanov, AM; Gribok, AV; Hines, JW; Uhrig, RE				Urmanov, AM; Gribok, AV; Hines, JW; Uhrig, RE			An information approach to regularization parameter selection under model misspecification	INVERSE PROBLEMS			English	Article							REGRESSION; CRITERION	We review the information approach to regularization parameter selection and its information complexity extension for the solution of discrete ill posed problems. An information criterion for regularization parameter selection was first proposed by Shibata in the context of ridge regression as an extension of Takeuchi's information criterion. In the information approach, the regularization parameter value is chosen to maximize the mean expected log likelihood (MELL) of a model whose parameters are estimated using the maximum penalized likelihood method. Under the Gaussian noise assumption such a choice coincides with the minimum of mean predictive error choice. Maximization of the MELL corresponds to minimization of the mean Kullback-Leibler information, that measures the deviation of the approximating (model) distribution from the true one. The resulting regularization parameter selection methods can handle possible functional and distributional misspecifications when the usual assumptions of Gaussian noise and/or linear relationship have been made but not met. We also suggest that in engineering applications it is beneficial to find ways of lowering the risk of getting grossly under-regularized solutions and that the new information complexity regularization parameter. selection method (RPSM) is one of the possibilities. Several examples of applying the reviewed RPSMs are given.	Univ Tennessee, Dept Nucl Engn, Knoxville, TN 37996 USA	Urmanov, AM (reprint author), Univ Tennessee, Dept Nucl Engn, Knoxville, TN 37996 USA.						Akaike H., 1973, INT S INFORMATION TH, V2, P267; PHILLIPS DL, 1962, J ACM, V9, P84, DOI 10.1145/321105.321114; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Bozdogan H, 2000, J MATH PSYCHOL, V44, P62, DOI 10.1006/jmps.1999.1277; HOERL AE, 1970, TECHNOMETRICS, V12, P55; BOZDOGAN H, 1987, PSYCHOMETRIKA, V52, P345, DOI 10.1007/BF02294361; KULLBACK S, 1951, ANN MATH STAT, V22, P79, DOI 10.1214/aoms/1177729694; MALLOWS CL, 1973, TECHNOMETRICS, V15, P661, DOI 10.2307/1267380; BOZDOGAN H, 1996, ASA ANN M CHIC IL AU; COX DD, 1990, ANN STAT, V18, P1676, DOI 10.1214/aos/1176347872; FISHER RA, 1922, PHILOS T R SOC A, V222, P309, DOI DOI 10.1098/RSTA.1922.0009; GOOD IJ, 1971, BIOMETRIKA, V58, P255, DOI 10.2307/2334515; GREEN PJ, 1987, INT STAT REV, V55, P255; Hansen P. C., 1994, Numerical Algorithms, V6, DOI 10.1007/BF02149761; Hansen P.C., 1998, SIAM MONOGRAPHS MATH; Huber P. J., 1981, ROBUST STAT; Konishi S, 1996, BIOMETRIKA, V83, P875, DOI 10.1093/biomet/83.4.875; LEONARD T, 1978, J ROY STAT SOC B MET, V40, P113; LEONOV AS, 1997, MOSCOW U PHYSICS B, V52, P20; Lukas MA, 1998, INVERSE PROBL, V14, P161, DOI 10.1088/0266-5611/14/1/014; Morozov V.A., 1984, METHODS SOLVING INCO; MURATA N, 1994, IEEE T NEURAL NETWOR, V5, P865, DOI 10.1109/72.329683; Sakamoto Y, 1986, AKAIKE INFORMATION C; SHAW CB, 1972, J MATH ANAL APPL, V37, P83, DOI 10.1016/0022-247X(72)90259-4; Shibata R, 1989, DATA MODEL, P215; Silverman BW, 1985, ENCY STAT SCI, V6, P664; Takeuchi K., 1976, MATH SCI, V153, P12; Thompson A.M., 1989, J STATIST COMPUT SIM, V33, P199, DOI 10.1080/00949658908811198; Tikhonov AN, 1963, SOV MATH DOKL, V4, P1035; Urmanov AM, 2002, INVERSE PROBL, V18, pL1, DOI 10.1088/0266-5611/18/2/101; Van Emden M, 1971, MATH CTR TRACTS, V35; Vogel CR, 1996, INVERSE PROBL, V12, P535, DOI 10.1088/0266-5611/12/4/013; Wahba G., 1990, SPLINE MODELS OBSERV; WAHBA G, 1993, 910 U WISC DEP STAT; WHITE H, 1980, INT ECON REV, V21, P149, DOI 10.2307/2526245; White H., 1994, ESTIMATION INFERENCE; WHITE H, 1981, J AM STAT ASSOC, V76, P419, DOI 10.2307/2287845	37	3	3	2	2	IOP PUBLISHING LTD	BRISTOL	DIRAC HOUSE, TEMPLE BACK, BRISTOL BS1 6BE, ENGLAND	0266-5611			INVERSE PROBL	Inverse Probl.	OCT	2002	18	5					1207	1228	PII S0266-5611(02)34102-9	10.1088/0266-5611/18/5/301		22	Mathematics, Applied; Physics, Mathematical	Mathematics; Physics	608ZP	WOS:000178878300001		
J	Hammer, B; Villmann, T				Hammer, B; Villmann, T			Generalized relevance learning vector quantization	NEURAL NETWORKS			English	Article						clustering; learning vector quantization; adaptive metric; relevance determination	SELF-ORGANIZING MAPS; SELECTION	We propose a new scheme for enlarging generalized learning vector quantization (GLVQ) with weighting factors for the input dimensions. The factors allow an appropriate scaling of the input dimensions according to their relevance. They are adapted automatically during training according to the specific classification task whereby training can be interpreted as stochastic gradient descent on an appropriate error function. This method leads to a more powerful classifier and to an adaptive metric with little extra cost compared to standard GLVQ. Moreover, the size of the weighting factors indicates the relevance of the input dimensions. This proposes a scheme for automatically pruning irrelevant input dimensions. The algorithm is verified on artificial data sets and the iris data from the UCI repository. Afterwards, the method is compared to several well known algorithms which determine the intrinsic data dimension on real world satellite image data. (C) 2002 Elsevier Science Ltd. All rights reserved.	Univ Osnabruck, Dept Math & Comp Sci, D-49069 Osnabruck, Germany; Univ Leipzig, Clin Psychotherapy & Psychosomat Med, D-04107 Leipzig, Germany	Hammer, B (reprint author), Univ Osnabruck, Dept Math & Comp Sci, Albrechtstr 28, D-49069 Osnabruck, Germany.	hammer@informatik.uni-osnabrueck.de	Hammer, Barbara /E-8624-2010				Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Hyvarinen A, 1997, NEURAL COMPUT, V9, P1483, DOI 10.1162/neco.1997.9.7.1483; Bauer HU, 1997, IEEE T NEURAL NETWOR, V8, P218, DOI 10.1109/72.557659; FRITZKE B, 1995, NEURAL PROCESS LETT, V2, P9, DOI 10.1007/BF02332159; Augusteijn M. F., 1993, P ICANN 93, P1010; Blake C.L., 1998, UCI REPOSITORY MACHI; Bojer T., 2001, P EUR S ART NEUR NET, P271; Campbell J.B, 1996, INTRO REMOTE SENSING; Duch W, 2001, IEEE T NEURAL NETWOR, V12, P277, DOI 10.1109/72.914524; GATH I, 1989, IEEE T PATTERN ANAL, V11, P773, DOI 10.1109/34.192473; Grandvalet Y., 1998, PERSPECTIVES NEURAL, V1, P201; Grandvalet Y, 2000, IEEE T NEURAL NETWOR, V11, P1201, DOI 10.1109/72.883393; GRASSBERGER P, 1983, PHYSICA D, V9, P189, DOI 10.1016/0167-2789(83)90298-1; Gustafson D. E., 1979, Proceedings of the 1978 IEEE Conference on Decision and Control Including the 17th Symposium on Adaptive Processes; Hofmann T, 2000, ADV NEUR IN, V12, P914; Kaski S, 2001, IEEE T NEURAL NETWOR, V12, P936, DOI 10.1109/72.935102; Kaski S., 1998, P IJCNN 98 INT JOINT, V1, P413; Kohonen T., 1995, HDB BRAIN THEORY NEU, P537; Kohonen T., 1997, SELF ORG MAPS; MARTINETZ T, 1994, NEURAL NETWORKS, V7, P507, DOI 10.1016/0893-6080(94)90109-0; MATECKI U, 1999, AUTOMATISCHE MERKMAL; MERENYI E, 1999, P EUR S ART NEUR NET, P93; MEYERING A, 1992, P IJCNN 92, P432; Neal R. M., 1996, BAYESIAN LEARNING NE; OJA E, 1995, HDB BRAIN THEORY NEU, P753; Pregenzer M, 1996, NEUROCOMPUTING, V11, P19, DOI 10.1016/0925-2312(94)00071-9; Ritter H, 1999, KOHONEN MAPS, P97, DOI 10.1016/B978-044450270-4/50007-3; Roth V, 2001, LECT NOTES COMPUT SC, V2130, P339; Sato A.S., 1995, ADV NEURAL INFORMATI, V7, P423; Sinkkonen J, 2002, NEURAL COMPUT, V14, P217, DOI 10.1162/089976602753284509; Somervuo P, 1999, NEURAL PROCESS LETT, V10, P151, DOI 10.1023/A:1018741720065; Tipping ME, 2000, ADV NEUR IN, V12, P652; Tsay MK, 1999, IEICE T INF SYST, VE82D, P687; VANGESTEL T, 2001, EUR S ART NEUR NETW, P13; Villmann T, 1997, IEEE T NEURAL NETWOR, V8, P256, DOI 10.1109/72.557663; Villmann T, 2001, SELF ORG MAPS RECENT, P121; VILLMANN T, 1999, P EUR S ART NEUR NET, P111; WIENHOLT W, 1996, ENTWURF NEURONALER N	38	154	160	0	2	PERGAMON-ELSEVIER SCIENCE LTD	OXFORD	THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND	0893-6080			NEURAL NETWORKS	Neural Netw.	OCT-NOV	2002	15	8-9					1059	1068	PII S0893-6080(02)00079-5	10.1016/S0893-6080(02)00079-5		10	Computer Science, Artificial Intelligence; Neurosciences	Computer Science; Neurosciences & Neurology	606VY	WOS:000178756400011	12416694	
J	Antoniadis, A; Leporini, D; Pesquet, JC				Antoniadis, A; Leporini, D; Pesquet, JC			Wavelet thresholding for some classes of non-Gaussian noise	STATISTICA NEERLANDICA			English	Article						wavelets; denoising; regularization; MAP; non-Gaussian noises; exponential power distributions; Cauchy distribution	SHRINKAGE; SELECTION; REGRESSION; REPRESENTATION; DECOMPOSITION; LASSO	Wavelet shrinkage and thresholding methods constitute a powerful way to carry out signal denoising, especially when the underlying signal has a sparse wavelet representation. They are computationally fast, and automatically adapt to the smoothness of the signal to be estimated. Nearly minimax properties for simple threshold estimators over a large class of function spaces and for a wide range of loss functions were established in a series of papers by Donoho and Johnstone. The notion behind these wavelet methods is that the unknown function is well approximated by a function with a relatively small proportion of nonzero wavelet coefficients. In this paper, we propose a framework in which this notion of sparseness can be naturally expressed by a Bayesian model for the wavelet coefficients of the underlying signal. Our Bayesian formulation is grounded on the empirical observation that the wavelet coefficients can be summarized adequately by exponential power prior distributions and allows us to establish close connections between wavelet thresholding techniques and Maximum A Posteriori estimation for two classes of noise distributions including heavy-tailed noises. We prove that a great variety of thresholding rules are derived from these MAP criteria. Simulation examples are presented to substantiate the proposed approach.	Univ Grenoble 1, LMC, Lab IMAG, F-38041 Grenoble 09, France; Univ Marne la Vallee, F-77454 Marne La Vallee, France	Antoniadis, A (reprint author), Univ Grenoble 1, LMC, Lab IMAG, BP 53, F-38041 Grenoble 09, France.						Abramovich F, 1998, J ROY STAT SOC B, V60, P725, DOI 10.1111/1467-9868.00151; Abramovich F, 1996, COMPUT STAT DATA AN, V22, P351, DOI 10.1016/0167-9473(96)00003-5; Fu WJJ, 1998, J COMPUT GRAPH STAT, V7, P397, DOI 10.2307/1390712; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Chambolle A, 1998, IEEE T IMAGE PROCESS, V7, P319; TIERNEY L, 1994, ANN STAT, V22, P1701, DOI 10.1214/aos/1176325750; Delyon B, 1996, APPL COMPUT HARMON A, V3, P215, DOI 10.1006/acha.1996.0017; FRANK IE, 1993, TECHNOMETRICS, V35, P109, DOI 10.2307/1269656; MALLAT SG, 1989, IEEE T PATTERN ANAL, V11, P674, DOI 10.1109/34.192463; DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425; DONOHO DL, 1995, IEEE T INFORM THEORY, V41, P613, DOI 10.1109/18.382009; Chipman HA, 1997, J AM STAT ASSOC, V92, P1413, DOI 10.2307/2965411; Antoniadis A, 1997, BIOMETRIKA, V84, P751, DOI 10.1093/biomet/84.4.751; AVERKAMP R, 1999, WAVELET THRESHOLDING; BUCCIGROSSI RW, 1997, P IEEE C AC SPEECH S; Clyde M, 1998, BIOMETRIKA, V85, P391, DOI 10.1093/biomet/85.2.391; Coifman R., 1995, LECT NOTES STAT; DECHEVSKY LT, 1998, S9815 U NEW S WAL DE; Donoho DL, 1995, J AM STAT ASSOC, V90, P1200, DOI 10.2307/2291512; Donoho DL, 1998, ANN STAT, V26, P879; DONOHO DL, 1995, J ROY STAT SOC B MET, V57, P301; FANG K. T., 1989, SYMMETRIC MULTIVARIA; George EI, 1997, STAT SINICA, V7, P339; JOHNSTONE IM, 1998, EMPIRICAL BAYES APPR; JUDITSKY A, 1997, MATH METHODS STAT, V6, P1; Krim H, 1999, IEEE T INFORM THEORY, V45, P898, DOI 10.1109/18.761331; LEPORINI D, 1998, THESIS U PARIS 11; Leporini D, 2001, SIGNAL PROCESS, V81, P55, DOI 10.1016/S0165-1684(00)00190-0; Leporini D, 1999, IEEE T INFORM THEORY, V45, P863, DOI 10.1109/18.761329; MALLAT SG, 1989, T AM MATH SOC, V315, P69, DOI 10.2307/2001373; Nason G. P., 1995, LECT NOTES STAT, V103, P281, DOI DOI 10.1007/978-1-4612-2544-7_17; NASON GP, 1995, LECT NOTES STAT, V103, P261; Nason GP, 1996, J ROY STAT SOC B MET, V58, P463; Neumanns MH, 1995, LECT NOTES STAT, P301; Ng KW, 1994, INST MATH S, V24, P359, DOI 10.1214/lnms/1215463808; Nikias C. L., 1995, SIGNAL PROCESSING AL; Nikolova M, 1997, CR ACAD SCI I-MATH, V325, P665, DOI 10.1016/S0764-4442(97)84780-5; Ogden R.T., 1996, STAT COMPUT, V63, P93; Ogden T, 1996, COMPUT STAT DATA AN, V22, P53, DOI 10.1016/0167-9473(95)00041-0; Ruggeri F, 1999, STAT SINICA, V9, P183; Simoncelli E, 1996, P IEEE INT C IM PROC; Simoncelli E. P., 1999, LECT NOTES STAT; SMITH AFM, 1993, J ROY STAT SOC B MET, V55, P3; Vidakovic B, 1998, J AM STAT ASSOC, V93, P173, DOI 10.2307/2669614	44	30	30	1	3	BLACKWELL PUBL LTD	OXFORD	108 COWLEY RD, OXFORD OX4 1JF, OXON, ENGLAND	0039-0402			STAT NEERL	Stat. Neerl.	NOV	2002	56	4					434	453		10.1111/1467-9574.00211		20	Statistics & Probability	Mathematics	629LG	WOS:000180050700004		
J	Loubes, JM; van de Geer, S				Loubes, JM; van de Geer, S			Adaptive estimation with soft thresholding penalties	STATISTICA NEERLANDICA			English	Article						adaptive estimation; empirical process; penalty; rate of convergence; regression; soft thresholding	MODEL SELECTION; REGRESSION; APPROXIMATION; SHRINKAGE; SPLINES; SPACES; RISK	We show that various robust nonparametric regression estimators, such as the least absolute deviations estimator, can be made adaptive (up to logarithmic factors), by adding a soft thresholding type penalty to the loss function. As an example, we consider the situation where the roughness of the regression function is described by a single parameter p. The theory is complemented with a simulation study.	Univ Toulouse 3, Lab Stat & Probabilities, F-31062 Toulouse, France; Leiden Univ, Inst Math, NL-2300 RA Leiden, Netherlands	Loubes, JM (reprint author), Univ Toulouse 3, Lab Stat & Probabilities, 118 Route Narbonne, F-31062 Toulouse, France.						Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Donoho DL, 1996, ANN STAT, V24, P508; DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425; DONOHO DL, 1995, IEEE T INFORM THEORY, V41, P613, DOI 10.1109/18.382009; Baraud Y, 2000, PROBAB THEORY REL, V117, P467, DOI 10.1007/PL00008731; Barron A, 1999, PROBAB THEORY REL, V113, P301, DOI 10.1007/s004400050210; BARRON AR, 1991, ANN STAT, V19, P1347, DOI 10.1214/aos/1176348252; Besov O., 1978, SCRIPTA SERIES MATH, VI; Birge L., 1997, FESTSCHRIFT LUCIEN C, P55, DOI 10.1007/978-1-4612-1880-7_4; Birge L., 2001, J EUR MATH SOC, V3, P203, DOI 10.1007/s100970100031; Birge L, 2000, CONSTR APPROX, V16, P1, DOI 10.1007/s003659910001; Birman M. S., 1967, MAT SBORNIK, V73, P331; BRUCE AS, 1999, ROBUST WAVELET DENOI; Chen S. S., 1999, SIAM J SCI COMPUT, V20, P33; DeVore R.A., 1993, CONSTRUCTIVE APPROXI; DONOHO D, 1996, FESTSCHRIFT L LECAM, P183; DONOHO D L, 1996, BERNOULLI, V2, P39, DOI 10.3150/bj/1193758789; DONOHO DL, 1994, PROBAB THEORY REL, V99, P277, DOI 10.1007/BF01199026; EDMUNDS DE, 1992, P LOND MATH SOC, V64, P153; HOEDING W, 1963, J AM STAT ASSOC, V58, P13; LEDOUX M., 1991, PROBABILITY BANACH S; Lofstrom J., 1976, INTERPOLATION SPACES; Mallat S., 1998, WAVELET TOUR SIGNAL; Mammen E, 1997, ANN STAT, V25, P387; Massart P., 2000, ANN FAC SCI TOULOUSE, V9, P245; MEYER Y, 1987, CONTRIBUTIONS NONLIN, V2, P158; Portnoy S, 1997, ANN STAT, V25, P414; Rockafellar R.T., 1997, CONVEX ANAL; SILVERMAN BW, 1982, ANN STAT, V10, P795, DOI 10.1214/aos/1176345872; STONE CJ, 1990, ANN STAT, V18, P717, DOI 10.1214/aos/1176347622; VANDEGEER S, 1990, ANN STAT, V18, P907, DOI 10.1214/aos/1176347632; van de Geer S, 2000, EMPIRICAL PROCESSES; Van der Vaart A., 1996, WEAK CONVERGENCE EMP; VANDEGEER S, 2002, IN PRESS J STAT PLAN; Yang YH, 1999, STAT SINICA, V9, P475	35	8	8	0	1	BLACKWELL PUBL LTD	OXFORD	108 COWLEY RD, OXFORD OX4 1JF, OXON, ENGLAND	0039-0402			STAT NEERL	Stat. Neerl.	NOV	2002	56	4					454	479		10.1111/1467-9574.00212		26	Statistics & Probability	Mathematics	629LG	WOS:000180050700005		
J	Ojelund, H; Brown, PJ; Madsen, H; Thyregod, P				Ojelund, H; Brown, PJ; Madsen, H; Thyregod, P			Prediction based on mean subset	TECHNOMETRICS			English	Article						Bayesian variable selection; best subset; calibration; garrote; lasso; model averaging; shrinkage	BAYESIAN VARIABLE SELECTION; REGRESSION; SHRINKAGE	Shrinkage methods have traditionally been applied in prediction problems. In this article we develop a shrinkage method (mean subset) that forms an average of regression coefficients from individual subsets of the explanatory variables. A Bayesian approach is taken to derive an expression of how the coefficient vectors from each subset should be weighted. It is not computationally feasible to calculate the mean subset coefficient vector for larger problems, and thus we suggest an algorithm to find an approximation to the mean subset coefficient vector. In a comprehensive Monte Carlo simulation study, it is found that the proposed mean subset method has superior prediction performance than prediction based on the best subset method, and in some settings also better than the ridge regression and lasso methods. The conclusions drawn from the Monte Carlo study is corroborated in an example in which prediction is made using spectroscopic data.	Tech Univ Denmark, DK-2800 Lyngby, Denmark; Univ Kent, Inst Math & Stat, Canterbury CT2 7NF, Kent, England	Ojelund, H (reprint author), Tech Univ Denmark, DK-2800 Lyngby, Denmark.						BREIMAN L, 1995, TECHNOMETRICS, V37, P373, DOI 10.2307/1269730; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; FURNIVAL GM, 1974, TECHNOMETRICS, V16, P499, DOI 10.2307/1267601; Raftery AE, 1997, J AM STAT ASSOC, V92, P179, DOI 10.2307/2291462; HOERL AE, 1970, TECHNOMETRICS, V12, P55; Kalivas JH, 1997, CHEMOMETR INTELL LAB, V37, P255, DOI 10.1016/S0169-7439(97)00038-5; COPAS JB, 1983, J R STAT SOC B, V45, P311; Breiman L, 1996, ANN STAT, V24, P2350; BREIMAN L, 1992, INT STAT REV, V60, P291, DOI 10.2307/1403680; Brown PJ, 1998, J R STAT SOC B, V60, P627, DOI 10.1111/1467-9868.00144; DEMPSTER AP, 1973, MULTIVARIATE STAT IN, P25; George EI, 1997, STAT SINICA, V7, P339; James W., 1961, 4TH P BERK S MATH ST, V1, P361; LEAMER EE, 1976, J ROY STAT SOC B MET, V38, P85; MILLER AJ, 1990, SUBSET SELECTION REG, V40; Vach K, 2001, STAT NEERL, V55, P53, DOI 10.1111/1467-9574.00156; ZELLNER A, 1980, BAYESIAN ANAL ECONOM	17	8	8	0	4	AMER STATISTICAL ASSOC	ALEXANDRIA	1429 DUKE ST, ALEXANDRIA, VA 22314 USA	0040-1706			TECHNOMETRICS	Technometrics	NOV	2002	44	4					369	378		10.1198/004017002188618563		10	Statistics & Probability	Mathematics	606LE	WOS:000178734800008		
J	Guisan, A; Edwards, TC; Hastie, T				Guisan, A; Edwards, TC; Hastie, T			Generalized linear and generalized additive models in studies of species distributions: setting the scene	ECOLOGICAL MODELLING			English	Article						statistical modeling; generalized linear model; generalized additive model; species distribution; predictive modeling	ENVIRONMENTAL GRADIENT; LOGISTIC-REGRESSION; ECOLOGICAL MODELS; RICHNESS; PATTERNS; VARIABLES; SCALE; TERRAIN; FOREST; ASSUMPTIONS	An important statistical development of the last 30 years has been the advance in regression analysis provided by generalized linear models (GLMs) and generalized additive models (GAMs). Here we introduce a series of papers prepared within the framework of an international workshop entitled: Advances in GLMs/GAMs modeling: from species distribution to environmental management, held in Riederalp, Switzerland, 6-11 August 2001. We first discuss some general uses of statistical models in ecology, as well as provide a short review of several key examples of the use of GLMs and GAMs in ecological modeling efforts. We next present an overview of GLMs and GAMs, and discuss some of their related statistics used for predictor selection, model diagnostics, and evaluation. Included is a discussion of several new approaches applicable to GLMs and GAMs, such as ridge regression, an alternative to stepwise selection of predictors, and methods for the identification of interactions by a combined use of regression trees and several other approaches. We close with an overview of the papers and how we feel they advance our understanding of their application to ecological modeling. (C) 2002 Elsevier Science B.V. All rights reserved.	Swiss Ctr Faunal Cartog, CH-2000 Neuchatel, Switzerland; Univ Lausanne, Inst Ecol, CH-1015 Lausanne, Switzerland; Utah State Univ, USGS Biol Resources, Utah Cooperat Fish & Wildlife Res Unit, Logan, UT 84322 USA; Stanford Univ, Dept Stat, Stanford, CA 94305 USA	Guisan, A (reprint author), Swiss Ctr Faunal Cartog, Terreaux 14, CH-2000 Neuchatel, Switzerland.		Guisan, Antoine/A-1057-2011	Guisan, Antoine/0000-0002-3998-4815			Akaike H., 1973, 2 INT S INF THEOR, P267; Guisan A, 2000, J VEG SCI, V11, P617, DOI 10.2307/3236568; Guisan A, 2000, ECOL MODEL, V135, P147, DOI 10.1016/S0304-3800(00)00354-9; Rykiel EJ, 1996, ECOL MODEL, V90, P229, DOI 10.1016/0304-3800(95)00152-2; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; CURRIE DJ, 1991, AM NAT, V137, P27, DOI 10.1086/285144; NELDER JA, 1972, J R STAT SOC SER A-G, V135, P370, DOI 10.2307/2344614; Pearce J, 2000, ECOL MODEL, V133, P225, DOI 10.1016/S0304-3800(00)00322-7; Jaberg C, 2001, J APPL ECOL, V38, P1169, DOI 10.1046/j.0021-8901.2001.00668.x; Roloff GJ, 1999, WILDLIFE SOC B, V27, P973; Augustin NH, 1996, J APPL ECOL, V33, P339, DOI 10.2307/2404755; Guisan A, 1999, PLANT ECOL, V143, P107, DOI 10.1023/A:1009841519580; Thornton PE, 1997, J HYDROL, V190, P214, DOI 10.1016/S0022-1694(96)03128-9; Fielding AH, 1997, ENVIRON CONSERV, V24, P38, DOI 10.1017/S0376892997000088; Hirzel AH, 2001, ECOL MODEL, V145, P111, DOI 10.1016/S0304-3800(01)00396-9; Harrell FE, 1996, STAT MED, V15, P361, DOI 10.1002/(SICI)1097-0258(19960229)15:4<361::AID-SIM168>3.0.CO;2-4; Austin M., 1981, P ECOL SOC AUST, V11, P109; AUSTIN MP, 1990, ECOL MONOGR, V60, P161, DOI 10.2307/1943043; AUSTIN MP, 1987, VEGETATIO, V69, P35, DOI 10.1007/BF00038685; AUSTIN MP, 1971, P ECOL SOC AUST, V6, P63; AUSTIN MP, 1983, AUST J ECOL, V8, P169, DOI 10.1111/j.1442-9993.1983.tb01604.x; AUSTIN MP, 1994, J VEG SCI, V5, P215, DOI 10.2307/3236154; Brauner N, 1998, AICHE J, V44, P603, DOI 10.1002/aic.690440311; Breslow NE, 1996, STAT APPL, V8, P23; BROWN DG, 1994, J VEG SCI, V5, P641, DOI 10.2307/3235880; BRZEZIECKI B, 1987, VEGETATIO, V71, P175; Burnham K. P., 1998, MODEL SELECTION INFE; CALE WG, 1983, ECOL MODEL, V18, P171, DOI 10.1016/0304-3800(83)90011-X; CANTONI E, IN PRESS BIOMETRIKA; COX DR, 1968, J ROY STAT SOC B, V49, P1; CURRIE DJ, 1987, NATURE, V329, P326, DOI 10.1038/329326a0; DAVIDSON BC, 1989, S AFR J FOOD SCI NUT, V1, P3; DAVIS FW, 1990, LANDSCAPE ECOL, V4, P69, DOI 10.1007/BF02573952; DAVISON AC, 1992, INT STAT REV, V60, P337, DOI 10.2307/1403682; DAVISON AC, 1989, REV BRASILEIRA PROBA, V3, P87; Davison AC, 2001, BIOMETRIKA, V88, P13, DOI 10.1093/biomet/88.1.13; Franklin J, 1998, J VEG SCI, V9, P733, DOI 10.2307/3237291; Fraser RH, 1998, GLOBAL ECOL BIOGEOGR, V7, P215, DOI 10.1046/j.1466-822X.1998.00294.x; Frescino TS, 2001, J VEG SCI, V12, P15, DOI 10.2307/3236670; Guisan A, 1998, J VEG SCI, V9, P65, DOI 10.2307/3237224; GUISAN A, 2001, REPTILES SUISSE REPA, P183; GUISAN A, 2002, PREDICTING SPECIES O; Guisan A, 2000, PHYTOCOENOLOGIA, V30, P353; Harrell FE, 1998, STAT MED, V17, P909, DOI 10.1002/(SICI)1097-0258(19980430)17:8<909::AID-SIM753>3.3.CO;2-F; Harrell FE, 2001, REGRESSION MODELING; Hastie T., 2001, ELEMENTS STAT LEARNI; Hastie T, 1986, STAT SCI, V1, P297, DOI DOI 10.1214/SS/1177013604; Hastie TJ, 1990, GEN ADDITIVE MODELS; Heikkinen RK, 1996, VEGETATIO, V126, P151, DOI 10.1007/BF00045601; Hosmer DW, 2000, APPL LOGISTIC REGRES; HUISMAN J, 1993, J VEG SCI, V4, P37, DOI 10.2307/3235732; Huntley B, 1995, J BIOGEOGR, V22, P967, DOI 10.2307/2845830; JONES MT, 2002, PREDICTING SPECIES O; Jongman R.H.G, 1995, DATA ANAL COMMUNITY; Legendre P., 1998, NUMERICAL ECOLOGY; Lehmann A, 1998, PLANT ECOL, V139, P113, DOI 10.1023/A:1009754417131; LENIHAN JM, 1993, J VEG SCI, V4, P667, DOI 10.2307/3236132; Ludwig J. A., 1988, STAT ECOLOGY; Manel S, 1999, ECOL MODEL, V120, P337, DOI 10.1016/S0304-3800(99)00113-1; Manel S, 2000, J APPL ECOL, V37, P756, DOI 10.1046/j.1365-2664.2000.00537.x; MANEL S, IN PRESS J APPL ECOL; Manly BFJ, 1997, RANDOMIZATION BOOTST; MARGULES CR, 1987, OECOLOGIA, V71, P229, DOI 10.1007/BF00377288; MARSHALL P, 1995, FOREST CHRON, V71, P213; Mateu J, 1997, ENVIRON MANAGE, V21, P767, DOI 10.1007/s002679900066; McCullagh P., 1989, GEN LINEAR MODELS; Nelder JA, 1983, GEN LINEAR MODELS; NICHOLLS AO, 1989, BIOL CONSERV, V50, P51, DOI 10.1016/0006-3207(89)90005-0; OWEN JG, 1989, J BIOGEOGR, V16, P141, DOI 10.2307/2845088; PAUSAS JG, 1994, J VEG SCI, V5, P517, DOI 10.2307/3235978; PEREIRA JMC, 1991, PHOTOGRAMM ENG REM S, V57, P1475; POWER M, 1993, ECOL MODEL, V68, P33, DOI 10.1016/0304-3800(93)90106-3; Pregibon D., 1980, APPLIED STATISTICS, V29, P15, DOI 10.2307/2346405; SAKAMOTO Y, 1988, AKAIKE INFORMATION C; Scott JM, 2002, PREDICTING SPECIES O; VINCENT PJ, 1983, J BIOGEOGR, V10, P153, DOI 10.2307/2844625; Wohlgemuth T, 1998, BIODIVERS CONSERV, V7, P159, DOI 10.1023/A:1008880317661; YEE TW, 1991, J VEG SCI, V2, P587, DOI 10.2307/3236170; Zar JH, 1996, BIOSTATISTICAL ANAL; ZIMMERMANN F, 2002, PREDICTING SPECIES O; Zimmermann NE, 1999, J VEG SCI, V10, P469, DOI 10.2307/3237182	81	597	650	26	200	ELSEVIER SCIENCE BV	AMSTERDAM	PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS	0304-3800			ECOL MODEL	Ecol. Model.	NOV 30	2002	157	2-3					89	100	PII S0304-3800(02)00204-1	10.1016/S0304-3800(02)00204-1		12	Ecology	Environmental Sciences & Ecology	615JD	WOS:000179241300001		
J	Ambler, G; Brady, AR; Royston, P				Ambler, G; Brady, AR; Royston, P			Simplifying a prognostic model: a simulation study based on clinical data	STATISTICS IN MEDICINE			English	Article; Proceedings Paper	22nd Annual Meeting of the International-Society-for-Clinical-Biostatistics	AUG 19-23, 2001	STOCKHOLM, SWEDEN	Int Soc Cliln Biostat		prognostic models; variable selection; penalisation; lassos; ROC; AIC	LOGISTIC-REGRESSION ANALYSIS; SMALL DATA SETS; FRACTIONAL POLYNOMIALS; SELECTION; PREDICTORS; CANCER; TRIALS	Prognostic models are designed to predict a clinical outcome in individuals or groups of individuals with a particular disease or condition. To avoid bias many researchers advocate the use of full models developed by prespecifying predictors. Variable selection is not employed and the resulting models may be large and complicated. In practice more parsimonious models that retain most of the prognostic information may be preferred. We investigate the effect on various performance measures, including mean square error and prognostic classification, of three methods for estimating full models (including penalized estimation and Tibshirani's lasso) and consider two methods (backwards elimination and a new proposal called stepdown) for simplifying full models. Simulation studies based on two medical data sets suggest that simplified models can be found that perform nearly as well as, or sometimes even better than, full models. Optimizing the Akaike information criterion appears to be appropriate for choosing the degree of simplification. Copyright (C) 2002 John Wiley Sons, Ltd.	Univ Coll, Dept Stat Sci, London WC1E 7HB, England; Intens Care Natl Audit & Res Ctr, London WC1H 9HR, England; MRC, Clin Trials Unit, London NW1 2DA, England	Ambler, G (reprint author), Univ Coll, Dept Stat Sci, 1-19 Torrington Pl, London WC1E 7HB, England.						ROYSTON P, 1994, APPL STAT-J ROY ST C, V43, P429, DOI 10.2307/2986270; HARRELL FE, 1988, J NATL CANCER I, V80, P1198, DOI 10.1093/jnci/80.15.1198; Ambler G, 2001, J STAT COMPUT SIM, V69, P89, DOI 10.1080/00949650108812083; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Sauerbrei W, 1999, J ROY STAT SOC A STA, V162, P71, DOI 10.1111/1467-985X.00122; HANLEY JA, 1982, RADIOLOGY, V143, P29; Altman DG, 2000, STAT MED, V19, P453, DOI 10.1002/(SICI)1097-0258(20000229)19:4<453::AID-SIM350>3.3.CO;2-X; Steyerberg EW, 1999, J CLIN EPIDEMIOL, V52, P935, DOI 10.1016/S0895-4356(99)00103-1; Brier G. W., 1950, MONTHLY WEATHER REV, V75, P1, DOI DOI 10.1175/1520-0493(1950)078<0001:V0FEIT>2.0.C0;2; LECESSIE S, 1992, APPL STAT-J ROY ST C, V41, P191; DURRLEMAN S, 1989, STAT MED, V8, P551, DOI 10.1002/sim.4780080504; GRAY RJ, 1992, J AM STAT ASSOC, V87, P942, DOI 10.2307/2290630; Harrell FE, 2001, REGRESSION MODELING; HARRELL FE, 1984, STAT MED, V3, P143, DOI 10.1002/sim.4780030207; MILLER ME, 1991, STAT MED, V10, P1213, DOI 10.1002/sim.4780100805; PEDUZZI P, 1996, J CLIN EPIDEMIOL, V49, P1503; Royston P, 2001, STAT NEERL, V55, P89, DOI 10.1111/1467-9574.00158; Sauerbrei W, 1999, APPL STAT, V48, P313; Schmoor C, 1996, STAT MED, V15, P263, DOI 10.1002/(SICI)1097-0258(19960215)15:3<263::AID-SIM165>3.0.CO;2-K; SPIEGELHALTER DJ, 1986, STAT MED, V5, P421, DOI 10.1002/sim.4780050506; *STAT, 1999, STAT STAT SOFTW REL; Steyerberg EW, 2000, STAT MED, V19, P1059, DOI 10.1002/(SICI)1097-0258(20000430)19:8<1059::AID-SIM412>3.3.CO;2-S; Powell JT, 1998, LANCET, V352, P1649; VERWEIJ PJM, 1994, STAT MED, V13, P2427, DOI 10.1002/sim.4780132307; WYATT JC, 1995, BRIT MED J, V311, P1539	25	45	47	1	6	JOHN WILEY & SONS LTD	W SUSSEX	BAFFINS LANE CHICHESTER, W SUSSEX PO19 1UD, ENGLAND	0277-6715			STAT MED	Stat. Med.	DEC 30	2002	21	24					3803	3822		10.1002/sim.1422		20	Mathematical & Computational Biology; Public, Environmental & Occupational Health; Medical Informatics; Medicine, Research & Experimental; Statistics & Probability	Mathematical & Computational Biology; Public, Environmental & Occupational Health; Medical Informatics; Research & Experimental Medicine; Mathematics	629FK	WOS:000180039600005	12483768	
J	Wand, MP				Wand, MP			Smoothing and mixed models	COMPUTATIONAL STATISTICS			English	Article; Proceedings Paper	Euroworkshop on Statistical Modelling	NOV 01-04, 2001	BERNRIED, GERMANY			best prediction; generalised linear mixed models; nonparametric regression; kriging; maximum likelihood; variance components; restricted maximum likelihood	SPLINE MODELS; VARIABLE SELECTION; REGRESSION-MODELS; CURVES; INFERENCE; PENALTIES; ERRORS	Smoothing methods that use basis functions with penalisation can be formulated as maximum likelihood estimators and best predictors in a mixed model framework. Such connections are at least a quarter of a century old but, perhaps with the advent of mixed model software, have led to a paradigm shift in the field of smoothing. The reason is that most, perhaps all, models involving smoothing can be expressed as a mixed model and hence enjoy the benefit of the growing body of methodology and software for general mixed model analysis. The handling of other complications such as clustering, missing data and measurement error is generally quite straightforward with mixed model representations of smoothing.	Harvard Univ, Sch Publ Hlth, Dept Biostat, Boston, MA 02115 USA	Wand, MP (reprint author), Harvard Univ, Sch Publ Hlth, Dept Biostat, 665 Huntington Ave, Boston, MA 02115 USA.						BRESLOW NE, 1993, J AM STAT ASSOC, V88, P9, DOI 10.2307/2290687; Lin XH, 1999, J ROY STAT SOC B, V61, P381, DOI 10.1111/1467-9868.00183; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Antoniadis A, 2001, J AM STAT ASSOC, V96, P939, DOI 10.1198/016214501753208942; LANGE KL, 1989, J AM STAT ASSOC, V84, P881, DOI 10.2307/2290063; WECKER WE, 1983, J AM STAT ASSOC, V78, P81; Shively TS, 1999, J AM STAT ASSOC, V94, P777, DOI 10.2307/2669990; JOHNSON ME, 1990, J STAT PLAN INFER, V26, P131, DOI 10.1016/0378-3758(90)90122-B; Hastie T, 2000, STAT SCI, V15, P196; Eilers PHC, 1996, STAT SCI, V11, P89, DOI 10.1214/ss/1038425655; Chaudhuri P, 1999, J AM STAT ASSOC, V94, P807, DOI 10.2307/2669996; Verbyla AP, 1999, J ROY STAT SOC C-APP, V48, P269, DOI 10.1111/1467-9876.00154; HASTIE T, 1993, J ROY STAT SOC B MET, V55, P757; Brumback BA, 1999, J AM STAT ASSOC, V94, P794, DOI 10.2307/2669991; Brumback BA, 1998, J AM STAT ASSOC, V93, P961, DOI 10.2307/2669837; CAI T, 2002, IN PRESS J COMPUTATI, V11; Carroll R, 1995, MEASUREMENT ERROR NO; Casella G., 1990, STAT INFERENCE; CHEN ZH, 1993, J ROY STAT SOC B MET, V55, P473; Coull BA, 2001, BIOMETRICS, V57, P539, DOI 10.1111/j.0006-341X.2001.00539.x; Cressie N., 1993, STAT SPATIAL DATA; DIGGLE PJ, 1997, LECT NOTES STAT, V122, P387; Diggle PJ, 1995, ANAL LONGITUDINAL DA; Draper N. R., 1998, APPL REGRESSION ANAL; French JL, 2001, J AM STAT ASSOC, V96, P1285; FRENCH JL, 2002, IN PRESS BIOSTATISTI; Fuller W. A., 1987, MEASUREMENT ERROR MO; Fung WK, 2002, J ROY STAT SOC B, V64, P565, DOI 10.1111/1467-9868.00351; GANGULI B, 2002, UNPUB ADDITIVE MODEL; Gelman A., 1995, BAYESIAN DATA ANAL; Gilks W. R., 1996, MARKOV CHAIN MONTE C; GRAY RJ, 1994, BIOMETRICS, V50, P640, DOI 10.2307/2532779; GREEN PJ, 1985, BIOMETRIKA, V72, P523; GREEN PJ, 1987, INT STAT REV, V55, P245, DOI 10.2307/1403404; Hastie T, 1996, J ROY STAT SOC B MET, V58, P379; Hastie T., 1990, GENERALIZED ADDITIVE; HUBER P, 1983, ANN MATH STAT, V35, P73; IBRAHIM JG, 1990, J AM STAT ASSOC, V85, P765, DOI 10.2307/2290013; Ibrahim JG, 2001, BIOMETRIKA, V88, P551, DOI 10.1093/biomet/88.2.551; James GM, 2001, J ROY STAT SOC B, V63, P533, DOI 10.1111/1467-9868.00297; James GM, 2000, BIOMETRIKA, V87, P587, DOI 10.1093/biomet/87.3.587; KAMMANN EE, 2002, APPL STAT, V52, P1; KAMMANN EE, 2002, UNPUB ROBUSTNESS GEN; Ke CL, 2001, J AM STAT ASSOC, V96, P1272, DOI 10.1198/016214501753381913; KELLY C, 1990, BIOMETRICS, V46, P1071, DOI 10.2307/2532449; LAIRD NM, 1982, BIOMETRICS, V38, P963, DOI 10.2307/2529876; Little Roderick, 1987, STAT ANAL MISSING DA; MCCULLOCH CE, 2000, GENERALIZED LINEAR M; NGO L, 2002, SMOOTHING MIXED MODE; Nychka D., 1998, CASE STUDIES ENV STA, P159; Nychka D., 1998, CASE STUDIES ENV STA, P51; Nychka D. W, 2000, SMOOTHING REGRESSION; OSULLIVAN F, 1988, SIAM J SCI STAT COMP, V9, P363, DOI 10.1137/0909024; OConnell MA, 1997, J COMPUT GRAPH STAT, V6, P224, DOI 10.2307/1390932; Parker R.L., 1985, J ROYAL STAT SOC B, V47, P40; PATTERSO.HD, 1971, BIOMETRIKA, V58, P545, DOI 10.2307/2334389; Pinheiro J. C., 2000, MIXED EFFECTS MODELS; Robinson G. K., 1991, STAT SCI, V6, P15, DOI DOI 10.1214/SS/1177011926; Rousseeuw P. J., 1987, ROBUST REGRESSION OU; RUPPERT D, 2002, IN PRESS J COMPUTATI; Ruppert D., 2003, SEMIPARAMETRIC REGRE; Ruppert D, 2000, AUST NZ J STAT, V42, P205, DOI 10.1111/1467-842X.00119; Schafer JL, 1997, ANAL INCOMPLETE MULT; Searle S. R., 1992, VARIANCE COMPONENTS; Speed TP, 1991, STAT SCI, V6, P42, DOI 10.1214/ss/1177011930; Stein M. L., 1999, INTERPOLATION SPATIA; VERBYLA AP, 1994, 17 INT BIOM C HAM AU, P177; WAHBA G, 1978, J ROY STAT SOC B MET, V40, P364; Wahba G., 1990, SPLINE MODELS OBSERV; WAHBA G, 1986, P 18 S INT, P75; Wang YD, 1998, J AM STAT ASSOC, V93, P341, DOI 10.2307/2669630; Wang YD, 1998, J ROY STAT SOC B, V60, P159, DOI 10.1111/1467-9868.00115; WELSH AH, 1997, HDB STAT, V15	73	121	121	5	18	PHYSICA-VERLAG GMBH & CO	HEIDELBERG	TIERGARTENSTRASSE 17, 69121 HEIDELBERG, GERMANY	0943-4062			COMPUTATION STAT	Comput. Stat.		2003	18	2					223	249				27	Statistics & Probability	Mathematics	689FR	WOS:000183481400005		
B	He, X; Kim, MO		Dutter, R; Filzmoser, P; Gather, U; Rousseeuw, PJ		He, X; Kim, MO			On marginal estimation in a semiparametric model for longitudinal data with time-independent covariates	DEVELOPMENTS IN ROBUST STATISTICS			English	Proceedings Paper	International Conference on Robust Statistics (ICOR 2001)	JUL 23-27, 2001	VORAU, AUSTRIA	European Commiss			PARTLY LINEAR-MODEL; SPLINES; SELECTION	We consider M-estimators for a class of semiparametric mixed-effect models without time-dependent covariates and show that the simple marginal estimation method is generally better than the same M-estimator applied to the de-correlated response based on a known or estimated covariance matrix for each subject.	Univ Illinois, Dept Stat, Champaign, IL 61820 USA	He, X (reprint author), Univ Illinois, Dept Stat, Champaign, IL 61820 USA.						Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; He XM, 1998, J ROY STAT SOC B, V60, P537, DOI 10.1111/1467-9868.00138; CHEN H, 1988, ANN STAT, V16, P136, DOI 10.1214/aos/1176350695; HE X, 2001, MEDIAN REGRESSION LO; HE X, 2001, IN PRESS BIOMETRIKA; He X., 1994, J NONPARAMETR STAT, V3, P299, DOI DOI 10.1080/10485259408832589; He XM, 2001, IEEE SIGNAL PROC LET, V8, P137; He XM, 1996, J MULTIVARIATE ANAL, V58, P162, DOI 10.1006/jmva.1996.0045; HUGGINS RM, 1993, BIOMETRICS, V49, P715, DOI 10.2307/2532192; Jung SH, 1996, J AM STAT ASSOC, V91, P251, DOI 10.2307/2291402; Portnoy S, 1997, ANN STAT, V25, P414; Richardson AM, 1997, J AM STAT ASSOC, V92, P154, DOI 10.2307/2291459; Ruppert D, 2000, AUST NZ J STAT, V42, P205, DOI 10.1111/1467-842X.00119; Schumaker L.L., 1981, SPLINE FUNCTIONS; SPECKMAN P, 1988, J ROY STAT SOC B MET, V50, P413; ZEGER SL, 1994, BIOMETRICS, V50, P689, DOI 10.2307/2532783; Zhang D, 1998, J AM STAT ASSOC, V93, P710	17	0	0	0	0	PHYSICA-VERLAG GMBH & CO	HEIDELBERG	TIERGARTENSTR 17, D-69121 HEIDELBERG, GERMANY			3-7908-1518-7				2003							160	168				9	Statistics & Probability	Mathematics	BV74W	WOS:000179943000013		
B	Li, RZ		Pham, H; Lu, MW		Li, RZ			Model selection for analysis of uniform design and computer experiment	EIGHTH ISSAT INTERNATIONAL CONFERENCE ON RELIABILITY AND QUALITY IN DESIGN, PROCEEDINGS			English	Proceedings Paper	8th ISSAT Conference on Reliability and Quality in Design	AUG 07-09, 2002	Anaheim, CA	ISSAT		computer experiment design; penalized least squares; uniform design		In this paper, a new variable selection procedure is introduced for the analysis of uniform design and computer experiment. The new procedure is distinguished from the traditional ones in such a way that it deletes insignificant variables and estimates the coefficients of significant variables simultaneously. The new procedure has an oracle property ([5]). It is better than the best subset variable selection in terms of computational cost and model stability. It is superior to the stepwise regression because it does not ignore stochastic errors during the course of selecting variables. The proposed procedure is illustrated by two examples, one is a typical example of uniform design, and the other one is a classical example for computer experiment.	Penn State Univ, Dept Stat, University Pk, PA 16802 USA	Li, RZ (reprint author), Penn State Univ, Dept Stat, University Pk, PA 16802 USA.						AN J, 2001, IN PRESS J COMPLEXIT, V17; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Antoniadis A, 2001, J AM STAT ASSOC, V96, P939, DOI 10.1198/016214501753208942; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; [Anonymous], 1996, HANDB STAT, DOI 10.1016/S0169-7161(96)13011-X; CRAVEN P, 1979, NUMER MATH, V31, P377; Breiman L, 1996, ANN STAT, V24, P2350; FAGN KT, 1994, UNIFORM DESIGN UNIFO; FANG K. T., 1994, APPL NUMBER THEORETI; Fang Kai-tai, 1980, Acta Mathematicae Applacatae Sinica, V3; FANG KT, 2001, HDB STAT; FANG KT, 2000, UNPUB CASE STUDIES C; LI R, 2002, IN PRESS STAT PROBAB; LI R, 2002, MODEL SELECTION ANAL; Matheron G., 1963, ECON GEOL, V58, P1246, DOI DOI 10.2113/GSECONGEO.58.8.1246; MORRIS MD, 1993, TECHNOMETRICS, V35, P243, DOI 10.2307/1269517; Rao C. R., 1973, LINEAR STAT INFERENC; Sacks J., 1989, STAT SCI, V4, P409, DOI DOI 10.1214/SS/1177012413; WORLEY BA, 1987, ORN0628 NAT TECHN IN	19	0	0	0	1	INT SOC SCI APPL TECHNOL	PISCATAWAY	36 CARRIAGE DR, PISCATAWAY, NJ 08854 USA							2003							240	244				5	Engineering, Industrial	Engineering	BAC64	WOS:000221560900050		
J	Segal, MR; Dahlquist, KD; Conklin, BR				Segal, MR; Dahlquist, KD; Conklin, BR			Regression approaches for microarray data analysis	JOURNAL OF COMPUTATIONAL BIOLOGY			English	Article						cardiomyopathy; gene harvesting; least angle regression; microarray; support vector machine	GENE-EXPRESSION DATA; CONDITIONAL EXPRESSION; G(I)-COUPLED RECEPTOR; MODEL SELECTION; CLASSIFICATION; CANCER; SAMPLES; LASSO	A variety of new procedures have been devised to handle the two-sample comparison (e.g., tumor versus normal tissue) of gene expression values as measured with microarrays. Such new methods are required in part because of some defining characteristics of microarray-based studies: (i) the very large number of genes contributing expression measures which far exceeds the number of samples (observations) available and (ii) the fact that by virtue of pathway/network relationships, the gene expression measures tend to be highly correlated. These concerns are exacerbated in the regression setting, where the objective is to relate gene expression, simultaneously for multiple genes, to some external outcome or phenotype. Correspondingly, several methods have been recently proposed for addressing these issues. We briefly critique some of these methods prior to a detailed evaluation of gene harvesting. This reveals that gene harvesting, without additional constraints, can yield artifactual solutions. Results obtained employing such constraints motivate the use of regularized regression procedures such as the lasso, least angle regression, and support vector machines. Model selection and solution multiplicity issues are also discussed. The methods are evaluated using a microarray-based study of cardiomyopathy in transgenic mice.	Univ Calif San Francisco, Dept Epidemiol & Biostat, San Francisco, CA 94143 USA; Univ Calif San Francisco, Gladstone Inst Cardiovasc Dis, San Francisco, CA 94143 USA; Univ Calif San Francisco, Cardiovasc Res Inst, San Francisco, CA 94143 USA	Segal, MR (reprint author), Univ Calif San Francisco, Dept Epidemiol & Biostat, 500 Parnassus Ave,MU 420-W, San Francisco, CA 94143 USA.	mark@biostat.ucsf.edu					Akaike H., 1973, 2 INT S INF THEOR, P267; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324; Tusher VG, 2001, P NATL ACAD SCI USA, V98, P5116, DOI 10.1073/pnas.091062498; Furey TS, 2000, BIOINFORMATICS, V16, P906, DOI 10.1093/bioinformatics/16.10.906; Golub TR, 1999, SCIENCE, V286, P531, DOI 10.1126/science.286.5439.531; Dudoit S, 2002, J AM STAT ASSOC, V97, P77, DOI 10.1198/016214502753479248; Breiman L, 2001, STAT SCI, V16, P199, DOI 10.1214/ss/1009213726; HOERL AE, 1970, TECHNOMETRICS, V12, P55; Guyon I, 2002, MACH LEARN, V46, P389, DOI 10.1023/A:1012487302797; Redfern CH, 2000, P NATL ACAD SCI USA, V97, P4826, DOI 10.1073/pnas.97.9.4826; Li C, 2001, P NATL ACAD SCI USA, V98, P31, DOI 10.1073/pnas.011404098; Eisen MB, 1998, P NATL ACAD SCI USA, V95, P14863, DOI 10.1073/pnas.95.25.14863; SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136; MALLOWS CL, 1973, TECHNOMETRICS, V15, P661, DOI 10.2307/1267380; West M, 2001, P NATL ACAD SCI USA, V98, P11462, DOI 10.1073/pnas.201162998; Brown MPS, 2000, P NATL ACAD SCI USA, V97, P262, DOI 10.1073/pnas.97.1.262; Newton MA, 2001, J COMPUT BIOL, V8, P37, DOI 10.1089/106652701300099074; Breiman L., 1984, CLASSIFICATION REGRE; Cristianini N., 2000, INTRO SUPPORT VECTOR; EFRON B, 2002, UNPUB LEAST ANGLE RE; FRIEDMAN JH, 1991, ANN STAT, V19, P1, DOI 10.1214/aos/1176347963; Goryachev AB, 2001, J COMPUT BIOL, V8, P443, DOI 10.1089/106652701752236232; GUARDAVACCARO D, 1995, CELL GROWTH DIFFER, V6, P159; Hastie T, 2001, GENOME BIOL, V2; Hastie T., 2001, ELEMENTS STAT LEARNI; KELES S, 2002, UNPUB IDENTIFICATION; KENT C, 1999, TRENDS BIOCHEM SCI, V24, P127; Kim S, 2002, J COMPUT BIOL, V9, P127, DOI 10.1089/10665270252833226; Kooperberg C, 2002, J COMPUT BIOL, V9, P55, DOI 10.1089/10665270252833190; LEE Y, 2002, UNPUB CLASSIFICATION; Li HZ, 2001, GENOME BIOL, V2; Nelson DP, 2002, ANN THORAC SURG, V73, P156, DOI 10.1016/S0003-4975(01)03303-3; Osborne MR, 2000, J COMPUT GRAPH STAT, V9, P319, DOI 10.2307/1390657; Redfern CH, 1999, NAT BIOTECHNOL, V17, P165; Tibshirani R, 1999, J ROY STAT SOC B, V61, P529, DOI 10.1111/1467-9868.00191; Vapnik V., 1998, STAT LEARNING THEORY; Westphal CH, 1999, CELL, V96, P689, DOI 10.1016/S0092-8674(00)80579-6; Ye JM, 1998, J AM STAT ASSOC, V93, P120, DOI 10.2307/2669609; Zhang HP, 2001, P NATL ACAD SCI USA, V98, P6730, DOI 10.1073/pnas.111153698	40	65	70	1	6	MARY ANN LIEBERT INC PUBL	LARCHMONT	2 MADISON AVENUE, LARCHMONT, NY 10538 USA	1066-5277			J COMPUT BIOL	J. Comput. Biol.		2003	10	6					961	980		10.1089/106652703322756177		20	Biochemical Research Methods; Biotechnology & Applied Microbiology; Computer Science, Interdisciplinary Applications; Mathematical & Computational Biology; Statistics & Probability	Biochemistry & Molecular Biology; Biotechnology & Applied Microbiology; Computer Science; Mathematical & Computational Biology; Mathematics	767MM	WOS:000188455100009	14980020	
B	Figueiredo, MAT		Dension, DD; Hansen, MH; Holmes, CC; Mallick, B; Yu, B		Figueiredo, MAT			Adaptive sparse regression	NONLINEAR ESTIMATION AND CLASSIFICATION	Lecture Notes in Statistics		English	Proceedings Paper	Workshop on Nonlinear Estimation and Classification	MAR, 2001	Berkeley, CA				VARIABLE SELECTION; APPROXIMATION	In sparse regression, the goal is to obtain an estimate of the regression coefficients in which several of them are set exactly to zero. Sparseness is a desirable feature in regression problems, for several reasons. For example, in linear regression, sparse models are interpretable, that is, we find which variables are relevant; in kernel-based methods, like in support vector regression, sparseness leads to regression equations involving only a subset of the learning data. In all approaches to sparse regression, it is necessary to estimate parameters which will ultimately control the degree of sparseness of the obtained solution. This commonly involves cross-validation methods which waste learning data and are time consuming. In this chapter we present a sparseness inducing prior which does not involve any (hyper)parameters that need to be adjusted or estimated. Experiments with several publicly available benchmark data sets show that the proposed approach yields state-of-the-art performance. In particular, our method outperforms support vector regression and performs competitively with the best alternative techniques, both in terms of error rates and sparseness, although it involves no tuning or adjusting of sparseness-controlling hyper-parameters.	Univ Tecn Lisboa, Inst Telecommun, Inst Super Tecn, P-1049001 Lisbon, Portugal	Figueiredo, MAT (reprint author), Univ Tecn Lisboa, Inst Telecommun, Inst Super Tecn, P-1049001 Lisbon, Portugal.	mtf@lx.it.pt	Figueiredo, Mario/C-5428-2008	Figueiredo, Mario/0000-0002-0970-7745			WILLIAMS PM, 1995, NEURAL COMPUT, V7, P117, DOI 10.1162/neco.1995.7.1.117; Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0; POGGIO T, 1990, P IEEE, V78, P1481, DOI 10.1109/5.58326; Williams CKI, 1998, IEEE T PATTERN ANAL, V20, P1342, DOI 10.1109/34.735807; Osborne MR, 2000, IMA J NUMER ANAL, V20, P389, DOI 10.1093/imanum/20.3.389; Lewicki MS, 2000, NEURAL COMPUT, V12, P337, DOI 10.1162/089976600300015826; DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425; Berger J. O., 1980, STAT DECISION THEORY; Bishop C., 2000, P 16 C UNC ART INT, P46; Bishop C. M., 1995, NEURAL NETWORKS PATT; Chen Y, 1998, NONCON OPTIM ITS APP, V20, P1; Cherkassky V. S., 1998, LEARNING DATA CONCEP; Cristianini N., 2000, SUPPORT VECTOR MACHI, V1st; FIGUEIREDO M, 2002, ADV NEURAL INFORMATI, V14; Figueiredo MAT, 2001, IEEE T IMAGE PROCESS, V10, P1322, DOI 10.1109/83.941856; HOERL AE, 1970, TECHNOMETRICS, V12, P55, DOI DOI 10.1080/00401706.1970.10488634; HOLMES CC, 1999, BAYESIAN STAT, V6; KIMELDOR.GS, 1970, ANN MATH STAT, V41, P495, DOI 10.1214/aoms/1177697089; Lange K. L., 1993, J COMPUT GRAPH STAT, V2, P175, DOI 10.2307/1390698; MacKay DJC, 1996, FUND THEOR, V62, P221; McCullagh P., 1989, GENERALIZED LINEAR M, VSecond; Neal R. M., 1996, BAYESIAN LEARNING NE; Osborne MR, 1998, COMP SCI STAT, V30, P44; Poggio T, 1998, NEURAL COMPUT, V10, P1445, DOI 10.1162/089976698300017250; Ripley B. D., 1996, PATTERN RECOGNITION; Smith M, 1996, J ECONOMETRICS, V75, P317, DOI 10.1016/0304-4076(95)01763-1; Tibshirani R, 1996, J ROYAL STAT SOC B, V58; Tipping ME, 2000, ADV NEUR IN, V12, P652; Vapnik V., 1998, STAT LEARNING THEORY; WILLIAMS CKI, 2001, ADV NEURAL INFORMATI, V13; WILLIAMS CKI, 1998, LEARNING GRAPHICAL M	31	7	7	0	0	SPRINGER	NEW YORK	233 SPRING STREET, NEW YORK, NY 10013, UNITED STATES			0-387-95471-6	LECT NOTES STAT			2003	171						237	247				11	Computer Science, Artificial Intelligence; Mathematics, Applied; Statistics & Probability	Computer Science; Mathematics	BY75J	WOS:000189454900012		
B	van de Geer, SA		Akritas, MG; Politis, DN		van de Geer, SA			Adaptive quantile regression	RECENT ADVANCES AND TRENDS IN NONPARAMETRIC STATISTICS			English	Proceedings Paper	International Conference on Recent Advances and Trends in NonParametric Statistics	JUL 15-19, 2002	IRAKLION, GREECE				SMOOTHING SPLINES; SHRINKAGE	In this paper, the estimation of a regression quantile function is studied. We consider a linear regression model with dimension m, where m, may be as large as the number of observations n. As penalty on the quantile regression estimator, the sum of the absolute value if its coefficients is used. We show that the estimator adapts to the unknown smoothness of the underlying quantile regression function, as well as to unknown identifiablity properties.	Leiden Univ, Math Inst, NL-2300 RA Leiden, Netherlands	van de Geer, SA (reprint author), Leiden Univ, Math Inst, POB 9512, NL-2300 RA Leiden, Netherlands.						Akaike H., 1973, P 2 INT S INF THEOR, P267; KOENKER R, 1978, ECONOMETRICA, V46, P33, DOI 10.2307/1913643; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Portnoy S, 1997, STAT SCI, V12, P279; DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425; SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136; DONOHO DL, 1995, IEEE T INFORM THEORY, V41, P613, DOI 10.1109/18.382009; EDMUNDS DE, 1992, P LOND MATH SOC, V64, P153; Hastie T., 2001, ELEMENTS STAT LEARNI; KOENKER R, 1994, BIOMETRIKA, V81, P673; KOENKER R., 1992, L1 STAT ANAL RELATED, P217; LEDOUX M., 1991, PROBABILITY BANACH S; Loubes J.-M., 2002, STAT NEERL, V56, P453; Massart P., 2000, ANN FAC SCI TOULOUSE, V9, P245; PINKUS A, 1985, N WIDTHS APPROXIAMAT; Portnoy S, 1997, ANN STAT, V25, P414; van de Geer S, 2000, EMPIRICAL PROCESSES; Van der Vaart A., 1996, WEAK CONVERGENCE EMP; van de Geer S, 2002, J STAT PLAN INFER, V108, P55, DOI 10.1016/S0378-3758(02)00270-7	19	5	5	0	0	ELSEVIER SCIENCE BV	AMSTERDAM	SARA BURGERHARTSTRAAT 25, PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS			0-444-51378-7				2003							235	250		10.1016/B978-044451378-6/50016-8		16	Statistics & Probability	Mathematics	BY29E	WOS:000188793600016		
J	Sugiyama, M; Muller, KR				Sugiyama, M; Muller, KR			The subspace information criterion for infinite dimensional hypothesis spaces	JOURNAL OF MACHINE LEARNING RESEARCH			English	Article						generalization error; model selection; subspace information criterion; cross-validation; kernel regression; reproducing kernel Hilbert space; fulite sample statistics; Gaussian processes; unbiased estimators	SUPPORT VECTOR MACHINES; IMAGE-RESTORATION FILTERS; STOCHASTIC COMPLEXITY; MODEL SELECTION; CROSS-VALIDATION; REGRESSION; REGULARIZATION; ALGORITHMS; SHRINKAGE; PARAMETER	A central problem in learning is selection of an appropriate model. This is typically done by estimating the unknown generalization errors of a set of models to be selected from and then choosing the model with minimal generalization error estimate. In this article, we discuss the problem of model selection and generalization error estimation in the context of kernel regression models, e.g., kernel ridge regression, kernel subset regression or Gaussian process regression. Previously, a non-asymptotic generalization error estimator called the subspace information criterion (SIC) was proposed, that could be successfully applied to finite dimensional subspace models. SIC is an unbiased estimator of the generalization error for the finite sample case under the conditions that the learning target function belongs to a specified reproducing kernel Hilbert space (RKHS) H and the reproducing kernels centered on training sample points span the whole space R. These conditions hold only if dim H less than or equal to l, where l (< infinity) is the number of training examples. Therefore, SIC could be applied only to finite dimensional RKHSs. In this paper, we extend the range of applicability of SIC, and show that even if the reproducing kernels centered on training sample points do not span the whole space R, SIC is an unbiased estimator of an essential part of the generalization error. Our extension allows the use of any RKHSs including infinite dimensional ones, i.e., richer function classes commonly used in Gaussian processes, support vector machines or boosting. We further show that when the kernel matrix is invertible, SIC can be expressed in a much simpler form, making its computation highly efficient. In computer simulations on ridge parameter selection with real and artificial data sets, SIC is compared favorably with other standard model selection techniques for instance leave-one-out cross-validation or an empirical Bayesian method.	Tokyo Inst Technol, Dept Comp Sci, Meguro Ku, Tokyo 1528552, Japan; Fraunhofer FIRST, IDA, D-12489 Berlin, Germany; Univ Potsdam, Dept Comp Sci, D-14482 Potsdam, Germany	Sugiyama, M (reprint author), Tokyo Inst Technol, Dept Comp Sci, Meguro Ku, 2-12-1 Ookayama, Tokyo 1528552, Japan.	SUGI@OG.CS.TITECH.AC.JP; KLAUS@FIRST.FHG.DE	Muller, Klaus/C-3196-2013				Akaike H, 1980, BAYESIAN STAT, P141; AKAIKE H, 1974, IEEE T AUTOMAT CONTR, VAC19, P716, DOI 10.1109/TAC.1974.1100705; Albert A., 1972, REGRESSION MOORE PEN; AMARI S, 1992, NEURAL COMPUT, V4, P605, DOI 10.1162/neco.1992.4.4.605; MACKAY DJC, 1992, NEURAL COMPUT, V4, P415, DOI 10.1162/neco.1992.4.3.415; NISHII R, 1984, ANN STAT, V12, P758, DOI 10.1214/aos/1176346522; WILLIAMS PM, 1995, NEURAL COMPUT, V7, P117, DOI 10.1162/neco.1995.7.1.117; GEMAN S, 1992, NEURAL COMPUT, V4, P1, DOI 10.1162/neco.1992.4.1.1; Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Muller KR, 2001, IEEE T NEURAL NETWOR, V12, P181, DOI 10.1109/72.914517; Vapnik V, 2000, NEURAL COMPUT, V12, P2013, DOI 10.1162/089976600300015042; SHIBATA R, 1981, BIOMETRIKA, V68, P45; Williams CKI, 1998, NATO ADV SCI I D-BEH, V89, P599; Scholkopf B, 2000, NEURAL COMPUT, V12, P1207, DOI 10.1162/089976600300015565; RISSANEN J, 1978, AUTOMATICA, V14, P465, DOI 10.1016/0005-1098(78)90005-5; Burges CJC, 1998, DATA MIN KNOWL DISC, V2, P121, DOI 10.1023/A:1009715923555; DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425; SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136; Rissanen JJ, 1996, IEEE T INFORM THEORY, V42, P40, DOI 10.1109/18.481776; DONOHO DL, 1995, IEEE T INFORM THEORY, V41, P613, DOI 10.1109/18.382009; MALLOWS CL, 1973, TECHNOMETRICS, V15, P661, DOI 10.2307/1267380; CRAVEN P, 1979, NUMER MATH, V31, P377; MACKAY DJC, 1992, NEURAL COMPUT, V4, P590, DOI 10.1162/neco.1992.4.4.590; ARONSZAJN N, 1950, T AM MATH SOC, V68, P337, DOI DOI 10.2307/1990404; Bishop C. M., 1995, NEURAL NETWORKS PATT; BUNKE O, 1984, ANN STAT, V12, P1400, DOI 10.1214/aos/1176346800; Cherkassky V, 1999, IEEE T NEURAL NETWOR, V10, P1075, DOI 10.1109/72.788648; Cohn DA, 1996, J ARTIF INTELL RES, V4, P129; Cristianini N., 2000, INTRO SUPPORT VECTOR; Daubechies I., 1992, 10 LECT WAVELETS; Devroye L., 1996, APPL MATH, V31; Fedorov V. V., 1972, THEORY OPTIMAL EXPT; Fukumizu K, 2000, IEEE T NEURAL NETWOR, V11, P17, DOI 10.1109/72.822506; Gibbs M., 1997, EFFICIENT IMPLEMENTA; Gibbs M., 1997, THESIS CAMBRIDGE U; Girosi F, 1998, NEURAL COMPUT, V10, P1455, DOI 10.1162/089976698300017269; HENKEL RE, 1979, TESTS SIGNIFICANCE; Heskes T, 1998, NEURAL COMPUT, V10, P1425, DOI 10.1162/089976698300017232; Huber P. J., 1981, ROBUST STAT; Hunter J., 2000, RES LETT INFORMATION, V1, P25; Jain AK, 1988, ALGORITHMS CLUSTERIN; Jolliffe I. T., 1986, PRINCIPAL COMPONENT; KIMELDOR.GS, 1970, ANN MATH STAT, V41, P495, DOI 10.1214/aoms/1177697089; Kohonen T., 1995, SELF ORG MAPS; Konishi S, 1996, BIOMETRIKA, V83, P875, DOI 10.1093/biomet/83.4.875; Mallows C. L., 1964, CENTR REG M I MATH S; MURATA N, 1994, IEEE T NEURAL NETWOR, V5, P865, DOI 10.1109/72.329683; ORR MJL, 1996, INTRO RADICAL BASIS; Rasmussen C. E., 1996, DELVE MANUAL; RISSANEN J, 1987, J ROY STAT SOC B MET, V49, P223; Saunders C, 1998, P 15 INT C MACH LEAR, P515; Schatten R., 1970, NORM IDEALS COMPLETE; Scholkopf B., 1998, ADV KERNEL METHODS S; Scholkopf B., 2002, LEARNING KERNELS; Smola A. J., 2000, P 17 INT C MACH LEAR, P911; Smola AJ, 1998, NEURAL NETWORKS, V11, P637, DOI 10.1016/S0893-6080(98)00032-X; SUGIURA N, 1978, COMMUN STAT A-THEOR, V7, P13, DOI 10.1080/03610927808827599; Sugiyama M, 2002, MACH LEARN, V48, P25, DOI 10.1023/A:1013995402903; Sugiyama M, 2002, NEURAL NETWORKS, V15, P349, DOI 10.1016/S0893-6080(02)00022-9; Sugiyama M, 2000, NEURAL COMPUT, V12, P2909, DOI 10.1162/089976600300014773; Sugiyama M, 2001, NEURAL COMPUT, V13, P1863, DOI 10.1162/08997660152469387; Sugiyama M, 2002, SIGNAL PROCESS, V82, P1773, DOI 10.1016/S0165-1684(02)00339-0; Sugiyama M, 2001, IEICE T INF SYST, VE84D, P1249; Takeuchi K., 1976, MATH SCI, V153, P12; Tanaka A, 2002, IEICE T FUND ELECTR, VE85A, P1104; Tsuda K, 2002, IEEE T NEURAL NETWOR, V13, P70, DOI 10.1109/72.977272; Vapnik V., 1998, STAT LEARNING THEORY; Vapnik V.N., 1995, NATURE STAT LEARNING; Vapnik VN, 1982, ESTIMATION DEPENDENC; WAHBA H, 1990, SPLINE MODEL OBSERVA; Watanabe S, 2001, NEURAL COMPUT, V13, P899, DOI 10.1162/089976601300014402; Williams CKI, 1996, ADV NEUR IN, V8, P514; Yamanishi K, 1998, IEEE T INFORM THEORY, V44, P1424, DOI 10.1109/18.681319	74	1	1	0	4	MICROTOME PUBL	BROOKLINE	31 GIBBS ST, BROOKLINE, MA 02446 USA	1532-4435			J MACH LEARN RES	J. Mach. Learn. Res.	FEB 15	2003	3	2					323	359		10.1162/153244303765208412		37	Automation & Control Systems; Computer Science, Artificial Intelligence	Automation & Control Systems; Computer Science	671WT	WOS:000182488500006		
J	Boracchi, P; Biganzoli, E; Marubini, E				Boracchi, P; Biganzoli, E; Marubini, E			Joint modelling of cause-specific hazard functions with cubic splines: an application to a large series of breast cancer patients	COMPUTATIONAL STATISTICS & DATA ANALYSIS			English	Article						competing risks; hazard regression; GLMs; splines; breast cancer	CENSORED SURVIVAL-DATA; LOG-LINEAR MODELS; COMPETING-RISKS; REGRESSION-MODELS; FAILURE; EVENTS	The time of appearance of several kinds of relapses after a therapeutic intervention is of increasing interest in oncology. Typically, in breast cancer patients, events of clinical interest are intra-breast tumor recurrences and distant metastases, which act in a competitive way when considered as first failure. The evaluation of differential effects of clinical and biological variables on each event can improve the knowledge on the course of the disease and the targeting of future therapy. A simple tool for the joint smoothed estimation of cause-specific hazards functions and continuous covariate effects has been developed. Within the framework of generalized linear models with Poisson error, an extension of the piecewise exponential model is proposed, based on grouping follow-up times and continuous covariates. Interpolation of cause-specific hazards is obtained by resorting to cubic splines, which are piecewise polynomials of simple implementation with standard statistical software; their flexibility and smoothness are easily controlled by the number of knots and constraints on polynomial derivatives. The approach was applied to a data set of 2233 breast cancer patients treated with conservative surgery. It allowed modelling time-dependent and cause-specific effects of covariates on the hazard functions. (C) 2002 Published by Elsevier Science B.V.	Univ Milan, Ist Stat Med & Biometria, I-20122 Milan, Italy; Ist Nazl Studio & Cura Tumori, Unita Stat Med & Biometria, I-20133 Milan, Italy				boracchi, patrizia/0000-0001-8506-2005; Biganzoli, Elia/0000-0003-1202-5873			Agresti A, 1990, CATEGORICAL DATA ANA; AITKIN M, 1983, J AM STAT ASSOC, V78, P264, DOI 10.2307/2288622; AITKIN M, 1989, STAT MODELLING GLIM, P256; Akaike H., 1973, P 2 INT S INF THEOR, P267; AndersonSprecher R, 1996, J AM STAT ASSOC, V91, P276, DOI 10.2307/2291405; SMITH PL, 1979, AM STAT, V33, P57, DOI 10.2307/2683222; VERONESI U, 1995, J NATL CANCER I, V87, P19, DOI 10.1093/jnci/87.1.19; Denison DGT, 1998, J ROY STAT SOC B, V60, P333, DOI 10.1111/1467-9868.00128; Demicheli R, 1996, BREAST CANCER RES TR, V41, P177, DOI 10.1007/BF01807163; Harrell FE, 1996, STAT MED, V15, P361, DOI 10.1002/(SICI)1097-0258(19960229)15:4<361::AID-SIM168>3.0.CO;2-4; Biganzoli E, 1998, STAT MED, V17, P1169, DOI 10.1002/(SICI)1097-0258(19980530)17:10<1169::AID-SIM796>3.0.CO;2-D; BORACCHI P, 2001, IN PRESS STAT MED; DURRLEMAN S, 1989, STAT MED, V8, P551, DOI 10.1002/sim.4780080504; Elandt-Johnson R. C., 1980, SURVIVAL MODELS DATA; EUBANK RL, 1984, COMMUN STAT-THEOR M, V13, P433, DOI 10.1080/03610928408828695; FRIEDMAN JH, 1991, ANN STAT, V19, P1, DOI 10.1214/aos/1176347963; FRIEDMAN M, 1982, ANN STAT, V10, P101, DOI 10.1214/aos/1176345693; Gray R., 1996, J COMPUTATIONAL GRAP, V5, P190, DOI 10.2307/1390780; HERNDON JE, 1995, STAT MED, V14, P2119, DOI 10.1002/sim.4780141906; Hess KR, 1999, STAT MED, V18, P3075, DOI 10.1002/(SICI)1097-0258(19991130)18:22<3075::AID-SIM244>3.0.CO;2-6; HOLFORD TR, 1976, BIOMETRICS, V32, P587, DOI 10.2307/2529747; HOLFORD TR, 1980, BIOMETRICS, V36, P299, DOI 10.2307/2529982; Kay R, 1983, Stat Med, V2, P41, DOI 10.1002/sim.4780020106; KOOPERBERG C, 1995, J AM STAT ASSOC, V90, P78, DOI 10.2307/2291132; Kooperberg C, 1997, BIOMETRICS, V53, P1485, DOI 10.2307/2533514; KRAMAR A, 1987, STAT MED, V6, P785, DOI 10.1002/sim.4780060709; LAIRD N, 1981, J AM STAT ASSOC, V76, P231, DOI 10.2307/2287816; LARSON MG, 1984, BIOMETRICS, V40, P459, DOI 10.2307/2531398; Lindsey JK, 1999, J ROY STAT SOC D-STA, V48, P401, DOI 10.1111/1467-9884.00198; Lindstrom MJ, 1999, J COMPUT GRAPH STAT, V8, P333, DOI 10.2307/1390640; Linhart H., 1986, MODEL SELECTION; Marubinis E, 1995, ANAL SURVIVAL DATA C, P331; *MATHS INC, 1995, S PLUS US MAN V 3 3; MOESCHBE.ML, 1971, BIOMETRICS, V27, P909, DOI 10.2307/2528828; PRENTICE RL, 1978, BIOMETRICS, V34, P541, DOI 10.2307/2530374; Stone CJ, 1986, STAT SCI, V1, P312, DOI 10.1214/ss/1177013607; Tibshirani R, 1996, J ROY STAT B, V58, P385; VENABLES WN, 1999, STAT COMPLEMENTS MOD	38	21	21	0	2	ELSEVIER SCIENCE BV	AMSTERDAM	PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS	0167-9473			COMPUT STAT DATA AN	Comput. Stat. Data Anal.	FEB 28	2003	42	1-2					243	262	PII S0167-9473(02)00122-6	10.1016/S0167-9473(02)00122-6		20	Computer Science, Interdisciplinary Applications; Statistics & Probability	Computer Science; Mathematics	644MC	WOS:000180920900015		
J	Fu, WJJ				Fu, WJJ			Penalized estimating equations	BIOMETRICS			English	Article						collinearity; lasso; longitudinal studies; penalty; quasi-GCV	LONGITUDINAL DATA-ANALYSIS; NONORTHOGONAL PROBLEMS; VARIABLE SELECTION; RIDGE REGRESSION; AIR-POLLUTION; LASSO; ADMISSIONS; CHILDREN; ASTHMA	Penalty models-such as the ridge estimator, the Stein estimator, the bridge estimator, and the Lasso-have been proposed to deal with collinearity in regressions. The Lasso, for instance, has been applied to linear models, logistic regressions, Cox proportional hazard models, and neural networks. This article considers the bridge penalty model with penalty Sigma(j)\beta(j)\(gamma) for estimating equations in general and applies this penalty model to the generalized estimating equations (GEE) in longitudinal studies. The lack of joint likelihood in the GEE is overcome by the penalized estimating equations, in which no joint likelihood is required. The asymptotic results for the penalty estimator are provided. It is demonstrated, with a simulation and an application, that the penalized GEE potentially improves the performance of the GEE estimator, and enjoys the same properties as linear penalty models.	Michigan State Univ, Dept Epidemiol, E Lansing, MI 48823 USA	Fu, WJJ (reprint author), Michigan State Univ, Dept Epidemiol, 4660 S Hagadorn Rd,Suite 600, E Lansing, MI 48823 USA.						Fu WJJ, 1998, J COMPUT GRAPH STAT, V7, P397, DOI 10.2307/1390712; LIANG KY, 1986, BIOMETRIKA, V73, P13, DOI 10.1093/biomet/73.1.13; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; HOERL AE, 1970, TECHNOMETRICS, V12, P55; George EI, 2000, J AM STAT ASSOC, V95, P1304, DOI 10.2307/2669776; FRANK IE, 1993, TECHNOMETRICS, V35, P109, DOI 10.2307/1269656; Norris G, 1999, ENVIRON HEALTH PERSP, V107, P489, DOI 10.2307/3434632; Anderson HR, 1998, THORAX, V53, P842; CRAVEN P, 1979, NUMER MATH, V31, P377; Durrett R., 1991, PROBABILITY THEORY E; FU WJ, 2002, UNPUB NONLINEAR GCV; HOERL AE, 1970, TECHNOMETRICS, V12, P69, DOI 10.2307/1267352; MAK C, 1999, THESIS U TORONTO TOR; McCullagh P., 1989, GEN LINEAR MODELS; Osborne MR, 2000, J COMPUT GRAPH STAT, V9, P319, DOI 10.2307/1390657; Shao J., 1995, JACKKNIFE BOOTSTRAP; SUN X, 1999, THESIS U TORONTO TOR; Tibshirani R, 1997, STAT MED, V16, P385, DOI 10.1002/(SICI)1097-0258(19970228)16:4<385::AID-SIM380>3.0.CO;2-3; van der Vaart A. W., 1998, ASYMPTOTIC STAT; Wong GWK, 2001, CLIN EXP ALLERGY, V31, P565, DOI 10.1046/j.1365-2222.2001.01063.x; Yuan KH, 2000, ANN I STAT MATH, V52, P343, DOI 10.1023/A:1004122007440; ZEGER SL, 1986, BIOMETRICS, V42, P121, DOI 10.2307/2531248	22	27	27	1	4	BLACKWELL PUBLISHING LTD	OXFORD	9600 GARSINGTON RD, OXFORD OX4 2DG, OXON, ENGLAND	0006-341X			BIOMETRICS	Biometrics	MAR	2003	59	1					126	132		10.1111/1541-0420.00015		7	Biology; Mathematical & Computational Biology; Statistics & Probability	Life Sciences & Biomedicine - Other Topics; Mathematical & Computational Biology; Mathematics	663GG	WOS:000181997400015	12762449	
J	Huang, FC				Huang, FC			Prediction error property of the lasso estimator and its generalization	AUSTRALIAN & NEW ZEALAND JOURNAL OF STATISTICS			English	Article						admissibility; generalized lasso estimator; lasso-shrinkable condition; least squares estimator	RIDGE REGRESSION; SELECTION	The lasso procedure is an estimator-shrinkage and variable selection method. This paper shows that there always exists an interval of tuning parameter values such that the corresponding mean squared prediction error for the lasso estimator is smaller than for the ordinary least squares estimator. For an estimator satisfying some condition such as unbiasedness, the paper defines the corresponding generalized lasso estimator. Its mean squared prediction error is shown to be smaller than that of the estimator for values of the tuning parameter in some interval. This implies that all unbiased estimators are not admissible. Simulation results for five models support the theoretical results.	Deakin Univ, Sch Informat Technol, Burwood, Vic 3125, Australia	Huang, FC (reprint author), Deakin Univ, Sch Informat Technol, 221 Burwood Highway, Burwood, Vic 3125, Australia.	fuchun@deakin.edu.au					BREIMAN L, 1995, TECHNOMETRICS, V37, P373, DOI 10.2307/1269730; Fu WJJ, 1998, J COMPUT GRAPH STAT, V7, P397, DOI 10.2307/1390712; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Osborne MR, 2000, IMA J NUMER ANAL, V20, P389, DOI 10.1093/imanum/20.3.389; Hastie T., 2001, ELEMENTS STAT LEARNI; HOERL AE, 1970, TECHNOMETRICS, V12, P55, DOI DOI 10.1080/00401706.1970.10488634; Osborne MR, 2000, J COMPUT GRAPH STAT, V9, P319, DOI 10.2307/1390657; Roth V, 2001, LECT NOTES COMPUT SC, V2130, P339; THEOBALD CM, 1974, J ROY STAT SOC B MET, V36, P103	9	2	2	0	1	WILEY-BLACKWELL	HOBOKEN	111 RIVER ST, HOBOKEN 07030-5774, NJ USA	1369-1473	1467-842X		AUST NZ J STAT	Aust. N. Z. J. Stat.	JUN	2003	45	2					217	228		10.1111/1467-842X.00277		12	Statistics & Probability	Mathematics	681XV	WOS:000183061600009		
J	Devlin, B; Roeder, K; Wasserman, L				Devlin, B; Roeder, K; Wasserman, L			Analysis of multilocus models of association	GENETIC EPIDEMIOLOGY			English	Article						false discovery rate; gene-gene interactions; Lasso; model selection; multiple testing; regression model	FALSE DISCOVERY RATE; COMPLEX TRAITS; SELECTION; EPISTASIS; DISEASE; LINKAGE; LASSO; POWER	It is increasingly recognized that multiple genetic variants, within the same or different genes, combine to affect liability for many common diseases. Indeed, the variants may interact among themselves and with environmental factors. Thus realistic genetic/statistical models can include an extremely large number of parameters, and it is by no means obvious how to find the variants contributing to liability. For models of multiple candidate genes and their interactions, we prove that statistical inference can be based on controlling the false discovery rate (FDR), which is defined as the expected number of false rejections divided by the number of rejections. Controlling the FDR automatically controls the overall error rate in the special case that all the null hypotheses are true. So do more standard methods such as Bonferroni correction. However, when some null hypotheses are false, the goals of Bonferroni and FDR differ, and FDR will have better power. Model selection procedures, such as forward stepwise regression, are often used to choose important predictors for complex models. By analysis of simulations of such models, we compare a computationally efficient form of forward stepwise regression against the FDR methods. We show that model selection includes numerous genetic variants having no impact on the trait, whereas FDR maintains a false-positive rate very close to the nominal rate. With good control over false positives and better power than Bonferroni, the FDR-based methods we introduce present a viable means of evaluating complex, multivariate genetic models. Naturally, as for any method seeking to explore complex genetic models, the power of the methods is limited by sample size and model complexity. (C) 2003 Wiley-Liss, Inc.	Univ Pittsburgh, Sch Med, Dept Psychiat, Pittsburgh, PA 15213 USA; Carnegie Mellon Univ, Dept Stat, Pittsburgh, PA 15213 USA	Devlin, B (reprint author), Univ Pittsburgh, Sch Med, Dept Psychiat, 3811 O Hara St, Pittsburgh, PA 15213 USA.						RISCH N, 1990, AM J HUM GENET, V46, P222; Storey JD, 2002, J ROY STAT SOC B, V64, P479, DOI 10.1111/1467-9868.00346; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Carrasquillo MM, 2002, NAT GENET, V32, P237, DOI 10.1038/ng998; Benjamini Y, 2001, ANN STAT, V29, P1165; Cordell HJ, 2002, HUM MOL GENET, V11, P2463, DOI 10.1093/hmg/11.20.2463; BENJAMINI Y, 1995, J ROY STAT SOC B MET, V57, P289; Collins FS, 2001, GENOME RES, V11, P641, DOI 10.1101/gr.189801; Genovese C, 2002, J ROY STAT SOC B, V64, P499, DOI 10.1111/1467-9868.00347; Bacanu SA, 2000, AM J HUM GENET, V66, P1933, DOI 10.1086/302929; Bacanu SA, 2002, GENET EPIDEMIOL, V22, P78, DOI 10.1002/gepi.1045; BROMAN KW, 2002, J ROY STAT SOC B, V64, P1; Culverhouse R, 2002, AM J HUM GENET, V70, P461, DOI 10.1086/338759; EFRON B, 2002, LEAST ANGLE REGRESSI; GENOVESE C, 2001, FALSE DISCOVERY RATE; Hastie T., 2001, ELEMENTS STAT LEARNI; Kooperberg C, 2001, GENET EPIDEMIOL, V21, P626; Longmate JA, 2001, AM J HUM GENET, V68, P1229, DOI 10.1086/320106; McCullagh P., 1989, GENERALIZED LINEAR M, VSecond; Nicolae DL, 2002, NAT GENET, V30, P3, DOI 10.1038/ng0102-3; NISHII R, 1988, J MULTIVARIATE ANAL, V27, P392, DOI 10.1016/0047-259X(88)90137-6; Olson JM, 2002, AM J HUM GENET, V71, P154, DOI 10.1086/341034; Osborne MR, 2000, J COMPUT GRAPH STAT, V9, P319, DOI 10.2307/1390657; PORTNOY S, 1985, ANN STAT, V13, P1403, DOI 10.1214/aos/1176349744; Raftery AE, 1995, SOCIOL METHODOL, V25, P111, DOI 10.2307/271063; Risch N, 1999, AM J HUM GENET, V65, P493, DOI 10.1086/302497; Risch NJ, 2000, NATURE, V405, P847, DOI 10.1038/35015718; SIEGMUND D, 2002, AM J HUM GENET, V71, P1883; STOREY J, 2002, UNIFIED ESTIMATION A; Tang HK, 2002, GENET EPIDEMIOL, V22, P313, DOI 10.1002/gepi.01108	30	40	40	0	0	WILEY-LISS	NEW YORK	DIV JOHN WILEY & SONS INC, 605 THIRD AVE, NEW YORK, NY 10158-0012 USA	0741-0395			GENET EPIDEMIOL	Genet. Epidemiol.	JUL	2003	25	1					36	47		10.1002/gepi.10237		12	Genetics & Heredity; Public, Environmental & Occupational Health	Genetics & Heredity; Public, Environmental & Occupational Health	696VV	WOS:000183909000004	12813725	
J	Figueiredo, MAT				Figueiredo, MAT			Adaptive sparseness for supervised learning	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						supervised learning; classification; regression; sparseness; feature selection; kernel methods; expectation-maximization algorithm	GAUSSIAN-PROCESSES; SELECTION; REGULARIZATION; REGRESSION; SHRINKAGE; MACHINES	The goal of supervised teaming is to infer a functional mapping based on a set of training examples. To achieve good generalization, it is necessary to control the "complexity" of the learned function. In Bayesian approaches, this is done by adapting a prior for the parameters of the function being teamed. We propose a Bayesian approach to supervised teaming, which leads to sparse solutions; that is, in which irrelevant parameters are automatically set exactly to zero. Other ways to obtain sparse classifiers (such as Laplacian priors, support vector machines) involve (hyper)parameters which control the degree of sparseness of the resulting classifiers; these parameters have to be somehow adjusted/estimated from the training data. In contrast, our approach does not involve any (hyper)parameters to be adjusted or estimated. This is achieved by a hierarchical-Bayes interpretation of the Laplacian prior, which is then modified by the adoption of a Jeffreys' noninformative hyperprior. Implementation is carried out by an expectation-maximization (EM) algorithm. Experiments with several benchmark data sets show that the proposed approach yields state-of-the-art performance. In particular, our method outperforms SVMs and performs competitively with the best alternative techniques, although it involves no tuning or adjustment of sparseness-controlling hyperparameters.	Univ Tecn Lisboa, Inst Telecommun, Inst Super Tecn, P-1049001 Lisbon, Portugal; Univ Tecn Lisboa, Dept Elect & Comp Engn, Inst Super Tecn, P-1049001 Lisbon, Portugal	Figueiredo, MAT (reprint author), Univ Tecn Lisboa, Inst Telecommun, Inst Super Tecn, P-1049001 Lisbon, Portugal.	mtf@lx.it.pt	Figueiredo, Mario/C-5428-2008	Figueiredo, Mario/0000-0002-0970-7745			ALBERT JH, 1993, J AM STAT ASSOC, V88, P669, DOI 10.2307/2290350; WILLIAMS PM, 1995, NEURAL COMPUT, V7, P117, DOI 10.1162/neco.1995.7.1.117; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; POGGIO T, 1990, P IEEE, V78, P1481, DOI 10.1109/5.58326; Williams CKI, 1998, IEEE T PATTERN ANAL, V20, P1342, DOI 10.1109/34.735807; Osborne MR, 2000, IMA J NUMER ANAL, V20, P389, DOI 10.1093/imanum/20.3.389; Williams CKI, 1998, NATO ADV SCI I D-BEH, V89, P599; DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425; Berger J. O., 1980, STAT DECISION THEORY; Bishop C., 2000, P 16 C UNC ART INT, P46; Breiman L, 1983, CLASSIFICATION REGRE; Chen Z, 2002, NEURAL COMPUT, V14, P2791, DOI 10.1162/089976602760805296; CHEN ZM, 1998, J GREY SYSTEM, V1, P33; Cristianini N., 2000, SUPPORT VECTOR MACHI, V1st; Fahrmeir L, 1994, MULTIVARIATE STAT MO; FIGUEIREDO M, 2002, P ADV NEURAL INFORMA, V14, P697; FIGUEIREDO M, 2000, P INT C PATT REC, V2, P618, DOI 10.1109/ICPR.2000.906151; Figueiredo M. A. T., 2001, P IEEE INT C COMP VI, V1, P35; Figueiredo MAT, 2001, IEEE T IMAGE PROCESS, V10, P1322, DOI 10.1109/83.941856; GRANDVALET Y, 1998, P 8 INT C ART NEUR N, P201; HOERL AE, 1970, TECHNOMETRICS, V12, P55, DOI DOI 10.1080/00401706.1970.10488634; Lange K. L., 1993, J COMPUT GRAPH STAT, V2, P175, DOI 10.2307/1390698; MacKay DJC, 1996, FUND THEOR, V62, P221; McCullagh P., 1989, GEN LINEAR MODELS; Moulin P, 1999, IEEE T INFORM THEORY, V45, P909, DOI 10.1109/18.761332; Neal R. M., 1996, BAYESIAN LEARNING NE; Ripley B. D., 1996, PATTERN RECOGNITION; Seeger M, 2000, ADV NEUR IN, V12, P603; Tipping ME, 2000, ADV NEUR IN, V12, P652; Vapnik V., 1998, STAT LEARNING THEORY; Wahba G., 1990, SOC IND APPL MATH SI; Williams CKI, 2001, ADV NEUR IN, V13, P682	32	199	212	2	16	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828	1939-3539		IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEP	2003	25	9					1150	1159		10.1109/TPAMI.2003.1227989		10	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Computer Science; Engineering	715MX	WOS:000184977300009		
J	Jolliffe, IT; Trendafilov, NT; Uddin, M				Jolliffe, IT; Trendafilov, NT; Uddin, M			A modified principal component technique based on the LASSO	JOURNAL OF COMPUTATIONAL AND GRAPHICAL STATISTICS			English	Article						interpretation; principal component analysis; simplification	VARIABLE SELECTION; SHRINKAGE; ROTATION; SUBSET	In many multivariate statistical techniques, a set of linear functions of the original p variables is produced. One of the more difficult aspects of these techniques is the interpretation of the linear functions, as these functions usually have nonzero coefficients on all p variables. A common approach is to effectively ignore (treat as zero) any coefficients less than some threshold value, so that the function becomes simple and the interpretation becomes easier for the users. Such a procedure can be misleading. There are alternatives to principal component analysis which restrict the coefficients to a smaller number of possible values in the derivation of the linear functions, or replace the principal components by "principal variables." This article introduces a new technique, borrowing an idea proposed by Tibshirani in the context of multiple regression where similar problems arise in interpreting regression equations. This approach is the so-called LASSO, the "least absolute shrinkage and selection operator," in which a bound is introduced on the sum of the absolute values of the coefficients, and in which some coefficients consequently become zero. We explore some of the properties of the new technique, both theoretically and using simulation studies, and apply it to an example.	Univ Aberdeen, Kings Coll, Dept Math Sci, Aberdeen AB24 3UE, Scotland; Univ W England, Fac Comp Engn & Math Sci, Bristol BS16 1QY, Avon, England; Univ Karachi, Dept Stat, Karachi 75270, Pakistan	Jolliffe, IT (reprint author), Univ Aberdeen, Kings Coll, Dept Math Sci, Meston Bldg, Aberdeen AB24 3UE, Scotland.						BREIMAN L, 1995, TECHNOMETRICS, V37, P373, DOI 10.2307/1269730; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; MCCABE GP, 1984, TECHNOMETRICS, V26, P137, DOI 10.2307/1268108; Cadima JFCL, 2001, J AGRIC BIOL ENVIR S, V6, P62, DOI 10.1198/108571101300325256; CADIMA J, 1995, J APPL STAT, V22, P203, DOI 10.1080/757584614; CHU MT, 2001, J COMPUTATIONAL GRAP, V10, P1; FU JW, 1998, J COMPUTATIONAL GRAP, V7, P397; GOFFE WL, 1994, J ECONOMETRICS, V60, P65, DOI 10.1016/0304-4076(94)90038-8; Hausman R., 1982, OPTIMIZATION STAT, P137; Helmke U., 1994, OPTIMIZATION DYNAMIC; Jeffers JNR, 1967, APPLIED STATISTICS, V16, P225, DOI 10.2307/2985919; Jolliffe I. T., 2002, PRINCIPAL COMPONENT; Jolliffe IT, 2002, CLIMATE RES, V20, P271, DOI 10.3354/cr020271; JOLLIFFE IT, 1989, APPL STAT-J ROY ST C, V38, P139, DOI 10.2307/2347688; Jolliffe IT, 2000, J COMPUT GRAPH STAT, V9, P689, DOI 10.2307/1391088; JOLLIFFE IT, 1995, J APPL STAT, V22, P29, DOI 10.1080/757584395; KRZANOWSKI WJ, 1995, MUTLIVARIATE ANAL 2; LeBlanc M, 1998, J COMPUT GRAPH STAT, V7, P417, DOI 10.2307/1390674; MORTON SC, 1989, 106 STANF U DEP STAT; Osborne MR, 2000, J COMPUT GRAPH STAT, V9, P319, DOI 10.2307/1390657; Tanaka Y, 1997, AM J MATH-S, V17, P61; UDDIN M, 1999, THESIS U ABERDEEN AB; Vines SK, 2000, J ROY STAT SOC C-APP, V49, P441, DOI 10.1111/1467-9876.00204	23	145	148	5	18	AMER STATISTICAL ASSOC	ALEXANDRIA	1429 DUKE ST, ALEXANDRIA, VA 22314 USA	1061-8600			J COMPUT GRAPH STAT	J. Comput. Graph. Stat.	SEP	2003	12	3					531	547		10.1198/1061860032148		17	Statistics & Probability	Mathematics	722VA	WOS:000185397600003		
J	Huang, XH; Pan, W				Huang, XH; Pan, W			Linear regression and two-class classification with gene expression data	BIOINFORMATICS			English	Article							PARTIAL LEAST-SQUARES; OLIGONUCLEOTIDE ARRAYS; CANCER; TUMOR; SELECTION	Motivation: Using gene expression data to classify (or predict) tumor types has received much research attention recently. Due to some special features of gene expression data, several new methods have been proposed, including the weighted voting scheme of Golub et al., the compound covariate method of Hedenfalk et al. (originally proposed by Tukey), and the shrunken centroids method of Tibshirani et al. These methods look different and are more or less ad hoc. Results: We point out a close connection of the three methods with a linear regression model. Casting the classification problem in the general framework of linear regression naturally leads to new alternatives, such as partial least squares (PLS) methods and penalized PLS (PPLS) methods. Using two real data sets, we show the competitive performance of our new methods when compared with the other three methods.	Univ Minnesota, Sch Publ Hlth, Div Biostat, Minneapolis, MN 55455 USA	Pan, W (reprint author), Univ Minnesota, Sch Publ Hlth, Div Biostat, A460 Mayo Bldg MMC 303, Minneapolis, MN 55455 USA.						Nguyen DV, 2002, BIOINFORMATICS, V18, P39, DOI 10.1093/bioinformatics/18.1.39; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Lipshutz RJ, 1999, NAT GENET, V21, P20, DOI 10.1038/4447; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; Alon U, 1999, P NATL ACAD SCI USA, V96, P6745, DOI 10.1073/pnas.96.12.6745; Brown PO, 1999, NAT GENET, V21, P33, DOI 10.1038/4462; Lander ES, 1999, NAT GENET, V21, P3, DOI 10.1038/4427; Golub TR, 1999, SCIENCE, V286, P531, DOI 10.1126/science.286.5439.531; Dudoit S, 2002, J AM STAT ASSOC, V97, P77, DOI 10.1198/016214502753479248; Tibshirani R, 2002, P NATL ACAD SCI USA, V99, P6567, DOI 10.1073/pnas.082099299; Ramaswamy S, 2001, P NATL ACAD SCI USA, V98, P15149, DOI 10.1073/pnas.211566398; GARTHWAITE PH, 1994, J AM STAT ASSOC, V89, P122, DOI 10.2307/2291207; GHOSH D, 2002, UNPUB PENALIZED REGR; Hedenfalk I, 2001, NEW ENGL J MED, V344, P539, DOI 10.1056/NEJM200102223440801; HUANG X, 2003, 2003005 U MINN DIV B; Johansson D, 2003, BIOINFORMATICS, V19, P467, DOI 10.1093/bioinformatics/btg017; Marx BD, 1996, TECHNOMETRICS, V38, P374, DOI 10.2307/1271308; Park Peter J, 2002, Bioinformatics, V18 Suppl 1, pS120; TUKEY JW, 1993, CONTROL CLIN TRIALS, V14, P266, DOI 10.1016/0197-2456(93)90225-3; WEST M, 2002, 0212 DUK U I STAT DE	20	57	59	0	2	OXFORD UNIV PRESS	OXFORD	GREAT CLARENDON ST, OXFORD OX2 6DP, ENGLAND	1367-4803			BIOINFORMATICS	Bioinformatics	NOV 1	2003	19	16					2072	2078		10.1093/bioinformatics/btg283		7	Biochemical Research Methods; Biotechnology & Applied Microbiology; Computer Science, Interdisciplinary Applications; Mathematical & Computational Biology; Statistics & Probability	Biochemistry & Molecular Biology; Biotechnology & Applied Microbiology; Computer Science; Mathematical & Computational Biology; Mathematics	741FW	WOS:000186448900009	14594712	
J	Shevade, SK; Keerthi, SS				Shevade, SK; Keerthi, SS			A simple and efficient algorithm for gene selection using sparse logistic regression	BIOINFORMATICS			English	Article							EXPRESSION DATA; CANCER; CLASSIFICATION; LASSO	Motivation: This paper gives a new and efficient algorithm for the sparse logistic regression problem. The proposed algorithm is based on the Gauss-Seidel method and is asymptotically convergent. It is simple and extremely easy to implement; it neither uses any sophisticated mathematical programming software nor needs any matrix operations. It can be applied to a variety of real-world problems like identifying marker genes and building a classifier in the context of cancer diagnosis using microarray data. Results: The gene selection method suggested in this paper is demonstrated on two real-world data sets and the results were found to be consistent with the literature.	Natl Univ Singapore, Dept Mech Engn, Control Div, Singapore 117576, Singapore; Indian Inst Sci, Dept Comp Sci & Automat, Bangalore 560012, Karnataka, India	Keerthi, SS (reprint author), Natl Univ Singapore, Dept Mech Engn, Control Div, Singapore 117576, Singapore.						AMBROISE C, 2002, P NATL ACAD SCI USA, V99, P668; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Alon U, 1999, P NATL ACAD SCI USA, V96, P6745, DOI 10.1073/pnas.96.12.6745; Furey TS, 2000, BIOINFORMATICS, V16, P906, DOI 10.1093/bioinformatics/16.10.906; Guyon I, 2002, MACH LEARN, V46, P389, DOI 10.1023/A:1012487302797; West M, 2001, P NATL ACAD SCI USA, V98, P11462, DOI 10.1073/pnas.201162998; Suykens JAK, 1999, NEURAL PROCESS LETT, V9, P293, DOI 10.1023/A:1018628609742; Brown MPS, 2000, P NATL ACAD SCI USA, V97, P262, DOI 10.1073/pnas.97.1.262; ARRICK AB, 1996, MOL ENDOCRINOLOGY CA, P51; Bertsekas D. P., 1989, PARALLEL DISTRIBUTED; Kos J., 2002, RADIOL ONCOL, V36, P176; Li Y, 2002, BIOINFORMATICS, V18, P1332, DOI 10.1093/bioinformatics/18.10.1332; Osborne MR, 2000, J COMPUT GRAPH STAT, V9, P319, DOI 10.2307/1390657; ROTH V, 2002, IAITR20028 U BONN CO; Roth V., 2001, Pattern Recognition. 23rd DAGM Symposium. Proceedings (Lecture Notes in Computer Science Vol.2191); Weston J, 2001, ADV NEUR IN, V13, P668	16	111	116	2	6	OXFORD UNIV PRESS	OXFORD	GREAT CLARENDON ST, OXFORD OX2 6DP, ENGLAND	1367-4803			BIOINFORMATICS	Bioinformatics	NOV 22	2003	19	17					2246	2253		10.1093/bioinformatics/btg308		8	Biochemical Research Methods; Biotechnology & Applied Microbiology; Computer Science, Interdisciplinary Applications; Mathematical & Computational Biology; Statistics & Probability	Biochemistry & Molecular Biology; Biotechnology & Applied Microbiology; Computer Science; Mathematical & Computational Biology; Mathematics	749KN	WOS:000186919200010	14630653	
J	Meulman, JJ				Meulman, JJ			Prediction and classification in nonlinear data analysis: Something old, something new, something borrowed, something blue	PSYCHOMETRIKA			English	Article; Proceedings Paper	13th International Meeting of the Psychometric-Society/68th Annual Meeting of the Psychometric-Society	JUL09, 2003	Cagliari, ITALY	Psychometr Soc		multiple regression; optimal scaling; optimal scoring; statistical learning; data mining; boosting; forward stagewise additive modeling; additive prediction components; monotonic regression; regression splines; distance based clustering; clustering on variable subsets; COSA; genomics; proteomics; systems biology; categorical data; ordinal data; ApoE3 data; cervix cancer data; Boston housing data	OPTIMAL-SCALING FEATURES; LEAST-SQUARES METHOD; DISCRIMINANT-ANALYSIS; REGRESSION; TRANSFORMATIONS	Prediction and classification are two very active areas in modern data analysis. In this paper, prediction with nonlinear optimal scaling transformations of the variables is reviewed, and extended to the use of multiple additive components, much in the spirit of statistical learning techniques that are currently popular, among other areas, in data mining. Also, a classification/clustering method is described that is particularly suitable for analyzing attribute-value data from systems biology (genomics, proteomics, and metabolomics), and which is able to detect groups of objects that have similar values on small subsets of the attributes.	Leiden Univ, Dept Educ, Data Theory Grp, NL-2300 RB Leiden, Netherlands	Meulman, JJ (reprint author), Leiden Univ, Dept Educ, Data Theory Grp, POB 9555, NL-2300 RB Leiden, Netherlands.	meulman@fsw.leidenuniv.nl					FRIEDMAN JH, 1981, J AM STAT ASSOC, V76, P817, DOI 10.2307/2287576; Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451; Friedman J, 2000, ANN STAT, V28, P337, DOI 10.1214/aos/1016218223; HASTIE T, 1994, J AM STAT ASSOC, V89, P1255, DOI 10.2307/2290989; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; MAX J, 1960, IRE T INFORM THEOR, V6, P7, DOI 10.1109/TIT.1960.1057548; Friedman JH, 2003, STAT MED, V22, P1365, DOI 10.1002/sim.1501; KRUSKAL JB, 1964, PSYCHOMETRIKA, V29, P115, DOI 10.1007/BF02289694; KRUSKAL JB, 1964, PSYCHOMETRIKA, V29, P1, DOI 10.1007/BF02289565; HARRISON D, 1978, J ENVIRON ECON MANAG, V5, P81, DOI 10.1016/0095-0696(78)90006-2; Bock R. D., 1960, METHODS APPL OPTIMAL; BOON ME, 1990, ACTA CYTOL, V35, P57; BREIMAN L, 1985, J AM STAT ASSOC, V80, P580, DOI 10.2307/2288473; BREIMAN L, 1996, MACH LEARN, V24, P51; Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.1023/A:1018054314350; Breiman L., 1984, CLASSIFICATION REGRE; BUJA A, 1990, ANN STAT, V18, P1032, DOI 10.1214/aos/1176347739; Davidov E., 2003, METABOLIC PROFILING, P170; De Leeuw J., 1980, MULTIVARIATE ANAL, P501; DELEEUW J, 1976, PSYCHOMETRIKA, V41, P471; Duda R.O., 2000, PATTERN CLASSIFICATI; Freund Y., 1996, Machine Learning. Proceedings of the Thirteenth International Conference (ICML '96); FRIEDMAN JH, IN PRESS J ROYAL STA; FRIEDMAN JH, 2003, COSA; Gifi A, 1990, NONLINEAR MULTIVARIA; Groenen PJF, 2000, PSYCHOMETRIKA, V65, P511, DOI 10.1007/BF02296341; Guttman L., 1950, MEASUREMENT PREDICTI; Hastie T., 2001, ELEMENTS STAT LEARNI; HASTIE TJ, 1990, GENERALIZED ADDICTIV; HAYASHI C, 1952, ANN I STAT MATH, V2, P93; Heiser WJ., 1995, RECENT ADV DESCRIPTI, P157; Takane Y, 2002, MEASUREMENT AND MULTIVARIATE ANALYSIS, P183; KRUSKAL JB, 1965, J ROY STAT SOC B, V27, P251; MCLACHLAN GJ, 1992, DISCRIMINATION ANAL; Meulman JJ, 2000, ST CLASS DAT ANAL, P32; MEULMAN JJ, 1992, ANAL QUANT CYTOL, V14, P60; MEULMAN JJ, 2000, INT C MEAS MULT AN I; NISHISATO S, 1980, ANAL CATEOGORICAL DA; Nishisato S., 1994, ELEMENTS DUAL SCALIN; Ramsay J. O., 1988, STAT SCI, V3, P425, DOI DOI 10.1214/SS/1177012761; Ripley B. D., 1996, PATTERN RECOGNITION; TAKANE Y, 1998, DATA SCI CLASSIFICAT, P527; Van der Kooij A. J., 1999, SPSS CATEGORIES 10 0, p[1, 77, 239]; VANDERKOOIJ AJ, 2003, UNPUB LOCAL MINIMA C; Vapnik V.N., 1996, NATURE STAT LEARNING; Whittaker J, 1990, GRAPHICAL MODELS APP; WINSBERG S, 1980, BIOMETRIKA, V67, P669; YOUNG FW, 1976, PSYCHOMETRIKA, V41, P505, DOI 10.1007/BF02296972	48	7	7	3	4	SPRINGER	NEW YORK	233 SPRING ST, NEW YORK, NY 10013 USA	0033-3123			PSYCHOMETRIKA	Psychometrika	DEC	2003	68	4					493	517		10.1007/BF02295607		25	Mathematics, Interdisciplinary Applications; Social Sciences, Mathematical Methods; Psychology, Mathematical	Mathematics; Mathematical Methods In Social Sciences; Psychology	823XK	WOS:000221647400002		
S	Li, G; Dai, HH; Tu, YQ		Dai, H; Srikant, R; Zhang, C		Li, G; Dai, HH; Tu, YQ			Identifying Markov blankets using Lasso estimation	ADVANCES IN KNOWLEDGE DISCOVERY AND DATA MINING, PROCEEDINGS	LECTURE NOTES IN ARTIFICIAL INTELLIGENCE		English	Article; Proceedings Paper	8th Pacific/Asia Conference on Advances in Knowledge Discovery and Data Mining	MAY 26-28, 2004	Sydney, AUSTRALIA	NICIA, SAS, Univ Technol Sydney, Deakin Univ				Determining the causal relation among attributes in a domain is a key task in data mining and knowledge discovery. The Minimum Message Length (MML) principle has demonstrated its ability in discovering linear causal models from training data. To explore the ways to improve efficiency, this paper proposes a novel Markov Blanket identification algorithm based on the Lasso estimator. For each variable, this algorithm first generates a Lasso tree, which represents a pruned candidate set of possible feature sets. The Minimum Message Length principle is then employed to evaluate all those candidate feature sets, and the feature set with minimum message length is chosen as the Markov Blanket. Our experiment results show the ability of this algorithm. In addition, this algorithm can be used to prune the search space of causal discovery, and further reduce the computational cost of those score-based causal discovery.. algorithms.	Deakin Univ, Sch Informat Technol, Geelong, Vic 3125, Australia	Li, G (reprint author), Deakin Univ, Sch Informat Technol, 221 Burwood Highway, Geelong, Vic 3125, Australia.	gangli@deakin.edu.au; hdai@deakin.edu.au; ytu@deakin.edu.au	Li, Gang/C-4925-2009				Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Wright S, 1934, ANN MATH STAT, V5, P161, DOI 10.1214/aoms/1177732676; Bollen Kenneth A., 1989, STRUCTURAL EQUATIONS; Conway J. H., 1988, SPHERE PACKINGS LATT; DAI H, 1997, P 15 INT JOINT C ART, P1304; DAI H, 2003, TECHNICAL REPORT; DAI H, 2002, P 13 EUR C MACH LEAR, P48; DAI HH, 2002, P 6 PAC AS C KNOWL D, P304; HARVEY AC, 1990, EC ANAL TIME SERIES; Koller D., 1996, P 13 INT C MACH LEAR, P284; LI G, 2002, P 2002 IEEE INT C DA, P274, DOI DOI 10.1109/ICDM.2002.1183913; Loehlin J. C., 1992, LATENT VARIABLE MODE, V2nd; PEARL J., 1988, PROBABILISTIC REASON; Tsamardinos I., 2003, P 9 INT WORKSH ART I; Wallace C, 1996, P 13 INT C MACH LEAR, P516; WALLACE CS, 1968, COMPUT J, V11, P185; WALLACE CS, 1987, J ROY STAT SOC B MET, V49, P240; Wright S, 1921, J AGR RES, V20, P557	18	0	0	0	0	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		3-540-22064-X	LECT NOTES ARTIF INT			2004	3056						308	318				11	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Computer Science	BAF29	WOS:000221955100037		
S	Zhu, J; Rosset, S; Hastie, T; Tibshirani, R		Thrun, S; Saul, K; Scholkopf, B		Zhu, J; Rosset, S; Hastie, T; Tibshirani, R			1-norm support vector machines	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA				CLASSIFICATION; SELECTION; CANCER	The standard 2-norm SVM is known for its good performance in two-class classipoundcation. In this paper, we consider the 1-norm SVM. We argue that the 1-norm SVNI may have some advantage over the standard 2-norm SVM, especially when there are redundant noise features. We also propose an efpoundcient algorithm that computes the whole solution path of the 1-norm SVNI, hence facilitates adaptive selection of the tuning parameter for the 1-norm SVM.	Stanford Univ, Dept Stat, Stanford, CA 94305 USA	Zhu, J (reprint author), Stanford Univ, Dept Stat, Stanford, CA 94305 USA.						Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Golub TR, 1999, SCIENCE, V286, P531, DOI 10.1126/science.286.5439.531; Guyon I, 2002, MACH LEARN, V46, P389, DOI 10.1023/A:1012487302797; Bradley Paul S., 1998, ICML 98; EVGENIOU T, 1999, ADV LARGE MARGIN CLA; FRIEDMAN J, 2004, IN PRESS ANN STAT; HASTIE T, 2001, ELEMENTS STAT LEANRI; MUKHERJEE S, 1999, 1677 MIT; ROSSET S, 2003, TECHNICAL REPORT DEP; SONG M, J CHEM INFORMATI SEP; Vapnik V.N., 1995, NATURE STAT LEARNING; Wahba G, 1999, ADVANCES IN KERNEL METHODS, P69; ZHU J, 2003, IN PRESS CLASSIFICAT; Zhu J, 2003, THESIS STANFORD U	14	75	82	2	4	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						49	56				8	Computer Science, Artificial Intelligence; Neurosciences	Computer Science; Neurosciences & Neurology	BBF99	WOS:000225309500007		
S	Roth, V; Lange, T		Thrun, S; Saul, K; Scholkopf, B		Roth, V; Lange, T			Feature selection in clustering problems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA				DISCRIMINANT-ANALYSIS; LASSO	A novel approach to combining clustering and feature selection is presented. It implements a wrapper strategy for feature selection, in the sense that the features are directly selected by optimizing the discriminative power of the used partitioning algorithm. On the technical side, we present an efficient optimization algorithm with guaranteed local convergence property. The only free parameter of this method is selected by a resampling-based stability analysis. Experiments with real-world datasets demonstrate that our method is able to infer both meaningful partitions and meaningful subsets of features.	ETH, Inst Computat Sci, CH-8092 Zurich, Switzerland	Roth, V (reprint author), ETH, Inst Computat Sci, Hirschengraben 84, CH-8092 Zurich, Switzerland.						HASTIE T, 1994, J AM STAT ASSOC, V89, P1255, DOI 10.2307/2290989; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1; Ben-Dor A., 2001, P 5 ANN INT C COMP M, P31, DOI DOI 10.1145/369133.369167; HASTIE T, 1995, ANN STAT, V23, P73, DOI 10.1214/aos/1176324456; FIGUEIREDO M, 2001, CVPR2001, P35; Hastie Trevor, 1996, J R STAT SOC B, V58, P158; Hofmann T, 1997, IEEE T PATTERN ANAL, V19, P1, DOI 10.1109/34.566806; LANGE T, 2003, IN PRESS ADV NEURAL, V15; LAW MH, 2003, IN PRESS ADV NEURAL, V15; MEINECKE F, 2002, ADV NEURAL INFORMATI, V14; Osborne MR, 2000, J COMPUT GRAPH STAT, V9, P319, DOI 10.2307/1390657; ROTH V, 2003, IN PRESS ADV NEURAL, V15; VONHEYDEBRECK A, 2001, BIOINFORMATICS, V17	14	7	7	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						473	480				8	Computer Science, Artificial Intelligence; Neurosciences	Computer Science; Neurosciences & Neurology	BBF99	WOS:000225309500060		
J	Goeman, JJ; van de Geer, SA; de Kort, F; van Houwelingen, HC				Goeman, JJ; van de Geer, SA; de Kort, F; van Houwelingen, HC			A global test for groups of genes: testing association with a clinical outcome	BIOINFORMATICS			English	Article							REGRESSION; DISCOVERY	Motivation: This paper presents a global test to be used for the analysis of microarray data. Using this test it can be determined whether the global expression pattern of a group of genes is significantly related to some clinical outcome of interest. Groups of genes may be any size from a single gene to all genes on the chip (e.g. known pathways, specific areas of the genome or clusters from a cluster analysis). Result: The test allows groups of genes of different size to be compared, because the test gives one p-value for the group, not a p-value for each gene. Researchers can use the test to investigate hypotheses based on theory or past research or to mine gene ontology databases for interesting pathways. Multiple testing problems do not occur unless many groups are tested. Special attention is given to visualizations of the test result, focussing on the associations between samples and showing the impact of individual genes on the test result.	Leiden Univ, Med Ctr, Dept Med Stat, NL-2300 RC Leiden, Netherlands; Leiden Univ, Math Inst, NL-2300 RA Leiden, Netherlands; Leiden Univ, Med Ctr, Ctr Human & Clin Genet, NL-2300 RA Leiden, Netherlands	Goeman, JJ (reprint author), Leiden Univ, Med Ctr, Dept Med Stat, POB 9604, NL-2300 RC Leiden, Netherlands.	j.j.goeman@lumc.nl	Goeman, Jelle/C-2260-2012; van houwelingen, hans/C-1872-2008	Goeman, Jelle/0000-0003-4283-0259; 			Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Tusher VG, 2001, P NATL ACAD SCI USA, V98, P5116, DOI 10.1073/pnas.091062498; Golub TR, 1999, SCIENCE, V286, P531, DOI 10.1126/science.286.5439.531; HOERL AE, 1970, TECHNOMETRICS, V12, P55; BENJAMINI Y, 1995, J ROY STAT SOC B MET, V57, P289; LECESSIE S, 1992, APPL STAT-J ROY ST C, V41, P191; Eilers P. H. C., 2001, Proceedings of the SPIE - The International Society for Optical Engineering, V4266, DOI 10.1117/12.427987; Huber Wolfgang, 2002, Bioinformatics, V18 Suppl 1, pS96; LECESSIE S, 1995, BIOMETRICS, V51, P600, DOI 10.2307/2532948; McCullagh P., 1989, GEN LINEAR MODELS; HouwingDuistermaat JJ, 1995, BIOMETRICS, V51, P1292, DOI 10.2307/2533260	11	448	455	3	16	OXFORD UNIV PRESS	OXFORD	GREAT CLARENDON ST, OXFORD OX2 6DP, ENGLAND	1367-4803			BIOINFORMATICS	Bioinformatics	JAN 1	2004	20	1					93	99		10.1093/bioinformatics/btg382		7	Biochemical Research Methods; Biotechnology & Applied Microbiology; Computer Science, Interdisciplinary Applications; Mathematical & Computational Biology; Statistics & Probability	Biochemistry & Molecular Biology; Biotechnology & Applied Microbiology; Computer Science; Mathematical & Computational Biology; Mathematics	761QW	WOS:000187925200013	14693814	
J	Kechris, KJ; van Zwet, E; Bickel, PJ; Eisen, MB				Kechris, KJ; van Zwet, E; Bickel, PJ; Eisen, MB			Detecting DNA regulatory motifs by incorporating positional trends in information content	GENOME BIOLOGY			English	Article							PROTEIN-BINDING SITES; EXPECTATION MAXIMIZATION; EM ALGORITHM; SEQUENCES; REGRESSION; LIKELIHOOD; ALIGNMENT; SELECTION; SAMPLER; MODELS	On the basis of the observation that conserved positions in transcription factor binding sites are often clustered together, we propose a simple extension to the model-based motif discovery methods. We assign position-specific prior distributions to the frequency parameters of the model, penalizing deviations from a specified conservation profile. Examples with both simulated and real data show that this extension helps discover motifs as the data become noisier or when there is a competing false motif.	Univ Calif Berkeley, Dept Stat, Berkeley, CA 94720 USA; Univ Calif Berkeley, Lawrence Berkeley Lab, Div Life Sci, Dept Genome Sci, Berkeley, CA 94720 USA; Univ Calif Berkeley, Dept Mol & Cell Biol, Ctr Integrat Genom, Berkeley, CA 94720 USA	Kechris, KJ (reprint author), Univ Calif San Francisco, Dept Biochem & Biophys, 600 16th St 2240, San Francisco, CA 94143 USA.	kechris@genome.ucsf.edu					Quandt K, 1995, NUCLEIC ACIDS RES, V23, P4878, DOI 10.1093/nar/23.23.4878; GELFAND AE, 1990, J AM STAT ASSOC, V85, P398, DOI 10.2307/2289776; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1; SCHNEIDER TD, 1986, J MOL BIOL, V188, P415, DOI 10.1016/0022-2836(86)90165-8; BAILEY TL, 1995, MACH LEARN, V21, P51, DOI 10.1023/A:1022617714621; BERG OG, 1988, J MOL BIOL, V200, P709, DOI 10.1016/0022-2836(88)90482-2; Brazma A, 1998, J COMPUT BIOL, V5, P279, DOI 10.1089/cmb.1998.5.279; Brent RP, 1973, ALGORITHMS MINIMIZAT; CARDON LR, 1992, J MOL BIOL, V223, P159, DOI 10.1016/0022-2836(92)90723-W; DIEBOLT J, 1998, MARKOV CHAIN MONTE C; DRAPER NR, 1979, TECHNOMETRICS, V21, P451, DOI 10.2307/1268284; FRECH K, 1993, NUCLEIC ACIDS RES, V21, P1655, DOI 10.1093/nar/21.7.1655; GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721; GREEN PJ, 1990, J ROY STAT SOC B MET, V52, P443; IHAKA R, 1996, J COMPUTATIONAL GRAP, V5, P299, DOI DOI 10.2307/1390807; Iyer VR, 2001, NATURE, V409, P533, DOI 10.1038/35054095; KECHRIS K, 2003, THESIS U CALIFORNIA; Latchman D.S., 1998, EUKARYOTIC TRANSCRIP; LAWRENCE CE, 1990, PROTEINS, V7, P41, DOI 10.1002/prot.340070105; Liu JS, 1995, J AM STAT ASSOC, V90, P1156, DOI 10.2307/2291508; Liu JS, 1999, BIOINFORMATICS, V15, P38, DOI 10.1093/bioinformatics/15.1.38; Liu X, 2001, Pac Symp Biocomput, P127; Mirny LA, 2002, NUCLEIC ACIDS RES, V30, P1704, DOI 10.1093/nar/30.7.1704; Ripley B. D., 1987, STOCHASTIC SIMULATIO; Rosen K. H., 2000, HDB DISCRETE COMBINA; Roth FP, 1998, NAT BIOTECHNOL, V16, P939, DOI 10.1038/nbt1098-939; SMITH AFM, 1993, J ROY STAT SOC B MET, V55, P3; SPANIER J, 1987, CUBIC FUNCTION X3 AX; STORMO GD, 1982, NUCLEIC ACIDS RES, V10, P2997, DOI 10.1093/nar/10.9.2997; STORMO GD, 1990, METHOD ENZYMOL, V183, P211, DOI 10.1016/0076-6879(90)83015-2; TATUSOV RL, 1994, P NATL ACAD SCI USA, V91, P12091, DOI 10.1073/pnas.91.25.12091; Thijs G, 2002, J COMPUT BIOL, V9, P447, DOI 10.1089/10665270252935566; Thompson W, 2003, NUCLEIC ACIDS RES, V31, P3580, DOI 10.1093/nar/gkg608; SCPD; DPINTERACT; ENTREZ NUCLEOTIDE; REGULATORY SEQUENCE	38	22	22	0	4	BIOMED CENTRAL LTD	LONDON	MIDDLESEX HOUSE, 34-42 CLEVELAND ST, LONDON W1T 4LB, ENGLAND	1465-6914			GENOME BIOL	Genome Biol.		2004	5	7							R50	10.1186/gb-2004-5-7-r50		21	Biotechnology & Applied Microbiology; Genetics & Heredity	Biotechnology & Applied Microbiology; Genetics & Heredity	834RX	WOS:000222429500015	15239835	
J	Roth, V				Roth, V			The generalized LASSO	IEEE TRANSACTIONS ON NEURAL NETWORKS			English	Article						kernel regression; probabilistic interpretation; robust loss functions; sparisity; support vector machines (SVMs)	LEAST-SQUARES PROBLEMS; REGRESSION; CLASSIFIERS; ALGORITHMS; SELECTION	In the last few years, the support vector machine (SVM) method has motivated new interest in kernel regression techniques. Although the SVM has been shown to exhibit excellent generalization properties in many experiments, it suffers from several drawbacks, both of a theoretical and a technical nature: the absence of probabilistic outputs, the restriction to Mercer kernels, and the steep growth of the number of support vectors with increasing size of the training set. In this paper, we present a different class of kernel regressors that effectively overcome the above problems. We call this approach generalrized LASSO regression. It has a clear probabilistic interpretation, can handle learning sets that are corrupted by outliers, produces extremely sparse solutions, and is capable of dealing with large-scale problems. For regression functionals which can be modeled as iteratively reweighted least-squares (IRLS) problems, we present a highly efficient algorithm with guaranteed global convergence. This defies a unique framework for sparse regression models in the very rich class of IRLS models, including various types of robust regression models and logistic regression. Performance studies for many standard benchmark datasets effectively demonstrate the advantages of this model over related approaches.	Univ Bonn, Dept Comp Sci 3, D-53117 Bonn, Germany	Roth, V (reprint author), Univ Bonn, Dept Comp Sci 3, D-53117 Bonn, Germany.	roth@cs.uni-bonn.de					Suykens JAK, 2002, NEUROCOMPUTING, V48, P85, DOI 10.1016/S0925-2312(01)00644-0; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Muller KR, 2001, IEEE T NEURAL NETWOR, V12, P181, DOI 10.1109/72.914517; Osborne MR, 2000, IMA J NUMER ANAL, V20, P389, DOI 10.1093/imanum/20.3.389; Tipping ME, 2001, J MACH LEARN RES, V1, P211, DOI 10.1162/15324430152748236; Bazaraa M., 1993, NONLINEAR PROGRAMMIN; Chen S. S., 1999, SIAM J SCI COMPUT, V20, P33; CLARK DI, 1988, IMA J NUMER ANAL, V8, P23, DOI 10.1093/imanum/8.1.23; Collobert R, 2000, IDIAPRR0017; Denison D., 2000, BAYESIAN PREDICTION; Drucker H., 1997, P 14 INT C MACH LEAR, P107; Faul AC, 2001, LECT NOTES COMPUT SC, V2130, P95; Faul AC, 2002, ADV NEUR IN, V14, P383; Fessler JA, 1997, P SOC PHOTO-OPT INS, V3170, P184, DOI 10.1117/12.279713; Figueiredo MAT, 2001, PROC CVPR IEEE, P35; FRIEDMAN JH, 1991, ANN STAT, V19, P1, DOI 10.1214/aos/1176347963; Gao JB, 2002, MACH LEARN, V46, P71, DOI 10.1023/A:1012494009640; GRANDVALET Y, 1998, P 8 INT C ART NEUR N, P201; Grandvalet Y, 2000, IEEE T NEURAL NETWOR, V11, P1201, DOI 10.1109/72.883393; GUNN S, 1999, IEEE INT WORKSH NEUR; HOERL AE, 1970, TECHNOMETRICS, V12, P55, DOI DOI 10.1080/00401706.1970.10488634; Huber P. J., 1981, ROBUST STAT; Lokhorst J, 1999, LASSO GEN LINEAR MOD; MacKay D, 1994, ASHRAE T, V100, P1053; NABNEY I, 1999, NCRG99002 AST U; Navia-Vazquez A, 2001, IEEE T NEURAL NETWOR, V12, P1047, DOI 10.1109/72.950134; Osborne MR, 2000, J COMPUT GRAPH STAT, V9, P319, DOI 10.2307/1390657; PONTIL M, 1998, 1651 AI MIT ART INT; Roth V, 2000, ADV NEUR IN, V12, P568; SAUNDERS S, 1998, RIDGE REGRESSION LEA; Smola A. J., 1998, NC2TR1998030 U LOND; Sofer A, 1996, LINEAR NONLINEAR PRO; Suykens J. A. K., 2002, LEAST SQUARES SUPPOR; Teukolsky SA, 1992, NUMERICAL RECIPES C; TIPPING M, 1999, ADV NEURAL INFORMATI, V12, P652; Vapnik V., 1998, STAT LEARNING THEORY; Weston J., 1999, ADV KERNEL METHODS S; Williams CKI, 1998, LEARNING INFERENCE G	38	64	66	4	9	IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC	PISCATAWAY	445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA	1045-9227	1941-0093		IEEE T NEURAL NETWOR	IEEE Trans. Neural Netw.	JAN	2004	15	1					16	28		10.1109/TNN.2003.809398		13	Computer Science, Artificial Intelligence; Computer Science, Hardware & Architecture; Computer Science, Theory & Methods; Engineering, Electrical & Electronic	Computer Science; Engineering	769AL	WOS:000188603900002	15387244	
B	Berge, A; Solberg, AS			ieee	Berge, A; Solberg, AS			Robust classification of hyperspectral data	IGARSS 2004: IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM PROCEEDINGS, VOLS 1-7: SCIENCE FOR SOCIETY: EXPLORING AND MANAGING A CHANGING PLANET	IEEE International Symposium on Geoscience and Remote Sensing (IGARSS)		English	Proceedings Paper	IEEE International Geoscience and Remote Sensing Symposium	SEP 20-24, 2004	Anchorage, AK	IEEE, IEEE Geosci & Remote Sensing Soc, Univ Alaska Fairbanks, Geophys Inst, Univ Missouri Columbia, NASA, NOAA, USN, Off Naval Res, Ball Aerosp &Technol Corp, Natl Polar Orbiting Operat Environm Satellite Syst, Japan Aerosp Explorat Agcy, Raytheon, US Geol Survey, ITT Ind, IEEE Ocean Engn Soc, Int Union Radio Sci			DISCRIMINANT-ANALYSIS	High dimensionality and highly correlated features are two important characteristics of hyperspectral data that leads to poor performance of conventional classification methods. Furthermore, hyperspectral sensors usually provide relatively low optical resolution, which implies that pixels are bound to cover a mixture of objects with different reflective properties. Since it is common to define sharp labels on pixels, classes might not be adequately described with a single mode Gaussian as it is clone in many conventional and contemporary classification methods for hyperspectral data. We study a framework that facilitates a penalized classification, making the classifier robust for overfitting. This framework also allows the classes to be modeled as a mixture of subclasses, giving the model more flexibility.	Univ Oslo, Dept Informat, N-0316 Oslo, Norway	Berge, A (reprint author), Univ Oslo, Dept Informat, N-0316 Oslo, Norway.						HASTIE T, 1994, J AM STAT ASSOC, V89, P1255, DOI 10.2307/2290989; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Figueiredo MAT, 2002, IEEE T PATTERN ANAL, V24, P381, DOI 10.1109/34.990138; BERGE A, 2004, P IEEE GEOSC REM SEN; HASTIE T, 1995, ANN STAT, V23, P73, DOI 10.1214/aos/1176324456; Hastie T., 2001, ELEMENTS STAT LEARNI	6	0	0	0	1	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA			0-7803-8742-2	INT GEOSCI REMOTE SE			2004							937	940				4	Geosciences, Multidisciplinary; Instruments & Instrumentation; Remote Sensing; Imaging Science & Photographic Technology	Geology; Instruments & Instrumentation; Remote Sensing; Imaging Science & Photographic Technology	BBP98	WOS:000227006900248		
J	Li, RZ; Lin, DKJ; Chen, Y				Li, RZ; Lin, DKJ; Chen, Y			Uniform design: design, analysis and applications	INTERNATIONAL JOURNAL OF MATERIALS & PRODUCT TECHNOLOGY			English	Article						uniform design; discrepancy; penalised least squares; SCAD	SELECTION; MODEL; MIXTURES	A brief introduction is given to explain the fundamental idea of uniform design and measure of uniformity. A new variable selection procedure is proposed for the analysis of data obtained from uniform designs. This new procedure is distinguished from the traditional ones in the way that it simultaneously deletes insignificant variables and estimates the coefficients of significant variables. This procedure possesses an oracle property, which means that it performs as well as if the true model were known in advance. An example is given to illustrate the application of the uniform design and this variable selection procedure.	Penn State Univ, Dept Stat, University Pk, PA 16802 USA; Penn State Univ, Dept Management Sci & Informat Syst, University Pk, PA 16802 USA; Shenyang Univ Technol, Coll Sci, Shenyang 110023, Liaoning, Peoples R China	Li, RZ (reprint author), Penn State Univ, Dept Stat, University Pk, PA 16802 USA.		Li, Runze/C-5444-2013	Li, Runze/0000-0002-0154-2202			AKAIKE H, 1974, IEEE T AUTOMAT CONTR, VAC19, P716, DOI 10.1109/TAC.1974.1100705; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; MCKAY MD, 1979, TECHNOMETRICS, V21, P239, DOI 10.2307/1268522; FRANK IE, 1993, TECHNOMETRICS, V35, P109, DOI 10.2307/1269656; SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136; MALLOWS CL, 1973, TECHNOMETRICS, V15, P661, DOI 10.2307/1267380; CRAVEN P, 1979, NUMER MATH, V31, P377; Breiman L, 1996, ANN STAT, V24, P2350; Chan LY, 2000, COMMUN STAT-THEOR M, V29, P2281, DOI 10.1080/03610920008832607; CHAN LY, 2003, INT J MAT PRODUCT TE, V19; CHAN LY, 2002, P 8 ISSAT INT C REL, P245; CORNELL J. A, 2002, EXPT MIXTURES DESIGN; FANG K. T., 1994, APPL NUMBER THEORETI; Fang Kai-tai, 1980, Acta Mathematicae Applacatae Sinica, V3; Fang K.T., 1994, UNIFORM DESIGN UNIFO; FANG KT, 1999, STAT PROBABIL LETT, V46, P113; FANG KT, 2000, UNIFORM DESIGN THEOR, V35, P109; Fang KT, 2000, BIOMETRIKA, V87, P193, DOI 10.1093/biomet/87.1.193; Fang KT, 1997, COMPUT STAT DATA AN, V24, P29; Hickernell FJ, 1998, MATH COMPUT, V67, P299, DOI 10.1090/S0025-5718-98-00894-1; Li RZ, 2002, STAT PROBABIL LETT, V59, P135, DOI 10.1016/S0167-7152(02)00140-2; LIU MQ, 2003, INT J MAT PRODUCT TE, V19; Niederreiter H., 1992, SIAM CBMS NSF REGION; Pukelsheim F., 1993, OPTIMAL DESIGN EXPT; Walpole R. E., 2002, PROBABILITY STAT ENG; Wang Y, 1996, SCI CHINA SER A, V39, P264; Wang Y, 1981, KEXUE TONGBAO, V26, P485; WANG Y, 1990, CHINESE ANN MATH B, V11, P264	29	8	8	6	8	INDERSCIENCE ENTERPRISES LTD	GENEVA	WORLD TRADE CENTER BLDG, 29 ROUTE DE PRE-BOIS, CASE POSTALE 896, CH-1215 GENEVA, SWITZERLAND	0268-1900			INT J MATER PROD TEC	Int. J. Mater. Prod. Technol.		2004	20	1-3					101	114		10.1504/IJMPT.2004.003915		14	Materials Science, Multidisciplinary	Materials Science	806JH	WOS:000220429700007		
J	Krishnapuram, B; Carin, L; Hartemink, AJ				Krishnapuram, B; Carin, L; Hartemink, AJ			Joint classifier and feature optimization for comprehensive cancer diagnosis using gene expression data	JOURNAL OF COMPUTATIONAL BIOLOGY			English	Article; Proceedings Paper	7th Annual International Conference on Computational Biology	APR 10-13, 2003	Berlin, GERMANY			disease diagnosis; classification; feature selection; joint optimization; sparse Bayesian methods; JCFO; RVM; SVM	VECTOR MACHINE; SELECTION; TUMOR	Recent research has demonstrated quite convincingly that accurate cancer diagnosis can be achieved by constructing classifiers that are designed to compare the gene expression profile of a tissue of unknown, cancer status to a database of stored expression profiles from tissues of known cancer status. This paper introduces the JCFO, a novel algorithm that uses a sparse Bayesian approach to jointly identify both the optimal nonlinear classifier for diagnosis and the optimal set of genes on which to base that diagnosis. We show that the diagnostic classification accuracy of the proposed algorithm is superior to a number of current state-of-the-art methods in a full leave-one-out cross-validation study of five widely used benchmark datasets. In addition to its superior classification accuracy, the algorithm is designed to automatically identify a small subset of genes (typically around twenty in our experiments) that are capable of providing complete discriminatory information for diagnosis. Focusing attention on a small subset of genes is useful not only because it produces a classifier with good generalization capacity, but also because this set of genes may provide insights into the mechanisms responsible for the disease itself. A number of the genes identified by the JCFO in our experiments are already in use as clinical markers for cancer diagnosis; some of the remaining genes maybe excellent candidates for further clinical investigation. If it is possible to identify a small set of genes that is indeed capable of providing complete discrimination, inexpensive diagnostic assays might be widely deployable in clinical settings.	Duke Univ, Dept Comp Sci, Durham, NC 27708 USA; Duke Univ, Dept Elect Engn, Durham, NC 27708 USA	Hartemink, AJ (reprint author), Duke Univ, Dept Comp Sci, Box 90129, Durham, NC 27708 USA.	amink@cs.duke.edu					ALBERT JH, 1993, J AM STAT ASSOC, V88, P669, DOI 10.2307/2290350; Alon U, 1999, P NATL ACAD SCI USA, V96, P6745, DOI 10.1073/pnas.96.12.6745; Furey TS, 2000, BIOINFORMATICS, V16, P906, DOI 10.1093/bioinformatics/16.10.906; Ambroise C, 2002, P NATL ACAD SCI USA, V99, P6562, DOI 10.1073/pnas.102102699; Golub TR, 1999, SCIENCE, V286, P531, DOI 10.1126/science.286.5439.531; Guyon I, 2002, MACH LEARN, V46, P389, DOI 10.1023/A:1012487302797; Coleman TF, 1996, SIAM J OPTIMIZ, V6, P418, DOI 10.1137/0806023; Tipping ME, 2001, J MACH LEARN RES, V1, P211, DOI 10.1162/15324430152748236; West M, 2001, P NATL ACAD SCI USA, V98, P11462, DOI 10.1073/pnas.201162998; BENDOR A, 2000, P 4 4 INT C COMP MOL; Cristianini N., 2000, INTRO SUPPORT VECTOR; Donoho D., 2002, OPTIMALLY SPARSE REP; FIGUEIREDO MAT, 2001, 2001 C COMP VIS PATT; FRANK A, 2002, THESIS TECHNION ISRA; FRIEDMAN J, 2004, IN PRESS ANN STAT; Fung G., 2002, FEATURE SELECTION NE; KRISHNAPURAM B, 2002, P 2002 WORKSH GEN SI; Li Y, 2002, BIOINFORMATICS, V18, P1332, DOI 10.1093/bioinformatics/18.10.1332; Neal R. M., 1996, BAYESIAN LEARNING NE; ROSSET S, 2003, NEUR INF PROC SYST, V15; ROTH V, 2003, IEEE T NEURAL NETWOR; Tibshirani R, 1996, J ROYAL STAT SOC B, V58; WESTON J, 2000, NEUR INF PROC SYST, V12; Xiong MM, 2001, MOL GENET METAB, V73, P239, DOI 10.1006/mgme.2001.3193; ZHU J, 2002, NEUR INF PROC SYST, V14	25	17	17	1	1	MARY ANN LIEBERT INC PUBL	LARCHMONT	2 MADISON AVENUE, LARCHMONT, NY 10538 USA	1066-5277			J COMPUT BIOL	J. Comput. Biol.		2004	11	2-3					227	242		10.1089/1066527041410463		16	Biochemical Research Methods; Biotechnology & Applied Microbiology; Computer Science, Interdisciplinary Applications; Mathematical & Computational Biology; Statistics & Probability	Biochemistry & Molecular Biology; Biotechnology & Applied Microbiology; Computer Science; Mathematical & Computational Biology; Mathematics	836XA	WOS:000222588300003	15285890	
J	Koenker, R; Mizera, I				Koenker, R; Mizera, I			Penalized triograms: total variation regularization for bivariate smoothing	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES B-STATISTICAL METHODOLOGY			English	Article						penalty methods; regularization; spline smoothing; total variation	ADAPTIVE REGRESSION SPLINES; POLYNOMIAL SPLINES; CROSS-VALIDATION; TENSOR-PRODUCTS; MODELS	Hansen, Kooperberg and Sardy introduced a family of continuous, piecewise linear functions defined over adaptively selected triangulations of the plane as a general approach to statistical modelling of bivariate densities and regression and hazard functions. These triograms enjoy a natural affine equivariance that offers distinct advantages over competing tensor product methods that are more commonly used in statistical applications. Triograms employ basis functions consisting of linear 'tent functions' defined with respect to a triangulation of a given planar domain. As in knot selection for univariate splines, Hansen and colleagues adopted the regression spline approach of Stone. Vertices of the triangulation are introduced or removed sequentially in an effort to balance fidelity to the data and parsimony. We explore a smoothing spline variant of the triogram model based on a roughness penalty adapted to the piecewise linear structure of the triogram model. We show that the roughness penalty proposed may be interpreted as a total variation penalty on the gradient of the fitted function. The methods are illustrated with real and artificial examples, including an application to estimated quantile surfaces of land value in the Chicago metropolitan area.	Univ Illinois, Dept Econ, Champaign, IL 61820 USA; Univ Alberta, Edmonton, AB T6G 2M7, Canada	Koenker, R (reprint author), Univ Illinois, Dept Econ, Champaign, IL 61820 USA.						Ambrosio L., 2000, FUNCTIONS BOUNDED VA; Davies PL, 2001, ANN STAT, V29, P1, DOI 10.1214/aos/996986501; Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010; RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; He XM, 1998, J ROY STAT SOC B, V60, P537, DOI 10.1111/1467-9868.00138; Portnoy S, 1997, STAT SCI, V12, P279; SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136; WAHBA G, 1980, MON WEATHER REV, V108, P1122, DOI 10.1175/1520-0493(1980)108<1122:SNMMFV>2.0.CO;2; BREIMAN L, 1991, TECHNOMETRICS, V33, P125, DOI 10.2307/1269038; De Giorgi E., 1954, ANN MAT PUR APPL, V36; DINCULEANN N, 1967, VECTOR MEASURES; DUCHON J., 1977, LECT NOTES MATH, V571, P85; Duchon J., 1976, REV FR AUTOM RECH OP, V10, P1; FICHERA G, 1954, LEZIONI TRANSFORMAZI, V1; FRIEDMAN JH, 1991, ANN STAT, V19, P1, DOI 10.1214/aos/1176347963; Green P, 1994, NONPARAMETRIC REGRES; GU C, 1989, SIAM J MATRIX ANAL A, V10, P457, DOI 10.1137/0610033; Hansen M, 1998, J AM STAT ASSOC, V93, P101; Hansen MH, 2002, STAT SCI, V17, P2, DOI 10.1214/ss/1023798997; HARDER R. L., 1972, J AIRCRAFT, V9, P189, DOI DOI 10.2514/3.44330; He XM, 1996, J MULTIVARIATE ANAL, V58, P162, DOI 10.1006/jmva.1996.0045; Jordan C., 1881, CR HEBD ACAD SCI, V2, P228; KOENKER R, 2002, STAT SCI, V17, P30; KOENKER R, 1994, BIOMETRIKA, V81, P673; KOENKER R, 2002, J STAT SOFTWR, V8, P1; Kronrod A., 1950, USP MAT NAUK, V5, P24; KRONROD AS, 1949, DOKL AKAD NAUK SSSR+, V66, P797; Mammen E, 1997, ANN STAT, V25, P387; Meinguet Jean, 1979, J APPL MATH PHYS, V30, P292; Meyer M, 2000, ANN STAT, V28, P1083; Natanson I. P., 1974, THEORY FUNCTIONS REA; Nicholls GK, 1998, J R STAT SOC B, V60, P643, DOI 10.1111/1467-9868.00145; OKABE A, 2000, SPATIAL TESSELATONS; Portnoy S, 1997, ANN STAT, V25, P414; RIPPA S, 1992, SIAM J NUMER ANAL, V29, P257, DOI 10.1137/0729017; RUDIN LI, 1994, P INT C IM PROC LOS, V1; Scherzer O, 1998, COMPUTING, V60, P1, DOI 10.1007/BF02684327; Serrin J., 1961, T AM MATH SOC, V101, P139, DOI DOI 10.1090/S0002-9947-1961-0138018-9; STONE CJ, 1994, ANN STAT, V22, P118, DOI 10.1214/aos/1176325361; Stone CJ, 1997, ANN STAT, V25, P1371; TASDIZEN T, 2003, IN PRESS ACM T GRAPH, V22; TONELLI L, 1936, ANN SC NORM SUP PI S, V5, P315; TONELLI L, 1926, REND ACC NAZ LINC, V3, P357; TONELLI L, 1926, REND ACC NAZ LINC, V3, P633; TONELLI L, 1926, REND ACC NAZ LINC, V3, P445; Wahba G., 1990, SPLINE MODELS OBSERV	47	37	38	1	3	BLACKWELL PUBL LTD	OXFORD	108 COWLEY RD, OXFORD OX4 1JF, OXON, ENGLAND	1369-7412			J ROY STAT SOC B	J. R. Stat. Soc. Ser. B-Stat. Methodol.		2004	66		1				145	163		10.1111/j.1467-9868.2004.00437.x		19	Statistics & Probability	Mathematics	756CN	WOS:000187448400010		
B	Kim, SP; Rao, YN; Erdogmus, D; Principe, JC		Barros, A; Principe, J; Larsen, J; Adali, T; Douglas, S		Kim, SP; Rao, YN; Erdogmus, D; Principe, JC			Tracking of multivariate time-variant systems based on on-line variable selection	MACHINE LEARNING FOR SIGNAL PROCESSING XIV			English	Proceedings Paper	14th IEEE International Workshop on Machine Learning for Signal Processing	SEP 29-OCT 01, 2004	Sao Luis, BRAZIL	IEEE Signal Proc Soc, TC Machine Learning Signal Proc			ALGORITHM; PERFORMANCE; LMS; RLS	Tracking time-variant systems has been of great interest in many engineering fields. Specifically, when system statistics change both in space (multivariate) and time with a short stationary regime, conventional adaptive algorithms suffer from the trade-off between convergence rate and accuracy. In this paper, we propose a tracking system consisting of a linear adaptive system accompanied by an on-fine variable selection algorithm that is based on the least angle regression algorithm. This algorithm explicitly employs local (in time) correlation between the input and the output of an unknown system to select a subset of input variables at every time step. Therefore, it enables the multivariate adaptive filter to track the temporal changes of correlated variables. Simulations involving tracking of multi-channel time-variant systems demonstrate superior performance of the proposed approach when compared with the conventional methods.	Univ Florida, Computat NeuroEngn Lab, Gainesville, FL 32611 USA	Kim, SP (reprint author), Univ Florida, Computat NeuroEngn Lab, Gainesville, FL 32611 USA.						Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; EFRON B, 2004, IN PRESS ANN STAT; Erdogmus D., 2002, THESIS U FLORIDA; EWEDA E, 1994, IEEE T SIGNAL PROCES, V42, P2937, DOI 10.1109/78.330354; Guyon I., 2003, Journal of Machine Learning Research, V3, DOI 10.1162/153244303322753616; HAJIVANDI M, 1990, IEEE T ACOUST SPEECH, V38, P1953, DOI 10.1109/29.103096; Hastie T., 2001, ELEMENTS STAT LEARNI; Haykin S., 1996, ADAPTIVE FILTER THEO; HAYKIN S, 1995, IEEE MILCOM 95, V2, P602; KROGH A, 1992, ADV NEUR IN, V4, P950; MACCHI OM, 1991, IEEE T SIGNAL PROCES, V39, P583; Nishiyama K, 2004, IEEE T SIGNAL PROCES, V52, P1335, DOI 10.1109/TSP.2004.826156; Weisberg S., 1980, APPL LINEAR REGRESSI	13	0	0	0	2	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA			0-7803-8608-6				2004							123	132				10	Computer Science, Artificial Intelligence; Computer Science, Software Engineering	Computer Science	BBK54	WOS:000225881500013		
S	Roth, V; Lange, T		Rasmussen, CE; Bulthoff, HH; Giese, MA; Scholkopf, B		Roth, V; Lange, T			Adaptive feature selection in image segmentation	PATTERN RECOGNITION	LECTURE NOTES IN COMPUTER SCIENCE		English	Article; Proceedings Paper	26th Annual Pattern Recognition of the German-Association-for-Pattern-Recognition	AUG 30-SEP 01, 2004	Tubingen, GERMANY	German Assoc Pattern Recognit			LASSO	Most image segmentation algorithms optimize some mathematical similarity criterion derived from several low-level image features. One possible way of combining different types of features, e.g. color- and texture features on different scales and/or different orientations, is to simply stack all the individual measurements into one high-dimensional feature vector. Due to the nature of such stacked vectors, however, only very few components (e.g. those which are defined on a suitable scale) will carry information that is relevant for the actual segmentation task. We present an approach to combining segmentation and adaptive feature selection that overcomes this relevance determination problem. All free model parameters of this method are selected by a resampling-based stability analysis. Experiments demonstrate that the built-in feature selection mechanism leads to stable and meaningful partitions of the images.	ETH, Inst Computat Sci, CH-8092 Zurich, Switzerland	Roth, V (reprint author), ETH, Inst Computat Sci, Hirschengraben 84, CH-8092 Zurich, Switzerland.	vroth@inf.ethz.ch; langet@inf.ethz.ch					HASTIE T, 1994, J AM STAT ASSOC, V89, P1255, DOI 10.2307/2290989; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1; Manjunath BS, 1996, IEEE T PATTERN ANAL, V18, P837, DOI 10.1109/34.531803; BELONGIE S, 1998, INT C COMPUTER VISIO; Ben-Dor A., 2001, P 5 ANN INT C COMP M, P31, DOI DOI 10.1145/369133.369167; FIGUEIREDO M, 2001, CVPR2001; Hastie Trevor, 1996, J R STAT SOC B, V58, P158; Kuhn H., 1955, NAV RES LOG, V2, P83, DOI DOI 10.1002/NAV.3800020109; LANGE T, 2003, ADV NEURAL INFORMATI, V15; LAW MH, 2003, ADV NEURAL INFORMATI, V15; Osborne MR, 2000, J COMPUT GRAPH STAT, V9, P319, DOI 10.2307/1390657; ROTH V, 2004, IEEE T BIOMEDICAL EN, V51; ROTH V, 2004, ADV NEURAL INFORMATI, V16; VONHEYDEBRECK A, 2001, BIOINFORMATICS, P17	15	4	4	0	8	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		3-540-22945-0	LECT NOTES COMPUT SC			2004	3175						9	17				9	Computer Science, Artificial Intelligence; Computer Science, Theory & Methods	Computer Science	BAV54	WOS:000223798300002		
S	Nudelman, E; Leyton-Brown, K; Hoos, HH; Devkar, A; Shoham, Y		Wallace, M		Nudelman, E; Leyton-Brown, K; Hoos, HH; Devkar, A; Shoham, Y			Understanding random SAT: Beyond the clauses-to-variables ratio	PRINCIPLES AND PRACTICE OF CONSTRAINT PROGRAMMING - CP 2004, PROCEEDINGS	Lecture Notes in Computer Science		English	Article; Proceedings Paper	10th International Conference on the Principles and Practice of Constraint Programming	SEP 27-OCT 01, 2004	Toronto, CANADA	Intelligent Informat Syst Inst, Amer Assoc Artificial Intelligence, Parc Technologies Ltd, ILOG Inc, Swedish Inst Comp Sci, CoLogNET, Microsoft Res, Natl ICT, Cork Constraint Computat Ctr, Dash Optimizat Ltd, Constraint Programming Organizing Comm			SATISFIABILITY	It is well known that the ratio of the number of clauses to the number of variables in a random k-SAT instance is highly correlated with the instance's empirical hardness. We consider the problem of identifying such features of random SAT instances automatically using machine learning. We describe and analyze models for three SAT solvers - kcnfs, oksolver and satz- and for two different distributions of instances: uniform random 3-SAT with varying ratio of clauses-to-variables, and uniform random 3-SAT with fixed ratio of clauses-to-variables. We show that surprisingly accurate models can be built in all cases. Furthermore, we analyze these models to determine which features are most useful in predicting whether an instance will be hard to solve. Finally we discuss the use of our models to build SATzi11a, an algorithm portfolio for SAV(1).	Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA; Univ British Columbia, Dept Comp Sci, Vancouver, BC V6T 1W5, Canada	Nudelman, E (reprint author), Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.	eugnud@cs.stanford.edu; kevinlb@cs.ubc.ca; hoos@cs.ubc.ca; avd@cs.stanford.edu; shoham@cs.stanford.edu					Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Monasson R, 1999, NATURE, V400, P133; Hoos HH, 1999, ARTIF INTELL, V112, P213, DOI 10.1016/S0004-3702(99)00048-X; Bacchus Fahiem, 2003, P SAT 2003, P341; Cheeseman P., 1991, P 12 INT JOINT C ART, V1, P331; Gomes CP, 2000, J AUTOM REASONING, V24, P67, DOI 10.1023/A:1006314320276; HOOS HH, 2004, IN PRESS STOCHASTIC; Hoos H.H., 1999, P 16 INT JOINT C ART, P296; Horvitz E., 2001, P 17 C UNC ART INT U, P235; KOLAITIS P, 2003, P IJCAI 2003; Leyton-Brown K., 2002, Principles and Practice of Constraint Programming - CP 2002. 8th International Conference, CP 2002. Proceedings (Lecture Notes in Computer Science Vol.2470); Leyton-Brown K, 2003, LECT NOTES COMPUT SC, V2833, P899; Leyton-Brown K., 2003, P 18 INT JOINT C ART, P1542; Lobjois L., 1998, Proceedings Fifteenth National Conference on Artificial Intelligence (AAAI-98). Tenth Conference on Innovative Applications of Artificial Intelligence; Ruan Y., 2002, P 8 INT C PRINC PRAC, P573; Selman B, 1996, ARTIF INTELL, V81, P17, DOI 10.1016/0004-3702(95)00045-3; Tompkins D. A. D., 2004, P 7 INT C THEOR APPL, P37; Williams R., 2003, P 18 INT JOINT C ART, P1173	18	40	40	0	0	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		3-540-23241-9	LECT NOTES COMPUT SC			2004	3258						438	452				15	Computer Science, Theory & Methods	Computer Science	BAX56	WOS:000224107900033		
S	Brown, M; Costen, NP; Akamatsu, S		Kittler, J; Petrou, M; Nixon, M		Brown, M; Costen, NP; Akamatsu, S			Efficient calculation of the complete optimal classification set	PROCEEDINGS OF THE 17TH INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION, VOL 2	INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION		English	Proceedings Paper	17th International Conference on Pattern Recognition (ICPR)	AUG 23-26, 2004	Cambridge, ENGLAND	Int Assoc Pattern Recognit, Univ Surrey, UniS, IEEE Comp Soc, HP Res Labs Bristol	British Machine Vis Assoc			Feature and structure selection is an important part of many classification problems. In previous papers, an approach called basis pursuit classification has been proposed which poses feature selection as a regularization problem using a 1-norm to measure parameter complexity. In addition, a complete optimal parameter set, here called the locus, can be calculated which contains every optimal collection of sparse features as a function of the regularization parameter This paper considers how to iteratively calculate the parameter locus using a set of rank-1 inverse matrix updates. The algorithm is tested on both artificial and real data and it is shown that the computational cost is reduced from a cubed to a squared problem in the number of features.	UMIST, Control Syst Ctr, Manchester M60 1QD, Lancs, England	Brown, M (reprint author), UMIST, Control Syst Ctr, Manchester M60 1QD, Lancs, England.						Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Guo GD, 2001, IMAGE VISION COMPUT, V19, P631, DOI 10.1016/S0262-8856(01)00046-4; Bradley PS, 1998, INT C MACH LEARN, P82; BROWN M, 2003, INT WORKSH ARTIFICAL; COSTEN NP, 2003, BRIT MACHINE VISION, P13; EDWARDS GJ, 1996, IEEE INT C FAC GEST, P328; Jesorskya O., 2000, AUDIO VIDEO BASED PE, P90; Orr M. J. L., 1996, INTRO RADIAL BASIS F; Vapnik V., 1998, STAT LEARNING THEORY	10	1	1	1	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA	1051-4651		0-7695-2128-2	INT C PATT RECOG			2004							307	310		10.1109/ICPR.2004.1334183		4	Computer Science, Artificial Intelligence	Computer Science	BAW22	WOS:000223877400074		
B	Costen, NP; Brown, M; Akamatsu, S			ieee computer society	Costen, NP; Brown, M; Akamatsu, S			Sparse models for gender classification	SIXTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS			English	Proceedings Paper	6th IEEE International Conference on Automatic Face and Gesture Recognition	MAY 17-19, 2004	Seoul, SOUTH KOREA	Korea Univ, Ctr Artificial Vis Res, IEEE Comp Soc, Korea Informat Sci Soc, Korea Sci & Engn Fdn, Minist Informat & Commun, USAF Off Sci Res, Watch Vis Inc				A class of sparse regularization functions are considered for the developing sparse classifiers for determining facial gender The sparse classification method aims to both select the most important features and maximize the classification margin, in a manner similar to support vector machines. An efficient process for directly calculating the complete set of optimal, sparse classifiers is developed. A single classification hyper-plane which maximizes posterior probability of describing training data is then efficiently selected. The classifier is tested on a Japanese gender-divided ensemble, described via a collection of appearance models. Performance is comparable with a linear SVM, and allows effective manipulation of apparent gender.	Manchester Metropolitan Univ, Dept Comp & Math, Manchester M1 5GD, Lancs, England	Costen, NP (reprint author), Manchester Metropolitan Univ, Dept Comp & Math, Manchester M1 5GD, Lancs, England.						Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Moghaddam B, 2002, IEEE T PATTERN ANAL, V24, P707, DOI 10.1109/34.1000244; Guo GD, 2001, IMAGE VISION COMPUT, V19, P631, DOI 10.1016/S0262-8856(01)00046-4; Bradley PS, 1998, INT C MACH LEARN, P82; BROWN M, 2003, INT WORKSH ART NEUR; BRUNELLI R, 1993, IEEE T PATTERN ANAL, V15, P1042, DOI 10.1109/34.254061; Cootes T. F., 1998, P EUR C COMP VIS, V2, P484; Edelman Betty, 1998, Journal of Biological Systems, V6, P241, DOI 10.1142/S0218339098000170; EDWARDS GJ, 1997, BRIT MACH VIS C, P130; FIGUEIREDO MAT, 2002, ADV NEURAL INFORMATI, V14, P705; GOLOMB BA, 1991, NIPS 3, P572; Gutta S., 1998, IEEE INT C AUT FAC G, P194; Neal R.M., 1996, LECT NOTES STAT, V118; PHILLIPS PJ, 1999, NEURAL INFORMATION P, P803; Vapnik V., 1998, STAT LEARNING THEORY	16	4	4	1	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA			0-7695-2122-3				2004							201	206		10.1109/AFGR.2004.1301531		6	Automation & Control Systems; Computer Science, Artificial Intelligence	Automation & Control Systems; Computer Science	BAG24	WOS:000222059800031		
B	Morgenthaler, S; Welsch, RE; Zenide, A		Huber, M; Pison, G; Struyf, A; VanAelst, S		Morgenthaler, S; Welsch, RE; Zenide, A			Algorithms for robust model selection in linear regression	THEORY AND APPLICATION OF RECENT ROBUST METHODS	STATISTICS FOR INDUSTRY AND TECHNOLOGY		English	Proceedings Paper	International Conference on Robust Statistics (ICORS 2003)	JUL 13-18, 2003	Antwerp, BELGIUM	Univ Antwerp, Minerva Fdn Princeton, Fund Sci Res Flanders, Belgian Stat Soc, KBC Banking & Insurance, SAS Inst		variable selection; outliers; cross-validation; robust regression	VARIABLE SELECTION; CROSS-VALIDATION; SMALL SAMPLES; SQUARES	In modelling situations, we often have a choice. We can add to model complexity to describe some unusual observations or groups of observations or we can remove those observations and fit the rest with a different, perhaps simpler, model. Or, we might put weights on observations and/or variables and use all of the data or all of the model complexity or some combination. To decide what is unusual generally requires some sort. of model. but that very model may be what is causing some observations to be seen as unusual or, in fact, masking unusual observations. When confronted both by unnecessary model complexity and by unusual observations, a model selection technique must be able to identify the correct, model structure as well as unusual observations. This paper proposes some new algorithms that are designed to address these problems and compares them with currently available approaches such as robust C-p and robust cross-validated selection.	Ecole Polytech Fed Lausanne, Inst Math, CH-1015 Lausanne, Switzerland	Morgenthaler, S (reprint author), Ecole Polytech Fed Lausanne, Inst Math, CH-1015 Lausanne, Switzerland.						Akaike H, 1972, 2ND P INT S INF THEO, p[267, 281]; ROUSSEEUW PJ, 1984, J AM STAT ASSOC, V79, P871, DOI 10.2307/2288718; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; SHIBATA R, 1981, BIOMETRIKA, V68, P45; HURVICH CM, 1989, BIOMETRIKA, V76, P297, DOI 10.1093/biomet/76.2.297; FRANK IE, 1993, TECHNOMETRICS, V35, P109, DOI 10.2307/1269656; SHAO J, 1993, J AM STAT ASSOC, V88, P486, DOI 10.2307/2290328; AKAIKE H, 1969, ANN I STAT MATH, V21, P243, DOI 10.1007/BF02532251; MALLOWS CL, 1973, TECHNOMETRICS, V15, P661, DOI 10.2307/1267380; ANSCOMBE FJ, 1967, J ROY STAT SOC B, V29, P1; BILLOR N, 1989, COMPUT STAT DATA AN, V34, P279; FERNHOLZ LT, 2004, IN PRESS STAT DATA A, V21; Hastie T., 2001, ELEMENTS STAT LEARNI; HAWKINS DM, 1984, TECHNOMETRICS, V26, P197, DOI 10.2307/1267545; Hoeting J, 1996, COMPUT STAT DATA AN, V22, P251, DOI 10.1016/0167-9473(95)00053-4; HURVICH CM, 1990, STAT PROBABIL LETT, V9, P259, DOI 10.1016/0167-7152(90)90065-F; Riani M, 2000, ROBUST DIAGNOSTIC RE; RONCHETTI E, 1994, J AM STAT ASSOC, V89, P550, DOI 10.2307/2290858; RONCHETTI E, 1985, STAT PROBABIL LETT, V3, P21, DOI 10.1016/0167-7152(85)90006-9; Ronchetti E, 1997, J AM STAT ASSOC, V92, P1017, DOI 10.2307/2965566; Rousseeuw P. J., 1987, ROBUST REGRESSION OU; SUGIURA N, 1978, COMMUN STAT A-THEOR, V7, P13, DOI 10.1080/03610927808827599; TURKEY JW, 1967, J ROY STAT SOC B MET, V29, P47	24	3	3	1	3	BIRKHAUSER VERLAG AG	BASEL	VIADUKSTRASSE 40-44, PO BOX 133, CH-4010 BASEL, SWITZERLAND			3-7643-7060-2	STAT IND TECHNOL			2004							195	206				12	Mathematics, Applied; Statistics & Probability	Mathematics	BBK72	WOS:000225886700018		
J	Li, L; Huang, J; Sun, S; Shen, JZ; Unverzagt, FW; Gao, SJ; Hendrie, HH; Hall, K; Hui, SL				Li, L; Huang, J; Sun, S; Shen, JZ; Unverzagt, FW; Gao, SJ; Hendrie, HH; Hall, K; Hui, SL			Selecting pre-screening items for early intervention trials of dementia - a case study	STATISTICS IN MEDICINE			English	Article; Proceedings Paper	2nd International Conference on Statistical Methodology on Alzheimers Disease Research	MAY 10-12, 2002	LEXINGTON, KENTUCKY			discrimination; classification; LASSO; logistic regression; neural network; decision tree	COGNITIVE IMPAIRMENT; INDIANAPOLIS; LASSO; SHRINKAGE; NIGERIA; IBADAN	Our goal was to review and extend statistical methods for discriminating between normal subjects and those with dementia or cognitive impairment. We compared six different methods to one constructed by expert opinion, in their brevity and predictive power. The methods include logistic regression and neural networks, with standard and least absolute shrinkage and selection operator (LASSO) variable selection, as well as decision trees with and without boosting. These methods were applied to the baseline data of a subgroup of subjects in a dementia study, using their screening interview items to predict their clinical diagnosis of normal or non-normal (cognitively impaired or demented). The derived models were then validated on a different subgroup of subjects in the same study who had the screening and clinical diagnosis two to five years later. Performance of different models was compared based on their sensitivity and specificity in the validation sample. Generally, the six statistical methods performed slightly to moderately better than the expert-opinion model. Neural networks generally performed better than the logistic and decision tree models. LASSO improved the performance of logistic and neural network models, but it eliminated few input variables in the neural network. The single decision tree performed at least as well as the standard logistic model, and with fewer items, making it an attractive pre-screening tool. Using the boosting option for decision trees did not substantially improve the performance. We recommend that for each situation, different methods of classification should be attempted to obtain optimal results for a given purpose. Copyright (C) 2004 John Wiley Sons, Ltd.	Indiana Univ, Sch Med, Div Biostat, Indianapolis, IN 46202 USA; Indiana Univ Purdue Univ, Dept Comp & Informat Sci, Indianapolis, IN 46202 USA; Univ Michigan, Dept Biostat, Ann Arbor, MI 48109 USA; Indiana Univ, Sch Med, Dept Psychiat, Indianapolis, IN 46202 USA; Regenstrief Inst Hlth Care, Indianapolis, IN 46202 USA	Hui, SL (reprint author), Regenstrief Inst Hlth Care, 1050 Wishard Blvd,RG6, Indianapolis, IN 46202 USA.	shui@iupui.edu					Fu WJJ, 1998, J COMPUT GRAPH STAT, V7, P397, DOI 10.2307/1390712; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Hendrie HC, 2001, JAMA-J AM MED ASSOC, V285, P739, DOI 10.1001/jama.285.6.739; Breiman L, 2001, STAT SCI, V16, P199, DOI 10.1214/ss/1009213726; [Anonymous], 1987, DIAGN STAT MAN MENT, P97; Unverzagt FW, 2001, NEUROLOGY, V57, P1655; Bishop C. M., 1995, NEURAL NETWORKS PATT; Breiman L., 1984, CLASSIFICATION REGRE; Callahan CM, 2002, MED CARE, V40, P771, DOI 10.1097/01.MLR.0000024610.33213.C8; FOLSTEIN MF, 1975, J PSYCHIAT RES, V12, P189, DOI 10.1016/0022-3956(75)90026-6; Freund Y., 1996, Machine Learning. Proceedings of the Thirteenth International Conference (ICML '96); Hall KS, 1996, INT J METHOD PSYCH, V6, P129, DOI 10.1002/(SICI)1234-988X(199610)6:3<129::AID-MPR164>3.3.CO;2-A; LeBlanc M, 1998, J COMPUT GRAPH STAT, V7, P417, DOI 10.2307/1390674; Ripley B. D., 1996, PATTERN RECOGNITION; SUN X, 2000, THESIS U TORONTO; Tibshirani R, 1997, STAT MED, V16, P385, DOI 10.1002/(SICI)1097-0258(19970228)16:4<385::AID-SIM380>3.0.CO;2-3	16	4	4	1	1	JOHN WILEY & SONS LTD	CHICHESTER	THE ATRIUM, SOUTHERN GATE, CHICHESTER PO19 8SQ, W SUSSEX, ENGLAND	0277-6715			STAT MED	Stat. Med.	JAN 30	2004	23	2					271	283		10.1002/sim.1715		13	Mathematical & Computational Biology; Public, Environmental & Occupational Health; Medical Informatics; Medicine, Research & Experimental; Statistics & Probability	Mathematical & Computational Biology; Public, Environmental & Occupational Health; Medical Informatics; Research & Experimental Medicine; Mathematics	763TW	WOS:000188117400011	14716728	
J	Buhlmann, P; Yu, B				Buhlmann, P; Yu, B			The golden chain - Discussion	ANNALS OF STATISTICS			English	Editorial Material							CLASSIFICATION TREES; REGRESSION; LASSO	Jiang, Lugosi and Vayatis, and Zhang ought to be congratulated for their different works on the original AdaBoost algorithm with early stopping (Jiang), an l(1)-penalized version of boosting (Lugosi and Vayatis) and a convex minimization method which can be viewed as an l(2)-penalized version of boosting (Zhang).	ETH Zentrum, CH-8092 Zurich, Switzerland; Univ Calif Berkeley, Dept Stat, Berkeley, CA 94720 USA	Buhlmann, P (reprint author), ETH Zentrum, LEO C17, CH-8092 Zurich, Switzerland.	buhlmann@stat.math.ethz.ch; binyu@stat.berkeley.edu	Buhlmann, Peter/A-2107-2013				Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451; Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Buhlmann P, 2003, J AM STAT ASSOC, V98, P324, DOI 10.1198/016214503000125; Breiman L, 2004, ANN STAT, V32, P1; BUHLMANN P, 2002, CONSISTENCY L2 BOOST; EFRON B, 2004, IN PRESS ANN STAT; Geman D, 2001, IEEE T INFORM THEORY, V47, P1075, DOI 10.1109/18.915664; Kim H, 2001, J AM STAT ASSOC, V96, P589, DOI 10.1198/016214501753168271; Mason L, 2000, ADV NEUR IN, P221; Osborne MR, 2000, J COMPUT GRAPH STAT, V9, P319, DOI 10.2307/1390657; ZHANG T, 2003, 635 U CAL DEP STAT	12	2	2	0	1	INST MATHEMATICAL STATISTICS	BEACHWOOD	PO BOX 22718, BEACHWOOD, OH 44122 USA	0090-5364			ANN STAT	Ann. Stat.	FEB	2004	32	1					96	101				6	Statistics & Probability	Mathematics	809BQ	WOS:000220612600008		
J	Kadane, JB; Lazar, NA				Kadane, JB; Lazar, NA			Methods and criteria for model selection	JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION			English	Review						AIC; Bayes factors; BIC; Mallow's C-p; model averaging; subset selection; variable selection	BAYESIAN VARIABLE SELECTION; MONTE-CARLO METHODS; MARKOV-CHAIN; REGRESSION-MODELS; NORMALIZING CONSTANTS; INFLATION CRITERION; MULTIPLE-REGRESSION; LINEAR-REGRESSION; GRAPHICAL MODELS; CHOICE	Model selection is an important part of any statistical analysis and, indeed, is central to the pursuit of science in general. Many authors have examined the question of model selection from both frequentist and Bayesian perspectives, and many tools for selecting the "best model" have been suggested in the literature. This paper considers the various proposals from a Bayesian decision-theoretic perspective.	Carnegie Mellon Univ, Dept Stat, Pittsburgh, PA 15213 USA	Kadane, JB (reprint author), Carnegie Mellon Univ, Dept Stat, Pittsburgh, PA 15213 USA.	kadane@stat.cmu.edu; nlazar@stat.cmu.edu					Agresti A, 1990, CATEGORICAL DATA ANA; AITKIN M, 1991, J ROY STAT SOC B MET, V53, P111; Akaike H, 1983, B INT STAT I, V44, P277; Akaike H., 1973, 2 INT S INF THEOR, P267; SMITH AFM, 1980, J ROY STAT SOC B MET, V42, P213; EFRON B, 1979, ANN STAT, V7, P1, DOI 10.1214/aos/1176344552; MACKAY DJC, 1992, NEURAL COMPUT, V4, P415, DOI 10.1162/neco.1992.4.3.415; BREIMAN L, 1995, TECHNOMETRICS, V37, P373, DOI 10.2307/1269730; KASS RE, 1995, J AM STAT ASSOC, V90, P773, DOI 10.1080/01621459.1995.10476572; Han C, 2001, J AM STAT ASSOC, V96, P1122, DOI 10.1198/016214501753208780; CARLIN BP, 1995, J ROY STAT SOC B MET, V57, P473; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; FURNIVAL GM, 1974, TECHNOMETRICS, V16, P499, DOI 10.2307/1267601; BREIMAN L, 1992, J AM STAT ASSOC, V87, P738, DOI 10.2307/2290212; GEISSER S, 1979, J AM STAT ASSOC, V74, P153, DOI 10.2307/2286745; KADANE JB, 1980, J AM STAT ASSOC, V75, P845, DOI 10.2307/2287171; Raftery AE, 1997, J AM STAT ASSOC, V92, P179, DOI 10.2307/2291462; Gelman A, 1998, STAT SCI, V13, P163; George EI, 2000, J AM STAT ASSOC, V95, P1304, DOI 10.2307/2669776; GEORGE EI, 1993, J AM STAT ASSOC, V88, P881, DOI 10.2307/2290777; LAWLESS JF, 1978, BIOMETRICS, V34, P318, DOI 10.2307/2530022; Chib S, 1995, J AM STAT ASSOC, V90, P1313, DOI 10.2307/2291521; CARLIN BP, 1991, CAN J STAT, V19, P399, DOI 10.2307/3315430; ALLEN DM, 1974, TECHNOMETRICS, V16, P125, DOI 10.2307/1267500; MADIGAN D, 1994, J AM STAT ASSOC, V89, P1535, DOI 10.2307/2291017; DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425; SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136; Berger JO, 1996, J AM STAT ASSOC, V91, P109, DOI 10.2307/2291387; MALLOWS CL, 1973, TECHNOMETRICS, V15, P661, DOI 10.2307/1267380; Richardson S, 1997, J ROY STAT SOC B MET, V59, P731, DOI 10.1111/1467-9868.00095; LINDLEY DV, 1976, ANN STAT, V4, P1, DOI 10.1214/aos/1176343343; BAYARRI MJ, 1999, BAYESIAN STAT, V6, P357; Berger J. O., 1998, SANKHYA B, V60, P1; BERGER JO, 1997, P WORKSH MOD SEL, P1; Bernardo J. M., 1994, BAYESIAN THEORY; Bernardo JM, 2002, INT STAT REV, V70, P351, DOI 10.2307/1403862; BERTOLINO F, 1997, METRON, V54, P5; BOX GEP, 1980, J ROY STAT SOC A STA, V143, P383, DOI 10.2307/2982063; Box G.E.P., 1992, BAYESIAN INFERENCE S; Brown PJ, 1998, J R STAT SOC B, V60, P627, DOI 10.1111/1467-9868.00144; Brown PJ, 1999, BIOMETRIKA, V86, P635; DiCiccio TJ, 1997, J AM STAT ASSOC, V92, P903, DOI 10.2307/2965554; DICKEY JM, 1986, BAYESIAN INFERENCE D, P177; DRAPER D, 1995, J R STAT SOC B, V57, P45; Draper N, 1981, APPL REGRESSION ANAL; Efron B., 1982, JACKKNIFE BOOTSTRAP; FOSTER DP, 1994, ANN STAT, V22, P1947, DOI 10.1214/aos/1176325766; Garthwaite PH, 2001, J ROY STAT SOC B, V63, P95, DOI 10.1111/1467-9868.00278; GARTHWAITE PH, 1992, ANN STAT, V20, P1697, DOI 10.1214/aos/1176348886; GELFAND AE, 1994, J ROY STAT SOC B MET, V56, P501; Gelfand AE, 1998, BIOMETRIKA, V85, P1, DOI 10.1093/biomet/85.1.1; George EI, 2000, BIOMETRIKA, V87, P731, DOI 10.1093/biomet/87.4.731; George EI, 1997, STAT SINICA, V7, P339; GEWEKE J, 1981, INT ECON REV, V22, P55, DOI 10.2307/2526135; GOLDSTEIN M, 1991, J ROY STAT SOC B MET, V53, P134; Good I.J., 1950, PROBABILITY WEIGHTIN; GRAYBILL FA, 1976, THEORY APPL LINEAR; Green PJ, 1995, BIOMETRIKA, V82, P711, DOI 10.1093/biomet/82.4.711; Hastie T, 1992, STAT MODELS S; Jeffreys H., 1961, THEORY PROBABILITY; Kadane J.B., 1980, EVALUATION ECONOMETR, P245; KADANE JB, 1998, J ROY STAT SOC D-STA, V47, P3, DOI 10.1111/1467-9884.00113; KATZ RW, 1981, TECHNOMETRICS, V23, P243, DOI 10.2307/1267787; KENNARD RW, 1971, TECHNOMETRICS, V13, P899, DOI 10.2307/1266968; Key J. T., 1999, BAYESIAN STAT, V6, P343; KOEHLER AB, 1988, APPL STAT-J ROY ST C, V37, P187, DOI 10.2307/2347338; Kuo L., 1998, SANKHYA B, V60, P65; LAUD PW, 1995, J ROY STAT SOC B MET, V57, P247; Leamer E. E., 1978, SPECIFICATION SEARCH; LINDLEY DV, 1991, J ROYAL STAT SOC B, V53, P130; LINDLEY DV, 1968, J R STAT SOC B, V30, P31; LINDLEY DV, 1957, BIOMETRIKA, V44, P187; Lindley DV, 1997, J STAT PLAN INFER, V61, P181, DOI 10.1016/S0378-3758(96)00189-9; MADIGAN D, 1995, INT STAT REV, V63, P215, DOI 10.2307/1403615; MALLOWS CL, 1995, TECHNOMETRICS, V37, P362, DOI 10.2307/1269729; Meng XL, 1996, STAT SINICA, V6, P831; Miller A, 2002, SUBSET SELECTION REG, V2nd; MITCHELL TJ, 1988, J AM STAT ASSOC, V83, P1023, DOI 10.2307/2290129; O'Hagan A., 1991, J R STAT SOC B, V53, P136; OHagan A, 1997, TEST, V6, P101, DOI 10.1007/BF02564428; OHAGAN A, 1998, J ROY STAT SOC D-STA, V47, P21, DOI 10.1111/1467-9884.00114; OHAGAN A, 1995, J ROY STAT SOC B MET, V57, P99; PETRONE S, 1997, P WORKSH MOD SEL, P355; PICCINATO L, 1997, P WORKSH MOD SEL, P350; San Martini A., 1984, J ROYAL STAT SOC B, V46, P296; SHAFER G, 1982, J AM STAT ASSOC, V77, P325, DOI 10.2307/2287244; STONE M, 1974, J R STAT SOC B, V36, P111; Tibshirani R, 1999, J ROY STAT SOC B, V61, P529, DOI 10.1111/1467-9868.00191; Weisberg S., 1985, APPL LINEAR REGRESSI; Whittaker J, 1990, GRAPHICAL MODELS APP; WINKLER RL, 1999, BAYESIAN STAT, V6, P369	91	99	101	6	24	AMER STATISTICAL ASSOC	ALEXANDRIA	1429 DUKE ST, ALEXANDRIA, VA 22314 USA	0162-1459			J AM STAT ASSOC	J. Am. Stat. Assoc.	MAR	2004	99	465					279	290		10.1198/016214504000000269		12	Statistics & Probability	Mathematics	809LM	WOS:000220638200026		
J	Efron, B; Hastie, T; Johnstone, I; Tibshirani, R				Efron, B; Hastie, T; Johnstone, I; Tibshirani, R			Least angle regression	ANNALS OF STATISTICS			English	Article						lasso; boosting; linear regression; coefficient paths; variable selection	SELECTION; LASSO	The purpose of model selection algorithms such as All Subsets, Forward Selection and Backward Elimination is to choose a linear model on the basis of the same set of data to which the model will be applied. Typically we have available a large collection of possible covariates from which we hope to select a parsimonious set for the efficient prediction of a response variable. Least Angle Regression (LARS), a new model selection algorithm, is a useful and less greedy version of traditional forward selection methods. Three main properties are derived: (1) A simple modification of the LARS algorithm implements the Lasso, an attractive version of ordinary least squares that constrains the sum of the absolute regression coefficients; the LARS modification calculates all possible Lasso estimates for a given problem, using an order of magnitude less computer time than previous methods. (2) A different LARS modification efficiently implements Forward Stagewise linear regression, another promising new model selection method; this connection explains the similar numerical results previously observed for the Lasso and Stagewise, and helps us understand the properties of both methods, which are seen as constrained versions of the simpler LARS algorithm. (3) A simple approximation for the degrees of freedom of a LARS estimate is available, from which we derive a C-p estimate of prediction error; this allows a principled choice among the range of possible LARS estimates. LARS and its variants are computationally efficient: the paper describes a publicly available algorithm that requires only the same order of magnitude of computational effort as ordinary least squares applied to the full set of covariates.	Stanford Univ, Dept Stat, Stanford, CA 94305 USA	Efron, B (reprint author), Stanford Univ, Dept Stat, Sequoia Hall, Stanford, CA 94305 USA.	brad@stat.stanford.edu					Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451; Friedman J, 2000, ANN STAT, V28, P337, DOI 10.1214/aos/1016218223; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Osborne MR, 2000, IMA J NUMER ANAL, V20, P389, DOI 10.1093/imanum/20.3.389; Efron B, 1997, J AM STAT ASSOC, V92, P548, DOI 10.2307/2965703; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; MALLOWS CL, 1973, TECHNOMETRICS, V15, P661, DOI 10.2307/1267380; Breiman L., 1984, CLASSIFICATION REGRE; EFRON B, 1986, J AM STAT ASSOC, V81, P461, DOI 10.2307/2289236; Golub G. H., 1983, MATRIX COMPUTATION; Hastie T., 2001, ELEMENTS STAT LEARNI; Lawson C. L., 1974, SOLVING LEAST SQUARE; Meyer M, 2000, ANN STAT, V28, P1083; Osborne MR, 2000, J COMPUT GRAPH STAT, V9, P319, DOI 10.2307/1390657; Rao C. R., 1973, LINEAR STAT INFERENC; STEIN CM, 1981, ANN STAT, V9, P1135, DOI 10.1214/aos/1176345632; Weisberg S., 1980, APPL LINEAR REGRESSI; Ye JM, 1998, J AM STAT ASSOC, V93, P120, DOI 10.2307/2669609	18	2159	2253	17	120	INST MATHEMATICAL STATISTICS	BEACHWOOD	PO BOX 22718, BEACHWOOD, OH 44122 USA	0090-5364			ANN STAT	Ann. Stat.	APR	2004	32	2					407	451				45	Statistics & Probability	Mathematics	820TA	WOS:000221411000001		
J	Huang, XH; Pan, W; Park, S; Han, XQ; Miller, LW; Hall, J				Huang, XH; Pan, W; Park, S; Han, XQ; Miller, LW; Hall, J			Modeling the relationship between LVAD support time and gene expression changes in the human heart by penalized partial least squares	BIOINFORMATICS			English	Article							VENTRICULAR ASSIST DEVICE; MECHANICAL CIRCULATORY SUPPORT; MICROARRAY DATA; CLASSIFICATION; REGRESSION; SELECTION; CARDIOMYOPATHY; SHRINKAGE; FAILURE; LONG	Motivation: Heart failure affects more than 20 million people in the world. Heart transplantation is the most effective therapy, but the number of eligible patients far outweighs the number of available donor hearts. The left mechanical ventricular assist device (LVAD) has been developed as a successful substitution therapy that aids the failing ventricle while a patient is waiting for the donor heart. We obtained genomics data from paired human heart samples harvested at the time of LVAD implant and explant. The heart failure patients in our study were supported by the LVAD for various periods of time. The goal of this study is to model the relationship between the time of LVAD support and gene expression changes. Results: To serve the purpose, we propose a novel penalized partial least squares (PPLS) method to build a regression model. Compared with partial least squares and Breiman's random forest method, PPLS gives the best prediction results for the LVAD data.	Univ Minnesota, Sch Publ Hlth, Div Biostat, Minneapolis, MN 55455 USA; Univ Minnesota, Fairview Med Ctr, Minneapolis, MN 55455 USA; Univ Minnesota, Sch Med, Dept Med, Div Cardiovasc, Minneapolis, MN 55455 USA	Pan, W (reprint author), Univ Minnesota, Sch Publ Hlth, Div Biostat, A460 Mayo Bldg,MMC 303, Minneapolis, MN 55455 USA.	weip@biostat.umn.edu					Altemose GT, 1997, J HEART LUNG TRANSPL, V16, P765; Rose EA, 2001, NEW ENGL J MED, V345, P1435, DOI 10.1056/NEJMoa012175; Nguyen DV, 2002, BIOINFORMATICS, V18, P39, DOI 10.1093/bioinformatics/18.1.39; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324; HELLAND IS, 1988, COMMUN STAT SIMULAT, V17, P581, DOI 10.1080/03610918808812681; Ambroise C, 2002, P NATL ACAD SCI USA, V99, P6562, DOI 10.1073/pnas.102102699; Simon R, 2003, J NATL CANCER I, V95, P14; Nguyen DV, 2002, BIOINFORMATICS, V18, P1216, DOI 10.1093/bioinformatics/18.9.1216; DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425; DONOHO DL, 1995, IEEE T INFORM THEORY, V41, P613, DOI 10.1109/18.382009; Tibshirani R, 2002, P NATL ACAD SCI USA, V99, P6567, DOI 10.1073/pnas.082099299; Dipla K, 1998, CIRCULATION, V97, P2316; Efron B., 1982, JACKKNIFE BOOTSTRAP; GARTHWAITE PH, 1994, J AM STAT ASSOC, V89, P122, DOI 10.2307/2291207; Goldstein DJ, 1998, NEW ENGL J MED, V339, P1522, DOI 10.1056/NEJM199811193392107; HAWKINS DM, 2003, EXPLORING BLOOD SPEC; Hoskuldsson A, 1988, J CHEMOMETR, V2, P211, DOI DOI 10.1002/CEM.1180020306; Huang XH, 2003, BIOINFORMATICS, V19, P2072, DOI 10.1093/bioinformatics/btg283; HUNT SA, 2002, J HEART LUNG TRANSPL, DOI UNSP 211189-211203; Johansson D, 2003, BIOINFORMATICS, V19, P467, DOI 10.1093/bioinformatics/btg017; Levin HR, 1996, J HEART LUNG TRANSPL, V15, P840; MCCARTHY PM, 1995, ANN THORAC SURG, V59, P609, DOI 10.1016/0003-4975(94)00953-8; MCCARTHY PM, 1994, CIRCULATION, V90, P83; MCCARTHY PM, 1995, J THORAC CARDIOV SUR, V109, P409, DOI 10.1016/S0022-5223(95)70271-7; McCarthy PM, 2002, SCIENCE, V295, P998, DOI 10.1126/science.1068555; Nguyen DV, 2002, BIOINFORMATICS, V18, P1625, DOI 10.1093/bioinformatics/18.12.1625; Park Peter J, 2002, Bioinformatics, V18 Suppl 1, pS120; WEST M, 2002, 0212 DUK U I STAT DE	30	20	20	0	4	OXFORD UNIV PRESS	OXFORD	GREAT CLARENDON ST, OXFORD OX2 6DP, ENGLAND	1367-4803			BIOINFORMATICS	Bioinformatics	APR 12	2004	20	6					888	894		10.1093/bioinformatics/btg499		7	Biochemical Research Methods; Biotechnology & Applied Microbiology; Computer Science, Interdisciplinary Applications; Mathematical & Computational Biology; Statistics & Probability	Biochemistry & Molecular Biology; Biotechnology & Applied Microbiology; Computer Science; Mathematical & Computational Biology; Mathematics	813GH	WOS:000220895100010	14751963	
J	Neumann, A; Holstein, J; Chatellier, G; Lepage, E				Neumann, A; Holstein, J; Chatellier, G; Lepage, E			A regression shrinkage method tailored to qualitative regressors and clustered data	STATISTICS IN MEDICINE			English	Article						regression; shrinkage; qualitative data; clustered data; frailty models		We propose a shrinkage method for estimation in linear regression models with qualitative regressors. Due to the nature of the shrinkage constraint, this method tends to give estimates that are exactly zero for some groups of coefficients belonging to the same regressor. The method hence results in concise models, since some of the regressors are entirely eliminated. In conjunction with this estimation method, a model with a fixed cluster effect turns out to be closely related to frailty models. We apply the method for modelling hospital readmissions. Copyright (C) 2004 John Wiley Sons, Ltd.	AP HP, DIME, DIREQ, DPM, F-75184 Paris 04, France	Neumann, A (reprint author), AP HP, DIME, DIREQ, DPM, 3 Ave Victoria, F-75184 Paris 04, France.	anke.neumann@sap.ap-hop-paris.fr					Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; HOERL AE, 1970, TECHNOMETRICS, V12, P55; Fang K, 1990, SYMMETRIC MULTIVARIA; Halfon P, 2002, J CLIN EPIDEMIOL, V55, P573, DOI 10.1016/S0895-4356(01)00521-2; Hastie T., 2001, ELEMENTS STAT LEARNI; KELKER D, 1970, SANKHYA SER A, V32, P419; MCGILCHRIST CA, 1994, J ROY STAT SOC B MET, V56, P61; *SAS I INC, 2000, SAS IML US GUID VERS	8	0	0	1	1	JOHN WILEY & SONS LTD	CHICHESTER	THE ATRIUM, SOUTHERN GATE, CHICHESTER PO19 8SQ, W SUSSEX, ENGLAND	0277-6715			STAT MED	Stat. Med.	APR 15	2004	23	7					1147	1157		10.1002/sim.1663		11	Mathematical & Computational Biology; Public, Environmental & Occupational Health; Medical Informatics; Medicine, Research & Experimental; Statistics & Probability	Mathematical & Computational Biology; Public, Environmental & Occupational Health; Medical Informatics; Research & Experimental Medicine; Mathematics	810CP	WOS:000220682700008	15057883	
J	Roth, V; Lange, T				Roth, V; Lange, T			Bayesian class discovery in microarray datasets	IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING			English	Article						automatic relevance determination; Bayesian inference; class discovery; gene expression; gene selection	B-CELL LYMPHOMA; MOLECULAR CLASSIFICATION; DISCRIMINANT-ANALYSIS; EXPRESSION; GENE; LEUKEMIA; DIFFERENTIATION; COMPONENTS; STABILITY; SURVIVAL	A novel approach to class discovery in gene expression datasets is presented. In the context of clinical diagnosis, the central goal of class discovery algorithms is to simultaneously find putative (sub-)types of diseases and to identify informative subsets of genes with disease-type specific expression profile. Contrary to many other approaches in the literature, the method presented implements a wrapper strategy for feature selection, in the sense that the features are directly selected by optimizing the discriminative power of the used partitioning algorithm. The usual combinatorial problems associated with wrapper approaches are overcome by a Bayesian inference mechanism. On the technical side, we present an efficient optimization algorithm with guaranteed local convergence property. The only free parameter of the optimization method is selected by a resampling-based stability analysis. Experiments with Leukemia and Lymphoma datasets demonstrate that our method is able to correctly infer partitions and corresponding subsets of genes which both are relevant in a biological sense. Moreover, the frequently observed problem of ambiguities caused by different but equally high-scoring partitions is successfully overcome by the model selection method proposed.	ETH, Inst Computat Sci, CH-8092 Zurich, Switzerland	Roth, V (reprint author), ETH, Inst Computat Sci, Hirschengraben 84, CH-8092 Zurich, Switzerland.	vroth@inf.ethz.ch					Meinecke F, 2002, IEEE T BIO-MED ENG, V49, P1514, DOI 10.1109/TBME.2002.805480; HASTIE T, 1994, J AM STAT ASSOC, V89, P1255, DOI 10.2307/2290989; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1; Alizadeh AA, 2000, NATURE, V403, P503, DOI 10.1038/35000501; Golub TR, 1999, SCIENCE, V286, P531, DOI 10.1126/science.286.5439.531; HARRINGTON DP, 1982, BIOMETRIKA, V69, P553, DOI 10.1093/biomet/69.3.553; Husson H, 2002, BRIT J HAEMATOL, V119, P492, DOI 10.1046/j.1365-2141.2002.03832.x; SHIPP MA, 1993, NEW ENGL J MED, V329, P987; Bartkova J, 1999, J PATHOL, V187, P573, DOI 10.1002/(SICI)1096-9896(199904)187:5<573::AID-PATH289>3.0.CO;2-H; Belov L, 2001, CANCER RES, V61, P4483; Ben-Dor A., 2001, P 5 ANN INT C COMP M, P31, DOI DOI 10.1145/369133.369167; Bittner M, 2000, NATURE, V406, P536, DOI 10.1038/35020115; HASTIE T, 1995, ANN STAT, V23, P73, DOI 10.1214/aos/1176324456; Chelbi-Alix MK, 1999, ONCOGENE, V18, P935, DOI 10.1038/sj.onc.1202366; Dudoit S, 2002, GENOME BIOL, V3; Figueiredo MAT, 2001, PROC CVPR IEEE, P35; Harmeling S, 2004, SIGNAL PROCESS, V84, P255, DOI 10.1016/j.sigpro.2003.10.009; Hastie Trevor, 1996, J R STAT SOC B, V58, P158; Hayashi Y, 2001, J NEURO-ONCOL, V55, P51, DOI 10.1023/A:1012946812930; Hofmann T, 1997, IEEE T PATTERN ANAL, V19, P1, DOI 10.1109/34.566806; Hoyer KK, 2002, P NATL ACAD SCI USA, V99, P14392, DOI 10.1073/pnas.212410199; Ishibe N, 2002, BLOOD, V100, P1100, DOI 10.1182/blood-2002-03-0938; Lange T., 2002, ADV NEURAL INFORMATI, P617; MacKay D, 1994, ASHRAE T, V100, P1053; MacKenzie JR, 2001, MOL CELL PROBE, V15, P235, DOI 10.1006/mcpr.2001.0362; MARKOWETZ F, 2002, P 26 ANN C GES KLASS; NOURSE J, 1990, CELL, V60, P535, DOI 10.1016/0092-8674(90)90657-Z; Osborne MR, 2000, J COMPUT GRAPH STAT, V9, P319, DOI 10.2307/1390657; ROTH V, 2002, IAITR20028; Roth V., 2002, ADV NEURAL INFORM PR, V15, P817; ROTH V, 2004, IEEE T NEURAL NETWOR, V15; Said JW, 2001, LAB INVEST, V81, P555; Schultz C, 2002, CANCER RES, V62, P6270; SCHULZ U, 1996, Z METALLKD, V87, P6; SHAFFER A, 2002, NATURE REV IMMUNOL, P920; Smith DA, 1995, MOL IMMUNOL, V32, P1339, DOI 10.1016/0161-5890(95)00113-1; SOBOL RE, 1987, NEW ENGL J MED, V316, P1111, DOI 10.1056/NEJM198704303161802; Sonoki T, 2001, BLOOD, V98, P2837, DOI 10.1182/blood.V98.9.2837; VONHEYDEBRECK WA, 2001, BIOINFORMATICS, V517, P107; Watano K, 2001, IMMUNOLOGY, V104, P307, DOI 10.1046/j.1365-2567.2001.01301.x	41	20	21	0	0	IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC	PISCATAWAY	445 HOES LANE, PISCATAWAY, NJ 08855 USA	0018-9294			IEEE T BIO-MED ENG	IEEE Trans. Biomed. Eng.	MAY	2004	51	5					707	718		10.1109/TBME.2004.824139		12	Engineering, Biomedical	Engineering	814IF	WOS:000220967700003	15132496	
J	Sugiyama, M; Kawanabe, M; Muller, KR				Sugiyama, M; Kawanabe, M; Muller, KR			Trading variance reduction with unbiasedness: The regularized subspace information criterion for robust model selection in kernel regression	NEURAL COMPUTATION			English	Article							GENERALIZED CROSS-VALIDATION; RIDGE-REGRESSION; ALGORITHMS; PARAMETER; SHRINKAGE	A well-known result by Stein (1956) shows that in particular situations, biased estimators can yield better parameter estimates than their generally preferred unbiased counterparts. This letter follows the same spirit, as we will stabilize the unbiased generalization error estimates by regularization and finally obtain more robust model selection criteria for learning. We trade a small bias against a larger variance reduction, which has the beneficial effect of being more precise on a single training set. We focus on the subspace information criterion (SIC), which is an unbiased estimator of the expected generalization error measured by the reproducing kernel Hilbert space norm. SIC can be applied to the kernel regression, and it was shown in earlier experiments that a small regularization of SIC has a stabilization effect. However, it remained open how to appropriately determine the degree of regularization in SIC. In this article, we derive an unbiased estimator of the expected squared error, between SIC and the expected generalization error and propose determining the degree of regularization of SIC such that the estimator of the expected squared error is minimized. Computer simulations with artificial and real data sets illustrate that the proposed method works effectively for improving the precision of SIC, especially in the high-noise-level cases. We furthermore compare the proposed method to the original SIC, the cross-validation, and an empirical Bayesian method in ridge parameter selection, with good results.	IDA, Fraunhofer FIRST, D-12489 Berlin, Germany; Tokyo Inst Technol, Dept Comp Sci, Meguro Ku, Tokyo 1528552, Japan; Univ Potsdam, Dept Comp Sci, D-14482 Potsdam, Germany	Sugiyama, M (reprint author), IDA, Fraunhofer FIRST, D-12489 Berlin, Germany.	sugi@cs.titech.ac.jp; nabe@first.fhg.de; klaus@first.fhg.de	Muller, Klaus/C-3196-2013				Akaike H, 1980, BAYESIAN STAT, P141; AKAIKE H, 1974, IEEE T AUTOMAT CONTR, VAC19, P716, DOI 10.1109/TAC.1974.1100705; GEMAN S, 1992, NEURAL COMPUT, V4, P1, DOI 10.1162/neco.1992.4.1.1; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Muller KR, 2001, IEEE T NEURAL NETWOR, V12, P181, DOI 10.1109/72.914517; FELSENSTEIN J, 1985, EVOLUTION, V39, P783, DOI 10.2307/2408678; Williams CKI, 1998, NATO ADV SCI I D-BEH, V89, P599; Scholkopf B, 2000, NEURAL COMPUT, V12, P1207, DOI 10.1162/089976600300015565; Cucker F, 2002, B AM MATH SOC, V39, P1; DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425; LI KC, 1986, ANN STAT, V14, P1101, DOI 10.1214/aos/1176350052; DONOHO DL, 1995, IEEE T INFORM THEORY, V41, P613, DOI 10.1109/18.382009; MALLOWS CL, 1973, TECHNOMETRICS, V15, P661, DOI 10.2307/1267380; CRAVEN P, 1979, NUMER MATH, V31, P377; ARONSZAJN N, 1950, T AM MATH SOC, V68, P337, DOI DOI 10.2307/1990404; Bergman S., 1970, KERNEL FUNCTION CONF; Bishop C. M., 1995, NEURAL NETWORKS PATT; Bousquet O, 2002, J MACH LEARN RES, V2, P499, DOI 10.1162/153244302760200704; Cherkassky V, 1999, IEEE T NEURAL NETWOR, V10, P1075, DOI 10.1109/72.788648; Cristianini N., 2000, INTRO SUPPORT VECTOR; Daubechies I., 1992, 10 LECT WAVELETS; Devroye L, 1996, PROBABILISTIC THEORY; Girosi F, 1998, NEURAL COMPUT, V10, P1455, DOI 10.1162/089976698300017269; GU C, 1992, STAT PROBABIL LETT, V14, P283, DOI 10.1016/0167-7152(92)90058-D; HENKEL RE, 1979, TESTS SIGNIFICANCE; Heskes T, 1998, NEURAL COMPUT, V10, P1425, DOI 10.1162/089976698300017232; HOERL AE, 1970, TECHNOMETRICS, V12, P55, DOI DOI 10.1080/00401706.1970.10488634; Joachims T, 1999, ADVANCES IN KERNEL METHODS, P169; Konishi S, 1996, BIOMETRIKA, V83, P875, DOI 10.1093/biomet/83.4.875; Lehmann E. L., 1983, THEORY POINT ESTIMAT; LINHART H, 1988, S AFR STAT J, V22, P153; Luntz A., 1969, TECHNICHESKAYA KIBER, V3; Mallows C. L., 1964, CENTR REG M I MATH S; MULLER KR, 1998, ADV KERNEL METHODS S, P243; MURATA N, 1994, IEEE T NEURAL NETWOR, V5, P865, DOI 10.1109/72.329683; MURATA N, 1998, P 1998 WORKSH INF BA, P87; Orr M. J. L., 1996, INTRO RADIAL BASIS F; Rasmussen C. E., 1996, DELVE MANUAL; Saitoh S., 1997, INTEGRAL TRANSFORMS; Saitoh S., 1988, THEORY REPROD KERNEL; Scholkopf B., 2002, LEARNING KERNELS; Shimodaira H, 1997, ANN I STAT MATH, V49, P395, DOI 10.1023/A:1003140609666; Shimodaira H, 1998, ANN I STAT MATH, V50, P1, DOI 10.1023/A:1003483128844; Smola AJ, 1998, NEURAL NETWORKS, V11, P637, DOI 10.1016/S0893-6080(98)00032-X; STEIN C, 1956, 3RD P BERK S MATH ST, V1, P197; SUGIURA N, 1978, COMMUN STAT A-THEOR, V7, P13, DOI 10.1080/03610927808827599; SUGIYAMA M, 2003, TR030003 TOK I TECHN; Sugiyama M, 2002, NEURAL NETWORKS, V15, P349, DOI 10.1016/S0893-6080(02)00022-9; Sugiyama M, 2001, NEURAL COMPUT, V13, P1863, DOI 10.1162/08997660152469387; Sugiyama M., 2002, J MACHINE LEARNING R, V3, P323; Takeuchi K., 1976, MATH SCI, V153, P12; Tsuda K, 2002, IEEE T NEURAL NETWOR, V13, P70, DOI 10.1109/72.977272; Vapnik V., 1998, STAT LEARNING THEORY; Vapnik V.N., 1995, NATURE STAT LEARNING; Vapnik VN, 1982, ESTIMATION DEPENDENC; Wahba G., 1990, SPLINE MODEL OBSERVA; WAHBA G, 1985, ANN STAT, V13, P1378, DOI 10.1214/aos/1176349743; Williams CKI, 1996, ADV NEUR IN, V8, P514	58	18	18	0	1	MIT PRESS	CAMBRIDGE	55 HAYWARD STREET, CAMBRIDGE, MA 02142 USA	0899-7667	1530-888X		NEURAL COMPUT	Neural Comput.	MAY	2004	16	5					1077	1104		10.1162/089976604773135113		28	Computer Science, Artificial Intelligence; Neurosciences	Computer Science; Neurosciences & Neurology	806OK	WOS:000220443000008	15070511	
J	Babyak, MA				Babyak, MA			What you see may not be what you get: A brief, nontechnical introduction to overfitting in regression-type models	PSYCHOSOMATIC MEDICINE			English	Article						statistical models; regression; simulation; dichotomization; overfitting	PREDICTION; SHRINKAGE; SELECTION; VARIABLES; EVENTS	Objective: Statistical models, such as linear or logistic regression or survival analysis, are frequently used as a means to answer scientific questions in psychosomatic research. Many who use these techniques, however, apparently fail to appreciate fully the problem of overfitting, ie, capitalizing on the idiosyncrasies of the sample at hand. Overfitted models will fail to replicate in future samples, thus creating considerable uncertainty about the scientific merit of the finding. The present article is a nontechnical discussion of the concept of overfitting and is intended to be accessible to readers with varying levels of statistical expertise. The notion of overfitting is presented in terms of asking too much from the available data. Given a certain number of observations in a data set, there is an upper limit to the complexity of the model that can be derived with any acceptable degree of uncertainty. Complexity arises as a function of the number of degrees of freedom expended (the number of predictors including complex terms such as interactions and nonlinear terms) against the same data set during any stage of the data analysis. Theoretical and empirical evidence-with a special focus on the results of computer simulation studies-is presented to demonstrate the practical consequences of overfitting with respect to scientific inference. Three common practices-automated variable selection, pretesting of candidate predictors, and dichotomization of continuous variables-are shown to pose a considerable risk for spurious findings in models. The dilemma between overfitting and exploring candidate confounders is also discussed. Alternative means of guarding against overfitting are discussed, including variable aggregation and the fixing of coefficients a priori. Techniques that account and correct for complexity, including shrinkage and penalization, also are introduced.	Duke Univ, Med Ctr, Dept Psychiat & Behav Sci, Durham, NC 27710 USA	Babyak, MA (reprint author), Duke Univ, Med Ctr, Dept Psychiat & Behav Sci, Box 3119, Durham, NC 27710 USA.	michael.babyak@duke.edu	pigeon, karine/C-6336-2014	pigeon, karine/0000-0002-7409-5511			ALTMAN DG, 1989, STAT MED, V8, P771, DOI 10.1002/sim.4780080702; ROECKER E, 1991, TECHNOMETRICS, V33, P459, DOI 10.2307/1269417; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Steyerberg EW, 2001, J CLIN EPIDEMIOL, V54, P774, DOI 10.1016/S0895-4356(01)00341-9; Steyerberg EW, 2001, MED DECIS MAKING, V21, P45; MacCallum RC, 2002, PSYCHOL METHODS, V7, P19, DOI 10.1037//1082-989X.7.1.19; Peduzzi P, 1995, J CLIN EPIDEMIOL, V48, P1503, DOI 10.1016/0895-4356(95)00048-8; DHOORE W, 1993, METHOD INFORM MED, V32, P382; COPAS JB, 1983, J R STAT SOC B, V45, P311; COHEN J, 1990, AM PSYCHOL, V45, P1304, DOI 10.1037//0003-066X.45.12.1304; Cohen J. C., 1983, APPL MULTIPLE REGRES; COX DR, 1972, J R STAT SOC B, V34, P187; DELEON CFM, 2003, ANN M AM PSYCH SOC P; DERKSEN S, 1992, BRIT J MATH STAT PSY, V45, P265; Efron B., 2003, INTRO BOOTSTRAP; FARAWAY JJ, 1992, J COMPUTATIONAL GRAP, V1, P213, DOI 10.2307/1390717; FREEDMAN DA, 1991, SOCIOL METHODOL, V21, P291, DOI 10.2307/270939; GRAMBSCH PM, 1991, STAT MED, V10, P697, DOI 10.1002/sim.4780100504; GREEN SB, 1991, MULTIVAR BEHAV RES, V26, P499, DOI 10.1207/s15327906mbr2603_7; HARDIN JW, 2003, GEN ESTIMATING EQUAT, P34; Harrell FE, 2001, REGRESSION MODELING; MAXWELL SE, 1993, PSYCHOL BULL, V113, P181, DOI 10.1037/0033-2909.113.1.181; MCCLELLAND G, NEGATIVE CONSEQUENCE; McCullagh P., 1989, GEN LINEAR MODELS; MULAIK SA, 1995, PHILOS SCI, V62, P283, DOI 10.1086/289857; Peduzzi P, 1996, J CLIN EPIDEMIOL, V49, P1373, DOI 10.1016/S0895-4356(96)00236-3; Taleb N. N., 2001, FOOLED RANDOMNESS; THOMPSON B, 1995, EDUC PSYCHOL MEAS, V55, P525, DOI 10.1177/0013164495055004001	28	485	487	10	44	LIPPINCOTT WILLIAMS & WILKINS	PHILADELPHIA	530 WALNUT ST, PHILADELPHIA, PA 19106-3621 USA	0033-3174			PSYCHOSOM MED	Psychosom. Med.	MAY-JUN	2004	66	3					411	421		10.1097/01.psy.0000127692.23278.a9		11	Psychiatry; Psychology; Psychology, Multidisciplinary	Psychiatry; Psychology	822OA	WOS:000221548300021	15184705	
J	Thissen, U; Ustun, B; Melssen, WJ; Buydens, LMC				Thissen, U; Ustun, B; Melssen, WJ; Buydens, LMC			Multivariate calibration with least-squares support vector machines	ANALYTICAL CHEMISTRY			English	Article							INDUCED SPECTRAL VARIATION; RIDGE-REGRESSION; TEMPERATURE; SELECTION; SPECTROMETRY; MODELS	This paper proposes the use of least-squares support vector machines (LS-SVMs) as a relatively new nonlinear multivariate calibration method, capable of dealing with ill-posed problems. LS-SVMs are an extension of "traditional" SVMs that have been introduced recently in the field of chemistry and chemometrics. The advantages of SVM-based methods over many other methods are that these lead to global models that are often unique, and nonlinear regression can be performed easily as an extension to linear regression. An additional advantage of LS-SVM (compared to SVM) is that model calculation and optimization can be performed relatively fast. As a test case to study the use of LS-SVM, the well-known and important chemical problem is considered in which spectra are affected by nonlinear interferences. As one specific example, a commonly used case is studied in which near-infrared spectra are affected by temperature-induced spectral variation. Using this test case, model optimization, pruning, and model interpretation of the LS-SVM have been demonstrated. Furthermore, excellent performance of the LS-SVM, compared to other approaches, has been presented on the specific example. Therefore, it can be concluded that LS-SVMs can be seen as very promising techniques to solve ill-posed problems. Furthermore, these have been shown to lead to robust models in cases of spectral variations due to nonlinear interferences.	Univ Nijmegen, Analyt Chem Lab, NL-6525 ED Nijmegen, Netherlands	Buydens, LMC (reprint author), Univ Nijmegen, Analyt Chem Lab, Toemooiveld 1, NL-6525 ED Nijmegen, Netherlands.	lbuydens@sci.kun.nl	Buydens, Lutgarde/D-4338-2012				Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; HOERL AE, 1970, TECHNOMETRICS, V12, P55; Wentzell PD, 2003, CHEMOMETR INTELL LAB, V65, P257, DOI 10.1016/S0169-7439(02)00138-7; FRANK IE, 1993, TECHNOMETRICS, V35, P109, DOI 10.2307/1269656; Suykens JAK, 1999, NEURAL PROCESS LETT, V9, P293, DOI 10.1023/A:1018628609742; Belousov AI, 2002, CHEMOMETR INTELL LAB, V64, P15, DOI 10.1016/S0169-7439(02)00046-1; Belousov AI, 2002, J CHEMOMETR, V16, P482, DOI 10.1002/cem.744; Centner V, 2000, APPL SPECTROSC, V54, P608, DOI 10.1366/0003702001949816; Cristianini N., 2000, INTRO SUPPORT VECTOR; de Kruif BJ, 2003, IEEE T NEURAL NETWOR, V14, P696, DOI 10.1109/TNN.2003.810597; Despagne F, 2000, ANAL CHEM, V72, P1657, DOI 10.1021/ac991076k; Eilers PHC, 2003, CHEMOMETR INTELL LAB, V66, P159, DOI 10.1016/S0169-7439(03)00029-7; Estienne F, 2001, ANAL CHIM ACTA, V450, P123, DOI 10.1016/S0003-2670(01)01372-1; Felipe-Sotelo M, 2003, ANAL CHEM, V75, P5254, DOI 10.1021/ac0343477; Gunn SR, 1997, SUPPORT VECTOR MACHI; Gusnanto A, 2003, J CHEMOMETR, V17, P174, DOI 10.1002/cem.787; Hageman JA, 2003, J CHEMOMETR, V17, P427, DOI 10.1002/cem.782; Hastie T., 2001, ELEMENTS STAT LEARNI; LUKAS L, 2002, ESANN 2002 P EUR S A, P131; MARX BD, 2000, J CHEMOMETR, V16, P129; Pavon JLP, 2003, ANAL CHEM, V75, P6361, DOI 10.1021/ac034543d; PELCKMANS K, 2002, LSSVMLAB MATLAB C TO; Scholkopf B., 2002, LEARNING KERNELS; Scholkopf B, 1999, IEEE T NEURAL NETWOR, V10, P1000, DOI 10.1109/72.788641; SMOLA AJ, 1998, NCTR98030; Song MH, 2002, J CHEM INF COMP SCI, V42, P1347, DOI 10.1021/ci025580t; Suykens J. A. K., 2002, LEAST SQUARES SUPPOR; SWIERENGA H, 2000, THESIS U NIMEGEN NIM; Swierenga H, 2000, ANAL CHIM ACTA, V411, P121, DOI 10.1016/S0003-2670(00)00718-2; THISSEN U, IN PRESS CHEMOM INTE; Thissen U., 2003, Chemometrics and Intelligent Laboratory Systems, V69, DOI 10.1016/S0169-7439(03)00111-4; Vapnik V., 1998, STAT LEARNING THEORY; Vapnik V.N., 1995, NATURE STAT LEARNING; Witjes H, 2000, CHEMOMETR INTELL LAB, V52, P105, DOI 10.1016/S0169-7439(00)00085-X; Wulfert F, 2000, CHEMOMETR INTELL LAB, V51, P189, DOI 10.1016/S0169-7439(00)00069-1; Wulfert F, 1998, ANAL CHEM, V70, P1761, DOI 10.1021/ac9709920; Wulfert F, 2000, ANAL CHEM, V72, P1639, DOI 10.1021/ac9906835; ZHU J, NEURAL INFORMATION P	38	128	130	2	14	AMER CHEMICAL SOC	WASHINGTON	1155 16TH ST, NW, WASHINGTON, DC 20036 USA	0003-2700			ANAL CHEM	Anal. Chem.	JUN 1	2004	76	11					3099	3105		10.1021/ac035522m		7	Chemistry, Analytical	Chemistry	825KH	WOS:000221755200024	15167788	
J	Fan, JQ; Peng, H				Fan, JQ; Peng, H			Nonconcave penalized likelihood with a diverging number of parameters	ANNALS OF STATISTICS			English	Article						model selection; nonconcave penalized likelihood; diverging parameters; oracle property; asymptotic normality; standard errors; likelihood ratio statistic	WALD MEMORIAL LECTURES; VARIABLE SELECTION; MODEL SELECTION; REGRESSION; LASSO; ASYMPTOTICS; EFFICIENCY; ESTIMATORS; SHRINKAGE	A class of variable selection procedures for parametric models via nonconcave penalized likelihood was proposed by Fan and Li to simultaneously estimate parameters and select important variables. They demonstrated that this class of procedures has an oracle property when the number of parameters is finite. However, in most model selection problems the number of parameters should be large and grow with the sample size. In this paper some asymptotic properties of the nonconcave penalized likelihood are established for situations in which the number of parameters tends to infinity as the sample size increases. Under regularity conditions we have established an oracle property and the asymptotic normality of the penalized likelihood estimators. Furthermore, the consistency of the sandwich formula of the covariance matrix is demonstrated. Nonconcave penalized likelihood ratio statistics are discussed, and their asymptotic distributions under the null hypothesis are obtained by imposing some mild conditions on the penalty functions. The asymptotic results are augmented by a simulation Study, and the newly developed methodology is illustrated by an analysis of a court case on the sexual discrimination of salary.	Princeton Univ, Dept Operat Res & Financial Engn, Princeton, NJ 08544 USA; Chinese Univ Hong Kong, Dept Stat, Hong Kong, Hong Kong, Peoples R China	Fan, JQ (reprint author), Princeton Univ, Dept Operat Res & Financial Engn, Princeton, NJ 08544 USA.	jqfan@princeton.edu; h_peng@sparc20c.sta.cuhk.edu.hk	PENG, HENG/B-4715-2009; Peng, Heng/B-7152-2009; HKBU, Mathematics/B-5086-2009; 	PENG, HENG/0000-0001-7807-8527			Akaike H., 1973, P 2 INT S INF THEOR, P267; Albright S. C., 1999, DATA ANAL DECISION M; Fu WJJ, 1998, J COMPUT GRAPH STAT, V7, P397, DOI 10.2307/1390712; HUBER PJ, 1973, ANN STAT, V1, P799, DOI 10.1214/aos/1176342503; Shen XT, 2002, J AM STAT ASSOC, V97, P210, DOI 10.1198/016214502753479356; Knight K, 2000, ANN STAT, V28, P1356; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Antoniadis A, 2001, J AM STAT ASSOC, V96, P939, DOI 10.1198/016214501753208942; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; Neyman J, 1948, ECONOMETRICA, V16, P1, DOI 10.2307/1914288; FRANK IE, 1993, TECHNOMETRICS, V35, P109, DOI 10.2307/1269656; DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425; SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136; MALLOWS CL, 1973, TECHNOMETRICS, V15, P661, DOI 10.2307/1267380; Antoniadis A., 1997, J ITAL STAT SOC, V6, P97, DOI DOI 10.1007/BF03178905; Bai ZD, 1999, J STAT PLAN INFER, V77, P103, DOI 10.1016/S0378-3758(98)00168-2; Blake A., 1987, VISUAL RECONSTRUCTIO; BLAKE A, 1989, IEEE T PATTERN ANAL, V11, P2, DOI 10.1109/34.23109; Breiman L, 1996, ANN STAT, V24, P2350; Kauermann G, 2001, J AM STAT ASSOC, V96, P1387, DOI 10.1198/016214501753382309; COX DD, 1990, ANN STAT, V18, P1676, DOI 10.1214/aos/1176347872; de Boor C., 1978, PRACTICAL GUIDE SPLI; Donoho D. L., 2000, AID MEM LECT AMS C M; Fan J., 1997, J ITALIAN STAT ASS, V6, P131; Fan JQ, 2002, ANN STAT, V30, P74; GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721; Gilks W. R., 1996, MARKOV CHAIN MONTE C; Green P, 1994, NONPARAMETRIC REGRES; Lehmann E. L., 1983, THEORY POINT ESTIMAT; McCullagh P., 1989, GEN LINEAR MODELS; MURPHY SA, 1993, SCAND J STAT, V20, P35; Nikolova M, 1998, IEEE T IMAGE PROCESS, V7, P571, DOI 10.1109/83.663502; PORTNOY S, 1988, ANN STAT, V16, P356, DOI 10.1214/aos/1176350710; Stone CJ, 1997, ANN STAT, V25, P1371; Tibshirani R, 1997, STAT MED, V16, P385, DOI 10.1002/(SICI)1097-0258(19970228)16:4<385::AID-SIM380>3.0.CO;2-3; van der Vaart A. W., 1998, ASYMPTOTIC STAT; Wahba G., 1990, SPLINE MODELS OBSERV	37	277	285	1	11	INST MATHEMATICAL STATISTICS	BEACHWOOD	PO BOX 22718, BEACHWOOD, OH 44122 USA	0090-5364			ANN STAT	Ann. Stat.	JUN	2004	32	3					928	961		10.1214/009053604000000256		34	Statistics & Probability	Mathematics	828OA	WOS:000221981400004		
J	Engel, J; van Kempen, J				Engel, J; van Kempen, J			Characterizing the measurement precision of thermal resistance measurements	APPLIED STOCHASTIC MODELS IN BUSINESS AND INDUSTRY			English	Article						statistical modelling; model selection; model uncertainty; tolerance design; Bayesian model averaging	SELECTION	The measurement process of the thermal interface resistance tester combines physical measurements and computer simulations to obtain the measured value of the thermal resistance of a material. The computer simulation model is calibrated by 14 parameters that have been estimated from various experiments. The estimation errors in these parameters contribute to the measurement error in the thermal resistance. The following research questions were raised: (1) What calibration parameter errors have a large contribution to the thermal resistance error? (2) How does this error depend on the reference value of the thermal resistance measured under standard conditions? The main point in this paper is to show the use of statistical modelling to estimate the effect of the calibration parameter errors on the thermal resistance measurement precision by means of a tolerance design procedure that is based on the model. Our final conclusion is that two out of 14 calibration parameters dominate the thermal resistance error, and that their effects strongly depend on the reference thermal resistance. Copyright (C) 2004 John Wiley Sons, Ltd.	CQM, NL-5600 AK Eindhoven, Netherlands	Engel, J (reprint author), CQM, Bldg HCZ-3,POB 414, NL-5600 AK Eindhoven, Netherlands.	Engel@cqm.nl					Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Hoeting JA, 1999, STAT SCI, V14, P382; Buckland ST, 1997, BIOMETRICS, V53, P603, DOI 10.2307/2533961; HEULE AF, 2004, UNPUB J QUALITY TECH; KRZANOWSKY WJ, 1998, PRINCIPLES MULTIVARI, P206; Miller A, 2001, TECHNOMETRICS, V43, P44, DOI 10.1198/00401700152404318; Molenberghs G., 2001, STAT MODEL, V1, P235, DOI DOI 10.1177/1471082X0100100402; Montgomery DC, 1991, DESIGN ANAL EXPT; Taguchi Genichi, 1986, INTRO QUALITY ENG	9	1	1	3	5	JOHN WILEY & SONS LTD	CHICHESTER	THE ATRIUM, SOUTHERN GATE, CHICHESTER PO19 8SQ, W SUSSEX, ENGLAND	1524-1904			APPL STOCH MODEL BUS	Appl. Stoch. Models. Bus. Ind.	JUL-SEP	2004	20	3					239	251		10.1002/asmb.519		13	Operations Research & Management Science; Mathematics, Interdisciplinary Applications; Statistics & Probability	Operations Research & Management Science; Mathematics	847YH	WOS:000223433500006		
J	Hastie, T; Tibshirani, R				Hastie, T; Tibshirani, R			Efficient quadratic regularization for expression arrays	BIOSTATISTICS			English	Article						eigengenes; euclidean methods; quadratic regularization; SVD	DISCRIMINANT-ANALYSIS; REGRESSION; CLASSIFICATION; MICROARRAYS; PREDICTION	Gene expression arrays typically have 50 to 100 samples and 1000 to 20 000 variables (genes). There have been many attempts to adapt statistical models for regression and classification to these data, and in many cases these attempts have challenged the computational resources. In this article we expose a class of techniques based on quadratic regularization of linear models, including regularized (ridge) regression, logistic and multinomial regression, linear and mixture discriminant analysis, the Cox model and neural networks. For all of these models, we show that dramatic computational savings are possible over naive implementations, using standard transformations in numerical linear algebra.	Stanford Univ, Dept Stat, Stanford, CA 94305 USA; Stanford Univ, Dept Hlth Res & Policy, Stanford, CA 94305 USA	Hastie, T (reprint author), Stanford Univ, Dept Stat, Sequoia Hall, Stanford, CA 94305 USA.	hastie@stanford.edu					Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Khan J, 2001, NAT MED, V7, P673, DOI 10.1038/89044; Tusher VG, 2001, P NATL ACAD SCI USA, V98, P5116, DOI 10.1073/pnas.091062498; Hastie T, 1996, J ROY STAT SOC B MET, V58, P155; HOERL AE, 1970, TECHNOMETRICS, V12, P55; Tibshirani R, 2003, STAT SCI, V18, P104, DOI 10.1214/ss/1056397488; Alter O, 2000, P NATL ACAD SCI USA, V97, P10101, DOI 10.1073/pnas.97.18.10101; Ramaswamy S, 2001, P NATL ACAD SCI USA, V98, P15149, DOI 10.1073/pnas.211566398; HASTIE T, 1995, ANN STAT, V23, P73, DOI 10.1214/aos/1176324456; COX DR, 1972, J R STAT SOC B, V34, P187; EFRON B, 2002, LEAST ANGLE REGRESSI; EILERS P, 2001, P SOC PHOTO-OPT INS, V4266, P23; FRIEDMAN JH, 1989, J AM STAT ASSOC, V84, P165, DOI 10.2307/2289860; Ghosh D, 2003, BIOMETRICS, V59, P992, DOI 10.1111/j.0006-341X.2003.00114.x; Golub G. H., 1983, MATRIX COMPUTATIONS; GUO Y, 2003, REGULARIZED DISCRIMI; Hastie T., 2001, ELEMENTS STAT LEARNI; HASTIE T, 2004, ENTIRE REGULARIZATIO; ROSSET S, 2003, NEURAL INFORMATION P; Scholkopf B., 2001, LEARNING KERNELS SUP; Vapnik V.N., 1996, NATURE STAT LEARNING; Wahba G, 2000, ADV NEUR IN, P297; West M., 2003, BAYESIAN STAT, P723; Zhu J, 2004, BIOSTATISTICS, V5, P427, DOI 10.1093/biostatistics/kxg046; ZHU J, 2003, L1 NORM SUPPORT VECT	25	39	40	0	8	OXFORD UNIV PRESS	OXFORD	GREAT CLARENDON ST, OXFORD OX2 6DP, ENGLAND	1465-4644			BIOSTATISTICS	Biostatistics	JUL	2004	5	3					329	340		10.1093/biostatistics/kxh010		12	Mathematical & Computational Biology; Statistics & Probability	Mathematical & Computational Biology; Mathematics	838OV	WOS:000222723600001	15208198	
J	Steyerberg, EW; Borsboom, GJJM; van Houwelingen, HC; Eijkemans, MJC; Habbema, JDF				Steyerberg, EW; Borsboom, GJJM; van Houwelingen, HC; Eijkemans, MJC; Habbema, JDF			Validation and updating of predictive logistic regression models: a study on sample size and shrinkage	STATISTICS IN MEDICINE			English	Article						logistic regression; validation; updating; shrinkage	ACUTE MYOCARDIAL-INFARCTION; NONSEMINOMATOUS TESTICULAR CANCER; SMALL DATA SETS; PROGNOSTIC MODELS; MASS HISTOLOGY; MORTALITY; SELECTION; TRIAL; THROMBOLYSIS; REPERFUSION	A logistic regression model may be used to provide predictions of outcome for individual patients at another centre than where the model was developed. When empirical data are available from this centre, the validity of predictions can be assessed by comparing observed outcomes and predicted probabilities. Subsequently, the model may be updated to improve predictions for future patients. As an example, we analysed 30-day mortality after acute myocardial infarction in a large data set (GUSTO-I, n=40830). We validated and updated a previously published model from another study (TIMI-II, n=3339) in validation samples ranging from small (200 patients, 14 deaths) to large (10000 patients, 700 deaths). Updated models were tested on independent patients. Updating methods included re-calibration (re-estimation of the intercept or slope of the linear predictor) and more structural model revisions (re-estimation of some or all regression coefficients, model extension with more predictors). We applied heuristic shrinkage approaches in the model revision methods, such that regression coefficients were shrunken towards their re-calibrated values. Parsimonious updating methods were found preferable to more extensive model revisions, which should only be attempted with relatively large validation samples in combination with shrinkage. Copyright (C) 2004 John Wiley Sons, Ltd.	Erasmus Med Ctr, Ctr Clin Decis Sci, Dept Publ Hlth, NL-3000 DR Rotterdam, Netherlands; Leiden Univ, Dept Med Stat, Ctr Med, NL-2300 RA Leiden, Netherlands	Steyerberg, EW (reprint author), Erasmus Med Ctr, Ctr Clin Decis Sci, Dept Publ Hlth, Ee2093,POB 1738, NL-3000 DR Rotterdam, Netherlands.	e.steyerberg@erasmusmc.nl	van houwelingen, hans/C-1872-2008; Eijkemans, Marinus/D-8214-2013; 	Steyerberg, Ewout/0000-0002-7787-0122			BREIMAN L, 1995, TECHNOMETRICS, V37, P373, DOI 10.2307/1269730; MILLER ME, 1993, MED DECIS MAKING, V13, P49, DOI 10.1177/0272989X9301300107; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Justice AC, 1999, ANN INTERN MED, V130, P515; MAGGIONI AP, 1993, NEW ENGL J MED, V329, P1442, DOI 10.1056/NEJM199311113292002; LEE KL, 1995, CIRCULATION, V91, P1659; [Anonymous], 1989, NEW ENGL J MED, V320, P618; Altman DG, 2000, STAT MED, V19, P453, DOI 10.1002/(SICI)1097-0258(20000229)19:4<453::AID-SIM350>3.3.CO;2-X; COX DR, 1958, BIOMETRIKA, V45, P562, DOI 10.1093/biomet/45.3-4.562; Steyerberg EW, 1999, J CLIN EPIDEMIOL, V52, P935, DOI 10.1016/S0895-4356(99)00103-1; COPAS JB, 1983, J R STAT SOC B, V45, P311; Harrell FE, 1996, STAT MED, V15, P361, DOI 10.1002/(SICI)1097-0258(19960229)15:4<361::AID-SIM168>3.0.CO;2-4; ARKES HR, 1995, MED DECIS MAKING, V15, P120; Cleveland W. S., 1985, ELEMENTS GRAPHING DA; DeLong ER, 1997, STAT MED, V16, P2645, DOI 10.1002/(SICI)1097-0258(19971215)16:23<2645::AID-SIM696>3.0.CO;2-D; DUBOIS C, 1988, AM J CARDIOL, V61, P216, DOI 10.1016/0002-9149(88)90918-6; Ennis M, 1998, STAT MED, V17, P2501; GREENLAND S, 1993, STAT MED, V12, P717, DOI 10.1002/sim.4780120802; TOPOL E, 1993, NEW ENGL J MED, V329, P673; Harrell FE, 2001, REGRESSION MODELING; HARRELL FE, 1984, STAT MED, V3, P143, DOI 10.1002/sim.4780030207; Ivanov J, 1999, CIRCULATION, V99, P2098; MAYNARD C, 1993, AM J CARDIOL, V72, P877, DOI 10.1016/0002-9149(93)91099-4; MILLER ME, 1991, STAT MED, V10, P1213, DOI 10.1002/sim.4780100805; MUELLER HS, 1992, CIRCULATION, V85, P1254; PILOTE L, 1995, NEW ENGL J MED, V333, P565, DOI 10.1056/NEJM199508313330907; SPIEGELHALTER DJ, 1986, STAT MED, V5, P421, DOI 10.1002/sim.4780050506; Steyerberg EW, 2001, STAT MED, V20, P3847, DOI 10.1002/sim.915.abs; Steyerberg EW, 2000, STAT MED, V19, P1059, DOI 10.1002/(SICI)1097-0258(20000430)19:8<1059::AID-SIM412>3.3.CO;2-S; Steyerberg EW, 1998, J CLIN ONCOL, V16, P269; Steyerberg EW, 2001, STAT NEERL, V55, P76, DOI 10.1111/1467-9574.00157; Steyerberg EW, 2000, STAT MED, V19, P141, DOI 10.1002/(SICI)1097-0258(20000130)19:2<141::AID-SIM334>3.0.CO;2-O; VANHOUWELINGEN HC, 1995, STAT MED, V14, P1999, DOI 10.1002/sim.4780141806; van Houwelingen HC, 2000, STAT MED, V19, P3401, DOI 10.1002/1097-0258(20001230)19:24<3401::AID-SIM554>3.0.CO;2-2; VANHOUWELINGEN JC, 1990, STAT MED, V9, P1303, DOI 10.1002/sim.4780091109; Vergouwe Y, 2001, J UROLOGY, V165, P84, DOI 10.1097/00005392-200101000-00021	36	156	158	3	18	JOHN WILEY & SONS LTD	CHICHESTER	THE ATRIUM, SOUTHERN GATE, CHICHESTER PO19 8SQ, W SUSSEX, ENGLAND	0277-6715			STAT MED	Stat. Med.	AUG 30	2004	23	16					2567	2586		10.1002/sim.1844		20	Mathematical & Computational Biology; Public, Environmental & Occupational Health; Medical Informatics; Medicine, Research & Experimental; Statistics & Probability	Mathematical & Computational Biology; Public, Environmental & Occupational Health; Medical Informatics; Research & Experimental Medicine; Mathematics	846LZ	WOS:000223318400007	15287085	
J	Rosset, S; Zhu, J				Rosset, S; Zhu, J			Corrected proof of the result of 'A prediction error property of the Lasso estimator and its generalization' by Huang (2003)	AUSTRALIAN & NEW ZEALAND JOURNAL OF STATISTICS			English	Article						lasso; least squares estimator; piecewise linear; prediction error	REGRESSION; SHRINKAGE	The Lasso achieves variance reduction and variable selection by solving an l(1)-regularized least squares problem. Huang (2003) claims that 'there always exists an interval of regularization parameter values such that the corresponding mean squared prediction error for the Lasso estimator is smaller than for the ordinary least square estimator'. This result is correct. However, its proof in Huang (2003) is not. This paper presents a corrected proof of the claim, which exposes and uses some interesting fundamental properties of the Lasso.	IBM Corp, Thomas J Watson Res Ctr, Data Analyt Res Grp, Yorktown Hts, NY 10598 USA; Univ Michigan, Dept Stat, Ann Arbor, MI 48109 USA	Rosset, S (reprint author), IBM Corp, Thomas J Watson Res Ctr, Data Analyt Res Grp, POB 218, Yorktown Hts, NY 10598 USA.	srosset@us.ibm.com					Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Friedman J, 2004, ANN STAT, V32, P102; Efron B, 2004, ANN STAT, V32, P407; DONOHO DL, 1995, J ROY STAT SOC B MET, V57, P301; Huang FC, 2003, AUST NZ J STAT, V45, P217, DOI 10.1111/1467-842X.00277; Osborne MR, 2000, J COMPUT GRAPH STAT, V9, P319, DOI 10.2307/1390657	6	1	1	0	0	BLACKWELL PUBL LTD	OXFORD	108 COWLEY RD, OXFORD OX4 1JF, OXON, ENGLAND	1369-1473			AUST NZ J STAT	Aust. N. Z. J. Stat.	SEP	2004	46	3					505	510		10.1111/j.1467-842X.2004.00347.x		6	Statistics & Probability	Mathematics	853GR	WOS:000223815800014		
J	Krishnapuram, B; Hartemink, AJ; Carin, L; Figueiredo, MAT				Krishnapuram, B; Hartemink, AJ; Carin, L; Figueiredo, MAT			A Bayesian approach to joint feature selection and classifier design	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						pattern recognition; statistical learning; feature selection; sparsity; support vector machines; relevance vector machines; sparse probit regression; automatic relevance determination; EM algorithm		This paper adopts a Bayesian approach to simultaneously learn both an optimal nonlinear classifier and a subset of predictor variables (or features) that are most relevant to the classification task. The approach uses heavy-tailed priors to promote sparsity in the utilization of both basis functions and features; these priors act as regularizers for the likelihood function that rewards good classification on the training data. We derive an expectation-maximization (EM) algorithm to efficiently compute a maximum a posteriori (MAP) point estimate of the various parameters. The algorithm is an extension of recent state-of-the-art sparse Bayesian classifiers, which in turn can be seen as Bayesian counterparts of support vector machines. Experimental comparisons using kernel classifiers demonstrate both parsimonious feature selection and excellent classification accuracy on a range of synthetic and benchmark data sets.	Duke Univ, Dept Elect Engn, Durham, NC 27708 USA; Duke Univ, Dept Comp Sci, Durham, NC 27708 USA; Inst Super Tecn, Inst Telecomunicacoes, P-1049001 Lisbon, Portugal	Krishnapuram, B (reprint author), Duke Univ, Dept Elect Engn, Durham, NC 27708 USA.	balaji@ee.duke.edu; amink@cs.duke.edu; lcarin@ee/duke.edu; mtf@lx.it.pt	Figueiredo, Mario/C-5428-2008	Figueiredo, Mario/0000-0002-0970-7745			ALBERT JH, 1993, J AM STAT ASSOC, V88, P669, DOI 10.2307/2290350; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Williams CKI, 1998, IEEE T PATTERN ANAL, V20, P1342, DOI 10.1109/34.735807; Guyon I, 2002, MACH LEARN, V46, P389, DOI 10.1023/A:1012487302797; Tipping ME, 2001, J MACH LEARN RES, V1, P211, DOI 10.1162/15324430152748236; Ben-Dor A., 2000, P 4 ANN INT C COMP M; Duda R. O., 2001, PATTERN CLASSIFICATI; Figueiredo MAT, 2003, IEEE T PATTERN ANAL, V25, P1150, DOI 10.1109/TPAMI.2003.1227989; HERBERICH R, 2002, LEARNING KERNEL CLAS; HUSMEIER D, 1999, P INT C ART NEUR NET, P533; Husmeier D, 1999, NEURAL NETWORKS, V12, P677, DOI 10.1016/S0893-6080(99)00020-9; KRISHNAPURAM B, 2002, P 2002 WORKSH GEN SI; KRISHNAPURAM B, 2003, P 7 ANN INT C COMP M; Lee Y-J, 2001, P SIAM INT C DAT MIN; McCullagh P., 1989, GEN LINEAR MODELS; Neal R. M., 1996, BAYESIAN LEARNING NE; Scholkopf B., 2002, LEARNING KERNELS; SEEGER M, 2000, P ADV NEUR INF PROC, V12; Sun XB, 2003, SIAM J MATRIX ANAL A, V24, P768, DOI 10.1137/S0895479800380374; WESTON J, 2000, P ADV NEUR INF PROC, V12	20	66	71	1	10	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEP	2004	26	9					1105	1111		10.1109/TPAMI.2004.55		7	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Computer Science; Engineering	837CM	WOS:000222605100001	15742887	
J	Cardot, H; Koo, JY; Park, HJ; Trubuil, A				Cardot, H; Koo, JY; Park, HJ; Trubuil, A			Boosting diracs for electrophoresis	JOURNAL OF COMPUTATIONAL AND GRAPHICAL STATISTICS			English	Article						backfitting; deconvolution; GCV; merging; positivity; selection of diracs; spatial inhomogeneity; variables selection	WAVELET SHRINKAGE APPROACH; LINEAR INVERSE PROBLEMS; IMAGE-RECONSTRUCTION; REGRESSION; DECONVOLUTION; SELECTION; SPLINES; LASSO	Electrophoresis is a biochemical process widely used in life sciences and genetics. Recovering information on weights and proportions of molecules from electrophoresis signals may be viewed as a linear inverse problem in the context of a regression setup. The target function to be estimated is known to be positive so that a positive estimator based on discretized Dirac functions is proposed. The methodology is to use boosting method for the target function with respect to discretized Dirac functions. The method is illustrated on simulated data and applied to electrophoresis.	INRA Toulouse, F-31326 Castanet Tolosan, France; Inha Univ, Dept Stat, Inchon 402751, South Korea; INRA, Jouy En Josas, France	Cardot, H (reprint author), INRA Toulouse, F-31326 Castanet Tolosan, France.	cardot@toulouse.inra.fr; jykoo@stat.inha.ac.kr; hjpark@anova.inha.ac.kr; at@jouy.inra.fr	Cardot, Herve/B-6217-2009				ANDREWS AT, 1986, ELECTORPHORESIS THEO; Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451; Friedman J, 2000, ANN STAT, V28, P337, DOI 10.1214/aos/1016218223; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Efron B, 2004, ANN STAT, V32, P407; Babskii V.G., 1989, MATH THEORY ELECTROP; Cardot H, 2002, J MULTIVARIATE ANAL, V81, P100, DOI 10.1006/jmva.2001.1994; Cavalier L, 2002, IEEE T INFORM THEORY, V48, P2794, DOI 10.1109/TIT.2002.802632; Choi Y, 2001, IEEE T MED IMAGING, V20, P1188; Craig I., 1986, INVERSE PROBLEMS AST; DONOHO DL, 1995, APPL COMPUT HARMON A, V2, P101, DOI 10.1006/acha.1995.1008; Fan JQ, 2002, IEEE T INFORM THEORY, V48, P734; GLASBEY CA, 1998, ENCY BIOSTATISTICS, P1980; Hansen P., 1998, RANK DEFICIENT DISCR; HORGAN GW, 1995, ELECTROPHORESIS, V16, P298, DOI 10.1002/elps.1150160149; Kolaczyk ED, 1996, J AM STAT ASSOC, V91, P1079, DOI 10.2307/2291727; Koo JY, 1997, J COMPUT GRAPH STAT, V6, P266, DOI 10.2307/1390733; Li L, 2000, ANN STAT, V28, P1279; O'Sullivan F., 1986, STAT SCI, V1, P502, DOI 10.1214/ss/1177013525; Osborne MR, 1998, COMP SCI STAT, V30, P44; Serra J., 1982, IMAGE ANAL MATH MORP; Tikhonov A N, 1987, ILL POSED PROBLEMS N; Tikhonov AN, 1977, SOLUTIONS ILL POSED; TRIBUIL A, 1993, COMPUTER APPL BIOSCI, P451	24	1	1	0	2	AMER STATISTICAL ASSOC	ALEXANDRIA	1429 DUKE ST, ALEXANDRIA, VA 22314 USA	1061-8600			J COMPUT GRAPH STAT	J. Comput. Graph. Stat.	SEP	2004	13	3					659	673		10.1198/106186004X2615		15	Statistics & Probability	Mathematics	846CI	WOS:000223292000008		
J	Zhang, HH; Wahba, G; Lin, Y; Voelker, M; Ferris, M; Klein, R; Klein, B				Zhang, HH; Wahba, G; Lin, Y; Voelker, M; Ferris, M; Klein, R; Klein, B			Variable selection and model building via likelihood basis pursuit	JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION			English	Article						generalized approximate cross-validation; LASSO; Monte Carlo bootstrap test; nonparametric variable selection; slice modeling; smoothing spline ANOVA	SMOOTHING SPLINE ANOVA; BEAVER DAM EYE; DIABETIC-RETINOPATHY; CROSS-VALIDATION; NONPARAMETRIC REGRESSION; BERNOULLI OBSERVATIONS; PENALIZED LIKELIHOOD; VISUAL-ACUITY; DIAGNOSIS; LASSO	This article presents a nonparametric penalized likelihood approach for variable selection and model building, called likelihood basis pursuit (LBP). In the setting of a tenser product reproducing kernel Hilbert space, we decompose the log-likelihood into the sum of different functional components such as main effects and interactions, with each component represented by appropriate basis functions. Basis functions are chosen to be compatible with variable selection and model building in the context of a smoothing spline ANOVA model. Basis pursuit is applied to obtain the optimal decomposition in terms of having the smallest L-1 norm on the coefficients. We use the functional l(1) norm to measure the importance of each component and determine the "threshold" value by a sequential Monte Carlo bootstrap test algorithm. As a generalized LASSO-type method, LBP produces shrinkage estimates for the coefficients, which greatly facilitates the variable selection process and provides highly interpretable multivariate functional estimate,,, at the same time. To choose the regularization parameters appearing in the LBP models, generalized approximate cross-validation (GACV) is derived as a tuning criterion. To make GACV widely applicable to large datasets, its randomized version is proposed as well. A technique "slice modeling" is used to solve the optimization problem and makes the computation more efficient. LBP has great potential for a wide range of research and application areas such as medical studies, and in this article we apply it to two large ongoing epidermologic studies, the Wisconsin Epidermologic Study of Diabetic Retinopathy (WESDR) and the Beaver Dam Eye Study (BDES).	N Carolina State Univ, Dept Stat, Raleigh, NC 27695 USA; Univ Wisconsin, Dept Stat, Madison, WI 53706 USA; Univ Wisconsin, Dept Comp Sci & Ind Engn, Madison, WI 53706 USA; Alphatech Inc, Arlington, VA 22203 USA; Univ Wisconsin, Sch Med, Dept Ophthalmol, Madison, WI 53706 USA	Zhang, HH (reprint author), N Carolina State Univ, Dept Stat, Raleigh, NC 27695 USA.	hzhang2@stat.ncsu.edu; wahba@stat.wisc.edu; yilin@stat.wisc.edu; meta.voelker@dc.alphatech.com; ferris@cs.wisc.edu; kleinr@epi.ophth.wisc.edu; kleinB@epi.ophth.wisc.edu					Fu WJJ, 1998, J COMPUT GRAPH STAT, V7, P397, DOI 10.2307/1390712; Knight K, 2000, ANN STAT, V28, P1356; Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010; KLEIN R, 1991, OPHTHALMOLOGY, V98, P1310; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; WAHBA G, 1975, COMMUN STAT, V4, P1; FRANK IE, 1993, TECHNOMETRICS, V35, P109, DOI 10.2307/1269656; Klein R, 1998, OPHTHALMOLOGY, V105, P1801, DOI 10.1016/S0161-6420(98)91020-X; CRAVEN P, 1979, NUMER MATH, V31, P377; Bakin S, 1999, THESIS AUSTR NATL U; Craig BA, 1999, STAT MED, V18, P1355, DOI 10.1002/(SICI)1097-0258(19990615)18:11<1355::AID-SIM130>3.3.CO;2-B; FERRIS MC, 2000, OPTIMIZATION METHODS, V17, P1009; FERRIS MC, 2001, OPERATIONS RES P, P239; Gao FY, 2001, J AM STAT ASSOC, V96, P127, DOI 10.1198/016214501750332749; Girard DA, 1998, ANN STAT, V26, P315; Gu C, 2002, CAN J STAT, V30, P619, DOI 10.2307/3316100; Gu C., 2002, SMOOTHING SPLINE ANO; GUNN SR, 2002, MACH LEARN, V48, P115; Hastie TJ, 1990, GEN ADDITIVE MODELS; Hinkley D. V., 1997, BOOTSTRAP METHODS TH; HUTCHINSON MF, 1989, COMMUN STAT SIMULAT, V18, P1059, DOI 10.1080/03610918908812806; KIM K, 1995, STAT MED, V14, P1341, DOI 10.1002/sim.4780141207; KIMELDOR.G, 1971, J MATH ANAL APPL, V33, P82, DOI 10.1016/0022-247X(71)90184-3; KLEIN R, 1984, ARCH OPHTHALMOL-CHIC, V102, P520; Klein R, 2001, OPHTHALMOLOGY, V108, P1757, DOI 10.1016/S0161-6420(01)00769-2; KLEIN R, 1989, ARCH OPHTHALMOL-CHIC, V107, P237; KLEIN R, 1984, ARCH OPHTHALMOL-CHIC, V102, P527; Lin XW, 2000, ANN STAT, V28, P1570; Linhart H., 1986, MODEL SELECTION; MANGASARIAN O. L., 1969, NONLINEAR PROGRAMMIN; Murtagh B. A., 1983, 8320R SOL STANF U; Ruppert D, 2000, AUST NZ J STAT, V42, P205, DOI 10.1111/1467-842X.00119; Wahba G., 1990, SPLINE MODELS OBSERV; Wahba G, 1995, ANN STAT, V23, P1865; Xiang D, 1996, STAT SINICA, V6, P675; Xiang D., 1998, P 1997 ASA JOINT STA, P94; Yau P, 2003, J COMPUT GRAPH STAT, V12, P23, DOI 10.1198/1061860031301; ZHANG HH, 2002, 1066 U WISC MAD DEP	39	27	27	0	5	AMER STATISTICAL ASSOC	ALEXANDRIA	1429 DUKE ST, ALEXANDRIA, VA 22314 USA	0162-1459			J AM STAT ASSOC	J. Am. Stat. Assoc.	SEP	2004	99	467					659	672		10.1198/016214504000000593		14	Statistics & Probability	Mathematics	853VR	WOS:000223857500016		
J	Fan, JQ; Li, R				Fan, JQ; Li, R			New estimation and model selection procedures for semiparametric modeling in longitudinal data analysis	JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION			English	Article						local polynomial regression; partial linear model; penalized least squares; profile least squares; smoothly clipped absolute deviation	VARYING-COEFFICIENT MODELS; NONPARAMETRIC REGRESSION-ANALYSIS; LINEAR-MODELS; LIKELIHOOD	Semiparametric regression models are very useful for longitudinal data analysis. The complexity of semiparametric models and the structure of longitudinal data pose new challenges to parametric inferences and model selection that frequently arise from longitudinal data analysis. In this article, two new approaches are proposed for estimating the regression coefficients in a semiparametric model. The asymptotic normality of the resulting estimators is established. An innovative class of variable selection procedures is proposed to select significant variables in the semiparametric models. The proposed procedures are distinguished from others in that they simultaneously select significant variables and estimate unknown parameters. Rates of convergence of the resulting estimators are established. With a proper choice of regularization parameters and penalty functions, the proposed variable selection procedures are shown to perform as well as an oracle estimator. A robust standard error formula is derived using a sandwich formula and is empirically tested. Local polynomial regression techniques are used to estimate the baseline function in the semiparametric model.	Princeton Univ, Dept Operat Res & Financial Engn, Princeton, NJ 08544 USA; Penn State Univ, Dept Stat, University Pk, PA 16802 USA	Fan, JQ (reprint author), Princeton Univ, Dept Operat Res & Financial Engn, Princeton, NJ 08544 USA.	jqfan@Princeton.edu; rli@stat.psu.edu	Li, Runze/C-5444-2013	Li, Runze/0000-0002-0154-2202			ANDERSEN PK, 1982, ANN STAT, V10, P1100, DOI 10.1214/aos/1176345976; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Antoniadis A, 2001, J AM STAT ASSOC, V96, P939, DOI 10.1198/016214501753208942; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; FAN JQ, 1992, J AM STAT ASSOC, V87, P998, DOI 10.2307/2290637; Huang JHZ, 2002, BIOMETRIKA, V89, P111, DOI 10.1093/biomet/89.1.111; Fan JQ, 2000, J ROY STAT SOC B, V62, P303, DOI 10.1111/1467-9868.00233; Hoover DR, 1998, BIOMETRIKA, V85, P809, DOI 10.1093/biomet/85.4.809; FRANK IE, 1993, TECHNOMETRICS, V35, P109, DOI 10.2307/1269656; Carroll RJ, 1997, J AM STAT ASSOC, V92, P477, DOI 10.1080/01621459.1997.10474001; Breiman L, 1996, ANN STAT, V24, P2350; Cheng SC, 2000, BIOMETRIKA, V87, P89, DOI 10.1093/biomet/87.1.89; Chiang CT, 2001, J AM STAT ASSOC, V96, P605, DOI 10.1198/016214501753168280; Diggle P., 2002, ANAL LONGITUDINAL DA; Fan JQ, 2001, ANN STAT, V29, P153, DOI 10.1214/aos/996986505; Fan JQ, 2001, J AM STAT ASSOC, V96, P640, DOI 10.1198/016214501753168316; Gijbels I., 1996, LOCAL POLYNOMIAL MOD; KASLOW RA, 1987, AM J EPIDEMIOL, V126, P310; Lin DY, 2001, J AM STAT ASSOC, V96, P103, DOI 10.1198/016214501750333018; Lin XH, 2001, J AM STAT ASSOC, V96, P1045, DOI 10.1198/016214501753208708; Lin XH, 2001, J AM STAT ASSOC, V96, P114; Martinussen T, 1999, BIOMETRIKA, V86, P691, DOI 10.1093/biomet/86.3.691; Martinussen T, 2001, SCAND J STAT, V28, P303, DOI 10.1111/1467-9469.00239; Moyeeds RA, 1994, AUSTR J STAT, V36, P75; MULLER HG, 1993, J STAT PLAN INFER, V35, P213, DOI 10.1016/0378-3758(93)90046-9; Ruppert D, 1995, J AM STAT ASSOC, V90, P1257, DOI 10.2307/2291516; SEVERINI TA, 1994, J AM STAT ASSOC, V89, P501, DOI 10.2307/2290852; SPECKMAN P, 1988, J ROY STAT SOC B MET, V50, P413; Verbeke G., 2000, LINEAR MIXED MODELS; WANG N, 2004, IN PRESS J AM STAT A; Wu CO, 1998, J AM STAT ASSOC, V93, P1388, DOI 10.2307/2670054; ZEGER SL, 1994, BIOMETRICS, V50, P689, DOI 10.2307/2532783	32	179	199	3	11	AMER STATISTICAL ASSOC	ALEXANDRIA	1429 DUKE ST, ALEXANDRIA, VA 22314 USA	0162-1459			J AM STAT ASSOC	J. Am. Stat. Assoc.	SEP	2004	99	467					710	723		10.1198/0162145040000001060		14	Statistics & Probability	Mathematics	853VR	WOS:000223857500020		
J	Shen, XT; Huang, HC; Ye, J				Shen, XT; Huang, HC; Ye, J			Inference after model selection	JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION			English	Article						bootstrap; nonparametric; parametric; variable selection; wavelet thresholding	VARIABLE SELECTION; REGRESSION; SHRINKAGE; CRITERION	Typical modeling strategies involve model selection, which has a significant effect on inference of estimated parameters. Common practice is to use a selected model ignoring uncertainty introduced by the process of model selection. This could yield overoptimistic inferences, resulting in false discovery. In this article we develop a general methodology via optimal approximation for estimating the mean and variance of complex statistics that involve the process of model selection. This allows us to make approximately unbiased inferences, taking into account the selection process. We examine the operating characteristics of the proposed methodology via asymptotic analyses and simulations. These results show that the proposed methodology yields correct inferences and outperforms common alternatives.	Univ Minnesota, Sch Stat, Minneapolis, MN 55455 USA; CUNY, Baruch Coll, Stan Ross Dept Accountancy, New York, NY 10010 USA; Inst Stat Sci, Academia Sinica, Taipei 115, Taiwan	Shen, XT (reprint author), Univ Minnesota, Sch Stat, Minneapolis, MN 55455 USA.	xshen@stat.umn.edu; hchuang@stat.sinica.edu.tw; jimmy_ye@baruch.curry.edu	Huang, Hsin-Cheng/H-1743-2011				Abramovich F, 1996, COMPUT STAT DATA AN, V22, P351, DOI 10.1016/0167-9473(96)00003-5; Akaike H., 1973, INT S INF THEOR, P267, DOI DOI 10.2307/2334537; Shen XT, 2002, J AM STAT ASSOC, V97, P210, DOI 10.1198/016214502753479356; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; BREIMAN L, 1992, J AM STAT ASSOC, V87, P738, DOI 10.2307/2290212; KASS RE, 1995, J AM STAT ASSOC, V90, P928, DOI 10.2307/2291327; DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425; SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136; MALLOWS CL, 1973, TECHNOMETRICS, V15, P661, DOI 10.2307/1267380; Brillinger D., 1996, NONPARAMETRIC STAT, V6, P93; Bruce A., 1996, APPL WAVELET ANAL S; Bruce AG, 1996, BIOMETRIKA, V83, P727, DOI 10.1093/biomet/83.4.727; FAN J, 2004, IN PRESS ANN STAT; FARAWAY JJ, 1992, J COMPUTATIONAL GRAP, V1, P213, DOI 10.2307/1390717; Freedman DA, 1988, MODEL UNCERTAINTY IT, P1, DOI DOI 10.1007/978-3-642-61564-1_1; George EI, 2000, BIOMETRIKA, V87, P731, DOI 10.1093/biomet/87.4.731; KABAILA P, 1995, ECONOMET THEOR, V11, P537; POTSCHER BM, 1991, ECONOMET THEOR, V7, P163; SHIBATA R, 1976, BIOMETRIKA, V63, P117; STEIN CM, 1981, ANN STAT, V9, P1135, DOI 10.1214/aos/1176345632; Ye JM, 1998, J AM STAT ASSOC, V93, P120, DOI 10.2307/2669609; ZHANG P, 1992, BIOMETRIKA, V79, P741, DOI 10.1093/biomet/79.4.741	23	22	23	15	17	AMER STATISTICAL ASSOC	ALEXANDRIA	1429 DUKE ST, ALEXANDRIA, VA 22314 USA	0162-1459			J AM STAT ASSOC	J. Am. Stat. Assoc.	SEP	2004	99	467					751	762		10.1198/016214504000001097		12	Statistics & Probability	Mathematics	853VR	WOS:000223857500023		
J	Forina, M; Lanteri, S; Oliveros, MCC; Millan, CP				Forina, M; Lanteri, S; Oliveros, MCC; Millan, CP			Selection of useful predictors in multivariate calibration	ANALYTICAL AND BIOANALYTICAL CHEMISTRY			English	Review						multivariate calibration; predictor selection	PARTIAL LEAST-SQUARES; NEAR-INFRARED SPECTROSCOPY; WAVELENGTH SELECTION; GENETIC ALGORITHMS; VARIABLE SELECTION; REGRESSION; VALIDATION; PLS; ELIMINATION; SHRINKAGE	Ten techniques used for selection of useful predictors in multivariate calibration and in other cases of multivariate regression are described and discussed in terms of their performance (ability to detect useless predictors, predictive power, number of retained predictors) with real and artificial data. The techniques studied include classical stepwise ordinary least-squares (SOLS), techniques based on the genetic algorithms, and a family of methods based on partial least-squares (PLS) regression and on the optimization of the predictive ability. A short introduction presents the evaluation strategies, a description of the quantities used to evaluate the regression model, and the criteria used to define the complexity of PLS models. The selection techniques can be divided into conservative techniques that try to retain all the informative, useful predictors, and parsimonious techniques, whose objective is to select a minimum but sufficient number of useful predictors. Some combined techniques, in which a conservative technique is used to perform a preliminary selection before the use of parsimonious techniques, are also presented. Among the conservative techniques, the Westad-Martens uncertainty test (MUT) used in Unscrambler, and uninformative variables elimination (UVE), developed by Massart et al., seem the most efficient techniques. The old SOLS can be improved to become the most efficient parsimonious technique, by means of the use of plots of the F-statistics value of the entered predictors and comparison with parallel results obtained with a data matrix with random data. This procedure indicates correctly how many predictors can be accepted and substantially reduces the possibility of overfitting. A possible alternative to SOLS is iterative predictors weighting (IPW) that automatically selects a minimum set of informative predictors. The use of an external evaluation set, with objects never used in the elimination of predictors, or of "complete validation" is suggested to avoid overestimate of the prediction ability.	Univ Genoa, Dept Pharmaceut & Food Chem & Technol, I-16147 Genoa, Italy; Univ La Rioja, Dept Chem, Logrono 26006, La Rioja, Spain	Forina, M (reprint author), Univ Genoa, Dept Pharmaceut & Food Chem & Technol, Via Brigata Salerno S-N, I-16147 Genoa, Italy.	forina@dictfa.unige.it		Lanteri, Silvia/0000-0002-8895-1358			KENNARD RW, 1969, TECHNOMETRICS, V11, P137, DOI 10.2307/1266770; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Centner V, 1996, ANAL CHEM, V68, P3851, DOI 10.1021/ac960321m; Forina M, 1999, J CHEMOMETR, V13, P165; Hoskuldsson A, 2001, CHEMOMETR INTELL LAB, V55, P23, DOI 10.1016/S0169-7439(00)00113-1; Westad F, 2000, J NEAR INFRARED SPEC, V8, P117; HAALAND DM, 1988, ANAL CHEM, V60, P1193, DOI 10.1021/ac00162a020; LUCASIUS CB, 1991, TRAC-TREND ANAL CHEM, V10, P254, DOI 10.1016/0165-9936(91)85132-B; SHAO J, 1993, J AM STAT ASSOC, V88, P486, DOI 10.2307/2290328; Norgaard L, 2000, APPL SPECTROSC, V54, P413, DOI 10.1366/0003702001949500; SNEE RD, 1977, TECHNOMETRICS, V19, P415, DOI 10.2307/1267881; Kalivas JH, 1997, CHEMOMETR INTELL LAB, V37, P255, DOI 10.1016/S0169-7439(97)00038-5; BELSLEY DA, 1981, REGRESSION DIAGNOSTI; Boggia R, 1997, QUANT STRUCT-ACT REL, V16, P201, DOI 10.1002/qsar.19970160303; BREIMAN L, 1992, INT STAT REV, V60, P291, DOI 10.2307/1403680; Brown PJ, 1998, J CHEMOMETR, V12, P173, DOI 10.1002/(SICI)1099-128X(199805/06)12:3<173::AID-CEM505>3.3.CO;2-S; CRUCIANI G, 1998, 3D QSAR DRUG DESIGN; EFRON, 1982, JACKKNIFE BOOTSTRAP; Faber NM, 2001, ANAL CHIM ACTA, V432, P235, DOI 10.1016/S0003-2670(00)01381-7; FORINA M, 2003, V PARVUS RELEASE EXT; FORINA M, 1995, CHEMOMETR INTELL LAB, V27, P189, DOI 10.1016/0169-7439(94)00028-H; FORINA M, 1986, 6 CHEM AN CHEM C CAC; FRANK IE, 1987, CHEMOMETR INTELL LAB, V1, P233, DOI 10.1016/0169-7439(87)80067-9; Frenich AG, 1995, ANALYST, V120, P2787, DOI 10.1039/an9952002787; THOMAS EV, 1990, ANAL CHEM, V62, P1091, DOI 10.1021/ac00209a024; KETTANEHWOLD N, 1994, CHEMOMETR INTELL LAB, V23, P39, DOI 10.1016/0169-7439(93)E0072-C; KOWALSKI BR, 1991, J CHEMOMETR, V5, P129, DOI 10.1002/cem.1180050303; LEARDI R, 1992, J CHEMOMETR, V6, P267, DOI 10.1002/cem.1180060506; LINDGREN F, 1994, J CHEMOMETR, V8, P349, DOI 10.1002/cem.1180080505; Martens H., 1989, MULTIVARIATE CALIBRA; MASSART DL, 1998, HDB CHEMOMETRICS Q A; Ojelund H, 2001, J CHEMOMETR, V15, P497; Osten D.W., 1988, J CHEMOMETR, V2, P39, DOI DOI 10.1002/CEM.1180020106; VANDERVOET H, 1994, CHEMOMETR INTELL LAB, V25, P313, DOI 10.1016/0169-7439(94)85050-X	34	52	52	2	11	SPRINGER HEIDELBERG	HEIDELBERG	TIERGARTENSTRASSE 17, D-69121 HEIDELBERG, GERMANY	1618-2642			ANAL BIOANAL CHEM	Anal. Bioanal. Chem.	OCT	2004	380	3					397	418		10.1007/s00216-004-2768-x		22	Biochemical Research Methods; Chemistry, Analytical	Biochemistry & Molecular Biology; Chemistry	867SR	WOS:000224863000008	15349711	
J	Hastie, T; Rosset, S; Tibshirani, R; Zhu, J				Hastie, T; Rosset, S; Tibshirani, R; Zhu, J			The entire regularization path for the support vector machine	JOURNAL OF MACHINE LEARNING RESEARCH			English	Article						support vector machines; regularization; coefficient path		The support vector machine (SVM) is a widely used tool for classification. Many efficient implementations exist for fitting a two-class SVM model. The user has to supply values for the tuning parameters: the regularization cost parameter, and the kernel parameters. It seems a common practice is to use a default value for the cost parameter, often leading to the least restrictive model. In this paper we argue that the choice of the cost parameter can be critical. We then derive an algorithm that can fit the entire path of SVM solutions for every value of the cost parameter, with essentially the same computational cost as fitting one SVM model. We illustrate our algorithm on some examples, and use our representation to give further insight into the range of SVM solutions.	Stanford Univ, Dept Stat, Stanford, CA 94305 USA; IBM Corp, Thomas J Watson Res Ctr, Yorktown Hts, NY 10598 USA; Univ Michigan, Dept Stat, Ann Arbor, MI 48109 USA	Hastie, T (reprint author), Stanford Univ, Dept Stat, Stanford, CA 94305 USA.	HASTIE@STANFORD.EDU; SROSSET@US.IBM.COM; TIBS@STANFORD.EDU; JIZHU@UMICH.EDU					Allgower Eugene L., 1992, ACTA NUMERICA, P1; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Evgeniou T, 2000, ADV COMPUT MATH, V13, P1, DOI 10.1023/A:1018946025316; Ramaswamy S, 2001, P NATL ACAD SCI USA, V98, P15149, DOI 10.1073/pnas.211566398; Bach FR, 2002, J MACHINE LEARNING R, V3, P1; Cauwenberghs G., 2001, ADV NEURAL INFORM PR, V13; DeCoste D., 2000, Proceedings. KDD-2000. Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, DOI 10.1145/347090.347165; Diehl CP, 2003, IEEE IJCNN, P2685; EFRON B, 2004, IN PRESS ANN STAT; EFRON B, 2002, LEAST ANGLE REGRESSI; FINE S, 2002, INCAS INCREMENTAL AC; Hastie T., 2001, ELEMENTS STAT LEARNI; HASTIE T, 2003, EFFICIENT QUADRATIC; Hastie TJ, 1990, GEN ADDITIVE MODELS; Hsu C.-W., 2003, PRACTICAL GUIDE SUPP; JOACHIMS T, 1999, PRACTICAL ADV KERNEL; MARRON S, 2003, OVERVIEW SUPPORT VEC; Pontil M, 1998, NEURAL COMPUT, V10, P955, DOI 10.1162/089976698300017575; Ripley B. D., 1996, PATTERN RECOGNITION; ROSSET S, 2004, ADV NEURAL INFORM PR, V16; Rosset S., 2003, PIECEWISE LINEAR REG; ROSSET S, 2005, IN PRESS ADV NEURAL, V17; Scholkopf B., 2001, LEARNING KERNELS SUP; Vapnik V.N., 1996, NATURE STAT LEARNING; Wahba G, 2000, ADV NEUR IN, P297; Wahba G., 1990, SPLINE MODELS OBSERV; WESTON J, 1998, MULTI CLASS SUPPORT; Williams C., 2000, P 17 INT C MACH LEAR, P1159; ZHU J, 2003, L1 NORM SUPPORT VECT; ZHU J, 2004, IN PRESS BIOSTATISTI	30	223	237	0	11	MICROTOME PUBL	BROOKLINE	31 GIBBS ST, BROOKLINE, MA 02446 USA	1532-4435			J MACH LEARN RES	J. Mach. Learn. Res.	OCT	2004	5						1391	1415				25	Automation & Control Systems; Computer Science, Artificial Intelligence	Automation & Control Systems; Computer Science	026GV	WOS:000236328300007		
J	Koenker, R				Koenker, R			Quantile regression for longitudinal data	JOURNAL OF MULTIVARIATE ANALYSIS			English	Article						quantile regression; penalty methods; shrinkage; L-statistics; random effects; robust estimation; hierarchical models	LINEAR-MODEL; ESTIMATORS; LASSO	The penalized least squares interpretation of the classical random effects estimator suggests a possible way forward for quantile regression models with a large number of "fixed effects". The introduction of a large number of individual fixed effects can significantly inflate the variability of estimates of other covariate effects. Regularization, or shrinkage of these individual effects toward a common value can help to modify this inflation effect. A general approach to estimating quantile regression models for longitudinal data is proposed employing l(1) regularization methods. Sparse linear algebra and interior point methods for solving large linear programs are essential computational tools. (C) 2004 Elsevier Inc. All rights reserved.	Univ Illinois, Dept Econ, Champaign, IL 61820 USA	Koenker, R (reprint author), Univ Illinois, Dept Econ, Box 111-1206 S 6th St, Champaign, IL 61820 USA.	roger@ysidro.econ.uiuc.edu					LINDLEY DV, 1972, J ROY STAT SOC B, V34, P1; Knight K, 2000, ANN STAT, V28, P1356; Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010; KOENKER R, 1978, ECONOMETRICA, V46, P33, DOI 10.2307/1913643; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; CHAMBERS J M., 1998, PROGRAMMING DATA GUI; COLE TJ, 1988, J ROY STAT SOC A STA, V151, P385, DOI 10.2307/2982992; GOLDBERGER AS, 1962, J AM STAT ASSOC, V57, P369, DOI 10.2307/2281645; HENDERSON CR, 1950, ANN MATH STAT, V21, P309; IHAKA R, 1996, J COMPUTATIONAL GRAP, V5, P299, DOI DOI 10.2307/1390807; KNIGHT K, 2001, COMP CONDITIONAL QUA; Knight K, 1998, ANN STAT, V26, P755; KOENKER R, 1984, STAT PROBABIL LETT, V2, P323, DOI 10.1016/0167-7152(84)90040-3; Koenker R., 2003, J STAT SOFTW, V8, P1; MOSTELLER F, 1946, ANN MATH STAT, V17, P377, DOI 10.1214/aoms/1177730881; Rao C. R., 1973, LINEAR STAT INFERENC; Robinson G. K., 1991, STAT SCI, V6, P15, DOI DOI 10.1214/SS/1177011926; RUPPERT D, 1980, J AM STAT ASSOC, V75, P828, DOI 10.2307/2287169; Saunders M. A., 1994, ORSA Journal on Computing, V6; Scheffe H., 1959, ANAL VARIANCE	20	184	189	3	23	ACADEMIC PRESS INC ELSEVIER SCIENCE	SAN DIEGO	525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA	0047-259X			J MULTIVARIATE ANAL	J. Multivar. Anal.	OCT	2004	91	1					74	89		10.1016/j.jmva.2004.05.006		16	Statistics & Probability	Mathematics	843LS	WOS:000223086000005		
J	Ferris, MC; Voelker, MM; Zhang, HH				Ferris, MC; Voelker, MM; Zhang, HH			Model building with likelihood basis pursuit	OPTIMIZATION METHODS & SOFTWARE			English	Article						model building; slice models; parameter selection; basis pursuit	SUPPORT VECTOR MACHINES; SMOOTHING SPLINE ANOVA; NONPARAMETRIC REGRESSION; SIMPLEX-METHOD; OPTIMIZATION; SELECTION	We consider a non-parametric penalized likelihood approach for model building called likelihood basis pursuit (LBP) that determines the probabilities of binary outcomes given explanatory vectors while automatically selecting important features. The LBP model involves parameters that balance the competing goals of maximizing the log-likelihood and minimizing the penalized basis pursuit terms. These parameters are selected to minimize a proxy of misclassification error. namely, the randomized, generalized approximate cross validation (ranGACV) function. The ranGACV function is not easily represented in compact form; its functional values can only be obtained by solving two instances of the LBP model, which may be computationally expensive. A grid search is typically used to find appropriate parameters, requiring the solutions to hundreds or thousands of instances of the LBP model. Since only parameters (data) are changed between solves, the resulting problem is a nonlinear slice model in the parameter space. We show how slice-modeling techniques significantly improve the efficiency of individual solves and thus speed-up the grid search. In addition, we consider using derivative-free optimization algorithms for parameter selection, replacing the grid search. We show how, by seeding the derivative-free algorithms with a coarse grid search, these algorithms can find better solutions with fewer function evaluations. Our interest in this area comes directly from the seminal work that Olvi and his collaborators have carried out designing and applying optimization techniques to problems in machine learning and data mining.	Univ Wisconsin, Dept Comp Sci, Madison, WI 53706 USA; N Carolina State Univ, Dept Stat, Raleigh, NC 27613 USA	Ferris, MC (reprint author), Univ Wisconsin, Dept Comp Sci, 1210 W Dayton St, Madison, WI 53706 USA.	ferris@cs.wisc.edu; voelker@cs.wisc.edu; hzhang2@stat.ncsu.edu					Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; NELDER JA, 1965, COMPUT J, V7, P308; Lagarias JC, 1998, SIAM J OPTIMIZ, V9, P112, DOI 10.1137/S1052623496303470; Bradley P. S., 1998, Machine Learning. Proceedings of the Fifteenth International Conference (ICML'98); Ferris MC, 2002, OPTIM METHOD SOFTW, V17, P1009, DOI 10.1080/1055678021000034341; FUNG G, 2002, J MACHINE LEARNING R, P303; Girard DA, 1998, ANN STAT, V26, P315; Gu C., 2002, SMOOTHING SPLINE ANO; HUTCHINSON MF, 1989, COMMUN STAT SIMULAT, V18, P1059, DOI 10.1080/03610918908812806; JOHNSTON G, 1993, SUGI P; LEE YJ, 2001, IN PRESS CD P SIAM I; LEE YJ, 2000, 0007 U WISC COMP SCI; Lin XW, 2000, ANN STAT, V28, P1570; Lin Y, 2002, MACH LEARN, V48, P115, DOI 10.1023/A:1013951620650; Mangasarian OL, 2000, ADV NEUR IN, P135; Mangasarian OL, 2000, IEEE T PATTERN ANAL, V22, P950, DOI 10.1109/34.877518; Marazzi M, 2002, MATH PROGRAM, V91, P289, DOI 10.1007/s101070100264; MARAZZI M, 2001, COMMUNICATION    OCT; *MATHWORKS INC, 2002, FMINSEARCH MATLAB FU; MURTAGH B. A., 1983, 8320 SOL STANF U DEP; Powell MJD, 2002, MATH PROGRAM, V92, P555, DOI 10.1007/s101070100290; POWELL MJD, 2001, COMMUNICATION    NOV; Ruppert D, 2000, AUST NZ J STAT, V42, P205, DOI 10.1111/1467-842X.00119; VOELKER MM, 2002, OPTIMIZATION SLICE M; Wahba G., 1990, SPLINE MODELS OBSERV; Wahba G, 1995, ANN STAT, V23, P1865; Xiang D., 1998, P 1997 ASA JOINT STA, P94; Yau P, 2003, J COMPUT GRAPH STAT, V12, P23, DOI 10.1198/1061860031301; ZHANG H, 2002, THESIS U WISCONSIN M; ZHANG H, 2001, 2001 P AM STAT ASS B; ZHANG H, 2004, IN PRESS J AM STAT A	32	2	2	1	2	TAYLOR & FRANCIS LTD	ABINGDON	4 PARK SQUARE, MILTON PARK, ABINGDON OX14 4RN, OXON, ENGLAND	1055-6788			OPTIM METHOD SOFTW	Optim. Method Softw.	OCT	2004	19	5					577	594		10.1080/1055678042000221719		18	Computer Science, Software Engineering; Operations Research & Management Science; Mathematics, Applied	Computer Science; Operations Research & Management Science; Mathematics	863KO	WOS:000224560700009		
J	Daubechies, I; Defrise, M; De Mol, C				Daubechies, I; Defrise, M; De Mol, C			An iterative thresholding algorithm for linear inverse problems with a sparsity constraint	COMMUNICATIONS ON PURE AND APPLIED MATHEMATICS			English	Article							ILL-POSED PROBLEMS; WAVELET SHRINKAGE; DECOMPOSITION; SUPERRESOLUTION; DECONVOLUTION; TRANSFORM	We consider linear inverse problems where the solution is assumed to have a sparse expansion on an arbitrary preassigned orthonormal basis. We prove that replacing the usual quadratic regularizing penalties by weighted l(p)- penalties on the coefficients of such expansions, with 1 less than or equal to p less than or equal to 2, still regularizes the problem. Use of such l(p)-penalized problems with p < 2 is often advocated when one expects the underlying ideal noiseless solution to have a sparse expansion with respect to the basis under consideration. To compute the corresponding regularized solutions, we analyze an iterative algorithm that amounts to a Landweber iteration with thresholding (or nonlinear shrinkage) applied at each iteration step. We prove that this algorithm converges in norm. (C) 2004 Wiley Periodicals, Inc.	Princeton Univ, Dept Math, Princeton, NJ 08544 USA; Free Univ Brussels, Dept Nucl Med, B-1090 Brussels, Belgium; Free Univ Brussels, Dept Math, B-1050 Brussels, Belgium	Daubechies, I (reprint author), Princeton Univ, Dept Math, Fine Hall,Washington Rd, Princeton, NJ 08544 USA.	ingrid@math.princeton.edu; mdefrise@minf.vub.ac.be; demol@ulb.ac.be	Daubechies, Ingrid/B-5886-2012				Abramovich F, 1998, BIOMETRIKA, V85, P115, DOI 10.1093/biomet/85.1.115; OPIAL Z, 1967, B AM MATH SOC, V73, P591, DOI 10.1090/S0002-9904-1967-11761-0; Lange K, 2000, J COMPUT GRAPH STAT, V9, P1, DOI 10.2307/1390605; Li MH, 2002, PHYS MED BIOL, V47, P2599, DOI 10.1088/0031-9155/47/15/303; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Chambolle A, 1998, IEEE T IMAGE PROCESS, V7, P319; DUFFIN RJ, 1952, T AM MATH SOC, V72, P341, DOI 10.2307/1990760; Chen SSB, 2001, SIAM REV, V43, P129, DOI 10.1137/S003614450037906X; DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425; DONOHO DL, 1995, IEEE T INFORM THEORY, V41, P613, DOI 10.1109/18.382009; Figueiredo MAT, 2003, IEEE T IMAGE PROCESS, V12, P906, DOI 10.1109/TIP.2003.814255; Bertero M., 1998, INTRO INVERSE PROBLE; BERTERO M, 1989, ADV ELECTRON EL PHYS, V75, P1; Bertero M, 1996, PROG OPTICS, V36, P129, DOI 10.1016/S0079-6638(08)70314-7; BEYLKIN G, 1991, COMMUN PUR APPL MATH, V44, P141, DOI 10.1002/cpa.3160440202; Candes EJ, 2002, ANN STAT, V30, P784; COHEN A, IN PRESS SIAM J NUME; Cohen A, 2000, HDBK NUM AN, V7, P417; DEMOL C, 2000, CONT MATH, V313, P85; DEPIERRO AR, 1995, IEEE T MED IMAGING, V14, P132, DOI 10.1109/42.370409; DeVore R. A., 1998, Acta Numerica, V7, DOI 10.1017/S0962492900002816; Dicken V., 2006, J INVERSE ILL-POSE P, V4, P203, DOI 10.1515/jiip.1996.4.3.203; DONOHO DL, 1992, SIAM J MATH ANAL, V23, P1309, DOI 10.1137/0523074; DONOHO DL, 1995, J ROY STAT SOC B MET, V57, P301; DONOHO DL, 1995, APPL COMPUT HARMON A, V2, P101, DOI 10.1006/acha.1995.1008; Donoho DL, 2000, SIAM J MATH ANAL, V31, P1062, DOI 10.1137/S0036141098344403; EICKE B, 1992, NUMER FUNC ANAL OPT, V13, P413, DOI 10.1080/01630569208816489; Engl H. W., 1996, MATH ITS APPL, V375; FRANKLIN JN, 1974, MATH COMPUT, V28, P889, DOI 10.2307/2005354; Hyvarinen A., 2001, INDEPENDENT COMPONEN; Kalifa J, 2003, IEEE T IMAGE PROCESS, V12, P446, DOI 10.1109/TIP.2003.810592; Landweber L., 1951, AM J MATH, V73, P615, DOI 10.2307/2372313; Lee NY, 2001, IEEE T IMAGE PROCESS, V10, P79, DOI 10.1109/83.892445; Louis AK, 1997, WAVELETS THEORY APPL; Mallat S., 1999, WAVELET TOUR SIGNAL; Meyer Y., 1992, WAVELETS OPERATORS; NOWAK R, P 35 AS C SIGN SYST, V1, P371; Sardy S, 2000, J COMPUT GRAPH STAT, V9, P361, DOI 10.2307/1390659; Starck JL, 2003, SIGNAL PROCESS, V83, P2279, DOI 10.1016/S0165-1684(03)00150-6; Starck JL, 2003, ASTRON ASTROPHYS, V398, P785, DOI 10.1051/0004-6361:20021571	40	1113	1193	16	80	JOHN WILEY & SONS INC	HOBOKEN	111 RIVER ST, HOBOKEN, NJ 07030 USA	0010-3640			COMMUN PUR APPL MATH	Commun. Pure Appl. Math.	NOV	2004	57	11					1413	1457		10.1002/cpa.20042		45	Mathematics, Applied; Mathematics	Mathematics	857VF	WOS:000224146400001		
J	Fang, KT; Yin, H; Liang, YZ				Fang, KT; Yin, H; Liang, YZ			New approach by Kriging models to problems in QSAR	JOURNAL OF CHEMICAL INFORMATION AND COMPUTER SCIENCES			English	Article							PROJECTION PURSUIT; RETENTION INDEXES; REGRESSION	Most models in quantitative structure and activity relationship (QSAR) research, proposed by various techniques Such as ordinary least squares regression, principal components regression, partial least squares regression, and multivariate adaptive regression splines, involve a linear parametric part and a random error part. The random errors in those models are assumed to be independently identical distributed. However, the independence assumption is not reasonable in many cases. Some dependence among errors should be considered just like Kriging. It has been Successfully used ill computer experiments for modeling. The aim of this paper is to apply Kriging models to QSAR. Our experiments show that the Kriging models can significantly improve the performances of the models obtained by many existing methods.	Hong Kong Baptist Univ, Dept Math, Hong Kong, Hong Kong, Peoples R China; Wuhan Univ, Coll Math & Stat, Wuhan 430072, Peoples R China; Cent S Univ, Coll Chem & Chem Engn, Changsha 410083, Peoples R China	Fang, KT (reprint author), Hong Kong Baptist Univ, Dept Math, Hong Kong, Hong Kong, Peoples R China.	ktfang@hkbu.edu.hk	Fang, Kai Tai/B-7196-2009; HKBU, Mathematics/B-5086-2009				Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; MASSY WF, 1965, J AM STAT ASSOC, V60, P234, DOI 10.2307/2283149; Cressie N., 1993, STAT SPATIAL DATA; Daubechies I., 1992, 10 LECT WAVELETS; Du YP, 2002, J CHEM INF COMP SCI, V42, P1283, DOI 10.1021/ci020285u; Friedman J, 1999, STOCHASTIC GRADIENT; FRIEDMAN JH, 1991, ANN STAT, V19, P1, DOI 10.1214/aos/1176347963; Friedman J. H., 2001, ANN STAT, V29; Green P, 1994, NONPARAMETRIC REGRES; He P., 2003, J DATA SCI, V1, P425; LI KC, 1991, J AM STAT ASSOC, V86, P316, DOI 10.2307/2290563; Lophaven S., 2002, IMMREP200213; Rivoirard J., 1994, INTRO DISJUNCTIVE KR; ROHRBAUGH RH, 1985, ANAL CHEM, V57, P2770, DOI 10.1021/ac00291a008; Rucker G, 1999, J CHEM INF COMP SCI, V39, P788, DOI 10.1021/ci9900175; Sacks J., 1989, STAT SCI, V4, P409, DOI DOI 10.1214/SS/1177012413; FRIEDMAN JH, 1974, IEEE T COMPUT, VC 23, P881, DOI 10.1109/T-C.1974.224051; Wahba G., 1990, SPLINE MODELS OBSERV; Wold H., 1975, PERSPECTIVES PROBABI, P117	19	12	15	1	4	AMER CHEMICAL SOC	WASHINGTON	1155 16TH ST, NW, WASHINGTON, DC 20036 USA	0095-2338			J CHEM INF COMP SCI	J. Chem. Inf. Comput. Sci.	NOV-DEC	2004	44	6					2106	2113		10.1021/ci049798m		8	Chemistry, Multidisciplinary; Computer Science, Information Systems; Computer Science, Interdisciplinary Applications	Chemistry; Computer Science	875HQ	WOS:000225411300025	15554681	
J	Tibshirani, R; Hastie, T; Narasimhan, B; Soltys, S; Shi, GY; Koong, A; Le, QT				Tibshirani, R; Hastie, T; Narasimhan, B; Soltys, S; Shi, GY; Koong, A; Le, QT			Sample classification from protein mass spectrometry, by 'peak probability contrasts'	BIOINFORMATICS			English	Article							OVARIAN-CANCER; IDENTIFICATION; MICROARRAYS; DISCOVERY; BIOMARKERS; PROTEOMICS; SERUM	Motivation: Early cancer detection has always been a major research focus in solid tumor oncology. Early tumor detection can theoretically result in lower stage tumors, more treatable diseases and ultimately higher cure rates with less treatment-related morbidities. Protein mass spectrometry is a potentially powerful tool for early cancer detection. We propose a novel method for sample classification from protein mass spectrometry data. When applied to spectra from both diseased and healthy patients, the 'peak probability contrast' technique provides a list of all common peaks among the spectra, their statistical significance and their relative importance in discriminating between the two groups. We illustrate the method on matrix-assisted laser desorption and ionization mass spectrometry data from a study of ovarian cancers. Results: Compared to other statistical approaches for class prediction, the peak probability contrast method performs as well or better than several methods that require the full spectra, rather than just labelled peaks. It is also much more interpretable biologically. The peak probability contrast method is a potentially useful tool for sample classification from protein mass spectrometry data.	Stanford Univ, Dept Hlth Res & Policy, Stanford, CA 94305 USA; Stanford Univ, Dept Stat, Stanford, CA 94305 USA; Stanford Univ, Dept Radiat Oncol, Stanford, CA 94305 USA	Tibshirani, R (reprint author), Stanford Univ, Dept Hlth Res & Policy, Stanford, CA 94305 USA.	tibs@stanford.edu					ADAM B.L, 2003, CANCER RES, V63, P3609; Petricoin EF, 2002, LANCET, V359, P572, DOI 10.1016/S0140-6736(02)07746-2; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Swensen SJ, 2002, AM J RESP CRIT CARE, V165, P508, DOI 10.1164/rccm.2107006; Efron B, 2002, GENET EPIDEMIOL, V23, P70, DOI 10.1002/gepi.01124; Tusher VG, 2001, P NATL ACAD SCI USA, V98, P5116, DOI 10.1073/pnas.091062498; Petricoin EF, 2002, NAT REV DRUG DISCOV, V1, P683, DOI 10.1038/nrd891; Smith RA, 2003, CA-CANCER J CLIN, V53, P27; Hanash S, 2003, NATURE, V422, P226, DOI 10.1038/nature01514; Tibshirani R, 2003, STAT SCI, V18, P104, DOI 10.1214/ss/1056397488; Storey JD, 2003, P NATL ACAD SCI USA, V100, P9440, DOI 10.1073/pnas.1530509100; Klade CS, 2001, PROTEOMICS, V1, P890, DOI 10.1002/1615-9861(200107)1:7<890::AID-PROT890>3.0.CO;2-Z; BENJAMINI Y, 1985, J ROY STAT SOC B, V85, P289; Breiman L., 1984, CLASSIFICATION REGRE; EFRON B, 2002, LEAST ANGLE REGRESSI; Hansen M, 1986, CHEST, V89, p219S; HUTCHENS TW, 1993, RAPID COMMUN MASS SP, V7, P576, DOI 10.1002/rcm.1290070703; Kozak KR, 2003, P NATL ACAD SCI USA, V100, P12343, DOI 10.1073/pnas.2033602100; LI J, 2002, CLIN CHEM, V48, P296; Merchant M, 2000, ELECTROPHORESIS, V21, P1164, DOI 10.1002/(SICI)1522-2683(20000401)21:6<1164::AID-ELPS1164>3.3.CO;2-S; NASON GP, 1998, WAVETHRESH3 SOFTWARE; Qu YS, 2002, CLIN CHEM, V48, P1835; Rai AJ, 2002, ARCH PATHOL LAB MED, V126, P1518; Wu BL, 2003, BIOINFORMATICS, V19, P1636, DOI 10.1093/bioinformatics/btg210; Yasui Y, 2003, BIOSTATISTICS, V4, P449, DOI 10.1093/biostatistics/4.3.449	25	123	126	1	4	OXFORD UNIV PRESS	OXFORD	GREAT CLARENDON ST, OXFORD OX2 6DP, ENGLAND	1367-4803			BIOINFORMATICS	Bioinformatics	NOV 22	2004	20	17					3034	3044		10.1093/bioinformatics/bth357		11	Biochemical Research Methods; Biotechnology & Applied Microbiology; Computer Science, Interdisciplinary Applications; Mathematical & Computational Biology; Statistics & Probability	Biochemistry & Molecular Biology; Biotechnology & Applied Microbiology; Computer Science; Mathematical & Computational Biology; Mathematics	874PD	WOS:000225361400017	15226172	
J	Van De Geer, SA; Van Houwelingen, HC				Van De Geer, SA; Van Houwelingen, HC			High-dimensional data: p >> n in mathematical statistics and bio-medical applications	BERNOULLI			English	Editorial Material							DISCOVERY		Leiden Univ, Math Inst, NL-2300 RC Leiden, Netherlands; Leiden Univ, Ctr Med, Dept Med Stat, NL-2300 RC Leiden, Netherlands	Van De Geer, SA (reprint author), Leiden Univ, Math Inst, POB 9512, NL-2300 RC Leiden, Netherlands.	geer@math.leidenuniv.nl; jcvanhouwelingen@lumc.nl	van houwelingen, hans/C-1872-2008				Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Golub TR, 1999, SCIENCE, V286, P531, DOI 10.1126/science.286.5439.531; BENJAMINI Y, 1995, J ROY STAT SOC B MET, V57, P289; DONOHO DL, 1995, IEEE T INFORM THEORY, V41, P613, DOI 10.1109/18.382009	4	1	1	1	4	INT STATISTICAL INST	VOORBURG	428 PRINSES BEATRIXLAAN, 2270 AZ VOORBURG, NETHERLANDS	1350-7265			BERNOULLI	Bernoulli	DEC	2004	10	6					939	942		10.3150/bj/1106314843		4	Statistics & Probability	Mathematics	886BU	WOS:000226202300001		
J	Greenshtein, E; Ritov, Y				Greenshtein, E; Ritov, Y			Persistence in high-dimensional linear predictor selection and the virtue of overparametrization	BERNOULLI			English	Article; Proceedings Paper	Workshop on High-Dimensional Data	SEP 09-20, 2002	Leiden, NETHERLANDS			consistency; lasso; regression; variable selection	REGRESSION; SHRINKAGE	Let Z(i) = (Y-i, X-i... X-m(i)), i = 1..., n, be independent and identically distributed random vectors. Z(i) similar to F, F is an element of F. It is desired to predict Y by Sigmabeta(j)X(j), where (beta(1),.... beta(m)) is an element of B-n subset of or equal to R-m, under a prediction loss. Suppose that m = n(a), a > 1, that is, there are many more explanatory variables than observations. We consider sets B-n restricted by the maximal number of non-zero coefficients of their members, or by their 11 radius. We study the following asymptotic question: how 'large' may the set B-n be, so that it is still possible to select empirically a predictor whose risk under F is close to that of the best predictor in the set? Sharp bounds for orders of magnitudes are given under various assumptions on F. Algorithmic complexity of the ensuing procedures is also studied. The main message of this paper and the implications of the orders derived are that under various sparsity assumptions on the optimal predictor there is 'asymptotically no harm' in introducing many more explanatory variables than observations. Furthermore, such practice can be beneficial in comparison with a procedure that screens in advance a small subset of explanatory variables. Another main result is that 'lasso' procedures, that is. optimization under 11 constraints, could be efficient in finding optimal sparse predictors in high dimensions.	Univ Haifa, Dept Stat, IL-31905 Haifa, Israel; Hebrew Univ Jerusalem, Dept Stat, IL-91905 Jerusalem, Israel	Greenshtein, E (reprint author), Univ Haifa, Dept Stat, Mt Carmel, IL-31905 Haifa, Israel.	geitan@stat.haifa.ac.il; yaacov.ritov@huji.ac.il					HUBER PJ, 1973, ANN STAT, V1, P799, DOI 10.1214/aos/1176342503; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Chen SSB, 2001, SIAM REV, V43, P129, DOI 10.1137/S003614450037906X; Bickel PJ, 2004, BERNOULLI, V10, P989, DOI 10.3150/bj/1106314847; Breiman L, 2001, STAT SCI, V16, P199, DOI 10.1214/ss/1009213726; Efron B, 2004, ANN STAT, V32, P407; DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425; Billingsley P., 1995, PROBABILITY MEASURE; BREIMAN L, 1983, J AM STAT ASSOC, V78, P131, DOI 10.2307/2287119; Emery M., 2000, LECT NOTES MATH, V1738; FOSTER DP, 1994, ANN STAT, V22, P1947, DOI 10.1214/aos/1176325766; Juditsky A, 2000, ANN STAT, V28, P681; Le Cam L., 1990, ASYMPTOTICS STAT; Lee WS, 1996, IEEE T INFORM THEORY, V42, P2118; Nemirovski A., 1983, PROBLEM COMPLEXITY M; PORTNOY S, 1984, ANN STAT, V12, P1298, DOI 10.1214/aos/1176346793; SILVERSTEIN JW, 1985, ANN PROBAB, V13, P1364, DOI 10.1214/aop/1176992819	17	108	109	2	7	INT STATISTICAL INST	VOORBURG	428 PRINSES BEATRIXLAAN, 2270 AZ VOORBURG, NETHERLANDS	1350-7265			BERNOULLI	Bernoulli	DEC	2004	10	6					971	988		10.3150/bj/1106314846		18	Statistics & Probability	Mathematics	886BU	WOS:000226202300004		
J	Huang, J; Harrington, D				Huang, J; Harrington, D			Dimension reduction in the linear model for right-censored data: Predicting the change of HIV-1 RNA levels using clinical and protease gene mutation data	LIFETIME DATA ANALYSIS			English	Article						failure time model; cross-validation; dimension reduction; partial least squares	PARTIAL LEAST-SQUARES; PRINCIPAL COMPONENTS REGRESSION; RESISTANCE; INHIBITOR; SAQUINAVIR; SHRINKAGE; INDINAVIR; SELECTION	With rapid development in the technology of measuring disease characteristics at molecular or genetic level, it is possible to collect a large amount of data on various potential predictors of the clinical outcome of interest in medical research. It is often of interest to effectively use the information on a large number of predictors to make prediction of the interested outcome. Various statistical tools were developed to overcome the difficulties caused by the high-dimensionality of the covariate space in the setting of a linear regression model. This paper focuses on the situation, where the interested outcomes are subjected to right censoring. We implemented the extended partial least squares method along With other commonly used approaches for analyzing the high-dimensional covariates to the ACTG333 data set. Especially, we compared the prediction performance of different approaches with extensive cross-validation studies. The results show that the Buckley-James based partial least squares, stepwise subset model selection and principal components regression have similar promising predictive power and the partial least square method has several advantages in terms of interpretability and numerical computation.	Northwestern Univ, Feinberg Sch Med, Dept Prevent Med, Chicago, IL 60611 USA; Harvard Univ, Sch Publ Hlth, Dept Biostat, Boston, MA 02115 USA; Dana Farber Canc Inst, Dept Biostat Sci, Boston, MA 02115 USA	Huang, J (reprint author), Northwestern Univ, Feinberg Sch Med, Dept Prevent Med, 680 N Lake Shore,Suite 1102, Chicago, IL 60611 USA.	jjhuang@northwestern.edu					WOLD S, 1984, SIAM J SCI STAT COMP, V5, P735, DOI 10.1137/0905052; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; HELLAND IS, 1988, COMMUN STAT SIMULAT, V17, P581, DOI 10.1080/03610918808812681; Wentzell PD, 2003, CHEMOMETR INTELL LAB, V65, P257, DOI 10.1016/S0169-7439(02)00138-7; BUCKLEY J, 1979, BIOMETRIKA, V66, P429; COLLIER A, 1996, NEW ENGL J MED, V16, P1011; Condra JH, 1996, J VIROL, V70, P8270; CONDRA JH, 1995, NATURE, V374, P569, DOI 10.1038/374569a0; COX DR, 1972, J R STAT SOC B, V34, P187; Draper N, 1981, APPL REGRESSION ANAL; Goutis C, 1996, ANN STAT, V24, P816; HOCKING RR, 1976, BIOMETRICS, V32, P1, DOI 10.2307/2529336; HOTELLING H, 1933, J EDUC PSYCHOL, V24, P489; HUANG J, 2005, BIOMETRICS; Hughes JP, 1999, BIOMETRICS, V55, P625, DOI 10.1111/j.0006-341X.1999.00625.x; Jacobsen H, 1996, J INFECT DIS, V173, P1379; Jacqmin-Gadda H, 2000, Biostatistics, V1, P355, DOI 10.1093/biostatistics/1.4.355; Jin ZZ, 2003, BIOMETRIKA, V90, P341, DOI 10.1093/biomet/90.2.341; Jolliffe I. T., 1986, PRINCIPAL COMPONENT; LAIRD NM, 1982, BIOMETRICS, V38, P963, DOI 10.2307/2529876; Marschner IC, 1999, J ACQ IMMUN DEF SYND, V20, P220; Miller A. J., 1990, SUBSET SELECTION REG; Nguyen DV, 2002, BIOINFORMATICS, V18, P1625, DOI 10.1093/bioinformatics/18.12.1625; Para MF, 2000, J INFECT DIS, V182, P733, DOI 10.1086/315769; Park Peter J, 2002, Bioinformatics, V18 Suppl 1, pS120; STONE M, 1990, J ROY STAT SOC B MET, V52, P237; TSIATIS AA, 1990, ANN STAT, V18, P354, DOI 10.1214/aos/1176347504; Vaillancourt M, 1999, AIDS RES HUM RETROV, V15, P355, DOI 10.1089/088922299311321; WOLD H, 1966, RES PAPERS STAT, P411; WOLD H, 1976, PERSPECTIVES PROBABI, P117	30	0	0	1	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	1380-7870			LIFETIME DATA ANAL	Lifetime Data Anal.	DEC	2004	10	4					425	443		10.1007/s10985-004-4776-8		19	Mathematics, Interdisciplinary Applications; Statistics & Probability	Mathematics	890OB	WOS:000226519600008	15690994	
S	Sanyal, S; Kukreja, SL; Perreault, EJ; Westwick, DT			IEEE	Sanyal, Sreemoyi; Kukreja, Sunil L.; Perreault, Eric J.; Westwick, David T.			Identification of linear time varying systems using basis pursuit	2005 27th Annual International Conference of the IEEE Engineering in Medicine and Biology Society, Vols 1-7	PROCEEDINGS OF ANNUAL INTERNATIONAL CONFERENCE OF THE IEEE ENGINEERING IN MEDICINE AND BIOLOGY SOCIETY		English	Proceedings Paper	27th Annual International Conference of the IEEE-Engineering-in-Medicine-and-Biology-Society	AUG 31-SEP 03, 2005	Shanghai, PEOPLES R CHINA	IEEE Engn Med & Biol Soc, Chinese Acad Engn Sci			ALGORITHM; SELECTION	System identification involves creating mathematical models of systems using measurements of their inputs and outputs. Linear time-varying systems form an important sub-class of models that require the use of specialized system identification techniques. One such approach involves expanding the time-varying parameters onto a set of temporal basis functions and then estimating the resulting expansion coefficients. This, however, requires the estimation of a large number of parameters and often results in extreme noise sensitivity. In this paper a novel algorithm for identifying time-varying systems is presented. It combines a temporal expansion with a term selection step that uses the "Least Absolute Shrinkage and Selection Operator", or Lasso. The Lasso term selection technique constructs a model structure with a nearly minimal number of non-zero terms, and hence with relatively low estimation variances. The algorithm is demonstrated by using it to detect changes in the dynamic stiffness of the human elbow immediately following the onset of a broadband perturbation.	Univ Calgary, Dept Elect & Comp Engn, Calgary, AB T2N 1N4, Canada	Sanyal, S (reprint author), Univ Calgary, Dept Elect & Comp Engn, 2500 Univ Dr NW, Calgary, AB T2N 1N4, Canada.		Perreault, Eric/B-7632-2009				Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; SHAO J, 1993, J AM STAT ASSOC, V88, P486, DOI 10.2307/2290328; GRENIER Y, 1983, IEEE T ACOUST SPEECH, V31, P899, DOI 10.1109/TASSP.1983.1164152; HUNTER IW, 1983, MED BIOL ENG COMPUT, V21, P203, DOI 10.1007/BF02441539; KEARNEY RE, 1990, CRIT REV BIOMED ENG, V18, P55; KORENBERG MJ, 1989, BIOL CYBERN, V60, P267, DOI 10.1007/BF00204124; KUKREJA S, 2005, IEEE INT C DEC CONTR; Kukreja SL, 2004, INT J CONTROL, V77, P132, DOI 10.1080/00207170310001646264; MACNEIL JB, 1992, IEEE T BIO-MED ENG, V39, P1213, DOI 10.1109/10.184697; Mao KZ, 1997, INT J CONTROL, V68, P311, DOI 10.1080/002071797223631; McGaughey DR, 2003, ANN BIOMED ENG, V31, P741, DOI 10.1114/1.1574024; Zou R, 2003, ANN BIOMED ENG, V31, P840, DOI 10.1114/1.1584683	13	4	4	0	1	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA	1094-687X		0-7803-8740-6	P ANN INT IEEE EMBS			2005							22	25		10.1109/IEMBS.2005.1616332		4	Engineering, Biomedical	Engineering	BER00	WOS:000238998400006		
S	Kim, SP; Carmena, JM; Nicolelis, MA; Principe, JC			IEEE	Kim, SP; Carmena, JM; Nicolelis, MA; Principe, JC			Multiresolution representations and data mining of neural spikes for brain-machine interfaces	2005 2ND INTERNATINOAL IEEE/EMBS CONFERENCE ON NEURAL ENGINEERING	International IEEE EMBS Conference on Neural Engineering		English	Proceedings Paper	2nd International IEEE/EMBS Conference on Neural Engineering	MAR 16-20, 2005	Arlington, VA	IEEE, EMBS, Natl Sci Fdn, Inst Phys, Off Naval Res Global			ALGORITHMS; REGRESSION; PRIMATES	In brain-machine interface (BMI) applications, neural firing activities have been represented by spike counts with a fixed-width rime bin. Adaptive models have been designed to utilize these bin counts for mapping the associated behavior which is typically 2D or 3D hand movement. However, the representation of the firing activities can be enriched by binning neural spikes with multiple time scales based on multiresolution analysis. This multiresolution representation of neural activities can provide more accurate prediction of the hand movement parameters. Data mining techniques must be applied to models using multiresolution representation in order to avoid overfitting. In this paper, we demonstrate that the multiresolution representation improves the performance of the linear model for BMIs compared to the model with the fixed-width time bin.	Univ Florida, Computat NeuroEngn Lab, Gainesville, FL 32611 USA	Kim, SP (reprint author), Univ Florida, Computat NeuroEngn Lab, Gainesville, FL 32611 USA.						Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Serruya MD, 2002, NATURE, V416, P141, DOI 10.1038/416141a; Carmena JM, 2003, PLOS BIOL, V1, P193, DOI 10.1371/journal.pbio.0000042; HOERL AE, 1970, TECHNOMETRICS, V12, P55; Wessberg J, 2000, NATURE, V408, P361; Bishop C. M., 1995, NEURAL NETWORKS PATT; EFRON B, IN PRESS ANN STAT; Figueiredo M., 2003, IEEE T PATTERN ANAL, V25; GAO Y, 2003, IEEE EMBS CNE; Haykin S., 1996, ADAPTIVE FILTER THEO; KIM SP, UNPUB COMPARISON OPT; KROGH A, 1992, ADV NEUR IN, V4, P950; Murtagh F, 2004, DECIS SUPPORT SYST, V37, P475, DOI 10.1016/S0167-9236(03)00092-7; Schwartz AB, 2001, CURR OPIN NEUROBIOL, V11, P701, DOI 10.1016/S0959-4388(01)00272-0; SHENSA MJ, 1992, IEEE T SIGNAL PROCES, V40, P2464, DOI 10.1109/78.157290	15	1	1	0	1	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA	1948-3546		0-7803-8709-0	I IEEE EMBS C NEUR E			2005							221	224				4	Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Engineering, Biomedical	Computer Science; Engineering	BCI92	WOS:000229610400061		
S	Pelckmans, K; Goethals, I; Suykens, JAK; De Moor, B			IEEE	Pelckmans, K.; Goethals, I.; Suykens, J. A. K.; De Moor, B.			On model complexity control in identification of Hammerstein systems	2005 44th IEEE Conference on Decision and Control & European Control Conference, Vols 1-8	IEEE CONFERENCE ON DECISION AND CONTROL - PROCEEDINGS		English	Proceedings Paper	44th IEEE Conference on Decision Control/European Control Conference (CCD-ECC)	DEC 12-15, 2005	Seville, SPAIN	IEEE Control Syst Soc, European Union Control Assoc, IFAC, INFORMS, SIAM, SICE, Honeywell, MathWorks		identification; Hammerstein systems; model complexity and regularization; kernel methods	SUBSPACE IDENTIFICATION; REGRESSION; ALGORITHM; REGULARIZATION	Model complexity control and regularization play a crucial role in statistical learning theory and also for problems in system identification. This text discusses the potential of the issue of regularization in identification of Hammerstein systems in the context of primal-dual kernel machines and Least Squares Support Vector Machines (LS-SVMs) and proposes an extension of the Hammerstein class to finite order Volterra series and methods resulting in structure detection.	Katholieke Univ Leuven, ESAT, SCD SISTA, B-3001 Louvain, Belgium	Pelckmans, K (reprint author), Katholieke Univ Leuven, ESAT, SCD SISTA, Kasteelpk Arenberg 10, B-3001 Louvain, Belgium.		Pelckmans, Kristiaan/A-3118-2013; Suykens, Johan/C-9781-2014				FRIEDMAN JH, 1981, J AM STAT ASSOC, V76, P817, DOI 10.2307/2287576; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Antoniadis A, 2001, J AM STAT ASSOC, V96, P939, DOI 10.1198/016214501753208942; HOERL AE, 1970, TECHNOMETRICS, V12, P55; MALLOWS CL, 1973, TECHNOMETRICS, V15, P661, DOI 10.2307/1267380; Bai EW, 1998, AUTOMATICA, V34, P333, DOI 10.1016/S0005-1098(97)00198-2; Boyd S, 1984, IMA J MATH CONTROL I, V1, P243, DOI 10.1093/imamci/1.3.243; DAVIDSON TN, 2000, IEEE T SIGNAL PROCES, V50, P2702; GOETHALS I, 2004, SUBSPACE IDENTIFICAT; GOETHALS I, 2005, IN PRESS AUTOMATICA; Goethals I, 2003, IEEE T AUTOMAT CONTR, V48, P1843, DOI [10.1109/TAC.2003.817940, 10.1109/TAC.203.817940]; GREBLICKI W, 1986, IEEE T AUTOMAT CONTR, V31, P74, DOI 10.1109/TAC.1986.1104096; Hansen P., 1998, RANK DEFICIENT DISCR; Hastie T., 2001, ELEMENTS STAT LEARNI; Lang ZQ, 1997, IEEE T AUTOMAT CONTR, V42, P1435; LANG ZQ, 1993, AUTOMATICA, V29, P767; Ljung L., 1987, SYSTEM IDENTIFICATIO; Mari J, 2000, IEEE T SIGNAL PROCES, V48, P2092, DOI 10.1109/78.847793; PELCKMANS K, 2003, 03184 ESATSISTA KU L; PELCKMANS K, 2004, P 11 INT C NEUR INF; PELCKMANS K, 2005, IN PRESS NEUROCOMPUT; PELCKMANS K, 2004, IN PRESS SUPPORT VEC; STOICA P, 1981, IEEE T AUTOMAT CONTR, V26, P967, DOI 10.1109/TAC.1981.1102761; Suykens J. A. K., 2002, LEAST SQUARES SUPPOR; Tikhonov AN, 1977, SOLUTION ILL POSED P; Vandenberghe L., 2004, CONVEX OPTIMIZATION; Van Gestel T, 2001, IEEE T AUTOMAT CONTR, V46, P1416, DOI 10.1109/9.948469; Vapnik V., 1998, STAT LEARNING THEORY; Wahba G., 1990, SPLINE MODELS OBSERV	29	0	0	0	0	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA	0191-2216		0-7803-9567-0	IEEE DECIS CONTR P			2005							1203	1208				6	Automation & Control Systems	Automation & Control Systems	BFB21	WOS:000240653701033		
B	Goebel, K; Bonissone, P			IEEE	Goebel, K; Bonissone, P			Prognostic information fusion for constant load systems	2005 7th International Conference on Information Fusion (FUSION), Vols 1 and 2			English	Proceedings Paper	7th International Conference on Information Fusion (FUSION)	JUL 25-28, 2005	Philadelphia, PA	IEEE		prognostic fusion; prognostics; prognosis; decision fusion	REGRESSION; PREDICTION; MODEL	This paper describes a process for aggregating different information sources to estimate remaining equipment life. Specifically, the approach presents a rigorous chain of preprocessing, modeling and post processing steps that arrive at the desired prognostic result. The preprocessing steps deal with data reduction, filtering, and signature amplification. The prediction model applies Adaptive Neuro-Fuzzy Inference System (ANFIS) to the data. The post-processing steps include recursive trending which implicitly forces the prognostic trend to be confirmed before updated estimates are reported. Prognostic false positives and false negatives are introduced as innovative measures that help in assessing the performance of the approach. The method is illustrated using real-life data from industrial web paper breakage prediction.	GE Global Res, Niskayuna, NY 12309 USA	Goebel, K (reprint author), GE Global Res, K1-5C4A,1 Res Circle, Niskayuna, NY 12309 USA.						BABUSKA R, 1994, PROCEEDINGS OF THE THIRD IEEE CONFERENCE ON FUZZY SYSTEMS - IEEE WORLD CONGRESS ON COMPUTATIONAL INTELLIGENCE, VOLS I-III, P859, DOI 10.1109/FUZZY.1994.343848; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; MAMDANI EH, 1975, INT J MAN MACH STUD, V7, P1, DOI 10.1016/S0020-7373(75)80002-2; Efron B, 2004, ANN STAT, V32, P407; Bauer W, 2002, P SOC PHOTO-OPT INS, V4792, P193, DOI 10.1117/12.450354; BERSINI H, 1995, P INT C ART NEUR NET, V1, P169; Bonissone PP, 2002, PROC SPIE, V4787, P53, DOI 10.1117/12.455868; Frelicot C., 1996, RAIRO-APII-JESA Journal Europeen des Systemes Automatises, V30; FUH KH, 1995, INT J MACH TOOL MANU, V35, P1187; HASKELL R, 1999, P INT ICSC C COMP IN; JANG JSR, 1993, IEEE T SYST MAN CYB, V23, P665, DOI 10.1109/21.256541; Ojelund H, 2002, TECHNOMETRICS, V44, P369, DOI 10.1198/004017002188618563; Petit-Renaud S, 2004, INT J APPROX REASON, V35, P1, DOI 10.1016/S0888-613X(03)00056-2; Sfetsos A, 2004, ENG INTELL SYST ELEC, V12, P13; SINGH A, 1985, INT J REMOTE SENS, P989; TAKAGI T, 1985, IEEE T SYST MAN CYB, V15, P116; VACHTSEVANOS G, 2003, P AUTOTESTCON, P341, DOI 10.1109/AUTEST.2003.1243597; Wang P, 2001, AI EDAM, V15, P349, DOI 10.1017/S0890060401154089	18	0	0	0	0	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA			0-7803-9286-8				2005							1247	1255				9	Computer Science, Artificial Intelligence; Computer Science, Information Systems; Imaging Science & Photographic Technology	Computer Science; Imaging Science & Photographic Technology	BDP88	WOS:000234830400167		
B	Chi, HM; Ersoy, OK; Moskowitz, H			IEEE	Chi, Hoi-Ming; Ersoy, Okan K.; Moskowitz, Herbert			Feature selection using random probes and linear support vector machines	2005 ICSC CONGRESS ON COMPUTATIONAL INTELLIGENCE METHODS AND APPLICATIONS (CIMA 2005)			English	Proceedings Paper	ICSC Congress on Computational Intelligence Methods and Applications	DEC 15-17, 2005	Istanbul, TURKEY	ICSC			REGRESSION	A novel feature selection algorithm that combines the ideas of linear support vector machines (SVMs) and random probes is proposed. A random probe is first artificially generated from a Gaussian distribution and appended to the data set as an extra input variable. Next, a standard 2-norm or 1-norm linear support vector machine is trained using this new data set. Each coefficient, or weight, in a linear SVM is compared to that of the random probe feature. Under several statistical assumptions, the probability of each input feature being more relevant than the random probe can be computed easily. The proposed feature selection method is intuitive to use in real-world problems, and it automatically determines the optimal number of features needed. It can also be extended to selecting significant interaction and/or quadratic terms in a 2nd-order polynomial representation.	Purdue Univ, Sch Elect & Comp Engn, W Lafayette, IN 47907 USA	Chi, HM (reprint author), Purdue Univ, Sch Elect & Comp Engn, Elect Engn Bldg,465 Northwestern Ave, W Lafayette, IN 47907 USA.						Amiri-Simkooei A, 2003, J SURV ENG-ASCE, V129, P37, DOI 10.1061/(ASCE)0733-9453(2003)129:1(37); Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Bradley P. S., 1998, P 15 INT C MACH LEAR, P82; BRANK J, 2002, MSRTR200263 MICR COR; CHI HM, UNPUB EUROPEAN J OPE; Fletcher R., 1987, PRACTICAL METHODS OP; Bi J., 2003, Journal of Machine Learning Research, V3, DOI 10.1162/153244303322753643; Muller Klaus-Robert, 1997, ICANN 97 P 7 INT C A, P999; Scholkopf B., 1998, NEUROCOLT2 TECHNICAL; Stoppiglia H., 2003, Journal of Machine Learning Research, V3, DOI 10.1162/153244303322753733; Trafalis TB, 2000, IEEE IJCNN, P348; Vapnik V., 1997, NEURAL INFORM PROCES, V9; Vapnik V.N., 1995, NATURE STAT LEARNING; WANG S, 2001, INT C COMP VIS ICCV, V2, P209; ZHU J, 2003, 1 NORM SUPPORT VECTO	15	0	0	0	1	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA			1-4244-0020-1				2005							111	115				5	Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications	Computer Science	BEX04	WOS:000239918800023		
S	Veenman, CJ; Tax, DMJ		Schmid, C; Soatto, S; Tomasi, C		Veenman, CJ; Tax, DMJ			Weighted nearest mean classifier for sparse subspaces	2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, Vol 2, Proceedings	PROCEEDINGS - IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION		English	Proceedings Paper	Conference on Computer Vision and Pattern Recognition	JUN 20-25, 2005	San Diego, CA	IEEE Comp Soc		classification; high dimensional; feature subset selection; mathematical programming	GENE-EXPRESSION; FEATURE-SELECTION; ALGORITHMS; CANCER; PREDICTION; LASSO	In this paper we focus on high dimensional data sets for which the number of dimensions is an order of magnitude higher than the number of objects. From a classifier design standpoint, such small sample size problems have some interesting challenges. First, in any subspace with as many dimensions as objects the data set can be separated with an almost arbitrary linear hyperplane. Second, another important issue is to determine which features are responsible for the phenomenon under consideration. This problem comes down to finding as few features as possible that still can discriminate the classes involved. To attack these problems, we propose the LESS (Lowest Error in a Sparse Subspace) classifier. The LESS classifier is a weighted nearest mean classifier that efficiently finds linear discriminants in sparse subspaces, where the subspace is found automatically. In the experiments we compare LESS to related state-of-the-art classifiers like among others linear ridge regression with the LASSO and the Support Vector Machine. It turns out that LESS performs competitively while it uses the fewest features.	Delft Univ Technol, Man Machine Interact Grp, Dept Mediamat, NL-2600 GA Delft, Netherlands	Veenman, CJ (reprint author), Delft Univ Technol, Man Machine Interact Grp, Dept Mediamat, POB 5031, NL-2600 GA Delft, Netherlands.						Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Atkeson CG, 1997, ARTIF INTELL REV, V11, P11, DOI 10.1023/A:1006559212014; Alon U, 1999, P NATL ACAD SCI USA, V96, P6745, DOI 10.1073/pnas.96.12.6745; Ambroise C, 2002, P NATL ACAD SCI USA, V99, P6562, DOI 10.1073/pnas.102102699; Golub TR, 1999, SCIENCE, V286, P531, DOI 10.1126/science.286.5439.531; Burges CJC, 1998, DATA MIN KNOWL DISC, V2, P121, DOI 10.1023/A:1009715923555; SIEDLECKI W, 1989, PATTERN RECOGN LETT, V10, P335, DOI 10.1016/0167-8655(89)90037-8; Tibshirani R, 2002, P NATL ACAD SCI USA, V99, P6567, DOI 10.1073/pnas.082099299; Kohavi R, 1997, ARTIF INTELL, V97, P273, DOI 10.1016/S0004-3702(97)00043-X; Bhattacharyya C, 2003, SIGNAL PROCESS, V83, P729, DOI 10.1016/S0165-1684(02)00474-7; Blake C.L., 1998, UCI REPOSITORY MACHI; Debuse J. C. W., 1997, Journal of Intelligent Information Systems: Integrating Artificial Intelligence and Database Technologies, V9, DOI 10.1023/A:1008641220268; Deutsch JM, 2003, BIOINFORMATICS, V19, P45, DOI 10.1093/bioinformatics/19.1.45; *FREE SOFTW FDN, GNU LIN PROGR KIT; Fung G., 2000, P 6 ACM SIGKDD INT C; GORMAN RP, 1988, NEURAL NETWORKS, V1, P75, DOI 10.1016/0893-6080(88)90023-8; Karzynski M, 2003, ARTIF INTELL REV, V20, P39, DOI 10.1023/A:1026032530166; Koutroumbas K., 1999, PATTERN RECOGNITION; Martin-Bautista M. J., 1999, Proceedings of the 1999 Congress on Evolutionary Computation-CEC99 (Cat. No. 99TH8406), DOI 10.1109/CEC.1999.782599; Osborne MR, 2000, J COMPUT GRAPH STAT, V9, P319, DOI 10.2307/1390657; Raymer ML, 2000, IEEE T EVOLUT COMPUT, V4, P164, DOI 10.1109/4235.850656; SCHITTKOWSKI K, 1986, QLD FORTR COD QUADR; SIGILLITO VG, 1989, J HOPKINS APL TECH D, V10, P262; van't Veer LJ, 2002, NATURE, V415, P530, DOI 10.1038/415530a; Vapnik V., 1998, STAT LEARNING THEORY; VEENMAN CJ, IEEE T PATT AN MACH; Zhang HB, 2002, PATTERN RECOGN, V35, P701, DOI 10.1016/S0031-3203(01)00046-2; ZHU J, 2003, ADV NEURAL INFORMATI, V16	28	3	3	0	10	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA	1063-6919		0-7695-2372-2	PROC CVPR IEEE			2005							1171	1176				6	Computer Science, Artificial Intelligence; Imaging Science & Photographic Technology	Computer Science; Imaging Science & Photographic Technology	BCR47	WOS:000230925500161		
S	Zhang, N; Zeng, SQ; Weng, JY			IEEE	Zhang, N; Zeng, SQ; Weng, JY			Gradient sparse optimization via competitive learning	2005 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOLS 1-5: SPEECH PROCESSING	International Conference on Acoustics Speech and Signal Processing ICASSP		English	Proceedings Paper	30th IEEE International Conference on Acoustics, Speech, and Signal Processing	MAR 19-23, 2005	Philadelphia, PA	IEEE			REGRESSION	In this paper, we propose a new method to achieve sparseness via a competitive learning principle for the linear kernel regression and classification task. We form the duality of the LASSO criteria, and transfer an l(1) norm minimization to an l(infinity) norm maximization problem. We introduce a novel solution derived from gradient descending, which links the sparse representation and the competitive learning scheme. This framework is applicable to a variety of problems, such as regression, classification, feature selection, and data clustering.	Michigan State Univ, Dept Comp Sci & Engn, E Lansing, MI 48823 USA	Zhang, N (reprint author), Michigan State Univ, Dept Comp Sci & Engn, E Lansing, MI 48823 USA.	nanzhang@cse.msu.edu; zengshuq@cse.msu.edu; weng@cse.msu.edu					Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Tipping ME, 2001, J MACH LEARN RES, V1, P211, DOI 10.1162/15324430152748236; Amaldi E, 1998, THEOR COMPUT SCI, V209, P237, DOI 10.1016/S0304-3975(97)00115-1; Figueiredo MAT, 2003, IEEE T PATTERN ANAL, V25, P1150, DOI 10.1109/TPAMI.2003.1227989; HOERL AE, 1970, TECHNOMETRICS, V12, P55, DOI DOI 10.1080/00401706.1970.10488634; KOHONEN T, 2001, SELFORGANIZING MAPS, P60; LUENVERGER DG, 1969, OPTIMIZATION VECTOR; WENG J, 2004, P IEEE INT C AC SPEE	8	0	0	0	5	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA	1520-6149		0-7803-8874-7	INT CONF ACOUST SPEE			2005							645	648				4	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Computer Science; Engineering	BCI02	WOS:000229404203161		
S	Chodkiewicz, ML		Simos, T; Maroulis, G		Chodkiewicz, M. L.			Multicenter multipolar representation of electrostatic potential for flexible molecules	Advances in Computational Methods in Sciences and Engineering 2005, Vols 4 A & 4 B	LECTURE SERIES ON COMPUTER AND COMPUTATIONAL SCIENCES		English	Proceedings Paper	International Conference on Computational Methods in Sciences and Engineering (ICCMSE 2005)	OCT 21-26, 2005	Corinth, GREECE	Amer Chem Soc, Amer Phys Soc		electrostatic potential; distributed multipoles	REGRESSION; SELECTION	A method for generating a compact multicenter multipolar representation of the electrostatic potential (EP) for flexible molecules is presented. The fitting procedure used adopts the Least Absolute Shrinkage and Selection Operator (LASSO) technique (R. Tibshirani, J. Roy. Stat. Soc. B 58, 267 (1996)) which can be seen as penalized ordinary least squares (OLS). The constrains optimized for the particular molecule of interest effectively removes redundant multipoles. It is shown that the use of multiple conformations is crucial for the predictive ability of the EP model for flexible molecules. The multipole local coordinate systems are chosen in a way that best reflects the key conformational changes.	Univ Warsaw, Dept Chem, PL-02093 Warsaw, Poland	Chodkiewicz, ML (reprint author), Univ Warsaw, Dept Chem, Pasteura 1, PL-02093 Warsaw, Poland.						BREIMAN L, 1995, TECHNOMETRICS, V37, P373, DOI 10.2307/1269730; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; WILLIAMS DE, 1988, J COMPUT CHEM, V9, P745, DOI 10.1002/jcc.540090705; HOERL AE, 1970, TECHNOMETRICS, V12, P55; Myers R, 1990, CLASSICAL MODERN REG	6	0	0	0	0	VSP BV-C/O BRILL ACAD PUBL	LEIDEN	PO BOX 9000, 2300 PA LEIDEN, NETHERLANDS	1573-4196		90-6764-443-9	LECT SER COMPUTER CO			2005	4A-4B						945	948				4	Engineering, Multidisciplinary; Mathematics, Applied; Physics, Multidisciplinary; Physics, Mathematical	Engineering; Mathematics; Physics	BEL90	WOS:000238054400227		
S	Simila, T; Tikka, J		Duch, W; Kacprzyk, J; Oja, E; Zadrozny, S		Simila, T; Tikka, J			Multiresponse sparse regression with application to multidimensional scaling	ARTIFICIAL NEURAL NETWORKS: FORMAL MODELS AND THEIR APPLICATIONS - ICANN 2005, PT 2, PROCEEDINGS	LECTURE NOTES IN COMPUTER SCIENCE		English	Article; Proceedings Paper	15th International Conference on Artificial Neural Networks (ICANN 2005)	SEP 11-15, 2005	Warsaw, POLAND	European Neural Network Soc, Int Neural Network Soc, Japanese Neural Network Soc, IEEE Computat Intelligence Soc			PREDICTION; SHRINKAGE	Sparse regression is the problem of selecting a parsimonious subset of all available regressors for an efficient prediction of a target variable. We consider a general setting in which both the target and regressors may be multivariate. The regressors are selected by a forward selection procedure that extends the Least Angle Regression algorithm. Instead of the common practice of estimating each target variable individually, our proposed method chooses sequentially those regressors that allow, on average, the best predictions of all the target variables. We illustrate the procedure by an experiment with artificial data. The method is also applied to the task of selecting relevant pixels from images in multidimensional scaling of handwritten digits.	Helsinki Univ Technol, Lab Comp & Informat Sci, FI-02015 Helsinki, Finland	Simila, T (reprint author), Helsinki Univ Technol, Lab Comp & Informat Sci, POB 5400, FI-02015 Helsinki, Finland.	timo.simila@hut.fi; tikk@mail.cis.hut.fi	Kacprzyk, Janusz/M-9574-2014	Kacprzyk, Janusz/0000-0003-4187-5877			Abraham B, 2005, COMPUT STAT DATA AN, V48, P5, DOI 10.1016/j.csda.2003.11.021; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; HOERL AE, 1970, TECHNOMETRICS, V12, P55; Efron B, 2004, ANN STAT, V32, P407; COPAS JB, 1983, J R STAT SOC B, V45, P311; Breiman L, 1997, J ROY STAT SOC B MET, V59, P3, DOI 10.1111/1467-9868.00054; Burnham AJ, 1999, CHEMOMETR INTELL LAB, V48, P167, DOI 10.1016/S0169-7439(99)00018-0; Cox TF, 2001, MONOGRAPHS STAT APPL, V88; SAMMON JW, 1969, IEEE T COMPUT, VC 18, P401, DOI 10.1109/T-C.1969.222678; Tipping ME, 1998, NEUROCOMPUTING, V19, P211, DOI 10.1016/S0925-2312(97)00066-0; Zhang Z., 2003, INT C MACHINE LEARNI, P872	11	25	25	0	1	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		3-540-28755-8	LECT NOTES COMPUT SC			2005	3697						97	102				6	Computer Science, Artificial Intelligence; Computer Science, Theory & Methods	Computer Science	BCZ97	WOS:000232196000016		
S	Pelckmans, K; Suykens, JAK; De Moor, B		Duch, W; Kacprzyk, J; Oja, E; Zadrozny, S		Pelckmans, K; Suykens, JAK; De Moor, B			Componentwise support vector machines for structure detection	ARTIFICIAL NEURAL NETWORKS: FORMAL MODELS AND THEIR APPLICATIONS - ICANN 2005, PT 2, PROCEEDINGS	LECTURE NOTES IN COMPUTER SCIENCE		English	Article; Proceedings Paper	15th International Conference on Artificial Neural Networks (ICANN 2005)	SEP 11-15, 2005	Warsaw, POLAND	European Neural Network Soc, Int Neural Network Soc, Japanese Neural Network Soc, IEEE Computat Intelligence Soc			LASSO	This paper extends recent advances in Support Vector Machines and kernel machines in estimating additive models for classification from observed multivariate input/output data. Specifically, we address the question how to obtain predictive models which gives insight into the structure of the dataset. This contribution extends the framework of structure detection as introduced in recent publications by the authors towards estimation of component-wise Support Vector Machines (cSVMs). The result is applied to a benchmark classification task where the input variables all take binary values.	Katholieke Univ Leuven, ESAT, SCD, SISTA, B-3001 Louvain, Belgium	Pelckmans, K (reprint author), Katholieke Univ Leuven, ESAT, SCD, SISTA, Kasteelpk Arenberg 10, B-3001 Louvain, Belgium.	kristiaan.pelckmans@esat.kuleuven.ac.be; johan.suykens@esat.kuleuven.ac.be	Pelckmans, Kristiaan/A-3118-2013; Suykens, Johan/C-9781-2014				Fu WJJ, 1998, J COMPUT GRAPH STAT, V7, P397, DOI 10.2307/1390712; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Antoniadis A, 2001, J AM STAT ASSOC, V96, P939, DOI 10.1198/016214501753208942; FRANK IE, 1993, TECHNOMETRICS, V35, P109, DOI 10.2307/1269656; Suykens JAK, 1999, NEURAL PROCESS LETT, V9, P293, DOI 10.1023/A:1018628609742; CRISTANINI N, 2000, INTRO SUPPORT VECTOR; Gunn SR, 2002, MACH LEARN, V48, P137, DOI 10.1023/A:1013903804720; Hastie TJ, 1990, GEN ADDITIVE MODELS; PELCKMANS K, 2005, MAXIMAL VARIATION MI; PELCKMANS K, 2005, IN PRESS NEUROCOMPUT; PELCKMANS K, 2004, IN PRESS SUPPORT VEC; Schlimmer J.C., 1987, THESIS U CALIFORNIA; Scholkopf B., 2002, LEARNING KERNELS; Suykens J. A. K., 2002, LEAST SQUARES SUPPOR; Vandenberghe L., 2004, CONVEX OPTIMIZATION; Vapnik V., 1998, STAT LEARNING THEORY	16	1	1	0	0	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		3-540-28755-8	LECT NOTES COMPUT SC			2005	3697						643	648				6	Computer Science, Artificial Intelligence; Computer Science, Theory & Methods	Computer Science	BCZ97	WOS:000232196000102		
S	Madigan, D; Genkin, A; Lewis, DD; Fradkin, D		Knuth, KH; Abbas, AE; Morris, RD; Castle, JP		Madigan, D; Genkin, A; Lewis, DD; Fradkin, D			Bayesian multinomial logistic regression for author identification	Bayesian Inference and Maximum Entropy Methods in Science and Engineering	AIP CONFERENCE PROCEEDINGS		English	Proceedings Paper	25th International Workshop on Bayesian Inference and Maximum Entropy Methods in Science and Engineering	AUG 07-12, 2005	San Jose, CA	ET Jaynes Fdn, San Jose State Univ, NASA Ames Res Ctr, Int Soc Bayesian Anal, World Year Phys 2005		multinomial logistic regression; polytomous logistic regression; Bayesian estimation; classification; author identification; stylometry	SELECTION	Motivated by high-dimensional applications in authorship attribution, we describe a Bayesian multinomial logistic regression model together with an associated learning algorithm.	Rutgers State Univ, DIMACS, Piscataway, NJ 08855 USA	Madigan, D (reprint author), Rutgers State Univ, DIMACS, Piscataway, NJ 08855 USA.						Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Krishnapuram B, 2005, IEEE T PATTERN ANAL, V27, P957, DOI 10.1109/TPAMI.2005.127; Rifkin R, 2004, J MACH LEARN RES, V5, P101; Tipping ME, 2001, J MACH LEARN RES, V1, P211, DOI 10.1162/15324430152748236; Efron B, 2004, ANN STAT, V32, P407; Corney M., 2003, THESIS QUEENSLAND U; Figueiredo MAT, 2003, IEEE T PATTERN ANAL, V25, P1150, DOI 10.1109/TPAMI.2003.1227989; Genkin A., 2004, LARGE SCALE BAYESIAN; Goodman J., 2004, P HLT NAACL, P305; KOPPEL M, 2003, LIT LINGUITICS COMPU, V17; LI F, 2003, 20 INT C MACH LEARN, P472; Malouf R., 2002, P 6 C NAT LANG LEARN, P49; Poggio T, 1998, NEURAL COMPUT, V10, P1445, DOI 10.1162/089976698300017250; Sha F, 2003, SHALLOW PARSING COND; Shevade SK, 2003, BIOINFORMATICS, V19, P2246, DOI 10.1093/bioinformatics/btg308; Zhang J., 2003, P 26 ANN INT ACM SIG, P190; Zhang T, 2001, INFORM RETRIEVAL, V4, P5, DOI 10.1023/A:1011441423217	17	5	5	0	2	AMER INST PHYSICS	MELVILLE	2 HUNTINGTON QUADRANGLE, STE 1NO1, MELVILLE, NY 11747-4501 USA	0094-243X		0-7354-0292-2	AIP CONF PROC			2005	803						509	516				8	Physics, Mathematical; Statistics & Probability	Physics; Mathematics	BDP78	WOS:000234828300057		
J	Fu, WJJ				Fu, WJJ			A note on prediction error with collinearity	COMMUNICATIONS IN STATISTICS-THEORY AND METHODS			English	Article						collinearity; least-squares estimator; prediction error	VARIABLE SELECTION; NONORTHOGONAL PROBLEMS; RIDGE REGRESSION; LASSO	Least squares (LS) estimator is the best linear unbiased estimator for linear models. It is well known that LS performs poorly in estimation when collinearity is present among regressors. However, it is not fully understood and is even controversial whether LS performs well in prediction. To address this controversy, we study the mean and variance of the prediction squared error (PSE) of LS estimator, and conclude theoretically that although the mean PSE remains invariant regardless of the collinearity, the variance of PSE increases with the collinearity. Thus the prediction error is sensitive to the location in the feature space.	Texas A&M Univ, Dept Stat, College Stn, TX 77843 USA; Michigan State Univ, Dept Epidemiol, E Lansing, MI 48824 USA	Fu, WJJ (reprint author), Texas A&M Univ, Dept Stat, 447 Blocker Bldg, College Stn, TX 77843 USA.	wfu@stat.tamu.edu					Fu WJJ, 1998, J COMPUT GRAPH STAT, V7, P397, DOI 10.2307/1390712; Knight K, 2000, ANN STAT, V28, P1356; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; HOERL AE, 1970, TECHNOMETRICS, V12, P55; George EI, 2000, J AM STAT ASSOC, V95, P1304, DOI 10.2307/2669776; FRANK IE, 1993, TECHNOMETRICS, V35, P109, DOI 10.2307/1269656; Fan JQ, 2002, ANN STAT, V30, P74; Fu WJJ, 2000, COMMUN STAT-THEOR M, V29, P263, DOI 10.1080/03610920008832483; Fu WJJ, 2003, BIOMETRICS, V59, P126, DOI 10.1111/1541-0420.00015; Gruber M. H., 1990, REGRESSION ESTIMATOR; Hastie TJ, 1990, GEN ADDITIVE MODELS; HOERL AE, 1970, TECHNOMETRICS, V12, P69, DOI 10.2307/1267352; Li RZ, 2002, STAT PROBABIL LETT, V59, P135, DOI 10.1016/S0167-7152(02)00140-2; Miller A. J., 1990, SUBSET SELECTION REG; Sen A. K., 1990, REGRESSION ANAL THEO; Shao J., 1995, JACKKNIFE BOOTSTRAP	17	1	1	0	0	TAYLOR & FRANCIS INC	PHILADELPHIA	325 CHESTNUT ST, SUITE 800, PHILADELPHIA, PA 19106 USA	0361-0926			COMMUN STAT-THEOR M	Commun. Stat.-Theory Methods		2005	34	3					619	623		10.1081/STA-200052114		5	Statistics & Probability	Mathematics	915AH	WOS:000228269500011		
B	Rezek, I; Roberts, SJ; Siva, E; Conradt, R		Wani, MA; Milanova, M; Kurgan, L; Reformat, M; Hafeez, K		Rezek, I; Roberts, SJ; Siva, E; Conradt, R			Depth of anaesthesia assessment with generative polyspectral models	ICMLA 2005: Fourth International Conference on Machine Learning and Applications, Proceedings			English	Proceedings Paper	4th International Conference on Machine Learning and Applications	DEC 15-17, 2005	Los Angeles, CA	Calif State Univ, Bakersfield & Assoc Machine Learning & Applicat, IEEE SMC			REGRESSION	The application of anaesthetic agents is known to have significant effects on the EEG waveform. Information extraction now routinely goes beyond second order spectral analysis, as obtained via power spectral methods, and uses higher order spectral methods. In this paper we present a model which generalises the autoregressive class of polyspectral models by having a semi-parametric description of the residual probability density. We estimate the model in the Variational Bayesian framework and extract higher order spectral features. Testing their importance for depth of anaesthesia classification is done on three different EEG data sets collected under exposure to different agents. The results show that significant improvements can be made over standard methods of estimating higher order spectra. The results also indicate that in two out of three anaesthetic agents, better classification can be achieved with higher order spectral features.	Univ Oxford, Dept Engn Sci, Oxford OX1 3PJ, England	Rezek, I (reprint author), Univ Oxford, Dept Engn Sci, Oxford OX1 3PJ, England.						Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Barr G., 1999, BRIT J ANAESTH, V82, P8227; Godsill SJ, 1998, IEEE T SPEECH AUDI P, V6, P352, DOI 10.1109/89.701365; Hall JD, 1998, BRIT J ANAESTH, V80, P342; Huber P. J., 2003, ROBUST STAT; Jaakkola T., 2000, ADV MEAN FIELD METHO; Johansen JW, 2000, ANESTHESIOLOGY, V93, P1336, DOI 10.1097/00000542-200011000-00029; KUIZENGA K, 2003, BRIT J ANAESTH, V86, P354; Nabney Ian, 2002, NETLAB ALGORITHMS PA; Ord JK, 1994, KENDALLS ADV THEORY, V1; Papoulis A., 1991, PROBABILITY RANDOM V; RAGHUVEER MR, 1985, IEEE T ACOUST SPEECH, V33, P1213, DOI 10.1109/TASSP.1985.1164679; Ripley B. D., 2000, PATTERN RECOGNITION; Roberts SJ, 2002, IEEE T SIGNAL PROCES, V50, P2245, DOI 10.1109/TSP.2002.801921; WEST M, 1984, J ROY STAT SOC B MET, V46, P431	15	0	0	0	27	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA			0-7695-2495-8				2005							239	244				6	Computer Science, Artificial Intelligence	Computer Science	BDU59	WOS:000235473200035		
S	Gatnar, E		Baier, D; Wernecke, KD		Gatnar, E			Randomization in aggregated classification trees	INNOVATIONS IN CLASSIFICATION, DATA SCIENCE, AND INFORMATION SYSTEMS	STUDIES IN CLASSIFICATION, DATA ANALYSIS, AND KNOWLEDGE ORGANIZATION		English	Proceedings Paper	27th Annual Conference of the German-Classification-Society	MAR 12-14, 2003	Cottbus, GERMANY	German Classificat Soc, Brandenburg Univ Technol, Chair Marketing & Innovat Management, Holiday Inn Hotel, MTU, Maintenance Berlin-Brandenburg GmbH, Scicon Sci Consulting GmbH, Sparkasse Spree-Neibe, Synergy Microwave Europe GmbH & Co KG, Volkswagen AG, Wolfsburg, Producers Scottish Single Malt Whisky	Brandenburg Univ Technol			Tree-based models are popular and widely used because they are simple, flexible and powerful tools for classification. Unfortunately they are not stable classifiers. Significant improvement of model stability and prediction accuracy can be obtained by aggregation of multiple classification trees.-The reduction of classification error is a result of decreasing bias or/and variance of the committee of trees (called also an ensemble or a forest). In this paper we discuss and compare different methods for model aggregation. We also address the problem of finding minimal number of trees sufficient for the forest.	Katowice Univ Econ, Inst Stat, PL-40226 Katowice, Poland							AMIT Y, 2001, MULTIPLE RANDOMIZED; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324; Bauer E, 1999, MACH LEARN, V36, P105, DOI 10.1023/A:1007515423169; WOLPERT DH, 1992, NEURAL NETWORKS, V5, P241, DOI 10.1016/S0893-6080(05)80023-1; Breiman L, 1998, ANN STAT, V26, P801; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Dietterich TG, 2000, MACH LEARN, V40, P139, DOI 10.1023/A:1007607513941; Ho TK, 1998, IEEE T PATTERN ANAL, V20, P832; Blake C.L., 1998, UCI REPOSITORY MACHI; Breiman L., 1996, BIAS VARIANCE ARCING, V460; Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.1023/A:1018054314350; BREIMAN L, 1999, 547 U CAL DEP STAT B; BREIMAN L, 2002, WALD LECT MACHINE LE; Breiman L., 1984, CLASSIFICATION REGRE; CARTER C, 1987, IEEE EXPERT      FAL, P71; Dietterich T. G., 1995, MACHINE LEARNING BIA; Friedman J, 1999, STOCHASTIC GRADIENT; FRIEDMAN JH, 1996, BIAS VARIANCE O1LOSS; GATNAR E, 2002, CLASSIFICATION CLUST, P399; HASTIE T, 1991, SKRINKING TREES; JIANG W, 2000, 0005 IEEE T PATT AN; KOHAVI R, 1996, P 13 INT C MACH LEAR, P313; Latinne P., 2001, LNCS, V2096, P178; LUGOSI G, 2002, STAT STUDY REGLULARI; Quinlan J. R., 1993, C4 5 PROGR MACHINE L; Tukey J. W., 1977, EXPLORATORY DATA ANA	27	1	1	1	1	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	1431-8814		3-540-23221-4	ST CLASS DAT ANAL			2005							207	216		10.1007/3-540-26981-9_25		10	Computer Science, Artificial Intelligence; Computer Science, Information Systems; Economics; Medical Informatics; Statistics & Probability	Computer Science; Business & Economics; Medical Informatics; Mathematics	BBN27	WOS:000226261800025		
J	Tibshirani, R; Saunders, M; Rosset, S; Zhu, J; Knight, K				Tibshirani, R; Saunders, M; Rosset, S; Zhu, J; Knight, K			Sparsity and smoothness via the fused lasso	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES B-STATISTICAL METHODOLOGY			English	Article						fused lasso; gene expression; lasso; least squares regression; protein mass spectroscopy; sparse solutions; support vector classifier	GENE-EXPRESSION; CANCER; REGRESSION; SHRINKAGE	The lasso penalizes a least squares regression by the sum of the absolute values (L-1-norm) of the coefficients. The form of this penalty encourages sparse solutions (with many coefficients equal to 0). We propose the 'fused lasso', a generalization that is designed for problems with features that can be ordered in some meaningful way. The fused lasso penalizes the L-1-norm of both the coefficients and their successive differences. Thus it encourages sparsity of the coefficients and also sparsity of their differences-i.e. local constancy of the coefficient profile. The fused lasso is especially useful when the number of features p is much greater than N, the sample size. The technique is also extended to the 'hinge' loss function that underlies the support vector classifier. We illustrate the methods on examples from protein mass spectroscopy and gene expression data.	Stanford Univ, Dept Hlth Res & Policy, Stanford, CA 94305 USA; IBM Corp, Thomas J Watson Res Ctr, Yorktown Hts, NY 10598 USA; Univ Michigan, Ann Arbor, MI 48109 USA; Univ Toronto, Toronto, ON, Canada	Tibshirani, R (reprint author), Stanford Univ, Dept Hlth Res & Policy, HRP Redwood Bldg, Stanford, CA 94305 USA.	tibs@stat.stanford.edu					ADAM B.L, 2003, CANCER RES, V63, P3609; Knight K, 2000, ANN STAT, V28, P1356; Petricoin EF, 2002, LANCET, V359, P572, DOI 10.1016/S0140-6736(02)07746-2; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Chen SSB, 2001, SIAM REV, V43, P129, DOI 10.1137/S003614450037906X; Golub TR, 1999, SCIENCE, V286, P531, DOI 10.1126/science.286.5439.531; HOERL AE, 1970, TECHNOMETRICS, V12, P55; DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425; Tibshirani R, 2002, P NATL ACAD SCI USA, V99, P6567, DOI 10.1073/pnas.082099299; BOSER B, 1992, P COMPUTATIONAL LEAR, V2; EFRON B, 2002, LEAST ANGLE REGRESSI; GEYER CJ, 1996, ASYMPTOTICS CONVEX S; Gill P. E., 1997, 974 NA U CAL; Hastie T., 2001, ELEMENTS STAT LEARNI; Land S. R., 1996, VARIABLE FUSION NEW; ROSSET S, 2003, ADAPTABLE EFFICIENT; Rosset S, 2004, J MACH LEARN RES, V5, P941; STEIN C, 1981, ANN STAT, V9, P1131; TIBSHIRANI R, 2004, SPARSITY SMOOTHNESS; Vapnik V.N., 1996, NATURE STAT LEARNING; Wold H., 1975, PERSPECTIVES PROBABI, P117; Yoonkyung Lee, 2002, MULTICATEGORY SUPPOR; ZHU J, 2003, L1 NORM SUPPORT VECT	23	348	355	7	23	BLACKWELL PUBL LTD	OXFORD	108 COWLEY RD, OXFORD OX4 1JF, OXON, ENGLAND	1369-7412			J ROY STAT SOC B	J. R. Stat. Soc. Ser. B-Stat. Methodol.		2005	67		1				91	108		10.1111/j.1467-9868.2005.00490.x		18	Statistics & Probability	Mathematics	878ZT	WOS:000225686900006		
J	Li, LX; Cook, RD; Nachtsheim, CJ				Li, LX; Cook, RD; Nachtsheim, CJ			Model-free variable selection	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES B-STATISTICAL METHODOLOGY			English	Article						model selection; sliced inverse regression; stepwise regression; sufficient dimension reduction	SUFFICIENT DIMENSION REDUCTION; SLICED INVERSE REGRESSION; NONPARAMETRIC REGRESSION; INFLATION CRITERION; GRAPHICS	The importance of variable selection in regression has grown in recent years as computing power has encouraged the modelling of data sets of ever-increasing size. Data mining applications in finance, marketing and bioinformatics are obvious examples. A limitation of nearly all existing variable selection methods is the need to specify the correct model before selection. When the number of predictors is large, model formulation and validation can be difficult or even infeasible. On the basis of the theory of sufficient dimension reduction, we propose a new class of model-free variable selection approaches. The methods proposed assume no model of any form, require no nonparametric smoothing and allow for general predictor effects. The efficacy of the methods proposed is demonstrated via simulation, and an empirical example is given.	Univ Calif Davis, Sch Med, Dept Biochem & Mol Med, Davis, CA 95616 USA; Univ Minnesota, St Paul, MN 55108 USA; Univ Minnesota, Minneapolis, MN USA	Li, LX (reprint author), Univ Calif Davis, Sch Med, Dept Biochem & Mol Med, 4303 Tupper Hall,1 Shields Ave, Davis, CA 95616 USA.	lexli@ucdavis.edu					Akaike H., 1973, P 2 INT S INF THEOR, P267; Shen XT, 2002, J AM STAT ASSOC, V97, P210, DOI 10.1198/016214502753479356; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; [Anonymous], 2004, ANN STAT, V32, P1061; GEORGE EI, 1993, J AM STAT ASSOC, V88, P881, DOI 10.2307/2290777; BENJAMINI Y, 1995, J ROY STAT SOC B MET, V57, P289; SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136; Chiaromonte F, 2002, ANN I STAT MATH, V54, P768, DOI 10.1023/A:1022411301790; Cook R. D., 1994, P SECT PHYS ENG SCI, P18; COOK R. D., 1998, REGRESSION GRAPHICS; COOK RD, 1994, J AM STAT ASSOC, V89, P177, DOI 10.2307/2291214; Cook R.D., 1977, J APPLIED STATISTICS, V14, P117, DOI 10.1080/02664768700000016; COOK RD, 1994, J AM STAT ASSOC, V89, P592, DOI 10.2307/2290862; Cook R.D., 1982, RESIDUALS INFLUENCE; COOK RD, 1991, J AM STAT ASSOC, V86, P328, DOI 10.2307/2290564; Cook RD, 1996, J AM STAT ASSOC, V91, P983, DOI 10.2307/2291717; Cox DR, 1974, J R STAT SOC C-APPL, V23, P51; Foster DP, 2004, J AM STAT ASSOC, V99, P303, DOI 10.1198/016214504000000287; FOSTER DP, 1994, ANN STAT, V22, P1947, DOI 10.1214/aos/1176325766; Friedman J. H., 1994, STAT NEURAL NETWORKS; GATHER U, 2002, STATISTICS, V13, P271; HALL P, 1993, ANN STAT, V21, P867, DOI 10.1214/aos/1176349155; Hardle W, 1996, BIOMETRIKA, V83, P541, DOI 10.1093/biomet/83.3.541; Harrell Jr Frank E., 2001, REGRESSION MODELLING; Kallenberg WCM, 1999, J AM STAT ASSOC, V94, P285, DOI 10.2307/2669703; Li KC, 1997, ANN STAT, V25, P577; LI KC, 1991, J AM STAT ASSOC, V86, P316, DOI 10.2307/2290563; LI L, 2004, IN PRESS SIR3 DIMENS; LI L, 2003, THESIS U MINNESOTA S; Li LX, 2004, COMPUT STAT DATA AN, V47, P175, DOI 10.1016/j.csda.2003.10.017; Miller A, 2002, SUBSET SELECTION REG, V2nd; Tibshirani R, 1999, J ROY STAT SOC B, V61, P529, DOI 10.1111/1467-9868.00191; Wegkamp M, 2003, ANN STAT, V31, P252, DOI 10.1214/aos/1046294464; Weisberg S., 1999, APPL REGRESSION INCL; Yang YH, 1999, STAT SINICA, V9, P475; ZHANG P, 1993, ANN STAT, V21, P299, DOI 10.1214/aos/1176349027; ZHANG P, 1991, ANN STAT, V19, P1869, DOI 10.1214/aos/1176348375	37	34	34	0	4	BLACKWELL PUBL LTD	OXFORD	108 COWLEY RD, OXFORD OX4 1JF, OXON, ENGLAND	1369-7412			J ROY STAT SOC B	J. R. Stat. Soc. Ser. B-Stat. Methodol.		2005	67		2				285	299		10.1111/j.1467-9868.2005.00502.x		15	Statistics & Probability	Mathematics	904KZ	WOS:000227498200006		
J	Zou, H; Hastie, T				Zou, H; Hastie, T			Regularization and variable selection via the elastic net	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES B-STATISTICAL METHODOLOGY			English	Article						grouping effect; LARS algorithm; Lasso; penalization; p >> n problem; variable selection	GENE-EXPRESSION; MICROARRAY DATA; REGRESSION; CANCER; CLASSIFICATION; SHRINKAGE; LASSO	We propose the elastic net, a new regularization and variable selection method. Real world data and a simulation study show that the elastic net often outperforms the lasso, while enjoying a similar sparsity of representation. In addition, the elastic net encourages a grouping effect, where strongly correlated predictors tend to be in or out of the model together. The elastic net is particularly useful when the number of predictors (p) is much bigger than the number of observations (n). By contrast, the lasso is not a very satisfactory variable selection method in the p>n case. An algorithm called LARS-EN is proposed for computing elastic net regularization paths efficiently, much like algorithm LARS does for the lasso.	Stanford Univ, Dept Stat, Stanford, CA 94305 USA	Hastie, T (reprint author), Stanford Univ, Dept Stat, Stanford, CA 94305 USA.	hastie@stanford.edu					Fu WJJ, 1998, J COMPUT GRAPH STAT, V7, P397, DOI 10.2307/1390712; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; Tusher VG, 2001, P NATL ACAD SCI USA, V98, P5116, DOI 10.1073/pnas.091062498; Guyon I, 2002, MACH LEARN, V46, P389, DOI 10.1023/A:1012487302797; Segal MR, 2003, J COMPUT BIOL, V10, P961, DOI 10.1089/106652703322756177; FRANK IE, 1993, TECHNOMETRICS, V35, P109, DOI 10.2307/1269656; Friedman J, 2004, ANN STAT, V32, P102; Efron B, 2004, ANN STAT, V32, P407; West M, 2001, P NATL ACAD SCI USA, V98, P11462, DOI 10.1073/pnas.201162998; Tibshirani R, 2002, P NATL ACAD SCI USA, V99, P6567, DOI 10.1073/pnas.082099299; Breiman L, 1996, ANN STAT, V24, P2350; Dettling M, 2004, J MULTIVARIATE ANAL, V90, P106, DOI 10.1016/j.jmva.2004.02.012; DIAZURIARTE R, 2003, SIMPLE METHOD FINDIN; DONOHO DL, 1995, J ROY STAT SOC B MET, V57, P301; Friedman J., 1989, J AM STAT ASSOC, V84, P249; Golub G. H., 1983, MATRIX COMPUTATIONS; Golub T., 1999, SCIENCE, V286, P513; HASTIE T, 2003, GENOME BIOL, V2; Hastie T., 2001, ELEMENTS STAT LEARNI; Hastie T, 2000, GENOME BIOL, V1, P1, DOI DOI 10.1186/GB-2000-1-2-RESEARCH0003; Hoerl A., 1988, ENCY STAT SCI, V8, P129; Rosset S, 2004, J MACH LEARN RES, V5, P941; Stamey T., 1989, J UROLOGY, V16, P1076; Zhang T., 2004, ANN STAT, V32, P469; Zhu J, 2004, BIOSTATISTICS, V5, P427, DOI 10.1093/biostatistics/kxg046	26	1703	1758	44	147	BLACKWELL PUBL LTD	OXFORD	108 COWLEY RD, OXFORD OX4 1JF, OXON, ENGLAND	1369-7412			J ROY STAT SOC B	J. R. Stat. Soc. Ser. B-Stat. Methodol.		2005	67		2				301	320		10.1111/j.1467-9868.2005.00503.x		20	Statistics & Probability	Mathematics	904KZ	WOS:000227498200007		
S	Guigue, V; Rakotomamonjy, A; Canu, S		Gama, J; Camacho, R; Brazdil, P; Jorge, A; Torgo, L		Guigue, V; Rakotomamonjy, A; Canu, S			Kernel basis pursuit	MACHINE LEARNING: ECML 2005, PROCEEDINGS	LECTURE NOTES IN ARTIFICIAL INTELLIGENCE		English	Article; Proceedings Paper	16th European Conference on Machine Learning (ECML)/9th European Conference on Principles and Practice of Knowledge Discovery in Databases (PKDD)	OCT 03-07, 2005	Oporto, PORTUGAL	FCT, LIACC NIAAD		regression; multiple kernels; LASSO; parameter free	REGRESSION; SHRINKAGE	Estimating a non-uniformly sampled function from a set of learning points is a classical regression problem. Kernel methods have been widely used in this context, but every problem leads to two major tasks: optimizing the kernel and setting the fitness-regularization compromise. This article presents a new method, to estimate a function from noisy learning points in the context of RKHS (Reproducing Kernel Hilbert Space). We introduce the Kernel Basis Pursuit algorithm, which enables us to build a phi-regularized-multiple-kernel estimator. The general idea is to decompose the function to learn on a sparse-optimal set of spanning functions. Our implementation relies on the Least Absolute Shrinkage and Selection Operator (LASSO) formulation and on the Least Angle Regression (LARS) solver. The computation of the full regularization path, through the LARS, will enable us to propose new adaptive criteria to find an optimal fitness-regularization compromise. Finally, we aim at proposing a fast parameter-free method to estimate non-uniform-sampled functions.	CNRS, Lab Percept Syst Informat, FRE 2645, F-76801 St Etienne, France	Guigue, V (reprint author), CNRS, Lab Percept Syst Informat, FRE 2645, Ave Univ, F-76801 St Etienne, France.	Vincent.Guigue@insa-rouen.fr					Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; MALLAT SG, 1993, IEEE T SIGNAL PROCES, V41, P3397, DOI 10.1109/78.258082; Efron B, 2004, ANN STAT, V32, P407; DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425; GIROSI F, 1995, NEURAL COMPUT, V7, P219, DOI 10.1162/neco.1995.7.2.219; BACH F, 2004, NEURAL INFORM PROCES, V17; BLAKE C, 1998, UCI REP MACHINE LEAR; CHANG M, 2005, NEURAL COMPUTATION; CHEN S, 1995, THEISS STANFORD U; Grandvalet Y., 1998, ICANN 98. Proceedings of the 8th International Conference on Artificial Neural Networks; Bi J., 2003, Journal of Machine Learning Research, V3, DOI 10.1162/153244303322753643; KIMELDOR.G, 1971, J MATH ANAL APPL, V33, P82, DOI 10.1016/0022-247X(71)90184-3; Ljung L., 1987, SYSTEM IDENTIFICATIO; LOOSLI G, 2004, CAP; Pati Y. C., 1993, 27 AS C SIGN SYST CO; Scholkopf B., 2002, LEARNING KERNELS; Tikhonov AN, 1977, SOLUTIONS ILL POSED; Vincent P, 2002, MACH LEARN, V48, P165, DOI 10.1023/A:1013955821559; Wahba G., 1990, SERIES APPL MATH, V59	20	4	5	0	0	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		3-540-29243-8	LECT NOTES ARTIF INT			2005	3720						146	157				12	Computer Science, Artificial Intelligence	Computer Science	BDF41	WOS:000233235200018		
J	Markowetz, F; Spang, R				Markowetz, F; Spang, R			Molecular diagnosis - Classification, model selection and performance evaluation	METHODS OF INFORMATION IN MEDICINE			English	Article						microarrays; statistical classification; generalization error; model assessment; gene selection	GENE-EXPRESSION DATA; SHRUNKEN CENTROIDS; BREAST-CANCER; PREDICTION; VALIDATION; REGRESSION; DISCOVERY; LEUKEMIA	Objectives. We discuss supervised classification techniques applied to medical diagnosis based on gene expression profiles. Our focus lies on strategies of adaptive model selection to avoid overfitting in high-dimensional spaces. Methods: We introduce likelihood-based methods, classification trees, support vector machines and regularized binary regression. For regularization by dimension reduction, we describe feature selection methods: feature filtering, feature shrinkage and wrapper approaches. In small sample-size situations efficient methods of data re-use are needed to assess the predictive power of a model. We discuss two issues in using cross-validation: the difference between in-loop and out-of-loop feature selection, and estimating model parameters in nested-loop cross-validation. Results: Gene selection does not reduce the dimensionality of the model. Tuning parameters enable adaptive model selection. The feature selection bias is a common pitfall in performance evaluation. Model selection and performance evaluation can be combined by nested-loop cross-validation. Conclusions. Classification of microarrays is prone to overfitting. A rigorous and unbiased assessment of the predictive power of the model is a must.	Max Planck Inst Mol Genet, Computat Diagnost Grp, D-14195 Berlin, Germany	Markowetz, F (reprint author), Max Planck Inst Mol Genet, Computat Diagnost Grp, Ihnestr 63-73, D-14195 Berlin, Germany.	florian.markowetz@molgen.mpg.de					Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324; Breiman L, 1998, ANN STAT, V26, P801; Furey TS, 2000, BIOINFORMATICS, V16, P906, DOI 10.1093/bioinformatics/16.10.906; Yeoh EJ, 2002, CANCER CELL, V1, P133, DOI 10.1016/S1535-6108(02)00032-6; Cheok MH, 2003, NAT GENET, V34, P85, DOI 10.1038/ng1151; Ambroise C, 2002, P NATL ACAD SCI USA, V99, P6562, DOI 10.1073/pnas.102102699; Dudoit S, 2002, J AM STAT ASSOC, V97, P77, DOI 10.1198/016214502753479248; Guyon I, 2002, MACH LEARN, V46, P389, DOI 10.1023/A:1012487302797; Tibshirani R, 2003, STAT SCI, V18, P104, DOI 10.1214/ss/1056397488; Altman DG, 2000, STAT MED, V19, P453, DOI 10.1002/(SICI)1097-0258(20000229)19:4<453::AID-SIM350>3.3.CO;2-X; Blum AL, 1997, ARTIF INTELL, V97, P245, DOI 10.1016/S0004-3702(97)00063-5; West M, 2001, P NATL ACAD SCI USA, V98, P11462, DOI 10.1073/pnas.201162998; Tibshirani R, 2002, P NATL ACAD SCI USA, V99, P6567, DOI 10.1073/pnas.082099299; Kohavi R, 1997, ARTIF INTELL, V97, P273, DOI 10.1016/S0004-3702(97)00043-X; Ramaswamy S, 2001, P NATL ACAD SCI USA, V98, P15149, DOI 10.1073/pnas.211566398; Bishop C. M., 1995, NEURAL NETWORKS PATT; Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.1023/A:1018054314350; Breiman L., 1984, CLASSIFICATION REGRE; Duda R. O., 2001, PATTERN CLASSIFICATI; Eilers PHC, 2001, P SOC PHOTO-OPT INS, V2, P187; Gelman A., 2003, BAYESIAN DATA ANAL; Guyon I., 2003, Journal of Machine Learning Research, V3, DOI 10.1162/153244303322753616; Hastie T., 2001, ELEMENTS STAT LEARNI; Hastie T, 2004, J MACH LEARN RES, V5, P1391; Hochreiter S., 2004, KERNEL METHODS COMPU; Huang E, 2003, RECENT PROG HORM RES, V58, P55, DOI 10.1210/rp.58.1.55; Jager J., 2003, P PAC S BIOC, P53; Johnson VE, 1999, ORDINAL DATA MODELIN; Krishnapuram B., 2004, KERNEL METHODS COMPU; R Development Core Team, 2004, R LANG ENV STAT COMP; Ransohoff DF, 2004, NAT REV CANCER, V4, P309, DOI 10.1038/nrc1322; Ripley B. D., 1996, PATTERN RECOGNITION; ROTH V, 2004, IEEE T NEURAL NETWOR, V15; Scholkopf B., 2001, LEARNING KERNELS; SIMON R, 2003, J NATL CANC I, V95; Spang Rainer, 2002, In Silico Biology, V2, P369; Vapnik V., 1998, STAT LEARNING THEORY; Vapnik V.N., 1995, NATURE STAT LEARNING	39	31	31	3	3	SCHATTAUER GMBH-VERLAG MEDIZIN NATURWISSENSCHAFTEN	STUTTGART	HOLDERLINSTRASSE 3, D-70174 STUTTGART, GERMANY	0026-1270			METHOD INFORM MED	Methods Inf. Med.		2005	44	3					438	443				6	Computer Science, Information Systems; Health Care Sciences & Services; Medical Informatics	Computer Science; Health Care Sciences & Services; Medical Informatics	948RY	WOS:000230734600015	16113770	
J	Shimamura, T; Mizuta, M				Shimamura, T; Mizuta, M			Flexible regression modeling via radial basis function networks and Lasso-type estimator	NEURAL NETWORK WORLD			English	Article; Proceedings Paper	16th Symposium on Computational Statistics (COMPSTAT 2004)	2004	Prague, CZECH REPUBLIC			nonlinear regression; neural networks	SMOOTHING PARAMETER SELECTION; INFORMATION CRITERION; DIMENSION	Radial basis function networks provides a more flexible model and gives a very good performance over a wide range of applications. However, in the modeling process, care is taken not to choose the number of the basis functions and the positions of the centres, the regularization parameter and the smoothing parameter as appropriate according to the model complexity, they often gives poor generalization performance. In this paper, we develop a new model building procedure based on radial basis function networks; positioning the centres with k-means clustering for the conditional distribution Pr(x vertical bar y) and estimating the weights by maximum penalized likelihood with Lasso penalty. We present an information criterion for choosing the regularization and smoothing parameters in the models. The proposed procedure determines the proper number and location of the centres automatically. The simulation result shows that the proposed method performs very well.	Hokkaido Univ, Grad Sch Engn, Kita Ku, Sapporo, Hokkaido 0600811, Japan; Hokkaido Univ, Informat Initiat Ctr, Kita Ku, Sapporo, Hokkaido 0600811, Japan	Shimamura, T (reprint author), Hokkaido Univ, Grad Sch Engn, Kita Ku, N 11 W 5, Sapporo, Hokkaido 0600811, Japan.	shima@cims.hokudai.ac.jp	Mizuta, Masahiro/A-7799-2012				Akaike H., 1973, 2 INT S INF THEOR, P267; Andrieu C, 2001, NEURAL COMPUT, V13, P2359, DOI 10.1162/089976601750541831; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; MACKAY DJC, 1992, NEURAL COMPUT, V4, P448, DOI 10.1162/neco.1992.4.3.448; Hastie TJ, 1990, GEN ADDITIVE MODELS; Holmes CC, 1998, NEURAL COMPUT, V10, P1217, DOI 10.1162/089976698300017421; Hurvich CM, 1998, J ROY STAT SOC B, V60, P271, DOI 10.1111/1467-9868.00125; Konishi S, 2004, BIOMETRIKA, V91, P27, DOI 10.1093/biomet/91.1.27; LI KC, 1991, J AM STAT ASSOC, V86, P316, DOI 10.2307/2290563; NEAL RM, LECT NOTES STAT, V118; Rios Insua D., 1998, PRACTICAL NONPARAMET, P181; ROTH V, 2002, COMPUTATIONAL STAT C; SUGIURA N, 1978, COMMUN STAT A-THEOR, V7, P13, DOI 10.1080/03610927808827599	13	0	0	0	7	ACAD SCIENCES CZECH REPUBLIC, INST COMPUTER SCIENCE	182 07 PRAGUE 8	POD VODARENSKOU VEZI 2, 182 07 PRAGUE 8, 00000, CZECH REPUBLIC	1210-0552			NEURAL NETW WORLD	Neural Netw. World		2005	15	4					367	374				8	Computer Science, Artificial Intelligence; Neurosciences	Computer Science; Neurosciences & Neurology	973OM	WOS:000232532500012		
J	Lemeille, S; Latifi, A; Geiselmann, J				Lemeille, S; Latifi, A; Geiselmann, J			Inferring the connectivity of a regulatory network from mRNA quantification in Synechocystis PCC6803	NUCLEIC ACIDS RESEARCH			English	Article							SP STRAIN PCC-6803; SIGMA-FACTORS; GENE-EXPRESSION; CIRCADIAN CLOCK; NITROGEN STRESS; CYANOBACTERIUM; POLYMERASE; CROSSTALK; SEQUENCE; PROTEIN	A major task of contemporary biology is to understand and predict the functioning of regulatory networks. We use expression data to deduce the regulation network connecting the sigma factors of Synechocystis PCC6803, the most global regulators in bacteria. Synechocystis contains one group 1 (SigA) and four group 2 (SigB, SigC, SigD and SigE) sigma factors. From the relative abundance of the sig mRNA measured in the wild-type and the four group 2 sigma mutants, we derive a network of the influences of each sigma factor on the transcription of all other sigma factors. Internal or external stimuli acting on only one of the sigma factors will thus indirectly modify the expression of most of the others. From this model, we predict the control points through which the circadian time modulates the expression of the sigma factors. Our results show that the cross regulation between the group 1 and group 2 sigma factors is very important for the adaptation of the bacterium to different environmental and physiological conditions.	Univ Grenoble 1, CNRS, UMR 5163, Lab Adaptat & Pathogenie Microorganismes, F-38700 La Tronche, France; CNRS, IBSM, Chim Bacterienne Lab, F-13402 Marseille, France	Geiselmann, J (reprint author), Univ Grenoble 1, CNRS, UMR 5163, Lab Adaptat & Pathogenie Microorganismes, Batiment Jean Roget,Domaine Merci, F-38700 La Tronche, France.	hans.geiselmann@ujf-grenoble.fr					Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; RIPPKA R, 1979, J GEN MICROBIOL, V111, P1; Hubner A, 2001, P NATL ACAD SCI USA, V98, P12724, DOI 10.1073/pnas.231442498; Arora SK, 1997, J BACTERIOL, V179, P5574; Asayama M, 2004, BIOSCI BIOTECH BIOCH, V68, P477, DOI 10.1271/bbb.68.477; Bhaya D, 1999, P NATL ACAD SCI USA, V96, P3188, DOI 10.1073/pnas.96.6.3188; Caslake LF, 1997, MICROBIOL-UK, V143, P3807; Gardner TS, 2003, SCIENCE, V301, P102, DOI 10.1126/science.1081900; Goto-Seki A, 1999, MOL MICROBIOL, V34, P473, DOI 10.1046/j.1365-2958.1999.01608.x; Hecker M, 1996, MOL MICROBIOL, V19, P417, DOI 10.1046/j.1365-2958.1996.396932.x; Hengge-Aronis R, 2002, MICROBIOL MOL BIOL R, V66, P373, DOI 10.1128/MMBR.66.3.373-395.2002; Imamura S, 2003, J MOL BIOL, V325, P857, DOI 10.1016/S0022-2836(02)01242-1; Imamura S, 2003, FEBS LETT, V554, P357, DOI 10.1016/S0014-5793(03)01188-8; Iwasaki H, 2000, CELL, V101, P223, DOI 10.1016/S0092-8674(00)80832-6; Kaneko T, 1996, DNA Res, V3, P109, DOI 10.1093/dnares/3.3.109; Kondo T, 2000, BIOESSAYS, V22, P10, DOI 10.1002/(SICI)1521-1878(200001)22:1<10::AID-BIES4>3.0.CO;2-A; KUSTU S, 1989, MICROBIOL REV, V53, P367; Lemeille S, 2005, BMC MICROBIOL, V5, DOI 10.1186/1471-2180-5-18; LIU Y, 1995, GENE DEV, V9, P1469, DOI 10.1101/gad.9.12.1469; LONETTO M, 1992, J BACTERIOL, V174, P3843; MULVEY MR, 1989, NUCLEIC ACIDS RES, V17, P9979, DOI 10.1093/nar/17.23.9979; Muro-Pastor AM, 2001, J BACTERIOL, V183, P1090, DOI 10.1128/JB.183.3.1090-1095.2001; Mutsuda M, 2003, J BIOL CHEM, V278, P19102, DOI 10.1074/jbc.M213255200; Nair U, 2002, J BACTERIOL, V184, P3530, DOI 10.1128/JB.184.13.3530-3538.2002; Schmitz O, 2000, SCIENCE, V289, P765, DOI 10.1126/science.289.5480.765; Tuominen I, 2003, J BACTERIOL, V185, P1116, DOI 10.1128/JB.185.3.1116-1119.2003	26	14	14	0	1	OXFORD UNIV PRESS	OXFORD	GREAT CLARENDON ST, OXFORD OX2 6DP, ENGLAND	0305-1048			NUCLEIC ACIDS RES	Nucleic Acids Res.		2005	33	10					3381	3389		10.1093/nar/gki654		9	Biochemistry & Molecular Biology	Biochemistry & Molecular Biology	947MI	WOS:000230649600031	15944453	
S	Pelckmans, K; Suykens, JAK; De Moor, B			IEEE	Pelckmans, K; Suykens, JAK; De Moor, B			Maximal variation and missing values for componentwise support vector machines	Proceedings of the International Joint Conference on Neural Networks (IJCNN), Vols 1-5	IEEE International Joint Conference on Neural Networks (IJCNN)		English	Proceedings Paper	IEEE International Joint Conference on Neural Networks (IJCNN 2005)	JUL 31-AUG 04, 2005	Montreal, CANADA	Int Neural Network Soc, IEEE Computat Intelligence Soc			REGRESSION; PURSUIT; LASSO	This paper proposes primal-dual kernel machine classifiers based on worst-case analysis of a finite set of observations including missing values of the inputs. Key ingredients are the use of a componentwise Support Vector Machine (cSVM) and an empirical measure of maximal variation of the components to bound the influence of the component which can not be evaluated due to missing values. A regularization term based on the L, norm of the maximal variation is used to obtain a mechanism for structure detection in that context. An efficient implemtation using the hierarchical kernel machines framework is elaborated.	Katholieke Univ Leuven, ESAT, SCD, SISTA, B-3001 Heverlee, Belgium	Pelckmans, K (reprint author), Katholieke Univ Leuven, ESAT, SCD, SISTA, Kasteelpk Arenberg 10, B-3001 Heverlee, Belgium.		Pelckmans, Kristiaan/A-3118-2013; Suykens, Johan/C-9781-2014				FRIEDMAN JH, 1981, J AM STAT ASSOC, V76, P817, DOI 10.2307/2287576; RUBIN DB, 1976, BIOMETRIKA, V63, P581, DOI 10.1093/biomet/63.3.581; Fu WJJ, 1998, J COMPUT GRAPH STAT, V7, P397, DOI 10.2307/1390712; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Friedman JH, 1999, STAT COMPUT, V9, P123, DOI 10.1023/A:1008894516817; Antoniadis A, 2001, J AM STAT ASSOC, V96, P939, DOI 10.1198/016214501753208942; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1; Chen SSB, 2001, SIAM REV, V43, P129, DOI 10.1137/S003614450037906X; Suykens JAK, 1999, NEURAL PROCESS LETT, V9, P293, DOI 10.1023/A:1018628609742; FRANK LE, 1993, TECHNOMETRICS, P109; FRIEDMAN JH, 1991, ANN STAT, V19, P1, DOI 10.1214/aos/1176347963; Gunn SR, 2002, MACH LEARN, V48, P137, DOI 10.1023/A:1013903804720; Hastie T., 2001, ELEMENTS STAT LEARNI; Hastie T., 1990, GENERALIZED ADDITIVE; Little Roderick, 1987, STAT ANAL MISSING DA; PELCKMANS K, 2003, 03184 ESAT SISTA KU; PELCKMANS K, 2004, IN PRESS NEUROCOMPUT; PELECKMANS K, 2005, IN PRESS SUPPORT VEC; Rubin DB, 1987, MULTIPLE IMPUTATION; Scholkopf B., 2002, LEARNING KERNELS; Suykens J. A. K., 2002, LEAST SQUARES SUPPOR; Vandenberghe L., 2004, CONVEX OPTIMIZATION; Vapnik V., 1998, STAT LEARNING THEORY	23	0	0	0	0	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA	1098-7576		0-7803-9048-2	IEEE IJCNN			2005							2814	2819				6	Computer Science, Artificial Intelligence	Computer Science	BDS40	WOS:000235178004027		
S	Zhang, N; Zeng, SQ			IEEE	Zhang, N; Zeng, SQ			A gradient descending solution to the LASSO criteria	Proceedings of the International Joint Conference on Neural Networks (IJCNN), Vols 1-5	IEEE International Joint Conference on Neural Networks (IJCNN)		English	Proceedings Paper	IEEE International Joint Conference on Neural Networks (IJCNN 2005)	JUL 31-AUG 04, 2005	Montreal, CANADA	Int Neural Network Soc, IEEE Computat Intelligence Soc			VECTOR MACHINE	In this paper, we propose a new perspective to achieve sparseness via the winner-take-all principle for the linear kernel regression and classification tasks. We form the duality of the LASSO criteria, and transfer an l(1) norm minimization to an e. norm maximization problem. We introduce a novel winner-take-all neural network solution derived from gradient descending, which links the sparse representation and the competitive learning scheme. This scheme is a form of unsupervised learning in which each input pattern comes through learning, to be associated with the activity of one or at most a few neurons. However, the lateral interaction between neurons in the same layer is strictly preemptive in this model. This framework is applicable to a variety of problems, such as Independent Component Analysis (ICA), feature selection, and data clustering.	Michigan State Univ, Dept Comp Sci & Engn, E Lansing, MI 48824 USA	Zhang, N (reprint author), Michigan State Univ, Dept Comp Sci & Engn, E Lansing, MI 48824 USA.						Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Tipping ME, 2001, J MACH LEARN RES, V1, P211, DOI 10.1162/15324430152748236; Amaldi E, 1998, THEOR COMPUT SCI, V209, P237, DOI 10.1016/S0304-3975(97)00115-1; Suykens JAK, 1999, NEURAL PROCESS LETT, V9, P293, DOI 10.1023/A:1018628609742; Weng JY, 2003, IEEE T PATTERN ANAL, V25, P1034; Figueiredo MAT, 2003, IEEE T PATTERN ANAL, V25, P1150, DOI 10.1109/TPAMI.2003.1227989; KOHONEN T, 2001, SELFORGANIZING MAPS, P60; LUENVERGER DG, 1969, OPTIMIZATION VECTOR; WENG J, 2004, P IEEE INT C AC SPEE; ZHANG N, 2004, P IEEE INT JOINT C N	10	0	0	0	0	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA	1098-7576		0-7803-9048-2	IEEE IJCNN			2005							2942	2947				6	Computer Science, Artificial Intelligence	Computer Science	BDS40	WOS:000235178004049		
B	Juutilainen, M; Roning, J; Laurinen, P		Martikainen, J		Juutilainen, M; Roning, J; Laurinen, P			A study on the differences in the interpolation capabilities of models	SMCia/05: Proceedings of the 2005 IEEE Mid-Summer Workshop on Soft Computing in Industrial Applications			English	Proceedings Paper	IEEE Mid-Summer Workshop on Soft Computing in Industrial Applications	JUN 28-30, 2005	Espoo, FINLAND	IEEE, IEEE Syst Man & Cybernet Soc, IEEE Finland Sect, Helsinki Univ Technol, Acad Finland, Finnish Cultural Fdn, Kone Corp				We examined the interpolation capabilities of learning methods using simulated data sets and a real data set. We compared five common learning methods for their generalisation capability on the boundaries of the training data set. Also, we examined the effects of the complexity of models on interpolation capability. Our main results were that there are differences between the different model families, but model complexity does not have a major effect on interpolation capability. The multi-layer perceptron, support vector regression and additive spline models outperformed local linear regression and quadratic regression in interpolation capabilities. Information about the interpolation capability of models is useful when, for example, evaluating the reliability of prediction.	Univ Oulu, Comp Engn Lab, Intelligent Syst Grp, Oulu 90014, Finland	Juutilainen, M (reprint author), Univ Oulu, Comp Engn Lab, Intelligent Syst Grp, POB 4500, Oulu 90014, Finland.						Bontempi G, 1999, INT J CONTROL, V72, P643, DOI 10.1080/002071799220830; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Scholkopf B, 2000, NEURAL COMPUT, V12, P1207, DOI 10.1162/089976600300015565; JUUTILAINEN I, 2005, IN PRESS P INT S APP; Vapnik V., 1998, STAT LEARNING THEORY; WOOD SN, 2004, J AM STAT ASSOC, V99, P637; Wood SN, 2003, J ROY STAT SOC B, V65, P95, DOI 10.1111/1467-9868.00374	7	0	0	0	0	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA			0-7803-8942-5				2005							202	207				6	Computer Science, Artificial Intelligence	Computer Science	BCR70	WOS:000230955200036		
J	Schafer, J; Strimmer, K				Schafer, J; Strimmer, K			A shrinkage approach to large-scale covariance matrix estimation and implications for functional genomics	STATISTICAL APPLICATIONS IN GENETICS AND MOLECULAR BIOLOGY			English	Article						shrinkage; covariance estimation; "small n, large p" problem; graphical Gaussian model (GGM); genetic network; gene expression	GENE-EXPRESSION; NONORTHOGONAL PROBLEMS; RIDGE REGRESSION; CLUSTER-ANALYSIS; NETWORKS; DISCOVERY; INFERENCE; MODELS	Inferring large-scale covariance matrices from sparse genomic data is an ubiquitous problem in bioinformatics. Clearly, the widely used standard covariance and correlation estimators are ill-suited for this purpose. As statistically efficient and computationally fast alternative we propose a novel shrinkage covariance estimator that exploits the Ledoit-Wolf (2003) lemma for analytic calculation of the optimal shrinkage intensity. Subsequently, we apply this improved covariance estimator (which has guaranteed minimum mean squared error, is well-conditioned, and is always positive definite even for small sample sizes) to the problem of inferring large-scale gene association networks. We show that it performs very favorably compared to competing approaches both in simulations as well as in application to real expression data.	Univ Munich, Dept Stat, D-80539 Munich, Germany	Schafer, J (reprint author), Univ Munich, Dept Stat, D-80539 Munich, Germany.	juliane.schaefer@stat.math.ethz.ch; korbinian.strimmer@lmu.de	Strimmer, Korbinian/C-1522-2009	Strimmer, Korbinian/0000-0001-7917-2056			Storey JD, 2002, J ROY STAT SOC B, V64, P479, DOI 10.1111/1467-9868.00346; Cui XG, 2005, BIOSTATISTICS, V6, P59, DOI 10.1093/biostatistics/kxh018; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; EFRON B, 1977, SCI AM, V236, P119; Ledoit O, 2004, J MULTIVARIATE ANAL, V88, P365, DOI 10.1016/S0047-259X(03)00096-4; HOERL AE, 1970, TECHNOMETRICS, V12, P55; de la Fuente A, 2004, BIOINFORMATICS, V20, P3565, DOI 10.1093/bioinformatics/bth445; Butte AJ, 2000, P NATL ACAD SCI USA, V97, P12182, DOI 10.1073/pnas.220392197; Eisen MB, 1998, P NATL ACAD SCI USA, V95, P14863, DOI 10.1073/pnas.95.25.14863; Greenland S, 2000, INT J EPIDEMIOL, V29, P158, DOI 10.1093/ije/29.1.158; BENJAMINI Y, 1995, J ROY STAT SOC B MET, V57, P289; HOTELLING H, 1953, J ROY STAT SOC B, V15, P193; HIGHAM NJ, 1988, LINEAR ALGEBRA APPL, V103, P103, DOI 10.1016/0024-3795(88)90223-6; MORRIS CN, 1983, J AM STAT ASSOC, V78, P47, DOI 10.2307/2287098; Tibshirani R, 2002, P NATL ACAD SCI USA, V99, P6567, DOI 10.1073/pnas.082099299; EFRON B, 1975, J AM STAT ASSOC, V70, P311, DOI 10.2307/2285814; Barabasi AL, 2004, NAT REV GENET, V5, P101, DOI 10.1038/nrg1272; Cox DR, 2004, BIOMETRIKA, V91, P729, DOI 10.1093/biomet/91.3.729; Boginski V, 2005, COMPUT STAT DATA AN, V48, P431, DOI 10.1016/j.csda.2004.02.004; Daniels MJ, 2001, BIOMETRICS, V57, P1173, DOI 10.1111/j.0006-341X.2001.01173.x; Dobra A, 2004, J MULTIVARIATE ANAL, V90, P196, DOI 10.1016/j.jmva.2004.02.009; EFRON B, 1975, ADV MATH, V16, P259, DOI 10.1016/0001-8708(75)90114-0; EFRON B, 1982, ANN STAT, V10, P340, DOI 10.1214/aos/1176345778; EFRON B, 2005, CORRELATION LARGE SC; Efron B., 2005, LOCAL FALSE DISCOVER; Efron B, 2004, J AM STAT ASSOC, V99, P96, DOI 10.1198/016214504000000089; FRIEDMAN JH, 1989, J AM STAT ASSOC, V84, P165, DOI 10.2307/2289860; GUO Y, 2004, REGULARIZED DISCRIMI; HASTE T, 2001, ELEMENTS STAT LEARNI; HIRSCHBERGER M, 2004, RANDOMLY GENERATING; HOERL AE, 1970, TECHNOMETRICS, V12, P69, DOI 10.2307/1267352; Ledoit 0., 2003, J EMPIR FINANC, V10, P603, DOI DOI 10.1016/S0927-5398(03)00007-0; Ledoit O, 2004, J PORTFOLIO MANAGE, V30, P110, DOI 10.3905/jpm.2004.110; Leung PL, 1998, ANN I STAT MATH, V50, P523, DOI 10.1023/A:1003529529228; Magwene PM, 2004, GENOME BIOL, V5, DOI 10.1186/gb-2004-5-12-r100; Mantegna RN, 2000, INTRO ECONOPHYSICS C; MEINSHAUSEN N, 2005, IN PRESS ANN STAT; Schafer J, 2005, AIP CONF PROC, V776, P263; Schafer J, 2005, BIOINFORMATICS, V21, P754, DOI 10.1093/bioinformatics/bti062; SCHMIDTHECK W, 2004, P EUNITE S 10 12 JUN, P407; Smyth G. K., 2004, STAT APPL GENET MOL, V3, DOI [10.2202/1544-6115.1027, DOI 10.2202/1544-6115.1027]; Stein Charles, 1956, P 3 BERK S MATH STAT, V1, P197; Toh H, 2002, BIOINFORMATICS, V18, P287, DOI 10.1093/bioinformatics/18.2.287; Whittaker J, 1990, GRAPHICAL MODELS APP; Zhu J, 2004, BIOSTATISTICS, V5, P427, DOI 10.1093/biostatistics/kxg046; Wille A, 2004, GENOME BIOL, V5, DOI 10.1186/gb-2004-5-11-r92	46	294	297	6	28	BERKELEY ELECTRONIC PRESS	BERKELEY	2809 TELEGRAPH AVENUE, STE 202, BERKELEY, CA 94705 USA	1544-6115			STAT APPL GENET MO B	Stat. Appl. Genet. Mol. Biol.		2005	4								32	10.2202/1544-6115.1175		32	Biochemistry & Molecular Biology; Mathematical & Computational Biology; Statistics & Probability	Biochemistry & Molecular Biology; Mathematical & Computational Biology; Mathematics	055VG	WOS:000238478100006		
J	Thorsson, V; Hornquist, M; Siegel, AF; Hood, L				Thorsson, V; Hornquist, M; Siegel, AF; Hood, L			Reverse engineering galactose regulation in yeast through model selection	STATISTICAL APPLICATIONS IN GENETICS AND MOLECULAR BIOLOGY			English	Article						model selection; regression; AIC; BIC; MDL; Cp; bootstrap; Lasso	GENE NETWORKS; REGRESSION; LASSO	We examine the application of statistical model selection methods to reverse-engineering the control of galactose utilization in yeast from DNA microarray experiment data. In these experiments, relationships among gene expression values are revealed through modifications of galactose sugar level and genetic perturbations through knockouts. For each gene variable, we select predictors using a variety of methods, taking into account the variance in each measurement. These methods include maximization of log-likelihood with Cp, AIC, and BIC penalties, bootstrap and cross-validation error estimation, and coefficient shrinkage via the Lasso.	Linkoping Univ, S-58183 Linkoping, Sweden; Univ Washington, Seattle, WA 98195 USA		thorsson@systemsbiology.org; micho@itn.liu.se; asiegel@u.washington.edu; lhood@systemsbiology.org					Hansen MH, 2001, J AM STAT ASSOC, V96, P746, DOI 10.1198/016214501753168398; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; BREIMAN L, 1992, J AM STAT ASSOC, V87, P738, DOI 10.2307/2290212; Yu J, 2004, BIOINFORMATICS, V20, P3594, DOI 10.1093/bioinformatics/bth448; Yeung MKS, 2002, P NATL ACAD SCI USA, V99, P6163, DOI 10.1073/pnas.092576199; Ideker T, 2001, SCIENCE, V292, P929, DOI 10.1126/science.292.5518.929; George EI, 2000, J AM STAT ASSOC, V95, P1304, DOI 10.2307/2669776; Arkin A, 1997, SCIENCE, V277, P1275, DOI 10.1126/science.277.5330.1275; Friedman N, 2000, J COMPUT BIOL, V7, P601, DOI 10.1089/106652700750050961; Ideker T, 2000, J COMPUT BIOL, V7, P805, DOI 10.1089/10665270050514945; MALLOWS CL, 1973, TECHNOMETRICS, V15, P661, DOI 10.2307/1267380; Bickel DR, 2005, BIOINFORMATICS, V21, P1121, DOI 10.1093/bioinformatics/bti140; D'haeseleer P, 1999, PAC S BIOC, V4, P41; de Hoon M., 2003, PACIFIC S BIOCOMPUTI, V8, P17; Draper N. R., 1998, APPL REGRESSION ANAL; FARKAS IJ, 2003, PHYSICA A, V381, P601; Gardner TS, 2003, SCIENCE, V301, P102, DOI 10.1126/science.1081900; Hastie T., 2001, ELEMENTS STAT LEARNI; Ideker T. E., 2000, PAC S BIOC, V2000, P302; Jong H. D., 2002, J COMPUT BIOL, V9, P67; Miller A, 2002, SUBSET SELECTION REG, V2nd; Mitchell M, 1998, INTRO GENETIC ALGORI; Osborne MR, 2000, J COMPUT GRAPH STAT, V9, P319, DOI 10.2307/1390657; Pournara I, 2004, BIOINFORMATICS, V20, P2934, DOI 10.1093/bioinformatics/bth337; Rung J, 2002, BIOINFORMATICS, V18, pS202; Schafer J, 2005, BIOINFORMATICS, V21, P754, DOI 10.1093/bioinformatics/bti062; Shao J, 1997, STAT SINICA, V7, P221; VONSOMEREN EP, 2000, P 8 INT C INT SYST M, P355; Wahde M, 2001, J COMPUT BIOL, V8, P429, DOI 10.1089/106652701752236223; Weaver D. C., 1999, PAC S BIOCOMPUT, V4, P112; Wuensche A, 1998, PAC S BIOCOMPUT, V3, P89; YOO C, 2002, P PAC S BIOC, V7, P498; Zheng WJ, 1997, J BIOL CHEM, V272, P30350, DOI 10.1074/jbc.272.48.30350	33	7	8	0	1	BERKELEY ELECTRONIC PRESS	BERKELEY	2809 TELEGRAPH AVENUE, STE 202, BERKELEY, CA 94705 USA	1544-6115			STAT APPL GENET MO B	Stat. Appl. Genet. Mol. Biol.		2005	4								28			24	Biochemistry & Molecular Biology; Mathematical & Computational Biology; Statistics & Probability	Biochemistry & Molecular Biology; Mathematical & Computational Biology; Mathematics	055VG	WOS:000238478100010		
J	Leeb, H; Potscher, BM				Leeb, H; Potscher, BM			Model selection and inference: Facts and fiction	ECONOMETRIC THEORY			English	Article							VARIABLE-SELECTION; ASYMPTOTIC PROPERTIES; CONFIDENCE-REGIONS; PREDICTION REGIONS; SPECTRAL DENSITY; ORDER ESTIMATION; LEAST-SQUARES; ESTIMATORS; REGRESSION; LIKELIHOOD	Model selection has an important impact on subsequent inference. Ignoring the model selection step leads to invalid inference. We discuss some intricate aspects of data-driven model selection that do not seem to have been widely appreciated in the literature. We debunk some myths about model selection, in particular the myth that consistent model selection has no effect on subsequent inference asymptotically. We also discuss an "impossibility" result regarding the estimation of the finite-sample distribution of post-model-selection estimators.	Univ Vienna, Dept Stat, A-1010 Vienna, Austria; Yale Univ, New Haven, CT 06520 USA	Potscher, BM (reprint author), Univ Vienna, Dept Stat, Univ Str 5, A-1010 Vienna, Austria.	Benedikt.Poetscher@univie.ac.at	Leeb, Hannes/C-9026-2014	Leeb, Hannes/0000-0002-5770-5955			Ahmed SE, 2000, STAT NEERL, V54, P47, DOI 10.1111/1467-9574.00125; Altissimo F., 2002, ECONOMET J, V5, P494, DOI 10.1111/1368-423X.00095; Altissimo F, 2003, J ECONOMETRICS, V117, P207, DOI 10.1016/S0304-4076(03)00147-7; ANDREWS DWK, 1986, REV ECON STUD, V53, P263, DOI 10.2307/2297650; NISHII R, 1984, ANN STAT, V12, P758, DOI 10.1214/aos/1176346522; Knight K, 2000, ANN STAT, V28, P1356; Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; FRANK IE, 1993, TECHNOMETRICS, V35, P109, DOI 10.2307/1269656; Hjort NL, 2003, J AM STAT ASSOC, V98, P879, DOI 10.1198/016214503000000828; Bauer P., 1988, STATISTICS, V19, P39, DOI 10.1080/02331888808802068; Bunea F, 2004, ANN STAT, V32, P898, DOI 10.1214/009053604000000247; BUNEA F, 2003, CONSISTENCY FDR ESTI; Corradi V, 1999, ECONOMET THEOR, V15, P643; Danilov D, 2004, J ECONOMETRICS, V122, P27, DOI 10.1016/j.jeconom.2003.10.018; Dijkstra T. K., 1988, LECT NOTES EC MATH S, V307, P17; DUFOUR JM, 2003, IN PRESS J ECONOMETR; DUKIC VM, 2002, ESTIMATION MODEL SEL; ENSOR KB, 1988, BIOMETRIKA, V75, P587; Giles J.A., 1993, J EC SURVEYS, V7, P145, DOI 10.1111/j.1467-6419.1993.tb00163.x; HAJEK J, 1971, FDN STAT INFERENCE, P142; Hajek J., 1967, THEORY RANK TESTS; HALL AR, 2003, ECONOMET REV, V22, P269, DOI 10.1081/ETC-120024752; HANNAN EJ, 1979, J ROY STAT SOC B MET, V41, P190; Hansen P. R., 2003, REGRESSION ANAL MANY; Hidalgo J, 2002, J ECONOMETRICS, V110, P213, DOI 10.1016/S0304-4076(02)00094-5; Hosoya Y., 1984, TIME SERIES ANAL THE, V5, P39; Judge G., 1978, STAT IMPLICATIONS PR; Judge GG, 1986, IMPROVED METHODS INF; KABAILA P, 1996, INFORMATION STAT IND, P114; KABAILA P, 2004, LARGE SAMPLE MINIMAL; KABAILA P, 1995, ECONOMET THEOR, V11, P537; Kabaila P, 1998, ECONOMET THEOR, V14, P463; Kapetanios G, 2001, ECON LETT, V72, P137, DOI 10.1016/S0165-1765(01)00433-5; KEMPTHORNE PJ, 1984, BIOMETRIKA, V71, P593; Kilian L, 1998, J TIME SER ANAL, V19, P531, DOI 10.1111/1467-9892.00107; KNIGHT K, 1999, EPI CONVERGENCE DIST; KOUL HL, 1984, STAT DECISIONS S, V1, P17; KULPERGER RJ, 1992, COMMUN STAT THEORY, V21, P2071, DOI 10.1080/03610929208830898; LEEB H, 2002, PERFORMANCE LIMITS E; LEEB H, 2003, DISTRIBUTION LINEAR; Leeb H, 2003, ECONOMET THEOR, V19, P100, DOI 10.1017/S0266466603191050; LEEB H, 2004, UNPUB ONE ESTIMATE U; LEEB H, 2003, 1444 U VIENN DEP STA; LEEB H, 2003, IN PRESS J STAT PLAN; Lehmann E. L., 1998, THEORY POINT ESTIMAT; LUTKEPOHL H, 1990, REV ECON STAT, V72, P116, DOI 10.2307/2109746; Magnus JR, 1999, THEOR PROBAB APPL+, V44, P293; NICKL R, 2003, THESIS U VIENNA; POTSCHER BM, 1983, ANN STAT, V11, P872, DOI 10.1214/aos/1176346253; Potscher BM, 1998, J STAT COMPUT SIM, V60, P19, DOI 10.1080/00949659808811870; POTSCHER BM, 1991, ECONOMET THEOR, V7, P163; POTSCHER BM, 1981, 5 U TECHN DEP EC OP; POTSCHER BM, 1995, ECONOMET THEOR, V11, P550; Potscher BM, 2002, ECONOMETRICA, V70, P1035, DOI 10.1111/1468-0262.00318; RAO C. R., 2001, IMS LECT NOTES MONOG, V38, P1; SARGAN DJ, 2001, ECONOMETRIC REV, V20, P171; SCLOVE SL, 1972, ANN MATH STAT, V43, P1481, DOI 10.1214/aoms/1177692380; SEN PK, 1987, ANN STAT, V15, P1580, DOI 10.1214/aos/1176350611; SEN PK, 1979, ANN STAT, V7, P1019, DOI 10.1214/aos/1176344785; Shibata R, 1986, J APPL PROBAB A, V23, P127, DOI DOI 10.2307/3214348; SODERSTROM T, 1977, INT J CONTROL, V26, P1, DOI 10.1080/00207177708922285; YANG Y, 2003, STRENGHTS AIC BIC SH	64	123	124	4	10	CAMBRIDGE UNIV PRESS	NEW YORK	32 AVENUE OF THE AMERICAS, NEW YORK, NY 10013-2473 USA	0266-4666	1469-4360		ECONOMET THEOR	Economet. Theory	FEB	2005	21	1					21	59		10.1017/S0266466605050036		39	Economics; Mathematics, Interdisciplinary Applications; Social Sciences, Mathematical Methods; Statistics & Probability	Business & Economics; Mathematics; Mathematical Methods In Social Sciences	900AX	WOS:000227188900003		
J	Peng, XL; Hu, QN; Liang, YZ				Peng, XL; Hu, QN; Liang, YZ			Variable selection via nonconcave penalty function in structure-boiling points correlations	JOURNAL OF MOLECULAR STRUCTURE-THEOCHEM			English	Article						boiling points; penalized least squares; cross-validation; topological index	TOPOLOGICAL INDEXES; MOLECULAR GRAPHS; WIENER INDEX; VERTEX INVARIANTS; AUTO-CORRELATION; CHEMICAL GRAPHS; DETOUR INDEX; REGRESSION; DESCRIPTORS; SHRINKAGE	A new variable selection approach based on nonconcave penalized least squares is employed for interpretation and prediction of boiling points (bps) of 530 alkanes. The good performances of the proposed method, compared with stepwise regression and the improved least absolute shrinkage and selection operator (LASSO), along with its simplicity and fast speed, makes it a valid competitor to the existing variable selection methods. All the 530 saturated hydrocarbons with carbon numbers from 2 to 10 and 128 common topological indices are taken into account. As a result. only 12 topological indices (Tls) are selected from 95 pretreated ones but they still present a satisfying fitting and prediction effects. (C) 2004 Elsevier B.V. All rights reserved.	Hong Kong Baptist Univ, Dept Math, Kowloon, Hong Kong, Peoples R China; Sichuan Univ, Coll Math, Chengdu 610064, Peoples R China; Cent S Univ, Coll Chem & Chem Engn, Changsha 410083, Peoples R China	Peng, XL (reprint author), Hong Kong Baptist Univ, Dept Math, Kowloon, Hong Kong, Peoples R China.	01400908@hkbu.edu.hk					RANDIC M, 1975, J AM CHEM SOC, V97, P6609, DOI 10.1021/ja00856a001; Fu WJJ, 1998, J COMPUT GRAPH STAT, V7, P397, DOI 10.2307/1390712; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Ivanciuc O, 1998, J CHEM INF COMP SCI, V38, P395, DOI 10.1021/ci970021l; MASSY WF, 1965, J AM STAT ASSOC, V60, P234, DOI 10.2307/2283149; HOERL AE, 1970, TECHNOMETRICS, V12, P55; HOSOYA H, 1971, B CHEM SOC JPN, V44, P2332, DOI 10.1246/bcsj.44.2332; WIENER H, 1947, J AM CHEM SOC, V69, P17, DOI 10.1021/ja01193a005; DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425; Balaban A. T., 1999, TOPOLOGICAL INDICES, P21; BALABAN AT, 1992, J CHIM PHYS PCB, V89, P1735; BALABAN AT, 1982, CHEM PHYS LETT, V89, P399, DOI 10.1016/0009-2614(82)80009-2; BALABAN AT, 1991, J MATH CHEM, V8, P383, DOI 10.1007/BF01166951; BALABAN AT, 1979, THEOR CHIM ACTA, V5, P239; BALABAN AT, 1991, J CHEM INF COMP SCI, V31, P517, DOI 10.1021/ci00004a014; BONCHEV D, 1977, J CHEM PHYS, V67, P4517, DOI 10.1063/1.434593; Breiman L, 1996, ANN STAT, V24, P2350; BREIMAN L, 1992, INT STAT REV, V60, P291, DOI 10.2307/1403680; Fan J., 1997, J ITALIAN STAT ASS, V6, P131; FAN J, 2001, J AM STAT ASSOC, P1348; Filip PA, 1987, J MATH CHEM, V1, P61, DOI 10.1007/BF01205338; GRAVEN P, 1979, NUMER MATH, V31, P377; GUTMAN I, 1975, J CHEM PHYS, V62, P3399, DOI 10.1063/1.430994; Hall L. H., 1999, TOPOLOGICAL INDICES, P491; Hall LH, 1999, MOL STRUCTURE DESCRI; HU QN, 2003, COMPUT APPL CHEM, V20, P486; Hu Q.N., 2003, J DATA SCI, V1, P361; IVANCIUC O, 1999, REV ROUM CHIM, P44; Kier L. B., 1976, MOL CONNECTIVITY CHE; KIER LB, 1985, QUANT STRUCT-ACT REL, V4, P109, DOI 10.1002/qsar.19850040303; KIER LB, 1986, QUANT STRUCT-ACT REL, V5, P1, DOI 10.1002/qsar.19860050102; KIER LB, 1990, PHARMACEUT RES, V7, P801, DOI 10.1023/A:1015952613760; KLEIN DJ, 1995, J CHEM INF COMP SCI, V35, P50, DOI 10.1021/ci00023a007; Liu SS, 1998, J CHEM INF COMP SCI, V38, P387, DOI 10.1021/ci970109z; LIU SS, 2001, J CHEM INF COMP SCI, V41, P21; Liu SS, 2001, J CHIN CHEM SOC-TAIP, V48, P253; Lovasz L., 1973, PERIOD MATH HUNG, V3, P175, DOI 10.1007/BF02018473; Lukovits I, 1996, CROAT CHEM ACTA, V69, P873; Lukovits I, 1997, J CHEM INF COMP SCI, V37, P283, DOI 10.1021/ci960034j; Lukovits I, 1998, J CHEM INF COMP SCI, V38, P125, DOI 10.1021/ci9700541; MOHAR B, 1993, J CHEM INF COMP SCI, V33, P153, DOI 10.1021/ci00011a023; MOREAU G, 1980, NOUV J CHIM, V4, P757; MOREAU G, 1980, NOUV J CHIM, V4, P359; NEEDHAM DE, 1988, J AM CHEM SOC, V110, P4186, DOI 10.1021/ja00221a015; Ojelund H, 2001, J CHEMOMETR, V15, P497; PLAVSIC D, 1993, J MATH CHEM, V12, P235, DOI 10.1007/BF01164638; RANDIC M, 1977, J CHEM INF COMP SCI, V17, P171, DOI 10.1021/ci60011a013; Randic M, 2001, J CHEM INF COMP SCI, V41, P607, DOI 10.1021/ci0001031; RANDIC M, 1979, COMPUT CHEM, V3, P65; Rucker G, 1999, J CHEM INF COMP SCI, V39, P788, DOI 10.1021/ci9900175; RUCKER G, 1993, J CHEM INF COMP SCI, V33, P683; SCHULTZ HP, 1989, J CHEM INF COMP SCI, V29, P227, DOI 10.1021/ci00063a012; Tibshirani R, 1997, STAT MED, V16, P385, DOI 10.1002/(SICI)1097-0258(19970228)16:4<385::AID-SIM380>3.0.CO;2-3; WOLD H, 1975, SOFTMODELING LATENT; YANG YQ, 1994, J CHEM INF COMP SCI, V34, P1140, DOI 10.1021/ci00021a020	55	1	1	1	1	ELSEVIER SCIENCE BV	AMSTERDAM	PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS	0166-1280			J MOL STRUC-THEOCHEM	Theochem-J. Mol. Struct.	FEB 14	2005	714	2-3					235	242		10.1016/j.theochem.2004.11.016		8	Chemistry, Physical	Chemistry	898WC	WOS:000227105900021		
J	Ni, LQ; Cook, RD; Tsai, CL				Ni, LQ; Cook, RD; Tsai, CL			A note on shrinkage sliced inverse regression	BIOMETRIKA			English	Article						Garotte; Lasso; shrinkage estimator; sliced inverse regression; sufficient dimension reduction	SUFFICIENT DIMENSION REDUCTION; PRINCIPAL HESSIAN DIRECTIONS; BINARY RESPONSE; LASSO; MODEL; SELECTION	We employ Lasso shrinkage within the context of sufficient dimension reduction to obtain a shrinkage sliced inverse regression estimator, which provides easier interpretations and better prediction accuracy without assuming a parametric model. The shrinkage sliced inverse regression approach can be employed for both single-index and multiple-index models. Simulation studies suggest that the new estimator performs well when its tuning parameter is selected by either the Bayesian information criterion or the residual information criterion.	Univ Cent Florida, Dept Stat & Actuarial Sci, Orlando, FL 32816 USA; Univ Minnesota, Sch Stat, Minneapolis, MN 55455 USA; Univ Calif Davis, Grad Sch Management, Davis, CA 95616 USA	Ni, LQ (reprint author), Univ Cent Florida, Dept Stat & Actuarial Sci, Orlando, FL 32816 USA.	lni@mail.ucf.edu; dennis@stat.umn.edu; cltsai@ucdavis.edu					Akaike H., 1973, 2 INT S INF THEOR, P267; BREIMAN L, 1995, TECHNOMETRICS, V37, P373, DOI 10.2307/1269730; Fu WJJ, 1998, J COMPUT GRAPH STAT, V7, P397, DOI 10.2307/1390712; Bura E, 2001, J AM STAT ASSOC, V96, P996, DOI 10.1198/016214501753208979; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Cook RD, 1998, J AM STAT ASSOC, V93, P84; SCHOTT JR, 1994, J AM STAT ASSOC, V89, P141, DOI 10.2307/2291210; SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136; Chiaromonte F, 2002, ANN STAT, V30, P475, DOI 10.1214/aos/1021379862; COOK R. D., 1998, REGRESSION GRAPHICS; COOK RD, 1991, J AM STAT ASSOC, V86, P328, DOI 10.2307/2290564; Cook RD, 1996, J AM STAT ASSOC, V91, P983, DOI 10.2307/2291717; Cook RD, 2004, ANN STAT, V32, P1062, DOI 10.1214/009053604000000292; Cook RD, 1999, J AM STAT ASSOC, V94, P1187, DOI 10.2307/2669934; LI KC, 1992, J AM STAT ASSOC, V87, P1025, DOI 10.2307/2290640; LI KC, 1991, J AM STAT ASSOC, V86, P316, DOI 10.2307/2290563; Naik PA, 2001, BIOMETRIKA, V88, P821, DOI 10.1093/biomet/88.3.821; Osborne MR, 2000, J COMPUT GRAPH STAT, V9, P319, DOI 10.2307/1390657; Shi PD, 2002, J ROY STAT SOC B, V64, P237, DOI 10.1111/1467-9868.00335	19	29	30	0	2	BIOMETRIKA TRUST	LONDON	UNIV COLLEGE LONDON GOWER ST-BIOMETRIKA OFFICE, LONDON WC1E 6BT, ENGLAND	0006-3444			BIOMETRIKA	Biometrika	MAR	2005	92	1					242	247		10.1093/biomet/92.1.242		6	Biology; Mathematical & Computational Biology; Statistics & Probability	Life Sciences & Biomedicine - Other Topics; Mathematical & Computational Biology; Mathematics	912SH	WOS:000228099300019		
J	Hammer, B; Strickert, M; Villmann, T				Hammer, B; Strickert, M; Villmann, T			On the generalization ability of GRLVQ networks	NEURAL PROCESSING LETTERS			English	Article						adaptive metric; generalization bounds; LVQ; margin optimization; relevance LVQ	LEARNING VECTOR QUANTIZATION; SELF-ORGANIZING MAPS; SELECTION	We derive a generalization bound for prototype-based classifiers with adaptive metric. The bound depends on the margin of the classifier and is independent of the dimensionality of the data. It holds for classifiers based on the Euclidean metric extended by adaptive relevance terms. In particular, the result holds for relevance learning vector quantization (RLVQ) [4] and generalized relevance learning vector quantization (GRLVQ) [19].	Univ Osnabruck, Dept Math Comp Sci, LNM, D-4500 Osnabruck, Germany; Univ Leipzig, Clin Psychotherapy, Leipzig, Germany	Hammer, B (reprint author), Univ Osnabruck, Dept Math Comp Sci, LNM, D-4500 Osnabruck, Germany.	hammer@informatik.uni-osnabrueck.de; marc@informatik.uni-osnabrueck.de; villmann@informatik.uni-leipzig.de	Hammer, Barbara /E-8624-2010				ANGUITA D, 2002, ICANN 2002, P1345; VAPNIK VN, 1971, THEOR PROBAB APPL+, V16, P264, DOI 10.1137/1116025; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Hammer B, 2002, NEURAL NETWORKS, V15, P1059, DOI 10.1016/S0893-6080(02)00079-5; Villmann T, 2003, NEURAL NETWORKS, V16, P389, DOI 10.1016/S0893-6080(03)00021-2; Tipping ME, 2001, J MACH LEARN RES, V1, P211, DOI 10.1162/15324430152748236; Bartlett P. L., 2002, J MACHINE LEARNING R, V3, P463; Bezdek J.C., 1981, PATTERN RECOGNITION; Bojer T., 2001, P EUR S ART NEUR NET, P271; BOUSQUET O, IN PRESS ADV NEURAL; Cherkassky V, 1996, IEEE T NEURAL NETWOR, V7, P969, DOI 10.1109/72.508939; Cortes C., 1995, MACH LEARN, V20, P1; CRAMMER K, IN PRESS ADV NEURAL; DAVE RN, 1990, INT J GEN SYST, V16, P343, DOI 10.1080/03081079008935087; GATH I, 1989, IEEE T PATTERN ANAL, V11, P773, DOI 10.1109/34.192473; Grandvalet Y, 2000, IEEE T NEURAL NETWOR, V11, P1201, DOI 10.1109/72.883393; Gunter S., 2002, PATTERN RECOGN, V23, P401; Gustafson E., 1979, IEEE CDC, P761; HAMMER B, IN PRESS NEURAL PROC; HAMMER B, 2004, IN PRESS BIOINFORMAT; Hammer B., 2002, ICANN 2002, P877; Heskes T, 1999, KOHONEN MAPS, P303, DOI 10.1016/B978-044450270-4/50024-3; Kaski S, 2001, ADVANCES IN SELF-ORGANISING MAPS, P224; Kaski S, 2001, IEEE T NEURAL NETWOR, V12, P936, DOI 10.1109/72.935102; Kohonen T., 1995, HDB BRAIN THEORY NEU, P537; Kohonen T, 1997, SELF ORGANIZING MAPS; Kohonen T, 2002, NEURAL NETWORKS, V15, P945, DOI 10.1016/S0893-6080(02)00069-2; Marchand M., 2002, J MACHINE LEARNING R, V3, P723; Michie D., 1994, MACHINE LEARNING NEU; MULIER F, 1994, THESIS U MINNESOTA M; Patane G, 2001, NEURAL NETWORKS, V14, P1219, DOI 10.1016/S0893-6080(01)00104-6; Pregenzer M, 1996, NEUROCOMPUTING, V11, P19, DOI 10.1016/0925-2312(94)00071-9; Ridella S, 2001, IEEE T NEURAL NETWOR, V12, P371, DOI 10.1109/72.914531; SATO AS, 1998, ICANN 97, P172; Sato A.S., 1995, ADV NEURAL INFORMATI, V7, P423; Seo S, 2003, NEURAL COMPUT, V15, P1589, DOI 10.1162/089976603321891819; Strickert M, 2001, LECT NOTES COMPUT SC, V2130, P677; VANGESTEL T, 2001, EUR S ART NEUR NETW, P13; Vapnik V., 1998, STAT LEARNING THEORY; VILLMANN T, 2003, METRIC ADAPTATION RE	40	29	32	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	1370-4621			NEURAL PROCESS LETT	Neural Process. Lett.	APR	2005	21	2					109	120		10.1007/s11063-004-1547-1		12	Computer Science, Artificial Intelligence	Computer Science	913ZC	WOS:000228193200003		
J	Wu, BL				Wu, BL			Differential gene expression detection using penalized linear regression models: the improved SAM statistics	BIOINFORMATICS			English	Article							CDNA MICROARRAY; NORMALIZATION; DISCOVERY; CANCER	Differential gene expression detection using microarrays has received lots of research interests recently. Many methods have been proposed, including variants of F-statistics, non-parametric approaches and empirical Bayesian methods etc. The SAM statistics has been shown to have good performance in empirical studies. SAM is more like an ad hoc shrinkage method. The idea is that for small sample microarray data, it is often useful to pool information across genes to improve efficiency. Under Bayesian framework Smyth formally derived the test statistics with shrinkage using the hierarchical models. In this paper we cast differential gene expression detection in the familiar framework of linear regression model. Commonly used test statistics correspond to using least squares to estimate the regression parameters. Based on the vast literature of research on linear models, we can naturally consider other alternatives. Here we explore the penalized linear regression. We propose the penalized t-/F-statistics for two-class microarray data based on L-1 penalty. We will show that the penalized test statistics intuitively makes sense and through applications we illustrate its good performance.	Univ Minnesota, Sch Publ Hlth, Div Biostat, Minneapolis, MN 55455 USA	Wu, BL (reprint author), Univ Minnesota, Sch Publ Hlth, Div Biostat, A460 Mayo Bldg,MMC 303, Minneapolis, MN 55455 USA.	baolin@biostat.umn.edu					DeRisi J, 1996, NAT GENET, V14, P457; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Lockhart DJ, 1996, NAT BIOTECHNOL, V14, P1675, DOI 10.1038/nbt1296-1675; Tusher VG, 2001, P NATL ACAD SCI USA, V98, P5116, DOI 10.1073/pnas.091062498; Golub TR, 1999, SCIENCE, V286, P531, DOI 10.1126/science.286.5439.531; Pan W, 2002, BIOINFORMATICS, V18, P546, DOI 10.1093/bioinformatics/18.4.546; Bolstad BM, 2003, BIOINFORMATICS, V19, P185, DOI 10.1093/bioinformatics/19.2.185; Dudoit S, 2002, STAT SINICA, V12, P111; BENJAMINI Y, 1995, J ROY STAT SOC B MET, V57, P289; Efron B, 2004, ANN STAT, V32, P407; Efron B, 2001, J AM STAT ASSOC, V96, P1151, DOI 10.1198/016214501753382129; Yang YH, 2002, NUCLEIC ACIDS RES, V30, DOI 10.1093/nar/30.4.e15; Hochberg Y., 1987, MULTIPLE COMP PROCED; IHAKA R, 1996, J COMPUTATIONAL GRAP, V5, P299, DOI DOI 10.2307/1390807; Pan W, 2003, BIOINFORMATICS, V19, P1333, DOI 10.1093/bioinformatics/btg167; Smyth G. K., 2004, STAT APPL GENET MOL, V3, DOI [10.2202/1544-6115.1027, DOI 10.2202/1544-6115.1027]; Smyth Gordon K, 2003, Methods Mol Biol, V224, P111; SMYTH GK, 2003, FUNCTIONAL GENOMICS; STOREY JD, 2003, J R STATR SOC B, V64, P479; Westfall P. H., 1993, RESAMPLING BASED MUL	20	26	27	1	3	OXFORD UNIV PRESS	OXFORD	GREAT CLARENDON ST, OXFORD OX2 6DP, ENGLAND	1367-4803			BIOINFORMATICS	Bioinformatics	APR 15	2005	21	8					1565	1571		10.1093/bioinformatics/bti217		7	Biochemical Research Methods; Biotechnology & Applied Microbiology; Computer Science, Interdisciplinary Applications; Mathematical & Computational Biology; Statistics & Probability	Biochemistry & Molecular Biology; Biotechnology & Applied Microbiology; Computer Science; Mathematical & Computational Biology; Mathematics	916QX	WOS:000228401800041	15598833	
J	Wang, H; Zhang, YM; Li, XM; Masinde, GL; Mohan, S; Baylink, DJ; Xu, SZ				Wang, H; Zhang, YM; Li, XM; Masinde, GL; Mohan, S; Baylink, DJ; Xu, SZ			Bayesian shrinkage estimation of quantitative trait loci parameters	GENETICS			English	Article							CHAIN MONTE-CARLO; RIDGE-REGRESSION; MODEL SELECTION; GENETIC VALUE; IDENTIFICATION; PREDICTION; FRAMEWORK; GENOME; MAPS	Mapping multiple QTL is a typical problem of variable selection in an oversaturated model because the potential number of QTL can be substantially larger than the sample size. Currently, model selection is still the most effective approach to mapping multiple QTL, although further research is needed. An alternative approach to analyzing an oversaturated model is the shrinkage estimation in which all candidate variables are included in the model but their estimated effects are forced to shrink toward zero. In contrast to the usual shrinkage estimation where all model effects are shrunk by the same factor, we develop a Bayesian method that allows the shrinkage factor to vary across different effects. The new shrinkage method forces marker intervals that contain no QTL to have estimated effects close to zero whereas intervals containing notable QTL have estimated effects subject to virtually no shrinkage. We demonstrate the method using both simulated and real data for QTL mapping. A simulation experiment with 500 backcross (BC) individuals showed that the method can localize closely linked QTL and QTL with effects as small as 1% of the phenotypic variance of the trait. The method was also used to map QTL responsible for wound healing in a family of a (MRL/MPJ x SJL/J) cross with 633 F-2 mice derived from two inbred lines.	Univ Calif Riverside, Dept Bot & Plant Sci, Riverside, CA 92521 USA; Univ Chicago, Funct Genom Facil, Chicago, IL 60637 USA; Jerry L Pettis Mem Vet Adm Med Ctr, Div Mol Genet, Loma Linda, CA 92357 USA; Loma Linda Univ, Loma Linda, CA 92357 USA	Xu, SZ (reprint author), Univ Calif Riverside, Dept Bot & Plant Sci, 900 Univ Ave, Riverside, CA 92521 USA.	xu@genetics.ucr.edu					BREIMAN L, 1995, TECHNOMETRICS, V37, P373, DOI 10.2307/1269730; Yano M, 1997, THEOR APPL GENET, V95, P1025, DOI 10.1007/s001220050658; KASS RE, 1995, J AM STAT ASSOC, V90, P773, DOI 10.1080/01621459.1995.10476572; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; HASTINGS WK, 1970, BIOMETRIKA, V57, P97, DOI 10.2307/2334940; Whittaker JC, 2000, GENET RES, V75, P249, DOI 10.1017/S0016672399004462; LANDER ES, 1989, GENETICS, V121, P185; Kao CH, 1999, GENETICS, V152, P1203; ZENG ZB, 1994, GENETICS, V136, P1457; Heath SC, 1997, AM J HUM GENET, V61, P748, DOI 10.1086/515506; Meuwissen THE, 2001, GENETICS, V157, P1819; Ball RD, 2001, GENETICS, V159, P1351; Basten CJ, 2002, QTL CARTOGRAPHER; Broman KW, 2002, J R STAT SOC B, V64, P641, DOI 10.1111/1467-9868.00354; Bunyamin T., 2002, CROP SCI, V42, P544; Gelman A., 2004, BAYESIAN DATA ANAL; George EI, 1993, J AM STAT ASSOC, V91, P883; Gianola D, 2003, GENETICS, V163, P347; Green PJ, 1995, BIOMETRIKA, V82, P711, DOI 10.1093/biomet/82.4.711; HOERL AE, 1970, TECHNOMETRICS, V12, P55, DOI DOI 10.1080/00401706.1970.10488634; Hunt GJ, 1999, J HERED, V90, P585, DOI 10.1093/jhered/90.5.585; JANSEN RC, 1994, GENETICS, V136, P1447; JANSEN RC, 1993, GENETICS, V135, P205; Kopp A, 2003, GENETICS, V163, P771; Masinde GL, 2001, GENOME RES, V11, P2027, DOI 10.1101/gr.203701; METROPOLIS N, 1953, J CHEM PHYS, V21, P1087, DOI 10.1063/1.1699114; Piepho HP, 2001, GENETICS, V157, P433; Satagopan JM, 1996, GENETICS, V144, P805; Sauerbrei W, 1999, APPL STAT, V48, P313; Sen S, 2001, GENETICS, V159, P371; Sillanpaa MJ, 1998, GENETICS, V148, P1373; Xu SZ, 2003, GENETICS, V163, P789; Yi NJ, 2004, GENETICS, V167, P967, DOI 10.1534/genetics.104.026286; Yi NJ, 2003, GENETICS, V164, P1129	34	96	106	1	8	GENETICS SOC AM	BETHESDA	9650 ROCKVILLE AVE, BETHESDA, MD 20814 USA	1943-2631			GENETICS	Genetics	MAY	2005	170	1					465	480		10.1534/genetics.104.039354		16	Genetics & Heredity	Genetics & Heredity	933ZY	WOS:000229677500042	15781696	
J	Fu, WJ				Fu, WJ			Nonlinear GCV and quasi-GCV for shrinkage models	JOURNAL OF STATISTICAL PLANNING AND INFERENCE			English	Article						lasso; longitudinal studies; ridge; standard shrinkage rate; weighted deviance	GENERALIZED CROSS-VALIDATION; LONGITUDINAL DATA-ANALYSIS; AIR-POLLUTION; SELECTION; LASSO; ADMISSIONS; CHILDREN; ASTHMA	The generalized cross-validation (GCV) method has been a popular technique for the selection of tuning parameters for smoothing and penalty, and has been a standard tool to select tuning parameters for shrinkage models in recent works. Its computational ease and robustness compared to the cross-validation method makes it competitive for model selection as well. It is well known that the GCV method performs well for linear estimators, which are linear functions of the response variable, such as ridge estimator. However, it may not perform well for nonlinear estimators since the GCV emphasizes linear characteristics by taking the trace of the projection matrix. This paper aims to explore the GCV for nonlinear estimators and to further extend the results to correlated data in longitudinal studies. We expect that the nonlinear GCV and quasi-GCV developed in this paper will provide similar tools for the selection of tuning parameters in linear penalty models and penalized GEE models. (C) 2004 Elsevier B.V. All rights reserved.	Texas A&M Univ, Dept Stat, College Stn, TX 77843 USA; Michigan State Univ, Dept Epidemiol, E Lansing, MI 48823 USA	Fu, WJ (reprint author), Texas A&M Univ, Dept Stat, 44 Blocker Bldg, College Stn, TX 77843 USA.	wfu@stat.tamu.edu					BREIMAN L, 1995, TECHNOMETRICS, V37, P373, DOI 10.2307/1269730; Fu WJJ, 1998, J COMPUT GRAPH STAT, V7, P397, DOI 10.2307/1390712; LIANG KY, 1986, BIOMETRIKA, V73, P13, DOI 10.1093/biomet/73.1.13; Knight K, 2000, ANN STAT, V28, P1356; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; FRANK IE, 1993, TECHNOMETRICS, V35, P109, DOI 10.2307/1269656; SHAO J, 1993, J AM STAT ASSOC, V88, P486, DOI 10.2307/2290328; GOLUB GH, 1979, TECHNOMETRICS, V21, P215, DOI 10.1080/00401706.1979.10489751; Norris G, 1999, ENVIRON HEALTH PERSP, V107, P489, DOI 10.2307/3434632; Anderson HR, 1998, THORAX, V53, P842; CRAVEN P, 1979, NUMER MATH, V31, P377; Efron B, 1993, INTRO BOOTSTRAP; EFRON B, 2002, LEAST ANGEL REGRESSI; Fu WJJ, 2000, COMMUN STAT-THEOR M, V29, P263, DOI 10.1080/03610920008832483; FU WJ, 2003, EFFECTIVE NUMBER OBS; Fu WJJ, 2003, BIOMETRICS, V59, P126, DOI 10.1111/1541-0420.00015; Gu C, 2001, J COMPUT GRAPH STAT, V10, P581, DOI 10.1198/106186001317114992; Hastie T., 1990, GENERALIZED ADDITIVE; Lin XW, 2000, ANN STAT, V28, P1570; McCullagh P., 1989, GENERALIZED LINEAR M, VSecond; Shao J, 1997, STAT SINICA, V7, P221; Stamey T., 1989, J UROLOGY, V16, P1076; TIBSHIRANI R, 2003, SPARSITY SMOOTHNESS; Wong GWK, 2001, CLIN EXP ALLERGY, V31, P565, DOI 10.1046/j.1365-2222.2001.01063.x; Xiang D, 1996, STAT SINICA, V6, P675; ZEGER SL, 1986, BIOMETRICS, V42, P121, DOI 10.2307/2531248	27	7	9	0	2	ELSEVIER SCIENCE BV	AMSTERDAM	PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS	0378-3758			J STAT PLAN INFER	J. Stat. Plan. Infer.	MAY 1	2005	131	2					333	347		10.1016/j.jspi.2004.03.001		15	Statistics & Probability	Mathematics	904MO	WOS:000227502600007		
J	Gneiting, T; Raftery, AE; Westveld, AH; Goldman, T				Gneiting, T; Raftery, AE; Westveld, AH; Goldman, T			Calibrated probabilistic forecasting using ensemble model output statistics and minimum CRPS estimation	MONTHLY WEATHER REVIEW			English	Article							PREDICTION SYSTEM; SEASONAL CLIMATE; PRECIPITATION FORECASTS; PACIFIC-NORTHWEST; WEATHER; SKILL; SPREAD; SUPERENSEMBLE; DISTRIBUTIONS; VERIFICATION	Ensemble prediction systems typically show positive spread-error correlation, but they are subject to forecast bias and dispersion errors, and are therefore uncalibrated. This work proposes the use of ensemble model output statistics (EMOS), an easy-to-implement postprocessing technique that addresses both forecast bias and underdispersion and takes into account the spread-skill relationship. The technique is based on multiple linear regression and is akin to the superensemble approach that has traditionally been used for deterministic-style forecasts. The EMOS technique yields probabilistic forecasts that take the form of Gaussian predictive probability density functions (PDFs) for continuous weather variables and can be applied to gridded model output. The EMOS predictive mean is a bias-corrected weighted average of the ensemble member forecasts, with coefficients that can be interpreted in terms of the relative contributions of the member models to the ensemble, and provides a highly competitive deterministic-style forecast. The EMOS predictive variance is a linear function of the ensemble variance. For fitting the EMOS coefficients, the method of minimum continuous ranked probability score (CRPS) estimation is introduced. This technique finds the coefficient values that optimize the CRPS for the training data. The EMOS technique was applied to 48-h forecasts of sea level pressure and surface temperature over the North American Pacific Northwest in spring 2000, using the University of Washington mesoscale ensemble. When compared to the bias-corrected ensemble, deterministic-style EMOS forecasts of sea level pressure had root-mean-square error 9% less and mean absolute error 7% less. The EMOS predictive PDFs were sharp, and much better calibrated than the raw ensemble or the bias-corrected ensemble.	Univ Washington, Dept Stat, Seattle, WA 98195 USA	Gneiting, T (reprint author), Univ Washington, Dept Stat, Box 354320, Seattle, WA 98195 USA.	tilmann@stat.washington.edu	Gneiting, Tilmann/A-8773-2009	Gneiting, Tilmann/0000-0001-9397-3271			Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Hamill TM, 1997, MON WEATHER REV, V125, P1312, DOI 10.1175/1520-0493(1997)125<1312:VOERSR>2.0.CO;2; Raftery AE, 2005, MON WEATHER REV, V133, P1155, DOI 10.1175/MWR2906.1; Krishnamurti TN, 1999, SCIENCE, V285, P1548, DOI 10.1126/science.285.5433.1548; Hamill TM, 2001, MON WEATHER REV, V129, P550, DOI 10.1175/1520-0493(2001)129<0550:IORHFV>2.0.CO;2; ROSENBLATT M, 1952, ANN MATH STAT, V23, P470, DOI 10.1214/aoms/1177729394; Hersbach H, 2000, WEATHER FORECAST, V15, P559, DOI 10.1175/1520-0434(2000)015<0559:DOTCRP>2.0.CO;2; HUBER PJ, 1964, ANN MATH STAT, V35, P73, DOI 10.1214/aoms/1177703732; PFANZAGL J, 1969, METRIKA, V14, P249, DOI 10.1007/BF02613654; Anderson JL, 1996, J CLIMATE, V9, P1518, DOI 10.1175/1520-0442(1996)009<1518:AMFPAE>2.0.CO;2; Atger F, 2003, MON WEATHER REV, V131, P1509, DOI 10.1175//1520-0493(2003)131<1509:SAIVOT>2.0.CO;2; BIRGE L, 1993, PROBAB THEORY REL, V97, P113, DOI 10.1007/BF01199316; DAWID AP, 1984, J ROY STAT SOC A STA, V147, P278, DOI 10.2307/2981683; DEQUE M, 1994, TELLUS A, V46, P52, DOI 10.1034/j.1600-0870.1994.00005.x; Eckel FA, 1998, WEATHER FORECAST, V13, P1132, DOI 10.1175/1520-0434(1998)013<1132:CPQPFB>2.0.CO;2; ECKEL FA, 2003, THESIS U WASHINGTON; Gel Y, 2004, J AM STAT ASSOC, V99, P575, DOI 10.1198/016214504000000872; Glahn H. R., 1972, Journal of Applied Meteorology, V11, DOI 10.1175/1520-0450(1972)011<1203:TUOMOS>2.0.CO;2; GNEITING T, 2004, 463 U WASH DEP STAT; GNEITING T, 2003, P WORKSH ENS WEATH F; GOOD IJ, 1952, J ROY STAT SOC B, V14, P107; Grimit EP, 2002, WEATHER FORECAST, V17, P192, DOI 10.1175/1520-0434(2002)017<0192:IROAMS>2.0.CO;2; GRIMIT EP, 2004, 16 C NUM WEATH PRED; Hamill TM, 1998, MON WEATHER REV, V126, P711, DOI 10.1175/1520-0493(1998)126<0711:EOEREP>2.0.CO;2; Hamill TM, 2004, MON WEATHER REV, V132, P1434, DOI 10.1175/1520-0493(2004)132<1434:ERIMFS>2.0.CO;2; Houtekamer PL, 1996, MON WEATHER REV, V124, P1225, DOI 10.1175/1520-0493(1996)124<1225:ASSATE>2.0.CO;2; Huber P. J., 1981, ROBUST STAT; Jewson S, 2004, ATMOS SCI LETT, V5, P96, DOI 10.1002/asl.69; Kharin VV, 2002, J CLIMATE, V15, P793, DOI 10.1175/1520-0442(2002)015<0793:CPWME>2.0.CO;2; Krishnamurti TN, 2000, J CLIMATE, V13, P4196, DOI 10.1175/1520-0442(2000)013<4196:MEFFWA>2.0.CO;2; Mass CF, 2003, WEATHER FORECAST, V18, P75, DOI 10.1175/1520-0434(2003)018<0075:IATFOT>2.0.CO;2; Mass CF, 2003, B AM METEOROL SOC, V84, P1353, DOI 10.1175/BAMS-84-10-1353; McCullagh P., 1989, GEN LINEAR MODELS; Molteni F, 1996, Q J ROY METEOR SOC, V122, P73, DOI 10.1002/qj.49712252905; MURPHY AH, 1979, B AM METEOROL SOC, V60, P12, DOI 10.1175/1520-0477(1979)060<0012:PTFTCF>2.0.CO;2; Press W H, 1992, NUMERICAL RECIPES FO; Richardson DS, 2001, Q J ROY METEOR SOC, V127, P2473, DOI 10.1002/qj.49712757715; Roulston MS, 2002, MON WEATHER REV, V130, P1653, DOI 10.1175/1520-0493(2002)130<1653:EPFUIT>2.0.CO;2; Roulston MS, 2003, TELLUS A, V55, P16, DOI 10.1034/j.1600-0870.2003.201378.x; Saetra O, 2004, MON WEATHER REV, V132, P1487, DOI 10.1175/1520-0493(2004)132<1487:EOOEOT>2.0.CO;2; Scherrer SC, 2004, WEATHER FORECAST, V19, P552, DOI 10.1175/1520-0434(2004)019<0552:AOTSRU>2.0.CO;2; Stefanova L, 2002, J CLIMATE, V15, P537, DOI 10.1175/1520-0442(2002)015<0537:IOSCFU>2.0.CO;2; Stensrud DJ, 2003, MON WEATHER REV, V131, P2510, DOI 10.1175/1520-0493(2003)131<2510:SEPOMT>2.0.CO;2; Talagrand O., 1997, P ECMWF WORKSH PRED, P1; Toth Z, 1997, MON WEATHER REV, V125, P3297, DOI 10.1175/1520-0493(1997)125<3297:EFANAT>2.0.CO;2; Toth Z, 2003, FORECAST VERIFICATIO, P137; Unger D. A., 1985, 9 C PROB STAT ATM SC, P206; VANDENDOOL HM, 1994, WEATHER FORECAST, V9, P457, DOI 10.1175/1520-0434(1994)009<0457:OTWFAE>2.0.CO;2; Weigend AS, 2000, J FORECASTING, V19, P375, DOI 10.1002/1099-131X(200007)19:4<375::AID-FOR779>3.3.CO;2-L; Whitaker JS, 1998, MON WEATHER REV, V126, P3292, DOI 10.1175/1520-0493(1998)126<3292:TRBESA>2.0.CO;2; Wilks DS, 2002, Q J ROY METEOR SOC, V128, P2821, DOI 10.1256/qj.01.215; Wilks D. S., 1995, STAT METHODS ATMOSPH; Wilson LJ, 1999, MON WEATHER REV, V127, P956, DOI 10.1175/1520-0493(1999)127<0956:ASFVOW>2.0.CO;2	53	157	159	7	25	AMER METEOROLOGICAL SOC	BOSTON	45 BEACON ST, BOSTON, MA 02108-3693 USA	0027-0644			MON WEATHER REV	Mon. Weather Rev.	MAY	2005	133	5					1098	1118		10.1175/MWR2904.1		21	Meteorology & Atmospheric Sciences	Meteorology & Atmospheric Sciences	930CH	WOS:000229392900004		
J	Li, RZ; Sudjianto, A				Li, RZ; Sudjianto, A			Analysis of computer experiments using penalized likelihood in Gaussian kriging models	TECHNOMETRICS			English	Article						computer experiment; Fisher scoring algorithm; kriging; meta-model; penalized likelihood; smoothly clipped absolute deviation	REGRESSION; SELECTION; DESIGN	Kriging is a popular analysis approach for computer experiments for the purpose of creating a cheap-to-compute "meta-model" as a surrogate to a computationally expensive engineering simulation model. The maximum likelihood approach is used to estimate the parameters in the kriging model. However, the likelihood function near the optimum may be flat in some situations, which leads to maximum likelihood estimates for the parameters in the covariance matrix that have very large variance. To overcome this difficulty, a penalized likelihood approach is proposed for the kriging model. Both theoretical analysis and empirical experience using real world data suggest that the proposed method is particularly important in the context of a computationally intensive simulation model where the number of simulation runs must be kept small because collection of a large sample set is prohibitive. The proposed approach is applied to the reduction of piston slap, an unwanted engine noise due to piston secondary motion. Issues related to practical implementation of the proposed approach are discussed.	Penn State Univ, Dept Stat, University Pk, PA 16802 USA; Bank Amer, Risk Management Qual & Prod, Charlotte, NC 28255 USA	Li, RZ (reprint author), Penn State Univ, Dept Stat, University Pk, PA 16802 USA.	rli@stat.psu.edu; agus.sudjianto@bankofamerica.com	Li, Runze/C-5444-2013	Li, Runze/0000-0002-0154-2202			AKAIKE H, 1974, IEEE T AUTOMAT CONTR, VAC19, P716, DOI 10.1109/TAC.1974.1100705; Ruppert D, 2002, J COMPUT GRAPH STAT, V11, P735, DOI 10.1198/106186002853; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; Booker AJ, 1999, STRUCT OPTIMIZATION, V17, P1; [Anonymous], 1996, HANDB STAT, DOI 10.1016/S0169-7161(96)13011-X; Fang KT, 2000, TECHNOMETRICS, V42, P237, DOI 10.2307/1271079; WELCH WJ, 1992, TECHNOMETRICS, V34, P15, DOI 10.2307/1269548; Meckesheimer M, 2002, AIAA J, V40, P2053, DOI 10.2514/2.1538; SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136; Kalagnanam JR, 1997, TECHNOMETRICS, V39, P308, DOI 10.2307/1271135; CURRIN C, 1991, J AM STAT ASSOC, V86, P953, DOI 10.2307/2290511; DELUCA JC, 1996, 962396 SAE T; DU X, 2004, ASME, V126, P1; DU X, 2002, 2002 ASME DES AUT C; Fang Kai-tai, 1980, Acta Mathematicae Applacatae Sinica, V3; HOFFMAN RM, 2003, 2003010148 SAE T; JIN R, 2004, IN PRESS J STAT PLAN; Jin R, 2000, 8 AIAA NASA USAF ISS; Kimdeldorf G. S., 1970, ANN MATH STAT, V41, P495; Kleijnen J. P, 1987, STAT TOOLS SIMULATIO; KODIYALAM S, 2001, DETC2001 DAC 21082 2; MARDIA KV, 1984, BIOMETRIKA, V71, P135, DOI 10.1093/biomet/71.1.135; PATTERSO.HD, 1971, BIOMETRIKA, V58, P545, DOI 10.2307/2334389; Sacks J., 1989, STAT SCI, V4, P409, DOI DOI 10.1214/SS/1177012413; Santner TJ, 2003, DESIGN ANAL COMPUTER; Simpson T., 2002, 9 AIAA ISSMO S MULT; SWEETING TJ, 1980, ANN STAT, V8, P1375, DOI 10.1214/aos/1176345208; WAHBA G, 1978, J ROY STAT SOC B MET, V40, P364; Wahba G., 1990, SPLINE MODEL OBSERVA; Wu Y., 1998, J INTEGRATED DESIGN, V2, P13; Ye KQ, 2000, J STAT PLAN INFER, V90, P145, DOI 10.1016/S0378-3758(00)00105-1	32	41	45	2	6	AMER STATISTICAL ASSOC	ALEXANDRIA	1429 DUKE ST, ALEXANDRIA, VA 22314 USA	0040-1706			TECHNOMETRICS	Technometrics	MAY	2005	47	2					111	120		10.1198/004017004000000671		10	Statistics & Probability	Mathematics	922XF	WOS:000228872500001		
J	Cai, JW; Fan, JQ; Li, RZ; Zhou, HB				Cai, JW; Fan, JQ; Li, RZ; Zhou, HB			Vairiable selection for multivariate failure time data	BIOMETRIKA			English	Article						Cox's model; marginal hazards model; penalised likelihood; smoothly clipped absolute deviation; variable selection	PROPORTIONAL HAZARDS MODEL; VARIABLE SELECTION; REGRESSION-MODELS; RATIO PARAMETERS; LIKELIHOOD; NUMBER; LASSO	In this paper, we propose a penalised pseudo-partial likelihood method for variable selection with multivariate failure time data with a growing number of regression coefficients. Under certain regularity conditions, we show the consistency and asymptotic normality of the penalised likelihood estimators. We further demonstrate that, for certain penalty functions with proper choices of regularisation parameters, the resulting estimator can correctly identify the true model, as if it were known in advance. Based on a simple approximation of the penalty function, the proposed method can be easily carried out with the Newton-Raphson algorithm. We conduct extensive Monte Carlo simulation studies to assess the finite sample performance of the proposed procedures. We illustrate the proposed method by analysing a dataset from the Framingham Heart Study.	Univ N Carolina, Dept Biostat, Chapel Hill, NC 27599 USA; Princeton Univ, Dept Operat Res & Financial Engn, Princeton, NJ 08544 USA; Penn State Univ, Dept Stat, University Pk, PA 16802 USA	Cai, JW (reprint author), Univ N Carolina, Dept Biostat, Chapel Hill, NC 27599 USA.	cai@bios.unc.edu; jqfan@princeton.edu; rli@stat.psu.edu	Li, Runze/C-5444-2013	Li, Runze/0000-0002-0154-2202			AKAIKE H, 1973, BIOMETRIKA, V60, P255, DOI 10.1093/biomet/60.2.255; ANDERSEN PK, 1982, ANN STAT, V10, P1100, DOI 10.1214/aos/1176345976; HUBER PJ, 1973, ANN STAT, V1, P799, DOI 10.1214/aos/1176342503; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; LIN DY, 1994, STAT MED, V13, P2233, DOI 10.1002/sim.4780132105; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; CLAYTON D, 1985, J ROY STAT SOC A STA, V148, P82, DOI 10.2307/2981943; WEI LJ, 1989, J AM STAT ASSOC, V84, P1065, DOI 10.2307/2290084; SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136; Breiman L, 1996, ANN STAT, V24, P2350; Bunea F, 2005, J MULTIVARIATE ANAL, V92, P186, DOI 10.1016/j.jmva.2003.09.006; Cai J, 1997, Lifetime Data Anal, V3, P197, DOI 10.1023/A:1009613313677; CAI JW, 1995, BIOMETRIKA, V82, P151; Cai JW, 1999, LIFETIME DATA ANAL, V5, P39, DOI 10.1023/A:1009679032314; Clegg LMX, 1999, BIOMETRICS, V55, P805, DOI 10.1111/j.0006-341X.1999.00805.x; COX DR, 1972, J R STAT SOC B, V34, P187; Dawber TR, 1980, FRAMINGHAM STUDY EPI; Fan JQ, 2002, ANN STAT, V30, P74; Faraggi D, 1998, BIOMETRICS, V54, P1475, DOI 10.2307/2533672; FOSTER DP, 1994, ANN STAT, V22, P1947, DOI 10.1214/aos/1176325766; Huang J, 2002, BIOMETRICS, V58, P781, DOI 10.1111/j.0006-341X.2002.00781.x; LEE EW, 1992, NATO ADV SCI I E-APP, V211, P237; LIANG KY, 1993, J ROY STAT SOC B MET, V55, P441; OAKES D, 1989, J AM STAT ASSOC, V84, P487, DOI 10.2307/2289934; PORTNOY S, 1988, ANN STAT, V16, P356, DOI 10.1214/aos/1176350710; SHIBATA R, 1984, BIOMETRIKA, V71, P43, DOI 10.1093/biomet/71.1.43; Spiekerman CF, 1998, J AM STAT ASSOC, V93, P1164, DOI 10.2307/2669859; Tibshirani R, 1997, STAT MED, V16, P385, DOI 10.1002/(SICI)1097-0258(19970228)16:4<385::AID-SIM380>3.0.CO;2-3	28	35	35	1	7	OXFORD UNIV PRESS	OXFORD	GREAT CLARENDON ST, OXFORD OX2 6DP, ENGLAND	0006-3444			BIOMETRIKA	Biometrika	JUN	2005	92	2					303	316		10.1093/biomet/92.2.303		14	Biology; Mathematical & Computational Biology; Statistics & Probability	Life Sciences & Biomedicine - Other Topics; Mathematical & Computational Biology; Mathematics	942OZ	WOS:000230293000004		
J	Braun, WJ; Huang, LS				Braun, WJ; Huang, LS			Kernel spline regression	CANADIAN JOURNAL OF STATISTICS-REVUE CANADIENNE DE STATISTIQUE			English	Article						akaike information criterion; bandwidth; kernel regression; knots; p-splines; splines	BANDWIDTH SELECTOR	The authors propose "kernel spline regression," a method of combining spline regression and kernel smoothing by replacing the polynomial approximation for local polynomial kernel regression with the spline basis. The new approach retains the local weighting scheme and the use of a bandwidth to control the size of local neighborhood. The authors compute the bias and variance of the kernel linear spline estimator, which they compare with local linear regression. They show that kernel spline estimators can succeed in capturing the main features of the underlying curve more effectively than local polynomial regression when the curvature changes rapidly. They also show through simulation that kernel spline regression often performs better than ordinary spline regression and local polynomial regression.	Univ Western Ontario, Dept Stat & Actuarial Sci, London, ON N6A 5B7, Canada; Univ Rochester, Dept Biostat & Computat Biol, Rochester, NY 14642 USA	Braun, WJ (reprint author), Univ Western Ontario, Dept Stat & Actuarial Sci, London, ON N6A 5B7, Canada.	braun@stats.uwo.ca; lhuang@bst.rochester.edu					Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Eilers PHC, 1996, STAT SCI, V11, P89, DOI 10.1214/ss/1038425655; SILVERMAN BW, 1985, J R STAT SOC B, V47, P1; Cleveland WS, 1996, STAT THEORY COMPUTAT, P10; de Boor C., 1978, PRACTICAL GUIDE SPLI; Eubank R.L., 1988, SPLINE SMOOTHING NON; Fan J, 1995, J COMPUT GRAPH STAT, V4, P213, DOI 10.2307/1390848; Fan JQ, 1999, STAT PROBABIL LETT, V43, P309, DOI 10.1016/S0167-7152(98)00271-5; Gijbels I., 1996, LOCAL POLYNOMIAL MOD; Green P, 1994, NONPARAMETRIC REGRES; Gregoire G, 2002, J MULTIVARIATE ANAL, V83, P56, DOI 10.1006/jmva.2001.2038; HARDLE W, 1985, ANN STAT, V13, P1465, DOI 10.1214/aos/1176349748; Hurvich CM, 1998, J ROY STAT SOC B, V60, P271, DOI 10.1111/1467-9868.00125; Ihaka R., 1996, J COMPUTATIONAL GRAP, V5, P299, DOI DOI 10.2307/1390807; Loader C., 1999, LOCAL REGRESSION LIK; MCDONALD JA, 1986, TECHNOMETRICS, V28, P195, DOI 10.2307/1269075; Nadaraya E., 1964, THEOR PROBAB APPL, V9, P141, DOI DOI 10.1137/1109020; Osborne MR, 1998, COMP SCI STAT, V30, P44; Ruppert D, 1995, J AM STAT ASSOC, V90, P1257, DOI 10.2307/2291516; Seifert B, 1998, COMP SCI STAT, V30, P467; Seifert B, 2000, J COMPUT GRAPH STAT, V9, P338, DOI 10.2307/1390658; Simonoff J. S., 1996, SMOOTHING METHODS ST; WAND MP, 1997, KERNSMOOTH; Wand M.P., 1995, KERNEL SMOOTHING; Watson G. S., 1964, SANKHYA A, V26, P359	25	1	1	0	0	WILEY-BLACKWELL	HOBOKEN	111 RIVER ST, HOBOKEN 07030-5774, NJ USA	0319-5724	1708-945X		CAN J STAT	Can. J. Stat.-Rev. Can. Stat.	JUN	2005	33	2					259	278				20	Statistics & Probability	Mathematics	948JD	WOS:000230711100007		
J	Krishnapuram, B; Carin, L; Figueiredo, MAT; Hartemink, AJ				Krishnapuram, B; Carin, L; Figueiredo, MAT; Hartemink, AJ			Sparse multinomial logistic regression: Fast algorithms and generalization bounds	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						supervised learning; classification; sparsity; Bayesian inference; multinomial logistic regression; bound optimization; expectation maximization (EM); learning theory; generalization bounds	GAUSSIAN-PROCESSES; VECTOR MACHINE; CLASSIFICATION; SELECTION	Recently developed methods for learning sparse classifiers are among the state-of-the-art in supervised learning. These methods learn classifiers that incorporate weighted sums of basis functions with sparsity-promoting priors encouraging the weight estimates to be either significantly large or exactly zero. From a learning-theoretic perspective, these methods control the capacity of the learned classifier by minimizing the number of basis functions used, resulting in better generalization. This paper presents three contributions related to learning sparse classifiers. First, we introduce a true multiclass formulation based on multinomial logistic regression. Second, by combining a bound optimization approach with a component-wise update procedure, we derive fast exact algorithms for learning sparse multiclass classifiers that scale favorably in both the number of training samples and the feature dimensionality, making them applicable even to large data sets in high-dimensional feature spaces. To the best of our knowledge, these are the first algorithms to perform exact multinomial logistic regression with a sparsity-promoting prior. Third, we show how nontrivial generalization bounds can be derived for our classifier in the binary case. Experimental results on standard benchmark data sets attest to the accuracy, sparsity, and efficiency of the proposed methods.	Siemens Med Solut USA Inc, Comp Aided Diag & Therapy Grp, Malvern, PA 19355 USA; Duke Univ, Dept Elect Engn, Durham, NC 27708 USA; Inst Super Tecn, Dept Elect & Comp Engn, Inst Telecommun, P-1049001 Lisbon, Portugal; Duke Univ, Dept Comp Sci, Durham, NC 27708 USA	Krishnapuram, B (reprint author), Siemens Med Solut USA Inc, Comp Aided Diag & Therapy Grp, 51 Valley Stream Pkwy, Malvern, PA 19355 USA.	Balaji.Krishnapuram@siemens.com; lcarin@ee.duke.edu; mario.figueiredo@lx.it.pt; amink@cs.duke.edu	Figueiredo, Mario/C-5428-2008	Figueiredo, Mario/0000-0002-0970-7745			Csato L, 2002, NEURAL COMPUT, V14, P641, DOI 10.1162/089976602317250933; WILLIAMS PM, 1995, NEURAL COMPUT, V7, P117, DOI 10.1162/neco.1995.7.1.117; Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010; Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0; Lange K, 2000, J COMPUT GRAPH STAT, V9, P1, DOI 10.2307/1390605; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1; McAllester DA, 1999, MACH LEARN, V37, P355, DOI 10.1023/A:1007618624809; Williams CKI, 1998, IEEE T PATTERN ANAL, V20, P1342, DOI 10.1109/34.735807; Friedman J, 2004, ANN STAT, V32, P102; Tipping ME, 2001, J MACH LEARN RES, V1, P211, DOI 10.1162/15324430152748236; Lewicki MS, 2000, NEURAL COMPUT, V12, P337, DOI 10.1162/089976600300015826; Bartlett P. L., 2002, J MACHINE LEARNING R, V3, P463; BOHNING D, 1992, ANN I STAT MATH, V44, P197, DOI 10.1007/BF00048682; BOHNING D, 1988, ANN I STAT MATH, V40, P641, DOI 10.1007/BF00049423; Cristianini M, 2000, INTRO SUPPORT VECTOR; DELEEUW J, 1993, BLOCK RELAXATION MET; Donoho DL, 2003, P NATL ACAD SCI USA, V100, P2197, DOI 10.1073/pnas.0437847100; Figueiredo M. A. T., 2001, P IEEE INT C COMP VI, V1, P35; Figueiredo MAT, 2003, IEEE T PATTERN ANAL, V25, P1150, DOI 10.1109/TPAMI.2003.1227989; Graepel T, 2001, ADV NEUR IN, V13, P210; Graepel T., 2000, P 13 ANN C COMP LEAR, P298; Herbrich R., 2002, LEARNING KERNEL CLAS; KRISHNAPURAM B, 2003, P INT C RES COMP MOL; Krishnapuram B, 2004, IEEE T PATTERN ANAL, V26, P1105, DOI 10.1109/TPAMI.2004.55; LANGFORD J, 2003, P INT C MACH LEARN; Langford J., 2003, ADV NEURAL INFORMATI, V15, P423; Lawrence N., 2003, ADV NEURAL INFORM PR, V15, P609; Mallat S., 1998, WAVELET TOUR SIGNAL; Meir R., 2003, J MACHINE LEARNING R, V4, P839; Minka T. P., 2003, COMP NUMERICAL OPTIM; Neal R. M., 1996, BAYESIAN LEARNING NE; NG QY, 2004, P INT C MACH LEARN; QI Y, 2004, P INT C MACH LEARN; Salakhuttlinov R., 2003, P 20 INT C MACH LEAR, P664; Seeger M., 2002, JMLR, V3, P233; Tipping M. E., 2003, P 9 INT WORKSH ART I; VALIANT LG, 1984, COMMUN ACM, V27, P1134, DOI 10.1145/1968.1972; Vapnik V., 1998, STAT LEARNING THEORY; Weston J., 2003, Journal of Machine Learning Research, V3, DOI 10.1162/153244303322753751; Zhang T, 2001, INFORM RETRIEVAL, V4, P5, DOI 10.1023/A:1011441423217; Zhu J, 2002, ADV NEUR IN, V14, P1081	42	260	267	5	23	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	JUN	2005	27	6					957	968		10.1109/TPAMI.2005.127		12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Computer Science; Engineering	915TR	WOS:000228334700010	15943426	
J	Pelckmans, K; De Brabanter, J; Suykens, JAK; De Moor, B				Pelckmans, K; De Brabanter, J; Suykens, JAK; De Moor, B			Handling missing values in support vector machine classifiers	NEURAL NETWORKS			English	Article; Proceedings Paper	International Joint Conference on Neural Networks	JUL 31-AUG 04, 2005	Montreal, CANADA	IEEE Computat Intelligence Soc, Int Neural Networks Soc				This paper discusses the task of learning a classifier from observed data containing missing values amongst the inputs which are missing completely at random(1). A non-parametric perspective is adopted by defining a modified risk taking into account the uncertainty of the predicted outputs when missing values are involved. It is shown that this approach generalizes the approach of mean imputation in the linear case and the resulting kernel machine reduces to the standard Support Vector Machine (SVM) when no input values are missing. Furthermore, the method is extended to the multivariate case of fitting additive models using componentwise kernel machines, and an efficient implementation is based on the Least Squares Support Vector Machine (LS-SVM) classifier formulation. (c) 2005 Elsevier Ltd. All rights reserved.	Katholieke Univ Leuven, ESAT, SCD, SISTA, B-3001 Louvain, Belgium; Katholieke Univ Leuven, Hogesch KaHo St Lieven, B-9000 Ghent, Belgium	Pelckmans, K (reprint author), Katholieke Univ Leuven, ESAT, SCD, SISTA, Kasteelpk Arenberg 10, B-3001 Louvain, Belgium.	kristiaan.pelckmans@esat.kuleuven.ac.be; johan.suykens@esat.kuleuven.ac.be	Pelckmans, Kristiaan/A-3118-2013; Suykens, Johan/C-9781-2014				RUBIN DB, 1976, BIOMETRIKA, V63, P581, DOI 10.1093/biomet/63.3.581; Suykens JAK, 2002, NEUROCOMPUTING, V48, P85, DOI 10.1016/S0925-2312(01)00644-0; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Pelckmans K, 2005, NEUROCOMPUTING, V64, P137, DOI 10.1016/j.neucom.2004.11.029; Suykens JAK, 1999, NEURAL PROCESS LETT, V9, P293, DOI 10.1023/A:1018628609742; Bishop C. M., 1995, NEURAL NETWORKS PATT; BOUSQUET O, 2004, ADV LECT MACHINE LEA, P3176; Cristianini N., 2000, INTRO SUPPORT VECTOR; Hastie T., 2001, ELEMENTS STAT LEARNI; Hastie TJ, 1990, GEN ADDITIVE MODELS; Hoeffding W., 1961, U N CAROLINA I STAT, V302; Lee A. J., 1990, U STAT THEORY PRACTI; PELCKMANS K, 2005, CHAPTER SUPPORT VECT; PELCKMANS K, 2005, P INT JOINT C NEUR N; Pestman W.R., 1998, MATH STAT; Rubin DB, 1987, MULTIPLE IMPUTATION; Saunders C, 1998, P 15 INT C MACH LEAR, P515; SCHOLKOPF B, 2002, LEARNING KERNLS; Stitson M., 1999, ADV KERNEL METHODS S; Suykens J. A. K., 2002, LEAST SQUARES SUPPOR; Vandenberghe L., 2004, CONVEX OPTIMIZATION; Vapnik V., 1998, STAT LEARNING THEORY	22	30	31	1	2	PERGAMON-ELSEVIER SCIENCE LTD	OXFORD	THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND	0893-6080			NEURAL NETWORKS	Neural Netw.	JUN-JUL	2005	18	5-6					684	692		10.1016/j.neunet.2005.06.025		9	Computer Science, Artificial Intelligence	Computer Science	967MY	WOS:000232096500027	16111866	
J	Chodkiewicz, ML				Chodkiewicz, ML			Compact multipolar representation of the electrostatic potential for flexible molecules	JOURNAL OF CHEMICAL PHYSICS			English	Article							DERIVING ATOMIC CHARGES; FORCE-FIELD; DISTRIBUTED MULTIPOLES; RESP MODEL; N-ALKANES; DATA-BANK; SIMULATIONS; REGRESSION; SELECTION; DENSITY	A new method for generating a compact multipolar representation of the electrostatic potential (EP) for flexible molecules is presented. The method is based on a constrained minimization of the difference between the quantum mechanical and the classical EP. The fitting procedure used adopts the least absolute shrinkage and selection operator technique [R. Tibshirani, J. Roy. Stat. Soc. B 58, 267 (1996)] which can be seen as penalized ordinary least squares. The penalty function optimized for the particular molecule of interest effectively removes redundant multipoles. It is shown that the use of multiple conformations is crucial for the predictive ability of the EP model for flexible molecules. The multipole local coordinate systems are chosen in a way that best reflects the key conformational changes. It was demonstrated that such an approach improves the predictive ability of EP models. It also allows to exploit equivalence of atoms in the calculation of multipoles components. In the case of polar flexible molecules, the augmentation of the EP model based on charges by higher multipoles decreases the relative root mean square error by a factor of 1.5-5. The corresponding effect of enlargement of the set of multipoles was significantly reduced.	Warsaw Univ, Dept Chem, PL-02093 Warsaw, Poland	Chodkiewicz, ML (reprint author), Warsaw Univ, Dept Chem, Pasteura 1, PL-02093 Warsaw, Poland.						BREIMAN L, 1995, TECHNOMETRICS, V37, P373, DOI 10.2307/1269730; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; BRENEMAN CM, 1990, J COMPUT CHEM, V11, P361, DOI 10.1002/jcc.540110311; BESLER BH, 1990, J COMPUT CHEM, V11, P431, DOI 10.1002/jcc.540110404; STONE AJ, 1985, MOL PHYS, V56, P1047, DOI 10.1080/00268978500102891; Gordon MS, 2001, J PHYS CHEM A, V105, P293, DOI 10.1021/jp002747h; STONE AJ, 1981, CHEM PHYS LETT, V83, P233, DOI 10.1016/0009-2614(81)85452-8; WILLIAMS DE, 1988, J COMPUT CHEM, V9, P745, DOI 10.1002/jcc.540090705; HOERL AE, 1970, TECHNOMETRICS, V12, P55; Kaminski GA, 2001, J PHYS CHEM B, V105, P6474, DOI 10.1021/jp003919d; BAYLY CI, 1993, J PHYS CHEM-US, V97, P10269, DOI 10.1021/j100142a004; Spackman MA, 1996, J COMPUT CHEM, V17, P1, DOI 10.1002/(SICI)1096-987X(19960115)17:1<1::AID-JCC1>3.0.CO;2-V; BADER RFW, 1991, CHEM REV, V91, P893, DOI 10.1021/cr00005a013; Foloppe N, 2000, J COMPUT CHEM, V21, P86, DOI 10.1002/(SICI)1096-987X(20000130)21:2<86::AID-JCC2>3.0.CO;2-G; Wang JM, 2000, J COMPUT CHEM, V21, P1049, DOI 10.1002/1096-987X(200009)21:12<1049::AID-JCC3>3.0.CO;2-F; Bader R. F. W., 1990, ATOMS MOL QUANTUM TH; Basma M, 2001, J COMPUT CHEM, V22, P1125, DOI 10.1002/jcc.1072; Beyer T, 2001, J AM CHEM SOC, V123, P5086, DOI 10.1021/ja0102787; Brodersen S, 2003, PHYS CHEM CHEM PHYS, V5, P4923, DOI 10.1039/b306396b; CHIPOT C, 1993, J PHYS CHEM-US, V97, P6628, DOI 10.1021/j100127a011; Chipot C, 1998, MOL PHYS, V94, P881, DOI 10.1080/002689798167458; CHIRLIAN LE, 1987, J COMPUT CHEM, V8, P894, DOI 10.1002/jcc.540080616; CHODKIEWICZ ML, 2004, PG POINTS GENERATOR; Dixon RW, 1997, J COMPUT CHEM, V18, P1632, DOI 10.1002/(SICI)1096-987X(199710)18:13<1632::AID-JCC5>3.0.CO;2-S; DOGNON JP, 2000, J MOL STRUCT, V17, P507; FERENCZY GG, 1991, J COMPUT CHEM, V12, P913, DOI 10.1002/jcc.540120802; Ferenczy GG, 1997, J PHYS CHEM A, V101, P5446, DOI 10.1021/jp9712011; Francl MM, 1996, J COMPUT CHEM, V17, P367, DOI 10.1002/(SICI)1096-987X(199602)17:3<367::AID-JCC11>3.0.CO;2-H; Frisch M. J., 1998, GAUSSIAN98 REVISION; GERTZ EM, 2001, OBJECT ORIENTED SOFT; HADDON RC, 1990, J AM CHEM SOC, V112, P3385, DOI 10.1021/ja00165a020; Henchman RH, 1999, J COMPUT CHEM, V20, P483, DOI 10.1002/(SICI)1096-987X(19990415)20:5<483::AID-JCC2>3.0.CO;2-4; Hinsen K, 2000, J COMPUT CHEM, V21, P79, DOI 10.1002/(SICI)1096-987X(20000130)21:2<79::AID-JCC1>3.0.CO;2-B; Karamertzanis PG, 2004, MOL SIMULAT, V30, P413, DOI 10.1080/08927020410001680769; KOCH U, 1995, J COMPUT CHEM, V16, P937, DOI 10.1002/jcc.540160803; Kosov DS, 2000, J PHYS CHEM A, V104, P7339, DOI 10.1021/jp0003407; Miller A. J., 1990, SUBSET SELECTION REG; Myers R, 1990, CLASSICAL MODERN REG; PICHONPESME V, 1995, J PHYS CHEM-US, V99, P6242, DOI 10.1021/j100016a071; Ponder J. W., 2001, TINKER SOFTWARE TOOL; Popelier PLA, 2001, J PHYS CHEM A, V105, P8254, DOI 10.1021/jp011511q; RAMSAY OJ, 1997, FUNCTIONAL DATA ANAL; REYNOLDS CA, 1992, J AM CHEM SOC, V114, P9075, DOI 10.1021/ja00049a045; Ridard J, 1999, J COMPUT CHEM, V20, P473, DOI 10.1002/(SICI)1096-987X(19990415)20:5<473::AID-JCC1>3.3.CO;2-#; Sadlej-Sosnowska N, 2002, J PHYS CHEM A, V106, P10554, DOI 10.1021/jp0144496; Sagui C, 2004, J CHEM PHYS, V120, P4530, DOI 10.1063/1.1644800; Sigfridsson E, 1998, J COMPUT CHEM, V19, P377, DOI 10.1002/(SICI)1096-987X(199803)19:4<377::AID-JCC1>3.3.CO;2-E; SIGFRIDSSON E, 2001, J COMPUT CHEM, V23, P351; SOKALSKI WA, 1983, CHEM PHYS LETT, V98, P86, DOI 10.1016/0009-2614(83)80208-5; Tsiper EV, 2004, J CHEM PHYS, V120, P1153, DOI 10.1063/1.1640995; van de Streek J, 2002, J COMPUT CHEM, V23, P365, DOI 10.1002/jcc.10028; Volkov A, 2004, J PHYS CHEM A, V108, P4283, DOI 10.1021/jp0379796; WILLIAMS DE, 1994, J COMPUT CHEM, V15, P719, DOI 10.1002/jcc.540150705; Williams DE, 1999, J COMPUT CHEM, V20, P579, DOI 10.1002/(SICI)1096-987X(19990430)20:6<579::AID-JCC3>3.3.CO;2-5; Woods RJ, 2000, J MOL STRUC-THEOCHEM, V527, P149, DOI 10.1016/S0166-1280(00)00487-5	56	2	2	1	6	AMER INST PHYSICS	MELVILLE	CIRCULATION & FULFILLMENT DIV, 2 HUNTINGTON QUADRANGLE, STE 1 N O 1, MELVILLE, NY 11747-4501 USA	0021-9606			J CHEM PHYS	J. Chem. Phys.	JUN 8	2005	122	22							224107	10.1063/1.1926281		8	Physics, Atomic, Molecular & Chemical	Physics	936LW	WOS:000229858500009	15974651	
J	Ghosh, D; Chinnaiyan, AM				Ghosh, D; Chinnaiyan, AM			Classification and selection of biomarkers in genomic data using LASSO	JOURNAL OF BIOMEDICINE AND BIOTECHNOLOGY			English	Article							GENE-EXPRESSION DATA; DISCRIMINANT-ANALYSIS; COMBINATIONS; MARKERS; CANCER	High-throughput gene expression technologies such as microarrays have been utilized in a variety of scientific applications. Most of the work has been done on assessing univariate associations between gene expression profiles with clinical outcome (variable selection) or on developing classification procedures with gene expression data (supervised learning). We consider a hybrid variable selection/classification approach that is based on linear combinations of the gene expression profiles that maximize an accuracy measure summarized using the receiver operating characteristic curve. Under a specific probability model, this leads to the consideration of linear discriminant functions. We incorporate an automated variable selection approach using LASSO. An equivalence between LASSO estimation with support vector machines allows for model fitting using standard software. We apply the proposed method to simulated data as well as data from a recently published prostate cancer study.	Univ Michigan, Dept Biostat, Ann Arbor, MI 48109 USA; Univ Michigan, Dept Pathol, Ann Arbor, MI 48109 USA; Univ Michigan, Dept Urol, Ann Arbor, MI 48109 USA	Ghosh, D (reprint author), Univ Michigan, Dept Biostat, 1420 Washington Hts, Ann Arbor, MI 48109 USA.	ghoshd@umich.edu					Alizadeh AA, 2001, J PATHOL, V195, P41, DOI 10.1002/path.889; Dhanasekaran SM, 2001, NATURE, V412, P822, DOI 10.1038/35090585; HASTIE T, 1994, J AM STAT ASSOC, V89, P1255, DOI 10.2307/2290989; Nguyen DV, 2002, BIOINFORMATICS, V18, P39, DOI 10.1093/bioinformatics/18.1.39; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Khan J, 2001, NAT MED, V7, P673, DOI 10.1038/89044; Dudoit S, 2002, J AM STAT ASSOC, V97, P77, DOI 10.1198/016214502753479248; Eisen MB, 1998, P NATL ACAD SCI USA, V95, P14863, DOI 10.1073/pnas.95.25.14863; Efron B, 2001, J AM STAT ASSOC, V96, P1151, DOI 10.1198/016214501753382129; Lee KE, 2003, BIOINFORMATICS, V19, P90, DOI 10.1093/bioinformatics/19.1.90; Baker SG, 2000, BIOMETRICS, V56, P1082, DOI 10.1111/j.0006-341X.2000.01082.x; BAMBER D, 1975, J MATH PSYCHOL, V12, P387, DOI 10.1016/0022-2496(75)90001-2; HASTIE T, 1995, ANN STAT, V23, P73, DOI 10.1214/aos/1176324456; CRISTIANINI N, 2000, INTO SUPPORT VECTOR; Hastie T., 2000, GENOME BIOL, V1; McIntosh MW, 2002, BIOMETRICS, V58, P657, DOI 10.1111/j.0006-341X.2002.00657.x; Pavlidis P, 2002, PAC S BIOCOMPUT, V7, P474; Pepe MS, 2000, BIOSTATISTICS, V1, P123, DOI DOI 10.1093/BI0STATISTICS/1.2.123; SU JQ, 1993, J AM STAT ASSOC, V88, P1350; Tibshirani R, 2002, STAT SINICA, V12, P47; van't Veer LJ, 2002, NATURE, V415, P530, DOI 10.1038/415530a; VANDEVIJVER MJ, 1909, NEW ENGL J MED, V347, P1999; WEST M, 2003, BAYESIAN STAT, P723; Zien A, 2000, Proc Int Conf Intell Syst Mol Biol, V8, P407	24	17	17	0	2	HINDAWI PUBLISHING CORPORATION	SYLVANIA	PO BOX  1210, SYLVANIA, OH 43560 USA	1110-7243			J BIOMED BIOTECHNOL	J. Biomed. Biotechnol.	JUN 30	2005		2					147	154		10.1155/JBB.2005.147		8	Biotechnology & Applied Microbiology; Medicine, Research & Experimental	Biotechnology & Applied Microbiology; Research & Experimental Medicine	984DH	WOS:000233282200011		
J	Gui, J; Li, HZ				Gui, J; Li, HZ			Penalized Cox regression analysis in the high-dimensional and low-sample size settings, with applications to microarray gene expression data	BIOINFORMATICS			English	Article							B-CELL LYMPHOMA; SURVIVAL ANALYSIS; CENSORED-DATA; MODEL; SELECTION; LIKELIHOOD; PATTERNS; TUMOR; LASSO	Motivation: An important application of microarray technology is to relate gene expression profiles to various clinical phenotypes of patients. Success has been demonstrated in molecular classification of cancer in which the gene expression data serve as predictors and different types of cancer serve as a categorical outcome variable. However, there has been less research in linking gene expression profiles to the censored survival data such as patients' overall survival time or time to cancer relapse. It would be desirable to have models with good prediction accuracy and parsimony property. Results: We propose to use the L(1) penalized estimation for the Cox model to select genes that are relevant to patients' survival and to build a predictive model for future prediction. The computational difficulty associated with the estimation in the high-dimensional and low-sample size settings can be efficiently solved by using the recently developed least-angle regression (LARS) method. Our simulation studies and application to real datasets on predicting survival after chemotherapy for patients with diffuse large B-cell lymphoma demonstrate that the proposed procedure, which we call the LARS-Cox procedure, can be used for identifying important genes that are related to time to death due to cancer and for building a parsimonious model for predicting the survival of future patients. The LARS-Cox regression gives better predictive performance than the L(2) penalized regression and a few other dimension-reduction based methods. Conclusions: We conclude that the proposed LARS-Cox procedure can be very useful in identifying genes relevant to survival phenotypes and in building a parsimonious predictive model that can be used for classifying future patients into clinically relevant high- and low-risk groups based on the gene expression profile and survival times of previous patients.	Univ Calif Davis, Rowe Program Human Genet, Davis, CA 95616 USA; Univ Calif Davis, Dept Stat, Davis, CA 95616 USA	Li, HZ (reprint author), Univ Calif Davis, Rowe Program Human Genet, Davis, CA 95616 USA.	hli@ucdavis.edu					AKRITAS MG, 1994, ANN STAT, V22, P1299, DOI 10.1214/aos/1176325630; Troyanskaya O, 2001, BIOINFORMATICS, V17, P520, DOI 10.1093/bioinformatics/17.6.520; LIN DY, 1993, BIOMETRIKA, V80, P557, DOI 10.1093/biomet/80.3.557; Sorlie T, 2001, P NATL ACAD SCI USA, V98, P10869, DOI 10.1073/pnas.191367098; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; LIN DY, 1989, J AM STAT ASSOC, V84, P1074, DOI 10.2307/2290085; Garber ME, 2001, P NATL ACAD SCI USA, V98, P13784, DOI 10.1073/pnas.241500798; Alon U, 1999, P NATL ACAD SCI USA, V96, P6745, DOI 10.1073/pnas.96.12.6745; Alizadeh AA, 2000, NATURE, V403, P503, DOI 10.1038/35000501; Golub TR, 1999, SCIENCE, V286, P531, DOI 10.1126/science.286.5439.531; Segal MR, 2003, J COMPUT BIOL, V10, P961, DOI 10.1089/106652703322756177; Efron B, 2004, ANN STAT, V32, P407; Cheng SC, 1995, BIOMETRIKA, V82, P835, DOI 10.1093/biomet/82.4.835; Heagerty PJ, 2000, BIOMETRICS, V56, P337, DOI 10.1111/j.0006-341X.2000.00337.x; Rosenwald A, 2002, NEW ENGL J MED, V346, P1937, DOI 10.1056/NEJMoa012914; Bair E, 2004, PLOS BIOL, V2, P511, DOI 10.1371/journal.pbio.00200108; COX DR, 1972, J R STAT SOC B, V34, P187; Huang J, 2002, BIOMETRICS, V58, P781, DOI 10.1111/j.0006-341X.2002.00781.x; Li Hongzhe, 2004, Bioinformatics, V20 Suppl 1, pi208, DOI 10.1093/bioinformatics/bth900; Li H., 2003, PAC S BIOC, P65; Oquendo CE, 2004, J MED GENET, V41, P540, DOI 10.1136/jmg.2003.017426; Osborne M., 1998, LASSO ITS DUAL, DOI 10.1.1.34.2316; Park Peter J, 2002, Bioinformatics, V18 Suppl 1, pS120; Pekarsky Y, 1999, P NATL ACAD SCI USA, V96, P2949, DOI 10.1073/pnas.96.6.2949; Petruzzella V, 1998, GENOMICS, V54, P494, DOI 10.1006/geno.1998.5580; Smyth P, 2000, STAT COMPUT, V10, P63, DOI 10.1023/A:1008940618127; Tibshirani R, 1997, STAT MED, V16, P385, DOI 10.1002/(SICI)1097-0258(19970228)16:4<385::AID-SIM380>3.0.CO;2-3; VANDERLAAN MJ, 2003, ASYMPTOTIC OPTIMALIT; VERWEIJ PJM, 1993, STAT MED, V12, P2305, DOI 10.1002/sim.4780122407; WEI LJ, 1992, STAT MED, V11, P1871, DOI 10.1002/sim.4780111409; ZOU H, 2004, IN PRESS J R STAT B	31	117	121	3	12	OXFORD UNIV PRESS	OXFORD	GREAT CLARENDON ST, OXFORD OX2 6DP, ENGLAND	1367-4803			BIOINFORMATICS	Bioinformatics	JUL 1	2005	21	13					3001	3008		10.1093/bioinformatics/bti422		8	Biochemical Research Methods; Biotechnology & Applied Microbiology; Computer Science, Interdisciplinary Applications; Mathematical & Computational Biology; Statistics & Probability	Biochemistry & Molecular Biology; Biotechnology & Applied Microbiology; Computer Science; Mathematical & Computational Biology; Mathematics	942AN	WOS:000230255400010	15814556	
J	Gustafsson, M; Hornquist, M; Lombardi, A				Gustafsson, M; Hornquist, M; Lombardi, A			Constructing and analyzing a large-scale gene-to-gene regulatory network - Lasso-constrained inference and biological validation	IEEE-ACM TRANSACTIONS ON COMPUTATIONAL BIOLOGY AND BIOINFORMATICS			English	Article						biology and genetics; time series analysis; network problems; gene network; network inference; Lasso; yeast; validation; outdegree	EXPRESSION DATA; CELL-CYCLE; MICROARRAY DATA; REGRESSION; SELECTION	We construct a gene-to-gene regulatory network from time-series data of expression levels for the whole genome of the yeast Saccharomyces cerevisae, in a case where the number of measurements is much smaller than the number of genes in the network. This network is analyzed with respect to present biological knowledge of all genes (according to the Gene Ontology database), and we find some of its large-scale properties to be in accordance with known facts about the organism, The linear modeling employed here has been explored several times, but due to lack of any validation beyond investigating individual genes, it has been seriously questioned with respect to its applicability to biological systems. Our results show the adequacy of the approach and make further investigations of the model meaningful.	Linkoping Univ, Dept Sci & Technol, SE-60174 Norrkoping, Sweden	Gustafsson, M (reprint author), Linkoping Univ, Dept Sci & Technol, Campus Norrkoping, SE-60174 Norrkoping, Sweden.	mikgu@itn.liu.se; micho@itn.liu.se; annlo@itn.liu.se					Troyanskaya O, 2001, BIOINFORMATICS, V17, P520, DOI 10.1093/bioinformatics/17.6.520; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Spellman PT, 1998, MOL BIOL CELL, V9, P3273; Yeung MKS, 2002, P NATL ACAD SCI USA, V99, P6163, DOI 10.1073/pnas.092576199; Zou M, 2005, BIOINFORMATICS, V21, P71, DOI 10.1093/bioinformatics/bth463; Cho RJ, 1998, MOL CELL, V2, P65, DOI 10.1016/S1097-2765(00)80114-8; Ideker T, 2001, SCIENCE, V292, P929, DOI 10.1126/science.292.5518.929; George EI, 2000, J AM STAT ASSOC, V95, P1304, DOI 10.2307/2669776; van Someren EP, 2003, SIGNAL PROCESS, V83, P763, DOI 10.1016/S0165-1684(02)00473-5; Segal E, 2003, NAT GENET, V34, P166, DOI 10.1038/ng1165; Ashburner M, 2000, NAT GENET, V25, P25; Segal MR, 2003, J COMPUT BIOL, V10, P961, DOI 10.1089/106652703322756177; Eisen MB, 1998, P NATL ACAD SCI USA, V95, P14863, DOI 10.1073/pnas.95.25.14863; Milo R, 2002, SCIENCE, V298, P824, DOI 10.1126/science.298.5594.824; BJORCK A, 1996, NUMERICAL METHODS LE; Bornholdt S., 2003, HDB GRAPHS NETWORKS; Brazhnik P, 2002, TRENDS BIOTECHNOL, V20, P467, DOI 10.1016/S0167-7799(02)02053-X; Chen HC, 2004, BIOINFORMATICS, V20, P1914, DOI 10.1093/bioinformatics/bth178; D'haeseleer P, 1999, PAC S BIOC, V4, P41; Datta S, 2003, BIOINFORMATICS, V19, P459, DOI 10.1093/bioinformatics/btg025; de Boor C., 1978, PRACTICAL GUIDE SPLI; Dewey T. Gregory, 2001, Functional and Integrative Genomics, V1, P269, DOI 10.1007/s101420000035; DOLINSKI K, 2003, SACCHAROMYCES GENOME; Draper N. R., 1998, APPL REGRESSION ANAL; Eriksen Kasper Astrup, 2004, Functional & Integrative Genomics, V4, P241; Featherstone DE, 2002, BIOESSAYS, V24, P267, DOI 10.1002/bies.10054; Holter NS, 2001, P NATL ACAD SCI USA, V98, P1693, DOI 10.1073/pnas.98.4.1693; Hornquist M, 2003, BIOSYSTEMS, V71, P311, DOI 10.1016/S0303-2647(03)00128-X; Husmeier D, 2003, BIOINFORMATICS, V19, P2271, DOI 10.1093/bioinformatics/btg313; Jong H. D., 2002, J COMPUT BIOL, V9, P67; LI L, 2005, CITCDS04006; Ljung L, 1999, SYSTEM IDENTIFICATIO; Osborne MR, 2000, J COMPUT GRAPH STAT, V9, P319, DOI 10.2307/1390657; Press W.H., 1996, NUMERICAL RECIPES; van Someren E. P., 2000, Proceedings. Eighth International Conference on Intelligent Systems for Molecular Biology	35	44	47	1	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	1545-5963			IEEE ACM T COMPUT BI	IEEE-ACM Trans. Comput. Biol. Bioinform.	JUL-SEP	2005	2	3					254	261		10.1109/TCBB.2005.35		8	Biochemical Research Methods; Computer Science, Interdisciplinary Applications; Mathematics, Interdisciplinary Applications; Statistics & Probability	Biochemistry & Molecular Biology; Computer Science; Mathematics	017OZ	WOS:000235704200008	17044188	
J	Tutz, G; Binder, H				Tutz, G; Binder, H			Localized classification	STATISTICS AND COMPUTING			English	Article						local logistic regression; discrimination; data adaptive tuning parameters; selection of predictors; localized discrimination	LOGISTIC-REGRESSION; MODELS; APPROXIMATION	The main problem with localized discriminant techniques is the curse of dimensionality, which seems to restrict their use to the case of few variables. However, if localization is combined with a reduction of dimension the initial number of variables is less restricted. In particular it is shown that localization yields powerful classifiers even in higher dimensions if localization is combined with locally adaptive selection of predictors. A robust localized logistic regression ( LLR) method is developed for which all tuning parameters are chosen data- adaptively. In an extended simulation study we evaluate the potential of the proposed procedure for various types of data and compare it to other classification procedures. In addition we demonstrate that automatic choice of localization, predictor selection and penalty parameters based on cross validation is working well. Finally the method is applied to real data sets and its real world performance is compared to alternative procedures.	Univ Munich, Inst Stat, D-80799 Munich, Germany; Univ Regensburg, Klin & Poliklin Psychiat & Psychotherapie, D-8400 Regensburg, Germany	Tutz, G (reprint author), Univ Munich, Inst Stat, Akad Str 1, D-80799 Munich, Germany.	tutz@stat.uni-muenchen.de	Binder, Harald/C-7413-2009	Binder, Harald/0000-0002-5666-8662			ALBERT A, 1984, BIOMETRIKA, V71, P1; Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451; Friedman J, 2000, ANN STAT, V28, P337, DOI 10.1214/aos/1016218223; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324; Breiman L, 1999, NEURAL COMPUT, V11, P1493, DOI 10.1162/089976699300016106; HOERL AE, 1970, TECHNOMETRICS, V12, P55; Buhlmann P, 2003, J AM STAT ASSOC, V98, P324, DOI 10.1198/016214503000125; HOLM S, 1979, SCAND J STAT, V6, P65; Efron B, 2004, ANN STAT, V32, P407; Bellman R.E., 1961, ADAPTIVE CONTROL PRO; Bishop C. M., 1995, NEURAL NETWORKS PATT; Blake C.L., 1998, UCI REPOSITORY MACHI; Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.1023/A:1018054314350; Breiman L, 2002, MANUAL SETTING USING; Breiman L., 1984, CLASSIFICATION REGRE; LECESSIE S, 1992, APPL STAT-J ROY ST C, V41, P191; CHAPELLE O, 1999, ADV NEURAL INFORMATI, V12; Fix E., 1951, 4 US AIR FORC SCH AV; Gijbels I., 1996, LOCAL POLYNOMIAL MOD; GORMAN RP, 1988, NEURAL NETWORKS, V1, P75, DOI 10.1016/0893-6080(88)90023-8; Hand DJ, 2003, AM STAT, V57, P124, DOI 10.1198/0003130031423; Hastie T., 2001, ELEMENTS STAT LEARNI; Hastie T, 1996, IEEE T PATTERN ANAL, V18, P607, DOI 10.1109/34.506411; IHAKA R, 1996, J COMPUTATIONAL GRAP, V5, P299, DOI DOI 10.2307/1390807; John G., 1998, FEATURE SELECTION KN, P33; Kauermann G, 2000, J NONPARAMETR STAT, V12, P343, DOI 10.1080/10485250008832812; KIRA K, 1992, MACHINE LEARNING /, P249; Loader C., 1999, LOCAL REGRESSION LIK; Michie D., 1994, MACHINE LEARNING NEU; Powell MJD, 2002, MATH PROGRAM, V92, P555, DOI 10.1007/s101070100290; RIEDMAN JH, 1994, FLEXIBLE METRIC NEAR; Ripley B. D., 1996, PATTERN RECOGNITION; SCHAAL S, 1998, ADV NEURAL INFORMATI, V10; Venables WN, 1999, MODERN APPL STAT S P	35	12	12	1	1	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0960-3174			STAT COMPUT	Stat. Comput.	JUL	2005	15	3					155	166		10.1007/s11222-005-1305-x		12	Computer Science, Theory & Methods; Statistics & Probability	Computer Science; Mathematics	937EH	WOS:000229906900001		
J	Chong, IG; Jun, CH				Chong, IG; Jun, CH			Performance of some variable selection methods when multicollinearity is present	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS			English	Article						variable selection; VIP (Variable importance in the projection) scores; partial least squares regression; the lasso; stepwise regression; multicollinearity	LEAST-SQUARES REGRESSION	Variable selection is one of the important practical issues for many scientific engineers. Although the PLS (partial least squares) regression combined with the VIP (variable importance in the projection) scores is often used when the multicollinearity, is present among variables, there are few guidelines about its uses as well as its performance. The purpose of this paper is to explore the nature of the VIP method and to compare with other methods through computer simulation experiments. We design 108 experiments where observations are generated from true models considering four factors-the proportion of the number of relevant predictors, the magnitude of correlations between predictors, the structure of regression coefficients, and the magnitude of signal to noise. Confusion matrix is adopted to evaluate the performance of PLS, the Lasso, and stepwise method. We also discuss the proper cutoff value of the VIP method to increase its performance. Some practical hints for the use of the VIP method are given as simulation results. (c) 2005 Elsevier B.V. All rights reserved.	Pohang Univ Sci & Technol, Dept Ind Engn, Pohang 790784, South Korea	Jun, CH (reprint author), Pohang Univ Sci & Technol, Dept Ind Engn, San 31 Hyoja Dong, Pohang 790784, South Korea.	chjun@postech.ac.kr	Lee, Jung-Hye/F-6974-2013				Wold S, 2001, CHEMOMETR INTELL LAB, V58, P109, DOI 10.1016/S0169-7439(01)00155-1; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Kubat M, 1998, MACH LEARN, V30, P195, DOI 10.1023/A:1007452223027; HELLAND IS, 1988, COMMUN STAT SIMULAT, V17, P581, DOI 10.1080/03610918808812681; GELADI P, 1986, ANAL CHIM ACTA, V185, P1, DOI 10.1016/0003-2670(86)80028-9; Efron B, 2004, ANN STAT, V32, P407; MALLOWS CL, 1973, TECHNOMETRICS, V15, P661, DOI 10.2307/1267380; Eriksson L., 2001, MULTI MEGAVARIATE DA; Grenander U., 1958, TOEPLITZ FORMS THEIR; Hastie T., 2001, ELEMENTS STAT LEARNI, P214; Montgomery D. C., 2001, INTRO LINEAR REGRESS, P131; Wold S., 1993, 3D QSAR DRUG DESIGN, P523	12	390	406	16	79	ELSEVIER SCIENCE BV	AMSTERDAM	PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS	0169-7439			CHEMOMETR INTELL LAB	Chemometrics Intell. Lab. Syst.	JUL 28	2005	78	1-2					103	112		10.1016/j.chemolab.2004.12.011		10	Automation & Control Systems; Chemistry, Analytical; Computer Science, Artificial Intelligence; Instruments & Instrumentation; Mathematics, Interdisciplinary Applications; Statistics & Probability	Automation & Control Systems; Chemistry; Computer Science; Instruments & Instrumentation; Mathematics	946NS	WOS:000230579700011		
J	Hunter, DR; Li, RZ				Hunter, DR; Li, RZ			Variable selection using MM algorithms	ANNALS OF STATISTICS			English	Article						AIC; BIC; EM algorithm; LASSO; MM algorithm; penalized likelihood; oracle estimator; SCAD	NONCONCAVE PENALIZED LIKELIHOOD; SURROGATE OBJECTIVE FUNCTIONS; EM ALGORITHM; CONVERGENCE; MODEL	Variable selection is fundamental to high-dimensional statistical modeling. Many variable selection techniques may be implemented by maximum penalized likelihood using various penalty functions. Optimizing the penalized likelihood function is often challenging because it may be nondifferentiable and/or nonconcave. This article proposes a new class of algorithms for finding a maximizer of the penalized likelihood for a broad class of penalty functions. These algorithms operate by perturbing the penalty function slightly to render it differentiable, then optimizing this differentiable function using a minorize-maximize (MM) algorithm. MM algorithms are useful extensions of the well-known class of EM algorithms, a fact that allows us to analyze the local and global convergence of the proposed algorithm using some of the techniques employed for EM algorithms. In particular, we prove that when our MM algorithms converge, they must converge to a desirable point; we also discuss conditions under which this convergence may be guaranteed. We exploit the Newton-Raphson-like aspect of these algorithms to propose a sandwich estimator for the standard errors of the estimators. Our method performs well in numerical tests.	Penn State Univ, Dept Stat, University Pk, PA 16802 USA	Hunter, DR (reprint author), Penn State Univ, Dept Stat, University Pk, PA 16802 USA.	dhunter@stat.psu.edu; rli@stat.psu.edu	Li, Runze/C-5444-2013	Li, Runze/0000-0002-0154-2202			Lange K, 2000, J COMPUT GRAPH STAT, V9, P1, DOI 10.2307/1390605; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Antoniadis A, 2001, J AM STAT ASSOC, V96, P939, DOI 10.1198/016214501753208942; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1; COX DR, 1975, BIOMETRIKA, V62, P269, DOI 10.1093/biomet/62.2.269; FRANK IE, 1993, TECHNOMETRICS, V35, P109, DOI 10.2307/1269656; Meng XL, 1997, J ROY STAT SOC B MET, V59, P511, DOI 10.1111/1467-9868.00082; CRAVEN P, 1979, NUMER MATH, V31, P377; Antoniadis A., 1997, J ITAL STAT SOC, V6, P97, DOI DOI 10.1007/BF03178905; CAI J, 2005, IN PRESS BIOMETRIKA; Kauermann G, 2001, J AM STAT ASSOC, V96, P1387, DOI 10.1198/016214501753382309; Fan JQ, 2004, ANN STAT, V32, P928, DOI 10.1214/009053604000000256; Fan JQ, 2002, ANN STAT, V30, P74; Fan JQ, 2004, J AM STAT ASSOC, V99, P710, DOI 10.1198/0162145040000001060; Heiser WJ., 1995, RECENT ADV DESCRIPTI, P157; Hestenes M. R., 1975, OPTIMIZATION THEORY; Hunter DR, 2000, J COMPUT GRAPH STAT, V9, P52, DOI 10.2307/1390612; LANGE K, 1995, J ROY STAT SOC B MET, V57, P425; Mc Lachlan G.J., 1997, EM ALGORITHM EXTENSI; MENG XL, 1994, ANN STAT, V22, P326, DOI 10.1214/aos/1176325371; Miller A, 2002, SUBSET SELECTION REG, V2nd; Ortega J. M., 1990, NUMERICAL ANAL 2 COU; WU CFJ, 1983, ANN STAT, V11, P95, DOI 10.1214/aos/1176346060	24	141	147	1	9	INST MATHEMATICAL STATISTICS	BEACHWOOD	PO BOX 22718, BEACHWOOD, OH 44122 USA	0090-5364			ANN STAT	Ann. Stat.	AUG	2005	33	4					1617	1642		10.1214/009053605000000200		26	Statistics & Probability	Mathematics	957YE	WOS:000231408600005		
J	Malioutov, D; Cetin, M; Willsky, AS				Malioutov, D; Cetin, M; Willsky, AS			A sparse signal reconstruction perspective for source localization with sensor arrays	IEEE TRANSACTIONS ON SIGNAL PROCESSING			English	Article						direction-of-arrival estimation; overcomplete representation; sensor array processing; source localization; sparse representation; superresolution	SELECTION; REGULARIZATION	We present a source localization method based on a sparse representation of sensor measurements with an overcomplete basis composed of samples from the array manifold. We enforce sparsity by imposing penalties based on the l(1)-norm. A number of recent theoretical results on sparsifying properties of l(1) penalties justify this choice. Explicitly enforcing the sparsity of the representation is motivated by a desire to obtain a sharp estimate of the spatial spectrum that exhibits super-resolution. We propose to use the singular value decomposition (SVD) of the data matrix to summarize multiple time or frequency samples. Our formulation leads to an optimization problem, which we solve efficiently in a second-order cone (SOC) programming framework by an interior point implementation. We propose a grid refinement method to mitigate the effects of limiting estimates to a grid of spatial locations and introduce an automatic selection criterion for the regularization parameter involved in our approach. We demonstrate the effectiveness of the method on simulated data by plots of spatial spectra and by comparing the estimator variance to the Cramar-Rao bound (CRB). We observe that our approach has a number of advantages over other source localization techniques, including increased resolution, improved robustness to noise, limitations in data quantity, and correlation of the sources, as well as not requiring an accurate initialization.	MIT, Dept Elect Engn & Comp Sci, Cambridge, MA 02139 USA	Malioutov, D (reprint author), MIT, Dept Elect Engn & Comp Sci, 77 Massachusetts Ave, Cambridge, MA 02139 USA.						Sacchi MD, 1998, IEEE T SIGNAL PROCES, V46, P31, DOI 10.1109/78.651165; Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010; Rao BD, 2003, IEEE T SIGNAL PROCES, V51, P760, DOI 10.1109/TSP.2002.808076; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Gribonval R, 2003, IEEE T INFORM THEORY, V49, P3320, DOI 10.1109/TIT.2003.820031; Charbonnier P, 1997, IEEE T IMAGE PROCESS, V6, P298, DOI 10.1109/83.551699; Krim H, 1996, IEEE SIGNAL PROC MAG, V13, P67, DOI 10.1109/79.526899; MALLAT SG, 1993, IEEE T SIGNAL PROCES, V41, P3397, DOI 10.1109/78.258082; CAPON J, 1969, P IEEE, V57, P1408, DOI 10.1109/PROC.1969.7278; Rao BD, 1999, IEEE T SIGNAL PROCES, V47, P187, DOI 10.1109/78.738251; Cetin M, 2001, IEEE T IMAGE PROCESS, V10, P623, DOI 10.1109/83.913596; Lobo MS, 1998, LINEAR ALGEBRA APPL, V284, P193, DOI 10.1016/S0024-3795(98)10032-0; Bradley P. S., 1998, INFORMS Journal on Computing, V10, DOI 10.1287/ijoc.10.2.209; DONOHO DL, UNPUB IEEE T INF THE; DONOHO DL, 1992, SIAM J MATH ANAL, V23, P1309, DOI 10.1137/0523074; DONOHO DL, 2003, P NAT ACAD SCI, V1000, P2197; FUCHS JJ, P 13 S SYST ID, P1357; Fuchs JJ, 2001, IEEE T SIGNAL PROCES, V49, P702, DOI 10.1109/78.912914; Fuchs J.-J., 1996, P IEEE INT C AC SPEE, V6, P3161; FUCHS JJ, 1998, IEEE P ICASSP, V3, P1649; Gorodetsky VR, 2000, GEMATOL TRANSFUZIOL, V45, P3; GRIBONVAL R, 2003, UNPUB HIGHLY SPARSE; HANSEN PC, 1992, SIAM REV, V34, P561, DOI 10.1137/1034115; Jeffs B. D., 1998, P ICASSP, P1885, DOI 10.1109/ICASSP.1998.681832; Johnson D H, 1993, ARRAY SIGNAL PROCESS; MALIOUTOV DM, 2004, P IEEE INT C AC SPEE; Malioutov D.M., 2003, SPARSE SIGNAL RECONS; Miller A, 2002, SUBSET SELECTION REG, V2nd; Morozov V.A., 1966, SOV MATH DOKL, V7, P414; Nemirovski A., 2001, LECT MODERN CONVEX O; RAO BD, 1998, P 32 AS C SIGN SYST, V1, P752; Sardy S, 2001, IEEE T SIGNAL PROCES, V49, P1146, DOI 10.1109/78.923297; Schmidt R. O., 1981, THESIS STANFORD U ST; SIVANAND S, 1989, P IEEE INT C AC SPEE, V4, P2772; Sturm J. F., 2001, USING SEDUMI 1 02 MA; TROPP JA, 2003, 0304 ICES U TEX AUST; TROPP JA, 2004, UNPUB IEEE T INF THE	37	436	510	14	52	IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC	PISCATAWAY	445 HOES LANE, PISCATAWAY, NJ 08855 USA	1053-587X			IEEE T SIGNAL PROCES	IEEE Trans. Signal Process.	AUG	2005	53	8	2				3010	3022		10.1109/TSP.2005.850882		13	Engineering, Electrical & Electronic	Engineering	947NM	WOS:000230652900005		
J	Turlach, BA; Venables, WN; Wright, SJ				Turlach, BA; Venables, WN; Wright, SJ			Simultaneous variable selection	TECHNOMETRICS			English	Article						constrained least squares problem; constrained regression; convex programming; infrared spectrometry; interior-point algorithm; quadratic programming; subset selection; variable selection	LEAST-SQUARES PROBLEMS; REGRESSION; LASSO	We propose a new method for selecting a common subset of explanatory variables where the aim is to model several response variables. The idea is a natural extension of the LASSO technique proposed by Tibshirani (1996) and is based on the (joint) residual sum of squares while constraining the parameter estimates to lie within a suitable polyhedral region. The properties of the resulting convex programming problem are analyzed for the special case of an orthonormal design. For the general case, we develop an efficient interior point algorithm. The method is illustrated on a dataset with infrared spectrometry measurements on 14 qualitatively different but correlated responses using 770 wavelengths. The aim is to select a subset of the wavelengths suitable for use as predictors for as many of the responses as possible.	Univ Western Australia, Sch Math & Stat M019, Nedlands, WA 6009, Australia; CSIRO Math & Informat Sci, Cleveland, Qld 4163, Australia; Univ Wisconsin, Dept Comp Sci, Madison, WI 53706 USA	Turlach, BA (reprint author), Univ Western Australia, Sch Math & Stat M019, Nedlands, WA 6009, Australia.		Turlach, Berwin/A-4995-2008; Venables, William/B-3356-2010	Turlach, Berwin/0000-0001-8795-471X; 			BREIMAN L, 1995, TECHNOMETRICS, V37, P373, DOI 10.2307/1269730; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Osborne MR, 2000, IMA J NUMER ANAL, V20, P389, DOI 10.1093/imanum/20.3.389; Sturm JF, 1999, OPTIM METHOD SOFTW, V11-2, P625, DOI 10.1080/10556789908805766; Brown PJ, 2002, J R STAT SOC B, V64, P519, DOI 10.1111/1467-9868.00348; FRANK IE, 1993, TECHNOMETRICS, V35, P109, DOI 10.2307/1269656; Zou H, 2005, J ROY STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x; Efron B, 2004, ANN STAT, V32, P407; Lobo MS, 1998, LINEAR ALGEBRA APPL, V284, P193, DOI 10.1016/S0024-3795(98)10032-0; Bakin S, 1999, THESIS AUSTR NATL U; Breiman L, 1997, J ROY STAT SOC B MET, V59, P3, DOI 10.1111/1467-9868.00054; Brown P. J., 1993, MEASUREMENT REGRESSI; Brown PJ, 1998, J R STAT SOC B, V60, P627, DOI 10.1111/1467-9868.00144; Brown PJ, 1999, BIOMETRIKA, V86, P635; Burnham K. P., 1998, MODEL SELECTION INFE; CLARK DI, 1988, IMA J NUMER ANAL, V8, P23, DOI 10.1093/imanum/8.1.23; Clarke F. H., 1990, OPTIMIZATION NONSMOO; Draper N. R., 1998, APPL REGRESSION ANAL; Hastie TJ, 1990, GEN ADDITIVE MODELS; Hocking R. R., 1996, METHODS APPL LINEAR; LEAMER EE, 1978, J AM STAT ASSOC, V73, P580, DOI 10.2307/2286604; Martens H., 1989, MULTIVARIATE CALIBRA; McCullagh P., 1989, GEN LINEAR MODELS; Mehrotra S, 1992, SIAM J OPTIMIZ, V2, P575, DOI 10.1137/0802028; Miller A. J., 1990, SUBSET SELECTION REG; OSBORNE M. R., 1985, FINITE ALGORITHMS OP; Osborne MR, 2000, J COMPUT GRAPH STAT, V9, P319, DOI 10.2307/1390657; OSBORNE MR, 1992, IMA J NUMER ANAL, V12, P151, DOI 10.1093/imanum/12.2.151; Rockafellar R.T., 1970, CONVEX ANAL; ROOS C, 1997, THEORY ALGORITHMS LI; Wold H., 1985, ENCY STATISTICAL SCI, V6, P581; Wright SJ, 1997, PRIMAL DUAL INTERIOR; Ye Y., 1997, INTERIOR POINT ALGOR	33	88	89	5	11	AMER STATISTICAL ASSOC	ALEXANDRIA	1429 DUKE ST, ALEXANDRIA, VA 22314 USA	0040-1706			TECHNOMETRICS	Technometrics	AUG	2005	47	3					349	363		10.1198/004017005000000139		15	Statistics & Probability	Mathematics	949RS	WOS:000230806400010		
J	Huang, XH; Pan, W; Grindle, S; Han, XQ; Chen, YJ; Park, SJ; Miller, LW; Hall, J				Huang, XH; Pan, W; Grindle, S; Han, XQ; Chen, YJ; Park, SJ; Miller, LW; Hall, J			A comparative study of discriminating human heart failure etiology using gene expression profiles	BMC BIOINFORMATICS			English	Article							PARTIAL LEAST-SQUARES; TUMOR CLASSIFICATION; MICROARRAY DATA; LINEAR-REGRESSION; CANCER; CARDIOMYOPATHY; AMIODARONE; SHRINKAGE; DIAGNOSIS; SELECTION	Background: Human heart failure is a complex disease that manifests from multiple genetic and environmental factors. Although ischemic and non-ischemic heart disease present clinically with many similar decreases in ventricular function, emerging work suggests that they are distinct diseases with different responses to therapy. The ability to distinguish between ischemic and non-ischemic heart failure may be essential to guide appropriate therapy and determine prognosis for successful treatment. In this paper we consider discriminating the etiologies of heart failure using gene expression libraries from two separate institutions. Results: We apply five new statistical methods, including partial least squares, penalized partial least squares, LASSO, nearest shrunken centroids and random forest, to two real datasets and compare their performance for multiclass classification. It is found that the five statistical methods perform similarly on each of the two datasets: it is difficult to correctly distinguish the etiologies of heart failure in one dataset whereas it is easy for the other one. In a simulation study, it is confirmed that the five methods tend to have close performance, though the random forest seems to have a slight edge. Conclusions: For some gene expression data, several recently developed discriminant methods may perform similarly. More importantly, one must remain cautious when assessing the discriminating performance using gene expression profiles based on a small dataset; our analysis suggests the importance of utilizing multiple or larger datasets.	Univ Minnesota, Sch Publ Hlth, Div Biostat, Minneapolis, MN 55455 USA; Univ Minnesota, Sch Med, Dept Med, Div Cardiovasc, Minneapolis, MN 55455 USA	Pan, W (reprint author), Univ Minnesota, Sch Publ Hlth, Div Biostat, Minneapolis, MN 55455 USA.	xiaohong@biostat.umn.edu; weip@biostat.umn.edu; grind001@umn.edu; hanxx057@umn.edu; chenx106@umn.edu; jlhall@umn.edu; mille278@umn.edu; jlhall@umn.edu					Allwein E. L., 2001, J MACHINE LEARNING R, V1, P113, DOI DOI 10.1162/15324430152733133; Tan YX, 2004, COMPUT BIOL CHEM, V28, P235, DOI 10.1016/j.compbiolchem.2004.05.002; TITTERINGTON DM, 1981, J ROY STAT SOC A STA, V144, P145, DOI 10.2307/2981918; WOLD S, 1984, SIAM J SCI STAT COMP, V5, P735, DOI 10.1137/0905052; Nguyen DV, 2002, BIOINFORMATICS, V18, P39, DOI 10.1093/bioinformatics/18.1.39; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324; Levy D, 1996, JAMA-J AM MED ASSOC, V275, P1557, DOI 10.1001/jama.275.20.1557; Kittleson M, 2003, J AM COLL CARDIOL, V41, P2029, DOI 10.1016/S0735-1097(03)00417-0; Ambroise C, 2002, P NATL ACAD SCI USA, V99, P6562, DOI 10.1073/pnas.102102699; Golub TR, 1999, SCIENCE, V286, P531, DOI 10.1126/science.286.5439.531; Simon R, 2003, J NATL CANCER I, V95, P14; Dudoit S, 2002, J AM STAT ASSOC, V97, P77, DOI 10.1198/016214502753479248; Felker GM, 2003, J AM COLL CARDIOL, V41, P997, DOI 10.1016/S0735-1097(02)02968-6; Hastie T, 1998, ANN STAT, V26, P451; Segal MR, 2003, J COMPUT BIOL, V10, P961, DOI 10.1089/106652703322756177; FRANK IE, 1993, TECHNOMETRICS, V35, P109, DOI 10.2307/1269656; Felker GM, 2000, NEW ENGL J MED, V342, P1077, DOI 10.1056/NEJM200004133421502; DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425; DONOHO DL, 1995, IEEE T INFORM THEORY, V41, P613, DOI 10.1109/18.382009; DOVAL HC, 1994, LANCET, V344, P493, DOI 10.1016/S0140-6736(94)91895-3; Tibshirani R, 2002, P NATL ACAD SCI USA, V99, P6567, DOI 10.1073/pnas.082099299; Ramaswamy S, 2001, P NATL ACAD SCI USA, V98, P15149, DOI 10.1073/pnas.211566398; Aronow BJ, 2001, PHYSIOL GENOMICS, V6, P19; Breiman L., 1984, CLASSIFICATION REGRE; DETTLING M, 2003, BIOINFORMATICS, V19, P1063; Ding B., 2004, BIOCONDUCTOR PROJECT; Dries DL, 2001, J AM COLL CARDIOL, V38, P421, DOI 10.1016/S0735-1097(01)01408-5; Fort G, 2005, BIOINFORMATICS, V21, P1104, DOI 10.1093/bioinformatics/bti114; FRIEDMAN J, 1996, APPROACH POLYCHOTOMO; Fu WJJ, 2005, BIOINFORMATICS, V21, P1979, DOI 10.1093/bioinformatics/bti294; Ghosh D, 2003, BIOMETRICS, V59, P992, DOI 10.1111/j.0006-341X.2003.00114.x; Ghosh Debashis, 2002, Pac Symp Biocomput, P18; Hall JL, 2004, PHYSIOL GENOMICS, V17, P283, DOI 10.1152/physiolgenomics.00004.2004; Hastie T., 2001, ELEMENT STAT LEARNIN; HAWKINS DM, 2003, CHANCE, V16, P19; Hedenfalk I, 2001, NEW ENGL J MED, V344, P539, DOI 10.1056/NEJM200102223440801; Huang XH, 2003, BIOINFORMATICS, V19, P2072, DOI 10.1093/bioinformatics/btg283; Huang XH, 2005, COMPUT BIOL CHEM, V29, P204, DOI 10.1016/j.compbiolchem.2005.04.002; Huang XH, 2004, BIOINFORMATICS, V20, P888, DOI 10.1093/bioinformatics/btg499; Hwang JJ, 2002, PHYSIOL GENOMICS, V10, P31, DOI 10.1152/physiolgenomics.00122.2001.; IHAKA R, 1996, J COMPUTATIONAL GRAP, V5, P299, DOI DOI 10.2307/1390807; Kittleson MM, 2004, CIRCULATION, V110, P3444, DOI 10.1161/01.CIR.0000148178.19465.11; Lloyd-Jones D M, 2001, Curr Cardiol Rep, V3, P184, DOI 10.1007/s11886-001-0021-1; McLachlan G., 1992, DISCRIMINANT ANAL ST; Nicol RL, 2000, ANNU REV GENOM HUM G, V1, P179, DOI 10.1146/annurev.genom.1.1.179; Rifkin R, 2003, SIAM REV, V45, P706, DOI 10.1137/S0036144502411986; Simon R, 2004, NAT CLIN PRACT ONCOL, V1, P4, DOI 10.1038/ncponc0006; SINGH SN, 1995, NEW ENGL J MED, V333, P77, DOI 10.1056/NEJM199507133330201; Tan FL, 2002, P NATL ACAD SCI USA, V99, P11387, DOI 10.1073/pnas.162370099; West M., 2003, BAYESIAN STAT, P723; Wu BL, 2003, BIOINFORMATICS, V19, P1636, DOI 10.1093/bioinformatics/btg210; Zhang H, 1999, RECURSIVE PARTITIONI; Zhang HP, 2001, P NATL ACAD SCI USA, V98, P6730, DOI 10.1073/pnas.111153698	54	25	27	0	3	BIOMED CENTRAL LTD	LONDON	MIDDLESEX HOUSE, 34-42 CLEVELAND ST, LONDON W1T 4LB, ENGLAND	1471-2105			BMC BIOINFORMATICS	BMC Bioinformatics	AUG 24	2005	6								205	10.1186/1471-2105-6-205		15	Biochemical Research Methods; Biotechnology & Applied Microbiology; Mathematical & Computational Biology	Biochemistry & Molecular Biology; Biotechnology & Applied Microbiology; Mathematical & Computational Biology	966OU	WOS:000232031200001	16120216	
J	Veenman, CJ; Tax, DMJ				Veenman, CJ; Tax, DMJ			LESS: A model-based classifier for sparse subspaces	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						classification; support vector machine; high-dimensional; feature subset selection; mathematical programming	GENE-EXPRESSION; FEATURE-SELECTION; CANCER; LASSO	In this paper, we specifically focus on high-dimensional data sets for which the number of dimensions is an order of magnitude higher than the number of objects. From a classifier design standpoint, such small sample size problems have some interesting challenges. The first challenge is to find, from all hyperplanes that separate the classes, a separating hyperplane which generalizes well for future data. A second important task is to determine which features are required to distinguish the classes. To attack these problems, we propose the LESS ( Lowest Error in a Sparse Subspace) classifier that efficiently finds linear discriminants in a sparse subspace. In contrast with most classifiers for high-dimensional data sets, the LESS classifier incorporates a (simple) data model. Further, by means of a regularization parameter, the classifier establishes a suitable trade-off between subspace sparseness and classification accuracy. In the experiments, we show how LESS performs on several high-dimensional data sets and compare its performance to related state-of-the-art classifiers like, among others, linear ridge regression with the LASSO and the Support Vector Machine. It turns out that LESS performs competitively while using fewer dimensions.	Delft Univ Technol, Fac Elect Engn, Dept Mediamat, NL-2600 GA Delft, Netherlands	Veenman, CJ (reprint author), Delft Univ Technol, Fac Elect Engn, Dept Mediamat, POB 5031, NL-2600 GA Delft, Netherlands.	C.J.Veenman@ewi.tudelft.nl; D.M.J.Tax@ewi.tudelft.nl					Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Atkeson CG, 1997, ARTIF INTELL REV, V11, P11, DOI 10.1023/A:1006559212014; Alon U, 1999, P NATL ACAD SCI USA, V96, P6745, DOI 10.1073/pnas.96.12.6745; Golub TR, 1999, SCIENCE, V286, P531, DOI 10.1126/science.286.5439.531; Burges CJC, 1998, DATA MIN KNOWL DISC, V2, P121, DOI 10.1023/A:1009715923555; Tibshirani R, 2002, P NATL ACAD SCI USA, V99, P6567, DOI 10.1073/pnas.082099299; Kohavi R, 1997, ARTIF INTELL, V97, P273, DOI 10.1016/S0004-3702(97)00043-X; Bhattacharyya C, 2003, SIGNAL PROCESS, V83, P729, DOI 10.1016/S0165-1684(02)00474-7; Blake C.L., 1998, UCI REPOSITORY MACHI; Bradley P. S., 1998, P 15 INT C MACH LEAR, P82; BREIMAN L, 1993, BETTER SUBSET SELECT; Debuse J. C. W., 1997, Journal of Intelligent Information Systems: Integrating Artificial Intelligence and Database Technologies, V9, DOI 10.1023/A:1008641220268; Duda R. O., 2001, PATTERN CLASSIFICATI; *FREE SOFTW FDN, 2005, GNLI LIN PROGR; FUNG G, 2000, P 6 ACM SIGKDD INT C, V2094, P64; GORMAN RP, 1988, NEURAL NETWORKS, V1, P75, DOI 10.1016/0893-6080(88)90023-8; Karzynski M, 2003, ARTIF INTELL REV, V20, P39, DOI 10.1023/A:1026032530166; Koutroumbas K., 1999, PATTERN RECOGNITION; Osborne MR, 2000, J COMPUT GRAPH STAT, V9, P319, DOI 10.2307/1390657; SIGILLITO VG, 1989, J HOPKINS APL TECH D, V10, P262; van't Veer LJ, 2002, NATURE, V415, P530, DOI 10.1038/415530a; Vapnik V., 1998, STAT LEARNING THEORY; Zhang HB, 2002, PATTERN RECOGN, V35, P701, DOI 10.1016/S0031-3203(01)00046-2	23	17	17	1	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	SEP	2005	27	9					1496	1500		10.1109/TPAMI.2005.182		5	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Computer Science; Engineering	944XB	WOS:000230463300012	16173192	
J	Roberts, S; Martin, M				Roberts, S; Martin, M			A critical assessment of shrinkage-based regression approaches for estimating the adverse health effects of multiple air pollutants	ATMOSPHERIC ENVIRONMENT			English	Article						time series; air pollution; mortality; lasso; ridge regression	DAILY MORTALITY; PARTICULATE MATTER; TIME-SERIES; CARDIOVASCULAR-DISEASES; SULFUR-DIOXIDE; POLLUTION; CITIES; MODELS; PARTICLES; EXPOSURE	Most investigations of the adverse health effects of multiple air pollutants analyse the time series involved by simultaneously entering the multiple pollutants into a Poisson log-linear model. Concerns have been raised about this type of analysis, and it has been stated that new methodology or models should be developed for investigating the adverse health effects of multiple air pollutants. In this paper, we introduce the use of the lasso for this purpose and compare its statistical properties to those of ridge regression and the Poisson log-linear model. Ridge regression has been used in time series analyses on the adverse health effects of multiple air pollutants but its properties for this purpose have not been investigated. A series of simulation studies was used to compare the performance of the lasso, ridge regression, and the Poisson log-linear model. In these simulations, realistic mortality time series were generated with known air pollution mortality effects permitting the performance of the three models to be compared. Both the lasso and ridge regression produced more accurate estimates of the adverse health effects of the multiple air pollutants than those produced using the Poisson log-linear model. This increase in accuracy came at the expense of increased bias. Ridge regression produced more accurate estimates than the lasso, but the lasso produced more interpretable models. The lasso and ridge regression offer a flexible way of obtaining more accurate estimation of pollutant effects than that provided by the standard Poisson log-linear model. (c) 2005 Elsevier Ltd. All rights reserved.	Australian Natl Univ, Fac Econ & Commerce, Sch Finance & Appl Stat, Canberra, ACT 0200, Australia	Roberts, S (reprint author), Australian Natl Univ, Fac Econ & Commerce, Sch Finance & Appl Stat, Canberra, ACT 0200, Australia.	steven.roberts@anu.edu.au	Martin, Michael/A-1295-2008				Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Roberts S, 2004, ENVIRON RES, V96, P328, DOI 10.1016/j.envres.2004.01.015; Hong YC, 1999, ENVIRON HEALTH PERSP, V107, P873, DOI 10.2307/3454474; Stieb DM, 2002, J AIR WASTE MANAGE, V52, P470; Wong TW, 2002, OCCUP ENVIRON MED, V59, P30, DOI 10.1136/oem.59.1.30; Chock DP, 2000, J AIR WASTE MANAGE, V50, P1481; Cifuentes LA, 2000, J AIR WASTE MANAGE, V50, P1287; Cox LH, 2000, ENVIRONMETRICS, V11, P611, DOI 10.1002/1099-095X(200011/12)11:6<611::AID-ENV443>3.0.CO;2-Y; Daniels MJ, 2000, AM J EPIDEMIOL, V152, P397, DOI 10.1093/aje/152.5.397; DERRIENNIC F, 1989, INT J EPIDEMIOL, V18, P186, DOI 10.1093/ije/18.1.186; Dominici F, 2003, J TOXICOL ENV HEAL A, V66, P1883, DOI 10.1080/15287390390212468; HALES S, 1999, AUSTR NZ J PUBLIC HL, V24, P88; Hastie T., 2001, ELEMENTS STAT LEARNI; Hastie TJ, 1990, GEN ADDITIVE MODELS; ITO K, 1995, INHAL TOXICOL, V7, P735, DOI 10.3109/08958379509014477; Lee JT, 2000, ENVIRON RES, V84, P247, DOI 10.1006/enrs.2000.4096; McCullagh P., 1989, GEN LINEAR MODELS; Moolgavkar SH, 2003, INHAL TOXICOL, V15, P877, DOI 10.1080/08958370390215767; Ostro BD, 1999, ENVIRON RES, V81, P231, DOI 10.1006/enrs.1999.3978; Rahlenbeck SI, 1996, INT J EPIDEMIOL, V25, P1220, DOI 10.1093/ije/25.6.1220; Roberts S, 2005, J AIR WASTE MANAGE, V55, P273; Smith RL, 2000, ENVIRONMETRICS, V11, P719, DOI 10.1002/1099-095X(200011/12)11:6<719::AID-ENV438>3.3.CO;2-L; Vedal S, 2003, ENVIRON HEALTH PERSP, V111, P45, DOI 10.1289/ehp.5276; Venners SA, 2003, ENVIRON HEALTH PERSP, V111, P562, DOI 10.1289/ehp.5664; WAI WT, 1997, STUDY SHORT TERM EFF; Yang CY, 2004, J TOXICOL ENV HEAL A, V67, P483, DOI 10.1080/15287390490276502	26	6	7	1	3	PERGAMON-ELSEVIER SCIENCE LTD	OXFORD	THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND	1352-2310			ATMOS ENVIRON	Atmos. Environ.	OCT	2005	39	33					6223	6230		10.1016/j.atmosenv.2005.07.004		8	Environmental Sciences; Meteorology & Atmospheric Sciences	Environmental Sciences & Ecology; Meteorology & Atmospheric Sciences	976VW	WOS:000232762600021		
J	Viaene, S; Dedene, G; Derrig, RA				Viaene, S; Dedene, G; Derrig, RA			Auto claim fraud detection using Bayesian learning neural networks	EXPERT SYSTEMS WITH APPLICATIONS			English	Article						automobile insurance; claim fraud; neural network; Bayesian learning; evidence framework	ROC CURVE; CLASSIFICATION; REGRESSION; PREDICTIONS; FRAMEWORK; SHRINKAGE; MODELS; AREA	This article explores the explicative capabilities of neural network classifiers with automatic relevance determination weight regularization, and reports the findings from applying these networks for personal injury protection automobile insurance claim fraud detection. The automatic relevance determination objective function scheme provides us with a way to determine which inputs are most informative to the trained neural network model. An implementation of MacKay's, (1992a,b) evidence framework approach to Bayesian learning is proposed as a practical way of training such networks. The empirical evaluation is based on a data set of closed claims from accidents that occurred in Massachusetts, USA during 1993. (c) 2005 Elsevier Ltd. All rights reserved.	Katholieke Univ Leuven, B-3000 Louvain, Belgium; Vlerick Leuven Gent Management Sch, B-9000 Ghent, Belgium; Univ Amsterdam, NL-1018 WB Amsterdam, Netherlands; Automobile Insurers Bur Massachusetts, Boston, MA 02110 USA; Insurance Fraud Bur Massachusetts, Boston, MA 02110 USA	Viaene, S (reprint author), Katholieke Univ Leuven, Naamsestr 69, B-3000 Louvain, Belgium.	stijn.viaene@econ.kuleuven.ac.be	Viaene, Stijn/C-2981-2009				Allison PD, 1999, LOGISTIC REGRESSION; Provost F, 2003, MACH LEARN, V52, P199, DOI 10.1023/A:1024099825458; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324; Bauer E, 1999, MACH LEARN, V36, P105, DOI 10.1023/A:1007515423169; Provost F, 2001, MACH LEARN, V42, P203, DOI 10.1023/A:1007601015854; HANLEY JA, 1982, RADIOLOGY, V143, P29; Hand DJ, 2001, MACH LEARN, V45, P171, DOI 10.1023/A:1010920819831; MACKAY DJC, 1992, NEURAL COMPUT, V4, P448, DOI 10.1162/neco.1992.4.3.448; COPAS JB, 1983, J R STAT SOC B, V45, P311; Bellman R.E., 1961, ADAPTIVE CONTROL PRO; Bengio Y, 2000, NEURAL COMPUT, V12, P1889, DOI 10.1162/089976600300015187; Bishop C. M., 1995, NEURAL NETWORKS PATT; Breiman L., 1984, CLASSIFICATION REGRE; BREIMAN L, 2001, NONP LARG MULT DAT M; Buntine W., 1990, THESIS U TECHNOLOGY; *CAN COAL INS FRAU, 2002, INS FRAUD; Cestnik B., 1990, P EUR C ART INT, P147; *COAL INS FRAUD, 2002, INS FRAUD CRIM YOU P; *COM EUR ASS, 1996, EUR INS ANT GUID CEA; *COM EUR ASS, 1997, EUR INS ANT GUID CEA; Cussens J., 1993, P EUR C MACH LEARN V, P136; DERRIG RA, 2002, J RISK INSURANCE, V69; DERRIG RA, 1998, R9841 DOI AUT INS BU; Desai VS, 1996, EUR J OPER RES, V95, P24, DOI 10.1016/0377-2217(95)00246-4; DIACONIS P, 1983, SCI AM, V248, P116; Dietterichl TG, 2002, HDB BRAIN THEORY NEU, P405; Domingos P., 2000, P 17 INT C MACH LEAR, P223; Drummond C., 2000, Proceedings. KDD-2000. Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, DOI 10.1145/347090.347126; Duda R.O., 2000, PATTERN CLASSIFICATI; Fayyad U. M., 1993, P 13 INT JOINT C ART, P1022; Good I. J., 1965, ESTIMATION PROBABILI; GRANDVALET Y, 1998, P 8 INT C ART NEUR N, P201; Hand D.J., 1997, CONSTRUCTION ASSESSM; JOOS P, 1998, P ECML WORKSH APPL M, P59; Kass G. V., 1980, APPL STATIST, V29, P119, DOI DOI 10.2307/2986296; Kohavi R., 1997, P 9 EUR C MACH LEARN; LACHER RC, 1995, EUR J OPER RES, V85, P53, DOI 10.1016/0377-2217(93)E0274-2; Lee KC, 1996, DECIS SUPPORT SYST, V18, P63, DOI 10.1016/0167-9236(96)00018-8; MacKay D, 1994, ASHRAE T, V100, P1053; MACKAY DJC, 1992, NEURAL COMPUT, V4, P720, DOI 10.1162/neco.1992.4.5.720; MARQUARDT DW, 1980, J AM STAT ASSOC, V75, P87, DOI 10.2307/2287388; Mobley BA, 2000, ARTIF INTELL MED, V18, P187, DOI 10.1016/S0933-3657(99)00040-8; Nabney I, 2001, NETLAB ALGORITHMS PA; Neal R., 1998, NEURAL NETWORKS MACH; Neal R. M., 1996, BAYESIAN LEARNING NE; Opitz D., 1999, J ARTIFICIAL INTELLI, V11, P169, DOI DOI 10.1613/JAIR.614; Parmanto B., 1996, Connection Science, V8, DOI 10.1080/095400996116848; Piramuthu S, 1999, EUR J OPER RES, V112, P310, DOI 10.1016/S0377-2217(97)00398-6; Quinlan J., 1993, C4 5 PROGRAMS MACHIN; Salchenberger L, 1997, COMPUT OPER RES, V24, P435, DOI 10.1016/S0305-0548(96)00064-0; SHARMA SK, 1996, ITAL J FOOD SCI, V2, P107; VANDELAAR P, 2000, INPUT SELECTION BASE; VIAENE S, 2002, THESIS KU LEUEVEN; Viaene S, 2002, J RISK INSUR, V69, P373, DOI 10.1111/1539-6975.00023; Weisberg Herbert W., 1991, J INSURANCE REGULATI, V9, P497; WEISBERG HI, 1995, R9512 AUT INS BUR MA; Weisberg H.I., 1998, RISQUES, V35, P75; Zadrozny B., 2001, P 18 INT C MACH LEAR, P609	59	21	21	2	8	PERGAMON-ELSEVIER SCIENCE LTD	OXFORD	THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND	0957-4174			EXPERT SYST APPL	Expert Syst. Appl.	OCT	2005	29	3					653	666		10.1016/j.eswa.2005.04.030		14	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic; Operations Research & Management Science	Computer Science; Engineering; Operations Research & Management Science	961JW	WOS:000231659400014		
J	Huang, T; Wu, BL; Lizardi, P; Zhao, HY				Huang, T; Wu, BL; Lizardi, P; Zhao, HY			Detection of DNA copy number alterations using penalized least squares regression	BIOINFORMATICS			English	Article							COMPARATIVE GENOMIC HYBRIDIZATION; ARRAY CGH DATA; MICROARRAYS; IDENTIFICATION; SHRINKAGE; SELECTION	Motivation: Genomic DNA copy number alterations are characteristic of many human diseases including cancer. Various techniques and platforms have been proposed to allow researchers to partition the whole genome into segments where copy numbers change between contiguous segments, and subsequently to quantify DNA copy number alterations. In this paper, we incorporate the spatial dependence of DNA copy number data into a regression model and formalize the detection of DNA copy number alterations as a penalized least squares regression problem. In addition, we use a stationary bootstrap approach to estimate the statistical significance and false discovery rate. Results: The proposed method is studied by simulations and illustrated by an application to an extensively analyzed dataset in the literature. The results show that the proposed method can correctly detect the numbers and locations of the true breakpoints while appropriately controlling the false positives.	Yale Univ, Sch Med, Dept Epidemiol & Publ Hlth, New Haven, CT 06520 USA; Yale Univ, Sch Med, Dept Pathol, New Haven, CT 06520 USA; Yale Univ, Sch Med, Dept Mol Biochem & Biophys, New Haven, CT 06520 USA; Yale Univ, Sch Med, Dept Genet, New Haven, CT 06520 USA; Univ Minnesota, Sch Publ Hlth, Div Biostat, Minneapolis, MN 55455 USA	Zhao, HY (reprint author), Yale Univ, Sch Med, Dept Epidemiol & Publ Hlth, New Haven, CT 06520 USA.	hongyu.zhao@yale.edu					KALLIONIEMI A, 1992, SCIENCE, V258, P818, DOI 10.1126/science.1359641; Storey JD, 2002, J ROY STAT SOC B, V64, P479, DOI 10.1111/1467-9868.00346; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Antoniadis A, 2001, J AM STAT ASSOC, V96, P939, DOI 10.1198/016214501753208942; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; LISITSYN N, 1993, SCIENCE, V259, P946, DOI 10.1126/science.8438152; Snijders AM, 2001, NAT GENET, V29, P263, DOI 10.1038/ng754; Zhao XJ, 2004, CANCER RES, V64, P3060, DOI 10.1158/0008-5472.CAN-03-3308; Lengauer C, 1998, NATURE, V396, P643, DOI 10.1038/25292; BENJAMINI Y, 1995, J ROY STAT SOC B MET, V57, P289; Pinkel D, 1998, NAT GENET, V20, P207, DOI 10.1038/2524; Bertone P, 2004, SCIENCE, V306, P2242, DOI 10.1126/science.1103388; Olshen AB, 2004, BIOSTATISTICS, V5, P557, DOI 10.1093/biostatistics/kxh008; Efron B, 2004, ANN STAT, V32, P407; DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425; POLITIS DN, 1994, J AM STAT ASSOC, V89, P1303, DOI 10.2307/2290993; Brockwell PJ, 1996, INTRO TIME SERIES FO; Fan JQ, 2004, P NATL ACAD SCI USA, V101, P1135, DOI 10.1073/pnas.0307557100; Fridlyand J, 2004, J MULTIVARIATE ANAL, V90, P132, DOI 10.1016/j.jmva.2004.02.008; Hsu L, 2005, BIOSTATISTICS, V6, P211, DOI 10.1093/biostatistics/kxi004; JONG K, 2003, APPL EVOLUTIONARY CO, V2611, P54, DOI 10.1007/3-540-36605-9_6; LAI YL, 2005, COMP BIO CHEM, V29, P90; Picard F, 2005, BMC BIOINFORMATICS, V6, DOI 10.1186/1471-2105-6-27; Price TS, 2005, NUCLEIC ACIDS RES, V33, P3455, DOI 10.1093/nar/gki643; Wang P, 2005, BIOSTATISTICS, V6, P45, DOI 10.1093/biostatistics/kxh017	25	27	29	1	4	OXFORD UNIV PRESS	OXFORD	GREAT CLARENDON ST, OXFORD OX2 6DP, ENGLAND	1367-4803			BIOINFORMATICS	Bioinformatics	OCT 15	2005	21	20					3811	3817		10.1093/bioinformatics/bti646		7	Biochemical Research Methods; Biotechnology & Applied Microbiology; Computer Science, Interdisciplinary Applications; Mathematical & Computational Biology; Statistics & Probability	Biochemistry & Molecular Biology; Biotechnology & Applied Microbiology; Computer Science; Mathematical & Computational Biology; Mathematics	974MQ	WOS:000232596300001	16131523	
J	Chipman, HA; Gu, H				Chipman, HA; Gu, H			Interpretable dimension reduction	JOURNAL OF APPLIED STATISTICS			English	Article						principal component; interpretable; homogeneity; sparsity; stepwise algorithm; dimension reduction; data mining	PRINCIPAL COMPONENTS; LASSO	The analysis of high-dimensional data often begins with the identification of lower dimensional subspaces. Principal component analysis is a dimension reduction technique that identifies linear combinations of variables along which most variation occurs or which best "reconstruct" the original variables. For example, many temperature readings may be taken in a production process when in fact there are just a few underlying variables driving the process. A problem with principal components is that the linear combinations can seem quite arbitrary. To make them more interpretable, we introduce two classes of constraints. In the first, coefficients are constrained to equal a small number of values ( homogeneity constraint). The second constraint attempts to set as many coefficients to zero as possible ( sparsity constraint). The resultant interpretable directions are either calculated to be close to the original principal component directions, or calculated in a stepwise manner that may make the components more orthogonal. A small dataset on characteristics of cars is used to introduce the techniques. A more substantial data mining application is also given, illustrating the ability of the procedure to scale to a very large number of variables.	Acadia Univ, Dept Math & Stat, Wolfville, NS B4P 2R6, Canada; Dalhousie Univ, Dept Math & Stat, Halifax, NS, Canada	Chipman, HA (reprint author), Acadia Univ, Dept Math & Stat, Wolfville, NS B4P 2R6, Canada.	hugh.chipman@acadiau.ca					Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; FURNIVAL GM, 1974, TECHNOMETRICS, V16, P499, DOI 10.2307/1267601; Jolliffe IT, 2003, J COMPUT GRAPH STAT, V12, P531, DOI 10.1198/1061860032148; CADIMA J, 1995, J APPL STAT, V22, P203, DOI 10.1080/757584614; Hausman R., 1982, OPTIMIZATION STAT, P137; JOLLIFFE IT, 1989, APPL STAT-J ROY ST C, V38, P139, DOI 10.2307/2347688; Jolliffe IT, 2000, J COMPUT GRAPH STAT, V9, P689, DOI 10.2307/1391088; Lock RH, 1993, J STAT ED, V1; Radhakrisna Rao C., 1965, LINEAR STAT INFERENC; VINES SK, 2000, APPL STAT, V49, P441	10	17	17	0	1	ROUTLEDGE JOURNALS, TAYLOR & FRANCIS LTD	ABINGDON	4 PARK SQUARE, MILTON PARK, ABINGDON OX14 4RN, OXFORDSHIRE, ENGLAND	0266-4763			J APPL STAT	J. Appl. Stat.	NOV	2005	32	9					969	987		10.1080/02664760500168648		19	Statistics & Probability	Mathematics	995AH	WOS:000234071800007		
J	Neumann, J; Schnorr, C; Steidl, G				Neumann, J; Schnorr, C; Steidl, G			Combined SVM-based feature selection and classification	MACHINE LEARNING			English	Article						feature selection; SVMs; embedded methods; mathematical programming; difference of convex functions programming; non-convex optimisation		Feature selection is an important combinatorial optimisation problem in the context of supervised pattern classification. This paper presents four novel continuous feature selection approaches directly minimising the classifier performance. In particular, we include linear and nonlinear Support Vector Machine classifiers. The key ideas of our approaches are additional regularisation and embedded nonlinear feature selection. To solve our optimisation problems, we apply difference of convex functions programming which is a general framework for non-convex continuous optimisation. Experiments with artificial data and with various real-world problems including organ classification in computed tomography scans demonstrate that our methods accomplish the desired feature selection and classification performance simultaneously.	Univ Mannheim, Dept Math & Comp Sci, D-68131 Mannheim, Germany	Neumann, J (reprint author), Univ Mannheim, Dept Math & Comp Sci, D-68131 Mannheim, Germany.	neumann@uni-mannheim.de; schnorr@uni-mannheim.de; steidl@uni-mannheim.de					Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Yuille AL, 2003, NEURAL COMPUT, V15, P915, DOI 10.1162/08997660360581958; Chapelle O, 1999, IEEE T NEURAL NETWOR, V10, P1055, DOI 10.1109/72.788646; Bach Francis R., 2004, P 21 INT C MACH LEAR; Bennett KP, 1992, OPTIMIZATION METHODS, V1, P23, DOI 10.1080/10556789208805504; BenTal A, 1997, SIAM J OPTIMIZ, V7, P347, DOI 10.1137/S1052623493259215; Blake C.L., 1998, UCI REPOSITORY MACHI; BRADLEY P, 1998, TR9811 U WISC COMP S; Bradley P. S., 1998, P 15 INT C MACH LEAR, P82; Cristianini N, 2002, ADV NEUR IN, V14, P367; DINH TP, 1988, INT SERIES NUMER MAT, V84, P277; Duda R.O., 2000, PATTERN CLASSIFICATI; Guyon I., 2003, Journal of Machine Learning Research, V3, DOI 10.1162/153244303322753616; Haasdonk B, 2004, LECT NOTES COMPUT SC, V3175, P220; HEILER M, 2001, COMP SCI SERIES U MA; Hermes L., 2000, P 15 INT C PATT REC, P716; *IL INC, 2001, ILOG CPLEX 7 5; JAKUBIK OJ, 2003, THESIS U MANNHEIM; Jebara T., 2000, P 16 C UNC ART INT U, P291; John G., 1994, P 11 INT C MACH LEAR, P121; Mangasarian OL, 1997, TR199705 U WISC MATH; Neumann J, 2004, LECT NOTES COMPUT SC, V3175, P212; Pham Dinh T., 1998, SIAM J OPTIMIZ, V8, P476; Rockafellar R.T., 1970, CONVEX ANAL; SCHMIDT S, 2004, THESIS U MANNHEIM; Scholkopf B., 2002, LEARNING KERNELS; The MathWorks Inc, 2002, OPT TOOLB US GUID; Weston J, 2001, ADV NEUR IN, V13, P668; Weston J., 2003, Journal of Machine Learning Research, V3, DOI 10.1162/153244303322753751; Zhu J., 2004, ADV NEURAL INFORM PR, V16	30	67	69	1	9	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0885-6125			MACH LEARN	Mach. Learn.	NOV	2005	61	1-3					129	150		10.1007/s10994-005-1505-9		22	Computer Science, Artificial Intelligence	Computer Science	990XH	WOS:000233776200005		
J	Baumann, K				Baumann, K			Chance correlation in variable subset regression: Influence of the objective function, the selection mechanism, and ensemble averaging	QSAR & COMBINATORIAL SCIENCE			English	Article						variable selection; The LASSO; ensemble averaging; cross-validation; permutation test; bagging; chance correlation; overfitting	CROSS-VALIDATION; MODEL SELECTION; SYSTEMATIC EVALUATION; STRUCTURE DESCRIPTOR; ALGORITHM; QSAR; CALIBRATION; SHRINKAGE; BENEFITS; HAZARDS	Cross-validation is often used to guide variable selection algorithms. While cross-validation almost unbiasedly estimates the prediction error when no model selection (such as variable selection) is involved, it is heavily biased when a large amount of model selection is applied (i.e. sifting through thousands of models). In the latter case, the internal figures of merit such as R-CV(2), or RMSEPCV can be deceptively overoptimistic. The extent of this inflation (overoptimism) and the influence factors for the degree of inflation are studied here. It turns out, that the extent of inflation is extremely large for small data sets. The main influence factors for the degree of inflation are data set size, the size of the variable pool, the allowed object variable ratio, the objective function for guiding an stepwise selection technique, and the correlation structure of the data matrix. Moreover, chancying the selection mechanism from the commonly applied stepwise procedures to the more stable shrinking and selection technique LASSO eliminates the inflation largely. No inflation is observed when ensemble averaging is used to estimate the prediction error. The latter property combined with the potential of ensemble averaging to improve the predictivity and the possibility to use the information of the single models of the ensemble for validation tasks, renders ensemble averaging an attractive tool if prediction is the primary goal of the analysis.	Univ Wurzburg, Dept Pharm, D-97074 Wurzburg, Germany	Baumann, K (reprint author), Univ Wurzburg, Dept Pharm, Am Hubland, D-97074 Wurzburg, Germany.	knut.baumann@chemometriks.de					Baumann K, 2002, J CHEMOMETR, V16, P339, DOI 10.1002/cem.730; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; BREIMAN L, 1992, J AM STAT ASSOC, V87, P738, DOI 10.2307/2290212; Baumann K, 2003, TRAC-TREND ANAL CHEM, V22, P395, DOI 10.1016/S0165-9936(03)00607-1; Baumann K, 2002, J CHEM INF COMP SCI, V42, P26, DOI 10.1021/ci990070t; BARONI M, 1993, QUANT STRUCT-ACT REL, V12, P9, DOI 10.1002/qsar.19930120103; SHAO J, 1993, J AM STAT ASSOC, V88, P486, DOI 10.2307/2290328; Efron B, 2004, ANN STAT, V32, P407; Luco JM, 1997, J CHEM INF COMP SCI, V37, P392, DOI 10.1021/ci960487o; Baumann K, 2002, J CHEMOMETR, V16, P351, DOI 10.1002/cem.729; Baumann K, 2002, QUANT STRUCT-ACT REL, V21, P507, DOI 10.1002/1521-3838(200211)21:5<507::AID-QSAR507>3.0.CO;2-L; Baumann K, 2004, J COMPUT AID MOL DES, V18, P549, DOI 10.1007/s10822-004-4071-5; Breiman L., 1996, OUT BAG ESTIMATION; Breiman L, 1996, ANN STAT, V24, P2350; Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.1023/A:1018054314350; BREIMAN L, 2000, MACH LEARN, V10, P229; BROWN PJ, 1993, MEASUREMENT REGRESSI, P55; Buhlmann P, 2002, ANN STAT, V30, P927; BURMAN P, 1989, BIOMETRIKA, V76, P503, DOI 10.1093/biomet/76.3.503; CRUCIANI G, 1995, ADV COMPUTER ASSISTE, P61; CRUCIANI G, 1992, J CHEMOMETR, V6, P335, DOI 10.1002/cem.1180060604; Dietterich T.G., 2001, LNCS, V1857, P1; FLACK VF, 1987, AM STAT, V41, P84, DOI 10.2307/2684336; GRANDVALET Y, 1998, 8 INT C ART NEUR NET, P201; JouanRimbaud D, 1996, CHEMOMETR INTELL LAB, V35, P213, DOI 10.1016/S0169-7439(96)00062-7; KLOPMAN G, 1985, J COMPUT CHEM, V6, P492, DOI 10.1002/jcc.540060520; Lindgren F, 1996, J CHEMOMETR, V10, P521; Miller A, 2002, SUBSET SELECTION REG, V2nd; Ojelund H, 2001, J CHEMOMETR, V15, P497; RENCHER AC, 1980, TECHNOMETRICS, V22, P49, DOI 10.2307/1268382; Sippl W, 2001, J COMPUT AID MOL DES, V15, P395, DOI 10.1023/A:1011150215288; Stiefl N, 2003, J MED CHEM, V46, P1390, DOI 10.1021/jm021077w; Todeschini R, 2004, ANAL CHIM ACTA, V515, P199, DOI 10.1016/j.aca.2003.12.010; Todeschini R., DRAGON SOFTWARE CALC; TOPLISS JG, 1979, J MED CHEM, V22, P1238, DOI 10.1021/jm00196a017; TOPLISS JG, 1972, J MED CHEM, V15, P1066, DOI 10.1021/jm00280a017; Wolpert DH, 1999, MACH LEARN, V35, P41, DOI 10.1023/A:1007519102914; ZHANG P, 1993, ANN STAT, V21, P299, DOI 10.1214/aos/1176349027	38	47	47	3	7	WILEY-V C H VERLAG GMBH	WEINHEIM	PO BOX 10 11 61, D-69451 WEINHEIM, GERMANY	1611-020X			QSAR COMB SCI	QSAR Comb. Sci.	NOV	2005	24	9					1033	1046		10.1002/qsar.200530134		14	Chemistry, Medicinal; Chemistry, Multidisciplinary; Computer Science, Interdisciplinary Applications; Pharmacology & Pharmacy	Pharmacology & Pharmacy; Chemistry; Computer Science	990ZU	WOS:000233782700003		
J	Aragones, E; Gilboa, I; Postlewaite, A; Schmeidler, D				Aragones, E; Gilboa, I; Postlewaite, A; Schmeidler, D			Fact-free learning	AMERICAN ECONOMIC REVIEW			English	Article							REPRESENTATION	People may be surprised to notice certain regularities that hold in existing knowledge they have had for some time. That is, they may learn without getting new factual information. We argue that this can be partly explained by computational complexity. We show that, given a knowledge base, finding a small set of variables that obtain a certain value of R-2 is computationally hard, in the sense that this term is used in computer science. We discuss some of the implications of this result and of fact-free learning in general.			enriqueta.aragones@uab.es; igilboa@tau.ac.il; apostlew@econ.sas.upenn.edu; schmeid@tau.ac.il					Al-Najjar NI, 2003, J ECON THEORY, V111, P49, DOI 10.1016/S0022-0531(03)00075-9; ANDERLINI L, 1994, Q J ECON, V109, P1085, DOI 10.2307/2118357; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; MALLOWS CL, 1973, TECHNOMETRICS, V15, P661, DOI 10.2307/1267380; BRAY MM, 1986, ECONOMETRICA, V54, P1129, DOI 10.2307/1912325; Dekel E, 1998, EUR ECON REV, V42, P523, DOI 10.1016/S0014-2921(97)00114-1; DEKEL E, 1997, UNPUB UNIQUE SUBJECT; Gary M.R., 1979, COMPUTERS INTRACTABI; GILBOA I, 2001, THEORY CASE BASED DE; Goodman N., 1965, FACT FICTION FORECAS; Hastie T., 2001, ELEMENTS STAT LEARNI; KREPS DM, 1979, ECONOMETRICA, V47, P565, DOI 10.2307/1910406; Kreps David M., 1992, EC ANAL MARKETS GAME, P258; LAPORTA R, 1998, UNPUB QUALITY GOVT; LUND C, 1994, J ACM, V41, P960, DOI 10.1145/185675.306789; MAOZ Z, 1993, AM POLIT SCI REV, V87, P624, DOI 10.2307/2938740; Papadimitriou C.M., 1994, COMPUTATIONAL COMPLE; Raz R., 1997, P 29 ANN ACM S THEOR, P475, DOI DOI 10.1145/258533.258641; Simon HA, 1955, Q J ECON, V69, P99, DOI 10.2307/1884852; Wittgenstein L., 1922, TRACTATUS LOGICOPHIL	20	23	23	2	10	AMER ECONOMIC ASSOC	NASHVILLE	2014 BROADWAY, STE 305, NASHVILLE, TN 37203 USA	0002-8282			AM ECON REV	Am. Econ. Rev.	DEC	2005	95	5					1355	1368		10.1257/000282805775014308		14	Economics	Business & Economics	998FZ	WOS:000234305300002		
J	Yuan, M; Lin, Y				Yuan, M; Lin, Y			Efficient empirical Bayes variable selection and estimation in linear models	JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION			English	Article						hierarchical model; LARS algorithm; LASSO; model selection; penalized least squares	MULTIPLE-REGRESSION	We propose an empirical Bayes method for variable selection and coefficient estimation in linear regression models. The method is based on a particular hierarchical Bayes formulation, and the empirical Bayes estimator is shown to be closely related to the LASSO estimator. Such a connection allows us to take advantage of the recently developed quick LASSO algorithm to compute the empirical Bayes estimate, and provides a new way to select the tuning parameter in the LASSO method. Unlike previous empirical Bayes variable selection methods, which in most practical situations can be implemented only through a greedy stepwise algorithm, our method gives a global solution efficiently. Simulations and real examples show that the proposed method is very competitive in terms of variable selection, estimation accuracy, and computation speed compared with other variable selection and estimation methods.	Georgia Inst Technol, Sch Ind & Syst Engn, Atlanta, GA 30332 USA; Univ Wisconsin, Dept Stat, Madison, WI 53706 USA	Yuan, M (reprint author), Georgia Inst Technol, Sch Ind & Syst Engn, Atlanta, GA 30332 USA.	myuan@isye.gatech.edu; yilin@star.wisc.edu					ANDREWS DF, 1974, J ROY STAT SOC B MET, V36, P99; BREIMAN L, 1995, TECHNOMETRICS, V37, P373, DOI 10.2307/1269730; Shen XT, 2002, J AM STAT ASSOC, V97, P210, DOI 10.1198/016214502753479356; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; BREIMAN L, 1992, J AM STAT ASSOC, V87, P738, DOI 10.2307/2290212; Johnstone IM, 2005, ANN STAT, V33, P1700, DOI 10.1214/009053605000000345; Raftery AE, 1997, J AM STAT ASSOC, V92, P179, DOI 10.2307/2291462; George EI, 2000, J AM STAT ASSOC, V95, P1304, DOI 10.2307/2669776; Brown PJ, 2002, J R STAT SOC B, V64, P519, DOI 10.1111/1467-9868.00348; GEORGE EI, 1993, J AM STAT ASSOC, V88, P881, DOI 10.2307/2290777; Efron B, 2004, ANN STAT, V32, P407; HARRISON D, 1978, J ENVIRON ECON MANAG, V5, P81, DOI 10.1016/0095-0696(78)90006-2; BREIMAN L, 1985, J AM STAT ASSOC, V80, P580, DOI 10.2307/2288473; Chen S. S., 1999, SIAM J SCI COMPUT, V20, P33; Chipman H., 2001, IMS LECT NOTES MONOG, V38, P65; FOSTER DP, 1994, ANN STAT, V22, P1947, DOI 10.1214/aos/1176325766; George EI, 2000, BIOMETRIKA, V87, P731, DOI 10.1093/biomet/87.4.731	18	45	46	2	10	AMER STATISTICAL ASSOC	ALEXANDRIA	1429 DUKE ST, ALEXANDRIA, VA 22314 USA	0162-1459			J AM STAT ASSOC	J. Am. Stat. Assoc.	DEC	2005	100	472					1215	1225		10.1198/016214505000000367		11	Statistics & Probability	Mathematics	988GQ	WOS:000233581100015		
J	Basak, SC; Natarajan, R; Mills, D; Hawkins, DM; Kraker, JJ				Basak, SC; Natarajan, R; Mills, D; Hawkins, DM; Kraker, JJ			Quantitative structure-activity relationship modeling of insect juvenile hormone activity of 2,4-dienoates using computed molecular descriptors	SAR AND QSAR IN ENVIRONMENTAL RESEARCH			English	Article; Proceedings Paper	4th Indo-US Workshop on Mathematical Chemistry	JAN 08-12, 2005	Pune, INDIA		Univ Pune	QSAR; juvenile hormone mimic; molecular descriptors; 2,4-dienoates; variable selection	HIERARCHICAL QSAR APPROACH; CHEMOMETRICS REGRESSION TOOLS; PROPERTY-BASED METHODS; RIDGE-REGRESSION; NONORTHOGONAL PROBLEMS; MIMETIC COMPOUNDS; STATISTICAL VIEW; PREDICTION; SELECTION; ETHERS	Juvenile hormone (JH) activity of one hundred and eighty 2,4-dienoates reported for the larvae/ pupae of six insect species was modeled using 915 atom pairs and 258 global molecular descriptors (topological and geometrical). Ridge regression, principal component regression and partial least square regression methods were used to model each of the JH activities. The use of all of the available parameters did not yield any good models, and extensive predictor trimming was necessary to improve the models. Ridge regression was found to give the best results among the three statistical tools used. The top ten molecular descriptors selected based on the t-statistic for each of the six models were found to be mostly atom pairs containing heteroatoms and topochemical descriptors. This suggests the importance of the chemical nature of the ligand rather than mere space-filling as the basis of the JH bioactivity. The residual plots indicate the existence of some non-linear relations, and recursive partitioning was used to capture any nonlinear relation between the bioassays and the molecular descriptors.	Univ Minnesota, Nat Resources Res Inst, Ctr Water & Environm, Duluth, MN 55811 USA; Univ Minnesota, Sch Stat, Minneapolis, MN 55455 USA	Basak, SC (reprint author), Univ Minnesota, Nat Resources Res Inst, Ctr Water & Environm, 5013 Miller Trunk Hwy, Duluth, MN 55811 USA.	sbasak@nrri.umn.edu	Natarajan, Ramanathan/A-5851-2008				AITKIN M, 1981, TECHNOMETRICS, V23, P161, DOI 10.2307/1268032; Basak SC, 2003, RISK ANAL, V23, P1173, DOI 10.1111/j.0272-4332.2003.00390.x; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Hawkins DM, 2003, J CHEM INF COMP SCI, V43, P579, DOI 10.1021/ci025626i; MASSY WF, 1965, J AM STAT ASSOC, V60, P234, DOI 10.2307/2283149; HOERL AE, 1970, TECHNOMETRICS, V12, P55; FRANK IE, 1993, TECHNOMETRICS, V35, P109, DOI 10.2307/1269656; Zou H, 2005, J ROY STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x; Balaban A.T., 1997, CHEM TOPOLOGY 3 DIME; Basak S. C., 2002, TOPOLOGY CHEM DISCRE, P113; Basak S. C., 1988, POLLY V 2 3; Basak SC, 2002, SAR QSAR ENVIRON RES, V13, P727, DOI 10.1080/1062936021000043463; BASAK SC, 2004, ADV QUANTUM CHEM; Basak SC, 2003, INDIAN J CHEM A, V42, P1385; Basak S.C., 1999, TOPOLOGICAL INDICES, P675; Basak SC, 2001, SAR QSAR ENVIRON RES, V12, P481, DOI 10.1080/10629360108039830; Basak SC, 2002, SAR QSAR ENVIRON RES, V13, P649, DOI 10.1080/1062936021000043409; Basak S.C., 1999, TOPOLOGICAL INDICES, P563; Basak SC, 2001, J CHEM INF COMP SCI, V41, P692, DOI 10.1021/ci000165r; BASAK SC, IN PRESS BIOL CONCEP; COOK R, 1999, APPL REGRESSION COMP; Devillers J., 1999, TOPOLOGICAL INDICES; Filip PA, 1987, J MATH CHEM, V1, P61, DOI 10.1007/BF01205338; Gute B D, 1997, SAR QSAR Environ Res, V7, P117, DOI 10.1080/10629369708039127; Gute BD, 2001, SAR QSAR ENVIRON RES, V11, P363, DOI 10.1080/10629360108035359; Gute BD, 1999, SAR QSAR ENVIRON RES, V10, P1, DOI 10.1080/10629369908039162; *HALL ASS CONS, 2000, MOLC Z VERS 3 5; HAWKINS DM, 1995, 546 U MINN SCH STAT; HAWKINS DM, 2005, LINMODS PROGRAM; HAYASHI T, 1990, J AGR FOOD CHEM, V38, P1965, DOI 10.1021/jf00100a019; HAYASHI T, 1991, J AGR FOOD CHEM, V39, P2039, DOI 10.1021/jf00011a032; HAYASHI T, 1991, J AGR FOOD CHEM, V39, P2029, DOI 10.1021/jf00011a031; HENRICK CA, 1976, J AGR FOOD CHEM, V24, P207, DOI 10.1021/jf60204a018; HOERL AE, 1970, TECHNOMETRICS, V12, P69, DOI 10.2307/1267352; Maki A, 2005, IMAGING NOTES, V20, P24; NAKAYAMA A, 1984, J MED CHEM, V27, P1493, DOI 10.1021/jm00377a019; NIWA A, 1989, J AGR FOOD CHEM, V37, P462, DOI 10.1021/jf00086a042; NIWA A, 1989, J AGR FOOD CHEM, V37, P467; NIWA A, 1989, J AGR FOOD CHEM, V37, P378; NIWA A, 1990, J AGR FOOD CHEM, V38, P514, DOI 10.1021/jf00092a040; RENCHER AC, 1980, TECHNOMETRICS, V22, P49, DOI 10.2307/1268382; SAS Institute, 1988, SAS STAT US GUID REL; Thisted R. A., 1988, ELEMENTS STAT COMPUT; Todeschini R., 2000, METHODS PRINCIPLES M, V11; *U MINN, 1993, APPROBE; WALKER E, 1988, TECHNOMETRICS, V30, P221, DOI 10.2307/1270168; WOLD S, 1993, TECHNOMETRICS, V35, P136, DOI 10.2307/1269657; Young SS, 1998, SAR QSAR ENVIRON RES, V8, P183, DOI 10.1080/10629369808039140	48	8	9	0	6	TAYLOR & FRANCIS LTD	ABINGDON	4 PARK SQUARE, MILTON PARK, ABINGDON OX14 4RN, OXON, ENGLAND	1062-936X			SAR QSAR ENVIRON RES	SAR QSAR Environ. Res.	DEC	2005	16	6					581	606		10.1080/10659360500468526		26	Chemistry, Multidisciplinary; Computer Science, Interdisciplinary Applications; Environmental Sciences; Mathematical & Computational Biology; Toxicology	Chemistry; Computer Science; Environmental Sciences & Ecology; Mathematical & Computational Biology; Toxicology	011YG	WOS:000235304800005	16428133	
J	Ma, SG; Huang, J				Ma, SG; Huang, J			Regularized ROC method for disease classification and biomarker selection with microarray data	BIOINFORMATICS			English	Article							GENE-EXPRESSION DATA; RANK CORRELATION ESTIMATOR; REGRESSION; LASSO; TUMOR; MODEL	Motivation: An important application of microarrays is to discover genomic biomarkers, among tens of thousands of genes assayed, for disease classification. Thus there is a need for developing statistical methods that can efficiently use such high-throughput genomic data, select biomarkers with discriminant power and construct classification rules. The ROC (receiver operator characteristic) technique has been widely used in disease classification with low-dimensional biomarkers because (1) it does not assume a parametric form of the class probability as required for example in the logistic regression method; (2) it accommodates case-control designs and (3) it allows treating false positives and false negatives differently. However, due to computational difficulties, the ROC-based classification has not been used with microarray data. Moreover, the standard ROC technique does not incorporate built-in biomarker selection. Results: We propose a novel method for biomarker selection and classification using the ROC technique for microarray data. The proposed method uses a sigmoid approximation to the area under the ROC curve as the objective function for classification and the threshold gradient descent regularization method for estimation and biomarker selection. Tuning parameter selection based on the V-fold cross validation and predictive performance evaluation are also investigated. The proposed approach is demonstrated with a simulation study, the Colon data and the Estrogen data. The proposed approach yields parsimonious models with excellent classification performance.	Univ Iowa, Program Publ Hlth Genet, Dept Stat & Actuarial Sci, Iowa City, IA 52242 USA; Univ Washington, Dept Biostat, Washington, DC USA	Huang, J (reprint author), Univ Iowa, Program Publ Hlth Genet, Dept Stat & Actuarial Sci, Iowa City, IA 52242 USA.	jian@stat.uiowa.edu					Abrevaya J, 1999, ECON LETT, V62, P279, DOI 10.1016/S0165-1765(98)00255-9; Provost F, 2003, MACH LEARN, V52, P199, DOI 10.1023/A:1024099825458; Cui XG, 2005, BIOSTATISTICS, V6, P59, DOI 10.1093/biostatistics/kxh018; Nguyen DV, 2002, BIOINFORMATICS, V18, P39, DOI 10.1093/bioinformatics/18.1.39; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Ben-Dor A, 2000, J COMPUT BIOL, V7, P559, DOI 10.1089/106652700750050943; Pepe MS, 2004, AM J EPIDEMIOL, V159, P882, DOI 10.1093/aje/kwh101; Alon U, 1999, P NATL ACAD SCI USA, V96, P6745, DOI 10.1073/pnas.96.12.6745; Ambroise C, 2002, P NATL ACAD SCI USA, V99, P6562, DOI 10.1073/pnas.102102699; Dudoit S, 2002, J AM STAT ASSOC, V97, P77, DOI 10.1198/016214502753479248; HAN AK, 1987, J ECONOMETRICS, V35, P303, DOI 10.1016/0304-4076(87)90030-3; Efron B, 2004, ANN STAT, V32, P407; DETTLING M, 2003, BIOINFORMATICS, V9, P1061; Friedman JH, 2004, GRADIENT DIRECTED RE; GAMMERAMAN A, 1996, COMPUTATIONAL LEARNI; Ghosh D, 2005, J BIOMED BIOTECHNOL, P147, DOI 10.1155/JBB.2005.147; GUI J, 2005, P PSB 2005; HOROWITZ JL, 1992, ECONOMETRICA, V60, P505, DOI 10.2307/2951582; MA S, 2005, IN PRESS BIOMETRICS; Mossman D, 1999, MED DECIS MAKING, V19, P78, DOI 10.1177/0272989X9901900110; PEPE MS, 2005, IN PRESS BIOMETRICS; Pepe MS, 2003, STAT EVALUATION MED; POCHET N, 2004, BIOINFORMATICS, V17, P3185; R Development Core Team, 2005, R LANG ENV STAT COMP; Spang R., 2001, P GERM C BIOINF GCB; WAHBA G, 1990, P CBMS NSF REG C SER, V59; WEST M, 2001, P NATL ACAD SCI USA, V98, P11562	27	72	74	1	5	OXFORD UNIV PRESS	OXFORD	GREAT CLARENDON ST, OXFORD OX2 6DP, ENGLAND	1367-4803			BIOINFORMATICS	Bioinformatics	DEC 15	2005	21	24					4356	4362		10.1093/bioinformatics/bti724		7	Biochemical Research Methods; Biotechnology & Applied Microbiology; Computer Science, Interdisciplinary Applications; Mathematical & Computational Biology; Statistics & Probability	Biochemistry & Molecular Biology; Biotechnology & Applied Microbiology; Computer Science; Mathematical & Computational Biology; Mathematics	991XY	WOS:000233849400007	16234316	
S	Shu, Y; Chao, Z		Tang, YY; Wang, SP; Lorette, G; Yeung, DS; Yan, H		Shu Yang; Chao Zhang			Regression Nearest Neighbor in face recognition	18th International Conference on Pattern Recognition, Vol 3, Proceedings	INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION		English	Proceedings Paper	18th International Conference on Pattern Recognition (ICPR 2006)	AUG 20-24, 2006	Hong Kong, PEOPLES R CHINA	IAPR, CAA, Hong Kong Baptist Univ			CLASSIFICATION	In this paper, we introduce a Regression Nearest Neighbor framework for general classification tasks. To alleviate potential problems caused by nonlinearity, we propose a kernel regression nearest neighbor (KRNN) algorithm and its convex counterpart (CKRNN) as two specific extensions of nearest neighbor algorithm and present a fast and useful kernel selection method correspondingly. Comprehensive analysis and extensive experiments are used to demonstrate the effectiveness of our methods in real face datasets.	Peking Univ, Natl Lab Machine Percept, Beijing 100871, Peoples R China	Shu, Y (reprint author), Peking Univ, Natl Lab Machine Percept, Beijing 100871, Peoples R China.						Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Suykens JAK, 1999, NEURAL PROCESS LETT, V9, P293, DOI 10.1023/A:1018628609742; COVER TM, 1967, IEEE T INFORM THEORY, V13, P21, DOI 10.1109/TIT.1967.1053964; Fung G., 2001, P KDD 2001; Hastie T., 2001, ELEMENTS STAT LEARNI; Hastie T, 1996, IEEE T PATTERN ANAL, V18, P607, DOI 10.1109/34.506411; LI S, 1998, INT C COMP VIS PATT; PENG J, 2004, IEEE T PATTERN ANAL, V26; Phillips PJ, 1998, IMAGE VISION COMPUT, V16, P295, DOI 10.1016/S0262-8856(97)00070-X; Poggio T., 2003, NOTICES AMS, V50, P537; Scholkopf B., 2002, LEARNING KERNELS; Sim T., 2002, INT C AUT FAC GEST R; VICENT P, 2001, NEURAL INFORM PROCES; YANG C, 2004, NEURAL INFORM PROCES	14	0	0	0	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA	1051-4651		0-7695-2521-0	INT C PATT RECOG			2006							515	518				4	Computer Science, Artificial Intelligence	Computer Science	BFB29	WOS:000240705600123		
B	Jin, R; Si, L; Srivastava, S; Li, Z; Chan, C			IEEE	Jin, Rong; Si, Luo; Srivastava, Shireesh; Li, Zheng; Chan, Christina			A knowledge driven regression model for gene expression and microarray analysis	2006 28th Annual International Conference of the IEEE Engineering in Medicine and Biology Society, Vols 1-15			English	Proceedings Paper	28th Annual International Conference of the IEEE-Engineering-in-Medicine-and-Biology-Society	AUG 30-SEP 03, 2006	New York, NY	IEEE Engn Med & Biol Sci				The linear regression model has been widely used in the analysis of gene expression and microarray data to identify a subset of genes that are important to a given metabolic function. One of the key challenges in applying the linear regression model to gene expression data analysis arises from the sparse data problem, in which the number of genes is significantly larger than the number of conditions. To resolve this problem, we present a knowledge driven regression model that incorporates the knowledge of genes from the Gene Ontology (GO) database into the linear regression model. It is based on the assumption that two genes are likely to be assigned similar weights when they share similar sets of GO codes. Empirical studies show that the proposed knowledge driven regression model is effective in reducing the regression errors, and furthermore effective in identifying genes that are relevant to a given metabolite.	Michigan State Univ, Fac Comp Sci & Engn, E Lansing, MI 48824 USA	Jin, R (reprint author), Michigan State Univ, Fac Comp Sci & Engn, E Lansing, MI 48824 USA.						Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; HOERL AE, 1970, TECHNOMETRICS, V12, P55; Chiarugi P, 2003, TRENDS BIOCHEM SCI, V28, P509, DOI 10.1016/S0968-0004(03)00174-9; Bishop C. M., 1995, NEURAL NETWORKS PATT; Common P., 1994, SIGNAL PROCESS, V36, P287; FRIEDMAN JH, 1989, J AM STAT ASSOC, V84, P165, DOI 10.2307/2289860; HSKULDSSON A, 1988, J CHEMOMETR, V2, P211; Weisberg S., 1980, APPL LINEAR REGRESSI	8	0	0	0	1	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA			978-1-4244-0032-4				2006							1480	1483				4	Engineering, Biomedical	Engineering	BGI19	WOS:000247284701155		
S	Selen, Y; Gudmundson, E; Stoica, P			IEEE	Selen, Y.; Gudmundson, E.; Stoica, P.			An approach to sparse model selection and averaging	2006 IEEE INSTRUMENTATION AND MEASUREMENT TECHNOLOGY CONFERENCE PROCEEDINGS, VOLS 1-5	IEEE Instrumentation and Measurement Technology Conference		English	Proceedings Paper	23rd IEEE Instrumentation and Measurement Technology Conference	APR 24-27, 2006	Sorrento, ITALY	IEEE Instrumentat & Measurement Soc		linear systems; model reduction; channel measurement; least squares estimation; parameter estimation; signal processing; system identification	REGRESSION	Parameter estimation, when the true model structure is unknown is a commonly occurring task in measurement problems. In a sparse modeling scenario, the number of possible models grows exponentially with the total number of parameters. The full set of models therefore becontes computationally infeasible to handle. We propose a method, based on successive model reduction, for finding a sound and computationally feasible set of sparse linear regression models. Once this set of models has been found, standard model selection or model averaging techniques can be applied. We demonstrate the performance of our method by some numerical examples.	Uppsala Univ, Dept Informat Technol, SE-75105 Uppsala, Sweden	Selen, Y (reprint author), Uppsala Univ, Dept Informat Technol, POB 337, SE-75105 Uppsala, Sweden.	ys@it.uu.se					AKAIKE H, 1974, IEEE T AUTOMAT CONTR, VAC19, P716, DOI 10.1109/TAC.1974.1100705; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Hoeting JA, 1999, STAT SCI, V14, P382; Wipf DP, 2004, IEEE T SIGNAL PROCES, V52, P2153, DOI 10.1109/TSP.2004.831016; Stoica P, 2004, IEEE SIGNAL PROC MAG, V21, P36, DOI 10.1109/MSP.2004.1311138; Efron B, 2004, ANN STAT, V32, P407; SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136; Wasserman L, 2000, J MATH PSYCHOL, V44, P92, DOI 10.1006/jmps.1999.1278; Buckland ST, 1997, BIOMETRICS, V53, P603, DOI 10.2307/2533961; Homer J, 1998, IEEE T SIGNAL PROCES, V46, P2651, DOI 10.1109/78.720368; IHAKA R, 1996, J COMPUTATIONAL GRAP, V5, P299, DOI DOI 10.2307/1390807; OBRIEN MS, 1994, IEEE T SIGNAL PROCES, V42, P3353, DOI 10.1109/78.340772; Soderstrom T., 1989, SYSTEM IDENTIFICATIO; Stoica P., 2005, SPECTRAL ANAL SIGNAL	14	1	1	1	1	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA	1091-5281		978-0-7803-9359-2	IEEE IMTC P			2006							113	116		10.1109/IMTC.2006.328295		4	Instruments & Instrumentation	Instruments & Instrumentation	BFS10	WOS:000244176700022		
S	Lin, YQ; Lee, DD			IEEE	Lin, Yuanqing; Lee, Daniel D.			Bayesian L-1-norm sparse learning	2006 IEEE International Conference on Acoustics, Speech, and Signal Processing, Vol V, Proceedings: AUDIO AND ELECTROACOUSTICS, MULTIMEDIA SIGNAL PROCESSING, MACHINE LEARNING FOR SIGNAL PROCESSING SPECIAL SESSIONS	International Conference on Acoustics Speech and Signal Processing (ICASSP)		English	Proceedings Paper	31st IEEE International Conference on Acoustics, Speech and Signal Processing	MAY 14-19, 2006	Toulouse, FRANCE	IEEE Signal Proc Soc		machine learning for signal processing; learning theory		We propose a Bayesian framework for learning the optimal regularization parameter in the L-1-norm penalized least-mean-square (LMS) problem, also known as LASSO [1] or basis pursuit [2]. The setting of the regularization parameter is critical for deriving, a correct solution. In most existing methods, the scalar regularization parameter is often determined in a heuristic manner; in contrast, our approach infers the optimal regularization setting under a Bayesian framework. Furthermore, Bayesian inference enables an independent regularization scheme where each coefficient (or weight) is associated with an independent regularization parameter. Simulations illustrate the improvement using our method in discovering sparse structure from noisy data.	Univ Penn, Grasp Lab, Dept Elect & Syst Engn, Philadelphia, PA 19104 USA	Lin, YQ (reprint author), Univ Penn, Grasp Lab, Dept Elect & Syst Engn, Philadelphia, PA 19104 USA.						Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010; Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Duttweiler DL, 2000, IEEE T SPEECH AUDI P, V8, P508, DOI 10.1109/89.861368; Tipping ME, 2001, J MACH LEARN RES, V1, P211, DOI 10.1162/15324430152748236; Fuchs JJ, 1999, IEEE T SIGNAL PROCES, V47, P237, DOI 10.1109/78.738263; LIN YP, IN PRESS IEEE T SIGN; Malioutov D. M., 2005, IEEE INT C AC SPEECH; ZANGWILL W. I., 1969, NONLINEAR PROGRAMMIN	9	1	2	0	0	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA	1520-6149			INT CONF ACOUST SPEE			2006							605	+				5	Acoustics; Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic; Imaging Science & Photographic Technology	Acoustics; Computer Science; Engineering; Imaging Science & Photographic Technology	BFW40	WOS:000245100500152		
S	Larsson, EG; Selen, Y			IEEE	Larsson, Erik G.; Selen, Yngve			Linear regression with a sparse parameter vector	2006 IEEE International Conference on Acoustics, Speech and Signal Processing, Vols 1-13	International Conference on Acoustics Speech and Signal Processing (ICASSP)		English	Proceedings Paper	31st IEEE International Conference on Acoustics, Speech and Signal Processing	MAY 14-19, 2006	Toulouse, FRANCE	IEEE Signal Proc Soc			SELECTION	We consider linear regression under a model where the parameter vector is known to be sparse. Using a Bayesian framework, we derive a computationally efficient approximation to the minimum mean-square error (MMSE) estimate of the parameter vector. The performance of the so-obtained estimate is illustrated via numerical examples.	Royal Inst Technol, Sch EE Commun Theory, S-10044 Stockholm, Sweden	Larsson, EG (reprint author), Royal Inst Technol, Sch EE Commun Theory, Osquldas Vag 10, S-10044 Stockholm, Sweden.						Rao BD, 2003, IEEE T SIGNAL PROCES, V51, P760, DOI 10.1109/TSP.2002.808076; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Wipf DP, 2004, IEEE T SIGNAL PROCES, V52, P2153, DOI 10.1109/TSP.2004.831016; Efron B, 2004, ANN STAT, V32, P407; Fuchs JJ, 2004, IEEE T INFORM THEORY, V50, P1341, DOI 10.1109/TIT.2004.828141; George EI, 2000, BIOMETRIKA, V87, P731, DOI 10.1093/biomet/87.4.731; Homer J, 1998, IEEE T SIGNAL PROCES, V46, P2651, DOI 10.1109/78.720368; JOHNSTONE I. M., 2005, J STAT SOFTWARE, V12; Kay S., 1993, FUNDAMENTALS STAT SI; Mackay D. J. C., 2003, INFORM THEORY INFERE; OBRIEN MS, 1994, IEEE T SIGNAL PROCES, V42, P3353, DOI 10.1109/78.340772	11	0	0	0	0	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA	1520-6149		978-1-4244-0468-1	INT CONF ACOUST SPEE			2006							2760	2763				4	Acoustics; Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic; Imaging Science & Photographic Technology	Acoustics; Computer Science; Engineering; Imaging Science & Photographic Technology	BFZ22	WOS:000245559903101		
S	Drori, I; Donoho, DL			IEEE	Drori, Iddo; Donoho, David L.			Solution of l(1) minimization problems by LARS/homotopy methods	2006 IEEE International Conference on Acoustics, Speech and Signal Processing, Vols 1-13	International Conference on Acoustics Speech and Signal Processing (ICASSP)		English	Proceedings Paper	31st IEEE International Conference on Acoustics, Speech and Signal Processing	MAY 14-19, 2006	Toulouse, FRANCE	IEEE Signal Proc Soc			REGRESSION; SELECTION	Many applications in signal processing lead to the optimization problems min parallel to x parallel to(1) subject to y = Ax, and min parallel to x parallel to(1) subject to parallel to y - Ax parallel to <= epsilon, where A is a given d times n matrix, d < n, and y is a given n x 1 vector. In this work we consider l(1) minimization by using LARS, Lasso, and homotopy methods [1, 2, 3] (Efron et el., Tibshirani, Osborne et al.). While these methods were first proposed for use in statistical model selection, we show that under certain conditions these methods find the sparsest solution rapidly, as opposed to conventional general purpose optimizers which are prohibitively slow. We define a phase transition diagram which shows how algorithms behave for random problems, as the ratio of unknowns to equations and the ratio of the sparsity to equations varies. We find that whenever the number k of nonzeros in the sparsest solution is less than d/2log(n) then LARS/homotopy obtains the sparsest solution in k steps each of complexity O(d(2)).	Stanford Univ, Dept Stat, Stanford, CA 94305 USA	Drori, I (reprint author), Stanford Univ, Dept Stat, Sequoia Hall,390 Serra Mall, Stanford, CA 94305 USA.						Donoho DL, 2001, IEEE T INFORM THEORY, V47, P2845, DOI 10.1109/18.959265; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Osborne MR, 2000, IMA J NUMER ANAL, V20, P389, DOI 10.1093/imanum/20.3.389; Efron B, 2004, ANN STAT, V32, P407; Cand'es E.J., 2004, ROBUST UNCERTAINTY P; CANDES E, 2004, DECODING LINEAR PROG; Chen S. S., 1999, SIAM J SCI COMPUT, V20, P33; DMITRY M, 2004, IEEE INT C AC SPEECH; Donoho DL, 2003, P NATL ACAD SCI USA, V100, P2197, DOI 10.1073/pnas.0437847100; Donoho David L., 2004, COMPRESSED SENSING, P14; DONOHO DL, 2005, HIGH DIMENSIONAL CEN; Donoho D.L, 2004, COMM PURE APPL MATH; Donoho D.L., 2004, NEIGHBORLY POLYTOPES; EFRON B, LARS SOFTARE WEB SIT; GILBERT AC, 2002, NEAR OPTIMAL SPARSE; SAUNDERS M. A., PDCO PRIMAL DUAL INT	16	0	0	0	6	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA	1520-6149		978-1-4244-0468-1	INT CONF ACOUST SPEE			2006							3087	3090				4	Acoustics; Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic; Imaging Science & Photographic Technology	Acoustics; Computer Science; Engineering; Imaging Science & Photographic Technology	BFZ22	WOS:000245559903183		
S	Rangarajan, R; Raich, R; Hero, AO			IEEE	Rangarajan, Raghuram; Raich, Raviv; Hero, Alfred O., III			Single-stage waveform selection for adaptive resource constrained state estimation	2006 IEEE International Conference on Acoustics, Speech and Signal Processing, Vols 1-13	International Conference on Acoustics Speech and Signal Processing (ICASSP)		English	Proceedings Paper	31st IEEE International Conference on Acoustics, Speech and Signal Processing	MAY 14-19, 2006	Toulouse, FRANCE	IEEE Signal Proc Soc			REGRESSION	We consider the problem of optimal waveform selection. We would like to choose a small subset from a given set of waveforms that minimizes state prediction mean squared error (MSE) given the past observations. This differs from previous approaches to this problem since the optimal waveforms cannot be computed offline; it requires the previous observations. Since the optimal solution to this subset selection problem is combinatorially complex, we propose a convex relaxation of the problem and provide a low complexity suboptimal solution. We present a specific model and show that the performance of this suboptimal procedure approaches that of the optimal waveforms.	Univ Michigan, Dept EECS, Ann Arbor, MI 48109 USA	Rangarajan, R (reprint author), Univ Michigan, Dept EECS, Ann Arbor, MI 48109 USA.						Malioutov D, 2005, IEEE T SIGNAL PROCES, V53, P3010, DOI 10.1109/TSP.2005.850882; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Osborne MR, 2000, IMA J NUMER ANAL, V20, P389, DOI 10.1093/imanum/20.3.389; Efron B, 2004, ANN STAT, V32, P407; BELL MR, 1993, IEEE T INFORM THEORY, V39, P1578, DOI 10.1109/18.259642; Daubechies I, 2004, COMMUN PUR APPL MATH, V57, P1413, DOI 10.1002/cpa.20042; Blahut R. E., 1991, RADAR SONAR 1; HASTIE T, 2000, SPRINGER SERIES STAT; NAPARST H, 1991, IEEE T INFORM THEORY, V37, P317, DOI 10.1109/18.75247; RANGARAJAN R, 2005, P IEEE INT C AC SPEE, V4, P1117; RANGARAJAN R, 2005, P IEEE WORKSH STAT S; Sowelam SM, 2000, IEEE T INFORM THEORY, V46, P1014, DOI 10.1109/18.841178; Sowelam SM, 1998, J FRANKLIN I, V335B, P1341, DOI 10.1016/S0016-0032(98)00005-2	13	0	0	0	0	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA	1520-6149		978-1-4244-0468-1	INT CONF ACOUST SPEE			2006							3123	3126				4	Acoustics; Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic; Imaging Science & Photographic Technology	Acoustics; Computer Science; Engineering; Imaging Science & Photographic Technology	BFZ22	WOS:000245559903192		
S	Lin, YQ; Lee, DD			IEEE	Lin, Yuanqing; Lee, Daniel D.			Bayesian L-1-norm sparse learning	2006 IEEE International Conference on Acoustics, Speech and Signal Processing, Vols 1-13	International Conference on Acoustics Speech and Signal Processing (ICASSP)		English	Proceedings Paper	31st IEEE International Conference on Acoustics, Speech and Signal Processing	MAY 14-19, 2006	Toulouse, FRANCE	IEEE Signal Proc Soc				We propose a Bayesian framework for learning the optimal regularization parameter in the L-1-norm penalized least-mean-square (LMS) problem, also known as LASSO [1] or basis pursuit [2]. The setting of the regularization parameter is critical for deriving a correct solution. In most existing methods, the scalar regularization parameter is often determined in a heuristic manner; in contrast, our approach infers the optimal regularization setting under a Bayesian framework. Furthermore, Bayesian inference enables an independent regularization scheme where each coefficient (or weight) is associated with an independent regularization parameter. Simulations illustrate the improvement using our method in discovering sparse structure from noisy data.	Univ Penn, Dept Elect & Syst Engn, GRASP Lab, Philadelphia, PA 19104 USA	Lin, YQ (reprint author), Univ Penn, Dept Elect & Syst Engn, GRASP Lab, Philadelphia, PA 19104 USA.						Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010; Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Duttweiler DL, 2000, IEEE T SPEECH AUDI P, V8, P508, DOI 10.1109/89.861368; Tipping ME, 2001, J MACH LEARN RES, V1, P211, DOI 10.1162/15324430152748236; Fuchs JJ, 1999, IEEE T SIGNAL PROCES, V47, P237, DOI 10.1109/78.738263; LIN YP, IN PRESS IEEE T SIGN; Malioutov D. M., 2005, IEEE INT C AC SPEECH; ZANGWILL W. I., 1969, NONLINEAR PROGRAMMIN	9	0	0	0	7	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA	1520-6149		978-1-4244-0468-1	INT CONF ACOUST SPEE			2006							5463	5466				4	Acoustics; Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic; Imaging Science & Photographic Technology	Acoustics; Computer Science; Engineering; Imaging Science & Photographic Technology	BFZ22	WOS:000245559906069		
S	Raich, R; Hero, AO			IEEE	Raich, Raviv; Hero, Alfred O., III			Sparse image reconstruction for partially known blur functions	2006 IEEE International Conference on Image Processing, ICIP 2006, Proceedings	IEEE International Conference on Image Processing (ICIP)		English	Proceedings Paper	IEEE International Conference on Image Processing (ICIP 2006)	OCT 08-11, 2006	Atlanta, GA	IEEE		deconvolution; image reconstruction; sparse matrices	ALGORITHM; REGRESSION	In this paper, we consider the problem of image reconstruction from the noisy blurred version of an original image when the blurring operator is partially known and the original image is sparse. Using optimization transfer, we derive a novel iterative algorithm in closed-form that incorporates both sparseness and partial knowledge of the image. We demonstrate the performance of the algorithm using simulations.	Univ Michigan, Dept Elect Engn & Comp Sci, Ann Arbor, MI 48109 USA	Raich, R (reprint author), Univ Michigan, Dept Elect Engn & Comp Sci, Ann Arbor, MI 48109 USA.						ALLINEY S, 1994, IEEE T SIGNAL PROCES, V42, P618, DOI 10.1109/78.277854; Andrews H., 1977, DIGITAL IMAGE RESTOR; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Efron B, 2004, ANN STAT, V32, P407; Daubechies I, 2004, COMMUN PUR APPL MATH, V57, P1413, DOI 10.1002/cpa.20042; Figueiredo MAT, 2003, IEEE T IMAGE PROCESS, V12, P906, DOI 10.1109/TIP.2003.814255; Figueiredo M., 2005, P IEEE INT C IM PROC, V2, P782, DOI DOI 10.1109/ICIP.2005.1530172; SOLO V, 1996, P IEEE INT C IM PROC, V3, P89, DOI 10.1109/ICIP.1996.560376; STRANG G, 1988, LINEAR ALGEBRA ITS A; TING M, UNPUB BAYESIAN APPRO; Vandenberghe L., 2004, CONVEX OPTIMIZATION	11	2	2	1	2	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA	1522-4880		978-1-4244-0481-0	IEEE IMAGE PROC			2006							637	640		10.1109/ICIP.2006.312461		4	Computer Science, Artificial Intelligence; Computer Science, Information Systems; Computer Science, Software Engineering; Imaging Science & Photographic Technology; Radiology, Nuclear Medicine & Medical Imaging	Computer Science; Imaging Science & Photographic Technology; Radiology, Nuclear Medicine & Medical Imaging	BGA25	WOS:000245768500160		
S	Ting, M; Raich, R; Hero, AO			IEEE	Ting, Michael; Raich, Raviv; Hero, Alfred O., III			Sparse image reconstruction using sparse priors	2006 IEEE International Conference on Image Processing, ICIP 2006, Proceedings	IEEE International Conference on Image Processing (ICIP)		English	Proceedings Paper	IEEE International Conference on Image Processing (ICIP 2006)	OCT 08-11, 2006	Atlanta, GA	IEEE		sparse image reconstruction; empirical Bayes; stein's unbiased risk estimator; sparse Bayesian learning; LASSO estimator	REGRESSION; SELECTION	Sparse image reconstruction is of interest in the fields of radioastronomy and molecular imaging. The observation is assumed to be a linear transformation of the image, and corrupted by additive white Gaussian noise. We study the usage of sparse priors in the empirical Bayes framework: it permits the selection of the hyperparameters of the prior in a data-driven fashion. Three sparse image reconstruction methods are proposed. A simulation study was performed using a binary-valued image and a Gaussian point spread function. In the range of signal to noise ratios considered, the proposed methods had better performance than sparse Bayesian learning (SBL).	Univ Michigan, Ann Arbor, MI 48109 USA	Ting, M (reprint author), Univ Michigan, Ann Arbor, MI 48109 USA.						Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Gribonval R, 2003, IEEE T INFORM THEORY, V49, P3320, DOI 10.1109/TIT.2003.820031; Wipf DP, 2004, IEEE T SIGNAL PROCES, V52, P2153, DOI 10.1109/TSP.2004.831016; Efron B, 2004, ANN STAT, V32, P407; Figueiredo MAT, 2003, IEEE T IMAGE PROCESS, V12, P906, DOI 10.1109/TIP.2003.814255; Johnstone IM, 2004, ANN STAT, V32, P1594, DOI 10.1214/009053604000000030; Ng L., 1999, P IEEE INT C IM PROC, V3, P722, DOI 10.1109/ICIP.1999.817211; STEIN CM, 1981, ANN STAT, V9, P1135, DOI 10.1214/aos/1176345632; TING M, UNPUB EMPIRICAL BAYE	9	4	4	1	1	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA	1522-4880		978-1-4244-0481-0	IEEE IMAGE PROC			2006							1261	1264		10.1109/ICIP.2006.312574		4	Computer Science, Artificial Intelligence; Computer Science, Information Systems; Computer Science, Software Engineering; Imaging Science & Photographic Technology; Radiology, Nuclear Medicine & Medical Imaging	Computer Science; Imaging Science & Photographic Technology; Radiology, Nuclear Medicine & Medical Imaging	BGA25	WOS:000245768500316		
S	Simila, T; Tikka, J			IEEE	Simila, Timo; Tikka, Jarkko			Common subset selection of inputs in multiresponse regression	2006 IEEE INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORK PROCEEDINGS, VOLS 1-10	IEEE International Joint Conference on Neural Networks (IJCNN)		English	Proceedings Paper	IEEE International Joint Conference on Neural Network	JUL 16-21, 2006	Vancouver, CANADA	IEEE			VARIABLE SELECTION; MULTIVARIATE REGRESSION; LINEAR-REGRESSION; MODEL SELECTION; PREDICTION; SHRINKAGE	We propose the Multiresponse Sparse Regression algorithm, an input selection method for the purpose of estimating several response variables. It is a forward selection procedure for linearly parameterized models, which updates with carefully chosen step lengths. The step length rule extends the correlation criterion of the Least Angle Regression algorithm for many responses. We present a general concept and explicit formulas for three different variants of the algorithm. Based on experiments with simulated data, the proposed method competes favorably with other methods when many correlated inputs are available for model construction. We also study the performance with several real data sets.	Helsinki Univ Technol, Lab Comp & Informat Sci, FI-02015 Espoo, Finland	Simila, T (reprint author), Helsinki Univ Technol, Lab Comp & Informat Sci, POB 5400, FI-02015 Espoo, Finland.	timo.simila@hut.fi; tikka@cis.hut.fi					Abraham B, 2005, COMPUT STAT DATA AN, V48, P5, DOI 10.1016/j.csda.2003.11.021; Yuan M, 2006, J ROY STAT SOC B, V68, P49, DOI 10.1111/j.1467-9868.2005.00532.x; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Osborne MR, 2000, IMA J NUMER ANAL, V20, P389, DOI 10.1093/imanum/20.3.389; Brown PJ, 2002, J R STAT SOC B, V64, P519, DOI 10.1111/1467-9868.00348; Zou H, 2005, J ROY STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x; Turlach BA, 2005, TECHNOMETRICS, V47, P349, DOI 10.1198/004017005000000139; Efron B, 2004, ANN STAT, V32, P407; COPAS JB, 1983, J R STAT SOC B, V45, P311; BARRETT BE, 1994, STAT COMPUT, V4, P203, DOI 10.1007/BF00142572; BEDRICK EJ, 1994, BIOMETRICS, V50, P226, DOI 10.2307/2533213; Breiman L, 1997, J ROY STAT SOC B MET, V59, P3, DOI 10.1111/1467-9868.00054; Burnham AJ, 1999, CHEMOMETR INTELL LAB, V48, P167, DOI 10.1016/S0169-7439(99)00018-0; HOCKING RR, 1983, TECHNOMETRICS, V25, P219, DOI 10.2307/1268603; Miller A. J., 1990, SUBSET SELECTION REG; Reinsel G., 1998, MULTIVARIATE REDUCED; RENCHER AC, 2002, METHODS MULTIVATIATE; Simila T., 2005, P INT C ART NEUR NET, P97; Srivastava M. S., 2002, METHODS MULTIVARIATE; Srivastava MS, 2003, COMMUN STAT-SIMUL C, V32, P389, DOI 10.1081/SAC-120017497; Sulkava M, 2006, ECOL MODEL, V191, P118, DOI 10.1016/j.ecolmodel.2005.08.016; TURLACH BA, 2005, S OPT DAT AN HON M O	22	4	4	2	2	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA	1098-7576		978-0-7803-9490-2	IEEE IJCNN			2006							1908	1915				8	Computer Science, Artificial Intelligence	Computer Science	BFW67	WOS:000245125903045		
S	Donoho, D; Stodden, V			IEEE	Donoho, David; Stodden, Victoria			Breakdown point of model selection when the number of variables exceeds the number of observations	2006 IEEE INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORK PROCEEDINGS, VOLS 1-10	IEEE International Joint Conference on Neural Networks (IJCNN)		English	Proceedings Paper	IEEE International Joint Conference on Neural Network	JUL 16-21, 2006	Vancouver, CANADA	IEEE			REGRESSION	The classical multivariate linear regression problem assumes p variables X(1), X(2),..., X, and a response vector y, each with n observations, and a linear relationship between the two: y = X beta + z, where z similar to N(0, sigma(2)). We point out that when p > n, there is a breakdown point for standard model selection schemes, such that model selection only works well below a certain critical complexity level depending on n/p. We apply this notion to some standard model selection algorithms (Forward Stepwise, LASSO, LARS) in the case where p >> n. We nd that 1) the breakdown point is well-de ned for random X-models and low noise, 2) increasing noise shifts the breakdown point to lower levels of sparsity, and reduces the model recovery ability of the algorithm in a systematic way, and 3) below breakdown, the size of coef cient errors follows the theoretical error distribution for the classical linear model.	Stanford Univ, Dept Stat, Stanford, CA 94305 USA	Donoho, D (reprint author), Stanford Univ, Dept Stat, Stanford, CA 94305 USA.	donoho@stanford.edu; vcs@stanford.edu					Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Chen SSB, 2001, SIAM REV, V43, P129, DOI 10.1137/S003614450037906X; BOX GEP, 1986, TECHNOMETRICS, V28, P11, DOI 10.2307/1269599; Efron B, 2004, ANN STAT, V32, P407; Buckheit J, 1995, WAVELETS STAT; DONOHO D, 2005, DISCRETE COMPUT 1222; Donoho D. L., 2004, INT J WAVELETS MULTI, V2, P391, DOI 10.1142/S0219691304000615; DONOHO DL, IN PRESS ANN STAT; WEISBERG S, 1985, APPL LINEAR REGR	9	12	13	0	5	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA	1098-7576		978-0-7803-9490-2	IEEE IJCNN			2006							1916	1921				6	Computer Science, Artificial Intelligence	Computer Science	BFW67	WOS:000245125903046		
S	Costa, MA; Braga, AP			IEEE	Costa, Marcelo Azevedo; Braga, Antonio Padua			Optimization of neural networks with multi-objective LASSO algorithm	2006 IEEE INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORK PROCEEDINGS, VOLS 1-10	IEEE International Joint Conference on Neural Networks (IJCNN)		English	Proceedings Paper	IEEE International Joint Conference on Neural Network	JUL 16-21, 2006	Vancouver, CANADA	IEEE				This paper presents a bi-objective algorithm that optimizes the error and the sum of the absolute weights of a Multi-Layer Perceptron neural network. The algorithm is based on the linear Least Absolute Shrinkage and Selection Operator (LASSO) and provides simultaneous generalization and weight selection optimization. The algorithm searches for a set of optimal solutions called Pareto set from which a single weight vector with best performance and reduced number of weights is selected based on a validation criterion. The method is applied to classification and regression real problems and compared with the norm based multi-objective algorithm. Results show that the neural networks obtained have improved generalization performance and reduced topology.	UFMG Pampulha, BR-30161970 Belo Horizonte, MG, Brazil	Costa, MA (reprint author), UFMG Pampulha, Engn Elect Campus,Caixa Postal 209, BR-30161970 Belo Horizonte, MG, Brazil.	azevedo@est.ufmg.br; braga@cpdee.ufmg.br	Braga, Antonio/A-2912-2008	Braga, Antonio/0000-0002-9007-0920			Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0; Prechelt L, 1998, NEURAL NETWORKS, V11, P761, DOI 10.1016/S0893-6080(98)00010-0; HARRISON D, 1978, J ENVIRON ECON MANAG, V5, P81, DOI 10.1016/0095-0696(78)90006-2; Bartlett P.L., 1997, P NIPS 9, P134; BLAND RG, 1981, OPER RES, V29, P1039, DOI 10.1287/opre.29.6.1039; COSTA MA, 2002, P 7 BRAZ S NEUR NETW; Costa MA, 2003, NEUROCOMPUTING, V51, P467, DOI 10.1016/S0925-2312(02)00697-5; COSTA MA, 2003, J INTELL FUZZY SYST, V13, P73; Fieldsend JE, 2004, APPL MULTIOBJECTIVE, P675; Hastie T., 2001, SPRINGER SERIES STAT; Haykin S., 1994, NEURAL NETWORKS COMP; Jin Y, 2004, P 2004 IEEE C EV COM, P1; Krogh A, 1991, P NIPS, V4, P950; REED R, 1993, IEEE T NEURAL NETWOR, V4, P740, DOI 10.1109/72.248452; Rumelhart D. E., 1986, PARALLEL DISTRIBUTED; Teixeira R D, 2001, Int J Neural Syst, V11, P265, DOI 10.1016/S0129-0657(01)00070-9	17	0	0	1	4	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA	1098-7576		978-0-7803-9490-2	IEEE IJCNN			2006							3312	3318				7	Computer Science, Artificial Intelligence	Computer Science	BFW67	WOS:000245125906014		
B	Giron, FJ; Moreno, E; Martinez, ML		Balakrishnan, N; Castillo, E; Sarabia, JM		Giron, F. Javier; Moreno, Elias; Martinez, M. Lina			An objective Bayesian procedure for variable selection in regression	Advances in Distribution Theory, Order Statistics, and Inference	STATISTICS FOR INDUSTRY AND TECHNOLOGY		English	Proceedings Paper	International Conference on Distribution Theory, Order Statistics, and Inference	JUN 16-18, 2004	Santander, SPAIN		Univ Cantabria	calibration curve; determination coefficient; g-priors; intrinsic priors; lasso criterion; model selection; normal linear model; reference priors	MODEL SELECTION	The Bayesian analysis of the variable selection problem in linear regression when using objective priors needs some form of encompassing the class of all submodels of the full linear model as they are nonnested models. After we provide a nested setting, objective intrinsic priors suitable for computing model posterior probabilities, on which the selection is based, can be derived. The way of encompassing the models is not unique and there is no clear indications for the optimal way. Typically, the class of linear models are encompassed into the full model. In this paper, we explore a new way of encompassing the class of linear models that consequently produces a new method for variable selection. This method seems to have some advantages with respect to the usual one. Specific intrinsic priors and model posterior probabilities are provided along with some of their main properties. Comparisons are made with R-2 and adjusted R-2, along with other frequentist methods for variable selection as lasso. Some illustrations on simulated and real data are provided.	Univ Malaga, Fac Ciencias, Dept Estadist & IO, E-29071 Malaga, Spain	Giron, FJ (reprint author), Univ Malaga, Fac Ciencias, Dept Estadist & IO, Campus Teatinos SN, E-29071 Malaga, Spain.						Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; FURNIVAL GM, 1974, TECHNOMETRICS, V16, P499, DOI 10.2307/1267601; Berger JO, 1996, J AM STAT ASSOC, V91, P109, DOI 10.2307/2291387; MALLOWS CL, 1973, TECHNOMETRICS, V15, P661, DOI 10.2307/1267380; BERGER JO, 1996, MODELLING PREDICTION, P276; Casella G, 2005, STAT MED, V24, P583, DOI 10.1002/sim.2038; CASELLA G, 2006, IN PRESS J AM STAT A; Denison D. G. T., 2002, BAYESIAN METHODS NON; Giron FJ, 2003, J STAT PLAN INFER, V113, P49, DOI 10.1016/S0378-3758(01)00299-3; GIRON FJ, 2003, UNPUB COMP ANAL OBJE, P553; Hastie T., 2001, ELEMENTS STAT LEARNI; Miller A, 2002, SUBSET SELECTION REG, V2nd; Moreno E, 1998, J AM STAT ASSOC, V93, P1451, DOI 10.2307/2670059; MORENO E, 2000, J STAT PLANNING INFE, V81, P323; MORENO E, 2003, REV R ACAD CIEN S A, P53; Moreno E, 2005, J STAT PLAN INFER, V131, P117, DOI 10.1016/j.jspi.2003.12.016; Moreno E, 2003, J STAT PLAN INFER, V111, P129, DOI 10.1016/S0378-3758(02)00294-X; Moreno E, 2003, SCAND J STAT, V30, P565, DOI 10.1111/1467-9469.00349; Stamey T., 1989, J UROLOGY, V16, P1076; Zellner Arnold, 1986, BAYESIAN INFERENCE D	20	2	2	0	9	BIRKHAUSER BOSTON	CAMBRIDGE	675 MASSACHUSETTS AVE, CAMBRIDGE, MA 02139-2333 USA			0-8176-4361-3	STAT IND TECHNOL			2006							389	404				16	Mathematics, Applied; Statistics & Probability	Mathematics	BEM35	WOS:000238122900025		
S	Theiler, J; Glocer, K		Shen, SS; Lewis, PE		Theiler, James; Glocer, Karen			Sparse linear filters for detection and classification in hyperspectral imagery - art. no. 62330H	Algorithms and Technologies for Multispectral, Hyperspectral, and Ultraspectral Imagery XII Pts 1 and 2	PROCEEDINGS OF THE SOCIETY OF PHOTO-OPTICAL INSTRUMENTATION ENGINEERS (SPIE)		English	Proceedings Paper	Conference on Algorithms and Technologies for Multispectral, Hyperspectral, and Ultraspectral Imagery XII	APR 17-20, 2006	Kissimmee, FL			matched filter; hyperspectral imagery; adaptive signal detection	BAND SELECTION; FEATURE-EXTRACTION; BOUND ALGORITHM; SYSTEMS; BRANCH; REGRESSION	We investigate the use of convex optimization to identify sparse linear filters in hyperspectral. imagery. A linear filter is sparse if a large fraction of its coefficients are zero. A sparse linear filter can be advantageous because it only needs to access a subset of the available spectral channels, and it can be applied to high-dimensional data more cheaply than a standard linear detector. Finding good sparse filters is nontrivial because there is a combinatorially large number of discrete possibilities from which to choose the optimal subset of nonzero coefficients. But, by converting the optimality criterion into a convex loss function, and by employing an L1 penalty, one can obtain sparse solutions that are globally optimal. We investigate the performance of these sparse filters as a function of their sparsity, and compare the convex optimization approach with more traditional alternatives for feature selection. The methodology is applied both to the adaptive matched filter for weak signal detection, and to the Fisher linear discriminant for terrain categorization.	Los Alamos Natl Lab, Space & Remote Sensing Sci Grp, Los Alamos, NM USA	Theiler, J (reprint author), Los Alamos Natl Lab, Space & Remote Sensing Sci Grp, POB 1663, Los Alamos, NM USA.						Dietterich T, 1995, ACM COMPUT SURV, V27, P326, DOI 10.1145/212094.212114; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Jain A, 1997, IEEE T PATTERN ANAL, V19, P153, DOI 10.1109/34.574797; Osborne MR, 2000, IMA J NUMER ANAL, V20, P389, DOI 10.1093/imanum/20.3.389; HUGHES GF, 1968, IEEE T INFORM THEORY, V14, P55, DOI 10.1109/TIT.1968.1054102; PUDIL P, 1994, PATTERN RECOGN LETT, V15, P1119, DOI 10.1016/0167-8655(94)90127-9; Efron B, 2004, ANN STAT, V32, P407; Bassett EM, 1997, P SOC PHOTO-OPT INS, V3118, P28, DOI 10.1117/12.283840; Clodius WB, 1998, P SOC PHOTO-OPT INS, V3377, P11, DOI 10.1117/12.319369; De Backer S, 2005, IEEE GEOSCI REMOTE S, V2, P319, DOI 10.1109/LGRS.2005.848511; Fukunaga K., 1990, INTRO STAT PATTERN R; GLOCER K, 2005, P ICML, V22, P249; Golub G., 1996, MATRIX COMPUTATIONS; Guyon I., 2003, Journal of Machine Learning Research, V3, DOI 10.1162/153244303322753616; Harvey NR, 2002, IEEE T GEOSCI REMOTE, V40, P393, DOI 10.1109/36.992801; Hastie T., 2001, ELEMENTS STAT LEARNI; Karlholm J, 2002, APPL OPTICS, V41, P6786, DOI 10.1364/AO.41.006786; Keshava N, 2003, P SOC PHOTO-OPT INS, V5093, P440, DOI 10.1117/12.487534; KIM Y, 2004, P ICML, V21, P60; Korycinski D, 2004, P SOC PHOTO-OPT INS, V5238, P213, DOI 10.1117/12.517487; KUMAR M, 2004, P INT GEOSCI REMOTE, V5, P3264; Kumar S, 2001, IEEE T GEOSCI REMOTE, V39, P1368, DOI 10.1109/36.934070; NARENDRA P, 1977, IEEE T COMPUT, V26, P917; Paskaleva B, 2005, P SOC PHOTO-OPT INS, V5806, P131, DOI 10.1117/12.602995; Perkins S., 2003, Journal of Machine Learning Research, V3, DOI 10.1162/153244303322753698; Perkins S, 2003, P IMLS INT C MACH LE, V20, P592; PRICE JC, 1994, APPL OPTICS, V33, P3281, DOI 10.1364/AO.33.003281; RIEDMANN M, 2003, P IEEE INT GEOSC REM, V3, P1770; Serpico SB, 2003, P SOC PHOTO-OPT INS, V4885, P347, DOI 10.1117/12.463524; Serpico SB, 2001, IEEE T GEOSCI REMOTE, V39, P1360, DOI 10.1109/36.934069; Shen SS, 2002, P SOC PHOTO-OPT INS, V4725, P18, DOI 10.1117/12.478755; Somol P, 2004, IEEE T PATTERN ANAL, V26, P900, DOI 10.1109/TPAMI.2004.28; Stearns S. D., 1976, P 3 INT C PATT REC C, P71; STEARNS SD, 1993, P SOC PHOTO-OPT INS, V2028, P118, DOI 10.1117/12.158622; Vandenberghe L., 2004, CONVEX OPTIMIZATION; VANE G, 1993, REMOTE SENS ENVIRON, V44, P127, DOI 10.1016/0034-4257(93)90012-M; WIERSMA DJ, 1980, IEEE T GEOSCI REMOTE, V18, P180, DOI 10.1109/TGRS.1980.350271; YU B, 1993, PATTERN RECOGN, V26, P883, DOI 10.1016/0031-3203(93)90054-Z	38	5	5	1	3	SPIE-INT SOC OPTICAL ENGINEERING	BELLINGHAM	1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA	0277-786X		0-8194-6289-6	P SOC PHOTO-OPT INS			2006	6233		1&2				H2330	H2330	62330H	10.1117/12.665994		12	Instruments & Instrumentation; Remote Sensing; Optics; Imaging Science & Photographic Technology	Instruments & Instrumentation; Remote Sensing; Optics; Imaging Science & Photographic Technology	BES78	WOS:000239353500017		
J	Millstein, J; Conti, DV; Gilliland, FD; Gauderman, WJ				Millstein, J; Conti, DV; Gilliland, FD; Gauderman, WJ			A testing framework for identifying susceptibility genes in the presence of epistasis	AMERICAN JOURNAL OF HUMAN GENETICS			English	Article							MULTIFACTOR-DIMENSIONALITY REDUCTION; GENOTYPE-PHENOTYPE ASSOCIATIONS; S-TRANSFERASE M1; BREAST-CANCER; OXIDATIVE STRESS; HYPERTENSION; DISEASE; POLYMORPHISMS; COMPLEXITY; RECEPTOR	An efficient testing strategy called the "focused interaction testing framework" (FITF) was developed to identify susceptibility genes involved in epistatic interactions for case-control studies of candidate genes. In the FITF approach, likelihood-ratio tests are performed in stages that increase in the order of interaction considered. Joint tests of main effects and interactions are performed conditional on significant lower-order effects. A reduction in the number of tests performed is achieved by prescreening gene combinations with a goodness-of-fit x 2 statistic that depends on association among candidate genes in the pooled case-control group. Multiple testing is accounted for by controlling false-discovery rates. Simulation analysis demonstrated that the FITF approach is more powerful than marginal tests of candidate genes. FITF also outperformed multifactor dimensionality reduction when interactions involved additive, dominant, or recessive genes. In an application to asthma case-control data from the Children's Health Study, FITF identified a significant multilocus effect between the nicotinamide adenine dinucleotide ( phosphate) reduced: quinone oxidoreductase gene (NQO1), myeloperoxidase gene (MPO), and catalase gene ( CAT) (unadjusted P = .00026), three genes that are involved in the oxidative stress pathway. In an independent data set consisting primarily of African American and Asian American children, these three genes also showed a significant association with asthma status (P = .0008).	NOAA, Natl Marine Fisheries Serv, Alaska Fisheries Sci Ctr, Seattle, WA 98115 USA; Univ So Calif, Dept Prevent Med, Div Biostat, Los Angeles, CA 90089 USA	Millstein, J (reprint author), NOAA, Natl Marine Fisheries Serv, Alaska Fisheries Sci Ctr, 7600 Sand Point Way NE,Bldg 4,1133B, Seattle, WA 98115 USA.	josh.millstein@noaa.gov					Ambrosone CB, 2005, CANCER RES, V65, P1105; Andreadis AA, 2003, FREE RADICAL BIO MED, V35, P213, DOI 10.1016/S0891-5849(03)00278-8; Ritchie MD, 2001, AM J HUM GENET, V69, P138, DOI 10.1086/321276; Forsberg L, 2001, FREE RADICAL BIO MED, V30, P500, DOI 10.1016/S0891-5849(00)00487-1; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Ritchie MD, 2003, GENET EPIDEMIOL, V24, P150, DOI 10.1002/gepi.10218; MacNee W, 2001, EUR J PHARMACOL, V429, P195, DOI 10.1016/S0014-2999(01)01320-6; Staessen JA, 2001, J HYPERTENS, V19, P1349, DOI 10.1097/00004872-200108000-00002; Ritchie MD, 2003, BMC BIOINFORMATICS, V4, DOI 10.1186/1471-2105-4-28; BENJAMINI Y, 1995, J ROY STAT SOC B MET, V57, P289; Hoh J, 2001, GENOME RES, V11, P2115, DOI 10.1101/gr.204001; Hahn LW, 2003, BIOINFORMATICS, V19, P376, DOI 10.1093/bioinformatics/btf869; Aston CE, 2005, HUM GENET, V116, P208, DOI 10.1007/s00439-004-1206-7; Balmain A, 2000, CARCINOGENESIS, V21, P371, DOI 10.1093/carcin/21.3.371; Barlassina C, 2002, J AM SOC NEPHROL, V13, pS155, DOI 10.1097/01.ASN.0000032524.13069.88; Cho YM, 2004, DIABETOLOGIA, V47, P549, DOI 10.1007/s00125-003-1321-3; Coffey CS, 2004, BMC BIOINFORMATICS, V5, DOI 10.1186/1471-2105-5-49; Cook Nancy R., 2004, Statistics in Medicine, V23, P1439, DOI 10.1002/sim.1749; Culverhouse R, 2004, GENET EPIDEMIOL, V27, P141, DOI 10.1002/gepi.20006; Culverhouse R, 2002, AM J HUM GENET, V70, P461, DOI 10.1086/338759; David GL, 2003, AM J RESP CRIT CARE, V168, P1199, DOI 10.1164/rccm.200305-684OC; De Miglio MR, 2004, INT J CANCER, V111, P9, DOI 10.1002/ijc.20225; Devlin B, 2003, GENET EPIDEMIOL, V25, P36, DOI 10.1002/gepi.10237; Dong CH, 2005, EUR J HUM GENET, V13, P102, DOI 10.1038/sj.ejhg.5201292; Bastone L, 2004, HUM HERED, V58, P82, DOI 10.1159/000083029; Foulkes AS, 2005, STAT MED, V24, P775, DOI 10.1002/sim.1965; Foulkes AS, 2004, J ROY STAT SOC C-APP, V53, P311, DOI 10.1046/j.1467-9876.2003.05094.x; Gilliland FD, 1999, ENVIRON HEALTH PERSP, V107, P403; Hahn Lance W, 2004, In Silico Biol, V4, P183; Hsueh WC, 2001, DIABETES CARE, V24, P672, DOI 10.2337/diacare.24.4.672; Kim JH, 2001, GENOMICS, V74, P273, DOI 10.1006/geno.2001.6569; Kuida S, 2000, GENOME RES, V10, P49; Longmate JA, 2001, AM J HUM GENET, V68, P1229, DOI 10.1086/320106; Mak JCW, 2004, J ALLERGY CLIN IMMUN, V114, P260, DOI 10.1016/j.jaci.2004.05.013; Moore JH, 2004, EXPERT REV MOL DIAGN, V4, P795, DOI 10.1586/14737159.4.6.795; Moore Jason H, 2002, Pac Symp Biocomput, P53; NABAR CK, 2000, HYPERTENSION, V36, P986; Qin SY, 2005, EUR J HUM GENET, V13, P807, DOI 10.1038/sj.ejhg.5201418; ROLDAN C, 2005, HAEMATOLGICA, V90, P421; Schaid DJ, 1996, GENET EPIDEMIOL, V13, P423, DOI 10.1002/(SICI)1098-2272(1996)13:5<423::AID-GEPI1>3.0.CO;2-3; Siegel D, 2004, MOL PHARMACOL, V65, P1238, DOI 10.1124/mol.65.5.1238; Sindhu RK, 2005, BBA-MOL CELL RES, V1743, P86, DOI 10.1016/j.bbamer.2004.08.013; Soares ML, 2005, HUM MOL GENET, V14, P543, DOI 10.1093/hmg/ddi051; Tripodis N, 2001, J NATL CANCER I, V93, P1484, DOI 10.1093/jnci/93.19.1484; Tsai CT, 2004, CIRCULATION, V109, P1640, DOI 10.1161/01.CIR.0000124487.36586.26; Turan NN, 2000, PHARMACOL RES, V41, P589, DOI 10.1006/phrs.1999.0628; Ukkola O, 2001, INT J OBESITY, V25, P1332, DOI 10.1038/sj.ijo.0801735; Williams SM, 2000, HYPERTENSION, V36, P2; Williams SM, 2004, HUM HERED, V57, P28, DOI 10.1159/000077387; Yanchina ED, 2004, B EXP BIOL MED+, V137, P64, DOI 10.1023/B:BEBM.0000024389.16247.0a; Yang P, 2004, CARCINOGENESIS, V25, P1935, DOI 10.1093/carcin/bgh203	51	115	117	0	2	UNIV CHICAGO PRESS	CHICAGO	1427 E 60TH ST, CHICAGO, IL 60637-2954 USA	0002-9297			AM J HUM GENET	Am. J. Hum. Genet.	JAN	2006	78	1					15	27		10.1086/498850		13	Genetics & Heredity	Genetics & Heredity	993CN	WOS:000233930400003	16385446	
S	Hartmann, WM		Dongarra, J; Madsen, K; Wasniewski, J		Hartmann, WM			Dimension reduction vs. variable selection	APPLIED PARALLEL COMPUTING: STATE OF THE ART IN SCIENTIFIC COMPUTING	LECTURE NOTES IN COMPUTER SCIENCE		English	Article; Proceedings Paper	7th International Workshop on State of the Art in Scientific Computing	JUN 20-23, 2004	Lyngby, DENMARK	Tech Univ Denmark, Dept Informat & Math Modelling, High Performance Comp, Numerical Algorithms Grp Ltd, Comsol A/S Denmark, Sun Microsyst Denmark, UNI C Danish Comp Ctr Denmark, Microsoft Denmark, IBM, Denmark			REGRESSION; SHRINKAGE; LASSO	The paper clarifies the difference between dimension reduction and variable selection methods in statistics and data mining. Traditional and recent modeling methods are listed and a typical approach to variable selection is mentioned. In addition, the need for and types of cross validation in modeling is sketched.	SAS Inst Inc, Cary, NC USA	Hartmann, WM (reprint author), SAS Inst Inc, Cary, NC USA.						Anderberg M. R., 1973, CLUSTER ANAL APPL; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Osborne MR, 2000, IMA J NUMER ANAL, V20, P389, DOI 10.1093/imanum/20.3.389; Guyon I, 2002, MACH LEARN, V46, P389, DOI 10.1023/A:1012487302797; BENJAMINI Y, 1995, J ROY STAT SOC B MET, V57, P289; [Anonymous], 1990, SAS STAT US GUID; Tibshirani R, 2002, P NATL ACAD SCI USA, V99, P6567, DOI 10.1073/pnas.082099299; BI J, 2002, J MACH LEARN RES, V1, P1; BREIMAN L, 1993, BETTER SUBSET SELECT; Breiman L., 1984, CLASSIFICATION REGRE; CARROLL JD, 1998, HDB PERCEPTION COGNI, P179; Chen S. S., 1999, SIAM J SCI COMPUT, V20, P33; DONOHO DL, 1995, J ROY STAT SOC B MET, V57, P301; EFRON B, 2002, ANN STAT, V32, P407; FUNG G, 2003, COMPUTATIONAL OPTIMI, P1; Gifi A., 1981, NONLINEAR MULTIVARIA; GREENACRE MJ, 1988, THEORY APPL CORRES A; Harman H. H., 1976, MODERN FACTOR ANAL; HARTMANN W, 1995, CMAT USERS MANUAL; Joachims T., 1999, ADV KERNEL METHODS S; Kaufman L, 1990, FINDING GROUPS DATA; LI KC, 1992, J AM STAT ASSOC, V87, P1025, DOI 10.2307/2290640; LI KC, 1991, J AM STAT ASSOC, V86, P316, DOI 10.2307/2290563; MANGASARIAN OL, 2004, 0401 U WISC DAT MIN; MCCABE GP, 1984, TECHNOMETRICS, V26, P139; Miller A, 2002, SUBSET SELECTION REG, V2nd; Mulaik S. A., 1972, FDN FACTOR ANAL; Osborne MR, 2000, J COMPUT GRAPH STAT, V9, P319, DOI 10.2307/1390657; Ripley B. D., 1996, PATTERN RECOGNITION; Rosipal R., 2001, J MACHINE LEARNING R, V2, P97; Scholkopf B., 2002, LEARNING KERNELS; SOMERVILLE PN, 2004, STEP DOWN FDR PROCED; SOMERVILLE PN, 2001, J STAT SOFTWARE; Vapnik V.N., 1995, NATURE STAT LEARNING; WEISBERG S, 2002, JSS, V7; Weston J., 2000, NIPS, V13, P668; Wold H., 1966, MULTIVARIATE ANAL; YANG J, 1997, FEATURE SELECTION US; Zou H., 2003, REGRESSION SHRINKAGE; Zou H., 2004, SPARSE PRINCIPAL COM; R LANGUAGE PACKAGES	41	2	2	1	1	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		3-540-29067-2	LECT NOTES COMPUT SC			2006	3732						931	938				8	Computer Science, Theory & Methods	Computer Science	BEE79	WOS:000237003200113		
J	Zhang, HH; Ahn, J; Lin, XD; Park, C				Zhang, HH; Ahn, J; Lin, XD; Park, C			Gene selection using support vector machines with non-convex penalty	BIOINFORMATICS			English	Article							DIFFERENTIALLY EXPRESSED GENES; MICROARRAY DATA; CANCER CLASSIFICATION; VARIABLE SELECTION; BREAST-CANCER; SHRINKAGE; PROFILES	Motivation: With the development of DNA microarray technology, scientists can now measure the expression levels of thousands of genes simultaneously in one single experiment. One current difficulty in interpreting microarray data comes from their innate nature of 'high-dimensional low sample size'. Therefore, robust and accurate gene selection methods are required to identify differentially expressed group of genes across different samples, e.g. between cancerous and normal cells. Successful gene selection will help to classify different cancer types, lead to a better understanding of genetic signatures in cancers and improve treatment strategies. Although gene selection and cancer classification are two closely related problems, most existing approaches handle them separately by selecting genes prior to classification. We provide a unified procedure for simultaneous gene selection and cancer classification, achieving high accuracy in both aspects. Results: In this paper we develop a novel type of regularization in support vector machines (SVMs) to identify important genes for cancer classification. A special nonconvex penalty, called the smoothly clipped absolute deviation penalty, is imposed on the hinge loss function in the SVM. By systematically thresholding small estimates to zeros, the new procedure eliminates redundant genes automatically and yields a compact and accurate classifier. A successive quadratic algorithm is proposed to convert the non-differentiable and non-convex optimization problem into easily solved linear equation systems. The method is applied to two real datasets and has produced very promising results.	N Carolina State Univ, Dept Stat, Raleigh, NC 27695 USA; Univ N Carolina, Dept Stat & Operat Res, Chapel Hill, NC 27599 USA; Univ Cincinnati, Dept Math Sci, Cincinnati, OH 45221 USA; Univ Georgia, Dept Stat, Athens, GA 30602 USA	Zhang, HH (reprint author), N Carolina State Univ, Dept Stat, Raleigh, NC 27695 USA.	hzhang@stat.ncsu.edu					Perou CM, 2000, NATURE, V406, P747, DOI 10.1038/35021093; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; Chapelle O, 2002, MACH LEARN, V46, P131, DOI 10.1023/A:1012450327387; Sotiriou C, 2003, P NATL ACAD SCI USA, V100, P10393, DOI 10.1073/pnas.1732912100; Troyanskaya OG, 2002, BIOINFORMATICS, V18, P1454, DOI 10.1093/bioinformatics/18.11.1454; Furey TS, 2000, BIOINFORMATICS, V16, P906, DOI 10.1093/bioinformatics/16.10.906; Golub TR, 1999, SCIENCE, V286, P531, DOI 10.1126/science.286.5439.531; Stitt M, 2003, CURR OPIN BIOTECH, V14, P136, DOI 10.1016/S0958-1669(03)00023-5; Guyon I, 2002, MACH LEARN, V46, P389, DOI 10.1023/A:1012487302797; Hall P, 2005, J ROY STAT SOC B, V67, P427, DOI 10.1111/j.1467-9868.2005.00510.x; Pan W, 2002, BIOINFORMATICS, V18, P546, DOI 10.1093/bioinformatics/18.4.546; Lin Y, 2002, DATA MIN KNOWL DISC, V6, P259, DOI 10.1023/A:1015469627679; Benito M, 2004, BIOINFORMATICS, V20, P105, DOI 10.1093/bioinformatics/btg385; DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425; Lee KE, 2003, BIOINFORMATICS, V19, P90, DOI 10.1093/bioinformatics/19.1.90; Bae K, 2004, BIOINFORMATICS, V20, P3423, DOI 10.1093/bioinformatics/bth419; Boser E.B., 1992, P 5 ANN ACM WORKSH C, P144; Bradley P. S., 1998, P 15 INT C MACH LEAR, P82; CRISTIANINI N, 1994, INTRO SVM; Devore J, 1997, STAT EXPLORATION ANA; Fung GM, 2004, COMPUT OPTIM APPL, V28, P185, DOI 10.1023/B:COAP.0000026884.66338.df; Grandvalet Y., 2002, NEURAL INFORM PROCES, P553; Hastie T., 2001, ELEMENTS STAT LEARNI; He WQ, 2004, BIOINFORMATICS, V20, P2954, DOI 10.1093/bioinformatics/bth339; Kitter J., 1986, HDB PATTERN RECOGNIT; MARRON JS, 2004, IN PRESS J AM STAT A; More J.J, 1993, OPTIMIZATION SOFTWAR; MUKHERJEE S, 2000, 182 CBCL MIT; PARVLIDIS P, 2001, P 5 INT C COMP BIOL, P249; Rakotomamonjy A., 2003, Journal of Machine Learning Research, V3, DOI 10.1162/153244303322753706; Thomas JG, 2001, GENOME RES, V11, P1227, DOI 10.1101/gr.165101; van't Veer LJ, 2002, NATURE, V415, P530, DOI 10.1038/415530a; Vapnik V.N., 1995, NATURE STAT LEARNING; West M., 2003, BAYESIAN STAT, P723; Weston J, 2001, ADV NEUR IN, V13, P668; Zhu J, 2003, NEURAL INFORM PROCES, V16, P49	37	87	95	0	9	OXFORD UNIV PRESS	OXFORD	GREAT CLARENDON ST, OXFORD OX2 6DP, ENGLAND	1367-4803			BIOINFORMATICS	Bioinformatics	JAN 1	2006	22	1					88	95		10.1093/bioinformatics/bti736		8	Biochemical Research Methods; Biotechnology & Applied Microbiology; Computer Science, Interdisciplinary Applications; Mathematical & Computational Biology; Statistics & Probability	Biochemistry & Molecular Biology; Biotechnology & Applied Microbiology; Computer Science; Mathematical & Computational Biology; Mathematics	000AO	WOS:000234433500015	16249260	
B	Gao, JF; Suzuki, H; Yu, B			COLING	Gao, Jianfeng; Suzuki, Hisami; Yu, Bin			Approximation Lasso Methods for Language Modeling	COLING/ACL 2006, VOLS 1 AND 2, PROCEEDINGS OF THE CONFERENCE			English	Proceedings Paper	21st International Conference on Computational Linguistics/44th Annual Meeting of the Association for Computational Linguistics	JUL 17-21, 2006	Sydney, AUSTRALIA	Assoc Computat Linguist			REGRESSION	Lasso is a regularization method for parameter estimation in linear models. It optimizes the model parameters with respect to a loss function subject to model complexities. This paper explores the use of lasso for statistical language modeling for text input. Owing to the very large number of parameters, directly optimizing the penalized lasso loss function is impossible. Therefore, we investigate two approximation methods, the boosted lasso (BLasso) and the forward stagewise linear regression (FSLR). Both methods, when used with the exponential loss function, bear strong resemblance to the boosting algorithm which has been used as a discriminative training method for language modeling. Evaluations on the task of Japanese text input show that BLasso is able to produce the best approximation to the lasso solution, and leads to a significant improvement, in terms of character error rate, over boosting and the traditional maximum likelihood estimation.	[Gao, Jianfeng; Suzuki, Hisami] Microsoft Res, Redmond, WA 98052 USA	Gao, JF (reprint author), Microsoft Res, 1 Microsoft Way, Redmond, WA 98052 USA.	jfgao@microsoft.com; hisamis@microsoft.com; binyu@stat.berkeley.edu					Collins M, 2005, COMPUT LINGUIST, V31, P25, DOI 10.1162/0891201053630273; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Schapire RE, 1999, MACH LEARN, V37, P297, DOI 10.1023/A:1007614523901; Efron B, 2004, ANN STAT, V32, P407; BACCHIANI M, 2004, HLT NAACL, P21; DONOHO D, 1995, J ROY STAT SOC, V57, P201; Duda R. O., 2001, PATTERN CLASSIFICATI; FREUND Y, 1998, ICML 98; Gao J., 2002, EMNLP 2002; GAO J, 2005, HLT EMNLP 2005; Hastie T., 2001, ELEMENTS STAT LEARNI; Osborne MR, 2000, J COMPUT GRAPH STAT, V9, P319, DOI 10.2307/1390657; OSBORNE MR, 2000, J NUMERICAL ANAL, V20; ROARK B, 2004, ICASSP 2004; SUZUKI H, 2005, HLT EMNLP 2005; YUAN W, 2005, IJCNLP 05; Zhao P., 2004, BOOSTED LASSO; Zhu J., 2003, NIPS, V16	18	0	0	0	0	ASSOC COMPUTATIONAL LINGUISTICS-ACL	STROUDSBURG	209 N EIGHTH STREET, STROUDSBURG, PA 18360 USA			978-1-932432-65-7				2006							225	232				8	Computer Science, Artificial Intelligence; Computer Science, Theory & Methods	Computer Science	BNG36	WOS:000274500200029		
B	Plumbley, MD; Abdallah, SA; Blumensath, T; Jafari, MG; Nesbit, A; Vincent, E; Wang, BM		Rizzi, A; Vichi, M		Plumbley, Mark D.; Abdallah, Samer A.; Blumensath, Thomas; Jafari, Maria G.; Nesbit, Andrew; Vincent, Emmanuel; Wang, Beiming			Musical audio analysis using sparse representations	COMPSTAT 2006: PROCEEDINGS IN COMPUTATIONAL STATISTICS			English	Proceedings Paper	17th Symposium on Computational Statistics (COMSTAT 2006)	AUG 28-SEP 01, 2006	Rome, ITALY			sparse representations; music; audio; independent components analysis; automatic music transcription; non-negative matrix factorization; source separation	SOURCE SEPARATION; POLYPHONIC MUSIC; SHRINKAGE; BASES; CODE	Sparse representations are becoming an increasingly useful tool in the analysis of musical audio signals. In this paper we will given an overview of work by ourselves and others in this area, to give a flavour of the work being undertaken, and to give some pointers for further information about this interesting and challenging research topic.	Univ London Queen Mary & Westfield Coll, Dept Elect Engn, Ctr Digital Music, London E1 4NS, England	Plumbley, MD (reprint author), Univ London Queen Mary & Westfield Coll, Dept Elect Engn, Ctr Digital Music, Mile End Rd, London E1 4NS, England.	mark.plumbley@elec.qmul.ac.uk	Plumbley, Mark/A-7298-2008	Plumbley, Mark/0000-0002-9708-1075			Abdallah S, 2004, LECT NOTES COMPUT SC, V3195, P540; Abdallah S. A., 2004, P INT C MUS INF RETR, P318; Abdallah SA, 2006, IEEE T NEURAL NETWOR, V17, P179, DOI 10.1109/TNN.2005.861031; Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010; Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Bofill P, 2001, SIGNAL PROCESS, V81, P2353, DOI 10.1016/S0165-1684(01)00120-7; Gribonval R, 2003, IEEE T INFORM THEORY, V49, P3320, DOI 10.1109/TIT.2003.820031; Lee DD, 1999, NATURE, V401, P788; BELL AJ, 1995, NEURAL COMPUT, V7, P1129, DOI 10.1162/neco.1995.7.6.1129; Gorodnitsky IF, 1997, IEEE T SIGNAL PROCES, V45, P600, DOI 10.1109/78.558475; Lewicki MS, 2000, NEURAL COMPUT, V12, P337, DOI 10.1162/089976600300015826; Olshausen BA, 2004, CURR OPIN NEUROBIOL, V14, P481, DOI 10.1016/j.conb.2004.07.007; BELLO JP, 2003, THESIS U LONDON; Blumensath T., 2004, P IEEE INT C AC SPEE, V5, P497; Bregman A. S., 1990, AUDITORY SCENE ANAL; Ellis D. P. W., 1996, THESIS MIT; FIELD DJ, 1994, NEURAL COMPUT, V6, P559, DOI 10.1162/neco.1994.6.4.559; Figueiredo MAT, 2003, IEEE T PATTERN ANAL, V25, P1150, DOI 10.1109/TPAMI.2003.1227989; Fuchs JJ, 2004, IEEE T INFORM THEORY, V50, P1341, DOI 10.1109/TIT.2004.828141; Golub G. H., 1983, MATRIX COMPUTATIONS; Hoyer P.O., 2002, NEURAL NETWORKS SIGN, P557; Hyvarinen A, 1999, NEURAL COMPUT, V11, P1739, DOI 10.1162/089976699300016214; JAFARI MG, 2006, P 6 INT C IND COMP A, P132; JOURJINE A, 2000, P IEEE INT C AC SPEE, V5, P2985; Martin K. D., 1999, THESIS MIT; Kreutz-Delgado K, 2003, NEURAL COMPUT, V15, P349, DOI 10.1162/089976603762552951; NESBIT A, 2006, UNPUB SOURCE EXTRACT; O'Grady PD, 2005, INT J IMAG SYST TECH, V15, P18, DOI 10.1002/ima.20035; Plumbley MD, 2002, CYBERNET SYST, V33, P603, DOI 10.1080/01969720290040777; Plumbley MD, 2006, SIGNAL PROCESS, V86, P417, DOI 10.1016/j.sigpro.2005.06.007; Smaragdis P, 2004, LECT NOTES COMPUT SC, V3195, P494; VINCENT E, 2005, C4DMTR0501 QUEEN MAR; Virtanen T., 2004, P ISCA TUT RES WORKS; Wang B, 2005, P DMRN SUMM C GLASG	35	0	0	0	1	PHYSICA-VERLAG GMBH & CO	HEIDELBERG	TIERGARTENSTR 17, D-69121 HEIDELBERG, GERMANY			3-7908-1708-2				2006							105	117		10.1007/978-3-7908-1709-6_9		13	Computer Science, Artificial Intelligence; Statistics & Probability	Computer Science; Mathematics	BFJ07	WOS:000242170000009		
S	Buhlmann, P		Spiliopoulou, M; Kruse, R; Borgelt, C; Nurnberger, A; Gaul, W		Buhlmann, P			Boosting and e(1)-penalty methods for high-dimensional data with some applications in genomics	From Data and Information Analysis to Knowledge Engineering	STUDIES IN CLASSIFICATION, DATA ANALYSIS, AND KNOWLEDGE ORGANIZATION		English	Proceedings Paper	29th Annual Conference of the German-Classification-Society	MAR 09-11, 2005	Magdeburg, GERMANY	German Classificat Soc	Otto Guericke Univ Magdeburg		REGRESSION; CONSISTENCY; SELECTION	We consider Boosting and l(1)-penalty (regularization) methods for prediction and model selection (feature selection) and discuss some relations among the approaches. While Boosting has been originally proposed in the machine learning community (Freund and Schapire (1996)), l(1)-penalization has been developed in numerical analysis and statistics (Tibshirani (1996)). Both of the methods are attractive for very high-dimensional data: they are computationally feasible and statistically consistent (e.g. Bayes risk consistent) even when the number of covariates (predictor variables) p is much larger than sample size n and if the true underlying function (mechanism) is sparse: e.g. we allow for arbitrary polynomial growth p = p(n) = O(n(gamma)) for any gamma > 0. We demonstrate high-dimensional classification, regression and graphical modeling and outline examples from genomic applications.	ETH, Seminar Stat, CH-8092 Zurich, Switzerland	Buhlmann, P (reprint author), ETH, Seminar Stat, CH-8092 Zurich, Switzerland.		Buhlmann, Peter/A-2107-2013				Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451; Friedman J, 2000, ANN STAT, V28, P337, DOI 10.1214/aos/1016218223; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Jiang WX, 2004, ANN STAT, V32, P13; Zhang T, 2005, ANN STAT, V33, P1538, DOI 10.1214/009053605000000255; Breiman L, 1998, ANN STAT, V26, P801; MALLAT SG, 1993, IEEE T SIGNAL PROCES, V41, P3397, DOI 10.1109/78.258082; Buhlmann P, 2003, J AM STAT ASSOC, V98, P324, DOI 10.1198/016214503000125; Efron B, 2004, ANN STAT, V32, P407; BUHLMANN P, 2004, IN PRESS ANN STAT; Freund Y., 1996, Machine Learning. Proceedings of the Thirteenth International Conference (ICML '96); Greenshtein E, 2004, BERNOULLI, V10, P971, DOI 10.3150/bj/1106314846; MEINSHAUSEN N, 2004, IN PRESS ANN STAT; Tukey J. W., 1977, EXPLANATORY DATA ANA; Wille A., 2004, GENOME BIOL, V5, P1	15	0	0	0	2	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	1431-8814		3-540-31313-3	ST CLASS DAT ANAL			2006							1	12		10.1007/3-540-31314-1_1		12	Computer Science, Artificial Intelligence	Computer Science	BED68	WOS:000236886800001		
J	Bonneau, R; Reiss, DJ; Shannon, P; Facciotti, M; Hood, L; Baliga, NS; Thorsson, V				Bonneau, Richard; Reiss, David J.; Shannon, Paul; Facciotti, Marc; Hood, Leroy; Baliga, Nitin S.; Thorsson, Vesteinn			The Inferelator: an algorithm for learning parsimonious regulatory networks from systems-biology data sets de novo	GENOME BIOLOGY			English	Article							DIFFERENTIALLY-EXPRESSED GENES; ESCHERICHIA-COLI; MICROARRAY DATA; MODULES; PROTEINS; NRC-1; REPRESSOR; DISCOVERY; MODELS; LASSO	We present a method ( the Inferelator) for deriving genome-wide transcriptional regulatory interactions, and apply the method to predict a large portion of the regulatory network of the archaeon Halobacterium NRC-1. The Inferelator uses regression and variable selection to identify transcriptional influences on genes based on the integration of genome annotation and expression data. The learned network successfully predicted Halobacterium's global expression under novel perturbations with predictive power similar to that seen over training data. Several specific regulatory predictions were experimentally tested and verified.	NYU, Dept Biol, Ctr Comparat Funct Genom, New York, NY 10003 USA; NYU, Dept Comp Sci, Courant Inst, New York, NY 10003 USA; Inst Syst Biol, Seattle, WA 98103 USA	Bonneau, R (reprint author), NYU, Dept Biol, Ctr Comparat Funct Genom, New York, NY 10003 USA.	bonneau@cs.nyu.edu					Shen-Orr SS, 2002, NAT GENET, V31, P64, DOI 10.1038/ng881; Clementi C, 2000, J MOL BIOL, V298, P937, DOI 10.1006/jmbi.2000.3693; Barabasi AL, 1999, SCIENCE, V286, P509, DOI 10.1126/science.286.5439.509; Bar-Joseph Z, 2003, NAT BIOTECHNOL, V21, P1337, DOI 10.1038/nbt890; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Stuart JM, 2003, SCIENCE, V302, P249, DOI 10.1126/science.1087447; Tanay A, 2004, P NATL ACAD SCI USA, V101, P2981, DOI 10.1073/pnas.0308661100; Alm E, 2003, CURR OPIN STRUC BIOL, V13, P193, DOI 10.1016/S0959-440X(03)00031-9; Kanehisa M, 2002, NOVART FDN SYMP, V247, P91; Baliga NS, 2000, MOL MICROBIOL, V36, P1184, DOI 10.1046/j.1365-2958.2000.01916.x; Segal E, 2003, NAT GENET, V34, P166, DOI 10.1038/ng1165; Wuchty S, 2003, NAT GENET, V35, P176, DOI 10.1038/ng1242; Baliga NS, 2004, GENOME RES, V14, P1025, DOI 10.1101/gr.1993504; Friedman N, 2000, J COMPUT BIOL, V7, P601, DOI 10.1089/106652700750050961; Ideker T, 2000, J COMPUT BIOL, V7, P805, DOI 10.1089/10665270050514945; De Jong H, 2002, J COMPUT BIOL, V9, P67, DOI 10.1089/10665270252833208; Kluger Y, 2003, GENOME RES, V13, P703, DOI 10.1101/gr.648603; Dunn WB, 2005, ANALYST, V130, P606, DOI 10.1039/b418288j; Lee TI, 2002, SCIENCE, V298, P799, DOI 10.1126/science.1075090; Bateman A, 2004, NUCLEIC ACIDS RES, V32, pD138, DOI 10.1093/nar/gkh121; Baliga NS, 2002, P NATL ACAD SCI USA, V99, P14913, DOI 10.1073/pnas.192558999; Bernstein JA, 2002, P NATL ACAD SCI USA, V99, P9697, DOI 10.1073/pnas.112318199; Bonneau R, 2004, GENOME BIOL, V5, DOI 10.1186/gb-2004-5-8-r52; Bowers PM, 2004, GENOME BIOL, V5, DOI 10.1186/gb-2004-5-5-r35; Cheng Y, 2000, Proc Int Conf Intell Syst Mol Biol, V8, P93; D'haeseleer P, 1999, Pac Symp Biocomput, P41; Das D, 2004, P NATL ACAD SCI USA, V101, P16234, DOI 10.1073/pnas.0407365101; EFRON B, 2003, ANN STAT, V32, P407; Ettema TJG, 2003, TRENDS BIOCHEM SCI, V28, P170, DOI 10.1016/S0968-0004(03)00037-9; FRIEDMAN N, 2003, BIOINFORMATICS, P57; Gustafsson M, 2005, IEEE ACM T COMPUT BI, V2, P254, DOI 10.1109/TCBB.2005.35; Hashimoto RF, 2004, BIOINFORMATICS, V20, P1241, DOI 10.1093/bioinformatics/bth074; Hastie T., 2001, ELEMENTS STAT LEARNI; Herrgard MJ, 2004, CURR OPIN BIOTECH, V15, P70, DOI 10.1016/j.copbio.2003.11.002; Hill PJ, 1998, INFECT IMMUN, V66, P4123; Mellor JC, 2002, NUCLEIC ACIDS RES, V30, P306, DOI 10.1093/nar/30.1.306; Ng WV, 2000, P NATL ACAD SCI USA, V97, P12176, DOI 10.1073/pnas.190337797; Price MN, 2006, BMC BIOINFORMATICS, V7, DOI 10.1186/1471-2109-7-19; Que Q, 2000, MOL MICROBIOL, V35, P1454, DOI 10.1046/j.1365-2958.2000.01811.x; Segal E, 2001, Bioinformatics, V17 Suppl 1, pS243; SHANNON P, IN PRESS BMC BIOINFO; Sheng Q, 2003, BIOINFORMATICS S2, V19, pii196; Shmulevich I, 2004, PHYS REV LETT, V93, DOI 10.1103/PhysRevLett.93.048701; Shmulevich I, 2003, P NATL ACAD SCI USA, V100, P10734, DOI 10.1073/pnas.1534782100; Tanay Amos, 2002, Bioinformatics, V18 Suppl 1, pS136; THORSSON V, 2005, STAT APPL GENET MOL, V1; van Someren E P, 2000, Proc Int Conf Intell Syst Mol Biol, V8, P355; van Someren EP, 2002, PHARMACOGENOMICS, V3, P507; von Dassow G, 2000, NATURE, V406, P188, DOI 10.1038/35018085; Wahde M, 2001, J COMPUT BIOL, V8, P429, DOI 10.1089/106652701752236223; Weaver D. C., 1999, PAC S BIOCOMPUT, V4, P112; YANG J, 2002, 3 IEEE INT S BIOINF, P517; Yang J, 2003, THIRD IEEE SYMPOSIUM ON BIOINFORMATICS AND BIOENGINEERING - BIBE 2003, PROCEEDINGS, P321	53	184	190	2	8	BIOMED CENTRAL LTD	LONDON	MIDDLESEX HOUSE, 34-42 CLEVELAND ST, LONDON W1T 4LB, ENGLAND	1474-760X			GENOME BIOL	Genome Biol.		2006	7	5							R36	10.1186/gb-2006-7-5-r36		16	Biotechnology & Applied Microbiology; Genetics & Heredity	Biotechnology & Applied Microbiology; Genetics & Heredity	059DG	WOS:000238712900011	16686963	
J	Souverein, OW; Zwinderman, AH; Tanck, MWT				Souverein, OW; Zwinderman, AH; Tanck, MWT			Estimating haplotype effects on dichotomous outcome for unphased genotype data using a weighted penalized log-likelihood approach	HUMAN HEREDITY			English	Article						haplotype analysis; association study; penalty function; EM algorithm	ASSOCIATION ANALYSIS; UNRELATED INDIVIDUALS; MAXIMUM-LIKELIHOOD; CLADISTIC-ANALYSIS; CONTINUOUS TRAITS; EM ALGORITHM; POLYMORPHISM; INFERENCE; DISCRETE; GENE	Objective: To develop a method to estimate haplotype effects on dichotomous outcomes when phase is unknown, that can also estimate reliable effects of rare haplotypes. Methods: In short, the method uses a logistic regression approach, with weights attached to all possible haplotype combinations of an individual. An EM-algorithm was used: in the E-step the weights are estimated, and the M-step consists of maximizing the joint log-likelihood. When rare haplotypes were present, a penalty function was introduced. We compared four different penalties. To investigate statistical properties of our method, we performed a simulation study for different scenarios. The evaluation criteria are the mean bias of the parameter estimates, the root of the mean squared error, the coverage probability, power, Type I error rate and the false discovery rate. Results: For the unpenalized approach, mean bias was small, coverage probabilities were approximately 95%, power ranged from 15.2 to 44.7% depending on haplotype frequency, and Type I error rate was around 5%. All penalty functions reduced the standard errors of the rare haplotypes, but introduced bias. This trade off decreased power. Conclusion: The unpenalized weighted log-likelihood approach performs well. A penalty function can help to estimate an effect for rare haplotypes.	Univ Amsterdam, Acad Med Ctr, Dept Clin Epidemiol & Biostat, NL-1100 DE Amsterdam, Netherlands	Souverein, OW (reprint author), Univ Amsterdam, Acad Med Ctr, Dept Clin Epidemiol & Biostat, POB 22700, NL-1100 DE Amsterdam, Netherlands.	o.w.souverein@amc.uva.nl		Souverein, Olga/0000-0001-7549-6723			Zaykin DV, 2002, HUM HERED, V53, P79, DOI 10.1159/000057986; BREIMAN L, 1995, TECHNOMETRICS, V37, P373, DOI 10.2307/1269730; Durrant C, 2004, AM J HUM GENET, V75, P35, DOI 10.1086/422174; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1; Schaid DJ, 2002, AM J HUM GENET, V70, P425, DOI 10.1086/338688; Epstein MP, 2003, AM J HUM GENET, V73, P1316, DOI 10.1086/380204; EXCOFFIER L, 1995, MOL BIOL EVOL, V12, P921; WARM TA, 1989, PSYCHOMETRIKA, V54, P427, DOI 10.1007/BF02294627; GOLUB GH, 1979, TECHNOMETRICS, V21, P215, DOI 10.1080/00401706.1979.10489751; LECESSIE S, 1992, APPL STAT-J ROY ST C, V41, P191; Chiano MN, 1998, ANN HUM GENET, V62, P55, DOI 10.1046/j.1469-1809.1998.6210055.x; Fallin D, 2000, AM J HUM GENET, V67, P947, DOI 10.1086/303069; Jannot AS, 2004, HUM HERED, V58, P73, DOI 10.1159/000083028; Klerkx AHEM, 2003, HUM MOL GENET, V12, P111, DOI 10.1093/hmg/ddg013; Niu TH, 2004, GENET EPIDEMIOL, V27, P334, DOI 10.1002/gepi.20024; Niu TH, 2002, AM J HUM GENET, V70, P157, DOI 10.1086/338446; Schaid DJ, 2004, GENET EPIDEMIOL, V27, P348, DOI 10.1002/gepi.20037; Seltman H, 2003, GENET EPIDEMIOL, V25, P48, DOI 10.1002/gepi.10246; Sham PC, 2004, BEHAV GENET, V34, P207, DOI 10.1023/B:BEGE.0000013734.39266.a3; Stram DO, 2003, HUM HERED, V55, P179, DOI 10.1159/000073202; Tanck MWT, 2003, ANN HUM GENET, V67, P175, DOI 10.1046/j.1469-1809.2003.00021.x; TEMPLETON AR, 1987, GENETICS, V117, P343; Tregouet DA, 2004, ANN HUM GENET, V68, P165, DOI 10.1046/j.1529-8817.2003.00085.x; Yu K, 2004, GENET EPIDEMIOL, V27, P182, DOI 10.1002/gepi.20022	25	12	12	0	0	KARGER	BASEL	ALLSCHWILERSTRASSE 10, CH-4009 BASEL, SWITZERLAND	0001-5652			HUM HERED	Hum. Hered.		2006	61	2					104	110		10.1159/000093476		7	Genetics & Heredity	Genetics & Heredity	054DN	WOS:000238357100006	16717475	
S	Wang, L; Gordon, MD; Zhu, J		Clifton, CW; Zhong, N; Liu, JM; Wah, BW; Wu, XD		Wang, Li; Gordon, Michael D.; Zhu, Ji			Regularized least absolute deviations regression and an efficient algorithm for parameter tuning	ICDM 2006: Sixth International Conference on Data Mining, Proceedings	IEEE International Conference on Data Mining		English	Proceedings Paper	6th IEEE International Conference on Data Mining	DEC 18-22, 2006	Hong Kong, PEOPLES R CHINA	IEEE				Linear regression is one of the most important and widely used techniques for data analysis. However, sometimes people are not satisfied with it because of the following two limitations: 1) its results are sensitive to outliers, so when the error terms are not normally distributed, especially when they have heavy-tailed distributions, linear regression often works badly; 2) its estimated coefficients tend to have high variance, although their bias is low. To reduce the influence of outliers, robust regression models were developed. Least absolute deviation (LAD) regression is one of them. LAD minimizes the mean absolute errors, instead of mean squared errors, so its results are more robust. To address the second limitation, shrinkage methods were proposed, which add a penalty on the size of the coefficients. The LASSO is one of these methods and it uses the L1-norm penalty, which not only reduces the prediction error and the variance of estimated coefficients, but also provides an automatic feature selection function. In this paper, we propose the regularized least absolute deviation (RLAD) regression model, which combines the nice features of the LAD and the LASSO together. The RLAD is a regularization method, whose objective function has the form of "loss + penalty." The "loss" is the sum of the absolute deviations and the "penalty" is the L1-norm of the coefficient vector. Furthermore, to facilitate parameter tuning, we develop an efficient algorithm which can solve the entire regularization path in one pass. Simulations with various settings are performed to demonstrate its performance. Finally, we apply the algorithm to solve the image reconstruction problem and find interesting results.	Univ Michigan, Ross Sch Business, Ann Arbor, MI 48109 USA; Univ Michigan, Dept Stat, Ann Arbor, MI 48109 USA	Wang, L (reprint author), Univ Michigan, Ross Sch Business, Ann Arbor, MI 48109 USA.						Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; BLACK M, 1996, INT J COMPUT VISION, V25, P57; Giloni A, 2004, SIAM J OPTIMIZ, V14, P1028, DOI 10.1137/S1052623403424156; Giloni A, 2002, MATH COMPUT MODEL, V35, P361, DOI 10.1016/S0895-7177(01)00170-4; Hamza A. B., 2001, IEEE T SIGNAL PROCES, V49, P3045; HE XM, 1990, ECONOMETRICA, V58, P1195, DOI 10.2307/2938306; MIKA S, 1999, ADV NEURAL INFORM PR, V11, P537; MIZERA I, 2001, ADV MODEL ORIENTED D, P193; SHARPE WF, 1971, MANAGE SCI B-APPL, V18, pB1; TAKAHASHI T, 2002, ARTIFICIAL NEURAL NE, P727; TSUDA K, 2003, IMAGE RECONSTRUCTION; YUAN M, 2005, GACV QUANTILE SMOOTH	12	12	12	0	5	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA	1550-4786		978-0-7695-2701-7	IEEE DATA MINING			2006							690	700				11	Computer Science, Information Systems	Computer Science	BFZ37	WOS:000245601900068		
B	Wang, LF; Shen, XT; Zheng, YF		Wani, A; Li, T; Kurgan, L; Ye, J; Liu, Y		Wang, Lifeng; Shen, Xiaotong; Zheng, Yuan F.			On L1-norm multi-class Support Vector Machines	ICMLA 2006: 5th International Conference on Machine Learning and Applications, Proceedings			English	Proceedings Paper	5th International Conference on Machine Learning and Applications	DEC 14-16, 2006	Orlando, FL	Assoc Machine Learning & Applicat, Calif State Univ Bakersfield, Armstrong Atlantic State Univ, Florida Int Univ, IEEE, Univ Louisville			CLASSIFICATION; SELECTION	Binary Support Vector 'Machines (SVM) have proven effective in classification. However, problems remain with respect to feature selection in multi-class classification. This article proposes a novel multi-class SVM, which performs classification and feature selection simultaneously via L-1-norm penalized sparse representations. The proposed methodology, together with our developed regularization solution path, permits feature selection within the framework of classification. The operational characteristics of the proposed methodoloy is examined via both simulated and benchmark examples, and is compared to some competitors in terms of the accuracy of prediction and feature selection. The numerical results suggest that the proposed methodology is highly competitive.	Univ Minnesota, Sch Stat, Minneapolis, MN 55455 USA	Wang, LF (reprint author), Univ Minnesota, Sch Stat, Minneapolis, MN 55455 USA.						Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Dudoit S, 2002, J AM STAT ASSOC, V97, P77, DOI 10.1198/016214502753479248; Bertsimas D., 1997, INTRO LINEAR OPTIMIZ; Boser B, 1992, P 5 ANN WORKSH COMP, V5, P144, DOI DOI 10.1145/130385.130401; Bradley P. S., 1998, Machine Learning. Proceedings of the Fifteenth International Conference (ICML'98); Bredensteiner E.J., 1999, COMPUT OPTIM APPL, V12, P35; Cotes C., 1995, MACH LEARN, V20, P273; Guyon I., 2003, Journal of Machine Learning Research, V3, DOI 10.1162/153244303322753616; Hastie T, 2004, J MACH LEARN RES, V5, P1391; Lee YK, 2004, J AM STAT ASSOC, V99, P67, DOI 10.1198/016214504000000098; Liu Y, 2006, PATTERN RECOGN, V39, P1333, DOI 10.1016/j.patcog.2005.10.006; LIU Y, 2005, J AM STAT ASSOC, V101, P500; Szedmak S., 2004, PATT REC MACH LEARN; Vapnik V., 1998, STAT LEARNING THEORY; Weston J., 1999, P 7 EUR S ART NEUR N; Zhu J., 2003, NEURAL INFORM PROCES, V16	16	1	1	0	2	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA			978-0-7695-2735-2				2006							83	88				6	Computer Science, Artificial Intelligence	Computer Science	BFT06	WOS:000244477800013		
J	Donoho, DL; Elad, M; Temlyakov, VN				Donoho, DL; Elad, M; Temlyakov, VN			Stable recovery of sparse overcomplete representations in the presence of noise	IEEE TRANSACTIONS ON INFORMATION THEORY			English	Article						basis pursuit; greedy approximation; incoherent dictionary; Kruskal rank; matching pursuit; overcomplete representation; sparse representation; stability; stepwise regression; superresolution	GREEDY ALGORITHMS; BASES; APPROXIMATION; DICTIONARIES; RECONSTRUCTION; DECOMPOSITION; PURSUIT; ARRAYS; FRAMES; BOUNDS	Overcomplete representations are attracting interest in signal processing theory, particularly due to their potential to generate sparse representations of signals. However, in general, the problem of finding sparse representations must be unstable in the presence of noise. This paper establishes the possibility of stable recovery under a combination of sufficient sparsity and favorable structure of the overcomplete system. Considering an ideal underlying signal that has a sufficiently sparse representation, it is assumed that only a noisy version of it can be observed. Assuming further that the overcomplete system is incoherent, it is shown that the optimally sparse approximation to the noisy data differs from the optimally sparse decomposition of the ideal noiseless signal by at most a constant multiple of the noise level. As this optimal-sparsity method requires heavy (combinatorial) computational effort, approximation algorithms are considered. It is shown that similar stability is also available using the basis and the matching pursuit algorithms. Furthermore, it is shown that these methods result in sparse approximation of the noisy data that contains only terms also appearing in the unique sparsest representation of the ideal noiseless sparse signal.	Stanford Univ, Dept Stat, Stanford, CA 94305 USA; Technion Israel Inst Technol, Dept Comp Sci, IL-32000 Haifa, Israel; Univ S Carolina, Dept Math, Columbia, SC 29208 USA	Donoho, DL (reprint author), Stanford Univ, Dept Stat, Stanford, CA 94305 USA.	donoho@stanford.edu; elad@cs.tech-nion.ac.il; temlyak@math.sc.edu					KRUSKAL JB, 1977, LINEAR ALGEBRA APPL, V18, P95, DOI 10.1016/0024-3795(77)90069-6; Donoho DL, 2001, IEEE T INFORM THEORY, V47, P2845, DOI 10.1109/18.959265; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Liu XQ, 2001, IEEE T SIGNAL PROCES, V49, P2074; Gribonval R, 2003, IEEE T INFORM THEORY, V49, P3320, DOI 10.1109/TIT.2003.820031; Chen SSB, 2001, SIAM REV, V43, P129, DOI 10.1137/S003614450037906X; MALLAT SG, 1993, IEEE T SIGNAL PROCES, V41, P3397, DOI 10.1109/78.258082; Strohmer T, 2003, APPL COMPUT HARMON A, V14, P257, DOI 10.1016/S1063-5203(03)00023-X; Tropp JA, 2004, IEEE T INFORM THEORY, V50, P2231, DOI 10.1109/TIT.2004.834793; Gilbert AC, 2003, SIAM PROC S, P243; Elad M, 2002, IEEE T INFORM THEORY, V48, P2558, DOI 10.1109/TIT.2002.801410; BERG AP, 1999, P 1999 IEEE INT S CI, V4, P106; Candes EJ, 2004, COMMUN PUR APPL MATH, V57, P219, DOI 10.1002/cpa.10116; Chen S., 1995, THESIS STANFORD U ST; Coifman R. R., 1992, P ICIAM 91, P41; Cotter SF, 2002, IEEE T COMMUN, V50, P374, DOI 10.1109/26.990897; Daniel C, 1980, FITTING EQUATIONS DA; DeBrunner VE, 1997, IEEE T IMAGE PROCESS, V6, P1316, DOI 10.1109/83.623194; DEVORE RA, 1996, ADV COMPUT MATH, V12, P213; Donoho DL, 2003, P NATL ACAD SCI USA, V100, P2197, DOI 10.1073/pnas.0437847100; DONOHO DL, 1992, SIAM J MATH ANAL, V23, P1309, DOI 10.1137/0523074; DONOHO DL, 2004, STABLE RECOVERY SPAR; FUCHS JJ, UNPUB IEEE T INF THE; Fuchs JJ, 2004, IEEE T INFORM THEORY, V50, P1341, DOI 10.1109/TIT.2004.828141; Gorodetsky VR, 2000, GEMATOL TRANSFUZIOL, V45, P3; Hastie T., 2001, ELEMENTS STAT LEARNI; Horn R. A., 1985, MATRIX ANAL; HUO X, 1999, SPARSE REPRESENTATIO; Karlovitz L. A., 1970, J APPROXIMATION THEO, V37, P123, DOI 10.1016/0021-9045(70)90019-5; Kolda TG, 2001, SIAM J MATRIX ANAL A, V23, P243, DOI 10.1137/S0895479800368354; Olshausen B., 1997, VISION RES, V37, P311; Pati Y. C., 1993, P 27 AS C SIGN SYST, V1, P40, DOI DOI 10.1109/ACSSC.1993.342465; QIAN S, 1994, SIGNAL PROCESS, V36, P1, DOI 10.1016/0165-1684(94)90174-0; Starck J. L., 2002, IEEE T IMAGE PROCESS, V11, P131; STARCK JL, 2003, SPIE M SAN DIEG CA A; Temlyakov V. N., 2000, ADV COMPUTATIONAL MA, V5, P173; Temlyakov VN, 1999, J APPROX THEORY, V98, P117, DOI 10.1006/jath.1998.3265; Temlyakov VN, 2000, CONSTR APPROX, V16, P399, DOI 10.1007/s003659910017; Temlyakov VN, 2003, FOUND COMPUT MATH, V3, P33, DOI 10.1007/s102080010029; TROPP JA, UNPUB IEEE T INF THE; Venkataramani R, 2001, IEEE T SIGNAL PROCES, V49, P2301, DOI 10.1109/78.950786; Wickerhauser M. V., 1994, ADAPTED WAVELET ANAL; Wohlberg B, 2003, IEEE T SIGNAL PROCES, V51, P3053, DOI 10.1109/TSP.2003.819006	43	698	766	12	48	IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC	PISCATAWAY	445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA	0018-9448			IEEE T INFORM THEORY	IEEE Trans. Inf. Theory	JAN	2006	52	1					6	18		10.1109/TIT.2005.860430		13	Computer Science, Information Systems; Engineering, Electrical & Electronic	Computer Science; Engineering	999TD	WOS:000234412500001		
S	Zhang, K; Chan, LW		Corchado, E; Yin, H; Botti, V; Fyfe, C		Zhang, Kun; Chan, Lai-Wan			ICA with sparse connections	INTELLIGENT DATA ENGINEERING AND AUTOMATED LEARNING - IDEAL 2006, PROCEEDINGS	LECTURE NOTES IN COMPUTER SCIENCE		English	Article; Proceedings Paper	7th International Conference on Intelligent Data Engineering and Automated Learning (IDEAL 2006)	SEP 20-23, 2006	Burgos, SPAIN		Univ Burgos		LIKELIHOOD; SELECTION	When applying independent component analysis (ICA), sometimes that the connections between the observed mixtures and the recovered independent components (or the original sources) to be sparse, to make the interpretation easier or to reduce the model complexity. In this paper we propose natural gradient algorithms for ICA with a sparse separation matrix, as well as ICA with a sparse mixing matrix. The sparsity of the matrix is achieved by applying certain penalty functions to its entries. The properties of the penalty functions are investigated. Experimental results on both artificial data and causality discovery in financial stocks show the usefulness of the proposed methods.	Chinese Univ Hong Kong, Dept Comp Sci & Engn, Shatin, Hong Kong, Peoples R China	Zhang, K (reprint author), Chinese Univ Hong Kong, Dept Comp Sci & Engn, Shatin, Hong Kong, Peoples R China.	kzhang@cse.cuhk.edu.hk; lwchan@cse.cuhk.edu.hk					AMARI S, 1996, ADV NEURAL INFORMATI; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; Amari S, 1998, NEURAL COMPUT, V10, P251, DOI 10.1162/089976698300017746; Hyvarinen A., 2001, INDEPENDENT COMPONEN; Lehmann E. L., 1983, THEORY POINT ESTIMAT; Lewicki MS, 1998, ADV NEUR IN, V10, P815; Pham DT, 1997, IEEE T SIGNAL PROCES, V45, P1712; SHIMIZU S, 2006, UNPUB J MACHINE LEAR; WEIGEND AS, 1991, ADV NEURAL INFORMATI, V3	10	1	1	0	2	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		3-540-45485-3	LECT NOTES COMPUT SC			2006	4224						530	537				8	Computer Science, Artificial Intelligence; Computer Science, Theory & Methods	Computer Science	BFG81	WOS:000241790900064		
S	Young, N; Yang, ZR		Corchado, E; Yin, H; Botti, V; Fyfe, C		Young, Natasha; Yang, Zheng Rong			Multivariate crosstalk models	INTELLIGENT DATA ENGINEERING AND AUTOMATED LEARNING - IDEAL 2006, PROCEEDINGS	LECTURE NOTES IN COMPUTER SCIENCE		English	Article; Proceedings Paper	7th International Conference on Intelligent Data Engineering and Automated Learning (IDEAL 2006)	SEP 20-23, 2006	Burgos, SPAIN		Univ Burgos	multivariate models; crosstalk; systems biology	PROTEIN-KINASE CASCADE	Since 1960s unexpected communication activity between signaling pathways and signaling molecules in cells has been very often observed. As there is no biological theory to interpret it, this activity has been termed as crosstalk, unwanted communication. So far, no computer or statistical models have been developed for modeling crosstalk between signaling proteins although studying crosstalk between signaling pathways in wet laboratory has been one of the main stream. As the first attempt in the world, we have investigated multivariate crosstalk models. The simulation shows that such statistical crosstalk models work very well although more investigations are needed.	Univ Exeter, Sch Engn Comp Sci & Math, Exeter EX4 4QJ, Devon, England	Young, N (reprint author), Univ Exeter, Sch Engn Comp Sci & Math, Exeter EX4 4QJ, Devon, England.	N.Young@ex.ac.uk					Alberts B, 2002, MOL BIOL CELL, P831; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Lu ZM, 2002, MOL CELL, V9, P945, DOI 10.1016/S1097-2765(02)00519-1; Huang CYF, 1996, P NATL ACAD SCI USA, V93, P10078, DOI 10.1073/pnas.93.19.10078; Duda R. O., 2002, PATTERN CLASSIFICATI; Ferrell JE, 1996, TRENDS BIOCHEM SCI, V21, P460, DOI 10.1016/S0968-0004(96)20026-X; FRUMAN DA, 2004, CURRENT OPINION IMMU, V16; HOTOKEZAKA H, 2002, J BIOL CHEM, V277; Kolch W, 2000, BIOCHEM J, V351, P289, DOI 10.1042/0264-6021:3510289; LANGFORD D, 2005, BMC NEUROSCIENCE, V6; LAY D, 2003, LINEAR ALGEBRA ITS A; Sachs K, 2005, SCIENCE, V308, P523, DOI 10.1126/science.1105809; TAYLOR JE, 2003, J EXP BOT, V55, P147, DOI 10.1093/jxb/erh060; Weng GZ, 1999, SCIENCE, V284, P92, DOI 10.1126/science.284.5411.92; Zheng M, 2000, J BIOL CHEM, V275, P40635, DOI 10.1074/jbc.M006325200	15	0	0	0	12	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		3-540-45485-3	LECT NOTES COMPUT SC			2006	4224						1129	1136				8	Computer Science, Artificial Intelligence; Computer Science, Theory & Methods	Computer Science	BFG81	WOS:000241790900134		
J	Hannachi, A; Jolliffe, IT; Stephenson, DB; Trendafilov, N				Hannachi, A; Jolliffe, IT; Stephenson, DB; Trendafilov, N			In search of simple structures in climate: Simplifying EOFs	INTERNATIONAL JOURNAL OF CLIMATOLOGY			English	Article						principal components; simplified empirical orthogonal functions ordinary differential equations; gradient methods; North Atlantic Oscillation; Arctic Oscillation	ROSSBY-WAVE PROPAGATION; PRINCIPAL COMPONENTS; UNCONSTRAINED MINIMIZATION; CONSTRAINED OPTIMIZATION; TELECONNECTION PATTERNS; CIRCULATION PATTERNS; ARCTIC OSCILLATION; CAUTIONARY NOTE; ROTATION; CLASSIFICATION	Empirical orthogonal functions (EOFs) are widely used in climate research to identify dominant patterns of variability and to reduce the dimensionality of climate data. EOFs, however, can be difficult to interpret. Rotated empirical orthogonal functions (REOFs) have been proposed as more physical entities with simpler patterns than EOFs. This study presents a new approach for finding climate patterns with simple structures that overcomes the problems encountered with rotation. The method achieves simplicity of the patterns by using the main properties of EOFs and REOFs simultaneously. Orthogonal patterns that maximise variance subject to a constraint that induces a form of simplicity are found. The simplified empirical orthogonal function (SEOF) patterns, being more 'local'. are constrained to have zero loadings outside the main centre of action. The method is applied to winter Northern Hemisphere (NH) monthly mean sea level pressure (SLP) reanalyses over the period 1948-2000. The 'simplified' leading patterns of variability are identified and compared to the leading patterns obtained from EOFs and REOFs. Copyright (C) 2005 Royal Meteorological Society.	Univ Reading, Dept Meteorol, Reading RG6 6BB, Berks, England; Open Univ, Fac Math & Comp, Dept Stat, Milton Keynes MK7 6AA, Bucks, England	Hannachi, A (reprint author), Univ Reading, Dept Meteorol, Earley Gate,POB 243, Reading RG6 6BB, Berks, England.	a.Hannachi@reading.ac.uk	Stephenson, David/A-9903-2011				Ambaum MHP, 2002, J CLIMATE, V15, P553, DOI 10.1175/1520-0442(2002)015<0553:C>2.0.CO;2; Ambaum MHP, 2001, J CLIMATE, V14, P3495, DOI 10.1175/1520-0442(2001)014<3495:AOONAO>2.0.CO;2; KAISER HF, 1958, PSYCHOMETRIKA, V23, P187, DOI 10.1007/BF02289233; HOSKINS BJ, 1981, J ATMOS SCI, V38, P1179, DOI 10.1175/1520-0469(1981)038<1179:TSLROA>2.0.CO;2; LIN S, 1973, OPER RES, V21, P498, DOI 10.1287/opre.21.2.498; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; HOSKINS BJ, 1993, J ATMOS SCI, V50, P1661, DOI 10.1175/1520-0469(1993)050<1661:RWPOAR>2.0.CO;2; KILLWORTH PD, 1985, J FLUID MECH, V161, P449, DOI 10.1017/S0022112085003019; Kalnay E, 1996, B AM METEOROL SOC, V77, P437, DOI 10.1175/1520-0477(1996)077<0437:TNYRP>2.0.CO;2; Kistler R, 2001, B AM METEOROL SOC, V82, P247, DOI 10.1175/1520-0477(2001)082<0247:TNNYRM>2.3.CO;2; Jolliffe IT, 2003, J COMPUT GRAPH STAT, V12, P531, DOI 10.1198/1061860032148; Thompson DWJ, 2000, J CLIMATE, V13, P1000, DOI 10.1175/1520-0442(2000)013<1000:AMITEC>2.0.CO;2; AMBRIZZI T, 1995, J ATMOS SCI, V52, P3661, DOI 10.1175/1520-0469(1995)052<3661:RWPATP>2.0.CO;2; BARNSTON AG, 1987, MON WEATHER REV, V115, P1083, DOI 10.1175/1520-0493(1987)115<1083:CSAPOL>2.0.CO;2; BOTSARIS CA, 1976, J MATH ANAL APPL, V54, P217, DOI 10.1016/0022-247X(76)90246-8; BOTSARIS CA, 1978, J MATH ANAL APPL, V63, P729, DOI 10.1016/0022-247X(78)90068-9; BOTSARIS CAE, 1979, J MATH ANAL APPL, V69, P372, DOI 10.1016/0022-247X(79)90150-1; BOTSARIS CA, 1981, J MATH ANAL APPL, V79, P295, DOI 10.1016/0022-247X(81)90026-3; BROWN AA, 1986, THESIS HATFIELD POLY; CARROLL JB, 1953, PSYCHOMETRIKA, V18, P23; CHENG XH, 1995, J CLIMATE, V8, P1709, DOI 10.1175/1520-0442(1995)008<1709:ROLFCP>2.0.CO;2; Dommenget D, 2002, J CLIMATE, V15, P216, DOI 10.1175/1520-0442(2002)015<0216:ACNOTI>2.0.CO;2; EVTUSHENKO JG, 1977, USSR COMPUTATIONAL G, V17, P73; EVTUSHENKO JG, 1974, SOV MATH, V15, P420; Fukuoka A, 1951, GEOPHYS MAG, V22, P177; GILL PE, 1981, PRRACTICAL OPTIMIZAT; Golub G H, 1996, MATRIX COMPUTATION; Hannachi A, 2001, Q J ROY METEOR SOC, V127, P939, DOI 10.1256/smsqj.57311; HANNACHI A, 1995, TELLUS A, V47, P955, DOI 10.1034/j.1600-0870.1995.00203.x; Harman H. H., 1976, MODERN FACTOR ANAL; Held I. M., 1983, LARGE SCALE DYNAMICA, P127; Hirsch M.W., 1974, DIFFERENTIAL EQUATIO; HOREL JD, 1981, MON WEATHER REV, V109, P2080, DOI 10.1175/1520-0493(1981)109<2080:ARPCAO>2.0.CO;2; JOLLIFFE IT, 1987, J CLIMATOL, V7, P507; Jolliffe IT, 2002, CLIMATE RES, V20, P271, DOI 10.3354/cr020271; JOLLIFFE IT, 1995, J APPL STAT, V22, P29, DOI 10.1080/757584395; Jolliffe IT, 2003, J CLIMATE, V16, P1084, DOI 10.1175/1520-0442(2003)016<1084:ACNOAE>2.0.CO;2; KIRKPATRICK S, 1983, SCIENCE, V220, P671, DOI 10.1126/science.220.4598.671; Krzanowski W.J., 1995, MULTIVARIATE ANAL 2; Kutzbach J. E., 1967, J APPL METEOROL, V6, P791, DOI 10.1175/1520-0450(1967)006<0791:EEOSLP>2.0.CO;2; LORENZ EN, 1956, EMPIRICAL ORTHOGONAL, P49; Magnus J. R., 1995, MATRIX DIFFERENTIAL; Mardia KV, 1979, MULTIVARIATE ANAL; Mestas-Nunez AM, 2000, INT J CLIMATOL, V20, P1509, DOI 10.1002/1097-0088(200010)20:12<1509::AID-JOC553>3.0.CO;2-Q; METROPOLIS N, 1953, J CHEM PHYS, V21, P1087, DOI 10.1063/1.1699114; NIGAM S, 1983, J ATMOS SCI, V40, P2610, DOI 10.1175/1520-0469(1983)040<2610:TIOACL>2.0.CO;2; NORTH GR, 1982, MON WEATHER REV, V110, P699, DOI 10.1175/1520-0493(1982)110<0699:SEITEO>2.0.CO;2; Obukhov A. M., 1960, B ACAD SCI USSR GEOP, V1, P288; Obukhov A. M., 1947, USP MAT NAUK, V2, P196; Preisendorfer R. W., 1988, PRINCIPAL COMPONENT; RICHMAN MB, 1987, J CLIMATOL, V7, P511; RICHMAN MB, 1981, J APPL METEOROL, V20, P1145, DOI 10.1175/1520-0450(1981)020<1145:ORPCAI>2.0.CO;2; RICHMAN MB, 1986, J CLIMATOL, V6, P293; SIMMONS AJ, 1983, J ATMOS SCI, V40, P1363, DOI 10.1175/1520-0469(1983)040<1363:BWPAIA>2.0.CO;2; SNYMAN JA, 1982, APPL MATH MODEL, V6, P449, DOI 10.1016/S0307-904X(82)80007-3; Stephenson DB, 2004, Q J ROY METEOR SOC, V130, P583, DOI 10.1256/qj.02.146; Thompson DWJ, 1998, GEOPHYS RES LETT, V25, P1297, DOI 10.1029/98GL00950; TRENDAFILOV NT, 2005, COMPUATIONAL STAT DA, V50, P242; von Storch H., 1999, STAT ANAL CLIMATE RE; Wallace JM, 2002, J CLIMATE, V15, P1987, DOI 10.1175/1520-0442(2002)015<1987:TPCOAO>2.0.CO;2; Wallace JM, 2000, Q J ROY METEOR SOC, V126, P791, DOI 10.1256/smsqj.56401; WARD MN, 1991, INT J CLIMATOL, V11, P711; Wilks D. S., 1995, STAT METHODS ATMOSPH	63	22	22	1	7	JOHN WILEY & SONS LTD	CHICHESTER	THE ATRIUM, SOUTHERN GATE, CHICHESTER PO19 8SQ, W SUSSEX, ENGLAND	0899-8418			INT J CLIMATOL	Int. J. Climatol.	JAN	2006	26	1					7	28		10.1002/joc.1243		22	Meteorology & Atmospheric Sciences	Meteorology & Atmospheric Sciences	009HD	WOS:000235101500002		
J	Ding, C; He, XF; Xiong, H; Peng, HC; Holbrook, SR				Ding, Chris; He, Xiaofeng; Xiong, Hui; Peng, Hanchuan; Holbrook, Stephen R.			Transitive closure and metric inequality of weighted graphs: detecting protein interaction modules using cliques	INTERNATIONAL JOURNAL OF DATA MINING AND BIOINFORMATICS			English	Article						transitive closure; ultrametric; triangle inequality; weighted graph; protein interaction	SACCHAROMYCES-CEREVISIAE; COMPLEXES; NETWORKS	We study transitivity properties of edge weights in complex networks. We show that enforcing transitivity leads to a transitivity inequality which is equivalent to ultra-metric inequality. This can be used to define transitive closure on weighted undirected graphs, which can be computed using a modified Floyd-Warshall algorithm. These new concepts are extended to dissimilarity graphs and triangle inequalities. From this, we extend the clique concept from unweighted graph to weighted graph. We outline several applications and present results of detecting protein functional modules in a protein interaction network.	Univ Calif Berkeley, Lawrence Berkeley Lab, Berkeley, CA 94720 USA	Ding, C (reprint author), Univ Calif Berkeley, Lawrence Berkeley Lab, Berkeley, CA 94720 USA.	chqding@lbl.gov; xhe@lbl.gov; hui@rbs.rutgers.edu; pengh@janella.hhmi.org; srholbrook@lbl.gov	Peng, Hanchuan/A-1798-2011				Ito T, 2001, P NATL ACAD SCI USA, V98, P4569, DOI 10.1073/pnas.061034498; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Bader GD, 2002, NAT BIOTECHNOL, V20, P991, DOI 10.1038/nbt1002-991; Watts DJ, 1998, NATURE, V393, P440, DOI 10.1038/30918; Gavin AC, 2002, NATURE, V415, P141, DOI 10.1038/415141a; Uetz P, 2000, NATURE, V403, P623; Ho Y, 2002, NATURE, V415, P180, DOI 10.1038/415180a; Ding C, 2004, PROTEINS, V57, P99, DOI 10.1002/prot.20147; Hastie T., 2001, ELEMENTS STAT LEARNI; MOTZKIN TS, 1965, CANADIAN J MATH, V17, P533, DOI 10.4153/CJM-1965-053-6; Pelillo M, 1999, IEEE T PATTERN ANAL, V21, P1105, DOI 10.1109/34.809105; Wasserman S., 1994, SOCIAL NETWORK ANAL; Xiong H, 2005, PACIFIC SYMPOSIUM ON BIOCOMPUTING 2005, P221	13	5	5	1	2	INDERSCIENCE ENTERPRISES LTD	GENEVA	WORLD TRADE CENTER BLDG, 29 ROUTE DE PRE-BOIS, CASE POSTALE 896, CH-1215 GENEVA, SWITZERLAND	1748-5673			INT J DATA MIN BIOIN	Int. J. Data Min. Bioinform.		2006	1	2					162	177		10.1504/IJDMB.2006.010854		16	Mathematical & Computational Biology	Mathematical & Computational Biology	185UH	WOS:000247735400004	18399069	
J	Basak, SC; Natarajan, R; Mills, D; Hawkins, DM; Kraker, JJ				Basak, SC; Natarajan, R; Mills, D; Hawkins, DM; Kraker, JJ			Quantitative structure-activity relationship modeling of juvenile hormone mimetic compounds for Culex pipiens larvae, with a discussion of descriptor-thinning methods	JOURNAL OF CHEMICAL INFORMATION AND MODELING			English	Article; Proceedings Paper	4th Indo-US Workshop on Mathematical Chemistry	JAN 08-12, 2005	Pune, INDIA				CHEMOMETRICS REGRESSION TOOLS; AIR PARTITION-COEFFICIENTS; PAIR-CORRELATION METHOD; PROPERTY-BASED METHODS; VARIABLE SELECTION; RIDGE-REGRESSION; NONORTHOGONAL PROBLEMS; STATISTICAL VIEW; PREDICTION; ETHERS	Quantitative structure-activity relationship (QSAR) modelers often encounter the problem of multicollinearity owing to the availability of large numbers of computable molecular descriptors. Sparsity of the variables while using descriptors such as atom pairs increases the complexity. Three different predictor-thinning methods, namely, a modified Gram-Schmidt algorithm, a marginal soft thresholding algorithm, and LASSO (least absolute shrinkage and selection operator), were utilized to reduce the number of descriptors prior to developing linear models. Juvenile hormone (JH) activity of 304 compounds on Culex pipiens larvae was taken as the model data set, and predictor trimming of a large number of diverse descriptors comprising 268 global molecular descriptors (topostructural, topochemical, and geometrical), 13 quantum chemical descriptors, and 915 atom pairs (substructural counts) was applied prior to linear regression by the ridge regression method. The data set (N = 304) was split into five calibration data sets of random samples of sizes 60/110/160/210/260, and the remaining 244/194/144/94/44 compounds were used for validations. LASSO was not found to be a very effective method in handling a large set of descriptors because the number of predictors retained could not exceed the number of observations. The results indicated that the modified Gram-Schmidt algorithm could be used to trim the number of predictors in the global molecular descriptor set where collinearity of the descriptors was the major concern. On the contrary, the soft thresholding approach was found to be an effective tool in subset selection from a diverse set of descriptors having both sparsity and multicollinearity, as in the case of the combined set of atom pairs and global molecular descriptors. The final model developed after variable selection was dominated more by atom pairs, which indicated the important structural moieties that affect JH activity of the compounds. The success of the method reiterates the fact that QSAR or quantitative structure-property relationship (QSPR) models can be developed for a diverse set of compounds using properly parametrized and diverse sets of descriptors, of course, with the selection of the appropriate statistical tools.	Univ Minnesota, Nat Resources Res Inst, Ctr Water & Environm, Duluth, MN 55811 USA; Univ Minnesota, Sch Stat, Minneapolis, MN 55455 USA	Basak, SC (reprint author), Univ Minnesota, Nat Resources Res Inst, Ctr Water & Environm, 5013 Miller Trunk Hwy, Duluth, MN 55811 USA.	sbasak@nrri.umn.edu	Natarajan, Ramanathan/A-5851-2008				RANDIC M, 1975, J AM CHEM SOC, V97, P6609, DOI 10.1021/ja00856a001; Basak SC, 2003, RISK ANAL, V23, P1173, DOI 10.1111/j.0272-4332.2003.00390.x; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Karelson M, 1996, CHEM REV, V96, P1027, DOI 10.1021/cr950202r; Hawkins DM, 2004, J CHEM INF COMP SCI, V44, P1, DOI 10.1021/ci0342472; MASSY WF, 1965, J AM STAT ASSOC, V60, P234, DOI 10.2307/2283149; HOERL AE, 1970, TECHNOMETRICS, V12, P55; FRANK IE, 1993, TECHNOMETRICS, V35, P109, DOI 10.2307/1269656; CARHART RE, 1985, J CHEM INF COMP SCI, V25, P64, DOI 10.1021/ci00046a002; SHAO J, 1993, J AM STAT ASSOC, V88, P486, DOI 10.2307/2290328; WIENER H, 1947, J AM CHEM SOC, V69, P17, DOI 10.1021/ja01193a005; BALABAN AT, 1982, CHEM PHYS LETT, V89, P399, DOI 10.1016/0009-2614(82)80009-2; Basak SC, 1979, P 2 INT C MATH MOD, P851; Basak SC, 2001, MATCH-COMMUN MATH CO, P15; Basak SC, 2004, ENVIRON TOXICOL PHAR, V16, P45, DOI 10.1016/j.etap.2003.09.002; Basak SC, 2002, SAR QSAR ENVIRON RES, V13, P649, DOI 10.1080/1062936021000043409; Basak S. C., 2003, QUANTITATIVE STRUCTU, P207; Basak SC, 2001, J CHEM INF COMP SCI, V41, P692, DOI 10.1021/ci000165r; BASAK SC, 1988, POLLY VERSION 2 3; BASAK SC, 2005, UNPUB ARKIVOC; BONCHEV D, 1977, J CHEM PHYS, V67, P4517, DOI 10.1063/1.434593; *CAMBRIDGESOFT COR, 2000, CHEM3D ULTR VERS 8; Devillers J., 1999, TOPOLOGICAL INDICES; Draper NR, 1981, APPL REGRESSION ANAL, V2nd, P294; Filip PA, 1987, J MATH CHEM, V1, P61, DOI 10.1007/BF01205338; GENTLEMAN R, COMPREHENSIVE R ARCH; Gute B D, 1997, SAR QSAR Environ Res, V7, P117, DOI 10.1080/10629369708039127; *HALL ASS CONS, 2000, MALC Z VERS 3 5; Hansch C., 1978, CORRELATION ANAL CHE, P397; HAWKINS DM, LINMODS PROGRAM; HAYASHI T, 1990, J AGR FOOD CHEM, V38, P1965, DOI 10.1021/jf00100a019; HAYASHI T, 1991, J AGR FOOD CHEM, V39, P2039, DOI 10.1021/jf00011a032; HAYASHI T, 1991, J AGR FOOD CHEM, V39, P2029, DOI 10.1021/jf00011a031; Heberger K, 2002, SAR QSAR ENVIRON RES, V13, P541, DOI 10.1080/10629360290023368; Heberger K, 2002, J CHEMOMETR, V16, P436, DOI 10.1002/cem.748; Heberger K, 2004, CROAT CHEM ACTA, V77, P117; HOERL AE, 1970, TECHNOMETRICS, V12, P69, DOI 10.2307/1267352; KARCHER W, 1997, PRACTICAL APPL QUANT; Kier L. B., 1999, MOL STRUCTURE DESCRI; Kier L.B., 1986, MOL CONNECTIVITY STR; Kier LB, 1999, TOPOLOGICAL INDICES, P455; LIU J, 2005, POWERMV VERSION 0 61; Miller A.J., 1990, SUBSET SELECTION REG, P43; NIWA A, 1989, J AGR FOOD CHEM, V37, P462, DOI 10.1021/jf00086a042; NIWA A, 1989, J AGR FOOD CHEM, V37, P467; NIWA A, 1989, J AGR FOOD CHEM, V37, P378; NIWA A, 1990, J AGR FOOD CHEM, V38, P514, DOI 10.1021/jf00092a040; RENCHER AC, 1980, TECHNOMETRICS, V22, P49, DOI 10.2307/1268382; SAS Institute Inc, 1998, SAS STAT US GUID REL; Stanton DT, 2003, J CHEM INF COMP SCI, V43, P1423, DOI 10.1021/ci0340658; Thisted R. A., 1988, ELEMENTS STAT COMPUT; Todeschini R., 2000, HDB MOL DESCRIPTORS, V11; *U MINN, 1993, APPROBE; WALKER E, 1988, TECHNOMETRICS, V30, P221, DOI 10.2307/1270168; WOLD S, 1993, TECHNOMETRICS, V35, P136, DOI 10.2307/1269657	55	29	30	2	11	AMER CHEMICAL SOC	WASHINGTON	1155 16TH ST, NW, WASHINGTON, DC 20036 USA	1549-9596			J CHEM INF MODEL	J. Chem Inf. Model.	JAN-FEB	2006	46	1					65	77		10.1021/ci050215y		13	Chemistry, Medicinal; Chemistry, Multidisciplinary; Computer Science, Information Systems; Computer Science, Interdisciplinary Applications	Pharmacology & Pharmacy; Chemistry; Computer Science	008DU	WOS:000235021200010	16426041	
J	Zhang, M; Zhu, J; Djurdjanovic, D; Ni, J				Zhang, Min; Zhu, Ji; Djurdjanovic, Dragan; Ni, Jun			A comparative study on the classification of engineering surfaces with dimension reduction and coefficient shrinkage methods	JOURNAL OF MANUFACTURING SYSTEMS			English	Article						surface classification; dimension reduction; coefficient shrinkage; comparative study	NONORTHOGONAL PROBLEMS; TRIBOLOGICAL SURFACES; RIDGE REGRESSION; SELECTION; LASSO	As more automated and accurate surface inspection devices enter the manufacturing process, engineers collect a larger amount of surface inspection data, in terms of storage space and the number of parameters to characterize the surface, but sometimes smaller in terms of the number of coherent surface observations. In these cases, more features are preferable to characterize engineering surfaces for capturing the details of the surface finish patterns. When the number of surface parameters exceeds the number of collected surface observations, a difficulty with the dimensionality emerges in classification. This paper has researched the accuracy and interpretability of using the dimension reduction and coefficient shrinkage methods in combination with the logistic model to deal with this dimensionality problem in engineering surface classification. Five methods for dimension reduction and coefficient shrinkage are selected and compared. These are: subset selection (Sub), principal component analysis (PCA), partial least squares (PLS), ridge regression (Ridge), and least absolute and shrinkage and selection operator (Lasso). A case study is used to illustrate their effectiveness by classifying 30 pump body surfaces with 40 surface feature parameters. The obtained results show that the dimension reduction methods, PCA and PLS, could achieve higher classification accuracies but their results are not interpretable. Sub could achieve higher accuracy in this case, but the discrete parameter selection process is aggressive. Finally, the classification results of the coefficient shrinkage methods, Ridge and Lasso, are interpretable for process faults diagnosis purposes; however, the accuracies are lower than the other methods.	[Zhang, Min] Univ Michigan, Dept Mech Engn, Ann Arbor, MI 48109 USA; [Zhu, Ji] Univ Michigan, Dept Stat, Ann Arbor, MI 48109 USA; [Djurdjanovic, Dragan; Ni, Jun] Univ Michigan, Dept Mech Engn, Ann Arbor, MI 48109 USA	Zhang, M (reprint author), Univ Michigan, Dept Mech Engn, Ann Arbor, MI 48109 USA.	mzhangz@umich.edu; jizhu@umich.edu; ddjurdja@umich.edu; junni@umich.edu					Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; HOERL AE, 1970, TECHNOMETRICS, V12, P55; GELADI P, 1986, ANAL CHIM ACTA, V185, P1, DOI 10.1016/0003-2670(86)80028-9; BREIMAN L, 1992, INT STAT REV, V60, P291, DOI 10.2307/1403680; COHERIX, 1999, METHOD APPARATUS 3 D; FARAWAY J, 2004, TEXTS STAT SCI SERIE; HARRINGTON C, 1996, LIB WORLD, V5, P4; Hastie T., 2001, ELEMENTS STAT LEARNI; HOERL AE, 1970, TECHNOMETRICS, V12, P69, DOI 10.2307/1267352; McCullagh P., 1999, GENERALIZED LINEAR M; Muralikrishnan B, 2005, J MANUF SCI E-T ASME, V127, P193, DOI 10.1115/1.1830053; Podsiadlo P, 2003, WEAR, V254, P1189, DOI 10.1016/S0043-1648(03)00333-8; Podsiadlo P, 2004, TRIBOL LETT, V16, P163, DOI 10.1023/B:TRIL.0000009726.17576.b5; RAMAMOORTHY B, 1993, WEAR, V167, P155, DOI 10.1016/0043-1648(93)90320-L; Tibshirani R, 1997, STAT MED, V16, P385, DOI 10.1002/(SICI)1097-0258(19970228)16:4<385::AID-SIM380>3.0.CO;2-3; Tsai DM, 1999, PATTERN RECOGN, V32, P389, DOI 10.1016/S0031-3203(98)00077-6; ZHANG M, IN PRESS ASME J MFG	17	0	0	2	4	SOC MANUFACTURING ENGINEERS	DEARBORN	ONE SME DRIVE, PO BOX 930, DEARBORN, MI 48121-0930 USA	0278-6125			J MANUF SYST	J. Manuf. Syst.		2006	25	3					209	220				12	Engineering, Industrial; Engineering, Manufacturing; Operations Research & Management Science	Engineering; Operations Research & Management Science	244CM	WOS:000251843600005		
J	Yuan, M; Lin, Y				Yuan, M; Lin, Y			Model selection and estimation in regression with grouped variables	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES B-STATISTICAL METHODOLOGY			English	Article						analysis of variance; Lasso; least angle regression; non-negative garrotte; piecewise linear solution path		We consider the problem of selecting grouped variables (factors) for accurate prediction in regression. Such a problem arises naturally in many practical situations with the multifactor analysis-of-variance problem as the most important and well-known example. Instead of selecting factors by stepwise backward elimination, we focus on the accuracy of estimation and consider extensions of the lasso, the LARS algorithm and the non-negative garrotte for factor selection. The lasso, the LARS algorithm and the non-negative garrotte are recently proposed regression methods that can be used to select individual variables. We study and propose efficient algorithms for the extensions of these methods for factor selection and show that these extensions give superior performance to the traditional stepwise backward elimination method in factor selection problems. We study the similarities and the differences between these methods. Simulations and real examples are used to illustrate the methods.	Georgia Inst Technol, Sch Ind & Syst Engn, Atlanta, GA 30332 USA; Univ Wisconsin, Madison, WI USA	Yuan, M (reprint author), Georgia Inst Technol, Sch Ind & Syst Engn, Atlanta, GA 30332 USA.	myuan@isye.gatech.edu					BREIMAN L, 1995, TECHNOMETRICS, V37, P373, DOI 10.2307/1269730; Shen XT, 2002, J AM STAT ASSOC, V97, P210, DOI 10.1198/016214502753479356; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; GEORGE EI, 1993, J AM STAT ASSOC, V88, P881, DOI 10.2307/2290777; Efron B, 2004, ANN STAT, V32, P407; Bakin S, 1999, THESIS AUSTR NATL U; FOSTER DP, 1994, ANN STAT, V22, P1947, DOI 10.1214/aos/1176325766; FU WJ, 1999, J COMPUT GRAPH STAT, V7, P397; George EI, 2000, BIOMETRIKA, V87, P731, DOI 10.1093/biomet/87.4.731; Lemeshow S., 1989, APPL LOGISTIC REGRES; Lin Y, 2003, 1072 U WISC DEP STAT; ROSSET S, 2004, PIECEWISE LINEAR REG; YUAN M, 2005, 200525 GEORG I TECHN	14	1117	1145	15	80	BLACKWELL PUBLISHING	OXFORD	9600 GARSINGTON RD, OXFORD OX4 2DQ, OXON, ENGLAND	1369-7412			J ROY STAT SOC B	J. R. Stat. Soc. Ser. B-Stat. Methodol.		2006	68		1				49	67		10.1111/j.1467-9868.2005.00532.x		19	Statistics & Probability	Mathematics	995UF	WOS:000234129000003		
J	Goeman, JJ; van de Geer, SA; van Houwelingen, HC				Goeman, JJ; van de Geer, SA; van Houwelingen, HC			Testing against a high dimensional alternative	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES B-STATISTICAL METHODOLOGY			English	Article						empirical Bayes modelling; F-test; high dimensional data; hypothesis testing; locally most powerful test; power; score test	REGRESSION; ASSOCIATION; SURVIVAL; MODELS	As the dimensionality of the alternative hypothesis increases, the power of classical tests tends to diminish quite rapidly. This is especially true for high dimensional data in which there are more parameters than observations. We discuss a score test on a hyperparameter in an empirical Bayesian model as an alternative to classical tests. It gives a general test statistic which can be used to test a point null hypothesis against a high dimensional alternative, even when the number of parameters exceeds the number of samples. This test will be shown to have optimal power on average in a neighbourhood of the null hypothesis, which makes it a proper generalization of the locally most powerful test to multiple dimensions. To illustrate this new locally most powerful test we investigate the case of testing the global null hypothesis in a linear regression model in more detail. The score test is shown to have significantly more power than the F-test whenever under the alternative the large variance principal components of the design matrix explain substantially more of the variance of the outcome than do the small variance principal components. The score test is also useful for detecting sparse alternatives in truly high dimensional data, where its power is comparable with the test based on the maximum absolute t-statistic.	Leiden Univ, Ctr Med, Dept Med Stat, NL-2300 RC Leiden, Netherlands	Goeman, JJ (reprint author), Leiden Univ, Ctr Med, Dept Med Stat, POB 9604, NL-2300 RC Leiden, Netherlands.	j.j.goeman@lumc.nl	Goeman, Jelle/C-2260-2012; van houwelingen, hans/C-1872-2008	Goeman, Jelle/0000-0003-4283-0259; 			Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; AZZALINI A, 1993, J ROY STAT SOC B MET, V55, P549; Goeman JJ, 2004, BIOINFORMATICS, V20, P93, DOI 10.1093/bioinformatics/btg382; HOERL AE, 1970, TECHNOMETRICS, V12, P55; Bartholomew D., 1999, LATENT VARIABLE MODE, V2nd; Bernardo J. M., 1994, BAYESIAN THEORY; Brown P. J., 1993, MEASUREMENT REGRESSI; Cox D., 1974, THEORETICAL STAT; Goeman JJ, 2005, BIOINFORMATICS, V21, P1950, DOI 10.1093/bioinformatics/bti267; IMHOF JP, 1961, BIOMETRIKA, V48, P419, DOI 10.1093/biomet/48.3-4.419; JENNRICH RI, 1986, BIOMETRICS, V42, P805, DOI 10.2307/2530695; Kuk AYC, 1999, J STAT COMPUT SIM, V63, P143, DOI 10.1080/00949659908548522; Pawitan Y, 2001, ALL LIKELIHOOD STAT; van de Vijver MJ, 2002, NEW ENGL J MED, V347, P1999, DOI 10.1056/NEJMoa021967	14	75	77	4	8	BLACKWELL PUBLISHING	OXFORD	9600 GARSINGTON RD, OXFORD OX4 2DQ, OXON, ENGLAND	1369-7412			J ROY STAT SOC B	J. R. Stat. Soc. Ser. B-Stat. Methodol.		2006	68		3				477	493		10.1111/j.1467-9868.2006.00551.x		17	Statistics & Probability	Mathematics	043JL	WOS:000237597200007		
S	Bunea, F; Tsybakov, AB; Wegkamp, MH		Lugosi, G; Simon, HU		Bunea, Florentina; Tsybakov, Alexandre B.; Wegkamp, Marten H.			Aggregation and sparsity via l(1) penalized least squares	LEARNING THEORY, PROCEEDINGS	LECTURE NOTES IN ARTIFICIAL INTELLIGENCE		English	Article; Proceedings Paper	19th Annual Conference on Learning Theory (COLT 2006)	JUN 22-25, 2006	Pittsburgh, PA	IBM, Google, Machine Learning Journal, Natl ICT Australia	Carnegie Mellon Univ		NONPARAMETRIC REGRESSION; SHRINKAGE; SELECTION; RATES	This paper shows that near optimal rates of aggregation and adaptation to unknown sparsity can be simultaneously achieved via, penalized least squares in a nonparametric regression setting. The main tool is a novel oracle inequality on the sum between the empirical squared loss of the penalized least squares estimate and a term reflecting the sparsity of the unknown regression function.	Florida State Univ, Dept Stat, Tallahassee, FL 32306 USA; Univ Paris 06, Lab Probabilites & Modeles Aleatoires, F-75252 Paris 05, France; Inst Informat Transmiss Problems, Moscow, Russia	Bunea, F (reprint author), Florida State Univ, Dept Stat, Tallahassee, FL 32306 USA.	bunea@stat.fsu.edu; tsybakov@ccr.jussieu.fr; wegkamp@stat.fsu.edu					Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Birge L, 1998, BERNOULLI, V4, P329, DOI 10.2307/3318720; DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425; BARAUD Y, 2002, ESAIM-PROBAB STAT, V7, P127; BIRGE L, 2002, PREPUBLICATION U PAR, V783; BUNEA F, 2005, AGGREGATION GAUSSIAN; BUNEA F, 2004, CANADIAN J STAT, V22, P1; CANDES E, 2005, DANTZIG SELECTOR STA; Catoni O., 2004, LECT NOTES MATH; Gyorfi L, 2002, DISTRIBUTION FREE TH; Juditsky A, 2000, ANN STAT, V28, P681; KERKYACHARIAN G, 2005, PREPUBLICATION U PAR, V1017; KOLTCHINSKII V, 2005, M STAT PROB METH MOD; Nemirovski A., 2000, LECT NOTES MATH, V1738; Tsybakov AB, 2003, LECT NOTES ARTIF INT, V2777, P303, DOI 10.1007/978-3-540-45167-9_23; Wegkamp M, 2003, ANN STAT, V31, P252, DOI 10.1214/aos/1046294464; Yang Y, 2000, J MULTIVARIATE ANAL, V74, P135, DOI 10.1006/jmva.1999.1884; Yang YH, 2004, BERNOULLI, V10, P25, DOI 10.3150/bj/1077544602	18	25	25	0	1	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		3-540-35294-5	LECT NOTES ARTIF INT			2006	4005						379	391		10.1007/11776420_29		13	Computer Science, Artificial Intelligence	Computer Science	BEV20	WOS:000239587900029		
S	Sjostrand, K; Stegmann, MB; Larsen, R		Reinhardt, JM; Pluim, JPW		Sjostrand, Karl; Stegmann, Mikkel B.; Larsen, Rasmus			Sparse principal component analysis in medical shape modeling - art. no. 61444X	Medical Imaging 2006: Image Processing, Pts 1-3	PROCEEDINGS OF THE SOCIETY OF PHOTO-OPTICAL INSTRUMENTATION ENGINEERS (SPIE)		English	Proceedings Paper	Medical Imaging 2006 Conference	FEB 12-14, 2006	San Diego, CA	SPIE		sparse principal component analysis; PCA; evaluation of principal components; shape modeling; corpus callosum	REGRESSION; SELECTION; ROTATION; LASSO	Principal component analysis (PCA) is a widely used tool in medical image analysis for data reduction, model building, and data understanding and exploration. While PCA is a holistic approach where each new variable is a linear combination of all original variables, sparse PCA (SPCA) aims at producing easily interpreted models through sparse loadings, i.e. each new variable is a linear combination of a subset of the original variables. One of the aims of using SPCA is the possible separation of the results into isolated and easily identifiable effects. This article introduces SPCA for shape analysis in medicine. Results for three different data sets are given in relation to standard PCA and sparse PCA by simple thresholding of small loadings. Focus is on a recent algorithm for computing sparse principal components, but a review of other approaches is supplied as well. The SPCA algorithm has been implemented using Matlab and is available for download. The general behavior of the algorithm is investigated, and strengths and weaknesses are discussed. The original report on the SPCA algorithm argues that the ordering of modes is not an issue. We disagree on this point and propose several approaches to establish sensible orderings. A method that orders modes by decreasing variance and maximizes the sum of variances for all modes is presented and investigated in detail.	Tech Univ Denmark, DK-2800 Lyngby, Denmark	Sjostrand, K (reprint author), Tech Univ Denmark, Richard Petersens Plads, DK-2800 Lyngby, Denmark.						KAISER HF, 1958, PSYCHOMETRIKA, V23, P187, DOI 10.1007/BF02289233; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Jolliffe IT, 2003, J COMPUT GRAPH STAT, V12, P531, DOI 10.1198/1061860032148; Zou H, 2005, J ROY STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x; Efron B, 2004, ANN STAT, V32, P407; CADIMA J, 1995, J APPL STAT, V22, P203, DOI 10.1080/757584614; Chennubhotla C, 2001, P IEEE INT C COMP VI, P641, DOI 10.1109/ICCV.2001.937579; DASPREMONT A, DIRECT FORMULATION S; Gervini D, 2004, AM STAT, V58, P72, DOI 10.1198/0003130042863; Harman H., 1967, MODERN FACTOR ANAL; HAUSMAN R, 1982, STUDIES MANAGEMENT S, P137; Jeffers JNR, 1967, APPLIED STATISTICS, V16, P225, DOI 10.2307/2985919; JOLLIFFE IT, 1995, J APPL STAT, V22, P29, DOI 10.1080/757584395; Rousson V, 2004, J ROY STAT SOC C-APP, V53, P539, DOI 10.1111/j.1467-9876.2004.05359.x; STEGMANN MB, 2006, IN PRESS INT S MED I; STEGMANN MB, 2005, INT S MED IM 2005 SA, V5747; Vines SK, 2000, J ROY STAT SOC C-APP, V49, P441, DOI 10.1111/1467-9876.00204; ZOU Z, 2004, SPARSE PRINCIPAL COM	18	0	0	1	2	SPIE-INT SOC OPTICAL ENGINEERING	BELLINGHAM	1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA	0277-786X		0-8194-6423-6	P SOC PHOTO-OPT INS			2006	6144		1-3				X1444	X1444	61444X	10.1117/12.651658		12	Computer Science, Interdisciplinary Applications; Imaging Science & Photographic Technology; Radiology, Nuclear Medicine & Medical Imaging	Computer Science; Imaging Science & Photographic Technology; Radiology, Nuclear Medicine & Medical Imaging	BEL75	WOS:000238033302012		
J	Ioannidis, A; Nicolaou, C; Legakis, NJ; Ioannidou, V; Papavasileiou, E; Voyatzi, A; Chatzipanagiotou, S				Ioannidis, Anastassios; Nicolaou, Chryssoula; Legakis, Nicholas John; Ioannidou, Vasiliki; Papavasileiou, Eleni; Voyatzi, Aliki; Chatzipanagiotou, Stylianos			Genotyping of human Campylobacter jejuni isolates in Greece by pulsed-field gel electrophoresis	MOLECULAR DIAGNOSIS & THERAPY			English	Article							COLI; DNA; SCHEME	Background: Pulsed-field gel electrophoresis (PFGE) typing has been recognized by several groups as a relatively simple and quick method for genotyping of Campylobacter jejuni (C. jejuni). The present Study was carried out to determine the genetic variations among clinical isolates of C. jejuni from Greece and to establish a database, which could be used for future epidemiological and clinical studies. Methods: A total of 93 C. jejuni clinical isolates of known flagellin subunit A (flaA.) genotype, serotype. and antimicrobial susceptibility pattern, were collected from a general hospital in the Attica region of Greece, between the years 2000 and 2003. The PFGE profiles of Sinal DNA digests of each strain were compared using a bin analysis based on 44 molecular size intervals. Results: Forty-three different PFGE types, designated as C. jejuni (C. j.) 1 Greece (GR) to C j. 43 GR, were identified. There was no statistically significant association of PFGE type with flaA genotype, serotype, or antimicrobial susceptibility pattern. However, PFGE typing did show a remarkable discriminatory ability within the non-serotypable group. Conclusion: Evaluating our results, we observed that (i) there was no statistically significant clonality of a certain PFGE type among the strains examined. and (ii) the discriminatory ability of PFGE typing was much better than that of the other typing methods. This is the first report of the use of bin patterns to compare the PFGE genotypes identified.	Aeginit Hosp, Athens Med Sch, Dept Clin Microbiol, Athens, Greece; Athens Med Sch, Dept Microbiol, Athens, Greece; Penteli Childrens Hosp, Dept Clin Microbiol, Athens, Greece	Chatzipanagiotou, S (reprint author), Aeginit Hosp, Athens Med Sch, Dept Biopathol & Clin Microbiol, Vass Sophias Av 72-74, Athens 11528, Greece.	schatzi@med.uoa.gr					Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; TENOVER FC, 1995, J CLIN MICROBIOL, V33, P2233; LINK W, 1995, THEOR APPL GENET, V90, P27, DOI 10.1007/BF00220992; Wassenaar TM, 2000, APPL ENVIRON MICROB, V66, P1; VANDEPEER Y, 1994, COMPUT APPL BIOSCI, V10, P569; Engberg J, 1998, CLIN MICROBIOL INFEC, V4, P648, DOI 10.1111/j.1469-0691.1998.tb00348.x; Glazko G., 2005, BIOINFORMATICS S3, V21, piii3, DOI DOI 10.1093/BIOINFORMATICA/BTI1201; GRAJEWSKI BA, 1985, J CLIN MICROBIOL, V22, P13; Hewson I, 2006, MICROB ECOL, V51, P147, DOI 10.1007/s00248-005-0144-9; Iriarte MP, 1996, FEMS IMMUNOL MED MIC, V15, P17, DOI 10.1111/j.1574-695X.1996.tb00353.x; LIOR H, 1984, J CLIN MICROBIOL, V20, P636; NACHAMKIN I, 1993, J CLIN MICROBIOL, V31, P1531; National Committee for Clinical Laboratory Standards Methods for Dilution Antimicrobial Susceptibility Tests for Bacteria that Grow Aerobically, 1997, M7A4 NAT COMM CLIN L; Newell DG, 2000, CAMPYLOBACTER, P27; On SLW, 1998, FEMS MICROBIOL LETT, V165, P341, DOI 10.1016/S0378-1097(98)00298-5; On SLW, 1998, EPIDEMIOL INFECT, V120, P231, DOI 10.1017/S0950268898008668; OWEN RJ, 1995, J CLIN MICROBIOL, V33, P872; PENNER JL, 1983, EUR J CLIN MICROBIOL, V2, P378, DOI 10.1007/BF02019474; TAUXE RV, 1992, CAMPYLOBACTER JEJUNI; YAN W, 1991, J INFECT DIS, V163, P1068	20	4	4	0	5	ADIS INTERNATIONAL LTD	AUCKLAND	41 CENTORIAN DR, PRIVATE BAG 65901, MAIRANGI BAY, AUCKLAND 1311, NEW ZEALAND	1177-1062			MOL DIAGN THER	Mol. Diagn. Ther.		2006	10	6					391	396				6	Genetics & Heredity; Pharmacology & Pharmacy	Genetics & Heredity; Pharmacology & Pharmacy	127EF	WOS:000243567400007	17154656	
S	Ukil, A; Jordaan, J		King, I; Wang, J; Chan, L; Wang, DL		Ukil, Abhisek; Jordaan, Jaco			A new approach to load forecasting: Using semi-parametric method and neural networks	NEURAL INFORMATION PROCESSING, PT 2, PROCEEDINGS	LECTURE NOTES IN COMPUTER SCIENCE		English	Article; Proceedings Paper	13th International Conference on Neural Informational Processing	OCT 03-06, 2006	Hong Kong, PEOPLES R CHINA	Chinese Univ Hong Kong, Asia Pacific Neural Network Assembly, K C Wong Educ Fdn				A new approach to electrical load forecasting is investigated. The method is based on the semi-parametric spectral estimation method that is used to decompose a signal into a harmonic linear signal model and a non-linear part. A neural network is then used to predict the non-linear part. The final predicted signal is then found by adding the neural network predicted non-linear part and the linear part. The performance of the proposed method seems to be more robust than using only the raw load data.	Tshwane Univ Technol, ZA-0001 Pretoria, South Africa	Ukil, A (reprint author), Tshwane Univ Technol, Nelson Mandela Dr, ZA-0001 Pretoria, South Africa.	abhiukil@yahoo.com; jakop_2003@yahoo.com					Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; BIALKOWSKI SE, 1989, ANAL CHEM, V61, P1308, DOI 10.1021/ac00186a028; Bitzer B., 1997, UPEC 97 U POW ENG C; CASARCORREDERA J, 1985, CH211888500000796 IE, P796; Draper N, 1981, APPL REGRESSION ANAL; GORRY PA, 1990, ANAL CHEM, V62, P570, DOI 10.1021/ac00205a007; JORDAAN J, 2004, T S AFRICAN I ELECT, V94, P171; Jordaan J. A., 2004, T S AFRICAN I ELECT, V95, P35; Mathworks, 2002, MATLAB DOC NEUR NETW; Wasserman P. D., 1993, ADV METHODS NEURAL C; ZIVANOVIC R, 2005, POW TECH 2005 C STPE; Zivanovic R, 2004, IEEE T POWER DELIVER, V19, P1085, DOI 10.1109/TPWRD.2004.829945; ZIVANOVIC R, 2004, 8 INT C DEV POW SYST	13	3	3	0	2	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		3-540-46481-6	LECT NOTES COMPUT SC			2006	4233						974	983				10	Computer Science, Artificial Intelligence; Computer Science, Theory & Methods; Imaging Science & Photographic Technology	Computer Science; Imaging Science & Photographic Technology	BFG61	WOS:000241753100107		
S	Zhang, K; Chan, LW		King, I; Wang, J; Chan, L; Wang, DL		Zhang, Kun; Chan, Lai-Wan			Extensions of ICA for causality discovery in the Hong Kong stock market	NEURAL INFORMATION PROCESSING, PT 3, PROCEEDINGS	LECTURE NOTES IN COMPUTER SCIENCE		English	Article; Proceedings Paper	13th International Conference on Neural Informational Processing	OCT 03-06, 2006	Hong Kong, PEOPLES R CHINA	Chinese Univ Hong Kong, Asia Pacific Neural Network Assembly, K C Wong Educ Fdn			POST-NONLINEAR MIXTURES; BLIND SEPARATION; LIKELIHOOD; SELECTION	Recently independent component analysis (ICA) has been proposed for discovery of linear, non-Gaussian, and acyclic causal models (LiNGAM). As in practice the LiNGAM assumption usually does not exactly hold, in this paper we propose some methods to perform causality discovery even when LiNGAM is violated. The first method is ICA with a sparse separation matrix. By incorporating a suitable penalty term, the separation matrix produced by this method tends to satisfy the LiNGAM assumption. The other two methods are proposed to tackle nonlinearity in the data generation procedure, which violates the LiNGAM assumption. In the second method, the post-nonlinear mixing ICA model is exploited to do causality discovery when the nonlinearity is componentwise. The third method is proposed for the case where the nonlinear distortion in data generation is of arbitrary form, but smooth and weak. The separation system for such data is a linear transformation coupled with a nonlinear one, and the nonlinear one is as weak as possible such that it can be neglected when performing causality discovery. The linear causal relations in the data are then revealed. The proposed methods are applied to discover the causal relations in the Hong Kong stock market, and the last method works very well. The resulting causal diagram shows some interesting information in the stock market.	Chinese Univ Hong Kong, Dept Comp Sci & Engn, Shatin, Hong Kong, Peoples R China	Zhang, K (reprint author), Chinese Univ Hong Kong, Dept Comp Sci & Engn, Shatin, Hong Kong, Peoples R China.	kzhang@cse.cuhk.edu.hk; lwchan@cse.cuhk.edu.hk					Almeida L. B., 2003, J MACHINE LEARNING R, V4, P1297, DOI 10.1162/jmlr.2003.4.7-8.1297; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; Dodge Y, 2001, AM STAT, V55, P51, DOI 10.1198/000313001300339932; GRNAOVETTER M, 1994, HDB EC SOCIOLOGY, pCH18; HO RY, 2004, 027 KINGS COLL MAN C; Hyvarinen A., 2001, INDEPENDENT COMPONEN; Jutten C., 2003, P 4 INT S IND COMP A, P245; KHANNA T, 2006, IN PRESS ORG SCI; Pearl J., 2000, CAUSALITY MODELS REA; Pham DT, 1997, IEEE T SIGNAL PROCES, V45, P1712; SHIMIZU S, 2006, UNPUB J MACHINE LEAR; Taleb A, 1999, IEEE T SIGNAL PROCES, V47, P2807, DOI 10.1109/78.790661; ZHANG K, 2006, NONLINEAR ICA LTD NO; Zhang K, 2005, NEURAL COMPUT, V17, P425, DOI 10.1162/0899766053011500	15	2	2	1	1	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		3-540-46484-0	LECT NOTES COMPUT SC			2006	4234						400	409				10	Computer Science, Artificial Intelligence; Computer Science, Hardware & Architecture; Computer Science, Interdisciplinary Applications; Computer Science, Theory & Methods	Computer Science	BFG63	WOS:000241759000045		
B	Huang, J; Harrington, D		Nikulin, M; Commenges, D; Huber, C		Huang, J; Harrington, D			Operating characteristics of partial least squares in right-censored data analysis and its application in predicting the change of HIV-I RNA	Probability, Statistics and Modelling in Public Health			English	Proceedings Paper	Workshop on Probability, Statistics and Modelling in Public Health	SEP 22-23, 2003	Bordeaux, FRANCE	l IFR-99 Sante Publique		dimension reduction; partial least squares; accelerative failure time model; cross-validation; prediction	PRINCIPAL COMPONENTS REGRESSION; TYPE-1 PROTEASE INHIBITOR; LINEAR-REGRESSION; EFFECTS MODELS; SURVIVAL-DATA; RESISTANCE; SHRINKAGE; VARIABLES; ESTIMATORS; SAQUINAVIR	It is often of interest to effectively use the information on a large number of covariates in predicting response or outcome. Various statistical tools have been developed to overcome the difficulties caused by the high-dimensionality of the covariate space in the setting of a linear regression model. This paper focuses on the situation where the outcomes of interest are subjected to right censoring. We implement the extended partial least squares method along with other commonly used approaches for analyzing the high dimensional covariates to a data set from AIDS clinical trials (ACTG333). Predictions were computed on the covariate effect and the response for a future subject with a set of covariates. Simulation studies were conducted to compare our proposed methods with other prediction procedures for different numbers of covariates, different correlations among the covariates and different failure time distributions. Mean squared prediction error and mean absolute distance were used to measure the accuracy of prediction on the covariate effect and the response, respectively. We also compared the prediction performance of different approaches using numerical studies. The results show that the Buckley-James based partial least squares, stepwise subset model selection and principal components regression have similar predictive power and the partial least squares method has several advantages in terms of interpretability and numerical computation.	Northwestern Univ, Feinberg Sch Med, Dept Prevent Med, Chicago, IL 60611 USA	Huang, J (reprint author), Northwestern Univ, Feinberg Sch Med, Dept Prevent Med, 680 N Lake Shore Dr Suite 1102, Chicago, IL 60611 USA.						WOLD S, 1984, SIAM J SCI STAT COMP, V5, P735, DOI 10.1137/0905052; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; HELLAND IS, 1988, COMMUN STAT SIMULAT, V17, P581, DOI 10.1080/03610918808812681; Wentzell PD, 2003, CHEMOMETR INTELL LAB, V65, P257, DOI 10.1016/S0169-7439(02)00138-7; FRANK IE, 1993, TECHNOMETRICS, V35, P109, DOI 10.2307/1269656; Hotelling H, 1933, J EDUC PSYCHOL, V24, P417, DOI 10.1037/h0071325; BUCKLEY J, 1979, BIOMETRIKA, V66, P429; Brown P. J., 1993, MEASUREMENT REGRESSI; Butler NA, 2000, J ROY STAT SOC B, V62, P585, DOI 10.1111/1467-9868.00252; COLLIER A, 1996, NEW ENGL J MED, V16, P1011; Condra JH, 1996, J VIROL, V70, P8270; CONDRA JH, 1995, NATURE, V374, P569, DOI 10.1038/374569a0; COX DR, 1972, J R STAT SOC B, V34, P187; Currie ID, 1996, BIOMETRIKA, V83, P912, DOI 10.1093/biomet/83.4.912; DEJONG S, 1993, CHEMOMETR INTELL LAB, V18, P251, DOI 10.1016/0169-7439(93)85002-X; DENHAM MC, 1991, THESIS U LIVERPOOL; Draper N, 1981, APPL REGRESSION ANAL; Goutis C, 1996, ANN STAT, V24, P816; GUNST RF, 1983, COMMUN STAT-THEOR M, V12, P2217, DOI 10.1080/03610928308828603; HELLER G, 1992, BIOMETRICS, V48, P101, DOI 10.2307/2532742; HOCKING RR, 1976, BIOMETRICS, V32, P1, DOI 10.2307/2529336; HUANG J, IN PRESS BIOMETRICS; HUANG J, 2004, IN PRESS LIFETIME DA; Hughes JP, 1999, BIOMETRICS, V55, P625, DOI 10.1111/j.0006-341X.1999.00625.x; Jacobsen H, 1996, J INFECT DIS, V173, P1379; Jacqmin-Gadda H, 2000, Biostatistics, V1, P355, DOI 10.1093/biostatistics/1.4.355; Jin ZZ, 2003, BIOMETRIKA, V90, P341, DOI 10.1093/biomet/90.2.341; Jolliffe I. T., 1986, PRINCIPAL COMPONENT; LAIRD NM, 1982, BIOMETRICS, V38, P963, DOI 10.2307/2529876; Marschner IC, 1999, J ACQ IMMUN DEF SYND, V20, P220; Miller A. J., 1990, SUBSET SELECTION REG; MILLER R, 1982, BIOMETRIKA, V69, P521; Nguyen DV, 2002, BIOINFORMATICS, V18, P1625, DOI 10.1093/bioinformatics/18.12.1625; Para MF, 2000, J INFECT DIS, V182, P733, DOI 10.1086/315769; Park Peter J, 2002, Bioinformatics, V18 Suppl 1, pS120; STONE M, 1990, J ROY STAT SOC B MET, V52, P237; TSIATIS AA, 1990, ANN STAT, V18, P354, DOI 10.1214/aos/1176347504; Vaillancourt M, 1999, AIDS RES HUM RETROV, V15, P355, DOI 10.1089/088922299311321; WOLD H, 1966, RES PAPERS STAT, P411; WOLD H, 1976, PERSPECTIVES PROBABI, P117; YING Z, 1992, BIOMETRIKA, V79, P205	41	0	0	1	3	SPRINGER	NEW YORK	233 SPRING STREET, NEW YORK, NY 10013, UNITED STATES			0-387-26022-6				2006							202	230		10.1007/0-387-26023-4_14		29	Public, Environmental & Occupational Health; Mathematics, Interdisciplinary Applications; Statistics & Probability	Public, Environmental & Occupational Health; Mathematics	BDG73	WOS:000233422800014		
S	Bortolin, G; Gutman, PO			IEEE	Bortolin, Gianantonio; Gutman, Per-Olof			A new algorithm for variable selection	PROCEEDINGS OF THE 45TH IEEE CONFERENCE ON DECISION AND CONTROL, VOLS 1-14	IEEE Conference on Decision and Control		English	Proceedings Paper	45th IEEE Conference on Decision and Control	DEC 13-15, 2006	San Diego, CA	IEEE			GENERALIZED CROSS-VALIDATION; RIDGE REGRESSION; NONORTHOGONAL PROBLEMS; EXPLICIT	A new method for variable selection and estimation called Iteratively Scaled Ridge Regression, ISRR, is proposed. The method is an iterative algorithm based on ridge regression. Simulation studies show that ISRR shares the properties of both subset selection and ridge regression. It selects an optimal subset of the regressor variables and is robust to small changes in the data set. The ISRR algorithm was primarily developed for linear models, but is quite simple and general and can easily be extended to more general linear and nonlinear models.	[Bortolin, Gianantonio] Royal Inst Technol, Dept Math, S-10044 Stockholm, Sweden	Bortolin, G (reprint author), Royal Inst Technol, Dept Math, S-10044 Stockholm, Sweden.	bortolin@math.kth.se; peo@tx.technion.ac.il					BREIMAN L, 1995, TECHNOMETRICS, V37, P373, DOI 10.2307/1269730; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Kadane JB, 2004, J AM STAT ASSOC, V99, P279, DOI 10.1198/016214504000000269; George EI, 2000, J AM STAT ASSOC, V95, P1304, DOI 10.2307/2669776; FRANK IE, 1993, TECHNOMETRICS, V35, P109, DOI 10.2307/1269656; GOLUB GH, 1979, TECHNOMETRICS, V21, P215, DOI 10.1080/00401706.1979.10489751; CRAVEN P, 1979, NUMER MATH, V31, P377; Kohavi R, 1997, ARTIF INTELL, V97, P273, DOI 10.1016/S0004-3702(97)00043-X; BORTOLIN G, 2005, THESIS KTH; Daniel C, 1980, FITTING EQUATIONS DA; Draper N, 1981, APPL REGRESSION ANAL; Efron B, 1993, INTRO BOOTSTRAP; Guyon I., 2003, Journal of Machine Learning Research, V3, DOI 10.1162/153244303322753616; HEMMERLE WJ, 1978, TECHNOMETRICS, V20, P109, DOI 10.2307/1268701; HEMMERLE WJ, 1975, TECHNOMETRICS, V17, P309, DOI 10.2307/1268066; HOERL AE, 1970, TECHNOMETRICS, V12, P69, DOI 10.2307/1267352; HOERL AE, 1970, TECHNOMETRICS, V12, P55, DOI DOI 10.1080/00401706.1970.10488634; MARQUARD.DW, 1970, TECHNOMETRICS, V12, P591, DOI 10.2307/1267205; Miller A, 2002, SUBSET SELECTION REG, V2nd	19	2	2	1	1	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA	0191-2216		978-1-4244-0170-3	IEEE DECIS CONTR P			2006							1309	1314		10.1109/CDC.2006.376828		6	Automation & Control Systems; Engineering, Electrical & Electronic; Operations Research & Management Science	Automation & Control Systems; Engineering; Operations Research & Management Science	BHD08	WOS:000252251603124		
S	Roll, J; Lind, I; Ljung, L			IEEE	Roll, Jacob; Lind, Ingela; Ljung, Lennart			Connections between optimisation-based regressor selection and analysis of variance	PROCEEDINGS OF THE 45TH IEEE CONFERENCE ON DECISION AND CONTROL, VOLS 1-14	IEEE Conference on Decision and Control		English	Proceedings Paper	45th IEEE Conference on Decision and Control	DEC 13-15, 2006	San Diego, CA	IEEE			PREDICTION; SHRINKAGE; MODEL	Earlier contributions have shown that Analysis of Variance (ANOVA) can be successfully used for finding good regressors for nonlinear models in a nonlinear black-box system identification context. In this paper, it is shown that the ANOVA problem can be recast as an optimisation problem. Two modified, convex versions of the ANOVA optimisation problem are then proposed, and it turns out that they are closely related to the nn-garrote and wavelet shrinkage methods, respectively. In the case of balanced data, it is also shown that the methods have a nice orthogonality property in the sense that different groups of parameters can be computed independently.	[Roll, Jacob; Lind, Ingela; Ljung, Lennart] Linkoping Univ, Div Automat Control, SE-58183 Linkoping, Sweden	Roll, J (reprint author), Linkoping Univ, Div Automat Control, SE-58183 Linkoping, Sweden.	roll@isy.liu.se; ingela@isy.liu.se; ljung@isy.liu.se	Ljung, Lennart/B-3822-2014	Ljung, Lennart/0000-0003-4881-8955			Akaike H, 1981, TRENDS PROGR SYSTEM; BREIMAN L, 1995, TECHNOMETRICS, V37, P373, DOI 10.2307/1269730; Yuan M, 2006, J ROY STAT SOC B, V68, P49, DOI 10.1111/j.1467-9868.2005.00532.x; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; RISSANEN J, 1978, AUTOMATICA, V14, P465, DOI 10.1016/0005-1098(78)90005-5; AKAIKE H, 1969, ANN I STAT MATH, V21, P243, DOI 10.1007/BF02532251; Efron B, 2004, ANN STAT, V32, P407; DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425; SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136; MALLOWS CL, 1973, TECHNOMETRICS, V15, P661, DOI 10.2307/1267380; BILLINGS SA, 1986, INT J CONTROL, V44, P803, DOI 10.1080/00207178608933633; FRIEDMAN JH, 1991, ANN STAT, V19, P1, DOI 10.1214/aos/1176347963; Hastie T., 2001, ELEMENTS STAT LEARNI; KRISHNAIAH PR, 1980, HDB STAT, V1; LIND I, 2005, P 16 IFAC WORLD C PR; Lind I., 2006, THESIS LINKOPINGS U; Lind I, 2005, AUTOMATICA, V41, P693, DOI 10.1016/j.automatica.2004.11.017; LIND I, 2001, SE58183 LINK U DEP E; MILLER RG, 1997, ANOVA; Montgomery DC, 1991, DESIGN ANAL EXPT; RISSANEN J, 1986, ANN STAT, V14, P1080, DOI 10.1214/aos/1176350051; Vandenberghe L., 2004, CONVEX OPTIMIZATION; YAO QW, 1994, STAT SINICA, V4, P51; YUAN M, 2005, 200525 GEORG I TECHN	24	2	2	1	3	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA	0191-2216		978-1-4244-0170-3	IEEE DECIS CONTR P			2006							4907	4914		10.1109/CDC.2006.377519		8	Automation & Control Systems; Engineering, Electrical & Electronic; Operations Research & Management Science	Automation & Control Systems; Engineering; Operations Research & Management Science	BHD08	WOS:000252251600157		
J	Segal, MR				Segal, Mark R.			Validation in genomics: CpG island methylation revisited	STATISTICAL APPLICATIONS IN GENETICS AND MOLECULAR BIOLOGY			English	Article						multiple testing; cross-validation; local false discovery rate; classification; sequence features		In a recent article in PLoS Genetics, Bock et al., (2006) undertake an extensive computational epigenetics analysis of the ability of DNA sequence-derived features, capturing attributes such as tetramer frequencies, repeats and predicted structure, to predict the methylation status of CpG islands. Their suite of analyses appears highly rigorous with regard to accompanying validation procedures, employing stringent Bonferroni corrections, stratified cross-validation, and follow-up experimental verification. Here, however, we showcase concerns with the validation steps, in part ascribable to the genome scale of the investigation, that serve as a cautionary note and indicate the heightened need for careful selection of analytic and companion validation methods. A series of new analyses of the same CpG island methylation data helps illustrate these issues, not just for this particular study, but also analogous investigations involving high-dimensional predictors with complex between-feature dependencies.	Univ Calif San Francisco, San Francisco, CA USA	Segal, MR (reprint author), Univ Calif San Francisco, San Francisco, CA USA.	mark@biostat.ucsf.edu					Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451; Gui J, 2005, BIOINFORMATICS, V21, P3001, DOI 10.1093/bioinformatics/bti422; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324; Dudoit S, 2003, STAT SCI, V18, P71, DOI 10.1214/ss/1056397487; Tusher VG, 2001, P NATL ACAD SCI USA, V98, P5116, DOI 10.1073/pnas.091062498; Golub TR, 1999, SCIENCE, V286, P531, DOI 10.1126/science.286.5439.531; Efron B, 1997, J AM STAT ASSOC, V92, P548, DOI 10.2307/2965703; Segal MR, 2003, J COMPUT BIOL, V10, P961, DOI 10.1089/106652703322756177; BENJAMINI Y, 1995, J ROY STAT SOC B MET, V57, P289; Storey JD, 2003, P NATL ACAD SCI USA, V100, P9440, DOI 10.1073/pnas.1530509100; Efron B, 2004, ANN STAT, V32, P407; West M, 2001, P NATL ACAD SCI USA, V98, P11462, DOI 10.1073/pnas.201162998; Tibshirani R, 2002, P NATL ACAD SCI USA, V99, P6567, DOI 10.1073/pnas.082099299; Storey JD, 2004, J ROY STAT SOC B, V66, P187, DOI 10.1111/j.1467-9868.2004.00439.x; Jin W, 2001, NAT GENET, V29, P389, DOI 10.1038/ng766; Baldi P, 2000, BIOINFORMATICS, V16, P412, DOI 10.1093/bioinformatics/16.5.412; Rosenwald A, 2002, NEW ENGL J MED, V346, P1937, DOI 10.1056/NEJMoa012914; BHASIN M, 2005, FEBS LETT, V579, DOI UNSP 43024308; Bock C, 2006, PLOS GENET, V2, P243, DOI 10.1371/journal.pgen.0020026; Efron B., 2006, CORRELATION LARGE SC; Efron B, 2004, J AM STAT ASSOC, V99, P619, DOI 10.1198/016214504000000692; Efron B., 2005, LOCAL FALSE DISCOVER; Efron B, 2004, J AM STAT ASSOC, V99, P96, DOI 10.1198/016214504000000089; FEINBERG AP, 2004, NAT REV CANCER, V4, DOI UNSP 143153; FELTUS FA, 2003, P NATL ACAD SCI USA, V100, DOI UNSP 1225312258; Freund Y., 1996, P 13 INT C MACH LEAR, P148; GENTLEMAN R, 2005, BIOINFORMATICS COMPT; HANDA V, 2005, J MOL BIOL, V348, DOI UNSP 11031112; Hastie T., 2001, ELEMENTS STAT LEARNI; HEARD E, 2004, CURR OPIN CELL BIOL, V16, DOI UNSP 247255; PARK MY, 2006, IN PRESS BIOSTATISTI; Pollard K. S., 2005, BIOINFORMATICS COMPU, P251; REIK W, 2003, THERIOGENOLOGY, V59, P2132; Segal MR, 2006, BIOSTATISTICS, V7, P268, DOI 10.1093/biostatistics/kxj006; Smyth G. K., 2004, STAT APPL GENET MOL, V3, DOI [10.2202/1544-6115.1027, DOI 10.2202/1544-6115.1027]; Smyth Gordon K, 2003, Methods Mol Biol, V224, P111; STOREY JD, 2005, OPTIMAL DISCOVERY PR; TIBSHIRANI RJ, 2002, STAT APPL GENETICS M, V1; Venables W.N., 1999, MODERN APPL STAT SPL; Wu B., 2005, BIOINFORMATICS, V22, P472, DOI 10.1093/bioinformatics/bti827; YAMADA Y, 2004, GENOME RES, V14, DOI UNSP 247266; Zhu J, 2004, BIOSTATISTICS, V5, P427, DOI 10.1093/biostatistics/kxg046	43	2	2	0	1	BERKELEY ELECTRONIC PRESS	BERKELEY	2809 TELEGRAPH AVENUE, STE 202, BERKELEY, CA 94705 USA	1544-6115			STAT APPL GENET MOL	Stat. Appl. Genet. Mol. Biol.		2006	5								29			19	Biochemistry & Molecular Biology; Mathematical & Computational Biology; Statistics & Probability	Biochemistry & Molecular Biology; Mathematical & Computational Biology; Mathematics	V43TY	WOS:000202958600002		
J	Trendafilov, NT; Jolliffe, IT				Trendafilov, NT; Jolliffe, IT			Projected gradient approach to the numerical solution of the SCoTLASS	COMPUTATIONAL STATISTICS & DATA ANALYSIS			English	Article						principal component analysis; simple structure components; LASSO constraint; penalty function; constrained optimization; gradient dynamical system on manifolds; steepest ascent vector flows; optimality conditions	COMPONENT TECHNIQUE; PRINCIPAL; LASSO; ALGORITHMS; SELECTION	The SCoTLASS problem-principal component analysis modified so that the components satisfy the Least Absolute Shrinkage and Selection Operator (LASSO) constraint-is reformulated as a dynamical system on the unit sphere. The LASSO inequality constraint is tackled by exterior penalty function. A globally convergent algorithm is developed based on the projected gradient approach. The algorithm is illustrated numerically and discussed on a well-known data set. (c) 2004 Elsevier B.V. All rights reserved.	Univ W England, Fac Comp Engn & Math Sci, Bristol BS16 1QY, Avon, England; Univ Aberdeen, Dept Math Sci, Aberdeen AB24 3UE, Scotland	Trendafilov, NT (reprint author), Univ W England, Fac Comp Engn & Math Sci, Bristol BS16 1QY, Avon, England.	nickolay.trendafilov@uwe.ac.uk					Edelman A, 1998, SIAM J MATRIX ANAL A, V20, P303, DOI 10.1137/S0895479895290954; Shampine LF, 1997, SIAM J SCI COMPUT, V18, P1, DOI 10.1137/S1064827594276424; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Osborne MR, 2000, IMA J NUMER ANAL, V20, P389, DOI 10.1093/imanum/20.3.389; Jolliffe IT, 2003, J COMPUT GRAPH STAT, V12, P531, DOI 10.1198/1061860032148; Aoki M., 1971, INTRO OPTIMIZATION T; BROCKETT RW, 1989, LINEAR ALGEBRA APPL, V122, P761, DOI 10.1016/0024-3795(89)90675-7; Chen TP, 1998, NEURAL NETWORKS, V11, P385, DOI 10.1016/S0893-6080(98)00004-5; Chu MT, 1998, STAT COMPUT, V8, P125, DOI 10.1023/A:1008934100736; Chu MT, 2001, J COMPUT GRAPH STAT, V10, P746, DOI 10.1198/106186001317243430; Golub G., 1991, MATRIX COMPUTATIONS; Helmke U., 1994, OPTIMIZATION DYNAMIC; Hirsch M.W., 1974, DIFFERENTIAL EQUATIO; Jolliffe I. T., 2002, PRINCIPAL COMPONENT; Jolliffe IT, 2000, J COMPUT GRAPH STAT, V9, P689, DOI 10.2307/1391088; Mahony RE, 1996, J AUST MATH SOC B, V37, P430; *MATLAB, 1999, US MATLAB VERS 5; Osborne MR, 2000, J COMPUT GRAPH STAT, V9, P319, DOI 10.2307/1390657; TRENDAFILOV NT, 2001, INT M PSYCH SOC IMPS; TURLACH BA, 2001, SIMULTANEOUS VARIABL	20	18	19	0	3	ELSEVIER SCIENCE BV	AMSTERDAM	PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS	0167-9473			COMPUT STAT DATA AN	Comput. Stat. Data Anal.	JAN 10	2006	50	1					242	253		10.1016/j.csda.2004.07.017		12	Computer Science, Interdisciplinary Applications; Statistics & Probability	Computer Science; Mathematics	967LR	WOS:000232093200015		
J	Sulkava, M; Tikka, J; Hollmen, J				Sulkava, M; Tikka, J; Hollmen, J			Sparse regression for analyzing the development of foliar nutrient concentrations in coniferous trees	ECOLOGICAL MODELLING			English	Article; Proceedings Paper	4th International Workshop on Environmental Applications of Machine Learning (EAML)	SEP 27-OCT 01, 2004	Bled, SLOVENIA			linear sparse regression; prediction; foliar nutrition; coniferous tree	SELECTION; PREDICTION; SHRINKAGE	Analyzing and predicting the development of foliar nutrient concentrations are important and challenging tasks in environmental monitoring. This article presents how linear sparse regression models can be used to represent the relations between different foliar nutrient concentration measurements of coniferous trees in consecutive years. In the experiments the models proved to be capable of providing relatively good and reliable predictions of the development of foliage with a considerably small number of regressors. Two methods for estimating sparse models were compared to more conventional linear regression models. Differences in the prediction accuracies between the sparse and full models were minor, but the sparse models were found to highlight important dependencies between the nutrient measurements better than the other regression models. The use of sparse models is, therefore, advantageous in the analysis and interpretation of the development of foliar nutrient concentrations. (c) 2005 Elsevier B.V. All rights reserved.	Helsinki Univ Technol, Lab Comp & Informat Sci, FI-02015 Helsinki, Finland	Sulkava, M (reprint author), Helsinki Univ Technol, Lab Comp & Informat Sci, POB 5400, FI-02015 Helsinki, Finland.	Mika.Sulkava@hut.fi; tikka@mail.cis.hut.fi; Jaakko.Hollmen@hut.fi					BREIMAN L, 1995, TECHNOMETRICS, V37, P373, DOI 10.2307/1269730; Hansen MH, 2001, J AM STAT ASSOC, V96, P746, DOI 10.1198/016214501753168398; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; BREIMAN L, 1992, J AM STAT ASSOC, V87, P738, DOI 10.2307/2290212; HOERL AE, 1970, TECHNOMETRICS, V12, P55; Efron B, 2004, ANN STAT, V32, P407; MALLOWS CL, 1973, TECHNOMETRICS, V15, P661, DOI 10.2307/1267380; COPAS JB, 1983, J R STAT SOC B, V45, P311; Bishop C. M., 1995, NEURAL NETWORKS PATT; Breiman L, 1997, J ROY STAT SOC B MET, V59, P3, DOI 10.1111/1467-9868.00054; FRIEDMAN JH, 1991, ANN STAT, V19, P1, DOI 10.1214/aos/1176347963; Good P., 2000, SPRINGER SERIES STAT; GUTHRIE W, 2005, NIST SEMATECH E HDB; HAIR JF, 1995, MULTIVARIATE DATA AN; Hastie T., 2001, SPRINGER SERIES STAT; Haykin S, 1999, NEURAL NETWORKS COMP; LUYSSAERT S, 2003, FOREST NUTR FINNISH, P72; Luyssaert S, 2004, J ENVIRON MONITOR, V6, P160, DOI 10.1039/b313372a; Stefan K., 1997, FOREST FOLIAR CONDIT	19	5	5	1	1	ELSEVIER SCIENCE BV	AMSTERDAM	PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS	0304-3800			ECOL MODEL	Ecol. Model.	JAN 27	2006	191	1			SI		118	130		10.1016/j.ecolmodel.2005.08.016		13	Ecology	Environmental Sciences & Ecology	002RM	WOS:000234629100011		
J	Fan, JQ; Lin, HZ; Zhou, Y				Fan, JQ; Lin, HZ; Zhou, Y			Local partial-likelihood estimation for lifetime data	ANNALS OF STATISTICS			English	Article						local partial likelihood; one-step estimation; varying coefficient; proportional hazards model; variable selection	VARYING-COEFFICIENT MODELS; TIME-DEPENDENT COEFFICIENTS; PROPORTIONAL HAZARDS MODEL; COX REGRESSION-MODEL; VARIABLE SELECTION; LONGITUDINAL DATA; LASSO	This paper considers a proportional hazards model, which allows one to examine the extent to which covariates interact nonlinearly with an exposure variable, for analysis of lifetime data. A local partial-likelihood technique is proposed to estimate nonlinear interactions. Asymptotic normality of the proposed estimator is established. The baseline hazard function, the bias and the variance of the local likelihood estimator are consistently estimated. In addition, a one-step local partial-likelihood estimator is presented to facilitate the computation of the proposed procedure and is demonstrated to be as efficient as the fully iterated local partial-likelihood estimator. Furthermore, a penalized local likelihood estimator is proposed to select important risk variables in the model. Numerical examples are used to illustrate the effectiveness of the proposed procedures.	Chinese Univ Hong Kong, Dept Stat, Shatin, Hong Kong, Peoples R China; Princeton Univ, Dept Operat Res & Financial Engn, Princeton, NJ 08540 USA; Chinese Acad Sci, Inst Appl Math, Acad Math & Syst Sci, Beijing 100080, Peoples R China; Sichuan Univ, Sch Math, Chengdu 610064, Peoples R China	Fan, JQ (reprint author), Chinese Univ Hong Kong, Dept Stat, Shatin, Hong Kong, Peoples R China.	jqfan@princeton.edu; yzhou@amss.ac.cn					ANDERSEN PK, 1982, ANN STAT, V10, P1100, DOI 10.1214/aos/1176345976; Andersen P.K., 1993, STAT MODELS BASED CO; BREIMAN L, 1995, TECHNOMETRICS, V37, P373, DOI 10.2307/1269730; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Antoniadis A, 2001, J AM STAT ASSOC, V96, P939, DOI 10.1198/016214501753208942; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; Hoover DR, 1998, BIOMETRIKA, V85, P809, DOI 10.1093/biomet/85.4.809; ZUCKER DM, 1990, ANN STAT, V18, P329, DOI 10.1214/aos/1176347503; Carroll RJ, 1997, J AM STAT ASSOC, V92, P477, DOI 10.1080/01621459.1997.10474001; HASTIE T, 1993, J ROY STAT SOC B MET, V55, P757; BJERVE S, 1985, SCAND J STAT, V12, P159; Breslow N., 1972, J R STAT SOC B, V34, P216; Brumback BA, 1998, J AM STAT ASSOC, V93, P961, DOI 10.2307/2669837; Cai ZW, 2003, SCAND J STAT, V30, P93, DOI 10.1111/1467-9469.00320; Cai ZW, 2000, J AM STAT ASSOC, V95, P888, DOI 10.2307/2669472; Cai ZW, 2000, J AM STAT ASSOC, V95, P941, DOI 10.2307/2669476; CHEN R, 1993, J AM STAT ASSOC, V88, P298, DOI 10.2307/2290725; Cleveland WS, 1992, STAT MODELS S, P309; Fan JQ, 1999, ANN STAT, V27, P1491; Fan JQ, 1997, ANN STAT, V25, P1661; Fan JQ, 1999, J ROY STAT SOC B, V61, P927, DOI 10.1111/1467-9868.00211; Fan JQ, 2002, ANN STAT, V30, P74; Gamerman D, 1998, BIOMETRIKA, V85, P215, DOI 10.1093/biomet/85.1.215; HARDLE W, 1989, J MULTIVARIATE ANAL, V29, P163, DOI 10.1016/0047-259X(89)90022-5; Harrington D. P., 1991, COUNTING PROCESSES S; HASTIE T, 1990, BIOMETRICS, V46, P1005, DOI 10.2307/2532444; JOHNSTON GJ, 1982, J MULTIVARIATE ANAL, V12, P402, DOI 10.1016/0047-259X(82)90074-4; MARTINUSSEN T, 2000, UNPUB EFFICIENT ESTI; Marzec L, 1997, BIOMETRIKA, V84, P901, DOI 10.1093/biomet/84.4.901; Morris C.N., 1994, WILEY S PRO, P231; MURPHY SA, 1993, SCAND J STAT, V20, P35; MURPHY SA, 1991, STOCH PROC APPL, V39, P153, DOI 10.1016/0304-4149(91)90039-F; Carroll RJ, 1998, J AM STAT ASSOC, V93, P214, DOI 10.2307/2669618; TIAN L, 2002, COX MODEL TIME VARYI; Tibshirani R, 1997, STAT MED, V16, P385, DOI 10.1002/(SICI)1097-0258(19970228)16:4<385::AID-SIM380>3.0.CO;2-3; Wu CO, 2000, STAT SINICA, V10, P433; Wu CO, 1998, J AM STAT ASSOC, V93, P1388, DOI 10.2307/2670054	37	37	39	2	5	INST MATHEMATICAL STATISTICS	BEACHWOOD	PO BOX 22718, BEACHWOOD, OH 44122 USA	0090-5364			ANN STAT	Ann. Stat.	FEB	2006	34	1					290	325		10.1214/009053605000000796		36	Statistics & Probability	Mathematics	043BR	WOS:000237574800011		
J	Leeb, H; Potscher, BM				Leeb, H; Potscher, BM			Performance limits for estimators of the risk or distribution of shrinkage-type estimators, and some general lower risk-bound results	ECONOMETRIC THEORY			English	Article							MULTIVARIATE NORMAL-DISTRIBUTION; STEIN ESTIMATION; CONFIDENCE SETS; MODEL SELECTION; REGRESSION; CONVERGENCE; INFERENCE; EXISTENCE; LASSO	We consider the problem of estimating measures of precision of shrinkage-type estimators such as their risk or distribution. The notion of shrinkage-type estimators here refers to estimators such as the James-Stein estimator and Lasso-type estimators, in addition to "thresholding" estimators such as, e.g., Hodges' so-called superefficient estimator. Although the precision measures of such estimators typically can be estimated consistently, we show that they cannot be estimated uniformly consistently (even locally). This follows as a corollary to (locally) uniform lower bounds on the performance of estimators of the precision measures that we obtain in the paper. These lower bounds are typically quite large (e.g., they approach 1/2 or 1 depending on the situation considered). The analysis is based on some general lower risk bounds and related general results on the (non)existence of uniformly consistent estimators also obtained in the paper.	Univ Vienna, Dept Stat, A-1010 Vienna, Austria; Yale Univ, New Haven, CT 06520 USA	Potscher, BM (reprint author), Univ Vienna, Dept Stat, Univ Str 5, A-1010 Vienna, Austria.	Benedikt.Poetscher@univie.ac.at	Leeb, Hannes/C-9026-2014	Leeb, Hannes/0000-0002-5770-5955			Adkins L. C, 1992, ECONOMET REV, V11, P173, DOI 10.1080/07474939208800230; ADKINS LC, 1990, COMMUN STAT SIMULAT, V19, P401, DOI 10.1080/03610919008812864; Knight K, 2000, ANN STAT, V28, P1356; Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; FRANK IE, 1993, TECHNOMETRICS, V35, P109, DOI 10.2307/1269656; DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425; Bauer P., 1988, STATISTICS, V19, P39, DOI 10.1080/02331888808802068; Chen JS, 1997, CAN J STAT, V25, P545, DOI 10.2307/3315347; DONOHO DL, 1991, ANN STAT, V19, P633, DOI 10.1214/aos/1176348114; DONOHO DL, 1987, GEOMETRIZING RATES C, V1; GEORGE EI, 1986, ANN STAT, V14, P188, DOI 10.1214/aos/1176349849; GEYER CJ, 1996, UNPUB ASYMPTOTICS CO; Huber C., 1997, FESTSCHRIFT LUCIEN C, P245; JENNRICH RI, 1986, TECHNOMETRICS, V28, P113, DOI 10.2307/1270447; Judge G., 1978, STAT IMPLICATIONS PR; Kubokawa T, 2002, J MULTIVARIATE ANAL, V82, P39, DOI 10.1006/jmva.2001.2020; LECAM L, 1973, ANN STAT, V1, P38, DOI 10.1214/aos/1193342380; LECAM L, 1960, ANN MATH STAT, V31, P140, DOI 10.1214/aoms/1177705993; Leeb H, 2005, ECONOMET THEOR, V21, P21, DOI 10.1017/S0266466605050036; LEEB H, 2003, UNPUB ONE ESTIMATE C; LU KL, 1989, ANN STAT, V17, P890, DOI 10.1214/aos/1176347149; Pfanzagl J, 1998, J STAT PLAN INFER, V75, P9, DOI 10.1016/S0378-3758(98)00121-9; POTSCHER BM, 1991, ECONOMET THEOR, V7, P163; Potscher BM, 2002, ECONOMETRICA, V70, P1035, DOI 10.1111/1468-0262.00318; SEN PK, 1986, COMMUN STAT THEORY, V15, P2245, DOI 10.1080/03610928608829246; STEIN CM, 1981, ANN STAT, V9, P1135, DOI 10.1214/aos/1176345632; van der Vaart A. W., 1998, ASYMPTOTIC STAT; VENTER JH, 1990, CAN J STAT, V18, P221, DOI 10.2307/3315453; Wan ATK, 2004, J STAT PLAN INFER, V119, P17, DOI 10.1016/S0378-3758(02)00406-8; YATRACOS YG, 1985, P AM MATH SOC, V94, P479; Yu B, 1997, FESTSCHRIFT LUCIEN C, P423	33	22	22	0	1	CAMBRIDGE UNIV PRESS	NEW YORK	40 WEST 20TH ST, NEW YORK, NY 10011-4211 USA	0266-4666			ECONOMET THEOR	Economet. Theory	FEB	2006	22	1					69	97		10.1017/S0266466606060038		29	Economics; Mathematics, Interdisciplinary Applications; Social Sciences, Mathematical Methods; Statistics & Probability	Business & Economics; Mathematics; Mathematical Methods In Social Sciences	997YZ	WOS:000234286300003		
J	Setakis, E; Stirnadel, H; Balding, DJ				Setakis, E; Stirnadel, H; Balding, DJ			Logistic regression protects against population structure in genetic association studies	GENOME RESEARCH			English	Article							MULTILOCUS GENOTYPE DATA; FAMILY-BASED ASSOCIATION; GENOMIC CONTROL; STRATIFICATION; INFERENCE; MODEL; MARKERS; IMPACT; DETECT	We conduct an extensive simulation Study to compare the merits of several methods for using null (unlinked) markers to protect against false positives due to cryptic substructure in population-based genetic association studies. The more sophisticated "structured association" methods perform well but are computationally demanding and rely on estimating the correct number of subpopulations. The simple and fast "genomic control" approach can lose power in certain scenarios. We find that procedures based on logistic regression that are flexible, computationally fast, and easy to implement also provide good protection against the effects of cryptic substructure, even though they do not explicitly model the population structure.	Univ London Imperial Coll Sci Technol & Med, Dept Epidemiol & Publ Hlth, London W2 1PG, England; GlaxoSmithKline Inc, Worldwide Epidemiol, Harlow CM19 5AW, Essex, England	Setakis, E (reprint author), Univ London Imperial Coll Sci Technol & Med, Dept Epidemiol & Publ Hlth, St Marys Campus, London W2 1PG, England.	e.setakis@imperial.ac.uk	Balding, David/G-9898-2011	Balding, David/0000-0002-1480-6115			Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Devlin B, 1999, BIOMETRICS, V55, P997, DOI 10.1111/j.0006-341X.1999.00997.x; Sasieni PD, 1997, BIOMETRICS, V53, P1253, DOI 10.2307/2533494; Pritchard JK, 2000, GENETICS, V155, P945; Van Steen K, 2005, NAT GENET, V37, P683, DOI 10.1038/ng1582; THOMSON G, 1995, AM J HUM GENET, V57, P487; Wang YT, 2005, HUM HERED, V59, P165, DOI 10.1159/000085940; Cardon LR, 2003, LANCET, V361, P598, DOI 10.1016/S0140-6736(03)12520-2; Freedman ML, 2004, NAT GENET, V36, P388, DOI 10.1038/ng1333; Pritchard JK, 1999, AM J HUM GENET, V65, P220, DOI 10.1086/302449; Clayton D, 2004, GENET EPIDEMIOL, V27, P415, DOI 10.1002/gepi.20032; Campbell CD, 2005, NAT GENET, V37, P868, DOI 10.1038/ng1607; Pritchard JK, 2000, AM J HUM GENET, V67, P170, DOI 10.1086/302959; Falush D, 2003, GENETICS, V164, P1567; Hudson RR, 2002, BIOINFORMATICS, V18, P337, DOI 10.1093/bioinformatics/18.2.337; Helgason A, 2005, NAT GENET, V37, P90, DOI 10.1038/ng1492; Bacanu SA, 2000, AM J HUM GENET, V66, P1933, DOI 10.1086/302929; Balding DJ, 2003, THEOR POPUL BIOL, V63, P221, DOI 10.1016/S0040-5809(03)00007-8; Cavalli-Sforza LL, 1996, HIST GEOGRAPHY HUMAN; Hao K, 2004, EUR J HUM GENET, V12, P1001, DOI 10.1038/sj.ejhg.5201273; Hoggart CJ, 2003, AM J HUM GENET, V72, P1492, DOI 10.1086/375613; KOHLER K, 2005, ANN HUM GENET, V69, P1; MARCHINI J, 2004, NAT GENET, V36, pS12; Satten GA, 2001, AM J HUM GENET, V68, P466, DOI 10.1086/318195; Zhu XF, 2002, GENET EPIDEMIOL, V23, P181, DOI 10.1002/gepi.0210	25	64	66	1	2	COLD SPRING HARBOR LAB PRESS, PUBLICATIONS DEPT	WOODBURY	500 SUNNYSIDE BLVD, WOODBURY, NY 11797-2924 USA	1088-9051			GENOME RES	Genome Res.	FEB	2006	16	2					290	296		10.1101/gr.4346306		7	Biochemistry & Molecular Biology; Biotechnology & Applied Microbiology; Genetics & Heredity	Biochemistry & Molecular Biology; Biotechnology & Applied Microbiology; Genetics & Heredity	009OP	WOS:000235122000017	16354752	
J	Wu, BL				Wu, BL			Differential gene expression detection and sample classification using penalized linear regression models	BIOINFORMATICS			English	Article							CDNA MICROARRAY; STATISTICAL-METHODS; SHRUNKEN CENTROIDS; CLASS PREDICTION; CANCER; NORMALIZATION; SHRINKAGE; DISCOVERY	Differential gene expression detection and sample classification using microarray data have received much research interest recently. Owing to the large number of genes p and small number of samples n (p > n), microarray data analysis poses big challenges for statistical analysis. An obvious problem owing to the 'large p small n' is over-fitting. Just by chance, we are likely to find some non-differentially expressed genes that can classify the samples very well. The idea of shrinkage is to regularize the model parameters to reduce the effects of noise and produce reliable inferences. Shrinkage has been successfully applied in the microarray data analysis. The SAM statistics proposed by Tusher et al. and the 'nearest shrunken centroid' proposed by Tibshirani et al. are ad hoc shrinkage methods. Both methods are simple, intuitive and prove to be useful in empirical studies. Recently Wu proposed the penalized t/F-statistics with shrinkage by formally using the L-1 penalized linear regression models for two-class microarray data, showing good performance. In this paper we systematically discussed the use of penalized regression models for analyzing microarray data. We generalize the two-class penalized t/F-statistics proposed by Wu to multi-class microarray data. We formally derive the ad hoc shrunken centroid used by Tibshirani et al. using the L-1 penalized regression models. And we show that the penalized linear regression models provide a rigorous and unified statistical framework for sample classification and differential gene expression detection.	Univ Minnesota, Sch Publ Hlth, Div Biostat, Minneapolis, MN 55455 USA	Wu, BL (reprint author), Univ Minnesota, Sch Publ Hlth, Div Biostat, A460 Mayo Bldg,MMC 303, Minneapolis, MN 55455 USA.	baolin@biostat.umn.edu					DeRisi J, 1996, NAT GENET, V14, P457; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Khan J, 2001, NAT MED, V7, P673, DOI 10.1038/89044; Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324; Lockhart DJ, 1996, NAT BIOTECHNOL, V14, P1675, DOI 10.1038/nbt1296-1675; Tusher VG, 2001, P NATL ACAD SCI USA, V98, P5116, DOI 10.1073/pnas.091062498; Alizadeh AA, 2000, NATURE, V403, P503, DOI 10.1038/35000501; Golub TR, 1999, SCIENCE, V286, P531, DOI 10.1126/science.286.5439.531; Bolstad BM, 2003, BIOINFORMATICS, V19, P185, DOI 10.1093/bioinformatics/19.2.185; Dudoit S, 2002, STAT SINICA, V12, P111; BENJAMINI Y, 1995, J ROY STAT SOC B MET, V57, P289; Tibshirani R, 2003, STAT SCI, V18, P104, DOI 10.1214/ss/1056397488; Pomeroy SL, 2002, NATURE, V415, P436, DOI 10.1038/415436a; Efron B, 2004, ANN STAT, V32, P407; Burges CJC, 1998, DATA MIN KNOWL DISC, V2, P121, DOI 10.1023/A:1009715923555; DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425; West M, 2001, P NATL ACAD SCI USA, V98, P11462, DOI 10.1073/pnas.201162998; Tibshirani R, 2002, P NATL ACAD SCI USA, V99, P6567, DOI 10.1073/pnas.082099299; Yang YH, 2002, NUCLEIC ACIDS RES, V30, DOI 10.1093/nar/30.4.e15; Chang C., 2001, LIBSVM LIB SUPPORT V; Kutner M. H., 2004, APPL LINEAR REGRESSI; Parmigiani G., 2003, ANAL GENE EXPRESSION; Speed T, 2003, STAT ANAL GENE EXPRE; STOREY JD, 2005, UW BIOSTATISTICS WOR, V260; Wu BL, 2005, BIOINFORMATICS, V21, P1565, DOI 10.1093/bioinformatics/bti217; Wu BL, 2003, BIOINFORMATICS, V19, P1636, DOI 10.1093/bioinformatics/btg210	26	13	14	1	1	OXFORD UNIV PRESS	OXFORD	GREAT CLARENDON ST, OXFORD OX2 6DP, ENGLAND	1367-4803			BIOINFORMATICS	Bioinformatics	FEB 15	2006	22	4					472	476		10.1093/bioinformatics/bti827		5	Biochemical Research Methods; Biotechnology & Applied Microbiology; Computer Science, Interdisciplinary Applications; Mathematical & Computational Biology; Statistics & Probability	Biochemistry & Molecular Biology; Biotechnology & Applied Microbiology; Computer Science; Mathematical & Computational Biology; Mathematics	011OG	WOS:000235277300013	16352654	
J	van Someren, EP; Vaes, BLT; Steegenga, WT; Sijbers, AM; Dechering, KJ; Reinders, MJT				van Someren, EP; Vaes, BLT; Steegenga, WT; Sijbers, AM; Dechering, KJ; Reinders, MJT			Least absolute regression network analysis of the murine osteoblast differentiation network	BIOINFORMATICS			English	Article							GENETIC REGULATORY NETWORKS; EXPRESSION DATA; CELLS; BONE; IDENTIFICATION; PROTEIN; RECONSTRUCTION; GROWTH	Motivation: We propose a reverse engineering scheme to discover genetic regulation from genome-wide transcription data that monitors the dynamic transcriptional response after a change in cellular environment. The interaction network is estimated by solving a linear model using simultaneous shrinking of the least absolute weights and the prediction error. Results: The proposed scheme has been applied to the murine C2C12 cell-line stimulated to undergo osteoblast differentiation. Results show that our method discovers genetic interactions that display significant enrichment of co-citation in literature. More detailed study showed that the inferred network exhibits properties and hypotheses that are consistent with current biological knowledge.	Delft Univ Technol, Dept Mediamet, NL-2600 GA Delft, Netherlands; Univ Nijmegen, Dept Appl Biol, Nijmegen, Netherlands; NV Organon, Target Discovery Unit, NL-5340 BH Oss, Netherlands	van Someren, EP (reprint author), Delft Univ Technol, Dept Mediamet, NL-2600 GA Delft, Netherlands.	E.P.vanSomeren@tudelft.nl					Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Guelzim N, 2002, NAT GENET, V31, P60, DOI 10.1038/ng873; Ying QL, 2003, CELL, V115, P281, DOI 10.1016/S0092-8674(03)00847-X; van Someren EP, 2003, SIGNAL PROCESS, V83, P763, DOI 10.1016/S0165-1684(02)00473-5; Segal E, 2003, NAT GENET, V34, P166, DOI 10.1038/ng1165; Arkin A, 1997, SCIENCE, V277, P1275, DOI 10.1126/science.277.5330.1275; Danielson KG, 1997, J CELL BIOL, V136, P729, DOI 10.1083/jcb.136.3.729; Horiuchi K, 1999, J BONE MINER RES, V14, P1239, DOI 10.1359/jbmr.1999.14.7.1239; Corsi A, 2002, J BONE MINER RES, V17, P1180, DOI 10.1359/jbmr.2002.17.7.1180; FRANCESCHI RT, 1994, J BONE MINER RES, V9, P843; Lee TI, 2002, SCIENCE, V298, P799, DOI 10.1126/science.1075090; Tavazoie S, 1999, NAT GENET, V22, P281; Lai CF, 2001, J BIOL CHEM, V276, P14443; Franceschi RT, 1999, CRIT REV ORAL BIOL M, V10, P40; Aubin JE, 1998, BIOCHEM CELL BIOL, V76, P899, DOI 10.1139/bcb-76-6-899; Blake JA, 2003, NUCLEIC ACIDS RES, V31, P193, DOI 10.1093/nar/gkg047; Chaussabel D., 2002, GENOME BIOL, P3; D'haeseleer P, 1999, PAC S BIOC, V4, P41; de Jong DS, 2004, J BONE MINER RES, V19, P947, DOI 10.1359/JBMR.040216; de Jong DS, 2004, BIOCHEM BIOPH RES CO, V320, P100, DOI 10.1016/j.bbrc.2004.05.150; Featherstone DE, 2002, BIOESSAYS, V24, P267, DOI 10.1002/bies.10054; Gardner TS, 2003, SCIENCE, V301, P102, DOI 10.1126/science.1081900; Grandvalet Y, 1998, INT C ART NEUR NETW; Guthke R, 2005, BIOINFORMATICS, V21, P1626, DOI 10.1093/bioinformatics/bti226; Hartemink A., 2001, PAC S BIOCOMPUT, V6, P422; Hashimoto RF, 2004, BIOINFORMATICS, V20, P1241, DOI 10.1093/bioinformatics/bth074; Jenssen TK, 2001, NAT GENET, V28, P21, DOI 10.1038/ng0501-21; Liao JC, 2003, P NATL ACAD SCI USA, V100, P15522, DOI 10.1073/pnas.2136632100; Marzia M, 2000, J CELL BIOL, V151, P311, DOI 10.1083/jcb.151.2.311; Pe'er D, 2001, BIOINFORMATICS S1, V17, P215; Peng Y, 2004, J BIOL CHEM, V279, P32941, DOI 10.1074/jbc.M403344200; RUNG J, 2002, BIOINFORMATICS, V18, P202; Vaes BLT, 2002, J BONE MINER RES, V17, P2106, DOI 10.1359/jbmr.2002.17.12.2106; van Berlo RJP, 2003, SIMUL-T SOC MOD SIM, V79, P689, DOI 10.1177/003754903040942; Van Someren E, 2001, P 2 INT C SYST BIOL, P222; VANSOMEREN EP, 2002, COMPUTATIONAL STAT A, P211; Wahde M, 2001, J COMPUT BIOL, V8, P429, DOI 10.1089/106652701752236223; Weaver D. C., 1999, PAC S BIOCOMPUT, V4, P112; Yue HB, 2001, NUCLEIC ACIDS RES, V29, DOI 10.1093/nar/29.8.e41	39	37	38	1	3	OXFORD UNIV PRESS	OXFORD	GREAT CLARENDON ST, OXFORD OX2 6DP, ENGLAND	1367-4803			BIOINFORMATICS	Bioinformatics	FEB 15	2006	22	4					477	484		10.1093/bioinformatics/bti816		8	Biochemical Research Methods; Biotechnology & Applied Microbiology; Computer Science, Interdisciplinary Applications; Mathematical & Computational Biology; Statistics & Probability	Biochemistry & Molecular Biology; Biotechnology & Applied Microbiology; Computer Science; Mathematical & Computational Biology; Mathematics	011OG	WOS:000235277300014	16332709	
J	Rabinowitz, M; Myers, L; Banjevic, M; Chan, A; Sweetkind-Singer, J; Haberer, J; McCann, K; Wolkowicz, R				Rabinowitz, M; Myers, L; Banjevic, M; Chan, A; Sweetkind-Singer, J; Haberer, J; McCann, K; Wolkowicz, R			Accurate prediction of HIV-1 drug response from the reverse transcriptase and protease amino acid sequences using sparse models created by convex optimization	BIOINFORMATICS			English	Article							ANTIRETROVIRAL THERAPY; RANDOMIZED TRIAL; NEURAL-NETWORKS; RESISTANCE; GENOTYPE; PHENOTYPE; RITONAVIR; SELECTION; AIDS	Motivation: Genotype-phenotype modeling problems are often overcomplete, or ill-posed, since the number of potential predictors-genes, proteins, mutations and their interactions-is large relative to the number of measured outcomes. Such datasets can still be used to train sparse parameter models that generalize accurately, by exerting a principle similar to Occam's Razor: When many possible theories can explain the observations, the most simple is most likely to be correct. We apply this philosophy to modeling the drug response of Type-1 Human Immunodeficiency Virus (HIV-1). Owing to the decreasing expense of genetic sequencing relative to in vitro phenotype testing, a statistical model that reliably predicts viral drug response from genetic data is an important tool in the selection of antiretroviral therapy (ART). The optimization techniques described will have application to many genotype-phenotype modeling problems for the purpose of enhancing clinical decisions. Results: We describe two regression techniques for predicting viral phenotype in response to ART from genetic sequence data. Both techniques employ convex optimization for the continuous subset selection of a sparse set of model parameters. The first technique, the least absolute shrinkage and selection operator, uses the l(1) norm loss function to create a sparse linear model; the second, the support vector machine with radial basis kernel functions, uses the epsilon-insensitive loss function to create a sparse non-linear model. The techniques are applied to predict the response of the HIV-1 virus to 10 reverse transcriptase inhibitor and 7 protease inhibitor drugs. The genetic data are derived from the HIV coding sequences for the reverse transcriptase and protease enzymes. When tested by cross-validation with actual laboratory measurements, these models predict drug response phenotype more accurately than models previously discussed in the literature, and other canonical techniques described here. Key features of the methods that enable this performance are the tendency to generate simple models where many of the parameters are zero, and the convexity of the cost function, which assures that we can find model parameters to globally minimize the cost function for a particular training dataset. Availability: Results, tables and figures are available at ftp://ftp.genesecurity.net Contact: mrabinowitz@genesecurity.net Supplementary information: An Appendix to accompany this article is available at Bioinformatics online.	Gene Secur Network, Palo Alto, CA USA; Stanford Univ, Dept Engn, Palo Alto, CA 94304 USA; Northwestern Univ, Sch Med, Chicago, IL USA; Stanford Univ, Med Ctr, Dept Microbiol & Immunol, Palo Alto, CA 94304 USA	Rabinowitz, M (reprint author), Gene Secur Network, Palo Alto, CA USA.	mrabinowitz@genesecurity.net					Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; O'Brien SJ, 2004, NAT GENET, V36, P565, DOI 10.1038/ng1369; Meynard JL, 2002, AIDS, V16, P727, DOI 10.1097/00002030-200203290-00008; Osborne MR, 2000, IMA J NUMER ANAL, V20, P389, DOI 10.1093/imanum/20.3.389; Beerenwinkel N, 2003, BIOINFORMATICS, V19, pI16, DOI 10.1093/bioinformatics/btg1001; Beerenwinkel N, 2002, P NATL ACAD SCI USA, V99, P8271, DOI 10.1073/pnas.112177799; Beerenwinkel N, 2003, NUCLEIC ACIDS RES, V31, P3850, DOI 10.1093/nar/gkg575; Breiman L., 1984, CLASSIFICATION REGRE; Burges C., 1998, TUTORIAL SUPPORT VEC; Carpenter CCJ, 2000, JAMA-J AM MED ASSOC, V283, P381, DOI 10.1001/jama.283.3.381; D'Aquila Richard T, 2003, Top HIV Med, V11, P92; De Luca A, 2003, J INFECT DIS, V187, P1934, DOI 10.1086/375355; De Luca A, 2004, ANTIVIR THER, V9, P583; DiRienzo AG, 2003, STAT MED, V22, P2785, DOI 10.1002/sim.1516; Draghici S, 2003, BIOINFORMATICS, V19, P98, DOI 10.1093/bioinformatics/19.1.98; EFRON B, 2003, ANN STAT, V32, P407; Fazel M., 2002, THESIS STANFORD U EL; FESSEL WJ, 2003, INT C ANT AG CHEM; Hadamard J., 1923, LECT CAUCHY PROBLEM; Hahs D, 2004, GUID US ANT AG HIV 1; Hastie T., 2001, ELEMENTS STAT LEARNI; Hastie TJ, 1996, GEN ADDITIVE MODELS; *HIVDB, 2004, HIV RT PROT SEQ DAT; Kooperberg C, 2001, GENET EPIDEMIOL, V21, pS626; Kuritzkes DR, 1996, AIDS, V10, P975, DOI 10.1097/00002030-199610090-00007; Mardia K. V., 1980, MULTIVARIATE ANAL; Molla A, 1996, NAT MED, V2, P760, DOI 10.1038/nm0796-760; Murray W., 1981, PRACTICAL OPTIMIZATI; Neter J, 1990, APPL LINEAR STAT MOD, V3rd; Kijak G H, 2003, HIV Med, V4, P72, DOI 10.1046/j.1468-1293.2003.00131.x; Ravela J, 2003, JAIDS-J ACQ IMM DEF, V33, P8; Rhee SY, 2003, NUCLEIC ACIDS RES, V31, P298, DOI 10.1093/nar/gkg100; RYAN TP, 1996, MODERN REGRESSION; Scholkopf B., 1999, ADV KERNEL METHODS S; SHAFER RW, 2004, NIAID ACTG DATA ANAL; Shafer RW, 2002, CLIN MICROBIOL REV, V15, P247, DOI 10.1128/CMR.15.2.247-277.2002; Sherr L, 2000, AIDS CARE, V12, P373, DOI 10.1080/09540120050123765; SINGER J, 2004, THESIS STANFORD U ST; UNAIDS, 2004, REP GLOB AIDS EP; Vandenberghe L., 2004, CONVEX OPTIMIZATION; Vapnik Vladimir, 1998, NATURE STAT LEARNING; Wang DC, 2003, J INFECT DIS, V188, P653, DOI 10.1086/377453; Wang K, 2004, ANTIVIR THER, V9, P343; Zolopa AR, 1999, ANN INTERN MED, V131, P813	44	14	14	0	0	OXFORD UNIV PRESS	OXFORD	GREAT CLARENDON ST, OXFORD OX2 6DP, ENGLAND	1367-4803			BIOINFORMATICS	Bioinformatics	MAR 1	2006	22	5					541	549		10.1093/bioinformatics/btk011		9	Biochemical Research Methods; Biotechnology & Applied Microbiology; Computer Science, Interdisciplinary Applications; Mathematical & Computational Biology; Statistics & Probability	Biochemistry & Molecular Biology; Biotechnology & Applied Microbiology; Computer Science; Mathematical & Computational Biology; Mathematics	016EV	WOS:000235604400005	16368772	
J	Ma, S; Kosorok, MR; Fine, JP				Ma, S; Kosorok, MR; Fine, JP			Additive risk models for survival data with high-dimensional covariates	BIOMETRICS			English	Article						additive risk model; principal component regression	PRINCIPAL COMPONENTS REGRESSION; PARTIAL LEAST-SQUARES; B-CELL LYMPHOMA; VARIABLE SELECTION; GENE-EXPRESSION; LIKELIHOOD; LASSO	As a useful alternative to Cox's proportional hazard model, the additive risk model assumes that the hazard function is the sum of the baseline hazard function and the regression function of covariates. This article is concerned with estimation and prediction for the additive risk models with right censored survival data, especially when the dimension of the covariates is comparable to or larger than the sample size. Principal component regression is proposed to give unique and numerically stable estimators. Asymptotic properties of the proposed estimators, component selection based on the weighted bootstrap, and model evaluation techniques are discussed. This approach is illustrated with analysis of the primary biliary cirrhosis clinical data and the diffuse large B-cell lymphoma genomic data. It is shown that this methodology is numerically stable and effective in dimension reduction, while still being able to provide satisfactory prediction and classification results.	Univ Washington, Dept Biostat, Seattle, WA 98195 USA; Univ Wisconsin, Dept Stat, Madison, WI 53792 USA; Univ Wisconsin, Dept Biostat & Med Informat, Madison, WI 53792 USA	Fine, JP (reprint author), Univ Washington, Dept Biostat, Seattle, WA 98195 USA.	fine@biostat.wisc.edu					Aalen O. O., 1980, LECTURE NOTES STATIS, V2, P1; LIN DY, 1994, BIOMETRIKA, V81, P61, DOI 10.1093/biomet/81.1.61; ROECKER E, 1991, TECHNOMETRICS, V33, P459, DOI 10.2307/1269417; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; Garber ME, 2001, P NATL ACAD SCI USA, V98, P13784, DOI 10.1073/pnas.241500798; Alizadeh AA, 2000, NATURE, V403, P503, DOI 10.1038/35000501; Segal MR, 2003, J COMPUT BIOL, V10, P961, DOI 10.1089/106652703322756177; Rosenwald A, 2002, NEW ENGL J MED, V346, P1937, DOI 10.1056/NEJMoa012914; Breslow N. E., 1987, STAT MODELS CANC RES, V2; BUCKLEY JD, 1984, BIOMETRICS, V40, P51, DOI 10.2307/2530743; Fan JQ, 2002, ANN STAT, V30, P74; Fleming T. R., 1991, COUNTING PROCESSES S; Hastie T, 2001, GENOME BIOL, V2; HELLAND IS, 1994, J AM STAT ASSOC, V89, P583, DOI 10.2307/2290861; Huang J, 2002, BIOMETRICS, V58, P781, DOI 10.1111/j.0006-341X.2002.00781.x; HUFFER FW, 1991, J AM STAT ASSOC, V86, P114, DOI 10.2307/2289721; Hwang JTG, 2003, TECHNOMETRICS, V45, P70, DOI 10.1198/004017002188618716; Jolliffe I. T., 1986, PRINCIPAL COMPONENT; KOLLO T, 1993, J MULTIVARIATE ANAL, V47, P283, DOI 10.1006/jmva.1993.1084; Lan H, 2003, GENETICS, V164, P1607; Li H., 2003, PAC S BIOC, P65; Li HZ, 2004, BIOINFORMATICS, V20, P208, DOI 10.1093/bioinformatics/bth900; MASON RL, 1985, STAT PROBABIL LETT, V3, P299, DOI 10.1016/0167-7152(85)90059-8; MCKEAGUE IW, 1994, BIOMETRIKA, V81, P501; Miller A. J., 1990, SUBSET SELECTION REG; Nguyen DV, 2002, BIOINFORMATICS, V18, P1625, DOI 10.1093/bioinformatics/18.12.1625; Park PJ, 2002, BIOINFORMATICS, V18, P120; PEPE MS, 2004, 198 UW; STONE M, 1990, J ROY STAT SOC B MET, V52, P237; Tibshirani R, 1997, STAT MED, V16, P385, DOI 10.1002/(SICI)1097-0258(19970228)16:4<385::AID-SIM380>3.0.CO;2-3	31	23	24	0	5	BLACKWELL PUBLISHING	OXFORD	9600 GARSINGTON RD, OXFORD OX4 2DQ, OXON, ENGLAND	0006-341X			BIOMETRICS	Biometrics	MAR	2006	62	1					202	210		10.1111/j.1541-0420.2005.00405.x		9	Biology; Mathematical & Computational Biology; Statistics & Probability	Life Sciences & Biomedicine - Other Topics; Mathematical & Computational Biology; Mathematics	026CX	WOS:000236315400026	16542247	
J	Huang, JZ; Liu, NP; Pourahmadi, M; Liu, LX				Huang, JZ; Liu, NP; Pourahmadi, M; Liu, LX			Covariance matrix selection and estimation via penalised normal likelihood	BIOMETRIKA			English	Article						cholesky decomposition; crossvalidation; LASSO; L-p penalty; model selection; penalised likelihood; shrinkage	LONGITUDINAL DATA; NONPARAMETRIC-ESTIMATION; NONORTHOGONAL PROBLEMS; VARIABLE SELECTION; RIDGE REGRESSION; CALL CENTER; MODELS; SHRINKAGE; LASSO	We propose a nonparametric method for identifying parsimony and for producing a statistically efficient estimator of a large covariance matrix. We reparameterise a covariance matrix through the modified Cholesky decomposition of its inverse or the one-step-ahead predictive representation of the vector of responses and reduce the nonintuitive task of modelling covariance matrices to the familiar task of model selection and estimation for a sequence of regression models. The Cholesky factor containing these regression coefficients is likely to have many off-diagonal elements that are zero or close to zero. Penalised normal likelihoods in this situation with L-1 and L-2 penalities are shown to be closely related to Tibshirani's (1996) LASSO approach and to ridge regression. Adding either penalty to the likelihood helps to produce more stable estimators by introducing shrinkage to the elements in the Cholesky factor, while, because of its singularity, the L-1 penalty will set some elements to zero and produce interpretable models. An algorithm is developed for computing the estimator and selecting the tuning parameter. The proposed maximum penalised likelihood estimator is illustrated using simulation and a real dataset involving estimation of a 102 x 102 covariance matrix.	Texas A&M Univ, Dept Stat, College Stn, TX 77843 USA; Univ Penn, Dept Stat, Philadelphia, PA 19104 USA; No Illinois Univ, Div Stat, De Kalb, IL 60115 USA; Columbia Univ, Mailman Sch Publ Hlth, Dept Biostat, New York, NY 10032 USA	Huang, JZ (reprint author), Texas A&M Univ, Dept Stat, College Stn, TX 77843 USA.	jianhua@stat.tamu.edu; nliu@wharton.upenn.edu; pourahm@math.niu.edu; lxliu@biostat.columbia.edu					Anderson T. W., 2003, INTRO MULTIVARIATE S; Fu WJJ, 1998, J COMPUT GRAPH STAT, V7, P397, DOI 10.2307/1390712; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; Shen HP, 2005, APPL STOCH MODEL BUS, V21, P251, DOI 10.1002/asmb.598; Wong F, 2003, BIOMETRIKA, V90, P809, DOI 10.1093/biomet/90.4.809; Brown L, 2005, J AM STAT ASSOC, V100, P36, DOI 10.1198/016214504000001808; HOERL AE, 1970, TECHNOMETRICS, V12, P55; FRANK IE, 1993, TECHNOMETRICS, V35, P109, DOI 10.2307/1269656; DEMPSTER AP, 1972, BIOMETRICS, V28, P157, DOI 10.2307/2528966; CRAVEN P, 1979, NUMER MATH, V31, P377; Pourahmadi M, 1999, BIOMETRIKA, V86, P677, DOI 10.1093/biomet/86.3.677; Boik RJ, 2002, BIOMETRIKA, V89, P159, DOI 10.1093/biomet/89.1.159; Chiu TYM, 1996, J AM STAT ASSOC, V91, P198, DOI 10.2307/2291396; Diggle P., 2002, ANAL LONGITUDINAL DA; Diggle PJ, 1998, BIOMETRICS, V54, P401, DOI 10.2307/3109751; HOERL AE, 1970, TECHNOMETRICS, V12, P69, DOI 10.2307/1267352; Hunter DR, 2005, ANN STAT, V33, P1617, DOI 10.1214/009053605000000200; LEDOIT O, 2004, J PORTFOLIO MANAG, V4, P110; LEONARD T, 1992, ANN STAT, V20, P1669, DOI 10.1214/aos/1176348885; Lin S. P., 1985, MULTIVARIATE ANAL, P411; Muirhead R., 1982, ASPECTS MULTIVARIATE; Ojelund H, 2001, J CHEMOMETR, V15, P497; Pourahmadi M, 2000, BIOMETRIKA, V87, P425, DOI 10.1093/biomet/87.2.425; Smith M, 2002, J AM STAT ASSOC, V97, P1141, DOI 10.1198/016214502388618942; Wu WB, 2003, BIOMETRIKA, V90, P831, DOI 10.1093/biomet/90.4.831	26	118	119	3	9	OXFORD UNIV PRESS	OXFORD	GREAT CLARENDON ST, OXFORD OX2 6DP, ENGLAND	0006-3444			BIOMETRIKA	Biometrika	MAR	2006	93	1					85	98		10.1093/biomet/93.1.85		14	Biology; Mathematical & Computational Biology; Statistics & Probability	Life Sciences & Biomedicine - Other Topics; Mathematical & Computational Biology; Mathematics	020PI	WOS:000235921600007		
J	Gatu, C; Kontoghiorghes, EJ				Gatu, C; Kontoghiorghes, EJ			Branch-and-bound algorithms for computing the best-subset regression models	JOURNAL OF COMPUTATIONAL AND GRAPHICAL STATISTICS			English	Article						least squares; QR decomposition; subset regression	QR DECOMPOSITION; PARALLEL STRATEGIES; LINEAR-REGRESSION; SELECTION; VARIABLES	An efficient branch-and-bound algorithm for computing the best-subset regression models is proposed. The algorithm avoids the computation of the whole regression tree that generates all possible subset models. It is formally shown that if the branch-and-bound test holds, then the current subtree together with its right-hand side subtrees are cut. This reduces significantly the computational burden of the proposed algorithm when compared to an existing leaps-and-bounds method which generates two trees. Specifically, the proposed algorithm, which is based on orthogonal transformations, outperforms by O(n(3)) the leaps-and-bounds strategy. The criteria used in identifying the best subsets are based on monotone functions of the residual sum of squares (RSS) such as R-2, adjusted R-2, mean square error of prediction, and C-p. Strategies and heuristics that improve the computational performance of the proposed algorithm are investigated. A computationally efficient heuristic version of the branch-and-bound strategy which decides to cut subtrees using a tolerance parameter is proposed. The heuristic algorithm derives models close to the best ones. However, it is shown analytically that the relative error of the RSS, and consequently the corresponding statistic, of the computed subsets is smaller than the value of the tolerance parameter which lies between zero and one. Computational results and experiments on random and real data arc presented and analyzed.	Univ Neuchatel, Dept Comp Sci, CH-2007 Neuchatel, Switzerland; Univ Cyprus, Dept Publ & Business Adm, Nicosia, Cyprus; Univ London Birkbeck Coll, Sch Comp Sci & Informat Syst, London WC1E 7HX, England	Gatu, C (reprint author), Univ Neuchatel, Dept Comp Sci, Emile Argand 11, CH-2007 Neuchatel, Switzerland.	cristian.gatu@unine.ch; erricos@ucy.ac.cy	Gatu, Cristian/C-4814-2011				BREIMAN L, 1995, TECHNOMETRICS, V37, P373, DOI 10.2307/1269730; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; FURNIVAL GM, 1974, TECHNOMETRICS, V16, P499, DOI 10.2307/1267601; ALLEN DM, 1971, TECHNOMETRICS, V13, P469, DOI 10.2307/1267161; LAWLER EL, 1966, OPER RES, V14, P699, DOI 10.1287/opre.14.4.699; CLARKE MRB, 1981, APPL STATIST, V30, P198, DOI 10.2307/2346398; EDWARDS D, 1987, J AM STAT ASSOC, V82, P205, DOI 10.2307/2289155; GATU C, 2005, COMPUTATIONAL MANAGE, V2, P253, DOI 10.1007/s10287-004-0021-x; GATU C, IN PRESS J EC DYNAMI; Gatu C, 2003, PARALLEL COMPUT, V29, P505, DOI 10.1016/S0167-8191(03)00019-X; Golub G., 1996, MATRIX COMPUTATIONS; GOODNIGHT JH, 1979, AM STAT, V33, P116; Hastie T., 2001, ELEMENTS STAT LEARNI; Hastie TJ, 1990, GEN ADDITIVE MODELS; HOCKING RR, 1976, BIOMETRICS, V32, P1, DOI 10.2307/2529336; HOCKING RR, 1983, TECHNOMETRICS, V25, P219, DOI 10.2307/1268603; HOCKING RR, 1967, TECHNOMETRICS, V9, P531, DOI 10.2307/1266192; HOCKING RR, 1972, TECHNOMETRICS, V14, P967, DOI 10.2307/1267145; Kontoghiorghes E., 2000, PARALLEL ALGORITHMS; Kontoghiorghes EJ, 2000, SIAM J MATRIX ANAL A, V22, P714; Kontoghiorghes EJ, 1999, ALGORITHMICA, V25, P58, DOI 10.1007/PL00009283; KONTOGHIORGHES EJ, 1993, PARALLEL ALGORITHMS, V2, P243; KONTOGHIORGHES EJ, 1995, PARALLEL ALGORITHMS, V5, P229; KONTOGHIORGHES EJ, 1993, PARALLEL COMPUT, V6, P703; LAMOTTE LR, 1970, TECHNOMETRICS, V12, P83, DOI 10.2307/1267353; LEE EK, 2002, HDB APPL OPTIMIZATIO, P53; MALLOWS CL, 1995, TECHNOMETRICS, V37, P362, DOI 10.2307/1269729; Miller A, 2002, SUBSET SELECTION REG, V2nd; MILLER AJ, 1992, APPL STAT-J ROY ST C, V41, P458, DOI 10.2307/2347583; MILLER AJ, 1984, J ROY STAT SOC A STA, V147, P389, DOI 10.2307/2981576; NARENDRA P, 1977, IEEE T COMPUT, V26, P917; RIDOUT MS, 1988, APPL STAT-J ROY ST C, V37, P139, DOI 10.2307/2347512; ROBERTS SJ, 1984, APPL STAT, V33, P236, DOI 10.2307/2347457; SMITH DM, 1989, COMPUT STAT DATA AN, V7, P217, DOI 10.1016/0167-9473(89)90023-6; Sober G., 1977, LINEAR REGRESSION AN; Winker P., 2001, OPTIMIZATION HEURIST; ZELLNER D, 2004, COMMUNICATIONS STAT, V3, P787	38	31	31	2	5	AMER STATISTICAL ASSOC	ALEXANDRIA	732 N WASHINGTON ST, ALEXANDRIA, VA 22314-1943 USA	1061-8600	1537-2715		J COMPUT GRAPH STAT	J. Comput. Graph. Stat.	MAR	2006	15	1					139	156		10.1198/106186006X100290		18	Statistics & Probability	Mathematics	021AC	WOS:000235953700008		
J	Mukherjee, S; Zhou, DX				Mukherjee, S; Zhou, DX			Learning coordinate covariances via gradients	JOURNAL OF MACHINE LEARNING RESEARCH			English	Article						tikhnonov regularization; variable selection; reproducing kernel Hilbert space; generalization bounds	SUPPORT VECTOR MACHINES/; CLASSIFICATION; SELECTION; NETWORKS; CANCER	We introduce an algorithm that learns gradients from samples in the supervised learning framework. An error analysis is given for the convergence of the gradient estimated by the algorithm to the true gradient. The utility of the algorithm for the problem of variable selection as well as determining variable covariance is illustrated on simulated data as well as two gene expression data sets. For square loss we provide a very efficient implementation with respect to both memory and time.	Duke Univ, Dept Comp Sci, Inst Genome Sci & Policy, Inst Stat & Decis Sci, Durham, NC 27708 USA; City Univ Hong Kong, Dept Math, Kowloon, Hong Kong, Peoples R China	Mukherjee, S (reprint author), Duke Univ, Dept Comp Sci, Inst Genome Sci & Policy, Inst Stat & Decis Sci, Durham, NC 27708 USA.	SAYAN@STAT.DUKE.EDU; MAZHOU@CITYU.EDU.HK		Mukherjee, Sayan/0000-0002-6715-3920			Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Chapelle O, 2002, MACH LEARN, V46, P131, DOI 10.1023/A:1012450327387; Micchelli CA, 2005, NEURAL COMPUT, V17, P177, DOI 10.1162/0899766052530802; Golub TR, 1999, SCIENCE, V286, P531, DOI 10.1126/science.286.5439.531; ARONSZAJN N, 1950, T AM MATH SOC, V68, P337; Guyon I, 2002, MACH LEARN, V46, P389, DOI 10.1023/A:1012487302797; Evgeniou T, 2000, ADV COMPUT MATH, V13, P1, DOI 10.1023/A:1018946025316; Wu Q, 2005, NEURAL COMPUT, V17, P1160, DOI 10.1162/0899766053491896; WAHBA G, 1980, MON WEATHER REV, V108, P1122, DOI 10.1175/1520-0493(1980)108<1122:SNMMFV>2.0.CO;2; Smale S, 2003, ANAL APPL, V1, P17, DOI 10.1142/S0219530503000089; CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; Belkin M, 2004, MACH LEARN, V56, P209, DOI 10.1023/B:MACH.0000033120.25363.1e; Carrel L, 1999, P NATL ACAD SCI USA, V96, P14440, DOI 10.1073/pnas.96.25.14440; Chen S. S., 1999, SIAM J SCI COMPUT, V20, P33; Cucker F., 2001, B AM MATH SOC, V39, P1, DOI 10.1090/S0273-0979-01-00923-5; De Vito E, 2005, FOUND COMPUT MATH, V5, P59, DOI 10.1007/s10208-004-0134-1; DISTECHE C, 2002, CYTOGENET GENOME RES, V99, P35; Lee YK, 2004, J AM STAT ASSOC, V99, P67, DOI 10.1198/016214504000000098; LIAO M, 2005, THESIS DUKE U DURHAM; LIAO M, 2005, BAYESIAN KERNEL REGR; Pinelis I., 1994, ANN PROBAB, V22, P1679, DOI DOI 10.1214/A0P/1176988477; Pinelis I, 1999, ANN PROBAB, V27, P2119; POGGIO T, 1990, SCIENCE, V247, P978, DOI 10.1126/science.247.4945.978; Schoelkopf B., 2001, LEARNING KERNELS SUP; Slonim D.K., 2000, P 4 ANN INT C COMP M, P263, DOI 10.1145/332306.332564; SMALE, 2006, APPL COMPUT HARMONIC, V19, P285; SMALE S, 2006, CONSTR APPROX, P24; Smale S, 2004, B AM MATH SOC, V41, P279, DOI 10.1090/S0273-0979-04-01025-0; Subramanian A, 2005, P NATL ACAD SCI US; Sweet-Cordero A, 2005, NAT GENET, V37, P48, DOI 10.1038/ng1490; VAPNIK VN, 1908, STAT LEARNING THEORY; West M., 2003, BAYESIAN STAT, P723; Zhang T, 2003, NEURAL COMPUT, V15, P1397, DOI 10.1162/089976603321780326; Zhou DX, 2003, IEEE T INFORM THEORY, V49, P1743, DOI 10.1109/TIT.2003.813564	34	31	31	0	4	MICROTOME PUBLISHING	BROOKLINE	31 GIBBS STREET, BROOKLINE, MA 02446 USA	1532-4435			J MACH LEARN RES	J. Mach. Learn. Res.	MAR	2006	7						519	549				31	Automation & Control Systems; Computer Science, Artificial Intelligence	Automation & Control Systems; Computer Science	040DK	WOS:000237359000002		
J	Bair, E; Hastie, T; Paul, D; Tibshirani, R				Bair, E; Hastie, T; Paul, D; Tibshirani, R			Prediction by supervised principal components	JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION			English	Article						gene expression; microarray; regression; survival analysis	GENE-EXPRESSION DATA; SUFFICIENT DIMENSION REDUCTION; REGRESSION; SURVIVAL; DECOMPOSITION; SELECTION; CANCER; LASSO	In regression problems where the number of predictors greatly exceeds the number of observations, conventional regression techniques may produce unsatisfactory results. We describe a technique called supervised principal components that call be applied to this type of problem. Supervised principal components is similar to conventional principal components analysis except that it uses a subset of the predictors selected based on their association with the outcome. Supervised principal components can be applied to regression and generalized regression problems, such as survival analysis. It compares favorably to other techniques for this type of problem, and can also account for the effects of other covariates and help identify which predictor variables are most important. We also provide asymptotic consistency results to help support our empirical findings. These methods could become important tools for DNA microarray data. where they may be used to more accurately diagnose and treat cancer.	Univ Calif San Francisco, Dept Neurol, San Francisco, CA 94143 USA; Stanford Univ, Dept Stat, Stanford, CA 94305 USA; Stanford Univ, Dept Hlth Res & Policy, Stanford, CA 94305 USA		ebair@stat.stanford.edu; hastie@stat.stanford.edu; debashis@stat.stanford.edu; tibs@stat.stanford.edu		Bair, Eric/0000-0001-8733-7869			Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; HOERL AE, 1970, TECHNOMETRICS, V12, P55; Bullinger L, 2004, NEW ENGL J MED, V350, P1605, DOI 10.1056/NEJMoa031046; FRANK IE, 1993, TECHNOMETRICS, V35, P109, DOI 10.2307/1269656; Beer DG, 2002, NAT MED, V8, P816, DOI 10.1038/nm733; Efron B, 2004, ANN STAT, V32, P407; Tibshirani R, 2002, P NATL ACAD SCI USA, V99, P6567, DOI 10.1073/pnas.082099299; Alter O, 2000, P NATL ACAD SCI USA, V97, P10101, DOI 10.1073/pnas.97.18.10101; Rosenwald A, 2002, NEW ENGL J MED, V346, P1937, DOI 10.1056/NEJMoa012914; Antoniadis A, 2003, BIOINFORMATICS, V19, P563, DOI 10.1093/bioinformatics/btg062; Bair E, 2004, PLOS BIOL, V2, P511, DOI 10.1371/journal.pbio.00200108; Bura E, 2003, BIOINFORMATICS, V19, P1252, DOI 10.1093/bioinformatics/btg150; Chiaromonte F, 2002, MATH BIOSCI, V176, P123, DOI 10.1016/S0025-5564(01)00106-7; Chiaromonte F, 2002, ANN STAT, V30, P475, DOI 10.1214/aos/1021379862; Cook RD, 2004, ANN STAT, V32, P1062, DOI 10.1214/009053604000000292; DUAN N, 1991, ANN STAT, V19, P505, DOI 10.1214/aos/1176348109; GHOSH D, 2002, PACIFIC S BIOCOMPUT, V7, P18; Hastie T, 2000, GENOME BIOL, V1, P1, DOI DOI 10.1186/GB-2000-1-2-RESEARCH0003; HASTIE T, 2003, EFFICIENT QUADRATIC; HI H, 2004, BIOINFORMATICS, V5, P1208; Jiang H., 2004, BMC BIOINFORMATICS, V5, P1; JOHNSTONE I, 2001, FESTSCHRIFT WR VANZW, P399; JOHNSTONE I, 2006, IN PRESS J AM STAT A; Kneip A, 2001, J AM STAT ASSOC, V96, P519, DOI 10.1198/016214501753168235; LI KC, 1991, J AM STAT ASSOC, V86, P316, DOI 10.2307/2290563; LU AY, 2002, SPARSE PRINCIPAL COM; Mardia KV, 1979, MULTIVARIATE ANAL; Miller R. G., 1986, ANOVA BASICS APPL ST; Nguyen DV, 2002, BIOINFORMATICS, V18, P1625, DOI 10.1093/bioinformatics/18.12.1625; PAUL D., 2005, THESIS STANFORD U; Silverstein J. W., 2004, EIGENVALUES LARGE SA; Tibshirani R, 1997, STAT MED, V16, P385, DOI 10.1002/(SICI)1097-0258(19970228)16:4<385::AID-SIM380>3.0.CO;2-3; Van Huffel S., 2002, TOTAL LEAST SQUARES; van't Veer LJ, 2002, NATURE, V415, P530, DOI 10.1038/415530a; VONLUXBURG U, 2002, COMPRESSION APPROACH; Wold H., 1975, PERSPECTIVES PROBABI, P117; ZHAO H, 2006, PLOS MED, V3, P1	38	168	171	8	25	AMER STATISTICAL ASSOC	ALEXANDRIA	1429 DUKE ST, ALEXANDRIA, VA 22314 USA	0162-1459			J AM STAT ASSOC	J. Am. Stat. Assoc.	MAR	2006	101	473					119	137		10.1198/016214505000000628		19	Statistics & Probability	Mathematics	021BU	WOS:000235958400016		
J	Amato, U; Antoniadis, A; Pensky, M				Amato, U; Antoniadis, A; Pensky, M			Wavelet kernel penalized estimation for non-equispaced design regression	STATISTICS AND COMPUTING			English	Article						reproducing kernel; wavelet decomposition; penalization; Besov spaces; smoothing splines ANOVA; entropy	SMOOTHING SPLINE ANOVA; BESOV-SPACES; NOISY DATA; SHRINKAGE; REGULARIZATION; INTERPOLATION; SERIES	The paper considers regression problems with univariate design points. The design points are irregular and no assumptions on their distribution are imposed. The regression function is retrieved by a wavelet based reproducing kernel Hilbert space (RKHS) technique with the penalty equal to the sum of blockwise RKHS norms. In order to simplify numerical optimization, the problem is replaced by an equivalent quadratic minimization problem with an additional penalty term. The computational algorithm is described in detail and is implemented with both the sets of simulated and real data. Comparison with existing methods showed that the technique suggested in the paper does not oversmooth the function and is superior in terms of the mean squared error. It is also demonstrated that under additional assumptions on design points the method achieves asymptotic optimality in a wide range of Besov spaces.	Univ Cent Florida, Dept Stat, Orlando, FL 32816 USA; CNR, Ist Applicaz Calcolo M Picone, Sez Napoli, I-80131 Naples, Italy; Univ Grenoble 1, Lab IMAG LMC, F-38041 Grenoble, France	Pensky, M (reprint author), Univ Cent Florida, Dept Stat, Orlando, FL 32816 USA.	mpensky@pegasus.cc.ucf.edu	Amato, Umberto/N-6874-2013	Amato, Umberto/0000-0003-1482-4898			Abramovich F., 2000, STATISTICIAN, V49, P1; AMATO U, 1997, REV ROUMAINE MATH PU, V42, P481; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Antoniadis A, 2001, J AM STAT ASSOC, V96, P939, DOI 10.1198/016214501753208942; ARONSZAJN N, 1950, T AM MATH SOC, V68, P337; Cai TT, 1999, ANN STAT, V27, P898, DOI 10.1214/aos/1018031262; CRAVEN P, 1979, NUMER MATH, V31, P377; SILVERMAN BW, 1985, J R STAT SOC B, V47, P1; Antoniadis A, 1996, SCAND J STAT, V23, P313; Antoniadis Anestis, 2001, J STAT SOFTWARE, V6; Birge L, 2000, CONSTR APPROX, V16, P1, DOI 10.1007/s003659910001; Birman M. Sh., 1967, MAT SBORNIK, V2, P295; BRINKMAN N, 1981, SAE T, V90, P1414; Cai T. T., 2001, SANKHYA, V63, P127; Cai TT, 2001, J AM STAT ASSOC, V96, P960; Cai TT, 1998, ANN STAT, V26, P1783; CANU S, 2003, NATO SCI 3, V90, P89; Daubechies I., 1992, 10 LECT WAVELETS; Delouille V., 2001, SANKHYA A, V63, P328; DEVORE RA, 1988, T AM MATH SOC, V305, P397, DOI 10.2307/2001060; DONOHO DL, 2004, STABLE RECOVERY SPAR; DONOHO DL, 1995, J ROY STAT SOC B MET, V57, P301; Eubank R.L., 1988, SPLINE SMOOTHING NON; Gradshtein I. S., 1980, TABLES INTEGRALS SER; Green P, 1994, NONPARAMETRIC REGRES; GUNN SR, 2002, MACH LEARN, V48, P115; Hall P, 1997, ANN STAT, V25, P1912; Hall P, 1999, STAT SINICA, V9, P33; Hardle W., 1998, LECT NOTES STAT, V129; Karlovitz L. A., 1970, J APPROXIMATION THEO, V37, P123, DOI 10.1016/0021-9045(70)90019-5; KERKYACHARIAN G, 2003, ESAIM P S, V7, P239, DOI 10.1051/ps:2003011; KIMELDOR.G, 1971, J MATH ANAL APPL, V33, P82, DOI 10.1016/0022-247X(71)90184-3; Kohler M, 2003, J STAT PLAN INFER, V115, P491, DOI 10.1016/S0378-3758(02)00158-1; Kovac A, 2000, J AM STAT ASSOC, V95, P172, DOI 10.2307/2669536; Lin XW, 2000, ANN STAT, V28, P1570; Lin Y., 2003, COMPONENT SELECTION; Loubes JM, 2002, STAT NEERL, V56, P454, DOI 10.1111/1467-9574.00212; Mallat S., 1999, WAVELET TOUR SIGNAL; Meyer Y., 1992, WAVELETS OPERATORS; Nason GP, 2002, STAT COMPUT, V12, P219, DOI 10.1023/A:1020746709500; NASON GP, 1998, WAVETHRESH3 SOFTWARE; Sardy S, 1999, STAT COMPUT, V9, P65, DOI 10.1023/A:1008818328241; Tapia R.A., 1978, NONPARAMETRIC PROBAB; Triebel H., 1983, THEORY FUNCTION SPAC; van de Geer S, 2000, EMPIRICAL PROCESSES; Vidakovic B, 1999, STAT MODELING WAVELE; WAHBA G, 1990, SIAM CBMS NSF REG C, V59; Wahba G, 1995, ANN STAT, V23, P1865; ZHANG H, 2002, VARIABLE SELECTION M	49	17	19	0	2	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0960-3174			STAT COMPUT	Stat. Comput.	MAR	2006	16	1					37	55		10.1007/s11222-006-5283-4		19	Computer Science, Theory & Methods; Statistics & Probability	Computer Science; Mathematics	017DO	WOS:000235674500004		
J	Reineking, B; Schroder, B				Reineking, B; Schroder, B			Constrain to perform: Regularization of habitat models	ECOLOGICAL MODELLING			English	Article						regularization; habitat models; logistic regression; Lasso; Ridge; penalized maximum likelihood; prediction; model selection; variable selection	LOGISTIC-REGRESSION ANALYSIS; SPECIES DISTRIBUTION MODELS; VARIABLE SELECTION; CLIMATE-CHANGE; NEURAL-NETWORKS; EURASIAN LYNX; SUITABILITY; PREDICTION; CONSERVATION; POPULATION	Predictive habitat models are an important tool for ecological research and conservation. A major cause of unreliable models is excessive model complexity, and regularization methods aim to improve the predictive performance by adequately constraining model complexity. We compare three regularization methods for logistic regression: variable selection, lasso, and ridge. They differ in the way model complexity is measured: variable selection uses the number of estimated parameters, the lasso uses the sum of the absolute values of the parameter estimates, and the ridge uses the sum of the squared values of the parameter estimates. We performed a simulation study with environmental data of a real landscape and artificial species occupancy data. We investigated the effect of three factors on relative model performance: (1) the number of parameters (16, 10, 6, 2) in the 'true' model that determined the distribution of the artificial species, (2) the prevalence, i.e. the proportion of sites occupied by the species, and (3) the sample size (measured in events per variable, EPV). Regularization improved model discrimination and calibration. However, no regularization method performed best under all circumstances: the ridge generally performed best in the 16-parameter scenario. The lasso generally performed best in the 10-parameter scenario. Variable selection with AIC was best at large sample sizes (EPV >= 10) when less than half of the variables influenced the species distribution. However, at low sample sizes (EPV < 10), ridge and lasso always performed best, regardless of the parameter scenario or prevalence. Overall, calibration was best in ridge models. Other methods showed overconfidence, particularly at low sample sizes. The percentage of correctly identified models was low for both lasso and variable selection. Variable selection should be used with caution. Although it can produce the best performing models under certain conditions, these situations are difficult to infer from the data. Ridge and lasso are risk-averse model strategies that can be expected to perform well under a wide range of underlying species-habitat relationships, particularly at small sample sizes. (C) 2005 Elsevier B.V. All rights reserved.	Swiss Fed Inst Technol, Dept Environm Sci, CH-8092 Zurich, Switzerland; UFZ Helmholtz Ctr Environm Res, Ctr Environm Res Leipzig Halle, Dept Ecol Modelling, D-04301 Leipzig, Germany; Univ Potsdam, Inst Geoecol, D-14415 Potsdam, Germany	Schroder, B (reprint author), Swiss Fed Inst Technol, Dept Environm Sci, Univ Str 22, CH-8092 Zurich, Switzerland.	bjoern.reineking@env.ethz.ch; boschroe@rz.uni-potsdam.de	Schroder, Boris/B-7211-2009; 	Schroder, Boris/0000-0002-8577-7980; Reineking, Bjorn/0000-0001-5277-9181			AKAIKE H, 1974, IEEE T AUTOMAT CONTR, VAC19, P716, DOI 10.1109/TAC.1974.1100705; AKCAKAYA HR, 1995, BIOL CONSERV, V73, P169, DOI 10.1016/0006-3207(95)00054-8; Guisan A, 2000, ECOL MODEL, V135, P147, DOI 10.1016/S0304-3800(00)00354-9; Manel S, 2001, J APPL ECOL, V38, P921, DOI 10.1046/j.1365-2664.2001.00647.x; Wintle BA, 2003, CONSERV BIOL, V17, P1579, DOI 10.1111/j.1523-1739.2003.00614.x; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Debeljak M, 2001, ECOL MODEL, V138, P321, DOI 10.1016/S0304-3800(00)00411-7; Hoeting JA, 1999, STAT SCI, V14, P382; Steyerberg EW, 2001, MED DECIS MAKING, V21, P45; Pearce J, 2000, ECOL MODEL, V133, P225, DOI 10.1016/S0304-3800(00)00322-7; PRIESTLEY CHB, 1972, MON WEATHER REV, V100, P81, DOI 10.1175/1520-0493(1972)100<0081:OTAOSH>2.3.CO;2; Berry PM, 2002, GLOBAL ECOL BIOGEOGR, V11, P453, DOI 10.1046/j.1466-822x.2002.00304.x; Thomas CD, 2004, NATURE, V427, P145, DOI 10.1038/nature02121; Mac Nally R, 2002, BIODIVERS CONSERV, V11, P1397, DOI 10.1023/A:1016250716679; Stockwell DRB, 2002, ECOL MODEL, V148, P1, DOI 10.1016/S0304-3800(01)00388-X; Brown PJ, 2002, J R STAT SOC B, V64, P519, DOI 10.1111/1467-9868.00348; Pearce J, 2000, ECOL MODEL, V128, P127, DOI 10.1016/S0304-3800(99)00227-6; Railsback SF, 2003, ECOL APPL, V13, P1580, DOI 10.1890/02-5051; Moisen GG, 2002, ECOL MODEL, V157, P209, DOI 10.1016/S0304-3800(02)00197-7; SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136; Austin MP, 2002, ECOL MODEL, V157, P101, DOI 10.1016/S0304-3800(02)00205-3; Fielding AH, 1997, ENVIRON CONSERV, V24, P38, DOI 10.1017/S0376892997000088; Bakkenes M, 2002, GLOBAL CHANGE BIOL, V8, P390, DOI 10.1046/j.1354-1013.2001.00467.x; Kramer-Schadt S, 2004, J APPL ECOL, V41, P711, DOI 10.1111/j.0021-8901.2004.00933.x; Hirzel AH, 2001, ECOL MODEL, V145, P111, DOI 10.1016/S0304-3800(01)00396-9; Harrell FE, 1996, STAT MED, V15, P361, DOI 10.1002/(SICI)1097-0258(19960229)15:4<361::AID-SIM168>3.0.CO;2-4; Avalos M, 2003, LECT NOTES COMPUT SC, V2810, P509, DOI 10.1007/978-3-540-45231-7_47; Beven K. J., 1997, HYDROL SCI B, V24, P43, DOI DOI 10.1080/02626667909491834; Bradshaw CJA, 2002, ECOL MODEL, V148, P111, DOI 10.1016/S0304-3800(01)00425-2; Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.1023/A:1018054314350; Fernandez N, 2003, ECOL APPL, V13, P1310, DOI 10.1890/02-5081; EFRON B, 1986, J AM STAT ASSOC, V81, P461, DOI 10.2307/2289236; Elith J, 2002, PREDICTING SPECIES OCCURRENCES: ISSUES OF ACCURACY AND SCALE, P303; Harrell FE, 2001, REGRESSION MODELING; Hastie T., 2001, ELEMENTS STAT LEARNI; HOSMER DW, 1989, BIOMETRICS, V45, P1265, DOI 10.2307/2531779; KUBINYI H, 1994, QUANT STRUCT-ACT REL, V13, P285, DOI 10.1002/qsar.19940130306; LENG C, 2004, 1091 U WISC DEP STAT; LISCHKE H, 1998, VIEW ALPS REGIONAL P, P309; Lucic B, 1999, J CHEM INF COMP SCI, V39, P610, DOI 10.1021/ci980161a; MACNALLY R, 2000, BIODIVERS CONSERV, V9, P555; Manel S, 1999, ECOL MODEL, V120, P337, DOI 10.1016/S0304-3800(99)00113-1; MEINSHAUSEN N, 2004, SEM STAT SWISS FED I; Mitasova H, 2002, OPEN SOURCE GIS GRAS; Oppel S, 2004, BIOL CONSERV, V118, P33, DOI 10.1016/j.biocon.2003.07.006; Peduzzi P, 1996, J CLIN EPIDEMIOL, V49, P1373, DOI 10.1016/S0895-4356(96)00236-3; R Development Core Team, 2003, R LANG ENV STAT COMP; Richerson MA, 1999, INFECT DIS CLIN PRAC, V8, P195, DOI 10.1097/00019048-199905000-00008; Ripley B. D., 1996, PATTERN RECOGNITION; SAXTON KE, 1986, SOIL SCI SOC AM J, V50, P1031; Schadt S, 2002, J APPL ECOL, V39, P189, DOI 10.1046/j.1365-2664.2002.00700.x; Sondgerath D, 2002, LANDSCAPE ECOL, V17, P57, DOI 10.1023/A:1015237002145; Steyerberg EW, 2000, STAT MED, V19, P1059, DOI 10.1002/(SICI)1097-0258(20000430)19:8<1059::AID-SIM412>3.3.CO;2-S; SWETS JA, 1988, SCIENCE, V240, P1285, DOI 10.1126/science.3287615; Thuiller W, 2003, J VEG SCI, V14, P669, DOI 10.1658/1100-9233(2003)014[0669:GMVCTA]2.0.CO;2; Tibshirani R, 1997, STAT MED, V16, P385, DOI 10.1002/(SICI)1097-0258(19970228)16:4<385::AID-SIM380>3.0.CO;2-3; Tibshirani R, 1999, J ROY STAT SOC B, V61, P529, DOI 10.1111/1467-9868.00191; Tyre AJ, 2001, ECOL APPL, V11, P1722, DOI 10.2307/3061091; Wolpert DH, 1996, NEURAL COMPUT, V8, P1341, DOI 10.1162/neco.1996.8.7.1341; Yamada K, 2003, ECOL MODEL, V165, P251, DOI 10.1016/S0304-3800(03)00077-2; YANG Y, 2003, 200310 IOW STAT U DE	61	72	72	3	20	ELSEVIER SCIENCE BV	AMSTERDAM	PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS	0304-3800			ECOL MODEL	Ecol. Model.	MAR 15	2006	193	3-4					675	690		10.1016/j.ecolmodel.2005.10.003		16	Ecology	Environmental Sciences & Ecology	021WN	WOS:000236016700031		
J	Kato, T; Murata, Y; Miura, K; Asai, K; Horton, PB; Tsuda, K; Fujibuchi, W				Kato, T; Murata, Y; Miura, K; Asai, K; Horton, PB; Tsuda, K; Fujibuchi, W			Network-based de-noising improves prediction from microarray data	BMC BIOINFORMATICS			English	Article; Proceedings Paper	NIPS Workshop on New Problems and Methods in Computational Biology	DEC   18, 2004	Whistle, CANADA	NIPS			MYELOID-LEUKEMIA; CHEMOSENSITIVITY; CANCER; VALUES	Background: Prediction of human cell response to anti-cancer drugs (compounds) from microarray data is a challenging problem, due to the noise properties of microarrays as well as the high variance of living cell responses to drugs. Hence there is a strong need for more practical and robust methods than standard methods for real-value prediction. Results: We devised an extended version of the off-subspace noise-reduction (de-noising) method [1] to incorporate heterogeneous network data such as sequence similarity or protein-protein interactions into a single framework. Using that method, we first de-noise the gene expression data for training and test data and also the drug-response data for training data. Then we predict the unknown responses of each drug from the de-noised input data. For ascertaining whether de-noising improves prediction or not, we carry out 12-fold cross-validation for assessment of the prediction performance. We use the Pearson's correlation coefficient between the true and predicted response values as the prediction performance. De-noising improves the prediction performance for 65% of drugs. Furthermore, we found that this noise reduction method is robust and effective even when a large amount of artificial noise is added to the input data. Conclusion: We found that our extended off-subspace noise-reduction method combining heterogeneous biological data is successful and quite useful to improve prediction of human cell cancer dru responses from microarray data.	AIST Computat Biol Res Ctr, Koto Ku, Tokyo 1350064, Japan; Univ Tokyo, Grad Sch Frontier Sci, Kashiwa, Chiba 2778562, Japan; Tohoku Univ, Grad Sch Med, Div Biol Regulat & Oncol, Dept Surg,Aoba Ku, Sendai, Miyagi 9808574, Japan	Fujibuchi, W (reprint author), AIST Computat Biol Res Ctr, Koto Ku, 2-42 Aomi, Tokyo 1350064, Japan.	kato-tsuyoshi@aist.go.jp; yukio-m@surg1.med.tohoku.ac.jp; k-miura@surg1.med.tohoku.ac.jp; asai@cbrc.jp; horton-p@aist.go.jp; tsuda@cbrc.jp; fujibuchi-wataru@aist.go.jp		Miura, Koh/0000-0001-7303-9430			ALBERTS B, 2002, MOL BIOL CELL MICROA, V3; Troyanskaya O, 2001, BIOINFORMATICS, V17, P520, DOI 10.1093/bioinformatics/17.6.520; Staunton JE, 2001, P NATL ACAD SCI USA, V98, P10787, DOI 10.1073/pnas.191368598; [Anonymous], 1954, PSYCHOMETRIKA; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Furey TS, 2000, BIOINFORMATICS, V16, P906, DOI 10.1093/bioinformatics/16.10.906; Tipping ME, 1999, NEURAL COMPUT, V11, P443, DOI 10.1162/089976699300016728; Mariadason JM, 2003, CANCER RES, V63, P8791; Bo TH, 2004, NUCLEIC ACIDS RES, V32, DOI 10.1093/nar/gnh026; Gruvberger-Saal SK, 2004, MOL CANCER THER, V3, P161; Kaneta Y, 2002, JPN J CANCER RES, V93, P849; Okutsu J, 2002, MOL CANCER THER, V1, P1035; TSUDA K, 2004, ADV NEURAL INFORMATI, V16	13	1	2	3	4	BIOMED CENTRAL LTD	LONDON	MIDDLESEX HOUSE, 34-42 CLEVELAND ST, LONDON W1T 4LB, ENGLAND	1471-2105			BMC BIOINFORMATICS	BMC Bioinformatics	MAR 20	2006	7			1					S4	10.1186/1471-2105-7-S1-S4		11	Biochemical Research Methods; Biotechnology & Applied Microbiology; Mathematical & Computational Biology	Biochemistry & Molecular Biology; Biotechnology & Applied Microbiology; Mathematical & Computational Biology	032HJ	WOS:000236765200004	16723007	
J	Buhlmann, P				Buhlmann, Peter			Boosting for high-dimensional linear models	ANNALS OF STATISTICS			English	Article						binary classification; gene expression; Lasso; matching pursuit; over-complete dictionary; sparsity; variable selection; weak greedy algorithm	REGRESSION; SELECTION; CONSISTENCY; CLASSIFICATION; PREDICTION; ALGORITHMS	We prove that boosting with the squared error loss, L(2)Boosting, is consistent for very high-dimensional linear models, where the number of predictor variables is allowed to grow essentially as fast as O(exp(sample size)), assuming that the true underlying regression function is sparse in terms of the l(1)-norm of the regression coefficients. In the language of signal processing, this means consistency for de-noising using a strongly overcomplete dictionary if the underlying signal is sparse in terms of the l(1)-norm. We also propose here an AIC-based method for tuning, namely for choosing the number of boosting iterations. This makes L(2)Boosting computationally attractive since it is not required to run the algorithm multiple times for cross-validation as commonly used so far. We demonstrate L(2)Boosting for simulated data, in particular where the predictor dimension is large in comparison to sample size, and for a difficult tumor-classification problem with gene expression microarray data.	Swiss Fed Inst Technol, CH-8092 Zurich, Switzerland	Buhlmann, P (reprint author), Swiss Fed Inst Technol, CH-8092 Zurich, Switzerland.	buhlmann@stat.math.ethz.ch	Buhlmann, Peter/A-2107-2013				Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451; Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010; Friedman J, 2000, ANN STAT, V28, P337, DOI 10.1214/aos/1016218223; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Jiang WX, 2004, ANN STAT, V32, P13; Zhang T, 2005, ANN STAT, V33, P1538, DOI 10.1214/009053605000000255; Breiman L, 1998, ANN STAT, V26, P801; Breiman L, 1999, NEURAL COMPUT, V11, P1493, DOI 10.1162/089976699300016106; MALLAT SG, 1993, IEEE T SIGNAL PROCES, V41, P3397, DOI 10.1109/78.258082; Dudoit S, 2002, J AM STAT ASSOC, V97, P77, DOI 10.1198/016214502753479248; Buhlmann P, 2003, J AM STAT ASSOC, V98, P324, DOI 10.1198/016214503000125; Zou H, 2005, J ROY STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x; Efron B, 2004, ANN STAT, V32, P407; West M, 2001, P NATL ACAD SCI USA, V98, P11462, DOI 10.1073/pnas.201162998; BUHLMANN P, 2005, IN PRESS J MACHINE L; CRAN, 1997, COMPREHENSIVE R ARCH; Dettling M, 2004, J MULTIVARIATE ANAL, V90, P106, DOI 10.1016/j.jmva.2004.02.012; Devroye L., 1996, PROBABILISTIC THEORY; Freund Y., 1996, Machine Learning. Proceedings of the Thirteenth International Conference (ICML '96); Goldenshluger A, 2001, ANN STAT, V29, P1601; Greenshtein E, 2004, BERNOULLI, V10, P971, DOI 10.3150/bj/1106314846; Hurvich CM, 1998, J ROY STAT SOC B, V60, P271, DOI 10.1111/1467-9868.00125; Jiang W., 2004, ANN STAT, V32, P85; Lugosi G, 2004, ANN STAT, V32, P30; LUGOSI G, 2004, ANN STAT, V32, P85; Schapire R., 2002, LECT NOTES STAT, V171, P149; Temlyakov VN, 2000, ADV COMPUT MATH, V12, P213, DOI 10.1023/A:1018917218956; Tukey J. W., 1977, EXPLORATORY DATA ANA	28	106	107	1	8	INST MATHEMATICAL STATISTICS	BEACHWOOD	PO BOX 22718, BEACHWOOD, OH 44122 USA	0090-5364			ANN STAT	Ann. Stat.	APR	2006	34	2					559	583		10.1214/009053606000000092		25	Statistics & Probability	Mathematics	061PK	WOS:000238884500001		
J	Segal, MR				Segal, MR			Microarray gene expression data with linked survival phenotypes: diffuse large-B-cell lymphoma revisited	BIOSTATISTICS			English	Article						diffuse large-B-cell lymphoma; gene harvesting; least angle regression; microarray; proportional hazards; time-dependent ROC curve	ADAPTIVE REGRESSION SPLINES; MODEL SELECTION; COX MODEL; PREDICTION; CLASSIFICATION; CANCER; LASSO; SHRINKAGE; PATTERNS	Diffuse large-B-cell lymphoma (DLBCL) is an aggressive malignancy of mature B lymphocytes and is the most common type of lymphoma in adults. While treatment advances have been substantial in what was formerly a fatal disease, less than 50% of patients achieve lasting remission. In an effort to predict treatment success and explain disease heterogeneity clinical features have been employed for prognostic purposes, but have yielded only modest predictive performance. This has spawned a series of high-profile microarray-based gene expression studies of DLBCL, in the hope that molecular-level information could be used to refine prognosis. The intent of this paper is to reevaluate these microarray-based prognostic assessments, and extend the statistical methodology that has been used in this context. Methodological challenges arise in using patients' gene expression profiles to predict survival endpoints on account of the large number of genes and their complex interdependence. We initially focus on the Lymphochip data and analysis of Rosenwald et al. (2002). After describing relationships between the analyses performed and gene harvesting (Hastie et al., 2001a), we argue for the utility of penalized approaches, in particular least angle regression-least absolute shrinkage and selection operator (Efron et al., 2004). While these techniques have been extended to the proportional hazards/partial likelihood framework, the resultant algorithms are computationally burdensome. We develop residual-based approximations that eliminate this burden yet perform similarly. Comparisons of predictive accuracy across both methods and studies are effected using time-dependent receiver operating characteristic curves. These indicate that gene expression data, in turn, only delivers modest predictions of posttherapy DLBCL survival. We conclude by outlining possibilities for further work.	Univ Calif San Francisco, Div Biostat, San Francisco, CA 94143 USA	Segal, MR (reprint author), Univ Calif San Francisco, Div Biostat, San Francisco, CA 94143 USA.	mark@biostat.ucsf.edu					AKRITAS MG, 1994, ANN STAT, V22, P1299, DOI 10.1214/aos/1176325630; Lossos IS, 2004, NEW ENGL J MED, V350, P1828, DOI 10.1056/NEJMoa032520; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Tusher VG, 2001, P NATL ACAD SCI USA, V98, P5116, DOI 10.1073/pnas.091062498; NAGELKERKE NJD, 1991, BIOMETRIKA, V78, P691, DOI 10.1093/biomet/78.3.691; Alizadeh AA, 2000, NATURE, V403, P503, DOI 10.1038/35000501; Golub TR, 1999, SCIENCE, V286, P531, DOI 10.1126/science.286.5439.531; Simon R, 2003, J NATL CANCER I, V95, P14; Dudoit S, 2002, J AM STAT ASSOC, V97, P77, DOI 10.1198/016214502753479248; Segal MR, 2003, J COMPUT BIOL, V10, P961, DOI 10.1089/106652703322756177; Eisen MB, 1998, P NATL ACAD SCI USA, V95, P14863, DOI 10.1073/pnas.95.25.14863; Efron B, 2004, ANN STAT, V32, P407; Shipp MA, 2002, NAT MED, V8, P68, DOI 10.1038/nm0102-68; DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425; Tibshirani R, 2001, J ROY STAT SOC B, V63, P411, DOI 10.1111/1467-9868.00293; West M, 2001, P NATL ACAD SCI USA, V98, P11462, DOI 10.1073/pnas.201162998; Tibshirani R, 2002, P NATL ACAD SCI USA, V99, P6567, DOI 10.1073/pnas.082099299; Heagerty PJ, 2000, BIOMETRICS, V56, P337, DOI 10.1111/j.0006-341X.2000.00337.x; Rosenwald A, 2002, NEW ENGL J MED, V346, P1937, DOI 10.1056/NEJMoa012914; Bair E, 2004, PLOS BIOL, V2, P511, DOI 10.1371/journal.pbio.00200108; COX DR, 1972, J R STAT SOC B, V34, P187; Cristianini N., 2000, INTRO SUPPORT VECTOR; Dudoit S., 2002, GENOME BIOL, V3, DOI [10.1186/gb-2002-3-7-research0036, DOI 10.1186/GB-2002-3-7-RESEARCH0036]; Efron B, 2004, J AM STAT ASSOC, V99, P619, DOI 10.1198/016214504000000692; FRIEDMAN JH, 1991, ANN STAT, V19, P1, DOI 10.1214/aos/1176347963; Grambsch PM, 1995, BIOMETRICS, V51, P1469, DOI 10.2307/2533277; GUI J, 2004, PENALIZED COX REGRES; Hastie T., 2001, ELEMENTS STAT LEARNI; Hastie T., 2001, GENOME BIOL, V2, DOI DOI 10.1186/GB-2001-2-1-RESEARCH0003); HEAGERTY PJ, 2003, SURVIVAL MODEL PREDI; Keles S, 2002, STAT MED, V21, P313, DOI 10.1002/sim.981; LEBLANC M, 1992, BIOMETRICS, V48, P411, DOI 10.2307/2532300; LeBlanc M, 1999, BIOMETRICS, V55, P204, DOI 10.1111/j.0006-341X.1999.00204.x; Li Hongzhe, 2004, Bioinformatics, V20 Suppl 1, pi208, DOI 10.1093/bioinformatics/bth900; Liu GY, 2003, NUCLEIC ACIDS RES, V31, P82, DOI 10.1093/nar/gkg121; O'Quigley J, 2001, HANDBOOK OF STATISTICS IN CLINICAL ONCOLOGY, P397; Osborne MR, 2000, J COMPUT GRAPH STAT, V9, P319, DOI 10.2307/1390657; Parmigiani G., 2003, ANAL GENE EXPRESSION; Pittman J, 2004, P NATL ACAD SCI USA, V101, P8431, DOI 10.1073/pnas.0401736101; Rosset S, 2004, J MACH LEARN RES, V5, P941; SEGAL MR, 1995, INT STAT REV, V63, P179, DOI 10.2307/1403613; Shaffer AL, 2001, IMMUNITY, V15, P375, DOI 10.1016/S1074-7613(01)00194-7; Smyth Gordon K, 2003, Methods Mol Biol, V224, P111; Speed T, 2003, STAT ANAL GENE EXPRE; Therneau TM, 2000, MODELING SURVIVAL DA; Tibshirani R, 1997, STAT MED, V16, P385, DOI 10.1002/(SICI)1097-0258(19970228)16:4<385::AID-SIM380>3.0.CO;2-3; Tibshirani R, 1999, J ROY STAT SOC B, V61, P529, DOI 10.1111/1467-9868.00191; Tseng GC, 2005, BIOMETRICS, V61, P10, DOI 10.1111/j.0006-341X.2005.031032.x; Wright G, 2003, P NATL ACAD SCI USA, V100, P9991, DOI 10.1073/pnas.1732008100; Ye JM, 1998, J AM STAT ASSOC, V93, P120, DOI 10.2307/2669609; Yeung KY, 2001, BIOINFORMATICS, V17, P977, DOI 10.1093/bioinformatics/17.10.977; Zhu J, 2004, BIOSTATISTICS, V5, P427, DOI 10.1093/biostatistics/kxg046; ZOU H, 2003, REGULARIZATION VARIA	53	59	59	1	6	OXFORD UNIV PRESS	OXFORD	GREAT CLARENDON ST, OXFORD OX2 6DP, ENGLAND	1465-4644			BIOSTATISTICS	Biostatistics	APR	2006	7	2					268	285		10.1093/biostatistics/kxj006		18	Mathematical & Computational Biology; Statistics & Probability	Mathematical & Computational Biology; Mathematics	027SQ	WOS:000236436300008	16284340	
J	Li, HZ; Gui, J				Li, HZ; Gui, J			Gradient directed regularization for sparse Gaussian concentration graphs, with applications to inference of genetic networks	BIOSTATISTICS			English	Article						empirical Bayes thresholding; graphical models; microarray; threshold gradient descent	EXPRESSION DATA; SELECTION	Large-scale microarray gene expression data provide the possibility of constructing genetic networks or biological pathways. Gaussian graphical models have been suggested to provide an effective method for constructing such genetic networks. However, most of the available methods for constructing Gaussian graphs do not account for the sparsity of the networks and are computationally more demanding or infeasible, especially in the settings of high dimension and low sample size. We introduce a threshold gradient descent (TGD) regularization procedure for estimating the sparse precision matrix in the setting of Gaussian graphical models and demonstrate its application to identifying genetic networks. Such a procedure is computationally feasible and can easily incorporate prior biological knowledge about the network structure. Simulation results indicate that the proposed method yields a better estimate of the precision matrix than the procedures that fail to account for the sparsity of the graphs. We also present the results on inference of a gene network for isoprenoid biosynthesis in Arabidopsis thaliana. These results demonstrate that the proposed procedure can indeed identify biologically meaningful genetic networks based on microarray gene expression data.	Univ Penn, Sch Med, Dept Biostat & Epidemiol, Philadelphia, PA 19104 USA	Li, HZ (reprint author), Univ Penn, Sch Med, Dept Biostat & Epidemiol, 920 Blockley Hall,423 Guardian Dr, Philadelphia, PA 19104 USA.	hli@cceb.upenn.edu					Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Jeong H, 2001, NATURE, V411, P41, DOI 10.1038/35075138; Ideker T, 2001, SCIENCE, V292, P929, DOI 10.1126/science.292.5518.929; Segal E, 2003, NAT GENET, V34, P166, DOI 10.1038/ng1165; DEMPSTER AP, 1972, BIOMETRICS, V28, P157, DOI 10.2307/2528966; Tavazoie S, 1999, NAT GENET, V22, P281; Barabasi AL, 2004, NAT REV GENET, V5, P101, DOI 10.1038/nrg1272; Dobra A, 2004, J MULTIVARIATE ANAL, V90, P196, DOI 10.1016/j.jmva.2004.02.009; DRTON M, 2003, SINFUL APPROACH MODE; Edwards D., 2000, INTRO GRAPHICAL MODE; Friedman JH, 2004, GRADIENT DIRECTED RE; FRIEDMAN N, 2004, SCIENCE, V30, P799; Gardner TS, 2003, SCIENCE, V301, P102, DOI 10.1126/science.1081900; Gui J., 2005, PACIFIC S BIOCOMPUTI, V10, P272; Johnstone IM, 2004, ANN STAT, V32, P1594, DOI 10.1214/009053604000000030; Lin S. P., 1985, MULTIVARIATE ANAL, P411; MEINSHAUSEN N, 2006, IN PRESS ANN STAT; Schafer J, 2005, BIOINFORMATICS, V21, P754, DOI 10.1093/bioinformatics/bti062; Tegner J, 2003, P NATL ACAD SCI USA, V100, P5944, DOI 10.1073/pnas.0933416100; Wille A., 2004, GENOME BIOL, V5, P1; ZOU H, 2004, DEGREES FREEDOM LASS	21	70	72	1	5	OXFORD UNIV PRESS	OXFORD	GREAT CLARENDON ST, OXFORD OX2 6DP, ENGLAND	1465-4644			BIOSTATISTICS	Biostatistics	APR	2006	7	2					302	317		10.1093/biostatistics/kxj008		16	Mathematical & Computational Biology; Statistics & Probability	Mathematical & Computational Biology; Mathematics	027SQ	WOS:000236436300010	16326758	
J	Elith, J; Graham, CH; Anderson, RP; Dudik, M; Ferrier, S; Guisan, A; Hijmans, RJ; Huettmann, F; Leathwick, JR; Lehmann, A; Li, J; Lohmann, LG; Loiselle, BA; Manion, G; Moritz, C; Nakamura, M; Nakazawa, Y; Overton, JM; Peterson, AT; Phillips, SJ; Richardson, K; Scachetti-Pereira, R; Schapire, RE; Soberon, J; Williams, S; Wisz, MS; Zimmermann, NE				Elith, J; Graham, CH; Anderson, RP; Dudik, M; Ferrier, S; Guisan, A; Hijmans, RJ; Huettmann, F; Leathwick, JR; Lehmann, A; Li, J; Lohmann, LG; Loiselle, BA; Manion, G; Moritz, C; Nakamura, M; Nakazawa, Y; Overton, JM; Peterson, AT; Phillips, SJ; Richardson, K; Scachetti-Pereira, R; Schapire, RE; Soberon, J; Williams, S; Wisz, MS; Zimmermann, NE			Novel methods improve prediction of species' distributions from occurrence data	ECOGRAPHY			English	Review							CLIMATE-CHANGE; LOGISTIC-REGRESSION; DISTRIBUTION MODELS; HABITAT-SUITABILITY; POTENTIAL DISTRIBUTION; SPATIAL PREDICTION; ENVELOPE MODELS; CONSERVATION; BIODIVERSITY; PLANT	Prediction of species' distributions is central to diverse applications in ecology, evolution and conservation science. There is increasing electronic access to vast sets of occurrence records in museums and herbaria, yet little effective guidance on how best to use this information in the context of numerous approaches for modelling distributions. To meet this need, we compared 16 modelling methods over 226 species from 6 regions of the world, creating the most comprehensive set of model comparisons to date. We used presence-only data to fit models, and independent presence-absence data to evaluate the predictions. Along with well-established modelling methods such as generalised additive models and GARP and BIOCLIM, we explored methods that either have been developed recently or have rarely been applied to modelling species' distributions. These include machine-learning methods and community models, both of which have features that may make them particularly well suited to noisy or sparse information, as is typical of species' occurrence data. Presence-only data were effective for modelling species' distributions for many species and regions. The novel methods consistently outperformed more established methods. The results of our analysis are promising for the use of data from museums and herbaria, especially as methods suited to the noise inherent in such data improve.	Univ Melbourne, Sch Bot, Parkville, Vic 3010, Australia; SUNY Stony Brook, Dept Ecol & Evolut, Stony Brook, NY 11794 USA; CUNY City Coll, New York, NY 10031 USA; Princeton Univ, Princeton, NJ 08544 USA; Dept Environm & Conservat, Armidale, NSW, Australia; Univ Lausanne, CH-1015 Lausanne, Switzerland; Univ Calif Berkeley, Berkeley, CA 94720 USA; Univ Alaska Fairbanks, Fairbanks, AK USA; NIWA, Hamilton, New Zealand; Swiss Ctr Faunal Cartog, Neuchatel, Switzerland; CSIRO Atherton, Atherton, Qld, Australia; Univ Sao Paulo, BR-05508 Sao Paulo, Brazil; Univ Missouri, St Louis, MO 63121 USA; CIMAT, Mexico City, DF, Mexico; Univ Kansas, Lawrence, KS 66045 USA; Landcare Res, Hamilton, New Zealand; AT&T Labs Res, Florham Pk, NJ USA; McGill Univ, Montreal, PQ H3A 2T5, Canada; James Cook Univ N Queensland, Townsville, Qld 4811, Australia; Natl Environm Res Inst, Roskilde, Denmark; Swiss Fed Res Inst WSL, Birmensdorf, Switzerland	Elith, J (reprint author), Univ Melbourne, Sch Bot, Parkville, Vic 3010, Australia.	j.elith@unimelb.edu.au	Williams, Stephen/A-7250-2008; Graham, Catherine/A-9560-2011; Lehmann, Anthony/B-1544-2010; Moritz, Craig/A-7755-2012; Lohmann, Lucia/C-9492-2013; Peterson, A. Townsend/I-5697-2013; Wisz, Mary/J-7826-2013; Ferrier, Simon/C-1490-2009; Guisan, Antoine/A-1057-2011; Zimmermann, Niklaus/A-4276-2008; Elith, Jane/F-2022-2015; 	Lehmann, Anthony/0000-0002-8279-8567; Williams, Stephen/0000-0002-2510-7408; Lohmann, Lucia/0000-0003-4960-0587; Peterson, A. Townsend/0000-0003-0243-2379; Guisan, Antoine/0000-0002-3998-4815; Zimmermann, Niklaus/0000-0003-3099-9604; Elith, Jane/0000-0002-8706-0326; Soberon, Jorge/0000-0003-2160-4148			Anderson RP, 2003, J BIOGEOGR, V30, P591; Stockwell D, 1999, INT J GEOGR INF SCI, V13, P143, DOI 10.1080/136588199241391; Araujo MB, 2005, GLOBAL CHANGE BIOL, V11, P1504, DOI 10.1111/j.1365-2486.2005.001000.x; Araujo MB, 2000, BIOL CONSERV, V96, P331, DOI 10.1016/S0006-3207(00)00074-4; Peterson AT, 2003, Q REV BIOL, V78, P419, DOI 10.1086/378926; Guisan A, 2000, ECOL MODEL, V135, P147, DOI 10.1016/S0304-3800(00)00354-9; Friedman J, 2000, ANN STAT, V28, P337, DOI 10.1214/aos/1016218223; Liu CR, 2005, ECOGRAPHY, V28, P385, DOI 10.1111/j.0906-7590.2005.03957.x; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Ricklefs RE, 2004, ECOL LETT, V7, P1, DOI 10.1046/j.1461-0248.2003.00554.x; CARPENTER G, 1993, BIODIVERS CONSERV, V2, P667, DOI 10.1007/BF00051966; Thuiller W, 2005, P NATL ACAD SCI USA, V102, P8245, DOI 10.1073/pnas.0409902102; JAYNES ET, 1982, P IEEE, V70, P939, DOI 10.1109/PROC.1982.12425; COHEN J, 1960, EDUC PSYCHOL MEAS, V20, P37, DOI 10.1177/001316446002000104; Thuiller W, 2004, ECOLOGY, V85, P1688, DOI 10.1890/03-0148; Pearce J, 2000, ECOL MODEL, V133, P225, DOI 10.1016/S0304-3800(00)00322-7; Hugall A, 2002, P NATL ACAD SCI USA, V99, P6112, DOI 10.1073/pnas.092538699; McCarthy MA, 2005, J APPL ECOL, V42, P1012, DOI 10.1111/j.1365-2664.2005.01101.x; Thomas CD, 2004, NATURE, V427, P145, DOI 10.1038/nature02121; Boyce MS, 2002, ECOL MODEL, V157, P281, DOI 10.1016/S0304-3800(02)00200-4; Thuiller W, 2004, GLOBAL CHANGE BIOL, V10, P2020, DOI 10.1111/j.1365-2486.2004.00859.x; Stockwell DRB, 2002, ECOL MODEL, V148, P1, DOI 10.1016/S0304-3800(01)00388-X; Zheng BY, 2000, STAT MED, V19, P1771, DOI 10.1002/1097-0258(20000715)19:13<1771::AID-SIM485>3.3.CO;2-G; Luoto M, 2005, GLOBAL ECOL BIOGEOGR, V14, P575, DOI 10.1111/j.1466-822x.2005.00186.x; Hijmans RJ, 2005, INT J CLIMATOL, V25, P1965, DOI 10.1002/joc.1276; HANLEY JA, 1982, RADIOLOGY, V143, P29; Segurado P, 2004, J BIOGEOGR, V31, P1555, DOI 10.1111/j.1365-2699.2004.01076.x; Kadmon R, 2003, ECOL APPL, V13, P853, DOI 10.1890/1051-0761(2003)013[0853:ASAOFA]2.0.CO;2; Brotons L, 2004, ECOGRAPHY, V27, P437, DOI 10.1111/j.0906-7590.2004.03764.x; Pearce J, 2000, ECOL MODEL, V128, P127, DOI 10.1016/S0304-3800(99)00227-6; Anderson RP, 2002, OIKOS, V98, P3, DOI 10.1034/j.1600-0706.2002.t01-1-980116.x; Guisan A, 2005, ECOL LETT, V8, P993, DOI 10.1111/j.1461-0248.2005.00792.x; Graham CH, 2006, P NATL ACAD SCI USA, V103, P632, DOI 10.1073/pnas.0505754103; Moisen GG, 2002, ECOL MODEL, V157, P209, DOI 10.1016/S0304-3800(02)00197-7; Hirzel AH, 2002, ECOLOGY, V83, P2027, DOI 10.1890/0012-9658(2002)083[2027:ENFAHT]2.0.CO;2; Raxworthy CJ, 2003, NATURE, V426, P837, DOI 10.1038/nature02205; Turner W, 2003, TRENDS ECOL EVOL, V18, P306, DOI 10.1016/S0169-5347(03)00070-3; Thornton PE, 1997, J HYDROL, V190, P214, DOI 10.1016/S0022-1694(96)03128-9; Phillips SJ, 2006, ECOL MODEL, V190, P231, DOI 10.1016/j.ecolmodel.2005.03.026; Munoz J, 2004, J VEG SCI, V15, P285, DOI 10.1658/1100-9233(2004)015[0285:COSMCU]2.0.CO;2; Austin MP, 2002, ECOL MODEL, V157, P101, DOI 10.1016/S0304-3800(02)00205-3; Ferrier S, 2002, SYST BIOL, V51, P331, DOI 10.1080/10635150252899806; Keating KA, 2004, J WILDLIFE MANAGE, V68, P774, DOI 10.2193/0022-541X(2004)068[0774:UAIOLR]2.0.CO;2; Fielding AH, 1997, ENVIRON CONSERV, V24, P38, DOI 10.1017/S0376892997000088; Graham CH, 2004, TRENDS ECOL EVOL, V19, P497, DOI 10.1016/j.tree.2004.07.006; Loiselle BA, 2003, CONSERV BIOL, V17, P1591, DOI 10.1111/j.1523-1739.2003.00233.x; Bakkenes M, 2002, GLOBAL CHANGE BIOL, V8, P390, DOI 10.1046/j.1354-1013.2001.00467.x; Zaniewski AE, 2002, ECOL MODEL, V157, P261, DOI 10.1016/S0304-3800(02)00199-0; Araujo MB, 2004, GLOBAL CHANGE BIOL, V10, P1618, DOI 10.1111/j.1365-2486.2004.00828.x; Austin M., 1981, P ECOL SOC AUST, V11, P109; AUSTIN MP, 1995, DIVISION WILDLIFE EC; AUSTIN MP, 1994, J VEG SCI, V5, P215, DOI 10.2307/3236154; BARRY SC, IN PRESS J APPL ECOL; BIO AMF, 2000, THESIS UTRECHT U NET; BOJORQUEZTAPIA LA, 1995, ECOL APPL, V5, P215, DOI 10.2307/1942065; Brown J.H., 1998, BIOGEOGRAPHY; Burgman MA, 2005, ECOLOGY, V86, P2007, DOI 10.1890/04-0906; Burnham K. P., 2002, MODEL SELECTION INFE; Busby J., 1991, NATURE CONSERVATION, P64; Cawsey EM, 2002, BIODIVERS CONSERV, V11, P2239, DOI 10.1023/A:1021350813586; Cicero C, 2004, EVOLUTION, V58, P1573; Elith J, 2002, PREDICTING SPECIES OCCURRENCES: ISSUES OF ACCURACY AND SCALE, P303; Ferrier S, 2002, BIODIVERS CONSERV, V11, P2275, DOI 10.1023/A:1021302930424; Ferrier S, 1997, EVALUATION EFFECTIVE; Ferrier S, 2002, BIODIVERS CONSERV, V11, P2309, DOI 10.1023/A:1021374009951; Funk VA, 2002, SYST BIOL, V51, P303, DOI 10.1080/10635150252899789; Gelfand AE, 2006, BAYESIAN ANAL, V1, P41; Goolsby JA, 2004, NAT AREA J, V24, P351; Graham CH, 2004, EVOLUTION, V58, P1781, DOI 10.1554/03-274; Guisan A, 1998, J VEG SCI, V9, P65, DOI 10.2307/3237224; Guisan A, 2003, J BIOGEOGR, V30, P1233, DOI 10.1046/j.1365-2699.2003.00914.x; HANSKI I, 1994, TRENDS ECOL EVOL, V9, P131, DOI 10.1016/0169-5347(94)90177-5; Harrell FE, 2001, REGRESSION MODELING; Hastie T., 2001, ELEMENTS STAT LEARNI; Hijmans RJ, 2000, CONSERV BIOL, V14, P1755, DOI 10.1046/j.1523-1739.2000.98543.x; Hirzel A, 2002, ECOL MODEL, V157, P331, DOI 10.1016/S0304-3800(02)00203-X; Huettmann F, 2005, J WILDLIFE MANAGE, V69, P466, DOI 10.2193/0022-541X(2005)069[0466:DASMIT]2.0.CO;2; Kadmon R, 2004, ECOL APPL, V14, P401, DOI 10.1890/02-5364; Leathwick JR, 2005, FRESHWATER BIOL, V50, P2034, DOI 10.1111/j.1365-2427.2005.01448.x; Leathwick JR, 2002, BIODIVERS CONSERV, V11, P2177, DOI 10.1023/A:1021394628607; Leathwick JR, 2001, ECOLOGY, V82, P2560, DOI 10.1890/0012-9658(2001)082[2560:CIBTSI]2.0.CO;2; LEATHWICK JR, IN PRESS MAR ECOL PR; LOWE DG, 1995, NEURAL COMPUT, V7, P72, DOI 10.1162/neco.1995.7.1.72; Mac Nally R, 2004, CONSERV BIOL, V18, P646; Moilanen A, 2005, P ROY SOC B-BIOL SCI, V272, P1885, DOI 10.1098/rspb.2005.3164; MURPHY AH, 1992, INT J FORECASTING, V7, P435, DOI 10.1016/0169-2070(92)90028-8; Pearce J, 2001, J ENVIRON MANAGE, V62, P171, DOI 10.1006/jema.2001.0425; PEARCE JL, IN PRESS J APPL ECOL; Peterson AT, 2004, DIVERS DISTRIB, V10, P237, DOI 10.1111/j.1366-9516.2004.00097.x; PETERSON AT, 2005, SW NAT, P230; Phillips S. J., 2004, P 21 INT C MACH LEAR; Pielke RA, 2003, MODELS IN ECOSYSTEM SCIENCE, P111; POMPA AG, 1970, ANALES I BIOL UNAM B, V31, P137; RANDIN CF, IN PRESS J BIOGEOGR; RAPOPORT EH, 1982, AEROGRAPHY; Reese GC, 2005, ECOL APPL, V15, P554, DOI 10.1890/03-5374; Ridgeway G, 1999, COMPUTING SCI STAT, V31, P172; Rosenzweig M. L., 1995, SPECIES DIVERSITY SP; Rushton SP, 2004, J APPL ECOL, V41, P193, DOI 10.1111/j.0021-8901.2004.00903.x; Scotts David, 2003, Pacific Conservation Biology, V8, P235; Silverman B. M., 1986, DENSITY ESTIMATION S; Skov F, 2004, ECOGRAPHY, V27, P366, DOI 10.1111/j.0906-7590.2004.03823.x; Sneath PHA, 1973, NUMERICAL TAXONOMY P; Soberon Jorge, 2005, Biodiversity Informatics, V2, P1; Soberon JM, 2000, BIODIVERS CONSERV, V9, P1441, DOI 10.1023/A:1008987010383; Spiegelhalter D, 2003, WINBUGS USER MANUAL; Spiegelhalter D.J., 2003, J R STAT SOC B, V864, P583; Tyre AJ, 2001, ECOL APPL, V11, P1722, DOI 10.2307/3061091; VANHORNE B, 1983, J WILDLIFE MANAGE, V47, P893; Venier LA, 1999, J BIOGEOGR, V26, P315, DOI 10.1046/j.1365-2699.1999.00273.x; WALKER PA, 1991, GLOBAL ECOL BIOGEOGR, V1, P108, DOI 10.2307/2997706; WINTLE BA, IN PRESS ECOL APPL; YEE TW, 1991, J VEG SCI, V2, P587, DOI 10.2307/3236170	113	2480	2625	141	784	BLACKWELL PUBLISHING	OXFORD	9600 GARSINGTON RD, OXFORD OX4 2DQ, OXON, ENGLAND	0906-7590			ECOGRAPHY	Ecography	APR	2006	29	2					129	151		10.1111/j.2006.0906-7590.04596.x		23	Biodiversity Conservation; Ecology	Biodiversity & Conservation; Environmental Sciences & Ecology	032IB	WOS:000236767000001		
J	de Menezes, LM; Nikolaev, NY				de Menezes, LM; Nikolaev, NY			Forecasting with genetically programmed polynomial neural networks	INTERNATIONAL JOURNAL OF FORECASTING			English	Article						nonlinear models; tree-structured polynomial neural network models; statistical learning algorithms; genetic programming	REGULARIZATION; ALGORITHMS; SELECTION	Recent literature on nonlinear models has shown genetic programming to be a potential tool for forecasters. A special type of genetically programmed model, namely polynomial neural networks, is addressed. Their outputs are polynomials and, as such, they are open boxes that are amenable to comprehension, analysis, and interpretation. This paper presents a polynomial neural network forecasting system, PGP, which has three innovative features: polynomial block reformulation, local ridge regression for weight estimation, and regularized weight subset selection for pruning that uses a least absolute shrinkage and selection operator. The relative performance of this system to other established forecasting procedures is the focus of this research and is illustrated by three empirical studies. Overall, the results are very promising and indicate areas for further research. (c) 2005 International Institute of Forecasters. Published by Elsevier B.V. All rights reserved.	City Univ London, Fac Management, Cass Business Sch, London EC1V 0HB, England; Univ London, Univ London Goldsmiths Coll, Dept Comp, London, England	de Menezes, LM (reprint author), City Univ London, Fac Management, Cass Business Sch, London EC1V 0HB, England.	l.demenezes@city.ac.uk; n.nikolaev@gold.ac.uk					Allen F, 1999, J FINANC ECON, V51, P245, DOI 10.1016/S0304-405X(98)00052-X; MACKAY DJC, 1992, NEURAL COMPUT, V4, P415, DOI 10.1162/neco.1992.4.3.415; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; BARRON AR, 1991, ANN STAT, V19, P67, DOI 10.1214/aos/1176347964; Zhang GQ, 1998, INT J FORECASTING, V14, P35, DOI 10.1016/S0169-2070(97)00044-7; MACKAY DJC, 1992, NEURAL COMPUT, V4, P448, DOI 10.1162/neco.1992.4.3.448; Bazaraa M., 1993, NONLINEAR PROGRAMMIN; Brock W., 1996, ECONOMET REV, V15, P197, DOI DOI 10.1080/07474939608800353; Brock W. A., 1991, NONLINEAR DYNAMICS C; Charytoniuk W, 2000, IEEE T POWER SYST, V15, P263, DOI 10.1109/59.852131; CHEN SH, 1998, ARTIFICIAL NEURAL NE, P397; Darbellay GA, 2000, INT J FORECASTING, V16, P71, DOI 10.1016/S0169-2070(99)00045-X; Elder JF, 2000, NETWORK MODELS FOR CONTROL AND PROCESSING, P143; Faraway J., 1998, APPL STAT, V47, P231; Gerald C. F., 1994, APPL NUMERICAL ANAL; Granger C. W. J., 1993, MODELLING NONLINEAR; HEYWOOD J, 1996, IEEE T NEURAL NETWOR, V6, P893; Hippert HS, 2001, IEEE T POWER SYST, V16, P44, DOI 10.1109/59.910780; HIPPERT HS, 2004, INT J FORECASTING, V21, P425; IVAKHNEN.AG, 1971, IEEE T SYST MAN CYB, VSMC1, P364, DOI 10.1109/TSMC.1971.4308320; KABOUDAN M, 2000, COMPUTATIONAL EC, V6, P207; KANZLER L, 1999, SOCIAL SCI RES NETWO; Koza J., 1992, GENETIC PROGRAMMING; LEBARON B, 2001, BDSTEST C SOURCE COD; Marmarelis VZ, 1997, IEEE T NEURAL NETWOR, V8, P1421, DOI 10.1109/72.641465; NABNEY IT, 2002, NETLAB ALGORITHMS PA, P325; NIKOLAEV N, 2003, FORECASTING GENETICA; Nikolaev NY, 2001, IEEE T EVOLUT COMPUT, V5, P359, DOI 10.1109/4235.942530; ORR MJL, 1995, NEURAL COMPUT, V7, P606, DOI 10.1162/neco.1995.7.3.606; Patterson D. M., 2000, NONLINEAR TIME SERIE; deLima PJF, 1997, J ECONOMETRICS, V76, P251, DOI 10.1016/0304-4076(95)01791-7; REMUS W, 2001, PRINCIPLES FORECASTI, P245; Schetzen M., 1980, VOLTERRA WIENER THEO; Venkatesan R, 2002, INT J FORECASTING, V18, P625, DOI 10.1016/S0169-2070(02)00070-5; WAHBA G, 1990, NSF REGIONAL C SERIE, V59; Zapranis A., 1999, PRINCIPLES NEURAL MO	36	7	7	3	7	ELSEVIER SCIENCE BV	AMSTERDAM	PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS	0169-2070			INT J FORECASTING	Int. J. Forecast.	APR-JUN	2006	22	2					249	265		10.1016/j.ijforecast.2005.05.002		17	Economics; Management	Business & Economics	037AQ	WOS:000237119500004		
J	Lafferty, J; Wasserman, L				Lafferty, J; Wasserman, L			Challenges in statistical machine learning	STATISTICA SINICA			English	Editorial Material							CLASSIFICATION; REGRESSION; VARIABLES; BOUNDS; LASSO		Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA; Carnegie Mellon Univ, Machine Learning Dept, PhD Program Computat & Stat Learning, Pittsburgh, PA 15213 USA; Carnegie Mellon Univ, Machine Learning Dept, Sch Comp Sci, Pittsburgh, PA 15213 USA	Lafferty, J (reprint author), Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA.						Alekhnovich M., 2004, Proceedings. 45th Annual IEEE Symposium on Foundations of Computer Science; Altun Y., 2004, P 21 INT C MACH LEAR, P1; Kerkyacharian G, 2001, PROBAB THEORY REL, V121, P137, DOI 10.1007/PL00008800; Knight K, 2000, ANN STAT, V28, P1356; Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010; LINIAL N, 1993, J ACM, V40, P607, DOI 10.1145/174130.174138; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Taskar B, 2004, ADV NEUR IN, V16, P25; Tsochantaridis I, 2005, J MACH LEARN RES, V6, P1453; Lepski OV, 1997, ANN STAT, V25, P929; KUSHILEVITZ E, 1993, SIAM J COMPUT, V22, P1331, DOI 10.1137/0222080; AUDIBERT JY, 2005, PMA998 LAB PROB; Bartlett PL, 2006, J AM STAT ASSOC, V101, P138, DOI 10.1198/016214505000000907; BELKIN M, 2002, TR200212 U CHIC DEP; Boykov Y., 2001, IEEE T PATTERN ANAL, V23, P1; BUHLMANN P, 2006, IN PRESS ANN STAT, V34; Castellis V, 1996, IEEE T INFORM THEORY, V42, P2101; Chapelle O., 2003, ADV NEURAL INFORM PR, V15, P585; Collins M, 2002, PROCEEDINGS OF THE 2002 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING, P1; Donoho D. L., 2004, MOST LARGE UNDERDETE; Doyle P. G., 1984, RANDOM WALKS ELECT N; ELCHANAN M, 2004, J COMPUT SYST SCI, V69, P421; Fan JQ, 2004, ANN STAT, V32, P928, DOI 10.1214/009053604000000256; Freund Y., 1996, P 13 INT C MACH LEAR, P148; Kearns M, 1998, J ACM, V45, P983, DOI 10.1145/293347.293351; KIVINEN J, 1997, J INFORMATION COMPUT, V132, P1; Kivinen J, 1997, ARTIF INTELL, V97, P325, DOI 10.1016/S0004-3702(97)00039-8; Kumar S, 2004, ADV NEUR IN, V16, P1531; LAFFERTY J, 2005, RODEO SPARSE NONPARA; LAFFERTY J, 2001, P 18 INT C MACH LEAR, P282; Lee YK, 2004, J AM STAT ASSOC, V99, P67, DOI 10.1198/016214504000000098; Lepski OV, 1997, ANN STAT, V25, P2512; McCallum A., 2003, P 19 C UNC ART INT U, P403; Pinto D., 2003, P 26 ANN INT ACM SIG, P235; PITT L, 1988, J ACM, V35, P965, DOI 10.1145/48014.63140; RUPPERT D, 1994, ANN STAT, V22, P1346, DOI 10.1214/aos/1176325632; SCOTT C, 2006, IN PRESS IEEE T INFO; Sha F, 2003, HLT-NAACL 2003: HUMAN LANGUAGE TECHNOLOGY CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE MAIN CONFERENCE, P213; SHEN X, 2006, IN PRESS ANN STAT; Smola A., 2003, P 16 ANN C LEARN THE, P144; Tewari A, 2005, LECT NOTES COMPUT SC, V3559, P143, DOI 10.1007/11503415_10; VALIANT LG, 1984, COMMUN ACM, V27, P1134, DOI 10.1145/1968.1972; Wainwright M., 2003, 649 U CAL DEP STAT; ZHANG H, 2005, J AM STAT ASSOC, V99, P659; Zhu X., 2003, P 20 INT C MACH LEAR, V3, P912, DOI DOI 10.1109/18.850663	45	11	11	0	2	STATISTICA SINICA	TAIPEI	C/O DR H C HO, INST STATISTICAL SCIENCE, ACADEMIA SINICA, TAIPEI 115, TAIWAN	1017-0405			STAT SINICA	Stat. Sin.	APR	2006	16	2					307	321				15	Statistics & Probability	Mathematics	043GH	WOS:000237587700002		
J	Kim, Y; Kim, J; Kim, Y				Kim, Y; Kim, J; Kim, Y			Blockwise sparse regression	STATISTICA SINICA			English	Article						gradient projection method; LASSO; ridge; variable selection	VARIABLE SELECTION; LASSO	Yuan an Lin (2004) proposed the grouped LASSO, which achieves shrinkage and selection simultaneously, as LASSO does, but works on blocks of covariates. That is, the grouped LASSO provides a model where some blocks of regression coefficients are exactly zero. The grouped LASSO is useful when there are meaningful blocks of covariates such as polynomial regression and dummy variables from categorical variables. In this paper, we propose an extension of the grouped LASSO, called 'Blockwise Sparse Regression' (BSR). The BSR achieves shrinkage and selection simultaneously on blocks of covariates similarly to the grouped LASSO, but it works for general loss functions including generalized linear models. An efficient computational algorithm is developed and a blockwise standardization method is proposed. Simulation results show that the BSR compromises the ridge and LASSO for logistic regression. The proposed method is illustrated with two datasets.	Seoul Natl Univ, Stat Res Ctr Complex Syst, Seoul, South Korea; Seoul Natl Univ, Dept Stat, Seoul, South Korea	Kim, Y (reprint author), Seoul Natl Univ, Stat Res Ctr Complex Syst, Seoul, South Korea.			Kim, Jinseog/0000-0003-3172-3212			Knight K, 2000, ANN STAT, V28, P1356; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; Tibshirani R, 2005, J ROY STAT SOC B, V67, P91, DOI 10.1111/j.1467-9868.2005.00490.x; CRAVEN P, 1979, NUMER MATH, V31, P377; Bakin S, 1999, THESIS AUSTR NATL U; Bertsekas D., 1999, NONLINEAR PROGRAMMIN; Chen S. S., 1999, SIAM J SCI COMPUT, V20, P33; Gunn SR, 2002, MACH LEARN, V48, P137, DOI 10.1023/A:1013903804720; KIM Y, 2004, UNPUB GRADIENT PROJE; KRISHNAPURAM B, 2004, LEARNING SPARSE CLAS; LIN Y, 2003, 1072 U WISC MAD DEP; Roth V, 2004, IEEE T NEURAL NETWOR, V15, P16, DOI 10.1109/TNN.2003.809398; Tibshirani R, 1997, STAT MED, V16, P385, DOI 10.1002/(SICI)1097-0258(19970228)16:4<385::AID-SIM380>3.0.CO;2-3; Yuan M., 2004, 1095 U WISC DEP STAT; ZHANG H, 2003, 1059R U WISC DEP STA; ZOU H, 2004, REGULARIZATION VARIA	17	55	56	0	2	STATISTICA SINICA	TAIPEI	C/O DR H C HO, INST STATISTICAL SCIENCE, ACADEMIA SINICA, TAIPEI 115, TAIWAN	1017-0405			STAT SINICA	Stat. Sin.	APR	2006	16	2					375	390				16	Statistics & Probability	Mathematics	043GH	WOS:000237587700005		
J	Li, B; Goel, PK				Li, B; Goel, PK			Regularized optimization in statistical learning: A Bayesian perspective	STATISTICA SINICA			English	Article						Bayesian robustness; bridge regression; flat-tailed prior; group LASSO; LASSO; regularized optimization	VARIABLE SELECTION; REGRESSION; LASSO; MODEL	Regularization plays a major role in modern data analysis, whenever non-regularized fitting is likely to lead to over-fitted model. It is known that most regularized optimization problems have Bayesian interpretation in which the prior plays the role of the regularizer. In this paper, we consider the issue of sensitivity of the regularized solution to the prior specification within the Bayesian perspective. We suggest a class of flat-tailed priors for a general likelihood function for robust Bayesian solutions, in the same spirit as the t-disiribution being suggested as a flat-tail prior for normal likelihood. Results are applied to a family of regularized learning methods and group LASSO. In addition, the consistency issue for LASSO is discussed within this framework.	Ohio State Univ, Dept Stat, Columbus, OH 43210 USA	Li, B (reprint author), Ohio State Univ, Dept Stat, 1958 Neil Ave,Cockins Hall,Room 404, Columbus, OH 43210 USA.	bli@stat.ohio-state.edu; goel@stat.ohio-state.edu					Fu WJJ, 1998, J COMPUT GRAPH STAT, V7, P397, DOI 10.2307/1390712; Knight K, 2000, ANN STAT, V28, P1356; Yuan M, 2006, J ROY STAT SOC B, V68, P49, DOI 10.1111/j.1467-9868.2005.00532.x; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; STONE M, 1977, J R STAT SOC B, V39, P44; HOERL AE, 1970, TECHNOMETRICS, V12, P55; FRANK IE, 1993, TECHNOMETRICS, V35, P109, DOI 10.2307/1269656; Zou H, 2005, J ROY STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x; Donoho DL, 1996, ANN STAT, V24, P508; Berger J. O., 1994, TEST, V3, P5, DOI DOI 10.1007/BF02562676; Berger J. O., 1985, STAT DECISION THEORY; Box GEP, 1973, BAYESIAN INFERENCE S; Fan T. H., 1992, STAT DECISIONS, V10, P99; Gustafson P., 2000, ROBUST BAYESIAN ANAL, P71; Gustafson P., 1996, BAYESIAN STAT, V5, P197; LENG C, 2004, NOTE LASSO RELATED P; Moreno E., 2000, ROBUST BAYESIAN ANAL, P45; O'HAGAN A., 1988, BAYESIAN STATISTICS, V3, P345; PERICCHI LR, 1992, J ROY STAT SOC B MET, V54, P793; Sivaganesan S., 2000, ROBUST BAYESIAN ANAL, P89; Vapnik V., 1998, STAT LEARNING THEORY	22	3	4	0	11	STATISTICA SINICA	TAIPEI	C/O DR H C HO, INST STATISTICAL SCIENCE, ACADEMIA SINICA, TAIPEI 115, TAIWAN	1017-0405			STAT SINICA	Stat. Sin.	APR	2006	16	2					411	424				14	Statistics & Probability	Mathematics	043GH	WOS:000237587700007		
J	Liu, YF; Wu, YC				Liu, YF; Wu, YC			Optimizing phi-learning via mixed integer programming	STATISTICA SINICA			English	Article						classification; norm; regularization; SVM; variable selection		As a new margin-based classifier, psi-learning shows great potential for high accuracy. However, the optimization of psi-learning involves non-convex minimization and is very challenging to implement. In this article, we convert the optimization of psi-learning into a mixed integer programming (MIP) problem. This enables us to utilize the state-of-art algorithm of MIP to solve psi-learning. Moreover, the new algorithm can solve psi-learning with a general piecewise linear psi loss and does not require continuity of the loss function. We also examine the variable selection property of 1-norm psi-learning and make comparisons with the SVM.	Univ N Carolina, Dept Stat & Operat Res, Carolina Ctr Genome Sci, Chapel Hill, NC 27599 USA	Liu, YF (reprint author), Univ N Carolina, Dept Stat & Operat Res, Carolina Ctr Genome Sci, Chapel Hill, NC 27599 USA.	yfliu@email.unc.edu; wuy@email.unc.edu					Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Liu YF, 2005, J COMPUT GRAPH STAT, V14, P219, DOI 10.1198/106186005X37238; BOSER B, 1992, 5 ANN C COMP LEARN T, P142; Bradley Paul S., 1998, ICML 98; Cristianini N., 1999, INTRO SUPPORT VECTOR; Fourer R., 2002, AMPL MODELING LANGUA; Garfinkel R.S., 1972, INTEGER PROGRAMMING; KIMELDOR.G, 1971, J MATH ANAL APPL, V33, P82, DOI 10.1016/0022-247X(71)90184-3; LIU S, 2005, P 5 SIAM ASA INT C D, P1, DOI 10.1145/1089551.1089553; LIU Y, 2004, IN PRESS J AM STAT A; Shen XT, 2003, J AM STAT ASSOC, V98, P724, DOI 10.1198/016214503000000639; Street William Nick, 1993, IS T SPIE 1993 INT S, V1905, P861; Vapnik V., 1998, STAT LEARNING THEORY; WAHBA G, 1990, SIAM CBMS NSF REG AM, V59; Wahba G., 1998, ADV KERNEL METHODS S, P125; Wolsey L. A., 1999, INTEGER COMBINATORIA; Zhu J., 2003, NEURAL INF PROCESS S, V16	17	6	6	0	0	STATISTICA SINICA	TAIPEI	C/O DR H C HO, INST STATISTICAL SCIENCE, ACADEMIA SINICA, TAIPEI 115, TAIWAN	1017-0405			STAT SINICA	Stat. Sin.	APR	2006	16	2					441	457				17	Statistics & Probability	Mathematics	043GH	WOS:000237587700009		
J	Wang, L; Zhu, J; Zou, H				Wang, L; Zhu, J; Zou, H			The doubly regularized support vector machine	STATISTICA SINICA			English	Article						grouping effect; p >> n; quadratic programming; SVM; variable selection	SELECTION; RECOGNITION; REGRESSION	The standard L-2-norm support vector machine (SVM) is a widely used tool for classification problems. The L-1-norm SVM is a variant of the standard L-2-norm SVM, that constrains the L-1-norm of the fitted coefficients. Due to the nature of the L-1-norm, the L-1-norm SVM has the property of automatically selecting variables, not shared by the standard L-2-norm SVM. It has been argued that the L-1-norm SVM may have some advantage over the L-2-norm SVM, especially with high dimensional problems and when there are redundant noise variables. On the other hand, the L-1-norm SVM has two drawbacks: (1) when there are several highly correlated variables, the L-1-norm SVM tends to pick only a few of them, and remove the rest; (2) the number of selected variables is upper bounded by the size of the training data. A typical example where these occur is in gene microarray analysis. In this paper, we propose a doubly regularized support vector machine (DrSVM). The DrSVM uses the elastic-net penalty, a mixture of the L-2-norm and the L-1-norm penalties. By doing so, the DrSVM performs automatic variable selection in a way similar to the L-1-norm SVM. In addition, the DrSVM encourages highly correlated variables to be selected (or removed) together. We illustrate how the DrSVM can be particularly useful when the number of variables is much larger than the size of the training data (p >> n). We also develop efficient algorithms to compute the whole solution paths of the DrSVM.	Univ Michigan, Dept Stat, Ann Arbor, MI 48109 USA; Univ Minnesota, Sch Stat, Minneapolis, MN 55455 USA	Wang, L (reprint author), Univ Michigan, Dept Stat, Ann Arbor, MI 48109 USA.	wang@umich.edu; jizhu@umich.edu; hzou@stat.umn.edu					Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558; Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; MALLAT SG, 1993, IEEE T SIGNAL PROCES, V41, P3397, DOI 10.1109/78.258082; HOERL AE, 1970, TECHNOMETRICS, V12, P55; Guyon I, 2002, MACH LEARN, V46, P389, DOI 10.1023/A:1012487302797; Zou H, 2005, J ROY STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x; Burges CJC, 1998, DATA MIN KNOWL DISC, V2, P121, DOI 10.1023/A:1009715923555; CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; BOSER B, 1992, 5 ANN WORKSH COMP LE; Bradley P., 1998, INT C MACH LEARN; DONOHO D, 1995, J ROY STAT SOC, V57, P201; EVGENIOU T, 1999, ADV LARGE MARGIN CLA; FRIEDMAN J, 2004, ANN STAT, V32; GOLUB T, 2000, SCIENCE, V286, P531; Hastie T., 2001, ELEMENTS STAT LEARNI; LECUN Y, 1995, NEURAL NETWORKS STAT; Mukherjee S., 1999, 1677 AI MIT; Ng A.Y., 2004, INT C MACH LEARN; Palmer S. E., 1999, VISION SCI PHOTONS P; Rosset S, 2004, J MACH LEARN RES, V5, P941; ROSSET S, 2004, PIECEWISE LINEAR REG; Song M., 2002, J CHEM INFORM COMPUT; Vapnik V.N., 1995, NATURE STAT LEARNING; Wahba G., 2000, ADV LARGE MARGIN CLA; Wahba G., 1999, ADV KERNEL METHODS S; ZHANG H, 2003, IEEE COMPUTER VISION, V1, P242; ZHU J, 2004, NEUROL INF PROCESS S, V16	28	53	60	2	5	STATISTICA SINICA	TAIPEI	C/O DR H C HO, INST STATISTICAL SCIENCE, ACADEMIA SINICA, TAIPEI 115, TAIWAN	1017-0405			STAT SINICA	Stat. Sin.	APR	2006	16	2					589	615				27	Statistics & Probability	Mathematics	043GH	WOS:000237587700016		
J	Heidema, AG; Boer, JM; Nagelkerke, N; Mariman, ECM; van der A, DL; Feskens, EJM				Heidema, AG; Boer, JM; Nagelkerke, N; Mariman, ECM; van der A, DL; Feskens, EJM			The challenge for genetic epidemiologists: how to analyze large numbers of SNPs in relation to complex diseases	BMC GENETICS			English	Editorial Material							MULTIFACTOR-DIMENSIONALITY REDUCTION; NEURAL-NETWORK ARCHITECTURE; CASE-CONTROL ASSOCIATION; LOGISTIC-REGRESSION; PAI-1-4G/5G POLYMORPHISMS; RANDOM FORESTS; PAI-1 LEVELS; ACE-I/D; TRAITS; MODEL	Genetic epidemiologists have taken the challenge to identify genetic polymorphisms involved in the development of diseases. Many have collected data on large numbers of genetic markers but are not familiar with available methods to assess their association with complex diseases. Statistical methods have been developed for analyzing the relation between large numbers of genetic and environmental predictors to disease or disease-related variables in genetic association studies. In this commentary we discuss logistic regression analysis, neural networks, including the parameter decreasing method (PDM) and genetic programming optimized neural networks (GPNN) and several non-parametric methods, which include the set association approach, combinatorial partitioning method (CPM), restricted partitioning method (RPM), multifactor dimensionality reduction (MDR) method and the random forests approach. The relative strengths and weaknesses of these methods are highlighted. Logistic regression and neural networks can handle only a limited number of predictor variables, depending on the number of observations in the dataset. Therefore, they are less useful than the non-parametric methods to approach association studies with large numbers of predictor variables. GPNN on the other hand may be a useful approach to select and model important predictors, but its performance to select the important effects in the presence of large numbers of predictors needs to be examined. Both the set association approach and random forests approach are able to handle a large number of predictors and are useful in reducing these predictors to a subset of predictors with an important contribution to disease. The combinatorial methods give more insight in combination patterns for sets of genetic and/or environmental predictor variables that may be related to the outcome variable. As the non-parametric methods have different strengths and weaknesses we conclude that to approach genetic association studies using the case-control design, the application of a combination of several methods, including the set association approach, MDR and the random forests approach, will likely be a useful strategy to find the important genes and interaction patterns involved in complex diseases.	Natl Inst Publ Hlth & Environm, Ctr Nutr & Hlth, NL-3720 BA Bilthoven, Netherlands; United Arab Emirates Univ, Dept Community Med, Al Ain, U Arab Emirates; Maastricht Univ, NL-6200 MD Maastricht, Netherlands; Univ Wageningen & Res Ctr, Div Human Nutr, NL-6700 EV Wageningen, Netherlands	Heidema, AG (reprint author), Natl Inst Publ Hlth & Environm, Ctr Nutr & Hlth, POB 1, NL-3720 BA Bilthoven, Netherlands.	geert.heidema@rivm.nl; jma.boer@rivm.nl; nico.nagelkerke@uaeu.ac.ae; e.mariman@hb.unimaas.nl; daphne.van.der.a@rivm.nl; edith.feskens@wur.nl	Feskens, Edith/A-3757-2012				Ritchie MD, 2001, AM J HUM GENET, V69, P138, DOI 10.1086/321276; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Moore JH, 2005, BIOESSAYS, V27, P637, DOI 10.1002/bias.20236; Benjamini Y, 2001, BEHAV BRAIN RES, V125, P279, DOI 10.1016/S0166-4328(01)00297-2; Ritchie MD, 2003, GENET EPIDEMIOL, V24, P150, DOI 10.1002/gepi.10218; de Quervain DJF, 2004, HUM MOL GENET, V13, P47, DOI 10.1093/hmg/ddg361; Ritchie MD, 2003, BMC BIOINFORMATICS, V4, DOI 10.1186/1471-2105-4-28; Hoh J, 2003, NAT REV GENET, V4, P701, DOI 10.1038/nrg1155; Hoh J, 2001, GENOME RES, V11, P2115, DOI 10.1101/gr.204001; Hahn LW, 2003, BIOINFORMATICS, V19, P376, DOI 10.1093/bioinformatics/btf869; Nelson MR, 2001, GENOME RES, V11, P458, DOI 10.1101/gr.172901; Bellman R.E., 1961, ADAPTIVE CONTROL PRO; Bishop C. M., 1995, NEURAL NETWORKS PATT; Bureau A, 2005, GENET EPIDEMIOL, V28, P171, DOI 10.1002/gepi.20041; Cho YM, 2004, DIABETOLOGIA, V47, P549, DOI 10.1007/s00125-003-1321-3; Coffey CS, 2004, BMC BIOINFORMATICS, V5, DOI 10.1186/1471-2105-5-49; Cox D., 1974, THEORETICAL STAT; Culverhouse R, 2004, GENET EPIDEMIOL, V27, P141, DOI 10.1002/gepi.20006; Culverhouse R, 2002, AM J HUM GENET, V70, P416; Dohoo IR, 1997, PREV VET MED, V29, P221, DOI 10.1016/S0167-5877(96)01074-4; Li L, 2004, STAT MED, V23, P271, DOI 10.1002/sim.1715; Lucek PR, 1997, GENET EPIDEMIOL, V14, P1101, DOI 10.1002/(SICI)1098-2272(1997)14:6<1101::AID-GEPI90>3.0.CO;2-K; Lunetta KL, 2004, BMC GENET, V5, DOI 10.1186/1471-2156-5-32; Moore JH, 2004, EXPERT REV MOL DIAGN, V4, P795, DOI 10.1586/14737159.4.6.795; Moore JH, 2002, ANN MED, V34, P88, DOI 10.1080/07853890252953473; Moore JH, 2002, CLIN GENET, V62, P74, DOI 10.1034/j.1399-0004.2002.620110.x; Moore JH, 2002, CLIN GENET, V62, P53, DOI 10.1034/j.1399-0004.2002.620107.x; MOORE JH, IN PRESS J THEOR BIO; Motsinger AA, 2006, BMC BIOINFORMATICS, V7, DOI 10.1186/1471-2105-7-39; Nagelkerke N, 2005, STAT MED, V24, P121, DOI 10.1002/sim.1997; North BV, 2003, ANN HUM GENET, V67, P348, DOI 10.1046/j.1469-1809.2003.00030.x; Ott J, 2003, J COMPUT BIOL, V10, P569, DOI 10.1089/10665270360688192; Ott J, 2001, HUM MUTAT, V17, P285, DOI 10.1002/humu.25; Peduzzi P, 1996, J CLIN EPIDEMIOL, V49, P1373, DOI 10.1016/S0895-4356(96)00236-3; Province MA, 2001, ADV GENET, V42, P273, DOI 10.1016/S0065-2660(01)42028-1; Schork NJ, 2001, ADV GENET, V42, P191, DOI 10.1016/S0065-2660(01)42023-2; Thornton-Wells TA, 2004, TRENDS GENET, V20, P640, DOI 10.1016/j.tig.2004.09.007; Tomita Y, 2004, BMC BIOINFORMATICS, V5, DOI 10.1186/1471-2105-5-120; Wilke RA, 2005, NAT REV DRUG DISCOV, V4, P911, DOI 10.1038/nrd1874; Wille A, 2003, GENET EPIDEMIOL, V25, P350, DOI 10.1002/gepi.10263; Williams SM, 2004, HUM HERED, V57, P28, DOI 10.1159/000077387; Xu JF, 2005, CANCER EPIDEM BIOMAR, V14, P2563, DOI 10.1158/1055-9965.EPI-05-0356; Zee R. Y. L., 2002, Pharmacogenomics Journal, V2, P197, DOI 10.1038/sj.tpj.6500101	43	78	80	0	2	BIOMED CENTRAL LTD	LONDON	MIDDLESEX HOUSE, 34-42 CLEVELAND ST, LONDON W1T 4LB, ENGLAND	1471-2156			BMC GENET	BMC Genet.	APR 21	2006	7								23	10.1186/1471.1-2156-7-23		15	Genetics & Heredity	Genetics & Heredity	053FL	WOS:000238290900001	16630340	
J	Ma, SG; Song, X; Huang, J				Ma, Shuangge; Song, Xiao; Huang, Jian			Regularized binormal ROC method in disease classification using microarray data	BMC BIOINFORMATICS			English	Article							GENE-EXPRESSION DATA; DIAGNOSTIC-ACCURACY; SELECTION; CANCER; DISCRIMINATION; BIOMARKERS; REGRESSION; MODELS; LASSO; TESTS	Background: An important application of microarrays is to discover genomic biomarkers, among tens of thousands of genes assayed, for disease diagnosis and prognosis. Thus it is of interest to develop efficient statistical methods that can simultaneously identify important biomarkers from such high-throughput genomic data and construct appropriate classification rules. It is also of interest to develop methods for evaluation of classification performance and ranking of identified biomarkers. Results: The ROC (receiver operating characteristic) technique has been widely used in disease classification with low dimensional biomarkers. Compared with the empirical ROC approach, the binormal ROC is computationally more affordable and robust in small sample size cases. We propose using the binormal AUC (area under the ROC curve) as the objective function for two-sample classification, and the scaled threshold gradient directed regularization method for regularized estimation and biomarker selection. Tuning parameter selection is based on V-fold cross validation. We develop Monte Carlo based methods for evaluating the stability of individual biomarkers and overall prediction performance. Extensive simulation studies show that the proposed approach can generate parsimonious models with excellent classification and prediction performance, under most simulated scenarios including model mis-specification. Application of the method to two cancer studies shows that the identified genes are reasonably stable with satisfactory prediction performance and biologically sound implications. The overall classification performance is satisfactory, with small classification errors and large AUCs. Conclusion: In comparison to existing methods, the proposed approach is computationally more affordable without losing the optimality possessed by the standard ROC method.	Univ Washington, Dept Biostat, Seattle, WA 98195 USA; Univ Iowa, Dept Stat & Actuarial Sci, Iowa City, IA 52242 USA; Univ Iowa, Program Publ Hlth Genet, Iowa City, IA 52242 USA	Ma, SG (reprint author), Univ Washington, Dept Biostat, Seattle, WA 98195 USA.	shuangge@u.washington.edu; songx@u.washington.edu; jian@stat.uiowa.edu					Michiels S, 2005, LANCET, V365, P488, DOI 10.1016/S0140-6736(05)17866-0; Cui XG, 2005, BIOSTATISTICS, V6, P59, DOI 10.1093/biostatistics/kxh018; Nguyen DV, 2002, BIOINFORMATICS, V18, P39, DOI 10.1093/bioinformatics/18.1.39; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324; Pepe MS, 2004, AM J EPIDEMIOL, V159, P882, DOI 10.1093/aje/kwh101; Alon U, 1999, P NATL ACAD SCI USA, V96, P6745, DOI 10.1073/pnas.96.12.6745; Tusher VG, 2001, P NATL ACAD SCI USA, V98, P5116, DOI 10.1073/pnas.091062498; Dudoit S, 2002, J AM STAT ASSOC, V97, P77, DOI 10.1198/016214502753479248; Efron B, 2004, ANN STAT, V32, P407; West M, 2001, P NATL ACAD SCI USA, V98, P11462, DOI 10.1073/pnas.201162998; Tibshirani R, 2002, P NATL ACAD SCI USA, V99, P6567, DOI 10.1073/pnas.082099299; Ben-Dor A., 2000, P 4 ANN INT C COMP M; DABNEY AR, 2005, BIOINFORMATICS, V22, P4148; DETTLING M, 2003, BIOINFORMATICS, V9, P1061; DIACONIS P, 1983, SCI AM, V248, P116; Friedman JH, 2004, GRADIENT DIRECTED RE; Ghosh D, 2005, J BIOMED BIOTECHNOL, P147, DOI 10.1155/JBB.2005.147; GUI J, 2005, P PSB; Hanley JA, 1996, STAT MED, V15, P1575, DOI 10.1002/(SICI)1097-0258(19960730)15:14<1575::AID-SIM283>3.0.CO;2-2; HANLEY JA, 1988, MED DECIS MAKING, V8, P197, DOI 10.1177/0272989X8800800308; HASTIE T, 2001, ELEMENTS STATISTICAL; KOSOROK MR, IN PRESS ANN STAT; Liu AY, 2005, STAT MED, V24, P37, DOI 10.1002/sim.1922; Ma S, 2006, BIOMETRICS, V62, P202, DOI 10.1111/j.1541-0420.2005.00405.x; Ma SG, 2005, BIOINFORMATICS, V21, P4356, DOI 10.1093/bioinformatics/bti724; Pepe MS, 2006, BIOMETRICS, V62, P221, DOI 10.1111/j.1541-0420.2005.00420.x; Pepe MS, 2003, STAT EVALUATION MED; POCHET N, 2004, BIOINFORMATICS, V17, P3185; SPANG R, 2001, P GERM C BIOINF; SWETS JA, 1986, PSYCHOL BULL, V99, P100, DOI 10.1037/0033-2909.99.1.100; TSAI CA, 2005, J BIOPHARMACEUTICAL, V14, P985; WAHBA G, 1990, CBMS NSF REG C SER	33	16	16	0	0	BIOMED CENTRAL LTD	LONDON	MIDDLESEX HOUSE, 34-42 CLEVELAND ST, LONDON W1T 4LB, ENGLAND	1471-2105			BMC BIOINFORMATICS	BMC Bioinformatics	MAY 9	2006	7								253	10.1186/1471-2105-7-253		16	Biochemical Research Methods; Biotechnology & Applied Microbiology; Mathematical & Computational Biology	Biochemistry & Molecular Biology; Biotechnology & Applied Microbiology; Mathematical & Computational Biology	067LJ	WOS:000239303600001	16684357	
J	Du, PC; Angeletti, RH				Du, PC; Angeletti, RH			Automatic deconvolution of isotope-resolved mass spectra using variable selection and quantized peptide mass distribution	ANALYTICAL CHEMISTRY			English	Article							SPECTROMETRY; IDENTIFICATION; REGRESSION; PROTEINS	We present an algorithm for the deconvolution of isotope-resolved mass spectra of complex peptide mixtures where peaks and isotope series often overlap. The algorithm formulates the problem of mass spectrum deconvolution as a classical statistical problem of variable selection, which aims to interpret the spectrum with the least number of peptides. The LASSO method is used to perform automatic variable selection. The algorithm also makes use of the quantized distribution of peptide masses in the NCBInr database after in silico trypsin digestion as filters to aid the deconvolution process. Errors in the expected isotope pattern are accounted for to avoid spurious isotope series. The effectiveness of the algorithm is demonstrated with annotated ESI spectrum of known peptides for which the peaks and isotope series are highly overlapping. The algorithm successfully finds all correct masses in the experimental spectrum, except for one spectrum where an additional refinement procedure is required to obtain the correct results. Our results compare favorably to those from a widely used commercial program.	Albert Einstein Coll Med, Dept Dev & Mol Biol, Bronx, NY 10461 USA	Du, PC (reprint author), Albert Einstein Coll Med, Dept Dev & Mol Biol, 1300 Morris Pk Ave, Bronx, NY 10461 USA.	pdu@aecom.yu.edu					SENKO MW, 1995, J AM SOC MASS SPECTR, V6, P229, DOI 10.1016/1044-0305(95)00017-8; PAPPIN DJC, 1993, CURR BIOL, V3, P327, DOI 10.1016/0960-9822(93)90195-T; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Horn DM, 2000, J AM SOC MASS SPECTR, V11, P320, DOI 10.1016/S1044-0305(99)00157-9; Wehofsky M, 2001, EUR J MASS SPECTROM, V7, P39, DOI 10.1255/ejms.387; George EI, 2000, J AM STAT ASSOC, V95, P1304, DOI 10.2307/2669776; Perkins DN, 1999, ELECTROPHORESIS, V20, P3551, DOI 10.1002/(SICI)1522-2683(19991201)20:18<3551::AID-ELPS3551>3.0.CO;2-2; Wang WX, 2003, ANAL CHEM, V75, P4818, DOI 10.1021/ac026468x; Efron B, 2004, ANN STAT, V32, P407; Fernandez-De-Cossio J, 2004, RAPID COMMUN MASS SP, V18, P2465, DOI [10.1002/rcm.1647, 10.1002/rsm.1647]; Gay S, 1999, ELECTROPHORESIS, V20, P3527, DOI 10.1002/(SICI)1522-2683(19991201)20:18<3527::AID-ELPS3527>3.3.CO;2-0; Li XJ, 2005, MOL CELL PROTEOMICS, V4, P1328, DOI 10.1074/mcp.M500141-MCP200; MANN M, 1995, 43 ASMS C MASS SPECT, P639; Samuelsson J, 2004, BIOINFORMATICS, V20, P3628, DOI 10.1093/bioinformatics/bth460; Wehofsky M, 2002, J MASS SPECTROM, V37, P223, DOI 10.1002/jms.278; Zhang X, 2005, J AM SOC MASS SPECTR, V16, P1181, DOI 10.1016/j.jasms.2005.03.016; Zhang ZQ, 1998, J AM SOC MASS SPECTR, V9, P225, DOI 10.1016/S1044-0305(97)00284-5	17	36	36	2	5	AMER CHEMICAL SOC	WASHINGTON	1155 16TH ST, NW, WASHINGTON, DC 20036 USA	0003-2700			ANAL CHEM	Anal. Chem.	MAY 15	2006	78	10					3385	3392		10.1021/ac052212q		8	Chemistry, Analytical	Chemistry	043IF	WOS:000237593700022	16689541	
J	Meinshausen, N; Buhlmann, P				Meinshausen, Nicolai; Buehlmann, Peter			High-dimensional graphs and variable selection with the Lasso	ANNALS OF STATISTICS			English	Article						linear regression; covariance selection; Gaussian graphical models; penalized regression	MODEL SELECTION; REGRESSION; ESTIMATORS	The pattern of zero entries in the inverse covariance matrix of a multivariate normal distribution corresponds to conditional independence restrictions between variables. Covariance selection aims at estimating those structural zeros from data. We show that neighborhood selection with the Lasso is a computationally attractive alternative to standard covariance selection for sparse high-dimensional graphs. Neighborhood selection estimates the conditional independence restrictions separately for each node in the graph and is hence equivalent to variable selection for Gaussian linear models. We show that the proposed neighborhood selection scheme is consistent for sparse high-dimensional graphs. Consistency hinges on the choice of the penalty parameter. The oracle value for optimal prediction does not lead to a consistent neighborhood estimate. Controlling instead the probability of falsely joining some distinct connectivity components of the graph, consistent estimation for sparse graphs is achieved (with exponential rates), even when the number of variables grows as the number of observations raised to an arbitrary power.	ETH, Seminar Stat, CH-8092 Zurich, Switzerland	Meinshausen, N (reprint author), ETH, Seminar Stat, CH-8092 Zurich, Switzerland.	nicolai@stat.math.ethz.ch; buhlmann@stat.math.ethz.ch	Buhlmann, Peter/A-2107-2013				Knight K, 2000, ANN STAT, V28, P1356; Drton M, 2004, BIOMETRIKA, V91, P591, DOI 10.1093/biomet/91.3.591; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Chen SSB, 2001, SIAM REV, V43, P129, DOI 10.1137/S003614450037906X; FRANK IE, 1993, TECHNOMETRICS, V35, P109, DOI 10.2307/1269656; DEMPSTER AP, 1972, BIOMETRICS, V28, P157, DOI 10.2307/2528966; SHAO J, 1993, J AM STAT ASSOC, V88, P486, DOI 10.2307/2290328; Efron B, 2004, ANN STAT, V32, P407; BUHL SL, 1993, SCAND J STAT, V20, P263; Edwards D, 2000, INTRO GRAPHICAL MODE; Greenshtein E, 2004, BERNOULLI, V10, P971, DOI 10.3150/bj/1106314846; Juditsky A, 2000, ANN STAT, V28, P681; Kadie C., 2000, J MACHINE LEARNING R, V1, P49; Lauritzen S. L., 1996, GRAPHICAL MODELS; Osborne MR, 2000, J COMPUT GRAPH STAT, V9, P319, DOI 10.2307/1390657; SPEED TP, 1986, ANN STAT, V14, P138, DOI 10.1214/aos/1176349846; Van der Vaart A., 1996, WEAK CONVERGENCE EMP	17	612	617	9	50	INST MATHEMATICAL STATISTICS	BEACHWOOD	PO BOX 22718, BEACHWOOD, OH 44122 USA	0090-5364			ANN STAT	Ann. Stat.	JUN	2006	34	3					1436	1462		10.1214/009053606000000281		27	Statistics & Probability	Mathematics	069TO	WOS:000239471300013		
J	Hooper-van Veen, T; Berkhof, J; Polman, CH; Uitdehaag, BMJ				Hooper-van Veen, T; Berkhof, J; Polman, CH; Uitdehaag, BMJ			Analysing the effect of candidate genes on complex traits: an application in multiple sclerosis	IMMUNOGENETICS			English	Article						multiple sclerosis; genetic; prognosis; severity	POSTERIOR DISTRIBUTIONS; SUSCEPTIBILITY; DISEASE; POLYMORPHISM; DISABILITY; REGRESSION; LESIONS; MRI; DEMYELINATION; SELECTION	The conventional approach of candidate gene studies in complex diseases is to look at the effect of one gene at a time. However, as the outcome of chronic diseases is influenced by a large number of alleles, simultaneous analysis is needed. We demonstrate the application of multivariate regression and cluster analysis to a multiple sclerosis (MS) dataset with genotypes for 489 patients at 11 candidate genes selected on their involvement in the immune response. Using multivariate regression, we observed that different sets of genes were associated with different disease characteristics that reflect different aspects of disease. Out of 15 polymorphisms, we identified one that contributed to the severity of disease. In addition, the set of 15 polymorphisms was predictive for yearly increase in lesion volume as seen on T1-weighted MRI (p=0.044). From this set, no individual polymorphisms could be identified after adjustment for multiple hypotheses testing. By means of a cluster analysis, we aimed to identify subgroups of patients with different pathogenic subtypes of MS on the basis of their genetic profile. We constructed genetic profiles from the genotypes at the 11 candidate genes. The approach proved to be feasible. We observed three clusters in the sample of patients. In this study, we observed no significant differences in the usual clinical and MRI outcome measures between the different clusters. However, a number of consistent trends indicated that this clustering might be related to the course of disease. With a larger number of genes regulating the course of disease, we may be able to identify clinically relevant clusters. The analyses are easily implemented and will be applicable to candidate gene studies of complex traits in general.	Free Univ Amsterdam, Med Ctr, Dept Mol Cell Biol & Immunol, NL-1007 MB Amsterdam, Netherlands; VU Univ Med Ctr, Dept Mol Cell Biol & Immunol, NL-1007 MB Amsterdam, Netherlands; VU Univ Med Ctr, Dept Clin Epidemiol & Biostat, NL-1007 MB Amsterdam, Netherlands; VU Univ Med Ctr, Dept Neurol, NL-1007 MB Amsterdam, Netherlands	Hooper-van Veen, T (reprint author), Free Univ Amsterdam, Med Ctr, Dept Mol Cell Biol & Immunol, POB 7057, NL-1007 MB Amsterdam, Netherlands.	T.Hooper-vanVeen@vumc.nl					Alloza I, 2002, ANN NEUROL, V52, P524, DOI 10.1002/ana.10348; Miller DH, 1998, BRAIN, V121, P3, DOI 10.1093/brain/121.1.3; KURTZKE JF, 1983, NEUROLOGY, V33, P1444; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Brassat D, 1999, NEUROLOGY, V52, P1632; Comings DE, 2000, CLIN GENET, V57, P178, DOI 10.1034/j.1399-0004.2000.570304.x; HOERL AE, 1970, TECHNOMETRICS, V12, P55; Bo L, 2003, J NEUROPATH EXP NEUR, V62, P723; Eisen MB, 1998, P NATL ACAD SCI USA, V95, P14863, DOI 10.1073/pnas.95.25.14863; Lucchinetti C, 2000, ANN NEUROL, V47, P707, DOI 10.1002/1531-8249(200006)47:6<707::AID-ANA3>3.0.CO;2-Q; TANNER MA, 1987, J AM STAT ASSOC, V82, P528, DOI 10.2307/2289457; Berkhof J, 2003, STAT SINICA, V13, P423; Breiman L., 1984, CLASSIFICATION REGRE; Bruck W, 2003, NEUROL SCI, V24, pS265, DOI 10.1007/s10072-003-0170-7; Celeux G, 2000, J AM STAT ASSOC, V95, P957, DOI 10.2307/2669477; Chataway J, 2001, J NEUROL NEUROSUR PS, V71, P757, DOI 10.1136/jnnp.71.6.757; Comings DE, 1998, ALCOHOL, V16, P61, DOI 10.1016/S0741-8329(97)00178-X; De Groot CJA, 2001, BRAIN, V124, P1635, DOI 10.1093/brain/124.8.1635; DIEBOLT J, 1994, J ROY STAT SOC B MET, V56, P363; Hall MA, 2000, GENES IMMUN, V1, P219, DOI 10.1038/sj.gene.6363661; Hooper-van Veen T, 2003, MULT SCLER, V9, P535, DOI 10.1191/1352458503ms974oa; Johnson GCL, 2001, NAT GENET, V29, P233, DOI 10.1038/ng1001-233; Kalkers NF, 2001, NEUROLOGY, V57, P1253; Kantarci OH, 2002, J NEUROIMMUNOL, V123, P144, DOI 10.1016/S0165-5728(01)00481-7; LEBLANC M, 1992, BIOMETRICS, V48, P411, DOI 10.2307/2532300; Losseff NA, 1996, BRAIN, V119, P2009, DOI 10.1093/brain/119.6.2009; Mann CLA, 2002, J NEUROIMMUNOL, V129, P197, DOI 10.1016/S0165-5728(02)00181-9; Pastinen T, 1998, HUM MOL GENET, V7, P1453, DOI 10.1093/hmg/7.9.1453; POSER CM, 1983, ANN NEUROL, V13, P227, DOI 10.1002/ana.410130302; Pritchard JK, 2001, AM J HUM GENET, V69, P1, DOI 10.1086/321275; REVESZ T, 1994, BRAIN, V117, P759, DOI 10.1093/brain/117.4.759; SCHORK NJ, 1995, GENOME RES, V5, P164, DOI 10.1101/gr.5.2.164; Schrijver HM, 2004, EUR J IMMUNOGENET, V31, P133, DOI 10.1111/j.1365-2370.2004.00456.x; Spiegelhalter DJ, 1999, BRIT MED J, V319, P508; van Veen T, 2001, ANN NEUROL, V50, P275, DOI 10.1002/ana.1107; van Veen T, 2002, J NEUROIMMUNOL, V128, P95, DOI 10.1016/S0165-5728(02)00163-7; van Veen T, 2003, NEUROLOGY, V61, P1245; van Veen T, 2003, J NEUROIMMUNOL, V140, P188, DOI 10.1016/S0165-5728(03)00184-X; van Walderveen MAA, 1998, NEUROLOGY, V50, P1282; Westfall P. H., 1993, RESAMPLING BASED MUL	40	4	4	0	0	SPRINGER	NEW YORK	233 SPRING STREET, NEW YORK, NY 10013 USA	0093-7711			IMMUNOGENETICS	Immunogenetics	JUN	2006	58	5-6					347	354		10.1007/s00251-006-0116-3		8	Genetics & Heredity; Immunology	Genetics & Heredity; Immunology	049KK	WOS:000238014900003	16612628	
J	Zou, H; Hastie, T; Tibshirani, R				Zou, H; Hastie, T; Tibshirani, R			Sparse principal component analysis	JOURNAL OF COMPUTATIONAL AND GRAPHICAL STATISTICS			English	Article						arrays; gene expression; lasso/elastic net; multivariate analysis; singular value decomposition; thresholding	VARIABLE SELECTION; GENE-EXPRESSION; REGRESSION; DIAGNOSIS; LASSO	Principal component analysis (PCA) is widely used in data processing and dimensionality reduction. However, PCA suffers from the fact that each principal component is a linear combination of all the original variables, thus it is often difficult to interpret the results. We introduce a new method called sparse principal component analysis (SPCA) using the lasso (elastic net) to produce modified principal components with sparse loadings. We first show that PCA can be formulated as a regression-type optimization problem; sparse loadings are then obtained by imposing the lasso (elastic net) constraint on the regression coefficients. Efficient algorithms are proposed to fit our SPCA models for both regular multivariate data and gene expression arrays. We also give a new formula to compute the total variance of modified principal components. As illustrations, SPCA is applied to real and simulated data with encouraging results.	Univ Minnesota, Sch Stat, Minneapolis, MN 55455 USA; Stanford Univ, Dept Stat, Stanford, CA 94305 USA; Stanford Univ, Dept Hlth Res Policy, Stanford, CA 94305 USA	Zou, H (reprint author), Univ Minnesota, Sch Stat, Minneapolis, MN 55455 USA.	hzou@stat.umn.edu; hastie@stat.stanford.edu; tibs@stat.stanford.edu					Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Osborne MR, 2000, IMA J NUMER ANAL, V20, P389, DOI 10.1093/imanum/20.3.389; Jolliffe IT, 2003, J COMPUT GRAPH STAT, V12, P531, DOI 10.1198/1061860032148; Zou H, 2005, J ROY STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x; MCCABE GP, 1984, TECHNOMETRICS, V26, P137, DOI 10.2307/1268108; Efron B, 2004, ANN STAT, V32, P407; Tibshirani R, 2002, P NATL ACAD SCI USA, V99, P6567, DOI 10.1073/pnas.082099299; Alter O, 2000, P NATL ACAD SCI USA, V97, P10101, DOI 10.1073/pnas.97.18.10101; Ramaswamy S, 2001, P NATL ACAD SCI USA, V98, P15149, DOI 10.1073/pnas.211566398; CADIMA J, 1995, J APPL STAT, V22, P203, DOI 10.1080/757584614; Hancock PJB, 1996, MEM COGNITION, V24, P26, DOI 10.3758/BF03197270; Hastie T., 2001, ELEMENTS STAT LEARNI; Hastie T, 2000, GENOME BIOL, V1, P1, DOI DOI 10.1186/GB-2000-1-2-RESEARCH0003; Jeffers JNR, 1967, APPLIED STATISTICS, V16, P225, DOI 10.2307/2985919; Jolliffe I. T., 1986, PRINCIPAL COMPONENT; JOLLIFFE IT, 1995, J APPL STAT, V22, P29, DOI 10.1080/757584395; Mardia KV, 1979, MULTIVARIATE ANAL; VINES SK, 2000, APPL STAT, V49, P441	18	373	397	11	60	AMER STATISTICAL ASSOC	ALEXANDRIA	1429 DUKE ST, ALEXANDRIA, VA 22314 USA	1061-8600			J COMPUT GRAPH STAT	J. Comput. Graph. Stat.	JUN	2006	15	2					265	286		10.1198/106186006X113430		22	Statistics & Probability	Mathematics	049VB	WOS:000238044400001		
J	Lutz, RW; Buhlmann, P				Lutz, Roman Werner; Buhlmann, Peter			Conjugate direction boosting	JOURNAL OF COMPUTATIONAL AND GRAPHICAL STATISTICS			English	Article						forward stepwise variable selection; high-dimensional linear regression; L(0) and L(1)-penalization; least angle regression; orthogonal greedy algorithm	REGRESSION; CLASSIFICATION; ALGORITHMS; PREDICTION; SELECTION	Boosting in the context of linear regression has become more attractive with the invention of least angle regression (LARS), where the connection between the lasso and forward stagewise fitting (boosting) has been established. Earlier it has been found that boosting is a functional gradient optimization. Instead of the gradient, we propose a conjugate direction method (CDBoost). As a result, we obtain a fast forward stepwise variable selection algorithm. The conjugate direction of CDBoost is analogous to the constrained gradient in boosting. Using this analogy, we generalize CDBoost to: (1) include small step sizes (shrinkage) which often improves prediction accuracy; and (2) the nonparametric setting with fitting methods such as trees or splines, where least angle regression and the lasso seem to be unfeasible. The step size in CDBoost has a tendency to govern the degree between L(0)- and L(1)-penalization. This makes CDBoost surprisingly flexible. We compare the different methods on simulated and real datasets. CDBoost achieves the best predictions mainly in complicated settings with correlated covariates, where it is difficult to determine the contribution of a given covariate to the response. The gain of CDBoost over boosting is especially high in sparse cases with high signal to noise ratio and few effective covariates.	ETH, Seminar Stat, CH-8092 Zurich, Switzerland	Lutz, RW (reprint author), ETH, Seminar Stat, CH-8092 Zurich, Switzerland.	lutz@stat.math.ethz.ch; buhlmann@stat.math.ethz.ch	Buhlmann, Peter/A-2107-2013				Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Breiman L, 1999, NEURAL COMPUT, V11, P1493, DOI 10.1162/089976699300016106; Golub TR, 1999, SCIENCE, V286, P531, DOI 10.1126/science.286.5439.531; MALLAT SG, 1993, IEEE T SIGNAL PROCES, V41, P3397, DOI 10.1109/78.258082; Buhlmann P, 2003, J AM STAT ASSOC, V98, P324, DOI 10.1198/016214503000125; Zou H, 2005, J ROY STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x; Efron B, 2004, ANN STAT, V32, P407; Ratsch G, 2001, MACH LEARN, V42, P287, DOI 10.1023/A:1007618119488; BREIMAN L, 1985, J AM STAT ASSOC, V80, P580, DOI 10.2307/2288473; BUHLMANN P, 2005, IN PRESS J MACHINE L; DUFFY N, 2000, P 13 ANN C COMP LEAR; FREUND Y, 1995, INFORM COMPUT, V121, P256, DOI 10.1006/inco.1995.1136; Freund Y., 1996, P 13 INT C MACH LEAR, P148; Meir R, 2002, LECT NOTES ARTIF INT, V2600, P118; Miller A, 2002, SUBSET SELECTION REG, V2nd; Phatak A, 2002, J CHEMOMETR, V16, P361, DOI 10.1002/cem.728; Ratsch G., 2002, MACH LEARN, V48, P193; SCHAPIRE RE, 1990, MACH LEARN, V5, P197, DOI 10.1023/A:1022648800760; Temlyakov VN, 2000, ADV COMPUT MATH, V12, P213, DOI 10.1023/A:1018917218956; Wright SJ, 1999, SPRINGER SERIES OPER; ZEMEL R, 2001, NIPS13 ADV NEURAL IN, V13	22	4	4	0	1	AMER STATISTICAL ASSOC	ALEXANDRIA	732 N WASHINGTON ST, ALEXANDRIA, VA 22314-1943 USA	1061-8600			J COMPUT GRAPH STAT	J. Comput. Graph. Stat.	JUN	2006	15	2					287	311		10.1198/106186006X113548		25	Statistics & Probability	Mathematics	049VB	WOS:000238044400002		
J	Buhlmann, P; Yu, B				Buehlmann, Peter; Yu, Bin			Sparse boosting	JOURNAL OF MACHINE LEARNING RESEARCH			English	Article						lasso; minimum description length (MDL); model selection; nonnegative garrote; regression	MODEL SELECTION; REGRESSION; PREDICTION	We propose Sparse Boosting (the SparseL(2)Boost algorithm), a variant on boosting with the squared error loss. SparseL(2)Boost yields sparser solutions than the previously proposed L(2)Boosting by minimizing some penalized L-2-loss functions, the FPE model selection criteria, through small-step gradient descent. Although boosting may give already relatively sparse solutions, for example corresponding to the soft-thresholding estimator in orthogonal linear models, there is sometimes a desire for more sparseness to increase prediction accuracy and ability for better variable selection: such goals can be achieved with SparseL(2)Boost. We prove an equivalence of SparseL(2)Boost to Breiman's nonnegative garrote estimator for orthogonal linear models and demonstrate the generic nature of SparseL(2)Boost for nonparametric interaction modeling. For an automatic selection of the tuning parameter in SparseL(2)Boost we propose to employ the gMDL model selection criterion which can also be used for early stopping of L(2)Boosting. Consequently, we can select between SparseL(2)Boost and L(2)Boosting by comparing their gMDL scores.	ETH, Seminar Stat, CH-8092 Zurich, Switzerland; Univ Calif Berkeley, Dept Stat, Berkeley, CA 94720 USA	Buhlmann, P (reprint author), ETH, Seminar Stat, CH-8092 Zurich, Switzerland.	BUHLMANN@STAT.MATH.ETHZ.CH; BINYU@STAT.BERKELEY.EDU	Buhlmann, Peter/A-2107-2013				AKAIKE H, 1970, ANN I STAT MATH, V22, P203, DOI 10.1007/BF02506337; BREIMAN L, 1995, TECHNOMETRICS, V37, P373, DOI 10.2307/1269730; Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451; Friedman J, 2000, ANN STAT, V28, P337, DOI 10.1214/aos/1016218223; Hansen MH, 2001, J AM STAT ASSOC, V96, P746, DOI 10.1198/016214501753168398; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Breiman L, 1998, ANN STAT, V26, P801; Breiman L, 1999, NEURAL COMPUT, V11, P1493, DOI 10.1162/089976699300016106; MALLAT SG, 1993, IEEE T SIGNAL PROCES, V41, P3397, DOI 10.1109/78.258082; Buhlmann P, 2003, J AM STAT ASSOC, V98, P324, DOI 10.1198/016214503000125; Efron B, 2004, ANN STAT, V32, P407; Ratsch G, 2001, MACH LEARN, V42, P287, DOI 10.1023/A:1007618119488; West M, 2001, P NATL ACAD SCI USA, V98, P11462, DOI 10.1073/pnas.201162998; Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.1023/A:1018054314350; BUHLMANN P, 2006, IN PRESS ANN STAT, V34; Freund Y., 1996, Machine Learning. Proceedings of the Thirteenth International Conference (ICML '96); FRIEDMAN JH, 1991, ANN STAT, V19, P1, DOI 10.1214/aos/1176347963; Green P, 1994, NONPARAMETRIC REGRES; HANSEN M, 2002, IMS LECT NOTES MONOG, V40; HANSEN M, 1999, IEEE INF THEOR WORKS; Lugosi G, 2004, ANN STAT, V32, P30; LUGOSI G, 2004, ANN STAT, V32, P85; MEINSHAUSEN N, 2005, LASSO RELAXATION TEC; Ratsch G., 2002, MACH LEARN, V48, P193; SPEED TP, 1993, ANN I STAT MATH, V45, P35, DOI 10.1007/BF00773667; Tukey J. W., 1977, EXPLORATORY DATA ANA	26	32	32	1	1	MICROTOME PUBLISHING	BROOKLINE	31 GIBBS STREET, BROOKLINE, MA 02446 USA	1532-4435			J MACH LEARN RES	J. Mach. Learn. Res.	JUN	2006	7						1001	1024				24	Automation & Control Systems; Computer Science, Artificial Intelligence	Automation & Control Systems; Computer Science	152UV	WOS:000245388400005		
J	Shen, XT; Huang, HC				Shen, XT; Huang, HC			Optimal model assessment, selection, and combination	JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION			English	Article						data perturbation; degrees of freedom; dependent; modeling uncertainty; non/semiparametric; parametric; prediction	GENERALIZED CROSS-VALIDATION; INFLATION CRITERION; PREDICTION ERROR; REGRESSION; SHRINKAGE; COMPLEXITY; BOOTSTRAP; INFERENCE; CP	Central to statistical theory and application is statistical modeling, which typically involves choosing a single model or combining a number of models of different sizes and from different sources. Whereas model selection seeks a single best modeling procedure, model combination combines the strength of different modeling procedures. In this article we look at several key issues and argue that model assessment is the key to model selection and combination. Most important, we introduce a general technique of optimal model assessment based on data perturbation, thus yielding optimal selection, in particular model selection and combination. From a frequentist perspective, we advocate model combination over a selected subset of modeling procedures, because it controls bias while reducing variability, hence yielding better performance in terms of the accuracy of estimation and prediction. To realize the potential of model combination, we develop methodologies for determining the optimal tuning parameter, such as weights and subsets for combining via optimal model assessment. We present simulated and real data examples to illustrate main aspects.	Univ Minnesota, Sch Stat, Minneapolis, MN 55455 USA; Acad Sinica, Inst Stat Sci, Taipei 115, Taiwan	Shen, XT (reprint author), Univ Minnesota, Sch Stat, Minneapolis, MN 55455 USA.	xshen@stat.umn.edu; hchuang@stat.sinica.edu.tw	Huang, Hsin-Cheng/H-1743-2011				ABRAMOVICH F, 2006, IN PRESS ANN STAT; Akaike H., 1973, INT S INF THEOR, P267, DOI DOI 10.2307/2334537; Shen XT, 2002, J AM STAT ASSOC, V97, P210, DOI 10.1198/016214502753479356; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Hoeting JA, 1999, STAT SCI, V14, P382; BREIMAN L, 1992, J AM STAT ASSOC, V87, P738, DOI 10.2307/2290212; KASS RE, 1995, J AM STAT ASSOC, V90, P928, DOI 10.2307/2291327; HURVICH CM, 1989, BIOMETRIKA, V76, P297, DOI 10.1093/biomet/76.2.297; Hjort NL, 2003, J AM STAT ASSOC, V98, P879, DOI 10.1198/016214503000000828; DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425; SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136; Rissanen JJ, 1996, IEEE T INFORM THEORY, V42, P40, DOI 10.1109/18.481776; MALLOWS CL, 1973, TECHNOMETRICS, V15, P661, DOI 10.2307/1267380; Buckland ST, 1997, BIOMETRICS, V53, P603, DOI 10.2307/2533961; Bai ZD, 1999, J STAT PLAN INFER, V77, P103, DOI 10.1016/S0378-3758(98)00168-2; Bloomfield P, 1996, ATMOS ENVIRON, V30, P3067, DOI 10.1016/1352-2310(95)00347-9; Burnham K. P., 2002, MODEL SELECTION MULT; Cover T. M., 1991, INFORM THEORY; DEUTSCH MC, 1994, INT J FORECASTING, V81, P425; Efron B, 2004, J AM STAT ASSOC, V99, P619, DOI 10.1198/016214504000000692; EFRON B, 1986, J AM STAT ASSOC, V81, P461, DOI 10.2307/2289236; FOSTER DP, 1994, ANN STAT, V22, P1947, DOI 10.1214/aos/1176325766; Freund Y., 1996, Machine Learning. Proceedings of the Thirteenth International Conference (ICML '96); George EI, 2000, BIOMETRIKA, V87, P731, DOI 10.1093/biomet/87.4.731; GEORGE EI, 1986, ANN STAT, V14, P188, DOI 10.1214/aos/1176349849; HUDSON HM, 1978, ANN STAT, V6, P673; LI KC, 1987, ANN STAT, V15, P958, DOI 10.1214/aos/1176350486; LI KC, 1985, ANN STAT, V13, P1352, DOI 10.1214/aos/1176349742; Ljung G., 1978, BIOMETRIKA, V66, P67; Rao JS, 1999, STAT SINICA, V9, P273; SHAO J, 2000, J STAT PLANNING INFE, V77, P103; Shen XT, 2004, J AM STAT ASSOC, V99, P751, DOI 10.1198/016214504000001097; Shen XT, 2004, TECHNOMETRICS, V46, P306, DOI 10.1198/004017004000000338; Spiegelhalter D., 2002, J ROYAL STAT SOC B, V64, P1; STEIN CM, 1981, ANN STAT, V9, P1135, DOI 10.1214/aos/1176345632; Tibshirani R, 1999, J ROY STAT SOC B, V61, P529, DOI 10.1111/1467-9868.00191; Yang YH, 2001, J AM STAT ASSOC, V96, P574, DOI 10.1198/016214501753168262; Ye JM, 1998, J AM STAT ASSOC, V93, P120, DOI 10.2307/2669609; Zhang CM, 2003, J AM STAT ASSOC, V98, P609, DOI 10.1198/016214503000000521	39	27	28	2	3	AMER STATISTICAL ASSOC	ALEXANDRIA	1429 DUKE ST, ALEXANDRIA, VA 22314 USA	0162-1459			J AM STAT ASSOC	J. Am. Stat. Assoc.	JUN	2006	101	474					554	568		10.1198/016214505000001078		15	Statistics & Probability	Mathematics	049RD	WOS:000238033200013		
J	Tutz, G; Leitenstorfer, F				Tutz, G; Leitenstorfer, F			Response shrinkage estimators in binary regression	COMPUTATIONAL STATISTICS & DATA ANALYSIS			English	Article						logit model; response shrinkage estimator; shrinkage; ridge regression; LASSO; resistant fitting; data sharpening	GENERALIZED LINEAR-MODELS; LOGISTIC-REGRESSION; CLASSIFICATION; DIAGNOSTICS; ROBUSTNESS; SELECTION	A shrinkage type estimator is introduced which has favourable properties in binary regression. The proposed response shrinkage estimator is based on a smoothed version of the observed responses which is obtained by shifting the observation slightly towards the mean of the observations and therefore closer to the underlying probability. Estimates of this type are easily computed by using common program packages. They exist also in cases where the number of variables is large as compared to the number of observations. Comparison to alternative shrinkage methods like ridge regression and LASSO shows that response shrinkage performs rather well. Moreover, a combination of response shrinkage estimators and Pregibon's resistant fitting procedure is considered. The resulting estimate corrects for the overprediction of the resistant fitting in a very simple way. Estimators are compared in simulation studies and applications. (C) 2005 Elsevier B.V. All rights reserved.	Univ Munich, D-80799 Munich, Germany	Tutz, G (reprint author), Univ Munich, Akad Str 1, D-80799 Munich, Germany.	tutz@stat.uni-muenchen.de; leiten@stat.uni-muenchen.de					Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324; Choi E, 1999, BIOMETRIKA, V86, P941, DOI 10.1093/biomet/86.4.941; KUNSCH HR, 1989, J AM STAT ASSOC, V84, P460, DOI 10.2307/2289930; Buhlmann P, 2003, J AM STAT ASSOC, V98, P324, DOI 10.1198/016214503000125; LECESSIE S, 1991, BIOMETRICS, V47, P1267, DOI 10.2307/2532385; CARROLL RJ, 1993, J ROY STAT SOC B MET, V55, P693; LECESSIE S, 1992, APPL STAT-J ROY ST C, V41, P191; Christmann A, 2001, COMPUT STAT DATA AN, V37, P65, DOI 10.1016/S0167-9473(00)00063-3; COPAS JB, 1988, J ROY STAT SOC B MET, V50, P225; FOWLKES EB, 1987, BIOMETRIKA, V74, P503, DOI 10.1093/biomet/74.3.503; Hampel F. R., 1986, ROBUST STAT APPROACH; Hurvich CM, 1998, J ROY STAT SOC B, V60, P271, DOI 10.1111/1467-9868.00125; Klinger A, 2001, J ROY STAT SOC B, V63, P377, DOI 10.1111/1467-9868.00291; LANDWEHR JM, 1984, J AM STAT ASSOC, V79, P61, DOI 10.2307/2288334; Mallows C. L., 1975, SOME TOPICS ROBUSTNE; MARX BD, 1992, STAT MODEL, P227; MCMULLAGH P, 1989, GEN LINEAR MODELS; MEYER D, 2002, 78 U VIENN; NYQUIST H, 1991, APPL STAT-J ROY ST C, V40, P133, DOI 10.2307/2347912; PARZEN A, 2002, J COMPUTATIONAL GRAP, V2, P420; PREGIBON D, 1982, BIOMETRICS, V38, P485, DOI 10.2307/2530463; PREGIBON D, 1981, ANN STAT, V9, P705, DOI 10.1214/aos/1176345513; Rousseeuw PJ, 2003, COMPUT STAT DATA AN, V43, P315, DOI 10.1016/S0167-9473(02)00304-3; Santner T. J., 1989, STAT ANAL DISCRETE D; SEGERSTEDT B, 1992, COMMUN STAT THEORY, V21, P2227, DOI 10.1080/03610929208830909; Tutz G, 2005, STAT COMPUT, V15, P155, DOI 10.1007/s11222-005-1305-x	27	4	4	0	3	ELSEVIER SCIENCE BV	AMSTERDAM	PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS	0167-9473			COMPUT STAT DATA AN	Comput. Stat. Data Anal.	JUN 20	2006	50	10					2878	2901		10.1016/j.csda.2005.04.009		24	Computer Science, Interdisciplinary Applications; Statistics & Probability	Computer Science; Mathematics	047MG	WOS:000237882800023		
J	Donoho, DL				Donoho, David L.			For most large underdetermined systems of equations, the minimal l(1)-norm near-solution approximates the sparsest near-solution	COMMUNICATIONS ON PURE AND APPLIED MATHEMATICS			English	Article							RANDOM MATRICES; REPRESENTATIONS; BASES; DECOMPOSITION; DICTIONARIES; REGRESSION	We consider inexact linear equations y approximate to Phi x where y is a given vector in R-n, Phi is a given n x m matrix, and we wish to find x(0,epsilon) as sparse as possible while obeying parallel to y - Phi x(0,epsilon)parallel to(2) <= epsilon. In general, this requires combinatorial optimization and so is considered intractable. On the other hand, the l(1)-minimization problem min parallel to x parallel to(1) subject to parallel to y - Phi x parallel to(2) <= epsilon is convex and is considered tractable. We show that for most Phi, if the optimally sparse approximation x(0,epsilon) is sufficiently sparse, then the solution x(1,epsilon) of the l(1)-minimization problem is a good approximation to x(0,epsilon). We suppose that the columns of Phi are normalized to the unit l(2)-norm, and we place uniform measure on such Phi. We study the underdetermined case where m similar to tau n and tau > 1, and prove the existence of rho = rho(tau) > 0 and C = C(rho, tau) so that for large n and for all Phi's except a negligible fraction, the following approximate sparse solution property of Phi holds: for every y having an approximation parallel to y - Phi x(0)parallel to(2) < epsilon by a coefficient vector x(0) is an element of R-m with fewer than rho . n nonzeros, parallel to x(1,epsilon) - x(0)parallel to(2) <= C. epsilon. This has two implications. First, for most Phi, whenever the combinatorial optimization result x(0,epsilon) would be very sparse, x(1,epsilon) is a good approximation to x(0,epsilon). Second, suppose we are given noisy data obeying y = Phi x(0) + z where the unknown x(0) is known to be sparse and the noise parallel to z parallel to(2) <= epsilon. For most Phi, noise-tolerant l(1)-minimization will stably recover x(0) from y in the presence of noise z. We also study the barely determined case m = n and reach parallel conclusions by slightly different arguments. Proof techniques include the use of almost-spherical sections in Banach space theory and concentration of measure for eigenvalues of random matrices. (c) 2006 Wiley Periodicals, Inc.	Stanford Univ, Dept Stat, Stanford, CA 94305 USA	Donoho, DL (reprint author), Stanford Univ, Dept Stat, Stanford, CA 94305 USA.	donoho@stanford.edu					Donoho DL, 2001, IEEE T INFORM THEORY, V47, P2845, DOI 10.1109/18.959265; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Gribonval R, 2003, IEEE T INFORM THEORY, V49, P3320, DOI 10.1109/TIT.2003.820031; MALLAT SG, 1993, IEEE T SIGNAL PROCES, V41, P3397, DOI 10.1109/78.258082; Tropp JA, 2004, IEEE T INFORM THEORY, V50, P2231, DOI 10.1109/TIT.2004.834793; Efron B, 2004, ANN STAT, V32, P407; Donoho DL, 2006, COMMUN PUR APPL MATH, V59, P797, DOI 10.1002/cpa.20132; Elad M, 2002, IEEE T INFORM THEORY, V48, P2558, DOI 10.1109/TIT.2002.801410; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P6, DOI 10.1109/TIT.2005.860430; Band Johnson W, 2001, HDB GEOMETRY BANACH, V1, P317, DOI 10.1016/S1874-5849(01)80010-3; CANDES EJ, IN PRESS IEEE T INFO; Chen Y, 1998, NONCON OPTIM ITS APP, V20, P1; COIFMAN RR, 1993, PROGRESS IN WAVELET ANALYSIS AND APPLICATIONS, P77; Donoho DL, 2003, P NATL ACAD SCI USA, V100, P2197, DOI 10.1073/pnas.0437847100; DONOHO DL, 1992, J ROY STAT SOC B MET, V54, P41; Dvoretzky A., 1961, P INT S LIN SPAC JER, P123; EDELMAN A, 1988, SIAM J MATRIX ANAL A, V9, P543, DOI 10.1137/0609045; ELKAROUI N, 2004, THESIS STANFORD U; FIGIEL T, 1977, ACTA MATH-DJURSHOLM, V139, P53, DOI 10.1007/BF02392234; FUCHS JJ, UNPUB IEEE T INFORM; Fuchs JJ, 2004, IEEE T INFORM THEORY, V50, P1341, DOI 10.1109/TIT.2004.828141; Golub G., 1989, MATRIX COMPUTATIONS; Kashin B.S., 1977, IZV AN SSSR M, V41, P334; Ledoux M., 2001, MATH SURVEYS MONOGRA, V89; Milman V. D., 1986, LECT NOTES MATH, V1200; NATARAJAN BK, 1995, SIAM J COMPUT, V24, P227, DOI 10.1137/S0097539792240406; Parlett B. N., 1980, PRENTICE HALL SERIES; Pisier G., 1989, CAMBRIDGE TRACTS MAT, V94; SZAREK SJ, 1978, B ACAD POL SCI SMAP, V26, P691; Szarek S. J., 1991, Journal of Complexity, V7, DOI 10.1016/0885-064X(91)90002-F; SZAREK SJ, 1990, AM J MATH, V112, P899, DOI 10.2307/2374731; TROPP JA, IN PRESS IEEE T INFO	32	200	227	1	20	WILEY-BLACKWELL	HOBOKEN	111 RIVER ST, HOBOKEN 07030-5774, NJ USA	0010-3640			COMMUN PUR APPL MATH	Commun. Pure Appl. Math.	JUL	2006	59	7					907	934		10.1002/cpa.20131		28	Mathematics, Applied; Mathematics	Mathematics	044PZ	WOS:000237686000001		
J	Hoti, F; Sillanpaa, MJ				Hoti, F; Sillanpaa, MJ			Bayesian mapping of genotype x expression interactions in quantitative and qualitative traits	HEREDITY			English	Article						gene expression; QTL; molecular markers; sparse method; Bayes	HUMAN GENE-EXPRESSION; ALLELIC VARIATION; MICROARRAY DATA; COMPLEX TRAITS; HUMAN GENOME; LOCI; ASSOCIATION; IDENTIFICATION; SELECTION; PATHWAYS	A novel Bayesian gene mapping method, which can simultaneously utilize both molecular marker and gene expression data, is introduced. The approach enables a quantitative or qualitative phenotype to be expressed as a linear combination of the marker genotypes, gene expression levels, and possible genotype x gene expression interactions. The interaction data, given as marker-gene pairs, contains possible in cis and in trans effects obtained from earlier allelic expression studies, genetical genomics studies, biological hypotheses, or known pathways. The method is presented for an inbred line cross design and can be easily generalized to handle other types of populations and designs. The model selection is based on the use of effect-specific variance components combined with Jeffreys' non-informative prior - the method operates by adaptively shrinking marker, expression, and interaction effects toward zero so that non-negligible effects are expected to occur only at very few positions. The estimation of the model parameters and the handling of missing genotype or expression data is performed via Markov chain Monte Carlo sampling. The potential of the method including heritability estimation is presented using simulated examples and novel summary statistics. The method is also applied to a real yeast data set with known pathways.	Univ Helsinki, Rolf Nevanlinna Inst, Dept Math & Stat, FIN-00014 Helsinki, Finland	Sillanpaa, MJ (reprint author), Univ Helsinki, Rolf Nevanlinna Inst, Dept Math & Stat, POB 68, FIN-00014 Helsinki, Finland.	mjs@rolf.helsinki.fi					ALBERT JH, 1993, J AM STAT ASSOC, V88, P669, DOI 10.2307/2290350; CHIB S, 1995, AM STAT, V49, P327, DOI 10.2307/2684568; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; HASTINGS WK, 1970, BIOMETRIKA, V57, P97, DOI 10.2307/2334940; Conti DV, 2003, HUM HERED, V56, P83, DOI 10.1159/000073736; Butte A, 2002, NAT REV DRUG DISCOV, V1, P951, DOI 10.1038/nrd.961; Sellers TA, 2003, GENET EPIDEMIOL, V24, P83, DOI 10.1002/gepi.10226; Goeman JJ, 2004, BIOINFORMATICS, V20, P93, DOI 10.1093/bioinformatics/btg382; Yan H, 2002, SCIENCE, V297, P1143, DOI 10.1126/science.1072545; Schadt EE, 2003, NATURE, V422, P297, DOI 10.1038/nature01482; Wayne ML, 2002, P NATL ACAD SCI USA, V99, P14903, DOI 10.1073/pnas.222549199; Ashburner M, 2000, NAT GENET, V25, P25; Quackenbush J, 2001, NAT REV GENET, V2, P418, DOI 10.1038/35076576; Hubner N, 2005, NAT GENET, V37, P243, DOI 10.1038/ng1522; Brem RB, 2002, SCIENCE, V296, P752, DOI 10.1126/science.1069516; Meuwissen THE, 2001, GENETICS, V157, P1819; Jansen RC, 2001, TRENDS GENET, V17, P388, DOI 10.1016/S0168-9525(01)02310-1; Knight JC, 2004, TRENDS GENET, V20, P113, DOI 10.1016/j.tig.2004.01.001; Lo HS, 2003, GENOME RES, V13, P1855, DOI 10.1101/gr.1006603; Auger DL, 2005, GENETICS, V169, P389, DOI 10.1534/genetics.104.032987; Aune TM, 2004, GENET EPIDEMIOL, V27, P162, DOI 10.1002/gepi.20013; Basten C.J., 1994, P 5 WORLD C GEN APPL, V22, P65; Basten CJ, 2003, QTL CARTOGRAPHER VER; Gibbs RA, 2003, NATURE, V426, P789, DOI 10.1038/nature02168; Broman KW, 2002, J R STAT SOC B, V64, P641, DOI 10.1111/1467-9868.00354; Bystrykh L, 2005, NAT GENET, V37, P225, DOI 10.1038/ng1497; CASELLA G, 1992, AM STAT, V46, P164; Chesler EJ, 2005, NAT GENET, V37, P233, DOI 10.1038/ng1518; Darvasi A, 2003, NATURE, V422, P269, DOI 10.1038/422269a; Devlin B, 2003, GENET EPIDEMIOL, V25, P36, DOI 10.1002/gepi.10237; Doerge RW, 2002, NAT REV GENET, V3, P43, DOI 10.1038/nrg703; Figueiredo MAT, 2003, IEEE T PATTERN ANAL, V25, P1150, DOI 10.1109/TPAMI.2003.1227989; GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721; Gibson G, 2003, HEREDITY, V90, P1, DOI 10.1038/sj.hdy.6800195; Heighway J, 2005, ANN HUM GENET, V69, P127, DOI 10.1046/j.1529-8817.2004.00135.x; Altshuler D, 2005, NATURE, V437, P1299, DOI 10.1038/nature04226; Jansen RC, 2004, TRENDS GENET, V20, P223, DOI 10.1016/j.tig.2004.03.002; Jansen RC, 2003, NAT REV GENET, V4, P145, DOI 10.1038/nrg996; Jiang CJ, 1997, GENETICA, V101, P47, DOI 10.1023/A:1018394410659; Kell DB, 2002, TRENDS GENET, V18, P555, DOI 10.1016/S0168-9525(02)02765-8; Kilpikari R, 2003, GENET EPIDEMIOL, V25, P122, DOI 10.1002/gepi.10257; Kopp A, 2003, GENETICS, V163, P771; Kraft P, 2003, TRENDS BIOTECHNOL, V21, P377, DOI 10.1016/S0167-7799(03)00191-4; Kraft P, 2003, AM J HUM GENET, V72, P1323, DOI 10.1086/375167; LAN H, 2004, GENETICS, V164, P1607; LANDER ES, 1994, SCIENCE, V265, P2037, DOI 10.1126/science.8091226; Lohmueller KE, 2003, NAT GENET, V33, P177, DOI 10.1038/ng1071; Lopes HF, 2004, STAT SINICA, V14, P41; Lu Y, 2004, GENETICS, V168, P2395, DOI 10.1534/genetics.104.031666; Morley M, 2004, NATURE, V430, P743, DOI 10.1038/nature02797; Perez-Enciso M, 2004, GENETICS, V166, P547, DOI 10.1534/genetics.166.1.547; Perez-Enciso M, 2003, GENETICS, V164, P1597; Risch N., 1996, SCIENCE, V273, P1616; Ronald J, 2005, GENOME RES, V15, P284, DOI 10.1101/gr.2850605; Sen S, 2001, GENETICS, V159, P371; Shevade SK, 2003, BIOINFORMATICS, V19, P2246, DOI 10.1093/bioinformatics/btg308; Sillanpaa MJ, 2004, ANN HUM GENET, V68, P646, DOI 10.1046/j.1529-8817.2004.00122.x; Sillanpaa MJ, 2002, TRENDS GENET, V18, P301, DOI 10.1016/S0168-9525(02)02688-4; Sillanpaa MJ, 1998, GENETICS, V148, P1373; Sillanpaa MJ, 2005, GENETICS, V169, P427, DOI 10.1534/genetics.104.032680; Thomas DC, 2005, CANCER EPIDEM BIOMAR, V14, P557, DOI 10.1158/1055-9965.EPI-14-3-EDB; Wang H, 2005, GENETICS, V170, P465, DOI 10.1534/genetics.104.039354; Watts JA, 2002, AM J HUM GENET, V71, P791, DOI 10.1086/342974; West M., 2003, BAYESIAN STAT, P723; Xu SZ, 2003, GENETICS, V163, P789; Zhang M, 2005, GENETICS, V169, P2305, DOI 10.1534/genetics.104.034181; Zhang YM, 2005, HEREDITY, V95, P96, DOI 10.1038/sj.hdy.6800702	67	37	40	1	2	NATURE PUBLISHING GROUP	LONDON	MACMILLAN BUILDING, 4 CRINAN ST, LONDON N1 9XW, ENGLAND	0018-067X			HEREDITY	Heredity	JUL	2006	97	1					4	18		10.1038/sj.hdy.6800817		15	Ecology; Evolutionary Biology; Genetics & Heredity	Environmental Sciences & Ecology; Evolutionary Biology; Genetics & Heredity	057IT	WOS:000238590200003	16670709	
J	Renzullo, LJ; Blanchfield, AL; Powell, KS				Renzullo, Luigi J.; Blanchfield, Annette L.; Powell, Kevin S.			A method of wavelength selection and spectral discrimination of hyperspectral reflectance spectrometry	IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING			English	Article						cross-validation; discriminant analysis; reflectance spectrometry; regularized regression	MULTIPLE MEASUREMENTS; BAND SELECTION; CLASSIFICATION; REGRESSION	Regularized regression was used in a discriminant analysis framework to identify the key spectral regions for the separation of hyperspectral reflectance spectra of grapevine leaves. Choice of regularization parameter values was guided by cross-validation: for the field-measured spectra, estimated validation errors < 12% were used; whereas for the glasshouse-measured spectra, validation errors were estimated to be > 60% so choice was based on training error of < 20%. Out of the 1151 wavelength bands available in the data, the analysis selected 12 or so wavelengths that can be used to differentiate the groups of vines, studied. Moreover these wavelengths were repeatedly observed to occur in spectral regions known to be linked to plant physiology and condition, specifically 500-550 nm, 660-690 nm; 700-760 nm; and 900-1450 mn.	CSIRO, Canberra, ACT 2601, Australia	Renzullo, LJ (reprint author), CSIRO, Canberra, ACT 2601, Australia.	Luigi.Renzullo@csiro.au; Annette.Blanchfield@dpi.vic.gov.au; Kevin.Powell@dpi.vic.gov.au	Renzullo, Luigi/D-5797-2011	Renzullo, Luigi/0000-0003-3056-4109			Sims DA, 2002, REMOTE SENS ENVIRON, V81, P337, DOI 10.1016/S0034-4257(02)00010-X; Yu B, 1999, IEEE T GEOSCI REMOTE, V37, P2569, DOI 10.1109/36.789651; Hastie T, 2004, BIOSTATISTICS, V5, P329, DOI 10.1093/biostatistics/kxh010; HASTIE T, 1994, J AM STAT ASSOC, V89, P1255, DOI 10.2307/2290989; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; HOERL AE, 1970, TECHNOMETRICS, V12, P55; Curran PJ, 2001, REMOTE SENS ENVIRON, V76, P349, DOI 10.1016/S0034-4257(01)00182-1; Coops NC, 2003, IEEE T GEOSCI REMOTE, V41, P1338, DOI 10.1109/TGRS.2003.813135; Fisher RA, 1938, ANN EUGENIC, V8, P376; Bajcsy P, 2004, PHOTOGRAMM ENG REM S, V70, P793; BENEDIKTSSON JA, 1995, IEEE T GEOSCI REMOTE, V33, P1194, DOI 10.1109/36.469483; Blackburn GA, 1999, REMOTE SENS ENVIRON, V70, P224, DOI 10.1016/S0034-4257(99)00048-6; HASTIE T, 1995, ANN STAT, V23, P73, DOI 10.1214/aos/1176324456; BURNS AE, 2005, UNPUB AUST J GRAPE W; Clark R. N., 2003, J GEOPHYS RES, V108, p[5131, 5, 5], DOI 10.1029/2002JE001847; CSILLAG F, 1993, REMOTE SENS ENVIRON, V43, P231, DOI 10.1016/0034-4257(93)90068-9; Curran PJ, 1997, IEEE T GEOSCI REMOTE, V35, P415, DOI 10.1109/36.563280; Fisher RA, 1936, ANN EUGENIC, V7, P179; FRIEDMAN JH, 1989, J AM STAT ASSOC, V84, P165, DOI 10.2307/2289860; HSU PH, 2003, THESIS NATL CHEN KUN; JIA XP, 1994, IEEE T GEOSCI REMOTE, V32, P274; KIIVERI HT, 1992, TECHNOMETRICS, V34, P321, DOI 10.2307/1270038; LAMB DW, EARLY DETECTION PHYL; RENZULLO L, 2004, AUSTR NZ GRAPEGROWER, P126; RENZULLO LJ, IN PRESS ACTA HORTIC; Renzullo LJ, 2006, INT J REMOTE SENS, V27, P817, DOI 10.1080/01431160500239164; Tikhonov AN, 1977, SOLUTIONS ILL POSED; Tsai F, 1998, REMOTE SENS ENVIRON, V66, P41, DOI 10.1016/S0034-4257(98)00032-7; ZARCOTEJADA P, 2000, REMOTE SENS ENVIRON, V75, P582	29	11	11	2	8	IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC	PISCATAWAY	445 HOES LANE, PISCATAWAY, NJ 08855 USA	0196-2892			IEEE T GEOSCI REMOTE	IEEE Trans. Geosci. Remote Sensing	JUL	2006	44	7	2				1986	1994		10.1109/TGRS.2006.870441		9	Geochemistry & Geophysics; Engineering, Electrical & Electronic; Remote Sensing; Imaging Science & Photographic Technology	Geochemistry & Geophysics; Engineering; Remote Sensing; Imaging Science & Photographic Technology	061IL	WOS:000238864900005		
J	Druilhet, P; Mom, A				Druilhet, P; Mom, A			PLS regression: A directional signal-to-noise ratio approach	JOURNAL OF MULTIVARIATE ANALYSIS			English	Article						biased regression; constrained least squares; regression on components; partial least squares; principal components; shrinkage	PARTIAL LEAST-SQUARES; SHRINKAGE; COMPONENTS; SELECTION; RELEVANT	We present a new approach to univariate partial least squares regression (PLSR) based on directional signal-to-noise ratios (SNRs). We show how PLSR, unlike principal components regression, takes into account the actual value and not only the variance of the ordinary least squares (OLS) estimator. We find an orthogonal sequence of directions associated with decreasing SNR. Then, we state partial least squares estimators as least squares estimators constrained to be null on the last directions. We also give another procedure that shows how PLSR rebuilds the OLS estimator iteratively by seeking at each step the direction with the largest difference of signals over the noise. The latter approach does not involve any arbitrary scale or orthogonality constraints. (c) 2005 Elsevier Inc. All rights reserved.	ENSAI, CREST, Bruz, France; Univ Rennes 2, Stat Lab, F-35043 Rennes, France	Druilhet, P (reprint author), ENSAI, CREST, Campus Ker Lann,35, Bruz, France.	druilhet@ensai.fr					Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; HELLAND IS, 1988, COMMUN STAT SIMULAT, V17, P581, DOI 10.1080/03610918808812681; FRANK IE, 1993, TECHNOMETRICS, V35, P109, DOI 10.2307/1269656; Bibby J., 1977, PREDICTION IMPROVED; BREIMAN L, 1992, INT STAT REV, V60, P291, DOI 10.2307/1403680; Brown P. J., 1993, MEASUREMENT REGRESSI; Butler NA, 2000, J ROY STAT SOC B, V62, P585, DOI 10.1111/1467-9868.00252; DEJONG S, 1995, J CHEMOMETR, V9, P323, DOI 10.1002/cem.1180090406; Goutis C, 1996, ANN STAT, V24, P816; HELLAND IS, 1990, SCAND J STAT, V17, P97; HELLAND IS, 1994, J AM STAT ASSOC, V89, P583, DOI 10.2307/2290861; HELLAND IS, 1992, J ROY STAT SOC B MET, V54, P637; Lingjaerde OC, 2000, SCAND J STAT, V27, P459, DOI 10.1111/1467-9469.00201; Martens H., 1989, MULTIVARIATE CALIBRA; Rao C.R., 1974, LINEAR ALGEBRA APPL, V9, P155; Seber G. A. F., 1984, MULTIVARIATE OBSERVA; STONE M, 1990, J ROY STAT SOC B MET, V52, P237; 1995, UMETRICS MULTIVARIAT	18	10	10	1	2	ELSEVIER INC	SAN DIEGO	525 B STREET, STE 1900, SAN DIEGO, CA 92101-4495, UNITED STATES	0047-259X			J MULTIVARIATE ANAL	J. Multivar. Anal.	JUL	2006	97	6					1313	1329		10.1016/j.jmva.2005.06.009		17	Statistics & Probability	Mathematics	051JY	WOS:000238158600005		
J	Zhang, HH; Lin, Y				Zhang, Hao Helen; Lin, Yi			Component selection and smoothing for nonparametric regression in exponential families	STATISTICA SINICA			English	Article						exponential family; LASSO; nonparametric regression; penalized likelihood; smoothing spline ANOVA	PENALIZED LIKELIHOOD; VARIABLE SELECTION	We propose a new penalized likelihood method for model selection and nonparametric regression in exponential families. In the framework of smoothing spline ANOVA, our method employs a regularization with the penalty functional being the sum of the reproducing kernel Hilbert space norms of functional components in the ANOVA decomposition. It generalizes the LASSO in the linear regression to the nonparametric context, and conducts component selection and smoothing simultaneously. Continuous and categorical variables are treated in a unified fashion. We discuss the connection of the method to the traditional smoothing spline penalized likelihood estimation. We show that an equivalent formulation of the method leads naturally to an iterative algorithm. Simulations and examples are used to demonstrate the performances of the method.	N Carolina State Univ, Dept Stat, Raleigh, NC 27695 USA; Univ Wisconsin, Dept Stat, Madison, WI 53706 USA	Zhang, HH (reprint author), N Carolina State Univ, Dept Stat, Raleigh, NC 27695 USA.	hzhang2@stat.ncsu.edu; yilin@stat.wisc.edu					BREIMAN L, 1995, TECHNOMETRICS, V37, P373, DOI 10.2307/1269730; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; Efron B, 2004, ANN STAT, V32, P407; FRIEDMAN JH, 1989, TECHNOMETRICS, V31, P3, DOI 10.2307/1270359; BACH F, 2004, IN PRESS ADV NEURAL; Bach Francis R., 2004, P 21 INT C MACH LEAR; Breiman L., 1984, CLASSIFICATION REGRE; FRIEDMAN JH, 1991, ANN STAT, V19, P1, DOI 10.1214/aos/1176347963; Gu C, 2002, CAN J STAT, V30, P619, DOI 10.2307/3316100; GU C, 1992, J AM STAT ASSOC, V87, P1051, DOI 10.2307/2290642; Hastie T., 1990, GENERALIZED ADDITIVE; HASTIE T, 1989, TECHNOMETRICS, V31, P3; KIMELDOR.G, 1971, J MATH ANAL APPL, V33, P82, DOI 10.1016/0022-247X(71)90184-3; LIN Y, 2002, 1072 U WISC; Lim TS, 2000, MACH LEARN, V40, P203, DOI 10.1023/A:1007608224229; Mangasarian OL, 1990, SIAM NEWS, V23, P1; Ruppert D, 2000, AUST NZ J STAT, V42, P205, DOI 10.1111/1467-842X.00119; Tapia R.A., 1978, NONPARAMETRIC PROBAB; Wahba G., 1990, SIAM CBMS NSF REGION, V59; Wahba G, 1995, ANN STAT, V23, P1865; Xiang D., 1998, P 1997 ASA JOINT STA, P94; YAU P, 2002, J COMPUT GRAPH STAT, V12, P23; Zhang HH, 2004, J AM STAT ASSOC, V99, P659, DOI 10.1198/016214504000000593	24	16	16	0	1	STATISTICA SINICA	TAIPEI	C/O DR H C HO, INST STATISTICAL SCIENCE, ACADEMIA SINICA, TAIPEI 115, TAIWAN	1017-0405			STAT SINICA	Stat. Sin.	JUL	2006	16	3					1021	1041				21	Statistics & Probability	Mathematics	078SO	WOS:000240123900019		
J	Chen, YH; Kao, JT				Chen, Yi-Hau; Kao, Jau-Tsuen			Multinomial logistic regression approach to haplotype association analysis in population-based case-control studies	BMC GENETICS			English	Article							MAXIMUM-LIKELIHOOD-ESTIMATION; GENOTYPE DATA; INFERENCE; MODELS; FREQUENCIES; INFORMATION; PHASE; GENE	Background: The genetic association analysis using haplotypes as basic genetic units is anticipated to be a powerful strategy towards the discovery of genes predisposing human complex diseases. In particular, the increasing availability of high-resolution genetic markers such as the single-nucleotide polymorphisms (SNPs) has made haplotype-based association analysis an attractive alternative to single marker analysis. Results: We consider haplotype association analysis under the population-based case-control study design. A multinomial logistic model is proposed for haplotype analysis with unphased genotype data, which can be decomposed into a prospective logistic model for disease risk as well as a model for the haplotype-pair distribution in the control population. Environmental factors can be readily incorporated and hence the haplotype-environment interaction can be assessed in the proposed model. The maximum likelihood estimation with unphased genotype data can be conveniently implemented in the proposed model by applying the EM algorithm to a prospective multinomial logistic regression model and ignoring the case-control design. We apply the proposed method to the hypertriglyceridemia study and identifies 3 haplotypes in the apolipoprotein A5 gene that are associated with increased risk for hypertriglyceridemia. A haplotype-age interaction effect is also identified. Simulation studies show that the proposed estimator has satisfactory finite-sample performances. Conclusion: Our results suggest that the proposed method can serve as a useful alternative to existing methods and a reliable tool for the case-control haplotype-based association analysis.	Acad Sinica, Inst Stat Sci, Taipei 11529, Taiwan; Natl Taiwan Univ, Coll Med, Dept Clin Lab Sci & Med Biotechnol, Taipei 10764, Taiwan	Chen, YH (reprint author), Acad Sinica, Inst Stat Sci, Taipei 11529, Taiwan.	yhchen@stat.sinica.edu.tw; jtkao@ha.mc.ntu.edu.tw		KAO, JAU-TSUEN/0000-0003-1918-0366			Agresti A., 2002, CATEGORICAL DATA ANA; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Epstein MP, 2003, AM J HUM GENET, V73, P1316, DOI 10.1086/380204; EXCOFFIER L, 1995, MOL BIOL EVOL, V12, P921; BENJAMINI Y, 1995, J ROY STAT SOC B MET, V57, P289; Stephens M, 2001, AM J HUM GENET, V68, P978, DOI 10.1086/319501; Chatterjee N, 2005, BIOMETRIKA, V92, P399, DOI 10.1093/biomet/92.2.399; CORDELL HJ, 2002, AM J HUM GENET, V72, P351; Devlin B, 2003, GENET EPIDEMIOL, V25, P36, DOI 10.1002/gepi.10237; Hartl DL, 1988, PRIMER POPULATION GE; Hosmer DW, 2000, APPL LOGISTIC REGRES; Kao JT, 2003, HUM MOL GENET, V12, P2533, DOI 10.1093/hmg/ddg255; Lake SL, 2003, HUM HERED, V55, P56, DOI 10.1159/000071811; Li SSY, 2003, BIOSTATISTICS, V4, P513, DOI 10.1093/biostatistics/4.4.513; LOUIS TA, 1982, J ROY STAT SOC B MET, V44, P226; Niu TH, 2002, AM J HUM GENET, V70, P157, DOI 10.1086/338446; Pennacchio LA, 2001, SCIENCE, V294, P169, DOI 10.1126/science.1064852; PRENTICE RL, 1979, BIOMETRIKA, V66, P403, DOI 10.1093/biomet/66.3.403; Satten GA, 2004, GENET EPIDEMIOL, V27, P192, DOI 10.1002/gepi.20020; Schaid DJ, 2004, GENET EPIDEMIOL, V27, P348, DOI 10.1002/gepi.20037; Scott AJ, 1997, BIOMETRIKA, V84, P57, DOI 10.1093/biomet/84.1.57; Spinka C, 2005, GENET EPIDEMIOL, V29, P108, DOI 10.1002/gepi.20085; Stram DO, 2003, HUM HERED, V55, P179, DOI 10.1159/000073202; Tzeng JY, 2006, AM J HUM GENET, V78, P231, DOI 10.1086/500025; Zhao LP, 2003, AM J HUM GENET, V72, P1231, DOI 10.1086/375140	25	5	6	0	0	BIOMED CENTRAL LTD	LONDON	MIDDLESEX HOUSE, 34-42 CLEVELAND ST, LONDON W1T 4LB, ENGLAND	1471-2156			BMC GENET	BMC Genet.	AUG 15	2006	7								43	10.1186/1471-2156-7-43		12	Genetics & Heredity	Genetics & Heredity	082ZJ	WOS:000240425400001	16907993	
J	Huang, J; Ma, SG; Xie, HL				Huang, Jian; Ma, Shuangge; Xie, Huiliang			Regularized estimation in the accelerated failure time model with high-dimensional covariates	BIOMETRICS			English	Article						cross-validation; LASSO; threshold-gradient-directed regularization; variable selection; weighted least squares	COX REGRESSION-MODEL; LINEAR RANK-TESTS; CENSORED-DATA; VARIABLE SELECTION; LARGE-SAMPLE; LASSO	We consider two regularization approaches, the LASSO and the threshold-gradient-directed regularization, for estimation and variable selection in the accelerated failure time model with multiple covariates based on Stute's weighted least squares method. The Stute estimator uses Kaplan-Meier weights to account for censoring in the least squares criterion. The weighted least squares objective function makes the adaptation of this approach to multiple covariate settings computationally feasible. We use V-fold cross-validation and a modified Akaike's Information Criterion for tuning parameter selection, and a bootstrap approach for variance estimation. The proposed method is evaluated using simulations and demonstrated on a real data example.	Univ Iowa, Dept Stat & Actuarial Sci, Iowa City, IA 52242 USA; Univ Iowa, Program Publ Hlth Genet, Iowa City, IA 52242 USA; Univ Washington, Dept Biostat, Seattle, WA 98115 USA	Huang, J (reprint author), Univ Iowa, Dept Stat & Actuarial Sci, Iowa City, IA 52242 USA.	jian@stat.uiowa.edu					Akaike H., 1973, 2 INT S INF THEOR, P267; ANDERSEN PK, 1982, ANN STAT, V10, P1100, DOI 10.1214/aos/1176345976; Knight K, 2000, ANN STAT, V28, P1356; Friedman J, 2000, ANN STAT, V28, P337, DOI 10.1214/aos/1016218223; Gui J, 2005, BIOINFORMATICS, V21, P3001, DOI 10.1093/bioinformatics/bti422; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Zou H, 2005, J ROY STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x; Efron B, 2004, ANN STAT, V32, P407; BUCKLEY J, 1979, BIOMETRIKA, V66, P429; COX DR, 1972, J R STAT SOC B, V34, P187; Fleming T. R., 1991, COUNTING PROCESSES S; Friedman JH, 2004, GRADIENT DIRECTED RE; GUI J, 2005, P PAC S BIOC 2005; Hastie T., 2001, ELEMENTS STAT LEARNI; Jin ZZ, 2003, BIOMETRIKA, V90, P341, DOI 10.1093/biomet/90.2.341; Kim Y, 2004, P 21 INT C MACH LEAR; Prentice RL, 1980, STAT ANAL FAILURE TI; PRENTICE RL, 1978, BIOMETRIKA, V65, P167, DOI 10.1093/biomet/65.1.167; RITOV Y, 1990, ANN STAT, V18, P303, DOI 10.1214/aos/1176347502; STONE M, 1974, J R STAT SOC B, V36, P111; STUTE W, 1993, J MULTIVARIATE ANAL, V45, P89, DOI 10.1006/jmva.1993.1028; STUTE W, 1993, ANN STAT, V1, P1351; Stute W, 1996, SCAND J STAT, V23, P461; Tibshirani R, 1997, STAT MED, V16, P385, DOI 10.1002/(SICI)1097-0258(19970228)16:4<385::AID-SIM380>3.0.CO;2-3; TSIATIS AA, 1990, ANN STAT, V18, P354, DOI 10.1214/aos/1176347504; Van der Vaart A., 1996, WEAK CONVERGENCE EMP; Wahba G., 1990, CBMS NSF REGIONAL C, V59; WEI LJ, 1992, STAT MED, V11, P1871, DOI 10.1002/sim.4780111409; WEI LJ, 1990, BIOMETRIKA, V77, P845, DOI 10.1093/biomet/77.4.845; YING ZL, 1993, ANN STAT, V21, P76, DOI 10.1214/aos/1176349016	30	36	36	1	5	BLACKWELL PUBLISHING	OXFORD	9600 GARSINGTON RD, OXFORD OX4 2DQ, OXON, ENGLAND	0006-341X			BIOMETRICS	Biometrics	SEP	2006	62	3					813	820		10.1111/j.1541-0420.2006.00562.x		8	Biology; Mathematical & Computational Biology; Statistics & Probability	Life Sciences & Biomedicine - Other Topics; Mathematical & Computational Biology; Mathematics	086YE	WOS:000240708300024	16984324	
J	Lee, Y; Kim, Y; Lee, S; Koo, JY				Lee, Yoonkyung; Kim, Yuwon; Lee, Sangjun; Koo, Ja-Yong			Structured multicategory support vector machines with analysis of variance decomposition	BIOMETRIKA			English	Article						classification; feature selection; linear programming; l(1)-norm penalty; quadratic programming; regularisation method; reproducing kernel Hilbert space	CLASSIFICATION; SELECTION; REGULARIZATION; REGRESSION; NETWORKS	The support vector machine has been a popular choice of classification method for many applications in machine learning. While it often outperforms other methods in terms of classification accuracy, the implicit nature of its solution renders the support vector machine less attractive in providing insights into the relationship between covariates and classes. Use of structured kernels can remedy the drawback. Borrowing the flexible model-building idea of functional analysis of variance decomposition, we consider multicategory support vector machines with analysis of variance kernels in this paper. An additional penalty is imposed on the sum of weights of functional subspaces, which encourages a sparse representation of the solution. Incorporation of the additional penalty enhances the interpretability of a resulting classifier with often improved accuracy. The proposed method is demonstrated through simulation studies and an application to real data.	Ohio State Univ, Dept Stat, Columbus, OH 43210 USA; Seoul Natl Univ, Stat Res Ctr Complex Syst, Seoul 156747, South Korea; Seoul Natl Univ, Dept Stat, Seoul 156747, South Korea; Korea Univ, Dept Stat, Seoul 136701, South Korea	Lee, Y (reprint author), Ohio State Univ, Dept Stat, Columbus, OH 43210 USA.	yklee@stat.ohio-state.edu; gary@stats.snu.ac.kr; seaphant@stats.snu.ac.kr; jykoo@korea.ac.kr	Lee, Yoonkyung/K-4360-2015	Lee, Yoonkyung/0000-0002-5756-6588			BREIMAN L, 1995, TECHNOMETRICS, V37, P373, DOI 10.2307/1269730; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Khan J, 2001, NAT MED, V7, P673, DOI 10.1038/89044; Chapelle O, 2002, MACH LEARN, V46, P131, DOI 10.1023/A:1012450327387; Micchelli CA, 2005, J MACH LEARN RES, V6, P1099; Guyon I, 2002, MACH LEARN, V46, P389, DOI 10.1023/A:1012487302797; Evgeniou T, 2000, ADV COMPUT MATH, V13, P1, DOI 10.1023/A:1018946025316; Efron B, 2004, ANN STAT, V32, P407; Kohavi R, 1997, ARTIF INTELL, V97, P273, DOI 10.1016/S0004-3702(97)00043-X; ARGYRIOU A, 2005, P 18 ANN C LEARN THE, P323; ARONSZAJN N, 1950, T AM MATH SOC, V68, P3337; Bellman R.E., 1961, ADAPTIVE CONTROL PRO; Bradley P. S., 1998, P 15 INT C MACH LEAR, P82; Chen S. S., 1999, SIAM J SCI COMPUT, V20, P33; Cristianini N., 2000, INTRO SUPPORT VECTOR; Grandvalet Y., 2003, ADV NEURAL INFORM PR, V15, P553; Gu C., 2002, SMOOTHING SPLINE ANO; Gunn SR, 2002, MACH LEARN, V48, P137, DOI 10.1023/A:1013903804720; Hastie T, 1986, STAT SCI, V1, P297, DOI DOI 10.1214/SS/1177013604; Hastie T, 2004, J MACH LEARN RES, V5, P1391; Lee YK, 2004, J AM STAT ASSOC, V99, P67, DOI 10.1198/016214504000000098; Scholkopf B., 2002, LEARNING KERNELS SUP; Vapnik V., 1998, STAT LEARNING THEORY; Wahba G., 1998, ADV KERNEL METHODS S, P69; Wahba G., 1990, SPLINE MODELS OBSERV; Wahba G, 2002, P NATL ACAD SCI USA, V99, P16524, DOI 10.1073/pnas.242574899; WAHBA G, 1994, ADV NEURAL INFORMATI, V6, P415; Weston J, 2001, ADV NEUR IN, V13, P668; Weston J., 2003, Journal of Machine Learning Research, V3, DOI 10.1162/153244303322753751	29	17	17	0	6	OXFORD UNIV PRESS	OXFORD	GREAT CLARENDON ST, OXFORD OX2 6DP, ENGLAND	0006-3444			BIOMETRIKA	Biometrika	SEP	2006	93	3					555	571		10.1093/biomet/93.3.555		17	Biology; Mathematical & Computational Biology; Statistics & Probability	Life Sciences & Biomedicine - Other Topics; Mathematical & Computational Biology; Mathematics	094XE	WOS:000241271700005		
J	Friedman, JH				Friedman, Jerome H.			Recent advances in predictive (machine) learning	JOURNAL OF CLASSIFICATION			English	Article; Proceedings Paper	13th International Meeting of the of the Psychometric-Society/68th Annual Meeting of the Psychometric-Society	JUL   09, 2003	Sardinia, ITALY	Psychometr Soc		machine learning; boosting; support vector machine; kernel methods; decision trees	REGRESSION	Prediction involves estimating the unknown value of an attribute of a system under study given the values of other measured attributes. In prediction (machine) learning the prediction rule is derived from data consisting of previously solved cases. Most methods for predictive learning were originated many years ago at the dawn of the computer age. Recently two new techniques have emerged that have revitalized the field. These are support vector machines and boosted decision trees. This paper provides an introduction to these two new methods tracing their respective ancestral roots to standard kernel methods and ordinary decision trees.	Stanford Univ, Dept Stat, Stanford, CA 94305 USA; Stanford Univ, Stanford Linear Accelerator Ctr, Stanford, CA 94305 USA	Friedman, JH (reprint author), Stanford Univ, Dept Stat, Stanford, CA 94305 USA.	jhf@stanford.edu					GEMAN S, 1992, NEURAL COMPUT, V4, P1, DOI 10.1162/neco.1992.4.1.1; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; HOERL AE, 1970, TECHNOMETRICS, V12, P55; Efron B, 2004, ANN STAT, V32, P407; Bellman R.E., 1961, ADAPTIVE CONTROL PRO; Breiman L, 1983, CLASSIFICATION REGRE; Breiman L, 2001, RANDOM FORESTS RANDO; Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.1023/A:1018054314350; Freund Y., 1996, Machine Learning. Proceedings of the Thirteenth International Conference (ICML '96); FRIEDMAN JH, 2002, ANN STAT, V29, P1189; Hastie T., 2001, ELEMENTS STAT LEARNI; Nadaraya E. A., 1964, THEOR PROBAB APPL, V10, P186; Quinlan R. J., 1992, C4 5 PROGRAMS MACHIN; Vapnik V.N., 1995, NATURE STAT LEARNING; Wahba G., 1990, SPLINE MODELS OBSERV; Watson G. S., 1964, SANKHYA A, V26, P359	16	15	16	0	3	SPRINGER	NEW YORK	233 SPRING STREET, NEW YORK, NY 10013 USA	0176-4268			J CLASSIF	J. Classif.	SEP	2006	23	2					175	197		10.1007/s00357-006-0012-4		23	Mathematics, Interdisciplinary Applications; Psychology, Mathematical	Mathematics; Psychology	102DX	WOS:000241792300002		
J	Jaccard, J; Guilamo-Ramos, V; Johansson, M; Bouris, A				Jaccard, James; Guilamo-Ramos, Vincent; Johansson, Margaret; Bouris, Alida			Multiple regression analyses in clinical child and adolescent psychology	JOURNAL OF CLINICAL CHILD AND ADOLESCENT PSYCHOLOGY			English	Article							SAMPLE-SIZE; SELECTION; METAANALYSIS	A major form of data analysis in clinical child and adolescent psychology is multiple regression. This article reviews issues in the application of such methods in light of the research designs typical of this field. Issues addressed include controlling covariates, evaluation of predictor relevance, comparing predictors, analysis of moderation, analysis of mediation, assumption violations, outliers, limited dependent variables, and directed regression and its relation to structural equation modeling. Analytic guidelines are provided within each domain.	Florida Int Univ, Dept Psychol, Miami, FL 33199 USA; Columbia Univ, Sch Social Work, New York, NY 10027 USA	Jaccard, J (reprint author), Florida Int Univ, Dept Psychol, 11200 SW 8th St, Miami, FL 33199 USA.	jjaccard@fiu.edu					ALTMAN DG, 1989, STAT MED, V8, P771, DOI 10.1002/sim.4780080702; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; MacKinnon DP, 2002, PSYCHOL METHODS, V7, P83, DOI 10.1037//1082-989X.7.1.83; Wilkinson L, 1999, AM PSYCHOL, V54, P594, DOI 10.1037//0003-066X.54.8.594; Blanton H, 2006, AM PSYCHOL, V61, P27, DOI 10.1037/0003-066X.61.1.27; Kraemer HC, 2002, ARCH GEN PSYCHIAT, V59, P877, DOI 10.1001/archpsyc.59.10.877; Maxwell SE, 2000, PSYCHOL METHODS, V5, P434, DOI 10.1037//1082-989X.5.4.434; Arbuckle JL., 2005, AMOS 6 0 USERS GUIDE; Azen R, 2003, PSYCHOL METHODS, V8, P129, DOI 10.1037/1082-989X.8.2.129; BARON RM, 1986, J PERS SOC PSYCHOL, V51, P1173, DOI 10.1037/0022-3514.51.6.1173; Behrens JT, 1997, PSYCHOL METHODS, V2, P131, DOI 10.1037/1082-989X.2.2.131; Bollen Kenneth A., 1989, STRUCTURAL EQUATIONS; Cohen JP, 2003, APPL MULTIPLE REGRES; Dennis M. L., 1997, SCI PREVENTION METHO, P367, DOI 10.1037/10222-011; DERKSEN S, 1992, BRIT J MATH STAT PSY, V45, P265; Hansen W B, 1994, NIDA Res Monogr, V142, P184; HANSEN WB, 1994, NIH PUBLICATION, V94, P184; HEISE DR, 1986, SOCIOL METHOD RES, V14, P447, DOI 10.1177/0049124186014004005; Howard GS, 2000, PSYCHOL METHODS, V5, P315, DOI 10.1037//1082-989X.5.3.315; Hunka S, 1997, J EDUC BEHAV STAT, V22, P361, DOI 10.3102/10769986022004361; Jaccard J, 1998, INTERACTION EFFECTS; Jaccard J, 1996, LISREL ANAL INTERACT; JACCARD J, IN PRESS FUNCTIONAL; Judd CM, 1989, DATA ANAL MODEL COMP; Kenny D. A., 1998, HDB SOCIAL PSYCHOL, V1, P233; Kline R. B., 2004, PRINCIPLES PRACTICE, V2nd; Kraemer HC, 1998, PSYCHOL METHODS, V3, P23, DOI 10.1037/1082-989X.3.1.23; Lindenberger U, 1998, PSYCHOL METHODS, V3, P218, DOI 10.1037//1082-989X.3.2.218; Long J. S., 2006, REGRESSION MODELS CA; Long J. Scott, 1997, REGRESSION MODELS CA; MANTEL N, 1970, TECHNOMETRICS, V12, P621, DOI 10.2307/1267207; MAXWELL SE, 1993, PSYCHOL BULL, V113, P181, DOI 10.1037/0033-2909.113.1.181; McClelland GH, 2000, AM PSYCHOL, V55, P963, DOI 10.1037/0003-066X.55.8.963; MEEHL PE, 1971, J ABNORM PSYCHOL, V77, P143, DOI 10.1037/h0030750; Muthen BO, 2004, MPLUS TECHNICAL APPE; POTHOFF RF, 1964, PSYCHOMETRIKA, V29, P241; Turrisi R, 2003, INTERACTION EFFECTS; WILCOX R, 1999, FUNDAMENTALS MODERN; Wilcox R., 1997, INTRO ROBUST ESTIMAT; Wilcox R. R., 2002, APPL CONT STAT TECHN	40	33	33	0	9	LAWRENCE ERLBAUM ASSOC INC	MAHWAH	10 INDUSTRIAL AVE, MAHWAH, NJ 07430-2262 USA	1537-4416			J CLIN CHILD ADOLESC	J. Clin. Child Adolesc. Psychol.	SEP	2006	35	3					456	479		10.1207/s15374424jccp3503_11		24	Psychology, Clinical; Psychology, Developmental	Psychology	067TG	WOS:000239324900011	16836483	
J	Trendafilov, NT				Trendafilov, Nickolay T.			The dynamical system approach to multivariate data analysis	JOURNAL OF COMPUTATIONAL AND GRAPHICAL STATISTICS			English	Article						constrained optimization; factor analysis; initial value problems for ODEs on matrix manifolds; MATLAB; matrix fitting problems; metric multidimensional scaling; principal components; procrustes problems; steepest descent/ascent matrix flows; three-way	PROJECTED GRADIENT APPROACH; PROCRUSTES PROBLEM; NUMERICAL-SOLUTION; INDIVIDUAL-DIFFERENCES; PRINCIPAL-COMPONENTS; ORTHOGONAL ROTATION; OBLIQUE ROTATION; MATRICES; CONSTRAINTS; REDUCTION	In this survey a number of problems arising in multivariate data analysis (MDA) are listed and reformulated as matrix fitting (e.g., least-squares, maximum likelihood, etc.) constrained optimization problems (OPs). The goal is to demonstrate that consideration and solution of these diverse MDA problems can be unified by means of the dynamical system approach. The approach transforms the MDA problems into dynamical systems on a manifold defined by the constraints of the original OP.	Open Univ, Dept Stat, Milton Keynes MK7 6AA, Bucks, England	Trendafilov, NT (reprint author), Open Univ, Dept Stat, Walton Hall, Milton Keynes MK7 6AA, Bucks, England.	N.Trendafilov@open.ac.uk					Edelman A, 1998, SIAM J MATRIX ANAL A, V20, P303, DOI 10.1137/S0895479895290954; Shampine LF, 1997, SIAM J SCI COMPUT, V18, P1, DOI 10.1137/S1064827594276424; CARROLL JD, 1970, PSYCHOMETRIKA, V35, P283, DOI 10.1007/BF02310791; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; TUCKER LR, 1966, PSYCHOMETRIKA, V31, P279, DOI 10.1007/BF02289464; Jolliffe IT, 2003, J COMPUT GRAPH STAT, V12, P531, DOI 10.1198/1061860032148; ASCHER UM, 1998, COMPUTER METHODS ODE; Boik RJ, 2003, BIOMETRIKA, V90, P679, DOI 10.1093/biomet/90.3.679; Boik RJ, 2002, BIOMETRIKA, V89, P159, DOI 10.1093/biomet/89.1.159; Brockett R. W., 1993, P S PURE MATH, V54, P69, DOI 10.1090/pspum/054.1/1216576; BROWNE MW, 1972, BRIT J MATH STAT PSY, V25, P115; Chino N., 1978, BEHAVIORMETRIKA, V5, P23, DOI 10.2333/bhmk.5.23; Chipman HA, 2005, J APPL STAT, V32, P969, DOI 10.1080/02664760500168648; CHU MT, 1991, LINEAR ALGEBRA APPL, V147, P75, DOI 10.1016/0024-3795(91)90230-T; Chu M. T., 1994, FIELDS I COMMUNICATI, V3, P87; Chu MT, 1998, BEHAVIORMETRIKA, V25, P13, DOI 10.2333/bhmk.25.13; Chu MT, 1998, STAT COMPUT, V8, P125, DOI 10.1023/A:1008934100736; Chu MT, 2001, J COMPUT GRAPH STAT, V10, P746, DOI 10.1198/106186001317243430; CHU MT, 1990, SIAM J NUMER ANAL, V27, P1050, DOI 10.1137/0727062; CONSTANTINE A. G., 1978, APPLIED STATISTICS, V27, P297, DOI 10.2307/2347165; Cox T.F., 1995, MULTIDIMENSIONAL SCA; De Leeuw J., 1982, HDB STATISTICS, V2, P285, DOI 10.1016/S0169-7161(82)02016-1; Del Buono N, 2002, SIAM J MATRIX ANAL A, V23, P974, DOI 10.1137/S089547980037768X; deLeeuw J, 1994, INFORM SYSTEMS DATA, P308; Diele F, 1998, ADV COMPUT MATH, V8, P317, DOI 10.1023/A:1018908700358; Elden L, 1999, NUMER MATH, V82, P599, DOI 10.1007/s002110050432; ENGO K, 1997, DIFFMAN OBJECT ORIEN; Flury B., 1988, COMMON PRINCIPAL COM; Gifi A, 1990, NONLINEAR MULTIVARIA; Golub G., 1991, MATRIX COMPUTATIONS; Gower J. C., 1977, RECENT DEV STAT, P109; GOWER JC, 1984, HDB APPL MATH, V4; Guillemin V., 1974, DIFFERENTIAL TOPOLOG; HARSHMAN R, 1978, 1 JOINT M PSYCH SOC; Helmke U., 1994, OPTIMIZATION DYNAMIC; Hirsch M.W., 1974, DIFFERENTIAL EQUATIO; Iserles A., 2000, ACTA NUMERIKA, V9, P1; Jennrich RI, 2002, PSYCHOMETRIKA, V67, P7, DOI 10.1007/BF02294706; Jennrich RI, 2001, PSYCHOMETRIKA, V66, P289, DOI 10.1007/BF02294840; Jolliffe I. T., 2002, PRINCIPAL COMPONENT; Jolliffe IT, 2000, J COMPUT GRAPH STAT, V9, P689, DOI 10.2307/1391088; Kelley C.T., 1999, ITERATIVE METHODS OP; KIERS HAL, 1990, PSYCHOMETRIKA, V55, P417, DOI 10.1007/BF02294758; KIERS HAL, 1994, J CLASSIF, V11, P79, DOI 10.1007/BF01201024; Kiers HAL, 1998, J CLASSIF, V15, P245, DOI 10.1007/s003579900033; KOSCHAT MA, 1991, PSYCHOMETRIKA, V56, P229, DOI 10.1007/BF02294460; Krzanowski W. J., 2003, PRINCIPLES MULTIVARI; Lippert R., 2000, TEMPLATES SOLUTION A, P290; LOJASIEWICZ S, 1983, SEMINARI GEOMETRIA B, V15, P115; Magnus J. R., 1988, MATRIX DIFFERENTIAL; Mardia KV, 1979, MULTIVARIATE ANAL; *MATLAB, 2002, US MATLAB VERS 6; MOOIJAART A, 1990, PSYCHOMETRIKA, V55, P657, DOI 10.1007/BF02294614; Mulaik S. A., 1972, FDN FACTOR ANAL; NEUDECKER H, 1981, PSYCHOMETRIKA, V46, P343, DOI 10.1007/BF02293741; Owren B, 2000, BIT, V40, P121, DOI 10.1023/A:1022322503301; RAMSAY JO, 1970, COMPUT J, V13, P413; Shampine LF, 1999, COMPUT MATH APPL, V38, P61, DOI 10.1016/S0898-1221(99)00183-2; Stiefel E., 1935, COMMENT MATH HELV, V8, P305, DOI DOI 10.1007/BF01199559; STIEFEL EL, 1963, INTRO NUMERICAL ANAL; Stuart AM, 1996, DYNAMICAL SYSTEMS NU; TAKANE Y, 1977, PSYCHOMETRIKA, V42, P7, DOI 10.1007/BF02293745; ten Berge JMF, 1993, LEAST SQUARES OPTIMI; TENBERGE JMF, 1984, PSYCHOMETRIKA, V49, P347; TENBERGE JMF, 1988, COMPUTATIONAL STATIS, V3, P207; Trendafilov N. T., 1999, BEHAVIORMETRIKA, V26, P167, DOI 10.2333/bhmk.26.167; Trendafilov NT, 2003, BRIT J MATH STAT PSY, V56, P27, DOI 10.1348/000711003321645322; TRENDAFILOV NT, 1997, 1997113 ESATSISTATR; Trendafilov NT, 2002, STAT COMPUT, V12, P135, DOI 10.1023/A:1014882518644; Trendafilov NT, 2006, COMPUT STAT DATA AN, V50, P242, DOI 10.1016/j.csda.2004.07.017; Trendafilov NT, 2004, LECT NOTES COMPUT SC, V3044, P952; Trendafilov NT, 2002, LINEAR ALGEBRA APPL, V349, P245, DOI 10.1016/S0024-3795(02)00253-7; VINES K, 2000, APPL STAT, V49, P441; Watson G.A., 1994, ADV COMPUT MATH, V2, P393, DOI 10.1007/BF02521606	74	7	7	1	2	AMER STATISTICAL ASSOC	ALEXANDRIA	1429 DUKE ST, ALEXANDRIA, VA 22314 USA	1061-8600			J COMPUT GRAPH STAT	J. Comput. Graph. Stat.	SEP	2006	15	3					628	650		10.1198/106186006X130828		23	Statistics & Probability	Mathematics	086XE	WOS:000240705500008		
J	Shaby, BA; Field, CB				Shaby, Benjamin A.; Field, Christopher B.			Regression tools for CO2 inversions: application of a shrinkage estimator to process attribution	TELLUS SERIES B-CHEMICAL AND PHYSICAL METEOROLOGY			English	Article							GENERALIZED CROSS-VALIDATION; ATMOSPHERIC CARBON-DIOXIDE; GOOD RIDGE PARAMETER; TERRESTRIAL BIOSPHERE; MODEL; TRANSPORT; DELTA-C-13; SATELLITE; EMISSIONS; BUDGET	In this study we perform an atmospheric inversion based on a shrinkage estimator. This method is used to estimate surface fluxes Of CO2, first partitioned according to constituent geographic regions, and then according to constituent processes that are responsible for the total flux. Our approach differs from previous approaches in two important ways. The first is that the technique of linear Bayesian inversion is recast as a regression problem. Seen as such, standard regression tools are employed to analyse and reduce errors in the resultant estimates. A shrinkage estimator, which combines standard ridge regression with the linear 'Bayesian inversion' model, is introduced. This method introduces additional bias into the model with the aim of reducing variance such that errors are decreased overall. Compared with standard linear Bayesian inversion, the ridge technique seems to reduce both flux estimation errors and prediction errors. The second divergence from previous studies is that instead of dividing the world into geographically distinct regions and estimating the CO2 flux in each region, the flux space is divided conceptually into processes that contribute to the total global flux. Formulating the problem in this manner adds to the interpretability of the resultant estimates and attempts to shed light on the problem of attributing sources and sinks to their underlying mechanisms.	Carnegie Inst Washington, Dept Global Ecol, Stanford, CA 94305 USA	Shaby, BA (reprint author), Carnegie Inst Washington, Dept Global Ecol, 260 Panama St, Stanford, CA 94305 USA.	bshaby@globalecology.stanford.edu					Andres RJ, 1996, GLOBAL BIOGEOCHEM CY, V10, P419, DOI 10.1029/96GB01523; LINDLEY DV, 1972, J ROY STAT SOC B, V34, P1; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; ENTING IG, 1995, TELLUS B, V47, P35, DOI 10.1034/j.1600-0889.47.issue1.5.x; DeFries RS, 2002, P NATL ACAD SCI USA, V99, P14256, DOI 10.1073/pnas.182560099; POTTER CS, 1993, GLOBAL BIOGEOCHEM CY, V7, P811, DOI 10.1029/93GB02725; Goodale CL, 2002, ECOL APPL, V12, P891, DOI 10.2307/3060997; Ramankutty N, 1999, GLOBAL BIOGEOCHEM CY, V13, P997, DOI 10.1029/1999GB900046; HOERL AE, 1970, TECHNOMETRICS, V12, P55; Takahashi T, 2002, DEEP-SEA RES PT II, V49, P1601, DOI 10.1016/S0967-0645(02)00003-6; FRANK IE, 1993, TECHNOMETRICS, V35, P109, DOI 10.2307/1269656; Gurney KR, 2002, NATURE, V415, P626, DOI 10.1038/415626a; GOLUB GH, 1979, TECHNOMETRICS, V21, P215, DOI 10.1080/00401706.1979.10489751; Mouillot F, 2005, GLOBAL CHANGE BIOL, V11, P398, DOI 10.1111/j.1365-2486.2005.00920.x; Pacala SW, 2001, SCIENCE, V292, P2316, DOI 10.1126/science.1057320; TANS PP, 1990, SCIENCE, V247, P1431, DOI 10.1126/science.247.4949.1431; Schimel DS, 2001, NATURE, V414, P169, DOI 10.1038/35102500; BJORKSTROM A, 2001, RIDGE REGRESSION INV; *CD ROM NOAA CMDL, 2004, GLOBALVIEW CO2 COOPE; COOLEY W W, 1971, MULTIVAR DATA ANAL, P364; Efron B, 1994, INTRO BOOTSTRAP, P436; ENTING IG, 2002, INVERSE PROBLEMS ATM, P394; Fan SM, 1999, J GEOPHYS RES-ATMOS, V104, P21503, DOI 10.1029/1999JD900215; FRIEDMAN J, 2001, ELEMENTS STAT LEARNI, P533; FUNG I, 1983, J GEOPHYS RES-OC ATM, V88, P1281, DOI 10.1029/JC088iC02p01281; Gelman A, 2004, BAYESIAN DATA ANAL, P668; GIBBONS DG, 1981, J AM STAT ASSOC, V76, P131, DOI 10.2307/2287058; Golub G. H., 1996, MATRIX COMPUTATIONS, P694; Gurney KR, 2003, TELLUS B, V55, P555, DOI 10.1034/j.1600-0889.2003.00049.x; HANSON DL, 1983, ANN PROBAB, V11, P609, DOI 10.1214/aop/1176993505; Harman HH, 1976, MODERN FACTOR ANAL, P487; Kaminski T, 2002, GLOBAL BIOGEOCHEM CY, V16, DOI 10.1029/2001GB001463; Kaminski T, 1999, J GEOPHYS RES-ATMOS, V104, P18555, DOI 10.1029/1999JD900146; KNORR W, 1995, TELLUS B, V47, P461; Krakauer NY, 2004, GEOPHYS RES LETT, V31, DOI 10.1029/2004GL020323; Law RM, 2002, GLOBAL BIOGEOCHEM CY, V16, DOI 10.1029/2001GB001593; LEE T, 1987, APPL STAT C, V36, P112, DOI 10.2307/2347851; MANSBRIDGE J V, 1986, Tellus Series B Chemical and Physical Meteorology, V38, P11; MASARIE KA, 1995, J GEOPHYS RES-ATMOS, V100, P11593, DOI 10.1029/95JD00859; NORDBERG L, 1982, COMMUN STAT B-SIMUL, V11, P285, DOI 10.1080/03610918208812264; Prentice IC, 2000, ECOL APPL, V10, P1553, DOI 10.1890/1051-0761(2000)010[1553:TCBOTT]2.0.CO;2; Randerson JT, 2002, GLOBAL BIOGEOCHEM CY, V16, DOI 10.1029/2001GB001435; Rayner PJ, 1999, TELLUS B, V51, P213, DOI 10.1034/j.1600-0889.1999.t01-1-00008.x; RUDY Y, 1988, CRIT REV BIOMED ENG, V16, P215; Tarantola A, 1987, INVERSE PROBLEM THEO, P613; *UNFCCC, 1992, KYOT PROT; Wold H., 1975, PERSPECTIVES PROBABI, P117; YAN B, 1989, GEOPHYS RES LETT, V16, P449, DOI 10.1029/GL016i005p00449	48	1	1	0	0	BLACKWELL PUBLISHING	OXFORD	9600 GARSINGTON RD, OXFORD OX4 2DQ, OXON, ENGLAND	0280-6509			TELLUS B	Tellus Ser. B-Chem. Phys. Meteorol.	SEP	2006	58	4					279	292		10.1111/j.1600-0889.2006.00189.x		14	Meteorology & Atmospheric Sciences	Meteorology & Atmospheric Sciences	081RN	WOS:000240334800002		
J	Pearson, RK				Pearson, Ronald K.			Nonlinear empirical modeling techniques	COMPUTERS & CHEMICAL ENGINEERING			English	Article; Proceedings Paper	7th International Conference on Chemical Process Control (CPC 7)	JAN 08-13, 2006	Lake Louise, CANADA	CACHE Corp, AIChE, CACHE Div, AIChE, CAST Div, Natl Sci Fdn, Aspen Technol, Eastman Chem, ExxonMobil Chem, Praxair & Shell Global Solut		nonlinear discrete-time dynamic models; empirical model identification; multivariable models; nonlinear input-output models; nonlinear state-space models; subspace-based methods; data cleaning filters	SYSTEM-IDENTIFICATION; ESTIMATION ALGORITHM; HAMMERSTEIN SYSTEMS; PREDICTION-ERROR; NEURAL NETWORKS; NARMAX MODELS; KNOWLEDGE; FEEDBACK; FILTERS	One of the key enabling technologies for computer-based process control is dynamic model development. This problem can be approached from several different perspectives and this survey focuses on one of them: the empirical development of nonlinear, discrete-time dynamic models. Critical issues considered here include the formulation of multivariable problems, the range of popular model representations available and their practical implications for model development, the selection of useful identification inputs, the utility of constraints and regularization in parameter estimation, the treatment of data anomalies and the comparative assessment of modeling results. (c) 2006 Elsevier Ltd. All rights reserved.	ProSanos Co, Harrisburg, PA 17101 USA	Pearson, RK (reprint author), ProSanos Co, 225 Market St,Suite 502, Harrisburg, PA 17101 USA.	ronald.pearson@prosanos.com					Godfrey KR, 1999, IEE P-CONTR THEOR AP, V146, P535, DOI 10.1049/ip-cta:19990529; NARENDRA KS, 1966, IEEE T AUTOMAT CONTR, VAC11, P546, DOI 10.1109/TAC.1966.1098387; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; MARTIN RD, 1982, P IEEE, V70, P1097, DOI 10.1109/PROC.1982.12434; Liu HC, 2004, COMPUT CHEM ENG, V28, P1635, DOI 10.1016/j.compchemeng.2004.01.009; Bemporad A, 1999, AUTOMATICA, V35, P407, DOI 10.1016/S0005-1098(98)00178-2; Pearson RK, 2002, IEEE T CONTR SYST T, V10, P55, DOI 10.1109/87.974338; CHEONG HW, 1986, AICHE J, V32, P1334, DOI 10.1002/aic.690320811; CHEN S, 1992, INT J CONTROL, V55, P1051, DOI 10.1080/00207179208934272; Westwick D, 1996, SIGNAL PROCESS, V52, P235, DOI 10.1016/0165-1684(96)00056-4; BAHETI RS, 1980, IEEE T AUTOMAT CONTR, V25, P1141, DOI 10.1109/TAC.1980.1102516; Bai EW, 1998, AUTOMATICA, V34, P333, DOI 10.1016/S0005-1098(97)00198-2; BILLINGS SA, 1989, INT J SYST SCI, V20, P467, DOI 10.1080/00207728908910143; BILLINGS SA, 1986, INT J CONTROL, V44, P803, DOI 10.1080/00207178608933633; BORWEIN P, 1979, POLYNOMIALS POLYNOMI; BOYD S, 1985, IEEE T CIRCUITS SYST, V32, P1150, DOI 10.1109/TCS.1985.1085649; Chen LN, 1997, PHYSICA D, V104, P286, DOI 10.1016/S0167-2789(96)00302-8; Doyle FJ, 2002, IDENTIFICATION CONTR; Draper N, 1981, APPL REGRESSION ANAL; Farina L., 2000, POSITIVE LINEAR SYST; Ferrari-Trecate G, 2003, AUTOMATICA, V39, P205, DOI 10.1016/S0005-1098(02)00224-8; FINDEISEN R., 2002, P 21 BEN M SYST CONT; Fleming W., 1977, FUNCTIONS SEVERAL VA; Foss BA, 1998, J PROCESS CONTR, V8, P325, DOI 10.1016/S0959-1524(98)00018-3; Fruzzetti KP, 1997, J PROCESS CONTR, V7, P31, DOI 10.1016/S0959-1524(97)80001-B; GOMEZ J, 2004, IEE P-CONTR THEOR AP, V151, P437; GREBLICKI W, 1989, IEEE T INFORM THEORY, V35, P409, DOI 10.1109/18.32135; Groff R.E., 2003, THESIS U MICHIGAN; HEINONEN P, 1987, IEEE T ACOUST SPEECH, V35, P832, DOI 10.1109/TASSP.1987.1165198; HUBER P, 1981, ROBUST STATE; Isermann R., 1997, P IFAC SYST ID S IFA, P947; JOHANSEN TA, 1993, INT J CONTROL, V58, P1125, DOI 10.1080/00207179308923046; Johansen TA, 1996, AUTOMATICA, V32, P337, DOI 10.1016/0005-1098(95)00146-8; Johansen TA, 1998, MODEL IDENT CONTROL, V19, P109; Kalafatis AD, 1997, INT J CONTROL, V66, P923, DOI 10.1080/002071797224469; Kaufman L, 1990, FINDING GROUPS DATA; Klambauer G., 1975, MATH ANAL; LARSEN J, 1994, P IEEE WORKSH NEUR N; LEE JH, 1998, INT S NONL MOD PRED, P91; LEONTARITIS IJ, 1987, INT J SYST SCI, V18, P189, DOI 10.1080/00207728708963958; LIN S, 1992, IEEE T CIRCUITS SYST, V1, P39; Ljung L, 1999, SYSTEM IDENTIFICATIO; MANDLER J, 1998, 5 IFAC S DYN CONTR P, P405; MEADOWS E, 2003, NONLINEAR PROCESS CO, P233; Mhaskar HN, 1997, NEURAL COMPUT, V9, P143, DOI 10.1162/neco.1997.9.1.143; Narendra K S, 1990, IEEE Trans Neural Netw, V1, P4, DOI 10.1109/72.80202; Nikolaou M, 2003, COMPUT CHEM ENG, V27, P1043, DOI 10.1016/S0098-1354(03)00036-X; Norquay SJ, 1999, IEEE T CONTR SYST T, V7, P437, DOI 10.1109/87.772159; NOWAK RD, 1994, IEEE T SIGNAL PROCES, V42, P2124, DOI 10.1109/78.301847; OGUNNAIKE B, 1997, AICHE S SERIES, V316, P46; Parker RS, 2001, J PROCESS CONTR, V11, P237, DOI 10.1016/S0959-1524(00)00050-0; Pearson R., 1999, DISCRETE TIME DYNAMI; PEARSON R, 1999, P EUR CONTR C; Pearson R. K., 2005, MINING IMPERFECT DAT; Pearson RK, 2004, SIAM PROC S, P188; Pearson RK, 2000, J PROCESS CONTR, V10, P301, DOI 10.1016/S0959-1524(99)00055-4; Pearson RK, 2004, J PROCESS CONTR, V14, P533, DOI 10.1016/j.jprocont.2003.09.007; Pearson RK, 2003, J PROCESS CONTR, V13, P1, DOI 10.1016/S0959-1524(02)00022-7; Pottmann M., 1992, Journal of Process Control, V2, DOI 10.1016/0959-1524(92)80008-L; Pottmann M, 1998, AICHE J, V44, P131, DOI 10.1002/aic.690440114; QIN S, 2006, COMPUTERS CHEM ENG; QIN SJ, 1998, INT S NONL MOD PRED, P128; Rousseeuw P. J., 1987, ROBUST REGRESSION OU; Schweppe F. C., 1973, UNCERTAIN DYNAMIC SY; Sontag E.D., 1979, POLYNOMIAL RESPONSE; SUBBARAO T, 1984, INTRO BISPECTRAL ANA; Sugiki A, 2001, INT J ROBUST NONLIN, V11, P241, DOI 10.1002/rnc.573; TULLEKEN HJAF, 1993, AUTOMATICA, V29, P285, DOI 10.1016/0005-1098(93)90124-C; UNBEHAUEN H, 1998, ANNU REV CONTROL, V22, P325; Vapnik V., 1998, STAT LEARNING THEORY; Verdult V., 2002, THESIS U TWENTE; Verhaegen M, 1996, INT J CONTROL, V63, P331, DOI 10.1080/00207179608921846; Vinod H. D., 1981, RECENT ADV REGRESSIO; Wang XF, 2000, IEEE T CIRCUITS-I, V47, P410; Yin L, 1996, IEEE T CIRCUITS-II, V43, P157; Zhang SX, 1997, AICHE J, V43, P1265, DOI 10.1002/aic.690430515; ZHU Q, 1994, P SYSID 94, P259	77	9	9	1	4	PERGAMON-ELSEVIER SCIENCE LTD	OXFORD	THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND	0098-1354			COMPUT CHEM ENG	Comput. Chem. Eng.	SEP 12	2006	30	10-12					1514	1528		10.1016/j.compchemeng.2006.05.028		15	Computer Science, Interdisciplinary Applications; Engineering, Chemical	Computer Science; Engineering	088EJ	WOS:000240794000010		
J	Peng, XL; Yin, H; Li, R; Fang, KT				Peng, Xiao-Ling; Yin, Hong; Li, Runze; Fang, Kai-Tai			The application of Kriging and empirical Kriging based on the variables selected by SCAD	ANALYTICA CHIMICA ACTA			English	Article						Kriging models; empirical Kriging; penalized least squares; QSPR	TOPOLOGICAL INDEXES; MOLECULAR GRAPHS; RETENTION INDEXES; BOILING POINTS; REGRESSION; DESCRIPTORS; ATOMS	The commonly used approach for building a structure-activity/property relationship consists of three steps. First, one determines the descriptors for the molecular structure, then builds a metamodel by using some proper mathematical methods, and finally evaluates the meta-model. Some existing methods only can select important variables from the candidates, while most metamodels just explore linear relationships between inputs and outputs. Some techniques are useful to build more complicated relationship, but they may not be able to select important variables from a large number of variables. In this paper, we propose to screen important variables by the smoothly clipped absolute deviation (SCAD) variable selection procedure, and then apply Kriging model and empirical Kriging model for quantitative structure-activity/property relationship (QSAR/QSPR) research based on the selected important variables. We demonstrate the proposed procedure retains the virtues of both variable selection and Kriging model. (c) 2006 Elsevier B.V. All rights reserved.	Hong Kong Baptist Univ, Dept Math, Hong Kong, Hong Kong, Peoples R China; Shanghai Jiao Tong Univ, Dept Math, Shanghai 200030, Peoples R China; Renmin Univ China, Sch Informat, Beijing, Peoples R China; Penn State Univ, Dept Stat, University Pk, PA 16802 USA; Penn State Univ, Methodol Ctr, University Pk, PA 16802 USA	Fang, KT (reprint author), Hong Kong Baptist Univ, Dept Math, Hong Kong, Hong Kong, Peoples R China.	ktfang@hkbu.edu.hk	ruc, comp_xinxi/E-4212-2012; Li, Runze/C-5444-2013	Li, Runze/0000-0002-0154-2202			RANDIC M, 1975, J AM CHEM SOC, V97, P6609, DOI 10.1021/ja00856a001; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Antoniadis A, 2001, J AM STAT ASSOC, V96, P939, DOI 10.1198/016214501753208942; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; MASSY WF, 1965, J AM STAT ASSOC, V60, P234, DOI 10.2307/2283149; FRANK IE, 1993, TECHNOMETRICS, V35, P109, DOI 10.2307/1269656; HOSOYA H, 1971, B CHEM SOC JPN, V44, P2332, DOI 10.1246/bcsj.44.2332; WIENER H, 1947, J AM CHEM SOC, V69, P17, DOI 10.1021/ja01193a005; BONCHEV D, 1977, J CHEM PHYS, V67, P4517, DOI 10.1063/1.434593; Breiman L, 1996, ANN STAT, V24, P2350; Du YP, 2002, J CHEM INF COMP SCI, V42, P1283, DOI 10.1021/ci020285u; Fang KT, 2004, J CHEM INF COMP SCI, V44, P2106, DOI 10.1021/ci049798m; Fang KT, 2006, CH CRC COMP SCI DATA, P3; Filip PA, 1987, J MATH CHEM, V1, P61, DOI 10.1007/BF01205338; FRIEDMAN JH, 1991, ANN STAT, V19, P1, DOI 10.1214/aos/1176347963; HOERL AE, 1970, TECHNOMETRICS, V12, P55, DOI DOI 10.1080/00401706.1970.10488634; Kier L. B., 1976, MOL CONNECTIVITY CHE; KIER LB, 1985, QUANT STRUCT-ACT REL, V4, P109, DOI 10.1002/qsar.19850040303; KIER LB, 1986, QUANT STRUCT-ACT REL, V5, P1, DOI 10.1002/qsar.19860050102; KIER LB, 1990, PHARMACEUT RES, V7, P801, DOI 10.1023/A:1015952613760; KLEIN DJ, 1995, J CHEM INF COMP SCI, V35, P50, DOI 10.1021/ci00023a007; MOREAU G, 1980, NOUV J CHIM, V4, P757; MOREAU G, 1980, NOUV J CHIM, V4, P359; Peng XL, 2005, J MOL STRUC-THEOCHEM, V714, P235, DOI 10.1016/j.theochem.2004.11.016; PLAVSIC D, 1993, J MATH CHEM, V12, P235, DOI 10.1007/BF01164638; RANDIC M, 1977, J CHEM INF COMP SCI, V17, P171, DOI 10.1021/ci60011a013; Randic M, 2001, J CHEM INF COMP SCI, V41, P607, DOI 10.1021/ci0001031; RANDIC M, 1979, COMPUT CHEM, V3, P65; ROHRBAUGH RH, 1985, ANAL CHEM, V57, P2770, DOI 10.1021/ac00291a008; Rucker G, 1999, J CHEM INF COMP SCI, V39, P788, DOI 10.1021/ci9900175; RUCKER G, 1993, J CHEM INF COMP SCI, V33, P683; SCHULTZ HP, 1989, J CHEM INF COMP SCI, V29, P227, DOI 10.1021/ci00063a012; Wold H, 1975, SOFT MODELLING LATEN, P117	33	3	4	3	12	ELSEVIER SCIENCE BV	AMSTERDAM	PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS	0003-2670	1873-4324		ANAL CHIM ACTA	Anal. Chim. Acta	SEP 25	2006	578	2					178	185		10.1016/j.aca.2006.06.073		8	Chemistry, Analytical	Chemistry	091TC	WOS:000241050200010	17723710	
J	van Houwelingen, HC; Bruinsma, T; Hart, AAM; van't Veet, LJ; Wessels, LFA				van Houwelingen, Hans C.; Bruinsma, Tako; Hart, Augustinus A. M.; van't Veet, Laura J.; Wessels, Lodewyk F. A.			Cross-validated Cox regression on microarray gene expression data	STATISTICS IN MEDICINE			English	Article						Cox regression; high-dimensional data; cross-validation; gene expression data	BREAST-CANCER; CLASSIFICATION; PREDICTION; DISCOVERY; SELECTION; SURVIVAL; LASSO; MODEL	This paper describes how penalized Cox regression, in combination with cross-validated partial likelihood can be employed to obtain reliable survival prediction models for high dimensional microarray data. The suggested procedure is demonstrated on a breast cancer survival data set consisting of 295 tumours as collected in the National Cancer Institute in Amsterdam and previously reported in more general papers. The main aim of this paper it to show how generally accepted biostatistical procedures can be employed to analyse high-dimensional data. Copyright (c) 2005 John Wiley & Sons, Ltd.	Leiden Univ, Med Ctr, Dept Med Stat & Bioinformat, NL-2300 RC Leiden, Netherlands; Antoni Van Leeuwenhoek Hosp, Netherlands Canc Inst, Dept Pathol, NL-1066 CX Amsterdam, Netherlands; Antoni Van Leeuwenhoek Hosp, Netherlands Canc Inst, Dept Radiotherapy, NL-1066 CX Amsterdam, Netherlands; Delft Univ Technol, Fac Elect Engn Math & Comp Sci, Dept Mediamat, NL-2628 CD Delft, Netherlands	van Houwelingen, HC (reprint author), Leiden Univ, Med Ctr, Dept Med Stat & Bioinformat, POB 9604, NL-2300 RC Leiden, Netherlands.	jcvanhouwelingen@lumc.nl; tako@dzgn.nl; ghart@nki.nl; l.vt.veer@nki.nl; l.fa.wessels@ewi.tudelft.nl	van houwelingen, hans/C-1872-2008				LIN DY, 1991, J AM STAT ASSOC, V86, P725, DOI 10.2307/2290404; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Tusher VG, 2001, P NATL ACAD SCI USA, V98, P5116, DOI 10.1073/pnas.091062498; Goeman JJ, 2004, BIOINFORMATICS, V20, P93, DOI 10.1093/bioinformatics/btg382; Golub TR, 1999, SCIENCE, V286, P531, DOI 10.1126/science.286.5439.531; HOERL AE, 1970, TECHNOMETRICS, V12, P55; Wall MM, 2003, STAT MED, V22, P3671, DOI 10.1002/sim.1588; BENJAMINI Y, 1995, J ROY STAT SOC B MET, V57, P289; Efron B, 2004, ANN STAT, V32, P407; LECESSIE S, 1992, APPL STAT-J ROY ST C, V41, P191; Eilers PHC, 2001, P SOC PHOTO-OPT INS, V2, P187; GOEMAN JJ, 2005, BIOINFORMATICS; Harrell FE, 2001, REGRESSION MODELING; Hastie T., 2001, ELEMENTS STAT LEARNI; Pawitan Y, 2004, STAT MED, V23, P1767, DOI 10.1002/sim.1769; Stephens DA, 1998, BIOMETRICS, V54, P1334, DOI 10.2307/2533661; Sundberg R, 1999, SCAND J STAT, V26, P161, DOI 10.1111/1467-9469.00144; Tibshirani R, 1997, STAT MED, V16, P385, DOI 10.1002/(SICI)1097-0258(19970228)16:4<385::AID-SIM380>3.0.CO;2-3; van de Vijver MJ, 2002, NEW ENGL J MED, V347, P1999, DOI 10.1056/NEJMoa021967; van't Veer LJ, 2002, NATURE, V415, P530, DOI 10.1038/415530a; VANHOUWELINGEN JC, 1990, STAT MED, V9, P1303, DOI 10.1002/sim.4780091109; VERWEIJ PJM, 1993, STAT MED, V12, P2305, DOI 10.1002/sim.4780122407; VERWEIJ PJM, 1994, STAT MED, V13, P2427, DOI 10.1002/sim.4780132307	23	69	69	1	7	JOHN WILEY & SONS LTD	CHICHESTER	THE ATRIUM, SOUTHERN GATE, CHICHESTER PO19 8SQ, W SUSSEX, ENGLAND	0277-6715			STAT MED	Stat. Med.	SEP 30	2006	25	18					3201	3216		10.1002/sim.2353		16	Mathematical & Computational Biology; Public, Environmental & Occupational Health; Medical Informatics; Medicine, Research & Experimental; Statistics & Probability	Mathematical & Computational Biology; Public, Environmental & Occupational Health; Medical Informatics; Research & Experimental Medicine; Mathematics	084XQ	WOS:000240568100011	16143967	
J	Lin, Y; Zhang, HH				Lin, Yi; Zhang, Hao Helen			Component selection and smoothing in multivariate nonparametric regression	ANNALS OF STATISTICS			English	Article						smoothing spline ANOVA; method of regularization; nonparametric regression; nonparametric classification; model selection; machine learning	VARIABLE SELECTION; MODEL SELECTION; SPLINE FUNCTIONS; BASIS PURSUIT; LIKELIHOOD	We propose a new method for model selection and model fitting in multivariate nonparametric regression models, in the framework of smoothing spline ANOVA. The "COSSO" is a method of regularization with the penalty functional being the sum of component norms, instead of the squared norm employed in the traditional smoothing spline method. The COSSO provides a unified framework for several recent proposals for model selection in linear models and smoothing spline ANOVA models. Theoretical properties, such as the existence and the rate of convergence of the COSSO estimator, are studied. In the special case of a tensor product design with periodic functions, a detailed analysis reveals that the COSSO does model selection by applying a novel soft thresholding type operation to the function components. We give an equivalent formulation of the COSSO estimator which leads naturally to an iterative algorithm. We compare the COSSO with MARS, a popular method that builds functional ANOVA models, in simulations and real examples. The COSSO method can be extended to classification problems and we compare its performance with those of a number of machine learning algorithms on real datasets. The COSSO gives very competitive performance in these studies.	Univ Wisconsin, Dept Stat, Madison, WI 53706 USA; N Carolina State Univ, Dept Stat, Raleigh, NC 27695 USA	Lin, Y (reprint author), Univ Wisconsin, Dept Stat, Madison, WI 53706 USA.	yilin@stat.wisc.edu; hzhang2@stat.ncsu.edu					BREIMAN L, 1995, TECHNOMETRICS, V37, P373, DOI 10.2307/1269730; Shen XT, 2002, J AM STAT ASSOC, V97, P210, DOI 10.1198/016214502753479356; Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; FRANK IE, 1993, TECHNOMETRICS, V35, P109, DOI 10.2307/1269656; Efron B, 2004, ANN STAT, V32, P407; CRAVEN P, 1979, NUMER MATH, V31, P377; CHEN ZH, 1993, J ROY STAT SOC B MET, V55, P473; FRIEDMAN JH, 1991, ANN STAT, V19, P1, DOI 10.1214/aos/1176347963; GU C, 1992, J AM STAT ASSOC, V87, P1051, DOI 10.2307/2290642; Gu C, 2002, SMOOTHING SPLINE ANO; Gunn SR, 2002, MACH LEARN, V48, P137, DOI 10.1023/A:1013903804720; Shen XT, 2004, J AM STAT ASSOC, V99, P751, DOI 10.1198/016214504000001097; TAPIA R, 1978, NONPARAMETRIC PROBAB, DOI UNSP MR0502724; UTRERAS F, 1983, NUMER MATH, V42, P107, DOI 10.1007/BF01400921; van de Geer S, 2000, EMPIRICAL PROCESSES; van Gestel T, 2004, MACH LEARN, V54, P5, DOI 10.1023/B:MACH.0000008082.80494.e0; Wahba G, 1995, ANN STAT, V23, P1865; Wahba G., 1990, SPLINE MODELS OBSERV; Yau P, 2003, J COMPUT GRAPH STAT, V12, P23, DOI 10.1198/1061860031301; Ye JM, 1998, J AM STAT ASSOC, V93, P120, DOI 10.2307/2669609; Zhang HH, 2004, J AM STAT ASSOC, V99, P659, DOI 10.1198/016214504000000593	23	106	107	2	8	INST MATHEMATICAL STATISTICS	BEACHWOOD	PO BOX 22718, BEACHWOOD, OH 44122 USA	0090-5364			ANN STAT	Ann. Stat.	OCT	2006	34	5					2272	2297		10.1214/009053606000000722		26	Statistics & Probability	Mathematics	136XZ	WOS:000244258700012		
J	Greenshtein, E				Greenshtein, Eitan			Best subset selection, persistence in high-dimensional statistical learning and optimization under l(1) constraint	ANNALS OF STATISTICS			English	Article						variable selection; persistence	NONCONCAVE PENALIZED LIKELIHOOD; WALD MEMORIAL LECTURES; ASYMPTOTIC-BEHAVIOR; VARIABLE SELECTION; GOLDEN CHAIN; M-ESTIMATORS; REGRESSION; CONSISTENCY; PARAMETERS; LASSO	Let (Y, X-1,..., X-m) be a random vector. It is desired to predict Y based on (X-1,..., X-m). Examples of prediction methods are regression, classification using logistic regression or separating hyperplanes, and so on. We consider the problem of best subset selection, and study it in the context m = n(alpha), alpha > 1, where n is the number of observations. We investigate procedures that are based on empirical risk minimization. It is shown, that in common cases, we should aim to find the best subset among those of size which is of order o(n/log(n)). It is also shown, that in some "asymptotic sense," when assuming a certain sparsity condition, there is no loss in letting m be much larger than n, for example, m = n(alpha), alpha > 1. This is in comparison to starting with the "best" subset of size smaller than n and regardless of the value of alpha. We then study conditions under which empirical risk minimization subject to l(1) constraint yields nearly the best subset. These results extend some recent results obtained by Greenshtein and Ritov. Finally we present a high-dimensional simulation study of a "boosting type" classification procedure.	Purdue Univ, Dept Stat, W Lafayette, IN 47907 USA	Greenshtein, E (reprint author), Purdue Univ, Dept Stat, W Lafayette, IN 47907 USA.	egreensh@stat.purdue.edu					HUBER PJ, 1973, ANN STAT, V1, P799, DOI 10.1214/aos/1176342503; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; Chen SSB, 2001, SIAM REV, V43, P129, DOI 10.1137/S003614450037906X; Bickel PJ, 2004, BERNOULLI, V10, P989, DOI 10.3150/bj/1106314847; Breiman L, 2001, STAT SCI, V16, P199, DOI 10.1214/ss/1009213726; Friedman J, 2004, ANN STAT, V32, P102; Efron B, 2004, ANN STAT, V32, P407; Meinshausen N, 2006, ANN STAT, V34, P1436, DOI 10.1214/009053606000000281; Breiman L, 2004, ANN STAT, V32, P1; Buhlmann P, 2004, ANN STAT, V32, P96; DONOHO D, 2004, 20049 STANF U DEPT S; DONOHO D, 2004, 200410 STANF U DEPT; Fan JQ, 2004, ANN STAT, V32, P928, DOI 10.1214/009053604000000256; GREENHTEIN E, 2005, SANKHYA, V67, P46, DOI UNSP MR2203888; Greenshtein E, 2004, BERNOULLI, V10, P971, DOI 10.3150/bj/1106314846; Hastie T., 2001, ELEMENTS STAT LEARNI; Juditsky A, 2000, ANN STAT, V28, P681; Lee WS, 1996, IEEE T INFORM THEORY, V42, P2118; Lugosi G, 2004, ANN STAT, V32, P30; NEMIROVSKI A, 1983, PROBLEM COMPLEXITY M, DOI UNSP MR0702836; Nguyen DV, 2002, BIOMETRICS, V58, P701, DOI 10.1111/j.0006-341X.2002.00701.x; PISIER G, 1981, SEMINAIRE ANAL FONCT, V112, DOI UNSP MR0659306; PORTNOY S, 1984, ANN STAT, V12, P1298, DOI 10.1214/aos/1176346793; VAPNIK NV, 1998, STAT LEARNING THEORY, DOI UNSP MR1641250; YOHAI VJ, 1979, ANN STAT, V7, P258, DOI 10.1214/aos/1176344610	26	33	34	0	2	INST MATHEMATICAL STATISTICS	BEACHWOOD	PO BOX 22718, BEACHWOOD, OH 44122 USA	0090-5364			ANN STAT	Ann. Stat.	OCT	2006	34	5					2367	2386		10.1214/009053606000000768		20	Statistics & Probability	Mathematics	136XZ	WOS:000244258700015		
J	Pan, W; Shen, XT; Jiang, AX; Hebbel, RP				Pan, Wei; Shen, Xiaotong; Jiang, Aixiang; Hebbel, Robert P.			Semi-supervised learning via penalized mixture model with application to microarray sample classification	BIOINFORMATICS			English	Article							GENE-EXPRESSION; ENDOTHELIAL-CELLS; SHRUNKEN CENTROIDS; PREDICTION; REGRESSION; OUTGROWTH; SELECTION; BLOOD	Motivation: It is biologically interesting to address whether human blood outgrowth endothelial cells (BOECs) belong to or are closer to large vessel endothelial cells (LVECs) or microvascular endothelial cells (MVECs) based on global expression profiling. An earlier analysis using a hierarchical clustering and a small set of genes suggested that BOECs seemed to be closer to MVECs. By taking advantage of the two known classes, LVEC and MVEC, while allowing BOEC samples to belong to either of the two classes or to form their own new class, we take a semi-supervised learning approach; for high-dimensional data as encountered here, we propose a penalized mixture model with a weighted L-1 penalty to realize automatic feature selection while fitting the model. Results: We applied our penalized mixture model to a combined dataset containing 27 BOEC, 28 LVEC and 25 MVEC samples. Analysis results indicated that the BOEC samples appeared to form their own new class. A simulation study confirmed that, compared with the standard mixture model with or without initial variable selection, the penalized mixture model performed much better in identifying relevant genes and forming corresponding clusters. The penalized mixture model seems to be promising for high-dimensional data with the capability of novel class discovery and automatic feature selection.	Univ Minnesota, Sch Publ Hlth, Div Biostat, Minneapolis, MN 55455 USA; Univ Minnesota, Sch Stat, Minneapolis, MN 55455 USA; Vanderbilt Univ, Dept Biostat, Nashville, TN 37240 USA; Univ Minnesota, Sch Med, Vasc Biol Ctr, Minneapolis, MN 55455 USA; Univ Minnesota, Sch Med, Div Hematol Oncol Transplantat, Minneapolis, MN 55455 USA	Pan, W (reprint author), Univ Minnesota, Sch Publ Hlth, Div Biostat, Minneapolis, MN 55455 USA.	weip@biostat.umn.edu					ALEXANDRIDIS R, 2004, BIOINFORMATICS, V20, P2546; Shen XT, 2002, J AM STAT ASSOC, V97, P210, DOI 10.1198/016214502753479356; Fraley C, 1998, COMPUT J, V41, P578, DOI 10.1093/comjnl/41.8.578; Chi JT, 2003, P NATL ACAD SCI USA, V100, P10623, DOI 10.1073/pnas.1434429100; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Lin Y, 2000, J CLIN INVEST, V105, P71, DOI 10.1172/JCI8071; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1; Tusher VG, 2001, P NATL ACAD SCI USA, V98, P5116, DOI 10.1073/pnas.091062498; Lin Y, 2002, BLOOD, V99, P457, DOI 10.1182/blood.V99.2.457; Irizarry RA, 2003, BIOSTATISTICS, V4, P249, DOI 10.1093/biostatistics/4.2.249; SWERLICK RA, 1992, J IMMUNOL, V148, P78; Tibshirani R, 2003, STAT SCI, V18, P104, DOI 10.1214/ss/1056397488; Efron B, 2004, ANN STAT, V32, P407; SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136; Tibshirani R, 2002, P NATL ACAD SCI USA, V99, P6567, DOI 10.1073/pnas.082099299; Broet P, 2004, BIOINFORMATICS, V20, P2562, DOI 10.1093/bioinformatics/bth285; Broet P, 2002, J COMPUT BIOL, V9, P671, DOI 10.1089/106652702760277381; Efron B, 2004, J AM STAT ASSOC, V99, P619, DOI 10.1198/016214504000000692; Hastie T., 2001, ELEMENTS STAT LEARNI; Hebbel RP, 2005, BLOOD, V106, p26A; Huang XH, 2005, BMC BIOINFORMATICS, V6, DOI 10.1186/1471-2105-6-205; JIANG A, 2005, THESIS U MINNESOTA M; McLachlan G, 2002, FINITE MIXTURE MODEL; McLachlan G., 1992, DISCRIMINANT ANAL ST; McLachlan G. J., 1988, MIXTURE MODELS INFER; McLachlan GJ, 2002, BIOINFORMATICS, V18, P413, DOI 10.1093/bioinformatics/18.3.413; NIGAM K, 2006, SEMISUEPRVISED LEARN; PAN W, 2006, PENALIZED MODEL BASE; Tseng GC, 2005, BIOMETRICS, V61, P10, DOI 10.1111/j.0006-341X.2005.031032.x; Zhu X., 2006, 1530 U WISC MAD DEP; ZOU H, 2005, ADAPTIVE LASSO ITS O; ZOU H, 2006, FEATURE SELECTION CL; ZOU H, 2004, DEGREES FREEDOM LASS	33	15	15	1	3	OXFORD UNIV PRESS	OXFORD	GREAT CLARENDON ST, OXFORD OX2 6DP, ENGLAND	1367-4803			BIOINFORMATICS	Bioinformatics	OCT 1	2006	22	19					2388	2395		10.1093/bioinformatics/btl393		8	Biochemical Research Methods; Biotechnology & Applied Microbiology; Computer Science, Interdisciplinary Applications; Mathematical & Computational Biology; Statistics & Probability	Biochemistry & Molecular Biology; Biotechnology & Applied Microbiology; Computer Science; Mathematical & Computational Biology; Mathematics	094WX	WOS:000241271000011	16870935	
J	Bo, LF; Ling, W; Jiao, LC				Bo Liefeng; Wang Ling; Jiao Licheng			Multi-layer perceptrons with embedded feature selection with application in cancer classification	CHINESE JOURNAL OF ELECTRONICS			English	Article						multi-layer perceptrons (MLPs); neural networks; Laplace prior; feature selection; cancer classification		This paper proposed a novel neural network model, named Multi-layer perceptrons with Embedded feature selection (MLPs-EFS), where feature selection is incorporated into the training procedure. Compared with the classical MLPs, MLPs-EFS add a preprocessing step where each feature of the samples is multiplied by the corresponding scaling factor. By applying a truncated Laplace prior to the scaling factors, feature selection is integrated as a part of MLPs-EFS. Moreover, a variant of MLPs-EFS, named EFS+MLPs is also given, which perform feature selection more flexibly. Application in cancer classification validates the effectiveness of the proposed algorithms.	Xidian Univ, Inst Intelligent Informat Proc, Xian 710071, Peoples R China	Bo, LF (reprint author), Xidian Univ, Inst Intelligent Informat Proc, Xian 710071, Peoples R China.						Bo LF, 2006, NEURAL COMPUT, V18, P961; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Guyon I, 2002, MACH LEARN, V46, P389, DOI 10.1023/A:1012487302797; Blum AL, 1997, ARTIF INTELL, V97, P245, DOI 10.1016/S0004-3702(97)00063-5; Demuth H, 1998, NEURAL NETWORK TOOLB; Donoho DL, 2003, P NATL ACAD SCI USA, V100, P2197, DOI 10.1073/pnas.0437847100; Hsu CN, 2002, IEEE T SYST MAN CY B, V32, P207, DOI 10.1109/3477.990877; MOLLER MF, 1993, NEURAL NETWORKS, V6, P525, DOI 10.1016/S0893-6080(05)80056-5; Setiono R., 1997, IEEE T NEURAL NETWOR, V8, P645; Sindhwani V, 2004, IEEE T NEURAL NETWOR, V15, P937, DOI 10.1109/TNN.2004.828772; WANG SJ, 2005, INT C NEUR NETW BRAI, V3, P1487; Zhang GQP, 2000, IEEE T SYST MAN CY C, V30, P451, DOI 10.1109/5326.897072	12	0	0	1	4	TECHNOLOGY EXCHANGE LIMITED HONG KONG	SHATIN	26-28 AU PUI WAN ST, STE 1102, FO TAN INDUSTRIAL CENTRE, FO TAN, SHATIN, 00000, PEOPLES R CHINA	1022-4653			CHINESE J ELECTRON	Chin. J. Electron.	OCT	2006	15	4A					832	835				4	Engineering, Electrical & Electronic	Engineering	103XJ	WOS:000241920200015		
J	Francois, O; Ancelet, S; Guillot, G				Francois, Olivier; Ancelet, Sophie; Guillot, Gilles			Bayesian clustering using hidden Markov random fields in spatial population genetics	GENETICS			English	Article							MODEL; INFERENCE; REGRESSION; DIVERSITY; DISTANCE; IMAGES	We introduce a new Bayesian clustering algorithm for studying population structure using individually geo-referenced multilocus data sets. The algorithm is based on the concept of hidden Markov random field, which models the spatial dependencies at the cluster membership level. We argue that (i) a Markov chain Monte Carlo procedure call implement the algorithm efficiently, (ii) it can detect significant geographical discontinuities in allele frequencies and regulate the number of clusters, (iii) it call check whether the clusters obtained without the use of spatial priors are robust to the hypothesis of discontinuous geographical variation in allele frequencies, and (iv) it can reduce the number of loci required to obtain accurate assignments. We illustrate and discuss the implementation issues with the Scandinavian brown bear and the human CEPH diversity panel data set.	Fac Med, Dept Math Biol, TIMC, TIMB, F-38706 La Tronche, France; ENGREF, Unite Math & Informat Appl, F-75732 Paris 15, France	Francois, O (reprint author), Fac Med, Dept Math Biol, TIMC, TIMB, F-38706 La Tronche, France.	olivier.francois@imag.fr	guillot, gilles/A-6129-2009; Francois, Olivier/A-6051-2012	guillot, gilles/0000-0003-0675-8325; 			Zhang YY, 2001, IEEE T MED IMAGING, V20, P45, DOI 10.1109/42.906424; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Bamshad MJ, 2003, AM J HUM GENET, V72, P578, DOI 10.1086/368061; Corander J, 2003, GENETICS, V163, P367; Cann HM, 2002, SCIENCE, V296, P261; Pritchard JK, 2000, GENETICS, V155, P945; KIMURA M, 1964, GENETICS, V49, P561; WU FY, 1982, REV MOD PHYS, V54, P235, DOI 10.1103/RevModPhys.54.235; Gelman A, 1998, STAT SCI, V13, P163; HOERL AE, 1970, TECHNOMETRICS, V12, P55; Wright S, 1943, GENETICS, V28, P114; BESAG J, 1986, J ROY STAT SOC B MET, V48, P259; Bishop C. M., 1995, NEURAL NETWORKS PATT; Blum MGB, 2004, THEOR POPUL BIOL, V65, P249, DOI 10.1016/j.tpb.2003.11.002; Coulon A, 2006, MOL ECOL, V15, P1669, DOI 10.1111/j.1365-294X.2006.02861.x; Dawson KJ, 2001, GENET RES, V78, P59; DESTREMPES F, 2005, IEEE T IMAGE PROCESS, V14, P1097; Ramachandran Sohini, 2004, Human Genomics, V1, P87; GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721; Green PJ, 2002, J AM STAT ASSOC, V97, P1055, DOI 10.1198/016214502388618870; Guillot G, 2005, GENETICS, V170, P1261, DOI 10.1534/genetics.104.033803; Guillot G, 2005, MOL ECOL NOTES, V5, P708; Guttorp P., 1995, STOCHASTIC MODELLING; Hartl DL, 1997, PRINCIPLES POPULATIO; HURN M, 2003, SPATIAL STAT COMPUTA, P87; Malecot G., 1948, MATH HEREDITE; Manel S, 2004, MOL ECOL, V13, P1327, DOI 10.1111/j.1365-294X.2004.02074.x; METROPOLIS N, 1953, J CHEM PHYS, V21, P1087, DOI 10.1063/1.1699114; Nei M, 2000, MOL EVOLUTION PHYLOG; POTTS RB, 1952, P CAMB PHILOS SOC, V48, P106; Preston C.J., 1974, GIBBS STATES COUNTAB; Ripley B. D., 1996, PATTERN RECOGNITION; Ripley B.D., 1988, STAT INFERENCE SPATI; Rosenberg N, 2002, SCIENCE, V298, P2981, DOI 10.1126/science.1078311; Rosenberg NA, 2005, PLOS GENET, V1, P660, DOI 10.1371/journal.pgen.0010070; ROUSSET F, 2004, GENTIC STRUCTURE SEL; Serre D, 2004, GENOME RES, V14, P1679, DOI 10.1101/gn2529604; Thomas DC, 2003, HUM HERED, V56, P32, DOI 10.1159/000073730; WAITS L, 2000, MOL ECOL, V9, P610; Wasser SK, 2004, P NATL ACAD SCI USA, V101, P14847, DOI 10.1073/pnas.0403170101	40	157	159	5	32	GENETICS	BALTIMORE	428 EAST PRESTON ST, BALTIMORE, MD 21202 USA	0016-6731			GENETICS	Genetics	OCT	2006	174	2					805	816		10.1534/genetics.106.059923		12	Genetics & Heredity	Genetics & Heredity	102GJ	WOS:000241798700021	16888334	
J	Maggini, R; Lehmann, A; Zimmermann, NE; Guisan, A				Maggini, Ramona; Lehmann, Anthony; Zimmermann, Niklaus E.; Guisan, Antoine			Improving generalized regression analysis for the spatial prediction of forest communities	JOURNAL OF BIOGEOGRAPHY			English	Article; Proceedings Paper	Workshop on Generalized Regression Analyses and Spatial Predictions	AUG, 2004	Riederalp, SWITZERLAND			cross-validation; forest communities; generalized additive models; generalized regression analysis; potential distribution modelling; predictor interactions; receiver operating characteristic; spatial autocorrelation; spatial predictions; stepwise selection methods	HABITAT DISTRIBUTION MODELS; SPECIES DISTRIBUTION MODELS; CLIMATE-CHANGE; NEW-ZEALAND; ADDITIVE-MODELS; ENVIRONMENTAL DETERMINANTS; LOGISTIC-REGRESSION; PLANT ECOLOGY; DISTRIBUTIONS; SELECTION	Aim This study used data from temperate forest communities to assess: (1) five different stepwise selection methods with generalized additive models, (2) the effect of weighting absences to ensure a prevalence of 0.5, (3) the effect of limiting absences beyond the environmental envelope defined by presences, (4) four different methods for incorporating spatial autocorrelation, and (5) the effect of integrating an interaction factor defined by a regression tree on the residuals of an initial environmental model. Location State of Vaud, western Switzerland. Methods Generalized additive models (GAMs) were fitted using the grasp package (generalized regression analysis and spatial predictions, http://www.cscf.ch/grasp). Results Model selection based on cross-validation appeared to be the best compromise between model stability and performance (parsimony) among the five methods tested. Weighting absences returned models that perform better than models fitted with the original sample prevalence. This appeared to be mainly due to the impact of very low prevalence values on evaluation statistics. Removing zeroes beyond the range of presences on main environmental gradients changed the set of selected predictors, and potentially their response curve shape. Moreover, removing zeroes slightly improved model performance and stability when compared with the baseline model on the same data set. Incorporating a spatial trend predictor improved model performance and stability significantly. Even better models were obtained when including local spatial autocorrelation. A novel approach to include interactions proved to be an efficient way to account for interactions between all predictors at once. Main conclusions Models and spatial predictions of 18 forest communities were significantly improved by using either: (1) cross-validation as a model selection method, (2) weighted absences, (3) limited absences, (4) predictors accounting for spatial autocorrelation, or (5) a factor variable accounting for interactions between all predictors. The final choice of model strategy should depend on the nature of the available data and the specific study aims. Statistical evaluation is useful in searching for the best modelling practice. However, one should not neglect to consider the shapes and interpretability of response curves, as well as the resulting spatial predictions in the final assessment.	Univ Lausanne, Dept Ecol & Evolut, CH-1015 Lausanne, Switzerland; CSCF, Neuchatel, Switzerland; Swiss Fed Res Inst WSL, Birmensdorf, Switzerland	Maggini, R (reprint author), Univ Lausanne, Dept Ecol & Evolut, Biophore Bldg, CH-1015 Lausanne, Switzerland.	ramona.maggini@unil.ch	Lehmann, Anthony/B-1544-2010; Guisan, Antoine/A-1057-2011; Zimmermann, Niklaus/A-4276-2008; 	Guisan, Antoine/0000-0002-3998-4815; Zimmermann, Niklaus/0000-0003-3099-9604; Lehmann, Anthony/0000-0002-8279-8567			Akaike H., 1973, 2 INT S INF THEOR, P267; Guisan A, 2000, J VEG SCI, V11, P617, DOI 10.2307/3236568; AUSTIN MP, 1989, VEGETATIO, V83, P35, DOI 10.1007/BF00031679; Araujo MB, 2005, GLOBAL CHANGE BIOL, V11, P1504, DOI 10.1111/j.1365-2486.2005.001000.x; McPherson JM, 2004, J APPL ECOL, V41, P811, DOI 10.1111/j.0021-8901.2004.00943.x; Clarkson BR, 2004, WETLANDS, V24, P133, DOI 10.1672/0277-5212(2004)024[0133:VAPCIT]2.0.CO;2; Peterson AT, 2003, Q REV BIOL, V78, P419, DOI 10.1086/378926; Guisan A, 2000, ECOL MODEL, V135, P147, DOI 10.1016/S0304-3800(00)00354-9; LEGENDRE P, 1993, ECOLOGY, V74, P1659, DOI 10.2307/1939924; Araujo MB, 2005, GLOBAL ECOL BIOGEOGR, V14, P529, DOI 10.1111/j.1466-822x.2005.00182.x; Manel S, 2001, J APPL ECOL, V38, P921, DOI 10.1046/j.1365-2664.2001.00647.x; Liu CR, 2005, ECOGRAPHY, V28, P385, DOI 10.1111/j.0906-7590.2005.03957.x; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Suarez-Seoane S, 2002, J APPL ECOL, V39, P755, DOI 10.1046/j.1365-2664.2002.00751.x; Austin MP, 1996, FOREST ECOL MANAG, V85, P95, DOI 10.1016/S0378-1127(96)03753-X; Anselin L, 2006, GEOGR ANAL, V38, P5, DOI 10.1111/j.0016-7363.2005.00671.x; Castella E, 2001, FRESHWATER BIOL, V46, P1811, DOI 10.1046/j.1365-2427.2001.00860.x; DUTILLEUL P, 1993, BIOMETRICS, V49, P305, DOI 10.2307/2532625; STONE M, 1977, J R STAT SOC B, V39, P44; Keitt TH, 2002, ECOGRAPHY, V25, P616, DOI 10.1034/j.1600-0587.2002.250509.x; Lehmann A, 2002, BIODIVERS CONSERV, V11, P2217, DOI 10.1023/A:1021398729516; Burnham KP, 2004, SOCIOL METHOD RES, V33, P261, DOI 10.1177/0049124104268644; Thuiller W, 2004, GLOBAL CHANGE BIOL, V10, P2020, DOI 10.1111/j.1365-2486.2004.00859.x; Stockwell DRB, 2002, ECOL MODEL, V148, P1, DOI 10.1016/S0304-3800(01)00388-X; Lichstein JW, 2002, ECOL MONOGR, V72, P445, DOI 10.2307/3100099; Jaberg C, 2001, J APPL ECOL, V38, P1169, DOI 10.1046/j.0021-8901.2001.00668.x; Araujo MB, 2006, J BIOGEOGR, V33, P1712, DOI 10.1111/j.1365-2699.2006.01482.x; Guisan A, 2002, ECOL MODEL, V157, P89, DOI 10.1016/S0304-3800(02)00204-1; Leathwick JR, 1996, ENVIRON SOFTW, V11, P81, DOI 10.1016/S0266-9838(96)00045-7; Segurado P, 2004, J BIOGEOGR, V31, P1555, DOI 10.1111/j.1365-2699.2004.01076.x; Van den Berg MS, 2003, HYDROBIOLOGIA, V506, P611, DOI 10.1023/B:HYDR.0000008610.97044.39; Pearce J, 2000, ECOL MODEL, V128, P127, DOI 10.1016/S0304-3800(99)00227-6; Augustin NH, 1996, J APPL ECOL, V33, P339, DOI 10.2307/2404755; BRZEZIECKI B, 1995, J VEG SCI, V6, P257, DOI 10.2307/3236221; Guisan A, 2005, ECOL LETT, V8, P993, DOI 10.1111/j.1461-0248.2005.00792.x; Moisen GG, 2002, ECOL MODEL, V157, P209, DOI 10.1016/S0304-3800(02)00197-7; Moser D, 2005, J BIOGEOGR, V32, P1117, DOI 10.1111/j.1365-2699.2005.01265.x; Wagner HH, 2005, ECOLOGY, V86, P1975, DOI 10.1890/04-0914; Guisan A, 1999, PLANT ECOL, V143, P107, DOI 10.1023/A:1009841519580; Johnson JB, 2004, TRENDS ECOL EVOL, V19, P101, DOI 10.1016/j.tree.2003.10.013; Ozesmi SL, 1999, ECOL MODEL, V116, P15, DOI 10.1016/S0304-3800(98)00149-5; SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136; Austin MP, 2002, ECOL MODEL, V157, P101, DOI 10.1016/S0304-3800(02)00205-3; Fielding AH, 1997, ENVIRON CONSERV, V24, P38, DOI 10.1017/S0376892997000088; Graham CH, 2004, TRENDS ECOL EVOL, V19, P497, DOI 10.1016/j.tree.2004.07.006; Zaniewski AE, 2002, ECOL MODEL, V157, P261, DOI 10.1016/S0304-3800(02)00199-0; Araujo MB, 2002, P ROY SOC B-BIOL SCI, V269, P1971, DOI 10.1098/rspb.2002.2121; Austin MP, 1999, ECOGRAPHY, V22, P465, DOI 10.1111/j.1600-0587.1999.tb01276.x; AUSTIN MP, 1998, IMPROVED METHODS MOD; Borcard D, 1994, ENVIRON ECOL STAT, V1, P37, DOI 10.1007/BF00714196; BRZEZIECKI B, 1993, J VEG SCI, V4, P499, DOI 10.2307/3236077; Burnham K. P., 2002, MODEL SELECTION MULT; Cawsey EM, 2002, BIODIVERS CONSERV, V11, P2239, DOI 10.1023/A:1021350813586; CLOT F, 1998, BANQUE DONNEES PHYTO; COUDUN C, 2006, J BIOGEOGRAPHY; Dambacher JM, 2002, ECOLOGY, V83, P1372, DOI 10.2307/3071950; DELARZE R, 1998, GUIDE MILIEUX SUISSE; Dirnbock T, 2004, J VEG SCI, V15, P77, DOI 10.1658/1100-9233(2004)015[0077:HDMSAF]2.0.CO;2; Efron B, 1993, INTRO BOOTSTRAP; Ferrier S, 2002, BIODIVERS CONSERV, V11, P2309, DOI 10.1023/A:1021374009951; Franklin J, 2000, TERRAIN ANAL PRINCIP, P331; Franklin J, 1998, J VEG SCI, V9, P733, DOI 10.2307/3237291; Franklin J, 1995, PROG PHYS GEOG, V19, P474, DOI 10.1177/030913339501900403; Garza-Perez JR, 2004, MAR ECOL PROG SER, V269, P141, DOI 10.3354/meps269141; Guisan A, 1998, J VEG SCI, V9, P65, DOI 10.2307/3237224; GUISAN A, IN PRESS CONSERVATIO; Guisan A, 2003, J BIOGEOGR, V30, P1233, DOI 10.1046/j.1365-2699.2003.00914.x; Hampe A, 2004, GLOBAL ECOL BIOGEOGR, V13, P469, DOI 10.1111/j.1466-822X.2004.00090.x; Harrell FE, 2001, REGRESSION MODELING; Hastie T., 2001, ELEMENTS STAT LEARNI; Hastie TJ, 1990, GEN ADDITIVE MODELS; HURLBERT SH, 1984, ECOL MONOGR, V54, P187, DOI 10.2307/1942661; Joly P, 2001, CONSERV BIOL, V15, P239, DOI 10.1046/j.1523-1739.2001.99200.x; Kuha J, 2004, SOCIOL METHOD RES, V33, P188, DOI 10.1177/0049124103262065; Leathwick JR, 1998, J VEG SCI, V9, P719, DOI 10.2307/3237290; Leathwick JR, 2002, BIODIVERS CONSERV, V11, P2177, DOI 10.1023/A:1021394628607; Leathwick JR, 2001, FUNCT ECOL, V15, P233, DOI 10.1046/j.1365-2435.2001.00504.x; Leathwick JR, 2001, ECOLOGY, V82, P2560, DOI 10.1890/0012-9658(2001)082[2560:CIBTSI]2.0.CO;2; Legendre P, 2002, ECOGRAPHY, V25, P601, DOI 10.1034/j.1600-0587.2002.250508.x; Lehmann A., 2005, GRASP V 3 2 USERS MA; Lehmann A, 2002, ECOL MODEL, V157, P189, DOI 10.1016/S0304-3800(02)00195-3; Lehmann A, 2002, BIODIVERS CONSERV, V11, P2085, DOI 10.1023/A:1021354914494; Liebhold AM, 2002, ECOGRAPHY, V25, P553, DOI 10.1034/j.1600-0587.2002.250505.x; Lobo JM, 2001, BIOL J LINN SOC, V73, P233, DOI 10.1006/bijl.2001.0543; LUOTO L, 2006, J BIOGEGRAPHY; Maggini R, 2002, BIODIVERS CONSERV, V11, P2117, DOI 10.1023/A:1021338510860; Manel S, 1999, ECOL MODEL, V120, P337, DOI 10.1016/S0304-3800(99)00113-1; Manel S, 1999, J APPL ECOL, V36, P734, DOI 10.1046/j.1365-2664.1999.00440.x; MANLY BFJ, 2002, RESOURCE SELECTION A; Nogues-Bravo D, 2004, J BIOGEOGR, V31, P629; *OFS, 2003, GEOSTAT MAN UT; *OFT, 1990, ATL SUISS; Opdam P, 2004, BIOL CONSERV, V117, P285, DOI 10.1016/j.biocon.2003.12.008; Overton JM, 2002, BIODIVERS CONSERV, V11, P2093, DOI 10.1023/A:1021386426790; Palomares F, 1998, J ANIM ECOL, V67, P967, DOI 10.1046/j.1365-2656.1998.6760967.x; PEARSON RG, 2006, J BIOGEOGRAPHY; PEREIRA JMC, 1991, PHOTOGRAMM ENG REM S, V57, P1475; Perry JN, 2002, ECOGRAPHY, V25, P578, DOI 10.1034/j.1600-0587.2002.250507.x; RANDIN CF, 2006, J BIOGEOGRAPHY; Ray N, 2002, BIODIVERS CONSERV, V11, P2143, DOI 10.1023/A:1021390527698; Schmieder K, 2004, J VEG SCI, V15, P807, DOI 10.1111/j.1654-1103.2004.tb02324.x; Schwarz PA, 2003, ECOLOGY, V84, P1862, DOI 10.1890/0012-9658(2003)084[1862:FCSVOT]2.0.CO;2; Scott JM, 2002, PREDICTING SPECIES O; Svenning JC, 2005, J BIOGEOGR, V32, P1019, DOI 10.1111/j.1365-2699.2005.01219.x; SWETS JA, 1988, SCIENCE, V240, P1285, DOI 10.1126/science.3287615; Thuiller W, 2004, ECOGRAPHY, V27, P165, DOI 10.1111/j.0906-7590.2004.03673.x; TURC L., 1961, Ann. agron. Paris, V12, P13; Yee TW, 2002, ECOL MODEL, V157, P141, DOI 10.1016/S0304-3800(02)00192-8; YEE TW, 1991, J VEG SCI, V2, P587, DOI 10.2307/3236170; Zhang LJ, 2005, ECOL MODEL, V186, P154, DOI 10.1016/j.ecolmodel.2005.01.007; ZIMMERMANN NE, 2001, FINAL REPORT MLP CLI; Zimmermann NE, 1999, J VEG SCI, V10, P469, DOI 10.2307/3237182	112	93	97	7	37	BLACKWELL PUBLISHING	OXFORD	9600 GARSINGTON RD, OXFORD OX4 2DQ, OXON, ENGLAND	0305-0270			J BIOGEOGR	J. Biogeogr.	OCT	2006	33	10					1729	1749		10.1111/j.1365-2699.2006.01465.x		21	Ecology; Geography, Physical	Environmental Sciences & Ecology; Physical Geography	082DV	WOS:000240367800005		
J	Leng, CL; Zhang, HH				Leng, Chenlei; Zhang, Hao Helen			Model selection in nonparametric hazard regression	JOURNAL OF NONPARAMETRIC STATISTICS			English	Article						COSSO; Cox proportional hazard model; model selection; penalized likelihood	VARIABLE SELECTION; SURVIVAL-DATA; COX MODEL; LASSO	We propose a novel model selection method for a nonparametric extension of the Cox proportional hazard model, in the framework of smoothing splines ANOVA models. The method automates the model building and model selection processes simultaneously by penalizing the reproducing kernel Hilbert space norms. On the basis of a reformulation of the penalized partial likelihood, we propose an efficient algorithm to compute the estimate. The solution demonstrates great flexibility and easy interpretability in modeling relative risk functions for censored data. Adaptive choice of the smoothing parameter is discussed. Both simulations and a real example suggest that our proposal is a useful tool for multivariate function estimation and model selection in survival analysis.	Natl Univ Singapore, Dept Stat & Probabil, SG-117546 Singapore, Singapore; N Carolina State Univ, Dept Stat, Raleigh, NC 27695 USA	Leng, CL (reprint author), Natl Univ Singapore, Dept Stat & Probabil, SG-117546 Singapore, Singapore.	stalc@nus.edu.sg					Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; BRESLOW N, 1974, BIOMETRICS, V30, P89, DOI 10.2307/2529620; Cox DR, 1972, J R STAT SOC B, V34, P197; Fan JQ, 2006, ANN STAT, V34, P290, DOI 10.1214/009053605000000796; Fan JQ, 2002, ANN STAT, V30, P74; Fleming T. R., 1991, COUNTING PROCESSES S; GRAY RJ, 1992, J AM STAT ASSOC, V87, P942, DOI 10.2307/2290630; Gu C, 1996, STAT SINICA, V6, P861; GU C, 1992, J AM STAT ASSOC, V87, P1051, DOI 10.2307/2290642; GU C, 1991, SIAM J SCI STAT COMP, V12, P383, DOI 10.1137/0912021; Gu C., 2002, SMOOTHING SPLINE ANO; Hastie TJ, 1990, GEN ADDITIVE MODELS; Huang JHZ, 2000, ANN STAT, V28, P961; KALBFLEISCH JD, 2002, STAT ANAL FAILURE TI, P321; Kim YJ, 2004, J ROY STAT SOC B, V66, P337, DOI 10.1046/j.1369-7412.2003.05316.x; KOOPERBERG C, 1995, J AM STAT ASSOC, V90, P78, DOI 10.2307/2291132; LIN Y, 2006, IN PRESS ANN STAT, V34; OSULLIVAN F, 1993, ANN STAT, V21, P124, DOI 10.1214/aos/1176349018; Therneau TM, 2000, MODELING SURVIVAL DA; Tibshirani R, 1997, STAT MED, V16, P385, DOI 10.1002/(SICI)1097-0258(19970228)16:4<385::AID-SIM380>3.0.CO;2-3; Wahba G., 1990, CBMS NSF REGIONAL C, V59; Xiang D, 1996, STAT SINICA, V6, P675; Zhang HH, 2004, J AM STAT ASSOC, V99, P659, DOI 10.1198/016214504000000593	23	8	8	1	7	TAYLOR & FRANCIS LTD	ABINGDON	4 PARK SQUARE, MILTON PARK, ABINGDON OX14 4RN, OXON, ENGLAND	1048-5252			J NONPARAMETR STAT	J. Nonparametr. Stat.	OCT-NOV	2006	18	7-8					417	429		10.1080/10485250601027042		13	Statistics & Probability	Mathematics	162FD	WOS:000246071300001		
J	Leng, CL; Lin, Y; Wahba, G				Leng, Chenlei; Lin, Yi; Wahba, Grace			A note on the Lasso and related procedures in model selection	STATISTICA SINICA			English	Article						consistent model selection; Forward Stagewise regression; Lars; Lasso; variable selection	VARIABLE SELECTION; REGRESSION; LIKELIHOOD; SHRINKAGE	The Lasso, the Forward Stagewise regression and the Lars are closely related procedures recently proposed for linear regression problems. Each of them can produce sparse models and can be used both for estimation and variable selection. In practical implementations these algorithms are typically tuned to achieve optimal prediction accuracy. We show that, when the prediction accuracy is used as the criterion to choose the tuning parameter, in general these procedures are not consistent in terms of variable selection. That is, the sets of variables selected are not consistently the true set of important variables. In particular, we show that for any sample size n, when there are superfluous variables in the linear regression model and the design matrix is orthogonal, the probability that these procedures correctly identify the true set of important variables is less than a constant (smaller than one) not depending on n. This result is also shown to hold for two-dimensional problems with general correlated design matrices. The results indicate that in problems where the main goal is variable selection, prediction-accuracy-based criteria alone are not sufficient for this purpose. Adjustments will be discussed to make the Lasso and related procedures useful/consistent for variable selection.	Natl Univ Singapore, Dept Stat & Appl Probabil, Singapore 117546, Singapore; Univ Wisconsin, Dept Stat, Madison, WI 53706 USA	Leng, CL (reprint author), Natl Univ Singapore, Dept Stat & Appl Probabil, Singapore 117546, Singapore.	stalc@nus.edu.sg; yilin@stat.wisc.edu; wahba@stat.wisc.edu					Knight K, 2000, ANN STAT, V28, P1356; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; Osborne MR, 2000, IMA J NUMER ANAL, V20, P389, DOI 10.1093/imanum/20.3.389; Efron B, 2004, ANN STAT, V32, P407; DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425; CRAVEN P, 1979, NUMER MATH, V31, P377; Broman KW, 2002, J R STAT SOC B, V64, P641, DOI 10.1111/1467-9868.00354; MEINSHAUSEN N, 2004, SEM STAT ETH ZUR; RAO CR, 1989, BIOMETRIKA, V76, P369, DOI 10.2307/2336671; Shao J, 1997, STAT SINICA, V7, P221; STEIN CM, 1981, ANN STAT, V9, P1135, DOI 10.1214/aos/1176345632; Tibshirani R, 1997, STAT MED, V16, P385, DOI 10.1002/(SICI)1097-0258(19970228)16:4<385::AID-SIM380>3.0.CO;2-3; Zhang HH, 2004, J AM STAT ASSOC, V99, P659, DOI 10.1198/016214504000000593	14	78	79	2	8	STATISTICA SINICA	TAIPEI	C/O DR H C HO, INST STATISTICAL SCIENCE, ACADEMIA SINICA, TAIPEI 115, TAIWAN	1017-0405			STAT SINICA	Stat. Sin.	OCT	2006	16	4					1273	1284				12	Statistics & Probability	Mathematics	108ZE	WOS:000242276900011		
J	Ioannidis, A; Nicolaou, C; Legakis, NJ; Ioannidou, V; Chatzipanagiotou, S				Ioannidis, Anastassios; Nicolaou, Chryssoula; Legakis, Nicholas J.; Ioannidou, Vasiliki; Chatzipanagiotou, Stylianos			The first database comprised of flagellin gene (flaA) types of Campylobacter jejuni human clinical isolates from Greece	EUROPEAN JOURNAL OF EPIDEMIOLOGY			English	Article						bin patterns; Campylobacter jejuni; fagellin gene; flaA typing; serotyping	TYPING SYSTEM	Flagellin subunit A gene (flaA) typing of Campylobacter has been recognized by several groups as a relatively simple and quick genotyping method. The present study aimed to create, for the first time in Greece, a database with flaA restriction patterns, which could be used for future epidemiological and clinical studies. A total of 207 C. jejuni clinical isolates of known serotype were collected from 5 general hospitals of the area of Attica, during the period 2000-2003. The RFLP profiles of each strain were matched in 44 bins of 0 or 1. Thirty nine different flaA types, designated as flaA 1 GR to flaA 39 GR (GR: Greece) were found. There was no significant association of certain genotypes with certain serotypes. However flaA typing showed a remarkable discriminatory ability inside the non-typable (NT) group. Evaluating our results we observed (i) that there was no clonality of a certain flaA type among the strains and the serotypes examined and (ii) that the discriminatory ability of flaA typing was much better than that of serotyping. Giving a simple and detailed description of the data analysis, we are the first who publish the bin patterns for the flaA genotypes found.	Athens Med Sch, Aeginit Hosp, Dept Clin Microbiol, Athens 11528, Greece; Athens Med Sch, Dept Microbiol, Athens 11528, Greece	Chatzipanagiotou, S (reprint author), Athens Med Sch, Aeginit Hosp, Dept Clin Microbiol, Vass Sophias Av 72-74, Athens 11528, Greece.	schatzi@med.uoa.gr					Dingle KE, 2001, J CLIN MICROBIOL, V39, P14, DOI 10.1128/JCM.39.1.14-23.2001; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; LINK W, 1995, THEOR APPL GENET, V90, P27, DOI 10.1007/BF00220992; Wassenaar TM, 2000, APPL ENVIRON MICROB, V66, P1; VANDEPEER Y, 1994, COMPUT APPL BIOSCI, V10, P569; Harrington CS, 1997, J CLIN MICROBIOL, V35, P2386; LIOR H, 1982, J CLIN MICROBIOL, V15, P761; NACHAMKIN I, 1993, J CLIN MICROBIOL, V31, P1531; Nachamkin I, 1996, J CLIN MICROBIOL, V34, P277; Newell DG, 2000, CAMPYLOBACTER, P27; PENNER JL, 1980, J CLIN MICROBIOL, V12, P732	11	3	3	0	0	SPRINGER	NEW YORK	233 SPRING STREET, NEW YORK, NY 10013 USA	0393-2990			EUR J EPIDEMIOL	Eur. J. Epidemiol.	NOV	2006	21	11					823	829		10.1007/s10654-006-9073-2		7	Public, Environmental & Occupational Health	Public, Environmental & Occupational Health	111BQ	WOS:000242426300005	17111250	
J	Stout, F; Baines, MR; Kalivas, JH				Stout, Forrest; Baines, Maurita R.; Kalivas, John H.			Impartial graphical comparison of multivariate calibration methods and the harmony/parsimony tradeoff	JOURNAL OF CHEMOMETRICS			English	Article						degrees of freedom; effective rank; L-curve; meta-parameter selection; multivariate calibration; harmonious; parsimonious	PRINCIPAL COMPONENT REGRESSION; PARTIAL LEAST-SQUARES; WAVELENGTH SELECTION; TIKHONOV REGULARIZATION; PARETO CALIBRATION; CROSS-VALIDATION; MODEL SELECTION; MEAN SUBSET; BASIS-SETS; CLASSIFICATION	For multivariate calibration with the relationship y=Xb, it is often necessary to determine the degrees of freedom for parsimony consideration and for the error measure root mean square error of calibration (RMSEC). This paper shows that degrees of freedom can be estimated by an effective rank (ER) measure to estimate the model fitting degrees of freedom and the more parsimonious model has the smallest ER. This paper also shows that when such a measure is used on the X-axis, simultaneous graphing of model errors and other regression diagnostics is possible for ridge regression (RR), partial least squares (PLS) and principal component regression (PCR) and thus, a fair comparison between all potential models can be accomplished. The ER approach is general and applicable to other multivariate calibration methods. It is often noted that by selecting variables, more parsimonious models are obtained; typically by multiple linear regression (MLR). By using the ER, the more parsimonious model is graphically shown to not always be the MLR model. Additionally, a harmony measure is proposed that expresses the bias/variance tradeoff for a particular model. By plotting this new measure against the ER, the proper harmony/parsimony tradeoff can be graphically assessed for RR, PCR and PLS. Essentially, pluralistic criteria for fairly valuating and characterizing models are better than a dualistic or a single criterion approach which is the usual tactic. Results are presented using spectral, industrial and quantitative structure activity relationship (QSAR) data. Copyright (C) 2007 John Wiley & Sons, Ltd.	Idaho State Univ, Dept Chem, Pocatello, ID 83209 USA	Kalivas, JH (reprint author), Idaho State Univ, Dept Chem, Pocatello, ID 83209 USA.	kalijohn@isu.edu					Anderson KJ, 2003, APPL SPECTROSC, V57, P309, DOI 10.1366/000370203321558227; Fu WJJ, 1998, J COMPUT GRAPH STAT, V7, P397, DOI 10.2307/1390712; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; FRANK IE, 1993, TECHNOMETRICS, V35, P109, DOI 10.2307/1269656; SHAO J, 1993, J AM STAT ASSOC, V88, P486, DOI 10.2307/2290328; GOLUB GH, 1979, TECHNOMETRICS, V21, P215, DOI 10.1080/00401706.1979.10489751; ASTM (American Society of Testing and Materials), 1998, ANN BOOK ASTM STAND, V3, P10; Belge M, 2002, INVERSE PROBL, V18, P1161, DOI 10.1088/0266-5611/18/4/314; Booksh K.S., 1995, CHEMOMETRICS ENV CHE, P209; Booksh K.S., 1994, ANAL CHEM, V66, P782; Capron X, 2005, J CHEMOMETR, V19, P308, DOI 10.1002/cem.934; CLAERBOU.JF, 1973, GEOPHYSICS, V38, P826, DOI 10.1190/1.1440378; Dax A, 1992, SIAM J OPTIMIZ, V2, P602, DOI 10.1137/0802029; DiFoggio R, 2005, J CHEMOMETR, V19, P203, DOI 10.1002/cem.925; DUINEVELD CAA, 1993, ANAL CHIM ACTA, V277, P455, DOI 10.1016/0003-2670(93)80456-U; ELIERS PHC, 2003, ANAL CHEM, V75, P3631; Faber K, 1997, J CHEMOMETR, V11, P181, DOI 10.1002/(SICI)1099-128X(199705)11:3<181::AID-CEM459>3.0.CO;2-7; FEARN T, 1992, ADV NEAR INFRARED SP, P147; Forrester JB, 2004, J CHEMOMETR, V18, P372, DOI 10.1002/cem.883; Geladi P., 1995, FRONTIERS ANAL SPECT, P225; GELADI P, 1994, CHEMOMETR INTELL LAB, V24, P145, DOI 10.1016/0169-7439(94)00035-2; GELADI P, 1991, J CHEMOMETR, V5, P97, DOI 10.1002/cem.1180050206; Green RL, 2002, CHEMOMETR INTELL LAB, V60, P173, DOI 10.1016/S0169-7439(01)00194-0; Hansen P., 1998, RANK DEFICIENT DISCR; Hansen P.C., 2001, COMPUTATIONAL INVERS, P119; Hastie T., 2001, ELEMENTS STAT LEARNI; HOSKULDSSON A, 1992, CHEMOMETR INTELL LAB, V14, P139, DOI 10.1016/0169-7439(92)80099-P; Kalivas JH, 1999, J CHEMOMETR, V13, P111, DOI 10.1002/(SICI)1099-128X(199903/04)13:2<111::AID-CEM532>3.0.CO;2-N; Kalivas JH, 2004, ANAL CHIM ACTA, V505, P9, DOI 10.1016/S0003-2670(02)01603-3; Kalivas JH, 2001, APPL SPECTROSC, V55, P1645, DOI 10.1366/0003702011953955; Kalivas JH, 2001, ANAL CHIM ACTA, V428, P31, DOI 10.1016/S0003-2670(00)01225-3; Kalivas JH, 2005, ANAL LETT, V38, P2259, DOI 10.1080/0032710500315904; Kalivas JH, 2005, J CHEMOMETR, V19, P64, DOI 10.1002/cem.905; Mallows C., 1973, TECHNOMETRICS, V42, P87; MARK H, 1991, PRINCIPLES PRACTICES; Mattioni BE, 2002, J CHEM INF COMP SCI, V42, P94, DOI 10.1021/ci0100696; Myers R, 1990, CLASSICAL MODERN REG; Naes T., 2002, USER FRIENDLY GUIDE; O'Sullivan F., 1986, STAT SCI, V1, P502, DOI 10.1214/ss/1177013525; Ojelund H, 2001, J CHEMOMETR, V15, P497; Ojelund H, 2002, TECHNOMETRICS, V44, P369, DOI 10.1198/004017002188618563; Ojelund H, 2002, APPL SPECTROSC, V56, P887, DOI 10.1366/000370202760171563; SANTOSA F, 1986, SIAM J SCI STAT COMP, V7, P1307, DOI 10.1137/0907087; Seipel HA, 2004, J CHEMOMETR, V18, P306, DOI 10.1002/cem.874; STEINBERG DM, 1984, TECHNOMETRICS, V26, P71, DOI 10.2307/1268097; Stout F, 2007, APPL SPECTROSC, V61, P85, DOI 10.1366/000370207779701479; STOUT F, IN PRESS ANAL LET; Stout F, 2006, J CHEMOMETR, V20, P22, DOI 10.1002/cem.975; TAYLOR HL, 1979, GEOPHYSICS, V44, P39, DOI 10.1190/1.1440921; TILLMANN P, 2000, J NEAR INFRARED SPEC, V8, P103; van de Geer S., 2001, MATH METH STAT, V10, P355; Van der Voet H, 1999, J CHEMOMETR, V13, P195, DOI 10.1002/(SICI)1099-128X(199905/08)13:3/4<195::AID-CEM540>3.0.CO;2-L; Westerhaus M.O., 1990, P 3 INT C NEAR INFR, P671; Wise B. M., 2003, PLS TOOLBOX 3 0 USE; Ye JM, 1998, J AM STAT ASSOC, V93, P120, DOI 10.2307/2669609; Yeow YL, 2005, APPL SPECTROSC, V59, P584, DOI 10.1366/0003702053946056	57	13	13	1	4	JOHN WILEY & SONS LTD	CHICHESTER	THE ATRIUM, SOUTHERN GATE, CHICHESTER PO19 8SQ, W SUSSEX, ENGLAND	0886-9383			J CHEMOMETR	J. Chemometr.	NOV-DEC	2006	20	11-12					464	475		10.1002/cem.1025		12	Automation & Control Systems; Chemistry, Analytical; Computer Science, Artificial Intelligence; Instruments & Instrumentation; Mathematics, Interdisciplinary Applications; Statistics & Probability	Automation & Control Systems; Chemistry; Computer Science; Instruments & Instrumentation; Mathematics	175ZC	WOS:000247051500003		
J	Mukherjee, S; Wu, Q				Mukherjee, Sayan; Wu, Qiang			Estimation of gradients and coordinate covariation in classification	JOURNAL OF MACHINE LEARNING RESEARCH			English	Article						Tikhnonov regularization; variable selection; reproducing kernel Hilbert space; generalization bounds; classification	SUPPORT VECTOR MACHINES/; NETWORKS; SELECTION; CANCER	We introduce an algorithm that simultaneously estimates a classification function as well as its gradient in the supervised learning framework. The motivation for the algorithm is to find salient variables and estimate how they covary. An efficient implementation with respect to both memory and time is given. The utility of the algorithm is illustrated on simulated data as well as a gene expression data set. An error analysis is given for the convergence of the estimate of the classification function and its gradient to the true classification function and true gradient.	Duke Univ, Dept Comp Sci, Inst Stat & Decis Sci, Inst Genome Sci & Policy, Durham, NC 27708 USA	Mukherjee, S (reprint author), Duke Univ, Dept Comp Sci, Inst Stat & Decis Sci, Inst Genome Sci & Policy, Durham, NC 27708 USA.	SAYAN@STAT.DUKE.EDU; QIANG@STAT.DUKE.EDU	Wu, Qiang/B-1620-2008; 	Wu, Qiang/0000-0002-4698-6966; Mukherjee, Sayan/0000-0002-6715-3920			Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Chapelle O, 2002, MACH LEARN, V46, P131, DOI 10.1023/A:1012450327387; Tusher VG, 2001, P NATL ACAD SCI USA, V98, P5116, DOI 10.1073/pnas.091062498; Micchelli CA, 2005, NEURAL COMPUT, V17, P177, DOI 10.1162/0899766052530802; Golub TR, 1999, SCIENCE, V286, P531, DOI 10.1126/science.286.5439.531; ARONSZAJN N, 1950, T AM MATH SOC, V68, P337; Guyon I, 2002, MACH LEARN, V46, P389, DOI 10.1023/A:1012487302797; Evgeniou T, 2000, ADV COMPUT MATH, V13, P1, DOI 10.1023/A:1018946025316; Wu Q, 2005, NEURAL COMPUT, V17, P1160, DOI 10.1162/0899766053491896; CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; BARTLETT BL, 2005, J AM STAT ASSOC, V101, P138; CHEN X, 1999, P 14 INT S PLASM CHE, V1, P61; EVGENIOU T, 2000, P AS C COMP VIS; Hastie T., 2001, ELEMENTS STAT LEARNI; HERMES L, 2000, INT C PATT REC; Koltchinskii V., 1999, HIGH DIMENSIONAL PRO, VII, P443; Lee YK, 2004, J AM STAT ASSOC, V99, P67, DOI 10.1198/016214504000000098; LIANG F, 2006, IN PRESS STAT SCI; LIN Y, 2006, IN PRESS ANN STAT; MCDIARMID C, 1989, LOND MATH S, V141, P148; MUKHERJEE S, 2006, UNPUB ANN STAT; Mukherjee S, 2006, J MACH LEARN RES, V7, P519; POGGIO T, 1990, SCIENCE, V247, P978, DOI 10.1126/science.247.4945.978; Schoelkopf B., 2001, LEARNING KERNELS SUP; Slonim D.K., 2000, P 4 ANN INT C COMP M, P263, DOI 10.1145/332306.332564; Van der Vaart A., 1996, WEAK CONVERGENCE EMP; Vapnik V., 1998, STAT LEARNING THEORY; Wahba G., 1990, SERIES APPL MATH, V59; Zhang T, 2004, ANN STAT, V32, P56	29	29	30	0	6	MICROTOME PUBLISHING	BROOKLINE	31 GIBBS STREET, BROOKLINE, MA 02446 USA	1532-4435			J MACH LEARN RES	J. Mach. Learn. Res.	NOV	2006	7						2481	2514				34	Automation & Control Systems; Computer Science, Artificial Intelligence	Automation & Control Systems; Computer Science	152VS	WOS:000245390700008		
J	Zhao, P; Yu, B				Zhao, Peng; Yu, Bin			On model selection consistency of Lasso	JOURNAL OF MACHINE LEARNING RESEARCH			English	Article						Lasso; regularization; sparsity; model selection; consistency	REGRESSION	Sparsity or parsimony of statistical models is crucial for their proper interpretations, as in sciences and social sciences. Model selection is a commonly used method to find such models, but usually involves a computationally heavy combinatorial search. Lasso (Tibshirani, 1996) is now being used as a computationally feasible alternative to model selection. Therefore it is important to study Lasso for model selection purposes. In this paper, we prove that a single condition, which we call the Irrepresentable Condition, is almost necessary and sufficient for Lasso to select the true model both in the classical fixed p setting and in the large p setting as the sample size n gets large. Based on these results, sufficient conditions that are verifiable in practice are given to relate to previous works and help applications of Lasso for feature selection and sparse representation. This Irrepresentable Condition, which depends mainly on the covariance of the predictor variables, states that Lasso selects the true model consistently if and (almost) only if the predictors that are not in the true model are "irrepresentable" (in a sense to be clarified) by predictors that are in the true model. Furthermore, simulations are carried out to provide insights and understanding of this result.	Univ Calif Berkeley, Dept Stat, Berkeley, CA 94720 USA	Zhao, P (reprint author), Univ Calif Berkeley, Dept Stat, 367 Evans Hall, Berkeley, CA 94720 USA.	PENGZHAO@STAT.BERKELEY.EDU; BINYU@STAT.BERKELEY.EDU					Knight K, 2000, ANN STAT, V28, P1356; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; HOERL AE, 1970, TECHNOMETRICS, V12, P55; Efron B, 2004, ANN STAT, V32, P407; Bai ZD, 1999, STAT SINICA, V9, P611; DONOHO DL, 2004, STABLE RECOVERY SPAR; LENG C, 2004, IN PRESS STAT SINICA; Meinshausen N., 2006, ANN STAT, V34; MEINSHAUSEN N, 2005, LASSO RELAXATION; Osborne MR, 1998, COMP SCI STAT, V30, P44; Osborne MR, 2000, J COMPUT GRAPH STAT, V9, P319, DOI 10.2307/1390657; ROSSET S, 2004, NIPS; Zhao P., 2004, TECHNICAL REPORT; ZOU H, 2004, UNPUB DEGREES FREEDO	14	490	498	4	23	MICROTOME PUBLISHING	BROOKLINE	31 GIBBS STREET, BROOKLINE, MA 02446 USA	1532-4435			J MACH LEARN RES	J. Mach. Learn. Res.	NOV	2006	7						2541	2563				23	Automation & Control Systems; Computer Science, Artificial Intelligence	Automation & Control Systems; Computer Science	152VS	WOS:000245390700010		
J	Li, LX; Nachtsheim, CJ				Li, Lexin; Nachtsheim, Christopher J.			Sparse sliced inverse regression	TECHNOMETRICS			English	Article						lasso; regression shrinkage; sliced inverse regression; sufficient dimension reduction	DIMENSION REDUCTION; VARIABLE SELECTION; MODEL; LASSO	Sliced inverse regression (SIR) is an innovative and effective method for dimension reduction and data visualization of high-dimensional problems. It replaces the original variables with low-dimensional linear combinations of predictors without any loss of regression information and without the need to prespecify a model or an error distribution. However, it suffers from the fact that each SIR component is a linear combination of all the original predictors; thus, it is often difficult to interpret the extracted components. By representing SIR as a regression-type optimization problem, we propose in this article a new method, called sparse SIR, that combines the shrinkage idea of the lasso with SIR to produce both accurate and sparse solutions. The efficacy of the proposed method is verified by simulation, and a real data example is given.	N Carolina State Univ, Dept Stat, Raleigh, NC 27695 USA; Univ Minnesota, Dept Operat Res & Management Sci, Minneapolis, MN 55455 USA	Li, LX (reprint author), N Carolina State Univ, Dept Stat, Raleigh, NC 27695 USA.	li@stat.ncsu.edu; CNachtsheim@csom.umn.edu					Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; [Anonymous], 2004, ANN STAT, V32, P1061; Efron B, 2004, ANN STAT, V32, P407; ARGON Y, 1996, 9635438 GREMAQ; BECKER C, 2002, 132003 U DORTM; Cai TX, 2002, J AM STAT ASSOC, V97, P1099, DOI 10.1198/06214502388618915; Chen CH, 1998, STAT SINICA, V8, P289; Chiaromonte F, 2002, MATH BIOSCI, V176, P123, DOI 10.1016/S0025-5564(01)00106-7; CHIPMAN HA, 2001, IN PRESS J APPL STAT; CHU MT, 2001, J COMPUTATIONAL GRAP, V10, P1; COOK R. D., 1998, REGRESSION GRAPHICS; COOK RD, 1994, J AM STAT ASSOC, V89, P177, DOI 10.2307/2291214; COOK RD, 1994, J AM STAT ASSOC, V89, P592, DOI 10.2307/2290862; Cook RD, 2003, J AM STAT ASSOC, V98, P925, DOI 10.1198/016214503000000864; COOK RD, 1991, J AM STAT ASSOC, V86, P328, DOI 10.2307/2290564; Cook RD, 1996, J AM STAT ASSOC, V91, P983, DOI 10.2307/2291717; EATON ML, 1986, J MULTIVARIATE ANAL, V20, P272, DOI 10.1016/0047-259X(86)90083-7; Fung WK, 2002, STAT SINICA, V12, P1093; HALL P, 1993, ANN STAT, V21, P867, DOI 10.1214/aos/1176349155; Hastie T., 2001, ELEMENTS STAT LEARNI; Henderson PW, 1998, J MARKETING, V62, P14, DOI 10.2307/1252158; Kiers HAL, 2002, COMPUT STAT DATA AN, V41, P157, DOI 10.1016/S0167-9473(02)00142-1; LI KC, 1992, J AM STAT ASSOC, V87, P1025, DOI 10.2307/2290640; Li KC, 1999, ANN STAT, V27, P1; LI KC, 1991, J AM STAT ASSOC, V86, P316, DOI 10.2307/2290563; LI L, 2003, THESIS U MINNESOTA; Li LX, 2004, BIOINFORMATICS, V20, P3406, DOI 10.1093/bioinformatics/bth415; Li LX, 2005, J ROY STAT SOC B, V67, P285, DOI 10.1111/j.1467-9868.2005.00502.x; Li LX, 2004, COMPUT STAT DATA AN, V47, P175, DOI 10.1016/j.csda.2003.10.017; Naik PA, 2000, J MARKETING RES, V37, P88, DOI 10.1509/jmkr.37.1.88.18715; Tibshirani R, 1997, STAT MED, V16, P385, DOI 10.1002/(SICI)1097-0258(19970228)16:4<385::AID-SIM380>3.0.CO;2-3; Zou H., 2004, SPARSE PRINCIPAL COM	33	22	23	0	1	AMER STATISTICAL ASSOC	ALEXANDRIA	1429 DUKE ST, ALEXANDRIA, VA 22314 USA	0040-1706			TECHNOMETRICS	Technometrics	NOV	2006	48	4					503	510		10.1198/004017006000000129		8	Statistics & Probability	Mathematics	107ES	WOS:000242154100005		
J	Rhee, SY; Taylor, J; Wadhera, G; Ben-Hur, A; Brutlag, DL; Shafer, RW				Rhee, Soo-Yon; Taylor, Jonathan; Wadhera, Gauhar; Ben-Hur, Asa; Brutlag, Douglas L.; Shafer, Robert W.			Genotypic predictors of human immunodeficiency virus type 1 drug resistance	PROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES OF THE UNITED STATES OF AMERICA			English	Article						antiviral therapy; HIV; linear regression; machine learning	REVERSE-TRANSCRIPTASE; IN-VITRO; SUSCEPTIBILITY; PROTEASE; SENSITIVITY; HYPERSUSCEPTIBILITY; REGRESSION; MUTATIONS; PHENOTYPE	Understanding the genetic basis of HIV-1 drug resistance is essential to developing new antiretroviral drugs and optimizing the use of existing drugs. This understanding, however, is hampered by the large numbers of mutation patterns associated with cross-resistance within each antiretroviral drug class. We used five statistical learning methods (decision trees, neural networks, support vector regression, least-squares regression, and least angle regression) to relate HIV-1 protease and reverse transcriptase mutations to in vitro susceptibility to 16 antiretroviral drugs. Learning methods were trained and tested on a public data set of genotype-phenotype correlations by 5-fold cross-validation. For each learning method, four mutation sets were used as input features: a complete set of all mutations in >= 2 sequences in the data set, the 30 most common data set mutations, an expert panel mutation set, and a set of nonpolymorphic treatment-selected mutations from a public database linking protease and reverse transcriptase sequences to antiretroviral drug exposure. The nonpolymorphic treatment-selected mutations led to the best predictions: 80.1% accuracy at classifying sequences as susceptible, low/intermediate resistant, or highly resistant. Least angle regression predicted susceptibility significantly better than other methods when using the complete set of mutations. The three regression methods provided consistent estimates of the quantitative effect of mutations on drug susceptibility, identifying nearly all previously reported genotype-phenotype associations an providing strong statistical support for many new associations. Mutation regression coefficients showed that, within a drug class, cross-resistance patterns differ for different mutation subsets and that cross-resistance has been underestimated.	Stanford Univ, Div Infect Dis, Dept Med, Stanford, CA 94305 USA; Stanford Univ, Div Infect Dis, Dept Stat, Stanford, CA 94305 USA; Stanford Univ, Div Infect Dis, Dept Biochem, Stanford, CA 94305 USA	Shafer, RW (reprint author), Stanford Univ, Div Infect Dis, Dept Med, Room S-169, Stanford, CA 94305 USA.	rshafer@stanford.edu	Rhee, Soo-Yon/L-4597-2013				Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Parkin NT, 2004, ANTIMICROB AGENTS CH, V48, P437, DOI 10.1128/AAC.48.2.437-443.2004; Petropoulos CJ, 2000, ANTIMICROB AGENTS CH, V44, P920, DOI 10.1128/AAC.44.4.920-928.2000; Efron B, 2004, ANN STAT, V32, P407; Beerenwinkel N, 2002, P NATL ACAD SCI USA, V99, P8271, DOI 10.1073/pnas.112177799; Beerenwinkel N, 2003, NUCLEIC ACIDS RES, V31, P3850, DOI 10.1093/nar/gkg575; Johnson Victoria A, 2005, Top HIV Med, V13, P51; LARDER BA, 1994, J GEN VIROL, V75, P951, DOI 10.1099/0022-1317-75-5-951; Maguire MF, 2002, J VIROL, V76, P7398, DOI 10.1128/JVI.76.15.7398-7406.2002; Martinez-Picado J, 2005, J VIROL, V79, P5907, DOI 10.1128/JVI.79.10.5907-5913.2005; Parikh UM, 2005, ANTIMICROB AGENTS CH, V49, P1139, DOI 10.1128/AAC.49.3.1139-1144.2005; Parkin N., 2003, ANTIVIR THER, V8, pS34; Quinlan J., 1993, C4 5 PROGRAMS MACHIN; Rhee SY, 2005, J INFECT DIS, V192, P456, DOI 10.1086/431601; Rhee SY, 2003, NUCLEIC ACIDS RES, V31, P298, DOI 10.1093/nar/gkg100; Sevin AD, 2000, J INFECT DIS, V182, P59, DOI 10.1086/315673; Shulman NS, 2004, AIDS, V18, P1781, DOI 10.1097/00002030-200409030-00006; SMOLA A, 2002, LEARNING KERNELS; STCLAIR MH, 1991, SCIENCE, V253, P1557, DOI 10.1126/science.1716788; Wang DC, 2003, J INFECT DIS, V188, P653, DOI 10.1086/377453; Wang K, 2004, ANTIVIR THER, V9, P343; Zhang J, 2005, JAIDS-J ACQ IMM DEF, V38, P439, DOI 10.1097/01.qai.0000147526.64863.53	22	89	97	1	2	NATL ACAD SCIENCES	WASHINGTON	2101 CONSTITUTION AVE NW, WASHINGTON, DC 20418 USA	0027-8424			P NATL ACAD SCI USA	Proc. Natl. Acad. Sci. U. S. A.	NOV 14	2006	103	46					17355	17360		10.1073/pnas.0607274103		6	Multidisciplinary Sciences	Science & Technology - Other Topics	108OP	WOS:000242249400053	17065321	
J	ter Braak, CJF				ter Braak, Cajo J. F.			Bayesian sigmoid shrinkage with improper variance priors and an application to wavelet denoising	COMPUTATIONAL STATISTICS & DATA ANALYSIS			English	Article						Bayesian model; improper prior; shrinkage rule; wavelet denoising	ENTIRE GENOME; REGRESSION; SELECTION; PREDICTION; MARKERS; MODEL	The normal Bayesian linear model is extended by assigning a flat prior to the delta th power of the variance components of the regression coefficients (0 < delta <= 1/2) in order to improve prediction accuracy. In the case of orthonormal regressors, easy-to-compute analytic expressions are derived for the posterior distribution of the shrinkage and regression coefficients. The expected shrinkage is a sigmoid function of the squared value of the least-squares estimate divided by its standard error. This gives a small amount of shrinkage for large values and, provided delta is small, heavy shrinkage for small values. The limit behavior for both small and large values approaches that of the ideal coordinatewise shrinker in terms of the expected squared error of prediction, when delta is close to 0. In a simulation study of wavelet denoising, the proposed Bayesian shrinkage model yielded a lower mean squared error than soft thresholding (lasso), and was competitive with two recent wavelet shrinkage methods based on mixture prior distributions. (c) 2006 Elsevier B.V. All rights reserved.	Univ Wageningen & Res Ctr, NL-6700 AC Wageningen, Netherlands	ter Braak, CJF (reprint author), Univ Wageningen & Res Ctr, NL-6700 AC Wageningen, Netherlands.	cajo.terbraak@wur.nl	ter Braak, Cajo/G-7006-2011	ter Braak, Cajo/0000-0002-0414-8745			Abramovich F, 1998, J ROY STAT SOC B, V60, P725, DOI 10.1111/1467-9868.00151; Abramovich F, 2004, SCAND J STAT, V31, P217, DOI 10.1111/j.1467-9469.2004.02-087.x; Abramowitz M, 1972, HDB MATH FUNCTIONAL; BREIMAN L, 1995, TECHNOMETRICS, V37, P373, DOI 10.2307/1269730; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Johnstone IM, 2005, ANN STAT, V33, P1700, DOI 10.1214/009053605000000345; HOERL AE, 1970, TECHNOMETRICS, V12, P55; EFRON B, 1973, J AM STAT ASSOC, V68, P117, DOI 10.2307/2284155; Brown PJ, 2002, J R STAT SOC B, V64, P519, DOI 10.1111/1467-9868.00348; Semadeni C, 2004, BIOMETRIKA, V91, P497, DOI 10.1093/biomet/91.2.497; DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425; Cai TT, 1999, ANN STAT, V27, P898, DOI 10.1214/aos/1018031262; COPAS JB, 1983, J R STAT SOC B, V45, P311; Box GEP, 1973, BAYESIAN INFERENCE S; Breiman L, 1997, J ROY STAT SOC B MET, V59, P3, DOI 10.1111/1467-9868.00054; Clyde M, 1998, BIOMETRIKA, V85, P391, DOI 10.1093/biomet/85.2.391; CUTILLO L, 2005, 142005 GEORG I TECHN; De Canditiis D, 2004, J COMPUT GRAPH STAT, V13, P383, DOI 10.1198/1061860043461; Donoho DL, 1995, J AM STAT ASSOC, V90, P1200, DOI 10.2307/2291512; Efron B, 2004, J AM STAT ASSOC, V99, P619, DOI 10.1198/016214504000000692; Gelfand AE, 1999, J AM STAT ASSOC, V94, P247, DOI 10.2307/2669699; HARDY DW, 2000, WORKPLACE NOTEBOOK D; Hastie T., 2001, DATA MINING INFERENC; HOLMES CC, 1999, BAYESIAN STAT, V6, P769; JANSEN M, 2001, NOISE REDUCTION WAVE, V61; JOHNSTONE IM, 2000, EBAYESTHRESH R S PLU; Johnstone IM, 2004, ANN STAT, V32, P1594, DOI 10.1214/009053604000000030; O'Hagan A., 1994, BAYESIAN INFERENCE B, V2B; Ogden R. T., 1997, ESSENTIAL WAVELETS S; ter Braak CJF, 2005, GENETICS, V170, P1435, DOI 10.1534/genetics.105.040469; Xu SZ, 2003, GENETICS, V163, P789	31	11	11	0	1	ELSEVIER SCIENCE BV	AMSTERDAM	PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS	0167-9473			COMPUT STAT DATA AN	Comput. Stat. Data Anal.	NOV 15	2006	51	2					1232	1242		10.1016/j.csda.2006.06.011		11	Computer Science, Interdisciplinary Applications; Statistics & Probability	Computer Science; Mathematics	111VU	WOS:000242484200059		
J	Mansson, RA; Welsh, AH; Fey, N; Orpen, AG				Mansson, Ralph A.; Welsh, Alan H.; Fey, Natalie; Orpen, A. Guy			Statistical modeling of a ligand knowledge base	JOURNAL OF CHEMICAL INFORMATION AND MODELING			English	Article							CAMBRIDGE STRUCTURAL DATABASE; CONFORMATIONALLY FLEXIBLE LIGANDS; METAL-CARBONYL CLUSTERS; SIGMA-STAR ORBITALS; PROTEIN DATA-BANK; PHOSPHORUS LIGANDS; HOMOGENEOUS CATALYSIS; ASSOCIATIVE REACTIONS; MOLECULAR-GEOMETRY; CRYSTAL-STRUCTURES	A range of different statistical models has been fitted to experimental data for the Tolman electronic parameter (TEP) based on a large set of calculated descriptors in a prototype ligand knowledge base (LKB) of phosphorus(III) donor ligands. The models have been fitted by ordinary least squares using subsets of descriptors, principal component regression, and partial least squares which use variables derived from the complete set of descriptors, least angle regression, and the least absolute shrinkage and selection operator. None of these methods is robust against outliers, so we also applied a robust estimation procedure to the linear regression model. Criteria for model evaluation and comparison have been discussed, highlighting the importance of resampling methods for assessing the robustness of models and the scope for making predictions in chemically intuitive models. For the ligands covered by this LKB, ordinary least squares models of descriptor subsets provide a good representation of the data, while partial least squares, principal component regression, and least angle regression models are less suitable for our dual aims of prediction and interpretation. A linear regression model with robustly fitted parameters achieves the best model performance over all classes of models fitted to TEP data, and the weightings assigned to ligands during the robust estimation procedure are chemically intuitive. The increased model complexity when compared to the ordinary least squares linear model is justified by the reduced influence of individual ligands on the model parameters and predictions of new ligands. Robust linear regression models therefore represent the best compromise for achieving statistical robustness in simple, chemically meaningful models.	Univ Southampton, Sch Math, Southampton SO17 1BJ, Hants, England; Univ Bristol, Sch Chem, Bristol BS8 1TS, Avon, England	Mansson, RA (reprint author), Univ Southampton, Sch Math, Southampton SO17 1BJ, Hants, England.	ram@soton.ac.uk; Natalie.Fey@Bristol.ac.uk	Fey, Natalie/F-6177-2014	Fey, Natalie/0000-0003-0609-475X			PERDEW JP, 1986, PHYS REV B, V33, P8822, DOI 10.1103/PhysRevB.33.8822; BROWN TL, 1993, COORDIN CHEM REV, V128, P89, DOI 10.1016/0010-8545(93)80025-Z; BROWN TL, 1992, INORG CHEM, V31, P1286, DOI 10.1021/ic00033a029; Allen FH, 2004, CHEM SOC REV, V33, P463, DOI 10.1039/b309040j; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Berman HM, 2002, ACTA CRYSTALLOGR D, V58, P899, DOI 10.1107/S0907444902003451; Hawkins DM, 2004, J CHEM INF COMP SCI, V44, P1, DOI 10.1021/ci0342472; Dorta R, 2005, J AM CHEM SOC, V127, P2485, DOI 10.1021/ja0438821; Burello E, 2005, ADV SYNTH CATAL, V347, P803, DOI 10.1002/adsc.200404363; Berman HM, 2000, NUCLEIC ACIDS RES, V28, P235, DOI 10.1093/nar/28.1.235; PERDEW JP, 1981, PHYS REV B, V23, P5048, DOI 10.1103/PhysRevB.23.5048; TOLMAN CA, 1977, CHEM REV, V77, P313, DOI 10.1021/cr60307a002; BECKE AD, 1988, PHYS REV A, V38, P3098, DOI 10.1103/PhysRevA.38.3098; SHAO J, 1993, J AM STAT ASSOC, V88, P486, DOI 10.2307/2290328; Bruno IJ, 2004, J CHEM INF COMP SCI, V44, P2133, DOI 10.1021/ci049780b; Allen FH, 2002, ACTA CRYSTALLOGR B, V58, P380, DOI 10.1107/S0108768102003890; LEVER ABP, 1990, INORG CHEM, V29, P1271, DOI 10.1021/ic00331a030; Perrin L, 2001, INORG CHEM, V40, P5806, DOI 10.1021/ic0105258; Bruno IJ, 2002, ACTA CRYSTALLOGR B, V58, P389, DOI 10.1107/S0108768102003324; Baber RA, 2005, DALTON T, P659, DOI 10.1039/b416525j; Babij C, 2004, J PHYS ORG CHEM, V17, P162, DOI 10.1002/poc.708; BARTIK T, 1984, J ORGANOMET CHEM, V272, P29, DOI 10.1016/0022-328X(84)80440-4; Bjorsvik HR, 1997, ACTA CHEM SCAND, V51, P733, DOI 10.3891/acta.chem.scand.51-0733; BODNER GM, 1980, INORG CHEM, V19, P1951, DOI 10.1021/ic50209a025; BRUNO IJ, 1997, COMPUT AIDED MOL DES, V11, P525; Bubel RJ, 2000, J COMPUT CHEM, V21, P239, DOI 10.1002/(SICI)1096-987X(200002)21:3<239::AID-JCC7>3.0.CO;2-0; Bunten KA, 2003, ORGANOMETALLICS, V22, P3448, DOI 10.1021/om030071h; Burello E, 2003, ADV SYNTH CATAL, V345, P1334, DOI 10.1002/adsc.200303141; Burello E, 2004, ADV SYNTH CATAL, V346, P1844, DOI 10.1002/adsc.200404170; CHEN LZ, 1995, COORDIN CHEM REV, V143, P265, DOI 10.1016/0010-8545(94)07003-3; Cooney KD, 2003, J AM CHEM SOC, V125, P4318, DOI 10.1021/ja0212541; DIAS PB, 1994, COORDIN CHEM REV, V135, P737, DOI 10.1016/0010-8545(94)80082-0; Drago RS, 1996, J AM CHEM SOC, V118, P2654, DOI 10.1021/ja953581e; Draper N. R., 1998, APPL REGRESSION ANAL; ERIKSSON L, 2003, HDB CHEMOINFORMATICS, V4, P1134; Fey N, 2006, J CHEM INF MODEL, V46, P912, DOI 10.1021/ci0504768; Fey N, 2006, CHEM-EUR J, V12, P291, DOI 10.1002/chem.200500891; FIELDER SS, 1995, J AM CHEM SOC, V117, P6990, DOI 10.1021/ja00131a022; Filzmoser P., 2002, CLASSIFICATION CLUST, P227; Frenking G, 2002, ORGANOMETALLICS, V21, P2921, DOI 10.1021/om020311d; Gillespie A., 2002, INTERNET ELECT J MOL, V1, P242; Hadi AS, 1998, AM STAT, V52, P15, DOI 10.2307/2685559; Harris SE, 2005, J CHEM INF MODEL, V45, P1727, DOI 10.1021/ci0500785; Hastie T., 2001, ELEMENTS STAT LEARNI; HENDERSON WA, 1960, J AM CHEM SOC, V82, P5791, DOI 10.1021/ja01507a008; Hinkley D. V., 1997, BOOTSTRAP METHODS TH; Howard ST, 1996, INORG CHEM, V35, P5805, DOI 10.1021/ic951553r; HOWARD ST, 1995, J PHYS CHEM-US, V99, P9027, DOI 10.1021/j100022a013; Joerg S, 1998, ORGANOMETALLICS, V17, P589, DOI 10.1021/om970789v; Kuhl O, 2005, COORDIN CHEM REV, V249, P693, DOI 10.1016/j.ccr.2004.08.021; Landis CR, 2000, ORGANOMETALLICS, V19, P4878, DOI 10.1021/om000544+; Mansson RA, 2005, J CHEM INF MODEL, V45, P1791, DOI 10.1021/ci050056i; MARTIN R, DEV NATL E PRINTS AR; ORPEN AG, 1985, J CHEM SOC CHEM COMM, P1310, DOI 10.1039/c39850001310; Orpen AG, 2002, ACTA CRYSTALLOGR B, V58, P398, DOI 10.1107/S0108768102002446; ORPEN AG, 1990, ORGANOMETALLICS, V9, P1206, DOI 10.1021/om00118a048; PERDEW JP, 1986, PHYS REV B, V34, P7406, DOI 10.1103/PhysRevB.34.7406; *R DEV COR REAM, 2004, R LANG ENV STAT COMP; RAMSEY FL, 1986, AM STAT, V40, P323, DOI 10.2307/2684619; *SCHR INC, 2000, JAG VERS 4 0; SCHR LLC, 2002, JAG VERS 5 0; Senn HM, 2000, J MOL STRUC-THEOCHEM, V506, P233, DOI 10.1016/S0166-1280(00)00415-2; Serron S, 1996, ORGANOMETALLICS, V15, P4301, DOI 10.1021/om960335i; Shao J, 1996, J AM STAT ASSOC, V91, P655, DOI 10.2307/2291661; SLATER JC, 1974, TUANTUM THEORY MOL S, V4; Smith JM, 2000, ORGANOMETALLICS, V19, P5273, DOI 10.1021/om000347a; Smith JM, 2001, ORGANOMETALLICS, V20, P1210, DOI 10.1021/om000687w; Steinmetz WE, 1996, QUANT STRUCT-ACT REL, V15, P1, DOI 10.1002/qsar.19960150102; Wilson MR, 2002, ORGANOMETALLICS, V21, P2758, DOI 10.1021/om011035q; Wisnowski JW, 2003, COMPUT STAT DATA AN, V43, P341, DOI 10.1016/S0167-9473(02)00235-9; YOHAI VJ, 1987, ANN STAT, V15, P642, DOI 10.1214/aos/1176350366	71	13	13	2	6	AMER CHEMICAL SOC	WASHINGTON	1155 16TH ST, NW, WASHINGTON, DC 20036 USA	1549-9596			J CHEM INF MODEL	J. Chem Inf. Model.	NOV 27	2006	46	6					2591	2600		10.1021/ci600212t		10	Chemistry, Medicinal; Chemistry, Multidisciplinary; Computer Science, Information Systems; Computer Science, Interdisciplinary Applications	Pharmacology & Pharmacy; Chemistry; Computer Science	109HF	WOS:000242298100037	17125199	
J	Vert, JP; Foveau, N; Lajaunie, C; Vandenbrouck, Y				Vert, Jean-Philippe; Foveau, Nicolas; Lajaunie, Christian; Vandenbrouck, Yves			An accurate and interpretable model for siRNA efficacy prediction	BMC BIOINFORMATICS			English	Article							DOUBLE-STRANDED-RNA; SMALL INTERFERING RNAS; ENZYME COMPLEX; THERMODYNAMIC PARAMETERS; SECONDARY STRUCTURE; MAMMALIAN-CELLS; GENE-EXPRESSION; MESSENGER-RNA; TARGET; DESIGN	Background: The use of exogenous small interfering RNAs ( siRNAs) for gene silencing has quickly become a widespread molecular tool providing a powerful means for gene functional study and new drug target identification. Although considerable progress has been made recently in understanding how the RNAi pathway mediates gene silencing, the design of potent siRNAs remains challenging. Results: We propose a simple linear model combining basic features of siRNA sequences for siRNA efficacy prediction. Trained and tested on a large dataset of siRNA sequences made recently available, it performs as well as more complex state-of-the-art models in terms of potency prediction accuracy, with the advantage of being directly interpretable. The analysis of this linear model allows us to detect and quantify the effect of nucleotide preferences at particular positions, including previously known and new observations. We also detect and quantify a strong propensity of potent siRNAs to contain short asymmetric motifs in their sequence, and show that, surprisingly, these motifs alone contain at least as much relevant information for potency prediction as the nucleotide preferences for particular positions. Conclusion: The model proposed for prediction of siRNA potency is as accurate as a state-of-the-art nonlinear model and is easily interpretable in terms of biological features. It is freely available on the web at http://cbio.ensmp.fr/dsir.	Ecole Mines Paris, Ctr Computat Biol, F-77300 Fontainebleau, France; CEA Grenoble, Lab Biol Informat Math, Dept Reponse & Dynam Cellulaire, F-38054 Grenoble, France	Vert, JP (reprint author), Ecole Mines Paris, Ctr Computat Biol, 35 Rue St Honore, F-77300 Fontainebleau, France.	Jean-Philippe.Vert@ensmp.fr; Nicolas.Foveau@cea.fr; Christian.Lajaunie@ensmp.fr; Yves.Vandenbrouck@cea.fr					Amarzguioui M, 2004, BIOCHEM BIOPH RES CO, V316, P1050, DOI 10.1016/j.bbrc.2004.02.157; Pai SI, 2006, GENE THER, V13, P464, DOI 10.1038/sj.gt.3302694; Mathews DH, 1999, J MOL BIOL, V288, P911, DOI 10.1006/jmbi.1999.2700; Haley B, 2004, NAT STRUCT MOL BIOL, V11, P599, DOI 10.1038/nsmb780; Harborth J, 2003, ANTISENSE NUCLEIC A, V13, P83, DOI 10.1089/108729003321629638; Elbashir SM, 2001, NATURE, V411, P494, DOI 10.1038/35078107; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Hutvagner G, 2002, SCIENCE, V297, P2056, DOI 10.1126/science.1073827; Fire A, 1998, NATURE, V391, P806, DOI 10.1038/35888; Schwarz DS, 2003, CELL, V115, P199, DOI 10.1016/S0092-8674(03)00759-1; Schubert S, 2005, J MOL BIOL, V348, P883, DOI 10.1016/j.jmb.2005.03.011; Holen T, 2002, NUCLEIC ACIDS RES, V30, P1757, DOI 10.1093/nar/30.8.1757; Baulcombe DC, 1996, PLANT MOL BIOL, V32, P79, DOI 10.1007/BF00039378; Surabhi RM, 2002, J VIROL, V76, P12963, DOI 10.1128/JVI.76.24.12963-12973.2002; Hannon GJ, 2004, NATURE, V431, P371, DOI 10.1038/nature02870; Meister G, 2004, NATURE, V431, P343, DOI 10.1038/nature02873; Efron B, 2004, ANN STAT, V32, P407; Jackson AL, 2003, NAT BIOTECHNOL, V21, P635, DOI 10.1038/nbt831; Zamore PD, 2000, CELL, V101, P25, DOI 10.1016/S0092-8674(00)80620-0; Boese Q, 2005, METHOD ENZYMOL, V392, P73; Caplen NJ, 2001, P NATL ACAD SCI USA, V98, P9742, DOI 10.1073/pnas.171251798; Cogoni C, 1996, EMBO J, V15, P3153; DING Y, 2004, NUCLEIC ACIDS RES, pW135; FREIER SM, 1986, P NATL ACAD SCI USA, V83, P9373, DOI 10.1073/pnas.83.24.9373; Heale BSE, 2005, NUCLEIC ACIDS RES, V33, DOI 10.1093/nar/gni026; Huesken D, 2005, NAT BIOTECHNOL, V23, P995, DOI 10.1038/nbt1118; Huppi K, 2005, MOL CELL, V17, P1, DOI 10.1016/j.molcel.2004.12.017; JORGENSEN RA, 2003, RNAI GUDIE GENE SILE; Khvorova A, 2003, CELL, V115, P209, DOI 10.1016/S0092-8674(03)00801-8; Ma JB, 2004, NATURE, V429, P318, DOI 10.1038/nature02519; McManus MT, 2002, NAT REV GENET, V3, P737, DOI 10.1038/nrg908; Overhoff M, 2005, J MOL BIOL, V348, P871, DOI 10.1016/j.jmb.2005.03.012; REN Y, 2006, BIOINFORMATICS; Reynolds A, 2004, NAT BIOTECHNOL, V22, P326, DOI 10.1038/nbt936; Saetrom P, 2004, BIOINFORMATICS, V20, P3055, DOI 10.1093/bioinformatics/bth364; Saetrom P, 2004, BIOCHEM BIOPH RES CO, V321, P247, DOI 10.1016/j.bbrc.2004.06.116; Semizarov D, 2003, P NATL ACAD SCI USA, V100, P6347, DOI 10.1073/pnas.1131959100; Shabalina SA, 2006, BMC BIOINFORMATICS, V7, DOI 10.1186/1471-2105-7-65; Teramoto R, 2005, FEBS LETT, V579, P2878, DOI 10.1016/j.febslet.2005.04.045; TRUSS M, 2005, NUCLEIC ACIDS RES, pD108; Tuschl T, 1999, GENE DEV, V13, P3191, DOI 10.1101/gad.13.24.3191; Ui-Tei K, 2004, NUCLEIC ACIDS RES, V32, P936, DOI 10.1093/nar/gkh247; Vickers TA, 2003, J BIOL CHEM, V278, P7108, DOI 10.1074/jbc.M210326200; Xia HB, 2004, NAT MED, V10, P816, DOI 10.1038/nm1076; Xia TB, 1998, BIOCHEMISTRY-US, V37, P14719, DOI 10.1021/bi9809425; Yiu SM, 2005, BIOINFORMATICS, V21, P144, DOI 10.1093/bioinformatics/bth498; ZUKER M, 1989, SCIENCE, V244, P48, DOI 10.1126/science.2468181	47	91	95	2	5	BIOMED CENTRAL LTD	LONDON	MIDDLESEX HOUSE, 34-42 CLEVELAND ST, LONDON W1T 4LB, ENGLAND	1471-2105			BMC BIOINFORMATICS	BMC Bioinformatics	NOV 30	2006	7								520	10.1186/1471-2105-7-520		17	Biochemical Research Methods; Biotechnology & Applied Microbiology; Mathematical & Computational Biology	Biochemistry & Molecular Biology; Biotechnology & Applied Microbiology; Mathematical & Computational Biology	118OI	WOS:000242951200001	17137497	
J	Tarigan, B; Van De Geer, SA				Tarigan, Bernadetta; Van De Geer, Sara A.			Classifiers of support vector machine type with l(1) complexity regularization	BERNOULLI			English	Article						binary classification; hinge loss; margin; oracle inequality; penalized classification rule; sparsity	RISK MINIMIZATION; CLASSIFICATION; PENALTIES; MARGIN	We study the binary classification problem with hinge loss. We consider classifiers that are linear combinations of base functions. Instead of an l(2) penalty, which is used by the support vector machine, we put an l(1) penalty on the coefficients. Under certain conditions on the base functions, hinge loss with this complexity penalty is shown to lead to an oracle inequality involving both model complexity and margin.	ETH, Seminar Stat, CH-8092 Zurich, Switzerland	Tarigan, B (reprint author), ETH, Seminar Stat, LEO D11, CH-8092 Zurich, Switzerland.	tarigan@stat.math.ethz.ch; geer@stat.math.ethz.ch					Koltchinskii V, 2002, ANN STAT, V30, P1; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Tsybakov AB, 2004, ANN STAT, V32, P135; Lin Y, 2002, DATA MIN KNOWL DISC, V6, P259, DOI 10.1023/A:1015469627679; Donoho DL, 1999, ANN STAT, V27, P859, DOI 10.1214/aos/1018031261; DONOHO DL, 1995, IEEE T INFORM THEORY, V41, P613, DOI 10.1109/18.382009; AUDIBERT JY, 2004, PMA908; Bartlett PL, 2006, J AM STAT ASSOC, V101, P138, DOI 10.1198/016214505000000907; BOSER B, 1992, P 5 ANN C COMP LEARN, P142; Candes EJ, 2004, COMMUN PUR APPL MATH, V57, P219, DOI 10.1002/cpa.10116; Donoho D. L., 2004, MOST LARGE UNDERDETE; Hardy G. H., 1988, INEQUALITIES; Hastie T., 2001, ELEMENTS STAT LEARNI; Koltchinskii V, 2001, IEEE T INFORM THEORY, V47, P1902, DOI 10.1109/18.930926; KOLTCHINSKII V, 2006, IN PRESS ANN STAT, V34; LEDOUX M., 1991, PROBABILITY BANACH S; Ledoux M., 1996, ESAIM PROBAB STAT, V1, P63; Loubes J.-M., 2002, STAT NEERL, V56, P453; Lugosi G, 2004, ANN STAT, V32, P1679, DOI 10.1214/009053604000000463; Mammen E, 1999, ANN STAT, V27, P1808; Massart P, 2000, ANN PROBAB, V28, P863, DOI 10.1214/aop/1019160263; Scholkopf B., 2002, LEARNING KERNELS; Scott C, 2006, IEEE T INFORM THEORY, V52, P1335, DOI 10.1109/TIT.2006.871056; Shorack G.R., 1986, EMPIRICAL PROCESSES; STEINWART I, 2005, 048796 LAUR LOS AL N; TARIGAN B, 2004, 200414 MI U LEID; Tsybakov AB, 2005, ANN STAT, V33, P1203, DOI 10.1214/009053604000001066; van de Geer S, 2000, EMPIRICAL PROCESSES; van de Geer SA, 2003, RECENT ADVANCES AND TRENDS IN NONPARAMETRIC STATISTICS, P235, DOI 10.1016/B978-044451378-6/50016-8; Vapnik V., 1998, STAT LEARNING THEORY; Vapnik V.N., 1995, NATURE STAT LEARNING; Vayatis N, 2003, J MACHINE LEARNING R, V4, P861; Zhang T, 2004, ANN STAT, V32, P56	33	25	25	1	2	INT STATISTICAL INST	VOORBURG	428 PRINSES BEATRIXLAAN, 2270 AZ VOORBURG, NETHERLANDS	1350-7265			BERNOULLI	Bernoulli	DEC	2006	12	6					1045	1076		10.3150/bj/1165269150		32	Statistics & Probability	Mathematics	117RN	WOS:000242890700006		
J	Houseman, EA; Coull, BA; Betensky, RA				Houseman, E. Andres; Coull, Brent A.; Betensky, Rebecca A.			Feature-specific penalized latent class analysis for genomic data	BIOMETRICS			English	Article						constrained estimation; LASSO; loss of heterozygosity; mixture models; penalized likelihood; ridge regression	INFORMATION CRITERION; MODEL SELECTION; IDENTIFICATION; LIKELIHOOD; REGRESSION; TUMORS; LASSO	Genomic data are often characterized by a moderate to large number of categorical variables observed for relatively few subjects. Some of the variables may be missing or noninformative. An example of such data is loss of heterozygosity (LOH), a dichotomous variable, observed on a moderate number of genetic markers. We first consider a latent class model where, conditional on unobserved membership in one of k classes, the variables are independent with probabilities determined by a regression model of low dimension q. Using a family of penalties including the ridge and LASSO, we extend this model to address higher-dimensional problems. Finally, we present an orthogonal map that transforms marker space to a space of "features" for which the constrained model has better predictive power. We demonstrate these methods on LOH data collected at 19 markers from 93 brain tumor patients. For this data set, the existing unpenalized latent class methodology does not produce estimates. Additionally, we show that posterior classes obtained from this method are associated with survival for these patients.	Harvard Univ, Sch Publ Hlth, Dept Biostat, Boston, MA 02115 USA	Houseman, EA (reprint author), Harvard Univ, Sch Publ Hlth, Dept Biostat, 655 Huntington Ave, Boston, MA 02115 USA.	ahousema@hsph.harvard.edu					AGRESTI A, 1993, BIOMETRICS, V49, P131, DOI 10.2307/2532608; AKAIKE H, 1974, IEEE T AUTOMAT CONTR, VAC19, P716, DOI 10.1109/TAC.1974.1100705; Cairncross JG, 1998, J NATL CANCER I, V90, P1473, DOI 10.1093/jnci/90.19.1473; Bandeen-Roche K, 1997, J AM STAT ASSOC, V92, P1375, DOI 10.2307/2965407; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1; NELDER JA, 1965, COMPUT J, V7, P308; Qu YS, 1996, BIOMETRICS, V52, P797, DOI 10.2307/2533043; Ino Y, 2001, CLIN CANCER RES, V7, P839; Meulders M, 2002, J CLASSIF, V19, P277, DOI 10.1007/s00357-001-0046-6; BOZDOGAN H, 1987, PSYCHOMETRIKA, V52, P345, DOI 10.1007/BF02294361; BENJAMINI Y, 1995, J ROY STAT SOC B MET, V57, P289; SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136; HURVICH CM, 1995, BIOMETRICS, V51, P1077, DOI 10.2307/2533006; GOODMAN LA, 1974, BIOMETRIKA, V61, P215, DOI 10.1093/biomet/61.2.215; Bartholomew D.J., 1987, LATENT VARIABLE MODE; CAWKWELL L, 1993, BRIT J CANCER, V67, P1262, DOI 10.1038/bjc.1993.236; Dong Z, 2004, BRIT J CANCER, V91, P1105, DOI 10.1038/sj.bjc.6602093; HANNAN EJ, 1979, J ROY STAT SOC B MET, V41, P190; Hastie T., 2001, ELEMENTS STAT LEARNI; Hastie TJ, 1990, GEN ADDITIVE MODELS; HOUSEMAN EA, 2005, HARVARD U BIOSTATIST; Lazarsfeld P. F., 1968, LATENT STRUCTURE ANA; LINDSAY B, 1991, J AM STAT ASSOC, V86, P96, DOI 10.2307/2289719; Morris JS, 2003, J AM STAT ASSOC, V98, P573, DOI 10.1198/016214503000000422; Ruppert D., 2003, SEMIPARAMETRIC REGRE; SUGIURA N, 1978, COMMUN STAT A-THEOR, V7, P13, DOI 10.1080/03610927808827599; Tibshirani R, 1997, STAT MED, V16, P385, DOI 10.1002/(SICI)1097-0258(19970228)16:4<385::AID-SIM380>3.0.CO;2-3; VONDAVIER M, 1997, METHODS PSYCHOL RES, V2; Wahba G., 1980, INT C APPR THEOR HON; Westfall P. H., 1993, RESAMPLING BASED MUL	31	14	14	3	4	BLACKWELL PUBLISHING	OXFORD	9600 GARSINGTON RD, OXFORD OX4 2DQ, OXON, ENGLAND	0006-341X			BIOMETRICS	Biometrics	DEC	2006	62	4					1062	1070		10.1111/j.1541-0420.2006.00566.x		9	Biology; Mathematical & Computational Biology; Statistics & Probability	Life Sciences & Biomedicine - Other Topics; Mathematical & Computational Biology; Mathematics	115ZH	WOS:000242771800012	17156280	
J	Tanck, MWT; Jukema, JW; Zwinderman, AH				Tanck, Michael W. T.; Jukema, J. Wouter; Zwinderman, Aeilko H.			Simultaneous estimation of gene-gene and gene-environment interactions for numerous loci using double penalized log-likelihood	GENETIC EPIDEMIOLOGY			English	Article						statistical method; polymorphisms; complex trait; interactions; genetic epidemiology	MULTIFACTOR-DIMENSIONALITY REDUCTION; CANDIDATE GENES; COMPLEX TRAITS; DISEASE; ASSOCIATION; REGRESSION; POLYMORPHISMS; SELECTION; MARKERS; RISK	Many common human diseases are considered to be caused by complex multifactorial processes. For these diseases, it is expected that numerous genetic and environmental factors and, possibly, their interactions play a role. Therefore, simultaneously analyzing the effects of numerous genes and environmental factors is a more realistic approach compared to single gene analyses, but the large number of genes and environmental factors pose a challenge, not in the least due to the limitations created by the tools available for analyzing such high-dimensional models. In the present manuscript we propose a method that is capable of identifying "true" interactions in a setting where the number of effects to be estimated is very large and can even surpass the number of observations. Basically, all possible (interaction) effects are entered in a double penalized model, where main effects are ridge penalized, whereas the interactions are subjected to a least absolute shrinkage and selection operator (lasso) penalty. Results from the simulations and real data show that the proposed method is capable of detecting interactions even with relative small effect sizes. Genet. Epidemiol. 30:645-651, 2006. (c) 2006 Wiley-Liss, Inc.	Acad Med Ctr, Dept Clin Epidemiol Biostat & Bioinformat, Amsterdam, Netherlands; Leiden Univ, Med Ctr, Dept Cardiol, Leiden, Netherlands; Interuniv Cardiol Inst Netherlands, Utrecht, Netherlands	Tanck, MWT (reprint author), Acad Med Ctr, Dept Clin Epidemiol Biostat & Bioinformat, Room J1B-214-1, Amsterdam, Netherlands.	m.w.tanck@amc.uva.nl					Ritchie MD, 2001, AM J HUM GENET, V69, P138, DOI 10.1086/321276; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; LANDER E, 1995, NAT GENET, V11, P241, DOI 10.1038/ng1195-241; Ritchie MD, 2003, GENET EPIDEMIOL, V24, P150, DOI 10.1002/gepi.10218; Zou H, 2005, J ROY STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x; Ritchie MD, 2003, BMC BIOINFORMATICS, V4, DOI 10.1186/1471-2105-4-28; Hahn LW, 2003, BIOINFORMATICS, V19, P376, DOI 10.1093/bioinformatics/btf869; Eilers PHC, 1996, STAT SCI, V11, P89, DOI 10.1214/ss/1038425655; Boer MP, 2002, GENETICS, V162, P951; Cheng S, 1999, GENOME RES, V9, P936, DOI 10.1101/gr.9.10.936; Hastie TJ, 1990, GEN ADDITIVE MODELS; HOERL AE, 1970, TECHNOMETRICS, V12, P69, DOI 10.2307/1267352; Hoh J, 2000, ANN HUM GENET, V64, P413, DOI 10.1046/j.1469-1809.2000.6450413.x; JUKEMA JW, 1995, CIRCULATION, V91, P2528; Tahri-Daizadeh N, 2003, GENOME RES, V13, P1952, DOI 10.1101/gr.1254203; Wille A, 2003, GENET EPIDEMIOL, V25, P350, DOI 10.1002/gepi.10263; Yamada Y, 2002, NEW ENGL J MED, V347, P1916, DOI 10.1056/NEJMoa021445; Yoon Y, 2003, CLIN CHEM LAB MED, V41, P529, DOI 10.1515/CCLM.2003.080	18	9	9	1	1	WILEY-LISS	HOBOKEN	DIV JOHN WILEY & SONS INC, 111 RIVER ST, HOBOKEN, NJ 07030 USA	0741-0395			GENET EPIDEMIOL	Genet. Epidemiol.	DEC	2006	30	8					645	651		10.1002/gepi.20176		7	Genetics & Heredity; Public, Environmental & Occupational Health	Genetics & Heredity; Public, Environmental & Occupational Health	110MQ	WOS:000242383400001	16917921	
J	Chen, YX; Bi, JB; Wang, JZ				Chen, Yixin; Bi, Jinbo; Wang, James Z.			MILES: Multiple-Instance Learning via Embedded instance Selection	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE			English	Article						Multiple-instance Learning; feature subset selection; 1-norm support vector machine; image categorization; object recognition; drug activity prediction	RETRIEVAL; PICTURES; FEATURES; REGIONS; SCALE	Multiple- instance problems arise from the situations where training class labels are attached to sets of samples (named bags), instead of individual samples within each bag (called instances). Most previous multiple- instance learning (MIL) algorithms are developed based on the assumption that a bag is positive if and only if at least one of its instances is positive. Although the assumption works well in a drug activity prediction problem, it is rather restrictive for other applications, especially those in the computer vision area. We propose a learning method, MILES (Multiple- Instance Learning via Embedded instance Selection), which converts the multiple-instance learning problem to a standard supervised learning problem that does not impose the assumption relating instance labels to bag labels. MILES maps each bag into a feature space defined by the instances in the training bags via an instance similarity measure. This feature mapping often provides a large number of redundant or irrelevant features. Hence, 1-norm SVM is applied to select important features as well as construct classifiers simultaneously. We have performed extensive experiments. In comparison with other methods, MILES demonstrates competitive classification accuracy, high computation efficiency, and robustness to labeling uncertainty.	Univ Mississippi, Dept Comp & Informat Sci, University, MS 38677 USA; Siemens Med Solut Inc, Comp Aided Diag & Therapy Solut, Malvern, PA 19355 USA; Penn State Univ, Dept Comp Sci & Engn, Coll Informat Sci & Technol, University Pk, PA 16802 USA	Chen, YX (reprint author), Univ Mississippi, Dept Comp & Informat Sci, 201 Weir Hall, University, MS 38677 USA.	ychen@cs.olemiss.edu; jinbo.bi@siemens.com; jwang@ist.psu.edu					Agarwal S., 2002, P EUR C COMP VIS, P113; Andrews S, 2004, ADV NEUR IN, V16, P65; Andrews S., 2003, ADV NEURAL INFORM PR, V15, P561; Mitra P, 2002, IEEE T PATTERN ANAL, V24, P301, DOI 10.1109/34.990133; Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Jain A, 1997, IEEE T PATTERN ANAL, V19, P153, DOI 10.1109/34.574797; Dietterich TG, 1997, ARTIF INTELL, V89, P31, DOI 10.1016/S0004-3702(96)00034-3; Chen YX, 2004, J MACH LEARN RES, V5, P913; Li J, 2003, IEEE T PATTERN ANAL, V25, P1075; Manjunath BS, 1996, IEEE T PATTERN ANAL, V18, P837, DOI 10.1109/34.531803; Zhu J, 2004, ADV NEUR IN, V16, P49; Yu L, 2004, J MACH LEARN RES, V5, P1205; Barnard K, 2003, J MACH LEARN RES, V3, P1107, DOI 10.1162/153244303322533214; Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2; Maron O, 1998, ADV NEUR IN, V10, P570; Wang JZ, 2001, IEEE T PATTERN ANAL, V23, P947, DOI 10.1109/34.955109; Zhang ML, 2004, NEURAL PROCESS LETT, V19, P1, DOI 10.1023/B:NEPL.0000016836.03614.9f; Tuytelaars T, 2004, INT J COMPUT VISION, V59, P61, DOI 10.1023/B:VISI.0000020671.28016.e8; Law MHC, 2004, IEEE T PATTERN ANAL, V26, P1154, DOI 10.1109/TPAMI.2004.71; Chen YX, 2002, IEEE T PATTERN ANAL, V24, P1252; Kohavi R, 1997, ARTIF INTELL, V97, P273, DOI 10.1016/S0004-3702(97)00043-X; Kwak N, 2002, IEEE T PATTERN ANAL, V24, P1667, DOI 10.1109/TPAMI.2002.1114861; Auer P, 1997, P 14 INT C MACH LEAR, P21; BARHILLEL A, 2005, P IEEE C COMP VIS PA, V1, P702; Bennett KP, 1999, ADVANCES IN KERNEL METHODS, P307; Blake C.L., 1998, UCI REPOSITORY MACHI; BLUMSTEIN T, 1998, GERONTOLOGY, V1, P29; Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.1023/A:1018054314350; Bressan M, 2003, IEEE T PATTERN ANAL, V25, P1312, DOI 10.1109/TPAMI.2003.1233904; BUCHANAN BG, 1978, PATTERN DIRECTED INF, P297; Csurka G, 2004, P ECCV WORKSH STAT L, P59; De Raedt L., 1998, LECT NOTES ARTIF INT, V1446, P1; Dorko G., 2003, P INT C COMP VIS, V1, P634; Fergus R., 2003, P IEEE C COMP VIS PA, V2; Gartner T., 2002, P 19 INT C MACH LEAR, P179; Iannarilli FJ, 2003, IEEE T PATTERN ANAL, V25, P779, DOI 10.1109/TPAMI.2003.1201827; *ILOG CPLEX DIV, 1999, ILOG CPLEX 6 5 REF M; Bi J., 2003, Journal of Machine Learning Research, V3, DOI 10.1162/153244303322753643; Kadir T., 2004, P EUR C COMP VIS, P404; Kadir T, 2001, INT J COMPUT VISION, V45, P83, DOI 10.1023/A:1012460413855; Krishnapuram B, 2004, IEEE T PATTERN ANAL, V26, P1105, DOI 10.1109/TPAMI.2004.55; LONGHURST JC, 1998, PREV CARDIOL, V1, P21; Maron O, 1998, P 15 INT C MACH LEAR, P341; Maron O, 1998, LEARNING AMBIGUITY; Mohan A, 2001, IEEE T PATTERN ANAL, V23, P349, DOI 10.1109/34.917571; Novovicova J, 1996, IEEE T PATTERN ANAL, V18, P218, DOI 10.1109/34.481557; Opelt A., 2004, P ECCV, V2, P71; RAMON J, 2000, P ICML 2000 WORKSH A; Ray S., 2005, P 22 INT C MACH LEAR, P697, DOI 10.1145/1102351.1102439; Rothganger F., 2003, P IEEE C COMP VIS PA, p[18, 272]; Ruffo G., 2000, THESIS U TURIN ITALY; Scott S., 2005, International Journal of Computational Intelligence and Applications, V5, DOI 10.1142/S1469026805001453; Sivic J., 2005, P IEEE INT C COMP VI, V1, P370; SMOLA A, 1999, P 9 INT C ART NEUR N, P575; Somol P, 2004, IEEE T PATTERN ANAL, V26, P900, DOI 10.1109/TPAMI.2004.28; Wang J., 2000, P 17 INT C MACH LEAR, P1119; Weidmann N., 2003, P EUR C MACH LEARN, P468; Xu X., 2004, P PAC AS C KNOWL DIS, P272; Yang C., 2000, Proceedings of 16th International Conference on Data Engineering (Cat. No.00CB37073), DOI 10.1109/ICDE.2000.839416; Zhang Q, 2002, ADV NEUR IN, V14, P1073; Zhang Q., 2002, P 19 INT C MACH LEAR, P682; ZHANG Y, 2006, EURASIP J APPL SIGNA; Zhou ZH, 2003, LECT NOTES ARTIF INT, V2837, P492; Zucker JD, 2001, LECT NOTES ARTIF INT, V2056, P204	65	182	205	3	19	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA	0162-8828			IEEE T PATTERN ANAL	IEEE Trans. Pattern Anal. Mach. Intell.	DEC	2006	28	12					1931	1947				17	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Computer Science; Engineering	093UL	WOS:000241195700004	17108368	
J	Zou, H				Zou, Hui			The adaptive lasso and its oracle properties	JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION			English	Article						asymptotic normality; lasso; minimax; oracle inequality; oracle procedure; variable selection	NONCONCAVE PENALIZED LIKELIHOOD; VARIABLE SELECTION; WAVELET SHRINKAGE; MODEL SELECTION; REGRESSION; ASYMPTOTICS	The lasso is a popular technique for simultaneous estimation and variable selection. Lasso variable selection has been shown to be consistent under certain conditions. In this work we derive a necessary condition for the lasso variable selection to be consistent. Consequently, there exist certain scenarios where the lasso is inconsistent for variable selection. We then propose a new version of the lasso, called the adaptive lasso, where adaptive weights are used for penalizing different coefficients in the l(1) penalty. We show that the adaptive lasso enjoys the oracle properties; namely, it performs as well as if the true underlying model were given in advance. Similar to the lasso, the adaptive lasso is shown to be near-minimax optimal. Furthermore, the adaptive lasso can be solved by the same efficient algorithm for solving the lasso. We also discuss the extension of the adaptive lasso in generalized linear models and show that the oracle properties still hold under mild regularity conditions. As a byproduct of our theory, the nonnegative garotte is shown to be consistent for variable selection.	Univ Minnesota, Sch Stat, Minneapolis, MN 55455 USA	Zou, H (reprint author), Univ Minnesota, Sch Stat, Minneapolis, MN 55455 USA.	hzou@stat.umn.edu					BREIMAN L, 1995, TECHNOMETRICS, V37, P373, DOI 10.2307/1269730; Shen XT, 2002, J AM STAT ASSOC, V97, P210, DOI 10.1198/016214502753479356; Knight K, 2000, ANN STAT, V28, P1356; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Antoniadis A, 2001, J AM STAT ASSOC, V96, P939, DOI 10.1198/016214501753208942; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; Chen SSB, 2001, SIAM REV, V43, P129, DOI 10.1137/S003614450037906X; FRANK IE, 1993, TECHNOMETRICS, V35, P109, DOI 10.2307/1269656; Efron B, 2004, ANN STAT, V32, P407; DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425; Breiman L, 1996, ANN STAT, V24, P2350; DONOHO D, 2002, P NATL ACAD SCI USA, V1005, P2197; DONOHO D, 2002, IEEE T INFORM THEORY, V47, P2845; Donoho D. L., 2004, MOST LARGE UNDERDETE; DONOHO DL, 1995, J ROY STAT SOC B MET, V57, P301; FAN J, 2006, P MADR INT C MATH 20; Fan JQ, 2004, ANN STAT, V32, P928, DOI 10.1214/009053604000000256; GEYER CJ, 1994, ANN STAT, V22, P1993, DOI 10.1214/aos/1176325768; Hunter DR, 2005, ANN STAT, V33, P1617, DOI 10.1214/009053605000000200; Lehmann E. L., 1998, THEORY POINT ESTIMAT; LENG C, 2004, NOTE LASSO RELATED P; McCullagh P., 1989, GEN LINEAR MODELS; MEINSHAUSEN N, 2004, VARIABLE SELECTION H; ROSSET S, 2004, PIECEWISE LINEAR REG; STEIN CM, 1981, ANN STAT, V9, P1135, DOI 10.1214/aos/1176345632; Yuan M., 2005, NONNEGATIVE GAROTTE; ZHAO P, 2006, MODEL SELECTION CONS	27	1063	1106	8	49	AMER STATISTICAL ASSOC	ALEXANDRIA	1429 DUKE ST, ALEXANDRIA, VA 22314 USA	0162-1459			J AM STAT ASSOC	J. Am. Stat. Assoc.	DEC	2006	101	476					1418	1429		10.1198/016214506000000735		12	Statistics & Probability	Mathematics	117JP	WOS:000242869700012		
J	Bickel, PJ; Li, B				Bickel, Peter J.; Li, Bo			Regularization in statistics	TEST			English	Article						regularization; linear regression; nonparametric regression; boosting; covariance matrix; principal component; bootstrap; subsampling; model selection	GENERALIZED CROSS-VALIDATION; NONCONCAVE PENALIZED LIKELIHOOD; VARIABLE SELECTION; LONGITUDINAL DATA; COVARIANCE MATRICES; MODEL SELECTION; ASYMPTOTIC OPTIMALITY; PRINCIPAL COMPONENTS; RIDGE-REGRESSION; DENSITY-FUNCTION	This paper is a selective review of the regularization methods scattered in statistics literature. We introduce a general conceptual approach to regularization and fit most existing methods into it. We have tried to focus on the importance of regularization when dealing with today's high-dimensional objects: data and models. A wide range of examples are discussed, including nonparametric regression, boosting, covariance matrix estimation, principal component estimation, subsampling.	Univ Calif Berkeley, Dept Stat, Berkeley, CA 94720 USA; Tsinghua Univ, Sch Econ & Management, Beijing, Peoples R China	Bickel, PJ (reprint author), Univ Calif Berkeley, Dept Stat, Berkeley, CA 94720 USA.	bickel@stat.Berkeley.EDU					AKAIKE H, 1970, ANN I STAT MATH, V22, P203, DOI 10.1007/BF02506337; EFRON B, 1979, ANN STAT, V7, P1, DOI 10.1214/aos/1176344552; KUNSCH HR, 1989, ANN STAT, V17, P1217, DOI 10.1214/aos/1176347265; KASS RE, 1995, J AM STAT ASSOC, V90, P773, DOI 10.1080/01621459.1995.10476572; WIGNER EP, 1955, ANN MATH, V62, P548, DOI 10.2307/1970079; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Zhang T, 2005, ANN STAT, V33, P1538, DOI 10.1214/009053605000000255; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; Johnstone IM, 2005, ANN STAT, V33, P1700, DOI 10.1214/009053605000000345; Tsybakov AB, 2004, ANN STAT, V32, P135; Bickel PJ, 2004, BERNOULLI, V10, P989, DOI 10.3150/bj/1106314847; Ledoit O, 2004, J MULTIVARIATE ANAL, V88, P365, DOI 10.1016/S0047-259X(03)00096-4; KASS RE, 1995, J AM STAT ASSOC, V90, P928, DOI 10.2307/2291327; Dudoit S, 2002, J AM STAT ASSOC, V97, P77, DOI 10.1198/016214502753479248; Zou H, 2005, J ROY STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x; ROSENBLATT M, 1956, ANN MATH STAT, V27, P832, DOI 10.1214/aoms/1177728190; Efron B, 2004, ANN STAT, V32, P407; SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136; LI KC, 1986, ANN STAT, V14, P1101, DOI 10.1214/aos/1176350052; MALLOWS CL, 1973, TECHNOMETRICS, V15, P661, DOI 10.2307/1267380; CRAVEN P, 1979, NUMER MATH, V31, P377; Pourahmadi M, 1999, BIOMETRIKA, V86, P677, DOI 10.1093/biomet/86.3.677; Johnstone IM, 2001, ANN STAT, V29, P295, DOI 10.1214/aos/1009210544; Bair E, 2006, J AM STAT ASSOC, V101, P119, DOI 10.1198/016214505000000628; BICKEL PJ, 2005, UNPUB CHOICE M M OUT; BICKEL PJ, 2006, IN PRESS J MACHINE L; BICKEL PJ, 2006, 716 U CAL DEP STAT; Bickel PJ, 1997, STAT SINICA, V7, P1; BICKELI PJ, 1993, EFFICIENT ADAPTIVE E; Birge L., 1997, FESTSCHRIFT LUCIEN C, P55, DOI 10.1007/978-1-4612-1880-7_4; Birge L., 2001, J EUR MATH SOC, V3, P203, DOI 10.1007/s100970100031; Bottcher A., 1999, INTRO LARGE TRUNCATE; Breiman L, 1996, ANN STAT, V24, P2350; BREIMAN L., 1990, J STAT COMPUT SIM, V37, P127, DOI 10.1080/00949659008811300; Buhlmann P, 2006, ANN STAT, V34, P559, DOI 10.1214/009053606000000092; Buhlmann P, 2006, J MACH LEARN RES, V7, P1001; Bunea F, 2006, J STAT PLAN INFER, V136, P4349, DOI 10.1016/j.jspi.2005.03.011; CHEN H, 1988, ANN STAT, V16, P136, DOI 10.1214/aos/1176350695; Daniels MJ, 2002, BIOMETRIKA, V89, P553, DOI 10.1093/biomet/89.3.553; Datta S, 1995, J AM STAT ASSOC, V90, P1289, DOI 10.2307/2291519; Devroye L., 1996, APPL MATH NEW YORK, V31; Donoho D. L., 2000, MATH CHALLENGES 21 C; Donoho DL, 1998, ANN STAT, V26, P879; Draper NR, 1998, WILEY SERIES PROBABI; Dudoit S, 2005, STAT METHODOL, V2, P131, DOI 10.1016/j.stamet.2005.02.003; Efron B, 2004, J AM STAT ASSOC, V99, P619, DOI 10.1198/016214504000000692; Fan J., 2006, P INT C MATH, P595; Fan J., 1996, MONOGRAPHS STAT APPL, V66; Fan JQ, 2004, ANN STAT, V32, P928, DOI 10.1214/009053604000000256; FURRER R, 2006, IN PRESS J MULTIVARI; GOTZE F, 1993, IMS B, P305; Gotze F, 2001, INST MATH S, V36, P286, DOI 10.1214/lnms/1215090074; GREENSHTEIN E, 2006, IN PRESS ANN STAT, V34; Greenshtein E, 2004, BERNOULLI, V10, P971, DOI 10.3150/bj/1106314846; Gyorfi L, 2002, SPRINGER SERIES STAT; HALL P, 1995, BIOMETRIKA, V82, P561; HALL P., 1992, SPRINGER SERIES STAT; Hastie T., 2001, SPRINGER SERIES STAT; HOERL AE, 1970, TECHNOMETRICS, V12, P55, DOI DOI 10.1080/00401706.1970.10488634; Huang JZ, 2006, BIOMETRIKA, V93, P85, DOI 10.1093/biomet/93.1.85; Hunter DR, 2005, ANN STAT, V33, P1617, DOI 10.1214/009053605000000200; KOSOROK M, 2006, UNPUB MARGINAL ASYMP; LI KC, 1987, ANN STAT, V15, P958, DOI 10.1214/aos/1176350486; LI KC, 1985, ANN STAT, V13, P1352, DOI 10.1214/aos/1176349742; Lugosi G, 1999, ANN STAT, V27, P1830; Lugosi G, 2004, ANN STAT, V32, P30; Mammen E, 1999, ANN STAT, V27, P1808; Mammen E., 1992, DOES BOOTSTRAP WORK; MEINSHAUSEN N, 2005, UNPUB LASSO RELAXATI; Nadaraya E. A., 1964, THEOR PROBAB APPL, V10, P186; PARZEN E, 1962, ANN MATH STAT, V33, P1065, DOI 10.1214/aoms/1177704472; PAUL D, 2005, UNPUB ASYMPTOTICS LE; POLITIS DN, 1994, ANN STAT, V22, P2031, DOI 10.1214/aos/1176325770; Politis DN, 1999, SPRINGER SERIES STAT; Pourahmadi M, 2000, BIOMETRIKA, V87, P425, DOI 10.1093/biomet/87.2.425; RISSANEN J, 1984, IEEE T INFORM THEORY, V30, P629, DOI 10.1109/TIT.1984.1056936; Robert C.P., 2004, SPRINGER TEXTS STAT; Shao J, 1997, STAT SINICA, V7, P221; Smith M, 2002, J AM STAT ASSOC, V97, P1141, DOI 10.1198/016214502388618942; Stein C., 1961, P 4 BERK S MATH STAT, V1, P361; Stone CJ, 1997, ANN STAT, V25, P1371; STONE M, 1974, J R STAT SOC B, V36, P111; Tikhonov AN, 1943, CR ACAD SCI URSS, V39, P176; Vapnik V., 1998, ADAPTIVE LEARNING SY; WACHTER KW, 1978, ANN PROBAB, V6, P1, DOI 10.1214/aop/1176995607; Wang YD, 2004, HANDBOOK OF COMPUTATIONAL STATISTICS: CONCEPTS AND METHODS, P437; Watson G. S., 1964, SANKHYA A, V26, P359; Wu WB, 2003, BIOMETRIKA, V90, P831, DOI 10.1093/biomet/90.4.831; Zhang HH, 2004, J AM STAT ASSOC, V99, P659, DOI 10.1198/016214504000000593	89	41	41	0	7	SPRINGER	NEW YORK	233 SPRING ST, NEW YORK, NY 10013 USA	1133-0686	1863-8260		TEST-SPAIN	Test	DEC	2006	15	2					271	303		10.1007/BF02607055		33	Statistics & Probability	Mathematics	114SA	WOS:000242684700001		
J	Laaksonen, R; Katajamaa, M; Paiva, H; Sysi-Aho, M; Saarinen, L; Junni, P; Lutjohann, D; Smet, J; Van Coster, R; Seppanen-Laakso, T; Lehtimaki, T; Soini, J; Oresic, M				Laaksonen, Reijo; Katajamaa, Mikko; Paiva, Hannu; Sysi-Aho, Marko; Saarinen, Lilli; Junni, Paivi; Lutjohann, Dieter; Smet, Joel; Van Coster, Rudy; Seppanen-Laakso, Tuulikki; Lehtimaki, Terho; Soini, Juhani; Oresic, Matej			A Systems Biology Strategy Reveals Biological Pathways and Plasma Biomarker Candidates for Potentially Toxic Statin-Induced Changes in Muscle	PLOS ONE			English	Article								Background. Aggressive lipid lowering with high doses of statins increases the risk of statin-induced myopathy. However, the cellular mechanisms leading to muscle damage are not known and sensitive biomarkers are needed to identify patients at risk of developing statin-induced serious side effects. Methodology. We performed bioinformatics analysis of whole genome expression profiling of muscle specimens and UPLC/MS based lipidomics analyses of plasma samples obtained in an earlier randomized trial from patients either on high dose simvastatin (80 mg), atorvastatin (40 mg), or placebo. Principal Findings. High dose simvastatin treatment resulted in 111 differentially expressed genes (1.5-fold change and p-value < 0.05), while expression of only one and five genes was altered in the placebo and atorvastatin groups, respectively. The Gene Set Enrichment Analysis identified several affected pathways (23 gene lists with False Discovery Rate q-value < 0.1) in muscle following high dose simvastatin, including eicosanoid synthesis and Phospholipase C pathways. Using lipidomic analysis we identified previously uncharacterized drug-specific changes in the plasma lipid profile despite similar statin-induced changes in plasma LDL-cholesterol. We also found that the plasma lipidomic changes following simvastatin treatment correlate with the muscle expression of the arachidonate 5-lipoxygenase-activating protein. Conclusions. High dose simvastatin affects multiple metabolic and signaling pathways in skeletal muscle, including the pro-inflammatory pathways. Thus, our results demonstrate that clinically used high statin dosages may lead to unexpected metabolic effects in non-hepatic tissues. The lipidomic profiles may serve as highly sensitive biomarkers of statin-induced metabolic alterations in muscle and may thus allow us to identify patients who should be treated with a lower dose to prevent a possible toxicity.	[Laaksonen, Reijo] Tampere Univ Hosp, Res Unit, Tampere, Finland; [Katajamaa, Mikko; Saarinen, Lilli; Junni, Paivi; Soini, Juhani] Univ Turku, Turku Ctr Biotechnol, Turku, Finland; [Katajamaa, Mikko; Saarinen, Lilli; Junni, Paivi; Soini, Juhani] Abo Akad Univ, Turku, Finland; [Paiva, Hannu] Tampere Univ Hosp, Dept Internal Med, Tampere, Finland; [Sysi-Aho, Marko; Seppanen-Laakso, Tuulikki; Oresic, Matej] VTT Tech Res Ctr Finland, Espoo, Finland; [Lutjohann, Dieter] Univ Bonn, Dept Clin Pharmacol, D-5300 Bonn, Germany; [Smet, Joel; Van Coster, Rudy] Ghent Univ Hosp, Dept Pediat, Div Pediat Neurol & Metab, B-9000 Ghent, Belgium; [Lehtimaki, Terho] Tampere Univ Hosp, Dept Clin Chem, Lab Atherosclerosis Genet, FIN-33521 Tampere, Finland; [Lehtimaki, Terho] Tampere Univ Hosp, Ctr Lab Med, FIN-33521 Tampere, Finland; [Lehtimaki, Terho] Univ Tampere, Sch Med, FIN-33101 Tampere, Finland	Laaksonen, R (reprint author), Tampere Univ Hosp, Res Unit, Tampere, Finland.	reijo.laaksonen@helsinki.fi; matej.oresic@vtt.fi	Laaksonen, Reijo/D-6323-2014		Future Forum Research; Tampere University Hospital Research Fund; Academy of Finland SYSBIO [8207492]; Tekes MASI program	This study was supported by the Future Forum Research Grant, the Tampere University Hospital Research Fund, Academy of Finland SYSBIO program (grant #8207492), and the Tekes MASI program.	Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Bruckert E, 2005, CARDIOVASC DRUG THER, V19, P403, DOI 10.1007/s10557-005-5686-z; Phillips PS, 2002, ANN INTERN MED, V137, P581; Subramanian A, 2005, P NATL ACAD SCI USA, V102, P15545, DOI 10.1073/pnas.0506580102; [Anonymous], 1994, LANCET, V344, P1383; Fahy E, 2005, J LIPID RES, V46, P839, DOI 10.1194/jlr.E400004-JLR200; GELADI P, 1986, ANAL CHIM ACTA, V185, P1, DOI 10.1016/0003-2670(86)80028-9; Barker M, 2003, J CHEMOMETR, V17, P166, DOI 10.1002/cem.785; Katajamaa M, 2006, BIOINFORMATICS, V22, P634, DOI 10.1093/bioinformatics/btk039; Efron B, 2004, ANN STAT, V32, P407; Wenk MR, 2005, NAT REV DRUG DISCOV, V4, P594, DOI 10.1038/nrd1776; Paiva H, 2005, CLIN PHARMACOL THER, V78, P60, DOI 10.1016/j.clpt.2005.03.006; Antons KA, 2006, AM J MED, V119, P400, DOI 10.1016/j.amjmed.2006.02.007; Brentani H, 2003, P NATL ACAD SCI USA, V100, P13418, DOI 10.1073/pnas.1233632100; Burke JR, 1997, BBA-MOL CELL RES, V1359, P80, DOI 10.1016/S0167-4889(97)00094-3; Schachter M, 2005, FUND CLIN PHARMACOL, V19, P117, DOI 10.1111/j.1472-8206.2004.00299.x; DEJONG S, 1993, CHEMOMETR INTELL LAB, V18, P251, DOI 10.1016/0169-7439(93)85002-X; DIXON RAF, 1990, NATURE, V343, P282, DOI 10.1038/343282a0; Engelmann B, 2004, BIOCHEM SOC T, V32, P147, DOI 10.1042/BST0320147; Hastie T., 2001, ELEMENTS STAT LEARNI; Mukhtar RYA, 2005, CURR OPIN LIPIDOL, V16, P640; Nelson J, 2006, AM J EPIDEMIOL, V163, P903, DOI 10.1093/aje/kwj140; Oresic M, 2006, EXPERT REV MOL DIAGN, V6, P575, DOI 10.1586/14737159.6.4.575; Rubinfeld D. L., 1998, ECONOMETRIC MODELS E; SACKS FM, 1996, NEW ENGL J MED, V335, P1009; SHEPHERD J, 1995, NEW ENGL J MED, V333, P1301, DOI 10.1056/NEJM199511163332001; Urso ML, 2005, ARTERIOSCL THROM VAS, V25, P2560, DOI 10.1161/01.ATV.0000190608.28704.71; VLADUTIU GD, 2006, MUSCLE NERVE; Wise BM, 2005, PLS TOOLBOX 3 5 USE; WOLD S, 1987, CHEMOMETR INTELL LAB, V2, P37, DOI 10.1016/0169-7439(87)80084-9; Zoeller RA, 1999, BIOCHEM J, V338, P769, DOI 10.1042/0264-6021:3380769	31	129	130	4	10	PUBLIC LIBRARY SCIENCE	SAN FRANCISCO	185 BERRY ST, STE 1300, SAN FRANCISCO, CA 94107 USA	1932-6203			PLOS ONE	PLoS One	DEC 20	2006	1	1							e97	10.1371/journal.pone.0000097		9	Multidisciplinary Sciences	Science & Technology - Other Topics	V10DB	WOS:000207443600096	17183729	
B	Destrero, A; Odone, F; Verri, A				Destrero, Augusto; Odone, Francesca; Verri, Alessandro			A trainable system for face detection in unconstrained environments	14TH INTERNATIONAL CONFERENCE ON IMAGE ANALYSIS AND PROCESSING, PROCEEDINGS			English	Proceedings Paper	14th International Conference on Image Analysis and Processing	SEP 10-14, 2007	Modena, ITALY	Imagelab, Univ Studi Modena & Reggio Emilia, Comune Modena, GIRPR, Int Assoc Pattern Recognit, CPS Color, Franco Cosimo Panini, Banca Popolare, Emilia Romagna, Accad Mil Modena, CNA Modena				This paper describes a monitoring system that implements real-time face detection. The structure of the system is based on multiple cues,that discard non face areas as soon as possible: we combine motion, skin, and face detection. The latter is the core of our system and consists of a hierarchy of small SVM classifiers built on the output of a feature selection procedure. Following face detection, a Kalman tracking on the face region allows us to optimize results over time. We present., an experimental analysis of the face detection module and.-results obtained with the whole system on the specific task of counting people entering the scene.	[Destrero, Augusto; Odone, Francesca; Verri, Alessandro] Univ Genoa, DISI, I-16146 Genoa, Italy	Destrero, A (reprint author), Univ Genoa, DISI, Via Dodecaneso 35, I-16146 Genoa, Italy.						ANDERSON RR, 1981, J INVEST DERMATOL, V77, P13, DOI 10.1111/1523-1747.ep12479191; Yang MH, 2002, IEEE T PATTERN ANAL, V24, P34; BEVILACQUA A, 2002, MVA 2002 C P NAR JAP, P614; BURT PJ, 1988, P IEEE, P76; DAUBECHIES I, 2004, COMMUN PUR APPL MATH, P57; DESTRERO A, 2007, DISITR200701 U GEN; ELAZOUZI K, 2004, P 1 EUR C VIS MED PR; ELGAMMAL A, 1999, P ICIP; Fleuret F, 2001, INT J COMPUT VISION, V41, P85, DOI 10.1023/A:1011113216584; HEISELE B, 2001, IEEE P CVPR; Lehmann E. L., 1975, NONPARAMETRICS STAT; MOHAN A, 2001, IEEE T PATTERN ANAL, V23; Osuna E., 1997, CVPR; RESEFIELD A, 1977, IEEE T SYS MAN CYB, P2; Tibshirani R, 1996, J ROYAL STAT SOC B, V58; Viola P. A., 2004, INT J COMPUTER VISIO, V57; Wang Q., 2006, INT JOURN INTELLIGEN, P1	17	0	0	1	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA			978-0-7695-2877-9				2007							407	412		10.1109/ICIAP.2007.4362812		6	Engineering, Electrical & Electronic; Imaging Science & Photographic Technology	Engineering; Imaging Science & Photographic Technology	BGX96	WOS:000251198200064		
B	Jordaan, JA; van Wyk, MA; van Wyk, BJ			IEEE	Jordaan, J. A.; van Wyk, M. A.; van Wyk, B. J.			Analysis of the semi-parametric spectral estimation problem for power systems	2007 AFRICON, VOLS 1-3	IEEE Africon		English	Proceedings Paper	8th IEEE Africon Conference	SEP 26-28, 2007	Windhoek, NAMIBIA	IEEE				This paper shows an analysis of a fast solution method for the system of linear equations encountered in the semi-parametric spectral estimation problem. Spectral analysis forms the basis of a major part of signal processing applications typically used for tracking and distinguishing signals of interest, and for estimating dominant harmonics in signals. Power utility companies uses spectral analysis in analysing signals recorded from the power system. The signals recorded during power system disturbances could be analysed to determine the characteristics of the disturbance. The test results show that an increase in the order of the local polynomial filter used in the semi-parametric method affects the condition number of the coefficient matrix, and this results in an inaccurate solution.	[Jordaan, J. A.; van Wyk, M. A.; van Wyk, B. J.] Tshwane Univ Technol, French S African Tech Inst Elect, Pretoria, South Africa	Jordaan, JA (reprint author), Tshwane Univ Technol, French S African Tech Inst Elect, Pretoria, South Africa.	jacojordaan@webmail.co.za; mavanwyk@fsatie.ac.za; vanwykb@tut.ac.za					Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; BIALKOWSKI SE, 1989, ANAL CHEM, V61, P1308, DOI 10.1021/ac00186a028; Draper N, 1981, APPL REGRESSION ANAL; GORRY PA, 1990, ANAL CHEM, V62, P570, DOI 10.1021/ac00205a007; JORDAAN J, 2004, THESIS S WESTPHALIA; JORDAAN J, 2006, THESIS TSHWANE U TEC; JORDAAN JA, 2006, 6 IASTED INT C MOD S; KELLER P, 2005, SO AFR U POW ENG C S; VANAMERONGEN R, 1995, IEEE PES WINT M, P1; ZIVANOVIC R, 2005, POW TECH2005 C ST PE; Zivanovic R, 2004, IEEE T POWER DELIVER, V19, P1085, DOI 10.1109/TPWRD.2004.829945	11	0	0	0	0	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA			978-1-4244-0986-0	IEEE AFRICON			2007							1201	1204				4	Computer Science, Theory & Methods; Engineering, Electrical & Electronic	Computer Science; Engineering	BHQ38	WOS:000255394100183		
S	Rasouli, M; Westwick, D; Rosehart, W			IEEE	Rasouli, Mohammad; Westwick, David; Rosehart, William			Incorporating term selection into separable nonlinear least squares identification methods	2007 CANADIAN CONFERENCE ON ELECTRICAL AND COMPUTER ENGINEERING, VOLS 1-3	Canadian Conference on Electrical and Computer Engineering		English	Proceedings Paper	20th Annual Canadian Conference on Electrical and Computer Engineering	APR 22-26, 2007	Vancouver, CANADA	IEEE				In this paper, a method for the integration of the Least absolute shrinkage and selection operator (Lasso) into Separable Nonlinear Least Squares (SNLS) algorithms is presented. Lasso is reformulated as an equality constrained linear regression. The original SNLS problem is then solved subject to the resulting equality constraints. Simulations using the proposed algorithm to fit a Laguerre model to the output of a linear system are used to demonstrate its performance.	[Rasouli, Mohammad; Westwick, David; Rosehart, William] Univ Calgary, Dept Elect & Comp Engn, Schulich Sch Engn, Calgary, AB T2N 1N4, Canada	Rasouli, M (reprint author), Univ Calgary, Dept Elect & Comp Engn, Schulich Sch Engn, 2500 Univ Dr NW, Calgary, AB T2N 1N4, Canada.						Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Chen SSB, 2001, SIAM REV, V43, P129, DOI 10.1137/S003614450037906X; GOLUB GH, 1973, SIAM J NUMER ANAL, V10, P413, DOI 10.1137/0710036; HASTIE JFT, 2001, ELEMENTS STAT LEARNI; KAUFMAN L, 1978, SIAM J NUMER ANAL, V15, P12, DOI 10.1137/0715002; Kaufman L., 1975, BIT (Nordisk Tidskrift for Informationsbehandling), V15, DOI 10.1007/BF01932995; Ngia LSH, 2001, IEEE T CIRCUITS-II, V48, P562, DOI 10.1109/82.943327; Ogata K., 1995, DISCRETE TIME CONTRO; RUHE A, 1980, SIAM REV, V22, P318, DOI 10.1137/1022057; Sjoberg J, 1997, NEURAL NETWORKS FOR SIGNAL PROCESSING VII, P345, DOI 10.1109/NNSP.1997.622415	10	1	1	0	1	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA	0840-7789		978-1-4244-1020-0	CAN CON EL COMP EN			2007							892	895				4	Computer Science, Interdisciplinary Applications; Computer Science, Theory & Methods; Engineering, Electrical & Electronic	Computer Science; Engineering	BIF56	WOS:000259117300221		
S	Gu, L; Xing, EP; Kanade, T			IEEE	Gu, Lie; Xing, Eric P.; Kanade, Takeo			Learning GMRF structures for spatial priors	2007 IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, VOLS 1-8	PROCEEDINGS - IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION		English	Proceedings Paper	IEEE Conference on Computer Vision and Pattern Recognition	JUN 17-22, 2007	Minneapolis, MN	IEEE, hp invent, INI-GraphicsNet, VIOSO				The goal of this paper is to find sparse and representative spatial priors that can be applied to part-based object localization. Assuming a GMRF prior over part configurations, we construct the graph structure of the prior by regressing the position of each part on all other parts, and selecting the neighboring edges using a Lasso-based method. This approach produces a prior structure which is not only sparse, but also faithful to the spatial dependencies that are observed in training data. We evaluate the representation power of the learned prior structure in two ways: first is drawing samples from the prior, and comparing them with the samples produced by the GMRF priors of other structures; second is comparing the results when applying different priors to a facial components localization task. We show that the learned graph captures meaningful geometrical variations with significantly sparser structure and leads to better parts localization results.	Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA	Gu, L (reprint author), Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA.	gu@cs.cmu.edu; epxing@cs.cmu.edu; tk@cs.cmu.edu					Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; CRANDALL D, 2005, P COMPUTER VISION PA; Dobra A, 2004, J MULTIVARIATE ANAL, V90, P196, DOI 10.1016/j.jmva.2004.02.009; DRTON M, 2004, BIOMETRIKA; Felzenszwalb P., 2005, INT J COMPUTER VISIO; FERGUS R, 2003, P COMP VIS PATT REC; GOWER JC, 1975, PSYCHOMETRIKA; Lauritzen S. L., 1996, GRAPHICAL MODELS; Lowe D. G., 2004, INT J COMPUTER VISIO; Martinez A., 1998, 24 CVC; MEINSHAUSEN N, 2005, ANN STAT; SCHNEIDERMAN H, 2000, P COMP VIS PATT REC	12	0	0	1	1	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA	1063-6919		978-1-4244-1179-5	PROC CVPR IEEE			2007							102	107				6	Computer Science, Software Engineering; Mathematical & Computational Biology; Remote Sensing; Imaging Science & Photographic Technology	Computer Science; Mathematical & Computational Biology; Remote Sensing; Imaging Science & Photographic Technology	BGT02	WOS:000250382800014		
S	Barrio, I; Romero, E; Belanche, L			IEEE	Barrio, Ignacio; Romero, Enrique; Belanche, Lluis			Search strategies guided by the evidence for the selection of basis functions in regression	2007 IEEE INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS, VOLS 1-6	IEEE International Joint Conference on Neural Networks (IJCNN)		English	Proceedings Paper	International Joint Conference on Neural Networks	AUG 12-17, 2007	Orlando, FL	IEEE			NETWORKS	This work addresses the problem of selecting a subset of basis functions for a model linear in the parameters for regression tasks. Basis functions from a set of candidates are explicitly selected with search methods coming from the feature selection field. Following approximate Bayesian inference, the search is guided by the evidence. The tradeoff between model complexity and computational cost can be controlled by choosing the search strategy. The experimental results show that, under mild assumptions, compact and very competitive models are usually found.								MACKAY DJC, 1992, NEURAL COMPUT, V4, P415, DOI 10.1162/neco.1992.4.3.415; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; MALLAT SG, 1993, IEEE T SIGNAL PROCES, V41, P3397, DOI 10.1109/78.258082; Tipping ME, 2001, J MACH LEARN RES, V1, P211, DOI 10.1162/15324430152748236; PUDIL P, 1994, PATTERN RECOGN LETT, V15, P1119, DOI 10.1016/0167-8655(94)90127-9; MACKAY DJC, 1992, NEURAL COMPUT, V4, P448, DOI 10.1162/neco.1992.4.3.448; SILVERMAN BW, 1985, J R STAT SOC B, V47, P1; CANDELA JQ, 2004, THESIS TU DENMARK; CHEN S, 1991, IEEE T NEURAL NETWOR, V2, P302, DOI 10.1109/72.80341; Chen S. S., 1999, SIAM J SCI COMPUT, V20, P33; Kittler J, 1986, HDB PATTERN RECOGNIT; MacKay DJC, 1999, NEURAL COMPUT, V11, P1035, DOI 10.1162/089976699300016331; Neal R. M., 1996, LECT NOTES STAT, V1; Rasmussen C. E., 1996, DELVE MANUAL; Smola AJ, 2001, ADV NEUR IN, V13, P619; Smola AJ, 1998, ALGORITHMICA, V22, P211, DOI 10.1007/PL00013831; SOMOL P, 2000, P 15 INT C PATT REC, P2406; Tipping M. E., 2003, 9 INT WORKSH ART INT; Vapnik V.N., 1995, NATURE STAT LEARNING; Vincent P, 2002, MACH LEARN, V48, P165, DOI 10.1023/A:1013955821559	20	0	0	1	1	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA	1098-7576		978-1-4244-1379-9	IEEE IJCNN			2007							436	441		10.1109/IJCNN.2007.4370996		6	Computer Science, Artificial Intelligence; Computer Science, Software Engineering	Computer Science	BHM64	WOS:000254291100076		
B	Loth, M; Davy, M; Preux, P			IEEE	Loth, Manuel; Davy, Manuel; Preux, Philippe			Sparse temporal difference learning using LASSO	2007 IEEE International Symposium on Approximate Dynamic Programming and Reinforcement Learning			English	Proceedings Paper	IEEE International Symposium on Approximate Dynamic Programming and Reinforcement Learning	APR 01-05, 2007	Honolulu, HI				REGRESSION	We consider the problem of on-line value function estimation in reinforcement learning. We concentrate on the function approximator to use. To try to break the curse of dimensionality, we focus on non parametric function approxi-mators. We propose to fit the use of kernels into the temporal difference algorithms by using regression via the LASSO. We introduce the equi-gradient descent algorithm (EGD) which is a direct adaptation of the one recently introduced in the LARS algorithm family for solving the LASSO. We advocate our choice of the EGD as a judicious algorithm for these tasks. We present the EGD algorithm in details as well as some experimental results. We insist on the qualities of the EGD for reinforcement learning.	Univ Lille, CNRS, LIFL, SequeL INRIA Futurs, Villeneuve, France	Loth, M (reprint author), Univ Lille, CNRS, LIFL, SequeL INRIA Futurs, Villeneuve, France.						Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Efron B, 2004, ANN STAT, V32, P407; Atkeson CG, 1997, ARTIF INTELL REV, V11, P75, DOI 10.1023/A:1006511328852; BOYAN J, 1999, P ICML; COULOM R, 2002, THESIS INP GRENOBLE; Engel Y., 2005, THESIS HEBREW U; Girosi F, 1998, NEURAL COMPUT, V10, P1455, DOI 10.1162/089976698300017269; GUIGUE V, 2005, THESIS I NATL SCI AP; LEEMON C, 1995, INT C MACH LEARN, P30; MOORE A, 1995, MACH LEARNING, V21; Munos R., 1999, P 16 INT JOINT C ART, P1348; PLATT J, 1992, NEURAL COMPUT, P213; Ratitch B., 2004, P ECML; Sutton R. S., 1996, P ADV NEUR INF PROC, P1038; TESAURO G, 1995, COMMUN ACM, V38, P58, DOI 10.1145/203330.203343; THAM CK, 1996, THESIS CAMBRIDGE U, V58, P267; TIKHONOV A, 1977, SOLUTIONS ILL POSTED	17	0	0	0	0	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA			978-1-4244-0706-4				2007							352	359		10.1109/ADPRL.2007.368210		8	Computer Science, Artificial Intelligence	Computer Science	BGM94	WOS:000248536200049		
B	Wainwright, M			IEEE	Wainwright, Martin			Information-theoretic bounds on sparsity recovery in the high-dimensional and noisy setting	2007 IEEE INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY PROCEEDINGS, VOLS 1-7			English	Proceedings Paper	IEEE International Symposium on Information Theory	JUN 24-29, 2007	Nice, FRANCE	IEEE			LARGE UNDERDETERMINED SYSTEMS; EQUATIONS	The problem of recovering the sparsity pattern of a fixed but unknown vector beta* epsilon R-p based on a set of n noisy observations arises in a variety of settings, including subset selection in regression, graphical model selection, signal denoising, compressive sensing, and constructive approximation. Of interest are conditions on the model dimension p, the sparsity index s (number of non-zero entries in beta*), and the number of observations n that are necessary and/or sufficient to ensure asymptotically perfect recovery of the sparsity pattern. This paper focuses on the information-theoretic limits of sparsity recovery: in particular, for a noisy linear observation model based on measurement vectors drawn from the standard Gaussian ensemble, we derive both a set of sufficient conditions for asymptotically perfect recovery using the optimal decoder, as well as a set of necessary conditions that any decoder must satisfy for perfect recovery. This analysis of optimal decoding limits complements our previous work [19] on thresholds for the behavior of l(1)-constrained quadratic programming for Gaussian measurement ensembles.	[Wainwright, Martin] Univ Calif Berkeley, Dept Stat, Berkeley, CA 94720 USA	Wainwright, M (reprint author), Univ Calif Berkeley, Dept Stat, Berkeley, CA 94720 USA.						Tropp JA, 2006, IEEE T INFORM THEORY, V52, P1030, DOI 10.1109/TIT.2005.864420; Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Candes EJ, 2005, IEEE T INFORM THEORY, V51, P4203, DOI 10.1109/TIT.2005.858979; Donoho DL, 2006, COMMUN PUR APPL MATH, V59, P797, DOI 10.1002/cpa.20132; Candes EJ, 2006, COMMUN PUR APPL MATH, V59, P1207, DOI 10.1002/cpa.20124; Elad M, 2002, IEEE T INFORM THEORY, V48, P2558, DOI 10.1109/TIT.2002.801410; Candes E., 2006, ANN STAT; DONOHO DL, 2006, J AMS UNPUB; Donoho DL, 2006, COMMUN PUR APPL MATH, V59, P907, DOI 10.1002/cpa.20131; FLETCHER AK, 2007, ICASSP; Hasminskii R.Z, 1981, STAT ESTIMATION ASYM; Laurent B., 1998, ANN STAT, V28, P1303; Malioutov D.M., 2004, IEEE INT C AC SPEECH, V2, P793; MEINSHAUSEN N, 2006, ANN STAT IN PRESS; Thomas J. A., 1991, ELEMENTS INFORM THEO; WAINWRIGHT MJ, 2006, 725 UC BERK DEP STAT; WAINWRIGHT MJ, 2006, P ALL C COMM CONTR C; ZHAO P, 2006, J MACHINE L IN PRESS	20	1	1	1	1	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA			978-1-4244-1397-3				2007							961	965		10.1109/ISIT.2007.4557348		5	Engineering, Electrical & Electronic; Telecommunications	Engineering; Telecommunications	BHW32	WOS:000257010201024		
S	Seni, G; Yang, E; Akar, S			IEEE	Seni, Giovanni; Yang, Echvard; Akar, Said			Yield Modeling with rule ensembles	2007 IEEE/SEMI Advanced Semiconductor Manufacturing Conference	ADVANCED SEMICONDUCTOR MANUFACTURING CONFERENCE AND WORKSHOP - PROCEEDINGS		English	Proceedings Paper	18th Annual IEEE/SEMI Advanced Semiconductor Manufacturing Conference	JUN 11-12, 2007	Stresa, ITALY	IEEE, SEMI		yield-loss characterization; decision trees; regression; classification; predictive learning; ensembles		In this paper we introduce the application of a new statistical modeling algorithm called Rule Ensembles to the problem of yield-loss characterization. Yield loss modeling is viewed as a regression or classification problem, and a model is constructed as a linear combination of simple rules derived from the data. These rule ensembles have been shown to produce predictive models competitive with the best methods. In addition to their high accuracy, however, these rules are easy to understand. Similarly, the degree of relevance of each rule, and its respective variables, can be assessed. The algorithm also provides methodology for automatically identifying those variables that are involved in interactions with other variables, and the strength and degrees of those interactions. To illustrate the interpretation advantages of the method, an analysis on semiconductor manufacturing data is provided.	PDF Solut, San Jose, CA 95110 USA	Seni, G (reprint author), PDF Solut, San Jose, CA 95110 USA.						Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Breiman L., 1984, CLASSIFICATION REGRE; Freund Y., 1996, MACHINE LEARNING; Friedman J, 1999, STOCHASTIC GRADIENT; Friedman J., 2005, PREDICTIVE LEARNING; Hastie T., 2001, ELEMENTS STAT LEARNI; QUINLAN JR, 1987, C4 5 PROGRAMS MACHIN	8	0	0	0	0	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA	1078-8743		978-1-4244-0652-4	ASMC PROC			2007							130	135				6	Engineering, Manufacturing; Engineering, Electrical & Electronic; Instruments & Instrumentation	Engineering; Instruments & Instrumentation	BGR06	WOS:000249892600028		
B	Boufounos, P; Duarte, MF; Baraniuk, RG			IEEE	Boufounos, Petros; Duarte, Marco F.; Baraniuk, Richard G.			Sparse signal reconstruction from noisy compressive measurements using cross validation	2007 IEEE/SP 14TH WORKSHOP ON STATISTICAL SIGNAL PROCESSING, VOLS 1 AND 2			English	Proceedings Paper	14th IEEE/SP Workshop on Statistical Signal Processing	AUG 26-29, 2007	Madison, WI	IEEE, SP		data acquisition; sampling methods; data models; signal reconstruction; parameter estimation		Compressive sensing is a new data acquisition technique that aims to measure sparse and compressible signals at close to their intrinsic information rate rather than their Nyquist rate. Recent results in compressive sensing show that a sparse or compressible signal can be reconstructed from very few incoherent measurements. Although the sampling and reconstruction process is robust to measurement noise, all current reconstruction methods assume some knowledge of the noise power or the acquired signal to noise ratio. This knowledge is necessary to set algorithmic parameters and stopping conditions. If these parameters are set incorrectly, then the reconstruction algorithms either do not fully reconstruct the acquired signal (underfitting) or try to explain a significant portion of the noise by distorting the reconstructed signal (overfitting). This paper explores this behavior and examines the use of cross validation to determine the stopping conditions for the optimization algorithms. We demonstrate that by designating a small set of measurements as a validation set it is possible to optimize these algorithms and reduce the reconstruction error. Furthermore we explore the trade-off between using the additional measurements for cross validation instead of reconstruction.	[Boufounos, Petros; Duarte, Marco F.; Baraniuk, Richard G.] Rice Univ, Houston, TX 77005 USA	Boufounos, P (reprint author), Rice Univ, Houston, TX 77005 USA.		Duarte, Marco/G-6906-2012; Boufounos, Petros/C-3602-2013	Duarte, Marco/0000-0001-8410-3266; Boufounos, Petros/0000-0003-1369-0947			Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582; Haupt J, 2006, IEEE T INFORM THEORY, V52, P4036, DOI 10.1109/TIT.2006.880031; Osborne MR, 2000, IMA J NUMER ANAL, V20, P389, DOI 10.1093/imanum/20.3.389; Efron B, 2004, ANN STAT, V32, P407; Candes EJ, 2006, COMMUN PUR APPL MATH, V59, P1207, DOI 10.1002/cpa.20124; CANDES E, 2006, IN PRESS ANN STAT; Candes E., 2006, P INT C MATH, V3, P1433; DONOHO DL, 2006, P C INF SCI SYS PRIN; Duarte M. F., 2005, P SPARS05 RENN FRANC; FIGUEIREDO MAT, 2007, GRADIENT PROJECTIONS; KIM SJ, 2007, METHOD LARGE SCALE R; Mallat S., 1993, IEEE T SIGNAL PROCES, V41; TIBSHIRANI R., 1996, J ROYAL STAT SOC, V58; Tropp J. A., 2005, SIGNAL RECOVERY PART	15	0	0	1	2	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA			978-1-4244-1197-9				2007							299	303		10.1109/SSP.2007.4301267		5	Engineering, Electrical & Electronic; Telecommunications	Engineering; Telecommunications	BHI51	WOS:000253416000062		
B	Smyth, C; Coomans, D			IEEE	Smyth, Christine; Coomans, Danny			Clustering microarrays with predictive weighted ensembles	2007 IEEE Symposium on Computational Intelligence in Bioinformatics and Computational Biology			English	Proceedings Paper	IEEE Symposium on Computational Intelligence in Bioinformatics and Computational Biology	APR 01-05, 2007	Honolulu, HI				MULTIVARIATE REGRESSION TREES; EXPRESSION DATA; CLASS DISCOVERY; SELECTION	Cluster ensembles seek a consensus across many individual partitions and the resulting solution is usually stable. Cluster ensembles are well suited to the analysis of DNA microarrays, where the tremendous size of the dataset can thwart the discovery of stable groups. Post processing cluster ensembles, where each individual partition is weighted according to its relative accuracy improves the performance of the ensemble whilst maintaining its stability. However, weighted cluster ensembles remain relatively unexplored, primarily because there are no common means of assessing the accuracy of individual clustering solutions. This paper describes a technique of creating weighted cluster ensembles suitable for use with microarray datasets. A regression technique is used to obtain individual cluster solutions. Each solution is then weighted according to its predictive accuracy. The consensus partition is obtained using a novel modification to the traditional k-means algorithm which further enforces the predictability of the solution. An estimate of the natural number of clusters can also be obtained using the modified k-means algorithm. Furthermore, a valuable byproduct of this weighted ensemble approach is a variable importance list. The methodology is applied on two well-known microarray datasets with promising results.	James Cook Univ N Queensland, Stat & Intelligent Data Anal Grp, Sch Math Phys & Informat Technol, Townsville, Qld 4811, Australia	Smyth, C (reprint author), James Cook Univ N Queensland, Stat & Intelligent Data Anal Grp, Sch Math Phys & Informat Technol, Townsville, Qld 4811, Australia.						ACUNA E, DPREP DATA PREPROCES; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Alon U, 1999, P NATL ACAD SCI USA, V96, P6745, DOI 10.1073/pnas.96.12.6745; Dudoit S, 2003, BIOINFORMATICS, V19, P1090, DOI 10.1093/bioinformatics/btg038; Golub TR, 1999, SCIENCE, V286, P531, DOI 10.1126/science.286.5439.531; Yeung KY, 2001, BIOINFORMATICS, V17, P309, DOI 10.1093/bioinformatics/17.4.309; BAKER SC, 2006, BMC BIOINFORMATICS, V7, pNIL1; Breiman L., 1984, CLASSIFICATION REGRE; DATTA S, 2006, BMC BIOINFORMATICS, V7; Dudoit S., 2002, GENOME BIOL, V3, DOI [10.1186/gb-2002-3-7-research0036, DOI 10.1186/GB-2002-3-7-RESEARCH0036]; Dunteman GH, 1989, PRINCIPAL COMPONENTS; FERN XZ, 2006, CS063002 OR STAT U D; Friedman J. H., 2003, IMPORTANCE SAMPLED L; GREENE D, 2004, 17 IEEE S COMP BAS M; Grotkjaer T, 2006, BIOINFORMATICS, V22, P58, DOI 10.1093/bioinformatics/bti746; HANCOCK T, 2006, THESIS J COOK U; Hastie T., 2001, ELEMENTS STAT LEARNI; Kaufman L, 1990, FINDING GROUPS DATA; Kaufman L., 1987, Statistical Data Analysis Based on the L1-Norm and Related Methods. First International Conference; McLachlan GJ, 2002, BIOINFORMATICS, V18, P413, DOI 10.1093/bioinformatics/18.3.413; Monti S, 2003, MACH LEARN, V52, P91, DOI 10.1023/A:1023949509487; Questier F, 2005, CHEMOMETR INTELL LAB, V76, P45, DOI 10.1016/j.chemolab.2004.09.003; R Development Core Team, 2006, R LANG ENV STAT COMP; SEGAL MR, 1992, J AM STAT ASSOC, V87, P407, DOI 10.2307/2290271; Smyth C, 2006, PATTERN RECOGN, V39, P424, DOI 10.1016/j.patcog.2005.09.003; SMYTH C, UNPUB PREDICTIVE WEI; Smyth C, 2006, CHEMOMETR INTELL LAB, V80, P120, DOI 10.1016/j.chemolab.2005.09.001; Strehl A., 2002, J MACHINE LEARNING R, V3, P583, DOI DOI 10.1162/153244303321897735; Tibshirani R, 2005, J COMPUT GRAPH STAT, V14, P511, DOI 10.1198/106186005X59243; Weingessel A, 2003, DISTRIBUTED STAT COM	30	0	0	0	91	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA			978-1-4244-0710-1				2007							98	105				8	Computer Science, Interdisciplinary Applications; Engineering, Electrical & Electronic; Mathematical & Computational Biology	Computer Science; Engineering; Mathematical & Computational Biology	BGM70	WOS:000248516200014		
B	Koh, K; Kim, SJ; Boyd, S			IEEE	Koh, Kwangmoo; Kim, Seung-Jean; Boyd, Stephen			An efficient method for large-scale l(1)-regularized convex loss minimization	2007 Information Theory and Applications Workshop			English	Proceedings Paper	Workshop on Information Theory and Applications	FEB, 2006	La Jolla, CA		Univ Calif San Diego		ROBUST UNCERTAINTY PRINCIPLES; SPARSE REPRESENTATION; LASSO; SELECTION; BASES; REGRESSION; RECOVERY; MODEL; PAIRS	Convex loss minimization with l(1) regularization has been proposed as a promising method for feature selection in classification (e.g., l(1)-regularized logistic regression) and regression (e.g., l(1)-regularized least squares). In this paper we describe an efficient interior-point method for solving large-scale l(1)-regularized convex loss minimization problems that uses a preconditioned conjugate gradient method to compute the search step. The method can solve very large problems. For example, the method can solve an l(1)-regularized logistic regression problem with a million features and examples (e.g., the 20 Newsgroups data set), in a few minutes, on a PC.	Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA	Koh, K (reprint author), Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA.						Keerthi SS, 2005, J MACH LEARN RES, V6, P341; Knight K, 2000, ANN STAT, V28, P1356; Donoho DL, 2001, IEEE T INFORM THEORY, V47, P2845, DOI 10.1109/18.959265; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Gribonval R, 2003, IEEE T INFORM THEORY, V49, P3320, DOI 10.1109/TIT.2003.820031; Zhao P, 2006, J MACH LEARN RES, V7, P2541; Candes EJ, 2006, FOUND COMPUT MATH, V6, P227, DOI 10.1007/s10208-004-0162-x; Candes EJ, 2006, IEEE T INFORM THEORY, V52, P489, DOI 10.1109/TIT.2005.862083; Candes EJ, 2006, IEEE T INFORM THEORY, V52, P5406, DOI 10.1109/TIT.2006.885507; Zou H, 2006, J AM STAT ASSOC, V101, P1418, DOI 10.1198/016214506000000735; Tsaig Y, 2006, SIGNAL PROCESS, V86, P549, DOI 10.1016/j.sigpro.2005.05.029; Efron B, 2004, ANN STAT, V32, P407; Donoho DL, 2006, COMMUN PUR APPL MATH, V59, P797, DOI 10.1002/cpa.20132; Elad M, 2002, IEEE T INFORM THEORY, V48, P2558, DOI 10.1109/TIT.2002.801410; Bertsekas D., 1999, NONLINEAR PROGRAMMIN; CANDES E, 2006, P INT C MAHT; Conn A.R., 1992, SPRINGER SERIES COMP, V17; DEMBO RS, 1983, MATH PROGRAM, V26, P190, DOI 10.1007/BF02592055; Demmel J. W., 1997, APPL NUMERICAL LINEA; Donoho DL, 2003, P NATL ACAD SCI USA, V100, P2197, DOI 10.1073/pnas.0437847100; Feuer A, 2003, IEEE T INFORM THEORY, V49, P1579, DOI 10.1109/TIT.2003.811926; GILL P, 1986, 862 SOL STANF U OP R; Hastie T., 2001, ELEMENTS STAT LEARNI; KOH K, 2006, UNPUN INTERIOR POINT; Lang K., 1995, P 12 INT C MACH LEAR, P331; Lee S.-I., 2006, P 21 NAT C ART INT; Lewis A., 2000, CONVEX ANAL NONLINEA; Lustig M, 2006, P 14 ANN M ISMRM; LUSTIG M, 2007, UNPUB APPL COMPRESSE; McCallum A.K., 1996, BOW TOOLKIT STAT LAN; MEINSHAUSEN N, 2006, ANN STAT, V34, P61462; *MOSEK APS, 2002, MOSEK OPT TOOLS VERS; Nesterov Y., 1994, INTERIOR POINT POLYN, V13; NOCEDAL J., 1999, NUMERICAL OPTIMIZATI; PARK M, 2006, UNPUB L 1 REGULARIZA; Polyak B. T., 1987, INTRO OPTIMIZATION; PORTUGAL L, 1994, TRUNCATED PRIMALINFE; Ruszczynski A., 2006, NONLINEAR OPTIMIZATI; Shor N. Z., 1985, SPRINGER SERIES COMP; Tibshirani R, 1997, STAT MED, V16, P385, DOI 10.1002/(SICI)1097-0258(19970228)16:4<385::AID-SIM380>3.0.CO;2-3; TROPP J, 2006, IEEE T INFORM THEORY, V53, P1030; Tropp JA, 2005, IEEE T INFORM THEORY, V51, P1568, DOI 10.1109/TIT.2005.844057; Vandenberghe L., 2004, CONVEX OPTIMIZATION; VANDENBERGHE L, 1995, MATH PROGRAM, V69, P205, DOI 10.1007/BF01585558; Vanderbei R., 1997, LOQO USERS MANUAL VE; Wakin M. B., 2006, P INT C IM PROC ICIP; Wakin M. B., 2006, P PICT COD S PCS; WRIGHT S, 1997, PRIMAL DUAL INTER PO; Ye Y., 1997, INTERIOR POINT ALGOR	50	0	0	0	2	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA			978-0-615-15314-8				2007							221	228				8	Computer Science, Information Systems; Computer Science, Interdisciplinary Applications; Computer Science, Theory & Methods; Engineering, Electrical & Electronic	Computer Science; Engineering	BGV69	WOS:000250812500032		
B	Huber, D; Stadler, N; Falco-Jonasson, L; Dewarrat, F; Talary, M; Caduff, A; Stahel, W			IEEE	Huber, Daniel; Staedler, Nicolas; Falco-Jonasson, Lisa; Dewarrat, Francois; Talary, Mark; Caduff, Andreas; Stahel, Werner			Multi-sensor data fusion for non-invasive continuous glucose monitoring	2007 PROCEEDINGS OF THE 10TH INTERNATIONAL CONFERENCE ON INFORMATION FUSION, VOLS 1-4			English	Proceedings Paper	10th International Conference on Information Fusion	JUL, 2007	Quebec City, CANADA			statistical modeling; multiple sensors; glucose monitoring	REGRESSION; LASSO	Measurements of impedance spectra used for non-invasive glucose monitoring are affected by a variety of perturbing factors such as temperature and sweat/moisture fluctuations, changes in perfusion, and body movements. In order to quantify and compensate for these perturbing effects, a multi-sensor approach was suggested. Different sensors are used, measuring signals correlated with blood glucose and perturbing factors, respectively. Here, we investigate how the multiple sensor data can be transformed into meaningful information about changes in the concentration of blood glucose. Linear regression models and variable selection (stepwise for/back-ward and lasso) techniques are used to derive generally valid models allowing for the estimation of blood glucose concentration. We find that over-fining is best avoided by using a special version of cross-validated prediction error as the model selection criterion Indeed, the resulting models are reasonably small, plausible, and comprise an additive adjustment for the experimental run.	[Huber, Daniel; Falco-Jonasson, Lisa; Dewarrat, Francois; Talary, Mark; Caduff, Andreas] Solianis Monitoring AG, R&D Dept, Zurich, Switzerland	Caduff, A (reprint author), Solianis Monitoring AG, R&D Dept, Zurich, Switzerland.						Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Zou H, 2006, J AM STAT ASSOC, V101, P1418, DOI 10.1198/016214506000000735; Efron B, 2004, ANN STAT, V32, P407; CADUFF A, 2006, BIOSENS BIOELECT MAR; CADUFF A, 2007, INSTR MEAS IN PRESS; Caduff A, 2003, BIOSENS BIOELECTRON, V19, P209, DOI 10.1016/S0956-5663(03)00196-9; Forst T, 2006, DIABETES TECHNOL THE, V8, P94, DOI 10.1089/dia.2006.8.94; Hastie T., 2001, SPRINGER SERIES STAT; HEINEMANN L, 1994, HORM METAB RES, V26, P579, DOI 10.1055/s-2007-1001763; Pfützner Andreas, 2004, Diabetes Technol Ther, V6, P435, DOI 10.1089/1520915041705839; TALARY MS, 2007, J NONCRISTALINE SOLI	11	0	0	0	0	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA			978-0-662-47830-0				2007							963	972				10	Computer Science, Information Systems; Computer Science, Theory & Methods; Remote Sensing	Computer Science; Remote Sensing	BHW08	WOS:000256950200132		
S	Sha, F; Park, YA; Saul, LK		Berthold, MR; ShaweTaylor, J; Lavrac, N		Sha, Fei; Park, Y. Albert; Saul, Lawrence K.			Multiplicative updates for L-1-regularized linear and logistic regression	ADVANCES IN INTELLIGENT DATA ANALYSIS VII, PROCEEDINGS	LECTURE NOTES IN COMPUTER SCIENCE		English	Proceedings Paper	7th International Symposium on Intelligent Data Analysis	SEP 06-08, 2007	Ljubljana, SLOVENIA				ALGORITHMS	Multiplicative update rules have proven useful in many areas of machine learning. Simple to implement, guaranteed to converge, they account in part for the widespread popularity of algorithms such as nonnegative matrix factorization and Expectation-Maximization. In this paper, we show how to derive multiplicative updates for problems in L-1-regularized linear and logistic regression. For L-1-regularized linear regression, the updates are derived by reformulating the required optimization as a problem in nonnegative quadratic programming (NQP). The dual of this problem, itself an instance of NQP, can also be solved using multiplicative updates; moreover, the observed duality gap can be used to bound the error of intermediate solutions. For L-1-regularized logistic regression, we derive similar updates using an iteratively reweighted least squares approach. We present illustrative experimental results and describe efficient implementations for large-scale problems of interest (e.g., with tens of thousands of examples and over one million features).	Univ Calif Berkeley, Div Comp Sci, Berkeley, CA 94720 USA	Sha, F (reprint author), Univ Calif Berkeley, Div Comp Sci, Berkeley, CA 94720 USA.						Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Krishnapuram B, 2005, IEEE T PATTERN ANAL, V27, P957, DOI 10.1109/TPAMI.2005.127; Sha F, 2007, NEURAL COMPUT, V19, P2004, DOI 10.1162/neco.2007.19.8.2004; Lee DD, 2001, ADV NEUR IN, V13, P556; Efron B, 2004, ANN STAT, V32, P407; DIEGO JM, 2007, COMBINED RECONSTRUCT; Jaakkola TS, 2000, STAT COMPUT, V10, P25, DOI 10.1023/A:1008932416310; Kivinen J, 1997, INFORM COMPUT, V132, P1, DOI 10.1006/inco.1996.2612; Koh KM, 2007, J MACH LEARN RES, V8, P1519; LEE S, 2006, P 21 NAT C ART INT B; LIN Y, 2006, P INT C AC SPEECH SI, V5, P605; McCallum A.K., 1996, BOW TOOLKIT STAT LAN; Ng A. Y., 2004, P 21 INT C MACH LEAR, P78, DOI DOI 10.1145/1015330.1015435; Sha F., 2003, P 16 ANN C COMP LEAR, P188; Vandenberghe L., 2004, CONVEX OPTIMIZATION; Vapnik V., 1998, STAT LEARNING THEORY; Wright SJ, 1997, PRIMAL DUAL INTERIOR	17	0	0	0	0	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		978-3-540-74824-3	LECT NOTES COMPUT SC			2007	4723						13	24				12	Computer Science, Information Systems; Computer Science, Theory & Methods; Statistics & Probability	Computer Science; Mathematics	BGW02	WOS:000250855900002		
S	Weinshall, D; Zamir, L		Bebis, G; Boyle, R; Parvin, B; Koracin, D; Paragios, N; Tanveer, SM; Ju, T; Liu, Z; Coquillart, S; CruzNeira, C; Muller, T; Malzbender, T		Weinshall, Daphna; Zamir, Lior			Image classification from small sample, with distance learning and feature selection	ADVANCES IN VISUAL COMPUTING, PROCEEDINGS, PT 2	LECTURE NOTES IN COMPUTER SCIENCE		English	Proceedings Paper	3rd International Symposium on Visual Computing	NOV 26-28, 2007	Lake Tahoe, NV	UNR Comp Vis Lab, Desert Res Inst, Berkeley Lab, NASA, SIEMENS, Intel Corp, DigitalPersona, EQUINOX, hp, Ford, MITSUBISHI, Utopia Compress		feature selection; distance learning; small sample; L1 regularization		Small sample is an acute problem in many application domains. which may be partially addressed by feature selection or dimensionality reduction. For the purpose of distance learning, we describe a method for feature selection using equivalence constraints between pairs of datapoints. The method is based on L1. regularization and optimization. Feature selection is then incorporated into an existing nonparametric method for distance learning, which is based on the boosting of constrained generative models. Thus the final algorithm employs dynamical feature selection, where features are selected anew in each boosting iteration based on the weighted training data. We tested our algorithm on the classification of facial images, using two public domain databases. We show the results of extensive experiments where our method performed much better than a number of competing methods, including the original boosting-based distance learning method and two commonly used Mahalanobis metrics.	[Weinshall, Daphna; Zamir, Lior] Hebrew Univ Jerusalem, Sch Engn & Comp Sci, IL-91904 Jerusalem, Israel	Weinshall, D (reprint author), Hebrew Univ Jerusalem, Sch Engn & Comp Sci, IL-91904 Jerusalem, Israel.						Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Bart E, 2005, PROC CVPR IEEE, P672; ATHITSOS V, 2004, P CVPR; BILENKO M, 2004, ACM INT C P SERIES; CHANG H, 2004, ACM INT C P SERIES; De Bie T, 2003, LECT NOTES ARTIF INT, V2842, P175; Donoho DL, 2003, P NATL ACAD SCI USA, V100, P2197, DOI 10.1073/pnas.0437847100; Ferencz A., 2005, Proceedings. Tenth IEEE International Conference on Computer Vision; GEORGHIADES AS, 2000, IEEE INT C AUT FAC G, P277; Goldberger J., 2005, ADV NEURAL INFORM PR, V17; Hertz T., 2004, ICML; Hertz T., 2006, ICML; Li FF, 2006, IEEE T PATTERN ANAL, V28, P594, DOI 10.1109/TPAMI.2006.79; NG AY, 2004, ACM INT C P SERIES; SHENTAL N, 2002, LNCS, V2353; Shental N., 2003, NIPS; SUDDERTH E, 2005, P ICCV; TSYMBAL A, 2001, INT ICSC CIMA; ZHENG AX, 2003, ADV NEURAL INFORM PR, V17	19	0	0	0	2	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		978-3-540-76855-5	LECT NOTES COMPUT SC			2007	4842						106	115				10	Computer Science, Artificial Intelligence; Computer Science, Theory & Methods	Computer Science	BHA14	WOS:000251785200011		
S	Gao, JB; Xu, RY		Orgun, MA; Thornton, J		Gao, Junbin; Xu, Richard Y.			Mixture of the robust L1 distributions and its applications	AI 2007: ADVANCES IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS	LECTURE NOTES IN COMPUTER SCIENCE		English	Proceedings Paper	20th Australian Joint Conference on Artificial Intelligence	DEC 02-06, 2007	Gold Coast, AUSTRALIA	Griffith Univ, Natl ICT Australia, Univ Queensland, Queensland Univ Techynol, Bond Univ, Australian Comp Soc, Gold Coast City Council			MODELS	Recently a robust probabilistic L1-PCA model was introduced in [1] by replacing the conventional Gaussian noise model with the Laplacian L1 model. Due to the heavy tail characteristics of the L1 distribution, the proposed model is more robust against data outliers. In this paper, we generalized the L1-PCA into a mixture of L1-distributions so that the model can be used for possible multiclustering data. For the model learning we use the property that the L1 density can be expanded as a superposition of infinite number of Gaussian densities to include a tractable Bayesian learning and inference based on the variational EM-type algorithm.	[Gao, Junbin; Xu, Richard Y.] Charles Sturt Univ, Sch Acc & Copm Sci, Bathurst, NSW 2795, Australia	Gao, JB (reprint author), Charles Sturt Univ, Sch Acc & Copm Sci, Bathurst, NSW 2795, Australia.						Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1; Tipping ME, 1999, NEURAL COMPUT, V11, P443, DOI 10.1162/089976699300016728; Jordan MI, 2004, STAT SCI, V19, P140, DOI 10.1214/088342304000000026; ARCHAMBEAU C, 2005, THESIS U CATHOLIQUE; Bhowmick D, 2006, BIOSTATISTICS, V7, P630, DOI 10.1093/biostatistics/kxj032; GAO J, 2008, IN PRESS NEURAL COMP; Gao JB, 2002, LECT NOTES ARTIF INT, V2557, P395; Guo Y, 2006, LECT NOTES COMPUT SC, V4304, P1179; Guo Y, 2006, AIDM 2006: International Workshop on Integrating AI and Dating Mining, P11; Jolliffe I. T., 2002, PRINCIPAL COMPONENT; Ng A. Y., 2004, P INT C MACH LEARN; Park H. J., 2005, ADV NEURAL INFORM PR, V17, P1041; Peel D, 2000, STAT COMPUT, V10, P339, DOI 10.1023/A:1008981510081; Peel D, 2000, FINITE MIXTURE MODEL; PONTIL M, 1998, 1651 MIT AI LAB; RIDDER DD, 2003, BMVC 2003, P319; Tipping ME, 2005, NEUROCOMPUTING, V69, P123, DOI 10.1016/j.neucom.2005.02.016; Verbeek J, 2006, IEEE T PATTERN ANAL, V28, P1236, DOI 10.1109/TPAMI.2006.166; Zou H., 2004, SPARSE PRINCIPAL COM	20	2	2	0	1	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		978-3-540-76926-2	LECT NOTES COMPUT SC			2007	4830						26	35				10	Computer Science, Artificial Intelligence	Computer Science	BHC44	WOS:000252180000005		
J	Stout, F; Kalivas, JH				Stout, Forrest; Kalivas, John H.			Evaluation of multivariate calibration using a Tikhonov regularization approach and the generalized pair-correlation method with nonlinear data	ANALYTICAL LETTERS			English	Article						Tikhonov regularization; pair-correlation method; nonlinear calibration; variable selection	TRANSFORMED ORIGINAL VARIABLES; WAVELENGTH SELECTION; PARETO CALIBRATION; SPECTROSCOPIC DATA; MODEL SELECTION; MEAN SUBSET; REGRESSION; SHRINKAGE; LASSO	In order to reduce data nonlinearity and overfitting with the multivariate calibration model y = Xb, a modified Tikhonov regularization (TR) algorithm is evaluated for selecting key variables from an X augmented with extra columns that contain the original measured variables (x(ij)(2)) as squared terms (x(ij)) and other orders. The TR approach simultaneously develops the multivariate calibration model. The new generalized pair-correlation method (GPCM) is also studied for variable selection followed by partial least squares (PLS) for multivariate calibration. Results from synthetic spectral data are compared when using the modified TR approach, GPCM, and PLS without variable selection. The GPCM usually performs slightly better than the TR approach for tabulated bias and variance measures and in some cases, at a sacrifice to parsimony. The method of PLS without variable selection performs the worst. By using synthetic spectral data sets, how the methods work could be studied. Thus, results from this study will aid investigators of real spectral data sets exhibiting nonlinear behavior.	Idaho State Univ, Dept Chem, Pocatello, ID 83209 USA	Kalivas, JH (reprint author), Idaho State Univ, Dept Chem, Pocatello, ID 83209 USA.	kalijohn@isu.edu	Rohlf, F/A-8710-2008				Anderson KJ, 2003, APPL SPECTROSC, V57, P309, DOI 10.1366/000370203321558227; Fu WJJ, 1998, J COMPUT GRAPH STAT, V7, P397, DOI 10.2307/1390712; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; FRANK IE, 1993, TECHNOMETRICS, V35, P109, DOI 10.2307/1269656; SHAO J, 1993, J AM STAT ASSOC, V88, P486, DOI 10.2307/2290328; BARNETT HA, 1960, ANAL CHEM, V32, P1153, DOI 10.1021/ac60165a031; Berglund A, 1997, J CHEMOMETR, V11, P141, DOI 10.1002/(SICI)1099-128X(199703)11:2<141::AID-CEM461>3.0.CO;2-2; CLAERBOU.JF, 1973, GEOPHYSICS, V38, P826, DOI 10.1190/1.1440378; CLEMENTI S, 1989, Journal of Chemometrics, V3, P499, DOI 10.1002/cem.1180030307; Dax A, 1992, SIAM J OPTIMIZ, V2, P602, DOI 10.1137/0802029; DiFoggio R, 2005, J CHEMOMETR, V19, P203, DOI 10.1002/cem.925; ELIERS PHC, 2003, ANAL CHEM, V75, P3631; ESTEBANDIEZ I, 2005, ANAL CHIM ACTA, V555, P96; FABER J, 1997, J CHEMOMETR, V11, P181; Forrester JB, 2004, J CHEMOMETR, V18, P372, DOI 10.1002/cem.883; FRANK IE, 1995, CHEMOMETR INTELL LAB, V27, P1, DOI 10.1016/0169-7439(94)00005-4; GEMPERLINE PJ, 1992, CHEMOMETR INTELL LAB, V15, P115, DOI 10.1016/0169-7439(92)85002-K; GEMPERLINE PJ, 1991, ANAL CHEM, V63, P2313, DOI 10.1021/ac00020a022; Green RL, 2002, CHEMOMETR INTELL LAB, V60, P173, DOI 10.1016/S0169-7439(01)00194-0; Hansen P., 1998, RANK DEFICIENT DISCR; Hansen PC, 1996, NUMER LINEAR ALGEBR, V3, P513, DOI 10.1002/(SICI)1099-1506(199611/12)3:6<513::AID-NLA93>3.0.CO;2-4; Hastie T., 2001, ELEMENTS STAT LEARNI; Heberger K, 2002, J CHEMOMETR, V16, P436, DOI 10.1002/cem.748; HEBERGER K, 1997, QUANTITATIVE STRUCTU, V7, P423; Kalivas JH, 2004, ANAL CHIM ACTA, V505, P9, DOI 10.1016/S0003-2670(02)01603-3; Kalivas JH, 2001, APPL SPECTROSC, V55, P1645, DOI 10.1366/0003702011953955; KVALHEIM OM, 1989, ANAL CHIM ACTA, V223, P53, DOI 10.1016/S0003-2670(00)84074-X; MARIS MA, 1983, ANAL CHEM, V55, P1694, DOI 10.1021/ac00261a013; Miller C. E., 1993, NIR NEWS, V4, P3; Naes T., 2002, USER FRIENDLY GUIDE; Ojelund H, 2001, J CHEMOMETR, V15, P497; Ojelund H, 2002, TECHNOMETRICS, V44, P369, DOI 10.1198/004017002188618563; Ojelund H, 2002, APPL SPECTROSC, V56, P887, DOI 10.1366/000370202760171563; Robertsson G, 2001, APPL SPECTROSC, V55, P98, DOI 10.1366/0003702011951317; SANTOSA F, 1986, SIAM J SCI STAT COMP, V7, P1307, DOI 10.1137/0907087; Seipel HA, 2004, J CHEMOMETR, V18, P306, DOI 10.1002/cem.874; Sekulic S., 1993, ANAL CHEM, V65, P835; Stout F, 2006, J CHEMOMETR, V20, P22, DOI 10.1002/cem.975; TAYLOR HL, 1979, GEOPHYSICS, V44, P39, DOI 10.1190/1.1440921; TILLMANN P, 2000, J NEAR INFRARED SPEC, V8, P103; van de Geer S., 2001, MATH METH STAT, V10, P355; Verdu-Andres J, 1999, ANAL CHIM ACTA, V389, P115, DOI 10.1016/S0003-2670(99)00152-X; VerduAndres J, 1997, ANAL CHIM ACTA, V349, P271, DOI 10.1016/S0003-2670(97)00271-7; WESTERHAUS MO, 1990, P 3 INTL C NEAR INFR; Ye JM, 1998, J AM STAT ASSOC, V93, P120, DOI 10.2307/2669609; Yeow YL, 2005, APPL SPECTROSC, V59, P584, DOI 10.1366/0003702053946056	47	1	1	0	3	TAYLOR & FRANCIS INC	PHILADELPHIA	325 CHESTNUT ST, SUITE 800, PHILADELPHIA, PA 19106 USA	0003-2719			ANAL LETT	Anal. Lett.		2007	40	6					1227	1251		10.1080/00032710701298529		25	Chemistry, Analytical	Chemistry	171NV	WOS:000246743600017		
J	Lobo, MS; Fazel, M; Boyd, S				Sousa Lobo, Miguel; Fazel, Maryam; Boyd, Stephen			Portfolio optimization with linear and fixed transaction costs	ANNALS OF OPERATIONS RESEARCH			English	Article						portfolio optimization; transaction costs; convex programming	SELECTION; RISK; ALGORITHM	We consider the problem of portfolio selection, with transaction costs and constraints on exposure to risk. Linear transaction costs, bounds on the variance of the return, and bounds on different shortfall probabilities are efficiently handled by convex optimization methods. For such problems, the globally optimal portfolio can be computed very rapidly. Portfolio optimization problems with transaction costs that include a fixed fee, or discount breakpoints, cannot be directly solved by convex optimization. We describe a relaxation method which yields an easily computable upper bound via convex optimization. We also describe a heuristic method for finding a suboptimal portfolio, which is based on solving a small number of convex optimization problems (and hence can be done efficiently). Thus, we produce a suboptimal solution, and also an upper bound on the optimal solution. Numerical experiments suggest that for practical problems the gap between the two is small, even for large problems involving hundreds of assets. The same approach can be used for related problems, such as that of tracking an index with a portfolio consisting of a small number of assets.	Duke Univ, Fuqua Sch Business, Durham, NC 27705 USA; CALTECH, Control & Dynam Syst Dept, Pasadena, CA 91125 USA; Stanford Univ, Informat Syst Lab, Stanford, CA 94305 USA	Lobo, MS (reprint author), Duke Univ, Fuqua Sch Business, Durham, NC 27705 USA.	mlobo@duke.edu; maryam@cds.caltech.edu; boyd@stanford.edu	Sousa Lobo, Miguel/B-3850-2010				Alizadeh F., 1997, SDPPACK USERS GUIDE; ANDERSEN E, 1999, MOSEK V1 0B USERS MA; Delaney AH, 1998, IEEE T IMAGE PROCESS, V7, P204, DOI 10.1109/83.660997; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; KARMARKAR N, 1984, COMBINATORICA, V4, P373, DOI 10.1007/BF02579150; Kellerer H, 2000, ANN OPER RES, V99, P287, DOI 10.1023/A:1019279918596; LINTNER J, 1965, REV ECON STAT, V47, P13, DOI 10.2307/1924119; LAWLER EL, 1966, OPER RES, V14, P699, DOI 10.1287/opre.14.4.699; MEYER RR, 1976, J COMPUT SYST SCI, V12, P108, DOI 10.1016/S0022-0000(76)80021-9; Lobo MS, 1998, LINEAR ALGEBRA APPL, V284, P193, DOI 10.1016/S0024-3795(98)10032-0; Bertsimas D, 1999, INTERFACES, V29, P49, DOI 10.1287/inte.29.1.49; BLOG B, 1983, MANAGE SCI, V29, P792, DOI 10.1287/mnsc.29.7.792; Chen S. S., 2001, SIAM REV, V43; Fazel M., 2002, THESIS STANFORD U; FIACCO A. V., 1968, NONLINEAR PROGRAMMIN; Leibowitz M. L., 1996, RETURN TARGETS SHORT; LOBO MS, 1997, SOCP SOFTWARE 2 ORDE; Lucas A, 1998, J PORTFOLIO MANAGE, V25, P71, DOI 10.3905/jpm.1998.409657; Luenberger D. G., 1998, INVESTMENT SCI; Markowitz H, 1952, J FINANC, V7, P77, DOI 10.2307/2975974; Markowitz H., 1959, PORTFOLIO SELECTION; Nemirovski A., 2001, LECT MODERN CONVEX O; Nesterov Y.E., 1994, STUDIES APPL MATH, V13; PATEL NR, 1982, MANAGE SCI, V28, P303, DOI 10.1287/mnsc.28.3.303; PEROLD AF, 1984, MANAGE SCI, V30, P1143, DOI 10.1287/mnsc.30.10.1143; ROY AD, 1952, ECONOMETRICA, V20, P413; RUDOLPH M, 1994, ALGORITHMS PORTFOLIO; Schattman J. B., 2000, THESIS RUTGERS U; Schrijver A., 1986, WILEY INTERSCIENCE S; SHARPE WF, 1964, J FINANC, V19, P425, DOI 10.2307/2977928; STURM JF, 1999, OPTIM METHOD SOFTW, P625; Telser L. G., 1955, REV ECON STUD, V23, P1, DOI 10.2307/2296146; Vandenberghe L, 1996, SIAM REV, V38, P49, DOI 10.1137/1038003; Vandenberghe L., 2004, CONVEX OPTIMIZATION	34	52	54	4	8	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0254-5330			ANN OPER RES	Ann. Oper. Res.		2007	152						341	365		10.1007/s10479-006-0145-1		25	Operations Research & Management Science	Operations Research & Management Science	157WA	WOS:000245750900015		
J	Stout, F; Kalivas, JH; Heberger, K				Stout, Forrest; Kalivas, John H.; Heberger, Karoly			Wavelength selection for multivariate calibration using Tikhonov regularization	APPLIED SPECTROSCOPY			English	Article						wavelength selection; multivariate calibration; Tikhonov regularization	PARTIAL LEAST-SQUARES; LATENT VARIABLE REGRESSION; SYSTEMATIC EVALUATION; PREDICTION; SPECTRA; VALIDATION; SHRINKAGE; ERROR; LASSO; ALGORITHM	Prediction of sample properties using spectroscopic data with multivariate calibration is often enhanced by wavelength selection. This paper reports on a built-in wavelength selection method in which the estimated regression vector contains zero to near-zero coefficients for undesirable wavelengths. The method is based on Tikhonov regularization with the model 1-norm (TR1) and is applied to simulated and near-infrared (NIR) spectral data. Models are also formed from wavelength subsets determined by the standard method of stepwise regression (SWR). Harmonious (bias/variance tradeoff) and parsimonious considerations are compared with and without wavelength selection for principal component regression (PCR), ridge regression (RR), partial least squares (PLS), and multiple linear regression (MLR). Results show that TR1 models generally contain large baseline regions of near-zero coefficients, thereby essentially achieving built-in wavelength selection. For example, wavelengths with spectral interferences and/or poor signal-to-noise ratios obtain near zero regression coefficients. Results often improve with TR1 models, compared to full wavelength PCR, RR, and PLS models. The SWR subset results are similar to those for the TR1 models using the NIR data and worse with the simulated spectral situations. In general, wavelength selection improves prediction accuracy at a sacrifice to a potential increase in variance and the parsimony remains nearly equivalent compared to full wavelength models. New insights gained from the reported studies provide useful guidelines on when to use full wavelengths or use wavelength selection methods. Specifically, when a small number of large wavelength effects (good sensitivity and selectivity) exist, subset selection by SWR (with caution) and TR1 do well. With a small to moderate number of large to moderate sized wavelength effects, TR1 is better. Lastly when a large number of small effects are present, full wavelengths with the methods of PCR, RR, or PLS are best.	Idaho State Univ, Dept Chem, Pocatello, ID 83209 USA; Hungarian Acad Sci, Chem Res Ctr, Inst Chem, H-1525 Budapest, Hungary	Kalivas, JH (reprint author), Idaho State Univ, Dept Chem, Pocatello, ID 83209 USA.	kalijohn@isu.edu	Heberger, Karoly/A-4195-2011	Heberger, Karoly/0000-0003-0965-939X			Fu WJJ, 1998, J COMPUT GRAPH STAT, V7, P397, DOI 10.2307/1390712; Knight K, 2000, ANN STAT, V28, P1356; Baumann K, 2002, J CHEMOMETR, V16, P339, DOI 10.1002/cem.730; Chong IG, 2005, CHEMOMETR INTELL LAB, V78, P103, DOI 10.1016/j.chemolab.2004.12.011; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; Baumann K, 2003, TRAC-TREND ANAL CHEM, V22, P395, DOI 10.1016/S0165-9936(03)00607-1; Eilers PHC, 2003, ANAL CHEM, V75, P3631, DOI 10.1021/ac034173t; FRANK IE, 1993, TECHNOMETRICS, V35, P109, DOI 10.2307/1269656; Zou H, 2005, J ROY STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x; Leardi R, 1998, CHEMOMETR INTELL LAB, V41, P195, DOI 10.1016/S0169-7439(98)00051-3; Turlach BA, 2005, TECHNOMETRICS, V47, P349, DOI 10.1198/004017005000000139; Kalivas JH, 1997, CHEMOMETR INTELL LAB, V37, P255, DOI 10.1016/S0169-7439(97)00038-5; Baumann K, 2002, J CHEMOMETR, V16, P351, DOI 10.1002/cem.729; Baumann K, 2004, J COMPUT AID MOL DES, V18, P549, DOI 10.1007/s10822-004-4071-5; Brenchley JM, 1997, APPL SPECTROSC, V51, P689, DOI 10.1366/0003702971940837; CLAERBOU.JF, 1973, GEOPHYSICS, V38, P826, DOI 10.1190/1.1440378; Dax A, 1992, SIAM J OPTIMIZ, V2, P602, DOI 10.1137/0802029; de Groot PJ, 2003, APPL SPECTROSC, V57, P642, DOI 10.1366/000370203322005328; DiFoggio R, 2005, J CHEMOMETR, V19, P203, DOI 10.1002/cem.925; Faber K, 1996, CHEMOMETR INTELL LAB, V34, P283, DOI 10.1016/0169-7439(96)00022-6; Faber K, 1997, J CHEMOMETR, V11, P181, DOI 10.1002/(SICI)1099-128X(199705)11:3<181::AID-CEM459>3.0.CO;2-7; Faber NM, 2003, TRAC-TREND ANAL CHEM, V22, P330, DOI 10.1016/S0165-9936(03)00503-X; Forrester JB, 2004, J CHEMOMETR, V18, P372, DOI 10.1002/cem.883; Griffiths ML, 2002, J ANAL ATOM SPECTROM, V17, P800, DOI 10.1039/b203239m; Hansen P., 1998, RANK DEFICIENT DISCR; Hansen PC, 1996, NUMER LINEAR ALGEBR, V3, P513, DOI 10.1002/(SICI)1099-1506(199611/12)3:6<513::AID-NLA93>3.0.CO;2-4; Hastie T., 2001, ELEMENTS STAT LEARNI; JIANG J, 2003, ANAL CHEM, V74, P3555; Kalivas JH, 1999, J CHEMOMETR, V13, P111, DOI 10.1002/(SICI)1099-128X(199903/04)13:2<111::AID-CEM532>3.0.CO;2-N; Kalivas JH, 2004, ANAL CHIM ACTA, V505, P9, DOI 10.1016/S0003-2670(02)01603-3; Kalivas JH, 2006, PRACTICAL GUIDE CHEM; Kalivas JH, 2001, APPL SPECTROSC, V55, P1645, DOI 10.1366/0003702011953955; Kalivas JH, 2005, J CHEMOMETR, V19, P64, DOI 10.1002/cem.905; Lawson C., 1995, SOLVING LEAST SQUARE; Lorber A., 1988, J CHEMOMETR, V2, P67, DOI DOI 10.1002/CEM.1180020108; Mark H., 1991, PRINCIPLES PRACTICE; MARK H, 1988, APPL SPECTROSC, V42, P1427, DOI 10.1366/0003702884429661; Naes T., 2002, USER FRIENDLY GUIDE; Ojelund H, 2001, J CHEMOMETR, V15, P497; Ojelund H, 2002, TECHNOMETRICS, V44, P369, DOI 10.1198/004017002188618563; Pierna JAF, 2003, CHEMOMETR INTELL LAB, V65, P281; SANTOSA F, 1986, SIAM J SCI STAT COMP, V7, P1307, DOI 10.1137/0907087; Seipel HA, 2004, J CHEMOMETR, V18, P306, DOI 10.1002/cem.874; STOUT F, 2006, IN PRESS ANAL LETT; STOUT F, 2006, IN PRESS J CHEMOM; TAYLOR HL, 1979, GEOPHYSICS, V44, P39, DOI 10.1190/1.1440921; TILLMANN P, 2000, J NEAR INFRARED SPEC, V8, P103; TOPLISS JG, 1979, J MED CHEM, V22, P1238, DOI 10.1021/jm00196a017; Vach K, 2001, STAT NEERL, V55, P53, DOI 10.1111/1467-9574.00156; van de Geer S., 2001, MATH METH STAT, V10, P355; Van der Voet H, 1999, J CHEMOMETR, V13, P195, DOI 10.1002/(SICI)1099-128X(199905/08)13:3/4<195::AID-CEM540>3.0.CO;2-L; Weisberg S., 2005, APPL LINEAR REGRESSI; Westerhaus M.O., 1990, P 3 INT C NEAR INFR, P671; Wise B. M., 2003, PLS TOOLBOX 3 0 USE; Ye JM, 1998, J AM STAT ASSOC, V93, P120, DOI 10.2307/2669609; Yeow YL, 2005, APPL SPECTROSC, V59, P584, DOI 10.1366/0003702053946056	57	23	24	1	7	SOC APPLIED SPECTROSCOPY	FREDERICK	201B BROADWAY ST, FREDERICK, MD 21701 USA	0003-7028			APPL SPECTROSC	Appl. Spectrosc.	JAN	2007	61	1					85	95		10.1366/000370207779701479		11	Instruments & Instrumentation; Spectroscopy	Instruments & Instrumentation; Spectroscopy	131GP	WOS:000243857100013	17311721	
S	Tikka, J		MarquesDeSa, J; Alexandre, LA; Duch, W; Mandic, DP		Tikka, Jarkko			Input selection for radial basis function networks by constrained optimization	Artificial Neural Networks - ICANN 2007, Pt 1, Proceedings	LECTURE NOTES IN COMPUTER SCIENCE		English	Proceedings Paper	17th International Conference on Artificial Neural Networks (ICANN 2007)	SEP 09-13, 2007	Oporto, PORTUGAL	European Neural Networks Soc, Int Neural Networks Soc, Japanese Neural Network Soc, IEEE Comp Intelligence Soc, European Assoc Signal & Image Proc, Inst Engenhar Biomed, Univ Beira Interior, Inst Super Engenhar Porto, Reitor Univ Porto, UP, Dept Engenhar Elect Computadores, Inst Politecn Porto, Fund Cienc Tecnol, Fund Luso-Amer Desenvolvimento, Fund Calouste Gulbenkian, Microsoft Res Cambridge Lab, Portugal Telecom			APPROXIMATION	Input selection in the nonlinear function approximation is important and difficult problem. Neural networks provide good generalization in many cases, but their interpretability is usually limited. However, the contributions of input variables in the prediction of output would be valuable information in many real world applications. In this work, an input selection algorithm for Radial basis function networks is proposed. The selection of input variables is achieved using a constrained cost function, in which each input dimension is weighted. The constraints are imposed on the values of weights. The proposed algorithm solves a log-barrier reformulation of the original optimization problem. The input selection algorithm was applied to both simulated and benchmark data and obtained results were compelling.	Helsinki Univ Technol, Lab Comp & Informat Sci, FI-02015 Espoo, Finland	Tikka, J (reprint author), Helsinki Univ Technol, Lab Comp & Informat Sci, POB 5400, FI-02015 Espoo, Finland.						Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Tikka J, 2006, LECT NOTES COMPUT SC, V4132, P161; Kohavi R, 1997, ARTIF INTELL, V97, P273, DOI 10.1016/S0004-3702(97)00043-X; BAZARAA M. S., 1979, NONLINEAR PROGRAMMIN; Benoudjit N, 2003, NEURAL PROCESS LETT, V18, P139, DOI 10.1023/A:1026289910256; Bishop C. M., 1995, NEURAL NETWORKS PATT; Guyon I., 2003, Journal of Machine Learning Research, V3, DOI 10.1162/153244303322753616; Herrera LJ, 2006, LECT NOTES COMPUT SC, V4131, P41; Bi J., 2003, Journal of Machine Learning Research, V3, DOI 10.1162/153244303322753643; MOODY J, 1989, NEURAL COMPUT, V1, P218; Norgaard M., 2003, NEURAL NETWORKS MODE; ORR MJL, 1995, NEURAL COMPUT, V7, P606, DOI 10.1162/neco.1995.7.3.606; PARK J, 1993, NEURAL COMPUT, V5, P305, DOI 10.1162/neco.1993.5.2.305; Rakotomamonjy A., 2003, Journal of Machine Learning Research, V3, DOI 10.1162/153244303322753706; Valls JM, 2005, LECT NOTES COMPUT SC, V3512, P257; WESTON J, 2000, ADV NEURAL INFORM PR, P13	16	3	3	0	1	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		978-3-540-74689-8	LECT NOTES COMPUT SC			2007	4668		I				239	248				10	Computer Science, Artificial Intelligence; Computer Science, Theory & Methods	Computer Science	BGS58	WOS:000250338200025		
J	Guo, YQ; Hastie, T; Tibshirani, R				Guo, Yaqian; Hastie, Trevor; Tibshirani, Robert			Regularized linear discriminant analysis and its application in microarrays	BIOSTATISTICS			English	Article						classification; discriminant analysis; microarray; prediction analysis of microarrays (PAM); regularization; shrunken centriods	CLASS PREDICTION; REGRESSION; CLASSIFICATION; SELECTION; CANCER; BIAS	In this paper, we introduce a modified version of linear discriminant analysis, called the "shrunken centroids regularized discriminant analysis" (SCRDA). This method generalizes the idea of the "nearest shrunken centroids" (NSC) (Tibshirani and others, 2003) into the classical discriminant analysis. The SCRDA method is specially designed for classification problems in high dimension low sample size situations, for example, microarray data. Through both simulated data and real life data, it is shown that this method performs very well in multivariate classification problems, often outperforms the PAM method (using the NSC algorithm) and can be as competitive as the support vector machines classifiers. It is also suitable for feature elimination purpose and can be used as gene selection method. The open source R package for this method (named "rda") is available on CRAN (http://www.r-project.org) for download and testing.	Stanford Univ, Dept Stat, Stanford, CA 94305 USA; Stanford Univ, Dept Hlth Res & Policy, Stanford, CA 94305 USA	Guo, YQ (reprint author), Stanford Univ, Dept Stat, Stanford, CA 94305 USA.	yaqiang@stanford.edu					Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Golub TR, 1999, SCIENCE, V286, P531, DOI 10.1126/science.286.5439.531; HOERL AE, 1970, TECHNOMETRICS, V12, P55; Zou H, 2005, J ROY STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x; Tibshirani R, 2003, STAT SCI, V18, P104, DOI 10.1214/ss/1056397488; Efron B, 2004, ANN STAT, V32, P407; Ramaswamy S, 2001, P NATL ACAD SCI USA, V98, P15149, DOI 10.1073/pnas.211566398; DIPILLO PJ, 1976, COMMUN STAT A-THEOR, V5, P843; DIPILLO PJ, 1977, COMMUN STAT A-THEOR, V6, P933; FRIEDMAN JH, 1989, J AM STAT ASSOC, V84, P165, DOI 10.2307/2289860; Hastie T., 2001, ELEMENTS STAT LEARNI; Zhu J, 2004, BIOSTATISTICS, V5, P427, DOI 10.1093/biostatistics/kxg046	12	119	124	4	12	OXFORD UNIV PRESS	OXFORD	GREAT CLARENDON ST, OXFORD OX2 6DP, ENGLAND	1465-4644			BIOSTATISTICS	Biostatistics	JAN	2007	8	1					86	100		10.1093/biostatistics/kxj035		15	Mathematical & Computational Biology; Statistics & Probability	Mathematical & Computational Biology; Mathematics	115DQ	WOS:000242715400006	16603682	
J	Opgen-Rhein, R; Strimmer, K				Opgen-Rhein, Rainer; Strimmer, Korbinian			Learning causal networks from systems biology time course data: an effective model selection procedure for the vector autoregressive process	BMC BIOINFORMATICS			English	Article; Proceedings Paper	Workshop on Probabilistic Modeling and Machine Learning in Structural and Systems Biology	JUN 17-18, 2006	Tuusula, FINLAND				EMPIRICAL BAYES APPROACH; SHRINKAGE APPROACH; LASSO	Background: Causal networks based on the vector autoregressive (VAR) process are a promising statistical tool for modeling regulatory interactions in a cell. However, learning these networks is challenging due to the low sample size and high dimensionality of genomic data. Results: We present a novel and highly efficient approach to estimate a VAR network. This proceeds in two steps: (i) improved estimation of VAR regression coefficients using an analytic shrinkage approach, and (ii) subsequent model selection by testing the associated partial correlations. In simulations this approach outperformed for small sample size all other considered approaches in terms of true discovery rate (number of correctly identified edges relative to the significant edges). Moreover, the analysis of expression time series data from Arabidopsis thaliana resulted in a biologically sensible network. Conclusion: Statistical learning of large-scale VAR causal models can be done efficiently by the proposed procedure, even in the difficult data situations prevalent in genomics and proteomics. Availability: The method is implemented in R code that is available from the authors on request.	Univ Munich, Dept Stat, D-80539 Munich, Germany; Univ Leipzig, IMISE, D-04107 Leipzig, Germany	Opgen-Rhein, R (reprint author), Univ Munich, Dept Stat, Ludwigstr 33, D-80539 Munich, Germany.	opgen-rhein@stat.uni-muenchen.de; strimmer@uni-leipzig.de	Strimmer, Korbinian/C-1522-2009	Strimmer, Korbinian/0000-0001-7917-2056			Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Wichert S, 2004, BIOINFORMATICS, V20, P5, DOI 10.1093/bioinformatics/btg364; Smith SM, 2004, PLANT PHYSIOL, V136, P2687, DOI 10.1104/pp.104.044347; SIMS CA, 1980, ECONOMETRICA, V48, P1, DOI 10.2307/1912017; EFRON B, 1973, J AM STAT ASSOC, V68, P117, DOI 10.2307/2284155; Opgen-Rhein R, 2007, STAT APPL GENET MOL, V6; GOLUB GH, 1979, TECHNOMETRICS, V21, P215, DOI 10.1080/00401706.1979.10489751; Schafer J, 2005, STAT APPL GENET MO B, V4, DOI 10.2202/1544-6115.1175; Meinshausen N, 2006, ANN STAT, V34, P1436, DOI 10.1214/009053606000000281; Bay SD, 2004, J COMPUT BIOL, V11, P971, DOI 10.1089/1066527042432297; Demiralp S, 2003, OXFORD B ECON STAT, V65, P745, DOI 10.1046/j.0305-9049.2003.00087.x; EFRON B, 2005, TECH REP DEP STAT; GRANGER CWJ, 1980, J ECON DYN CONTROL, V2, P329, DOI 10.1016/0165-1889(80)90069-X; Lutkepohl H., 1993, INTRO MULTIPLE TIME; MONETA A, 2004, TECHNICAL REPORT LAB; Ni S, 2005, J BUS ECON STAT, V23, P105, DOI 10.1198/073500104000000622; OPGENRHEIN R, 2006, P 4 INT WORKSH COMP, V4, P73; Opgen-Rhein R., 2006, REVSTAT-STAT J, V4, P53; Schafer J, 2005, BIOINFORMATICS, V21, P754, DOI 10.1093/bioinformatics/bti062; Whittaker J, 1990, GRAPHICAL MODELS APP	20	47	50	1	11	BIOMED CENTRAL LTD	LONDON	MIDDLESEX HOUSE, 34-42 CLEVELAND ST, LONDON W1T 4LB, ENGLAND	1471-2105			BMC BIOINFORMATICS	BMC Bioinformatics		2007	8			2					S3	10.1186/1471-2105-8-S2-S3		8	Biochemical Research Methods; Biotechnology & Applied Microbiology; Mathematical & Computational Biology	Biochemistry & Molecular Biology; Biotechnology & Applied Microbiology; Mathematical & Computational Biology	169OU	WOS:000246602300003	17493252	
J	Xu, QF; You, JH				Xu, Qinfeng; You, Jinhong			Covariate selection for linear errors-in-variables regression models	COMMUNICATIONS IN STATISTICS-THEORY AND METHODS			English	Article						linear regression model; measurement errors; Newton-Raphson algorithm; nonconcave penalization; oracle property	NONCONCAVE PENALIZED LIKELIHOOD	In this article, we provide a procedure to select the significant covariates of the linear regression models in which some or all covariates are measured with errors. The proposed method is based on the combination of a non concave penalization and a corrected least squares, and it simultaneously selects significant covariates and estimates the unknown regression coefficients. Same as Fan and Li (2001), we show the resulted estimator has an oracle property with a proper choice of regularization parameters and penalty function. Some simulation studies are conducted to illustrate the finite sample performance of the proposed method.	Univ N Carolina, Dept Biostat, Chapel Hill, NC 27599 USA; Fudan Univ, Dept Stat, Shanghai 200433, Peoples R China	You, JH (reprint author), Univ N Carolina, Dept Biostat, Chapel Hill, NC 27599 USA.	jyou@bios.unc.edu					Akaike H., 1973, P 2 INT S INF THEOR, P267; BREIMAN L, 1995, TECHNOMETRICS, V37, P373, DOI 10.2307/1269730; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Antoniadis A, 2001, J AM STAT ASSOC, V96, P939, DOI 10.1198/016214501753208942; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; FRANK IE, 1993, TECHNOMETRICS, V35, P109, DOI 10.2307/1269656; MALLOWS CL, 1973, TECHNOMETRICS, V15, P661, DOI 10.2307/1267380; CAI J, 2005, BIOMETRIKA, V92, P313; Carroll R, 1995, MEASUREMENT ERROR NO; Cheng CL, 2004, J AM STAT ASSOC, V99, P805, DOI 10.1198/016214504000001141; Cheng CL, 1999, STAT REGRESSION MEAS; Fan JQ, 2004, ANN STAT, V32, P928, DOI 10.1214/009053604000000256; Fan JQ, 2002, ANN STAT, V30, P74; Fan JQ, 2004, J AM STAT ASSOC, V99, P710, DOI 10.1198/0162145040000001060; Fuller W. A., 1987, MEASUREMENT ERROR MO; HWANG JT, 1986, J AM STAT ASSOC, V81, P680; Iturria SJ, 1999, J ROY STAT SOC B, V61, P547, DOI 10.1111/1467-9868.00192; Liang H, 1999, ANN STAT, V27, P1519; Schwarz G, 1978, ANN STAT, V16, P356; TOSTESON TD, 1989, STAT MED, V8, P1139, DOI 10.1002/sim.4780080914; Wang N., 1996, BIOMETRICS, V52, P423; Zhu LX, 2003, SCAND J STAT, V30, P429, DOI 10.1111/1467-9469.00340	22	4	4	5	15	TAYLOR & FRANCIS INC	PHILADELPHIA	325 CHESTNUT ST, SUITE 800, PHILADELPHIA, PA 19106 USA	0361-0926			COMMUN STAT-THEOR M	Commun. Stat.-Theory Methods		2007	36	1-4					375	386		10.1080/03610920600974765		12	Statistics & Probability	Mathematics	153BV	WOS:000245407600030		
S	Destrero, A; De Mol, C; Odone, F; Verri, A		Yagi, Y; Kang, SB; Kweon, IS; Zha, H		Destrero, Augusto; De Mol, Christine; Odone, Francesca; Verri, Alessandro			A regularized approach to feature selection for face detection	COMPUTER VISION - ACCV 2007, PT II, PROCEEDINGS	LECTURE NOTES IN COMPUTER SCIENCE		English	Proceedings Paper	8th Asian Conference on Computer Vision	NOV 18-22, 2007	Tokyo, JAPAN	Asian Federat Comp Vis, IPSJ SIG CVIM, IEICE TC PRMU, Microsoft Res			OBJECT DETECTION; IMAGES	In this paper we present a trainable method for selecting features from an overcomplete dictionary of measurements. The starting point is a thresholded version of the Landweber algorithm for providing a sparse solution to a linear system of equations. We consider the problem of face detection and adopt rectangular features as an initial representation for allowing straightforward comparisons with existing techniques. For computational efficiency and memory requirements, instead of implementing the full optimization scheme on tenths of thousands of features, we propose to first solve a number of smaller size optimization problems obtained by randomly sub-sampling the feature vector, and then recombining the selected features. The obtained set is still highly redundant, so we further apply feature selection. The final feature selection system is an efficient two-stages architecture. Experimental results of an optimized version of the method on face images and image sequences indicate that this method is a serious competitor of other feature selection schemes recently popularized in computer vision for dealing with problems of real time object detection.	[Destrero, Augusto; Odone, Francesca; Verri, Alessandro] Univ Genoa, DISI, I-16146 Genoa, Italy	Destrero, A (reprint author), Univ Genoa, DISI, Via Dodecaneso 35, I-16146 Genoa, Italy.						Papageorgiou C, 2000, INT J COMPUT VISION, V38, P15, DOI 10.1023/A:1008162616689; Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Yang MH, 2002, IEEE T PATTERN ANAL, V24, P34; DAUBECHIES I, 2004, COMM PURE APPL MATH, V57; DESTRERO A, 2007, DISITR0701 U GEN DIP; Donoho D. L., 2004, MOST LARGE UNDETERMI; Guyon I., 2003, Journal of Machine Learning Research, V3, DOI 10.1162/153244303322753616; Mohan A, 2001, IEEE T PATTERN ANAL, V23, P349, DOI 10.1109/34.917571; Osuna E, 1997, TRAINING SUPPORT VEC; SHNEIDERMAN H, 2000, INT C COMP VIS; Ullman S., 2002, NATURE NEUROSCIENCE, V5; WESTON J, 2003, J MACHINE LEARNING, V3; Zhu J, 2004, ADV NEURAL INFORM PR	14	5	5	0	8	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		978-3-540-76389-5	LECT NOTES COMPUT SC			2007	4844						881	890				10	Computer Science, Theory & Methods; Imaging Science & Photographic Technology	Computer Science; Imaging Science & Photographic Technology	BHF08	WOS:000252603100086		
S	Zhang, X; Gader, P; Frigui, H		Harmon, RS; Broach, JT; Holloway, JH		Zhang, Xuping; Gader, Paul; Frigui, Hichem			Feature learning for a hidden Markov model approach to landmine detection - art. no. 655327	Detection and Remediation Technologies for Mines and Minelike Targets XII	PROCEEDINGS OF THE SOCIETY OF PHOTO-OPTICAL INSTRUMENTATION ENGINEERS (SPIE)		English	Proceedings Paper	Conference on Detection and Remediation Technologies for Mines and Minelike Targets XII	APR 11-12, 2007	Orlando, FL			HMM; feature learning; morphological image operator; discriminate training	RECOGNITION; REGULARIZATION; SELECTION; DESIGN; MINE	Hidden Markov Models (HMMs) are useful tools for landmine detection and discrimination using Ground Penetrating Radar (GPR). The performance of HMMs, as well as other feature-based methods, depends not only on the design of the classifier but on the features. Traditionally, algorithms for learning the parameters of classifiers have been intensely investigated while algorithms for learning parameters of the feature extraction process have been much less intensely investigated. In this paper, we describe experiments for learning feature extraction and classification parameters simultaneously in the context of using hidden Markov models for landmine detection.	Univ Florida, CISE Dept, Gainesville, FL 32611 USA	Zhang, X (reprint author), Univ Florida, CISE Dept, Gainesville, FL 32611 USA.						ALI K, 2003, J IMAGE VISION COMPU, V21, P663; WILLIAMS PM, 1995, NEURAL COMPUT, V7, P117, DOI 10.1162/neco.1995.7.1.117; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; FRIGUI H, 2005, EURASIP J APPL SIG P, P1867; GADER P, 1991, PATTERN RECOGN, V24, P421, DOI 10.1016/0031-3203(91)90055-A; GADER P, 1996, IEEE T PATTERN ANAL, V18, P1246; Gader PD, 2000, PATTERN RECOGN, V33, P935, DOI 10.1016/S0031-3203(99)00156-9; GARDER P, 2001, IEEE T GEOSCI REMOTE, V39, P1231; GILLIES A, 1990, P SPIE C IM ALG MORP; HILD K, 2006, IEEE T PATTERN ANAL, P1395; Krishnapuram B, 2004, IEEE T PATTERN ANAL, V26, P1105, DOI 10.1109/TPAMI.2004.55; LeCun Y, 1989, IEEE COMMUN MAG  NOV, P41; Lee WH, 2007, IEEE T GEOSCI REMOTE, V45, P389, DOI 10.1109/TGRS.2006.887018; MA C, 2004, THESIS U MISSOURI; RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626; STENTIFORD FWM, 1985, IEEE T PATTERN ANAL, V7, P349; WON Y, 1995, P 1995 IEEE INT C NE, V4, P2134; Zhao YX, 2003, IEEE T GEOSCI REMOTE, V41, P1016, DOI 10.1109/TGRS.2003.811761	18	0	0	1	1	SPIE-INT SOC OPTICAL ENGINEERING	BELLINGHAM	1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA	0277-786X		978-0-8194-6675-4	P SOC PHOTO-OPT INS			2007	6553						55327	55327	655327	10.1117/12.722593		12	Computer Science, Artificial Intelligence; Remote Sensing; Imaging Science & Photographic Technology	Computer Science; Remote Sensing; Imaging Science & Photographic Technology	BGJ54	WOS:000247632500062		
J	Hastie, T; Taylor, J; Tibshirani, R; Walther, G				Hastie, Trevor; Taylor, Jonathan; Tibshirani, Robert; Walther, Guenther			Forward stagewise regression and the monotone lasso	ELECTRONIC JOURNAL OF STATISTICS			English	Article						regression; lasso; stagewise		We consider the least angle regression and forward stagewise algorithms for solving penalized least squares regression problems. In Efron, Hastie, Johnstone & Tibshirani (2004) it is proved that the least angle regression algorithm, with a small modification, solves the lasso regression problem. Here we give an analogous result for incremental forward stage-wise regression, showing that it solves a version of the lasso problem that enforces monotonicity. One consequence of this is as follows: while lasso makes optimal progress in terms of reducing the residual sum-of-squares per unit increase in L(1)-norm of the coefficient beta, forward stage-wise is optimal per unit L(1) arc-length traveled along the coefficient path. We also study a condition under which the coefficient paths of the lasso are monotone, and hence the different algorithms coincide. Finally, we compare the lasso and forward stagewise procedures in a simulation study involving a large number of correlated predictors.	[Hastie, Trevor; Taylor, Jonathan; Tibshirani, Robert; Walther, Guenther] Stanford Univ, Dept Stat, Stanford, CA 94305 USA; [Hastie, Trevor; Tibshirani, Robert] Stanford Univ, Dept Hlth Res & Policy, Stanford, CA 94305 USA	Hastie, T (reprint author), Stanford Univ, Dept Stat, Sequoia Hall, Stanford, CA 94305 USA.	hastie@stanford.edu; jtaylor@stanford.edu; tibs@stanford.edu; walther@stanford.edu			National Science Foundation [DMS-0204612, DMS-0505676, DMS-9971405, DMS-0505682]; National Institutes of Health [2R01 CA 72028-07, N01-HV-28183, 5R33HL068522]	Hastie was partially supported by grants DMS-0204612 and DMS-0505676 from the National Science Foundation, and grant 2R01 CA 72028-07 from the National Institutes of Health.Tibshirani was partially supported by National Science Foundation Grant DMS-9971405 and National Institutes of Health Contract N01-HV-28183.Walther was partially supported by National Science Foundation Grant DMS-0505682 National Institutes of Health grant 5R33HL068522.	Tropp JA, 2006, IEEE T INFORM THEORY, V52, P1030, DOI 10.1109/TIT.2005.864420; Friedman J, 2000, ANN STAT, V28, P337, DOI 10.1214/aos/1016218223; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Osborne MR, 2000, IMA J NUMER ANAL, V20, P389, DOI 10.1093/imanum/20.3.389; Tropp JA, 2004, IEEE T INFORM THEORY, V50, P2231, DOI 10.1109/TIT.2004.834793; Efron B, 2004, ANN STAT, V32, P407; Schapire RE, 1998, ANN STAT, V26, P1651; Buhlmann P, 2006, ANN STAT, V34, P559, DOI 10.1214/009053606000000092; Chen S. S., 1998, SIAM J SCI COMPUT, P33; Donoho D., 2006, FAST SOLUTION L1 NOR; Friedman J., 2001, ANN STAT, V29, P1180; Friedman JH, 2004, GRADIENT DIRECTED RE; HASTIE T, 2001, ELEMENTS STAT EARNIN; PARK M. Y., 2006, REGULARIZATION PATH; ROSSET S, 2005, ADV NEURAL IN PRESS, V17; ROSSET S, 2007, ANN STAT IN PRESS; Schapire R. E., 1997, P 14 INT C MACH LEAR; ZHAO P, 2004, 678 U CAL STAT DEP	18	34	38	1	10	INST MATHEMATICAL STATISTICS	CLEVELAND	3163 SOMERSET DR, CLEVELAND, OH 44122 USA	1935-7524			ELECTRON J STAT	Electron. J. Stat.		2007	1						1	29		10.1214/07-EJS004		29	Statistics & Probability	Mathematics	V16EZ	WOS:000207854200001		
J	Bunea, F; Tsybakov, A; Wegkamp, M				Bunea, Florentina; Tsybakov, Alexandre; Wegkamp, Marten			Sparsity oracle inequalities for the Lasso	ELECTRONIC JOURNAL OF STATISTICS			English	Article						sparsity; oracle inequalities; Lasso; penalized least squares; nonparametric regression; dimension reduction; aggregation; mutual coherence; adaptive estimation		This paper studies oracle properties of l(1)-penalized least squares in nonparametric regression setting with random design. We show that the penalized least squares estimator satisfies sparsity oracle inequalities, i.e., bounds in terms of the number of non-zero components of the oracle vector. The results are valid even when the dimension of the model is(much) larger than the sample size and the regression matrix is not positive definite. They can be applied to high-dimensional linear regression, to nonparametric adaptive regression estimation and to the problem of aggregation of arbitrary estimators.	[Bunea, Florentina; Wegkamp, Marten] Florida State Univ, Dept Stat, Tallahassee, FL 32306 USA; [Tsybakov, Alexandre] Univ Paris 06, Lab Probabilites & Modeles Aleatoires, F-75252 Paris 05, France	Bunea, F (reprint author), Florida State Univ, Dept Stat, Tallahassee, FL 32306 USA.	bunea@stat.fsu.edu; tsybakov@ccr.jussieu.fr; wegkamp@stat.fsu.edu			NSF [DMS 0706829]	The research by F. Bunea and M. Wegkamp is supported in part by NSF grant DMS 0706829.	Knight K, 2000, ANN STAT, V28, P1356; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Antoniadis A, 2001, J AM STAT ASSOC, V96, P939, DOI 10.1198/016214501753208942; Osborne MR, 2000, IMA J NUMER ANAL, V20, P389, DOI 10.1093/imanum/20.3.389; Abramovich F, 2006, ANN STAT, V34, P584, DOI 10.1214/009053606000000074; Efron B, 2004, ANN STAT, V32, P407; DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425; Meinshausen N, 2006, ANN STAT, V34, P1436, DOI 10.1214/009053606000000281; Baraud Y., 2002, ESAIM-PROBAB STAT, V6, P127, DOI 10.1051/ps:2002007; Birge L, 2004, BERNOULLI, V10, P1039, DOI 10.3150/bj/1106314849; Bunea F, 2006, LECT NOTES ARTIF INT, V4005, P379, DOI 10.1007/11776420_29; BUNEA F, 2005, AGGREGATION GAUSSIAN; CANDES E, 2005, MANUSCRIPT; DONOHO DL, 2004, MANUSCRIPT; Golubev G. K., 2002, Problems of Information Transmission, V38, DOI 10.1023/A:1020098307781; Greenshtein E, 2004, BERNOULLI, V10, P971, DOI 10.3150/bj/1106314846; Gyorfi L, 2002, DISTRIBUTION FREE TH; Juditsky A, 2000, ANN STAT, V28, P681; KERKYACHARIAN G, 2005, PREPUBLICATION U PAR, V1017; KOLTCHINSKII V, 2006, MANUSCRIPT; KOLTCHINSKII V, 2005, OBERWOLFACH IN PRESS; Loubes J.-M., 2002, STAT NEERL, V56, P453; Nemirovski A., 2000, LECT NOTES MATH, V1738; Osborne MR, 2000, J COMPUT GRAPH STAT, V9, P319, DOI 10.2307/1390657; Tsybakov AB, 2003, LECT NOTES ARTIF INT, V2777, P303, DOI 10.1007/978-3-540-45167-9_23; TURLACH BA, 2005, 2004 P AM STAT ASS S, P2572; VANDEGEER SA, 2006, SEM STAT ETH ZUR; Wainwright M., 2006, 709 UC BERK DEP STAT; ZHAO P, 2005, MANUSCRIPT	29	122	122	2	7	INST MATHEMATICAL STATISTICS	CLEVELAND	3163 SOMERSET DR, CLEVELAND, OH 44122 USA	1935-7524			ELECTRON J STAT	Electron. J. Stat.		2007	1						169	194		10.1214/07-EJS008		26	Statistics & Probability	Mathematics	V16EZ	WOS:000207854200006		
J	Meier, L; Buhlmann, P				Meier, Lukas; Buehlmann, Peter			Smoothing l(1)-penalized estimators for high-dimensional time-course data	ELECTRONIC JOURNAL OF STATISTICS			English	Article						Lasso; Local least squares; Multivariate regression; Variable selection; Weighted like lihood	VARIABLE SELECTION; LINEAR-REGRESSION; LASSO; SHRINKAGE; EXPRESSION; SPARSITY	When a series of (related) linear models has to be estimated it is often appropriate to combine the different data-sets to construct more efficient estimators. We use l(1)-penalized estimators like the Lasso or the Adaptive Lasso which can simultaneously do parameter estimation and model selection. We show that for a time-course of high-dimensional linear models the convergence rates of the Lasso and of the Adaptive Lasso can be improved by combining the different time-points in a suitable way. Moreover, the Adaptive Lasso still enjoys oracle properties and consistent variable selection. The finite sample properties of the proposed methods are illustrated on simulated data and on a real problem of motif finding in DNA sequences.	[Meier, Lukas; Buehlmann, Peter] ETH, Seminar Stat, CH-8092 Zurich, Switzerland	Meier, L (reprint author), ETH, Seminar Stat, Leonhardstr 27, CH-8092 Zurich, Switzerland.	meier@stat.math.ethz.ch; buhlmann@stat.math.ethz.ch	Buhlmann, Peter/A-2107-2013				Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Park MY, 2007, J ROY STAT SOC B, V69, P659, DOI 10.1111/j.1467-9868.2007.00607.x; Conlon EM, 2003, P NATL ACAD SCI USA, V100, P3339, DOI 10.1073/pnas.0630591100; Meinshausen N, 2007, COMPUT STAT DATA AN, V52, P374, DOI 10.1016/j.csda.2006.12.019; Zou H, 2006, J AM STAT ASSOC, V101, P1418, DOI 10.1198/016214506000000735; Tibshirani R, 2005, J ROY STAT SOC B, V67, P91, DOI 10.1111/j.1467-9868.2005.00490.x; Turlach BA, 2005, TECHNOMETRICS, V47, P349, DOI 10.1198/004017005000000139; DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425; Beer MA, 2004, CELL, V117, P185, DOI 10.1016/S0092-8674(04)00304-6; Meinshausen N, 2006, ANN STAT, V34, P1436, DOI 10.1214/009053606000000281; Breiman L, 1997, J ROY STAT SOC B MET, V59, P3, DOI 10.1111/1467-9868.00054; Bunea F, 2007, ELECTRON J STAT, V1, P169, DOI 10.1214/07-EJS008; Greenshtein E, 2004, BERNOULLI, V10, P971, DOI 10.3150/bj/1106314846; Hu FF, 2002, CAN J STAT, V30, P347, DOI 10.2307/3316141; Huang J., 2006, ADAPTIVE LASSO SPARS; Lokhorst J, 1999, LASSO GEN LINEAR MOD; MEIER L, 2007, ANN STAT IN PRESS; MEINSHAUSEN N, 2006, LASSO TYPE RECOVERY; Simila T, 2007, COMPUT STAT DATA AN, V52, P406, DOI 10.1016/j.csda.2007.01.025; Tibshirani R, 1997, STAT MED, V16, P385, DOI 10.1002/(SICI)1097-0258(19970228)16:4<385::AID-SIM380>3.0.CO;2-3; VANDEGEER S, 2007, ANN STAT IN PRESS; Wasserman L., 2007, HIGH DIMENSIONAL VAR; ZHANG CH, 2007, ANN STAT IN PRESS; ZIDEK JV, 1997, J ROYAL STAT SOC B, V59, P42; ZOU H, 2007, ANN STAT IN PRESS	25	9	9	0	0	INST MATHEMATICAL STATISTICS	CLEVELAND	3163 SOMERSET DR, CLEVELAND, OH 44122 USA	1935-7524			ELECTRON J STAT	Electron. J. Stat.		2007	1						597	615		10.1214/07-EJS103		19	Statistics & Probability	Mathematics	V16EZ	WOS:000207854200024		
J	Drori, I				Drori, Iddo			Fast l(1) minimization by iterative thresholding for multidimensional NMR Spectroscopy	EURASIP JOURNAL ON ADVANCES IN SIGNAL PROCESSING			English	Article							TRIPLE-RESONANCE SPECTRA; UNCERTAINTY PRINCIPLES; MAXIMUM-ENTROPY; RECONSTRUCTION; RESOLUTION; ALGORITHM; REPRESENTATIONS; DECOMPOSITION; INFORMATION; REGRESSION	Fast multidimensional NMR is important in chemical shift assignment and for studying structures of large proteins. We present the first method which takes advantage of the sparsity of the wavelet representation of the NMR spectra and reconstructs the spectra from partial random measurements of its free induction decay (FID) by solving the following optimization problem: min parallel to x parallel to(1) subject to parallel to y-SF(T)W(T)x parallel to 2 <= epsilon, where y is a given n x 1 observation vector, S a random sampling operator, F denotes the Fourier transform, and W an orthogonal 2D wavelet transform. The matrix A = SF(T)W(T) is a given n x p matrix such that n < p. This problem can be solved by general-purpose solvers; however, these can be prohibitively expensive in large-scale applications. In the settings of interest, the underlying solution is sparse with a few nonzeros. We show here that for large practical systems, a good approximation to the sparsest solution is obtained by iterative thresholding algorithms running much more rapidly than general solvers. We demonstrate the applicability of our approach to fast multidimensional NMR spectroscopy. Our main practical result estimates a four-fold reduction in sampling and experiment time without loss of resolution while maintaining sensitivity for a wide range of existing settings. Our results maintain the quality of the peak list of the reconstructed signal which is the key deliverable used in protein structure determination. Copyright (C) 2007.	Stanford Univ, Dept Stat, Stanford, CA 94305 USA	Drori, I (reprint author), Stanford Univ, Dept Stat, Stanford, CA 94305 USA.						Donoho DL, 2001, IEEE T INFORM THEORY, V47, P2845, DOI 10.1109/18.959265; Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Rudelson M, 2005, INT MATH RES NOTICES, P4019; Freeman R, 2003, J BIOMOL NMR, V27, P101, DOI 10.1023/A:1024960302926; Mobli M, 2006, J MAGN RESON, V182, P96, DOI 10.1016/j.jmr.2006.06.007; Candes EJ, 2006, IEEE T INFORM THEORY, V52, P489, DOI 10.1109/TIT.2005.862083; Tropp JA, 2004, IEEE T INFORM THEORY, V50, P2231, DOI 10.1109/TIT.2004.834793; Kim S, 2003, J AM CHEM SOC, V125, P1385, DOI 10.1021/ja028197d; Orekhov VY, 2003, J BIOMOL NMR, V27, P165, DOI 10.1023/A:1024944720653; Candes EJ, 2005, IEEE T INFORM THEORY, V51, P4203, DOI 10.1109/TIT.2005.858979; Efron B, 2004, ANN STAT, V32, P407; Daubechies I, 2004, COMMUN PUR APPL MATH, V57, P1413, DOI 10.1002/cpa.20042; Hiller S, 2005, P NATL ACAD SCI USA, V102, P10876, DOI 10.1073/pnas.0504818102; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P6, DOI 10.1109/TIT.2005.860430; Figueiredo MAT, 2003, IEEE T IMAGE PROCESS, V12, P906, DOI 10.1109/TIP.2003.814255; BERLEKAMP ER, 1978, IEEE T INFORM THEORY, V24, P384, DOI 10.1109/TIT.1978.1055873; DAUBECHIES I, 1994, TEN LECT WAVELETS CB; Donoho D. L., 2006, 200602 STANF U DEP S; Donoho D. L., 2006, 200618 STANF U DEP S; DONOHO DL, 1992, SIAM J APPL MATH, V52, P577, DOI 10.1137/0152031; Donoho DL, 2006, COMMUN PUR APPL MATH, V59, P907, DOI 10.1002/cpa.20131; DRORI I, 2006, P IEEE INT WORKSH GE, P19; Drori I, 2003, IEEE T VIS COMPUT GR, V9, P395, DOI 10.1109/TVCG.2003.1207446; Drori I, 2003, ACM T GRAPHIC, V22, P303, DOI 10.1145/882262.882267; Elad M, 2006, IEEE T INFORM THEORY, V52, P5559, DOI 10.1109/TIT.2006.885522; ELAD M, 2006, J APPL COMPUTATIONAL; Figueiredo M., 2005, P IEEE INT C IM PROC, V2, P782, DOI DOI 10.1109/ICIP.2005.1530172; Gilbert A. C., 2002, P 34 ACM S THEOR COM, P152; Hoch J C, 1996, NMR DATA PROCESSING; Kupce E, 2004, J BIOMOL NMR, V28, P391, DOI 10.1023/B:JNMR.0000015421.60023.e5; LEE PT, SESAME EXPT MANAGEME; LUSTIG M, 2005, P SIGN PROC AD SPARS; LUSTIG M, 2006, P 9 ANN SCI SESS SOC; LUSTIG M, 2006, P INT SOC MAGN RES M; Malmodin D, 2005, PROG NUCL MAG RES SP, V46, P109, DOI 10.1016/j.pnmrs.2005.01.002; PAIGE CC, 1982, ACM T MATH SOFTWARE, V8, P43, DOI 10.1145/355984.355989; Pati Y. C., 1993, P 27 AS C SIGN SYST, V1, P40, DOI DOI 10.1109/ACSSC.1993.342465; Rovnyak D, 2004, J MAGN RESON, V170, P15, DOI 10.1016/j.jmr.2004.05.016; Sardy S, 2000, J COMPUT GRAPH STAT, V9, P361, DOI 10.2307/1390659; Sardy S, 2001, IEEE T SIGNAL PROCES, V49, P1146, DOI 10.1109/78.923297; SAUNDERS M. A., PDCO PRIMAL DUAL INT; SCHMEIDER P, 1994, J BIOMOL NMR, V4, P483, DOI 10.1007/BF00156615; SCHMIEDER P, 1993, J BIOMOL NMR, V3, P569; Ye Y., 1997, INTERIOR POINT ALGOR	46	10	10	1	11	SPRINGER	NEW YORK	233 SPRING ST, NEW YORK, NY 10013 USA	1687-6172			EURASIP J ADV SIG PR	EURASIP J. Adv. Signal Process.		2007									20248	10.1155/2007/20248		10	Engineering, Electrical & Electronic	Engineering	246RH	WOS:000252024200001		
J	Beyene, J; Tritchler, D				Beyene, Joseph; Tritchler, David			Multivariate analysis of complex gene expression and clinical phenotypes with genetic marker data	GENETIC EPIDEMIOLOGY			English	Article; Proceedings Paper	15th Genetic Analysis Workshop/International-Genetic-Epidemiology-Society Meeting	NOV, 2006	St Pete Beach, FL	Int Genet Epidemiol Soc		multivariate analysis; SNPs; gene expression; microarrays; simulated data; multivariate phenotypes	SELECTION	This paper summarizes contributions to group 12 of the 15th Genetic Analysis Workshop. The papers in this group focused on multivariate methods and applications for the analysis of molecular data including genotypic data as well as gene expression microarray measurements and clinical phenotypes. A range of multivariate techniques have been employed to extract signals from the multi-feature data sets that were provided by the workshop organizers. The methods included data reduction techniques such as principal component analysis and cluster analysis; latent variable models including structural equations and item response modeling; joint multivariate modeling techniques as well as multivariate visualization tools. This summary paper categorizes and discusses individual contributions with regard to multiple classifications of multivariate methods. Given the wide variety in the data considered, the objectives of the analysis and the methods applied, direct comparison of the results of the various papers is difficult. However, the group was able to make many interesting comparisons and parallels between the various approaches. In summary, there was a consensus among authors in group 12 that the genetic research community should continue to draw experiences from other fields such as statistics, econometrics, chemometrics, computer science and linear systems theory.	[Beyene, Joseph; Tritchler, David] Univ Toronto, Dept Publ Hlth Sci, Toronto, ON M5T3M7, Canada; [Beyene, Joseph] Ontario Canc Inst, Div Epidemiol & Stat, Toronto, ON, Canada	Beyene, J (reprint author), Univ Toronto, Dept Publ Hlth Sci, Hllth Sci Bldg 155 Coll St, Toronto, ON M5T3M7, Canada.	joseph@utstat.toronto.edu	Bull, Shelley/A-1920-2013				Amos Christopher I, 2007, BMC Proc, V1 Suppl 1, pS3; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Draghici S, 2003, GENOMICS, V81, P98, DOI 10.1016/S0888-7543(02)00021-6; Hirschhorn JN, 2005, NAT REV GENET, V6, P95, DOI 10.1038/nrg1521; Zou H, 2005, J ROY STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x; BENJAMINI Y, 1995, J ROY STAT SOC B MET, V57, P289; Blom G., 1958, STAT ELEMENTS TRANSF; Bollen Kenneth A., 1989, STRUCTURAL EQUATIONS; Cartier Kevin C, 2007, BMC Proc, V1 Suppl 1, pS115; Cheung Vivian G, 2007, BMC Proc, V1 Suppl 1, pS2; Hauser MA, 2003, HUM MOL GENET, V12, P671, DOI 10.1093/hmg/ddg070; Jolliffe I. T., 1986, PRINCIPAL COMPONENT; Kraja Aldi T, 2007, BMC Proc, V1 Suppl 1, pS116; Li Na, 2007, BMC Proc, V1 Suppl 1, pS117; Miller Michael B, 2007, BMC Proc, V1 Suppl 1, pS4; Morley M, 2004, NATURE, V430, P743, DOI 10.1038/nature02797; Nock Nora L, 2007, BMC Proc, V1 Suppl 1, pS118; Parkhomenko Elena, 2007, BMC Proc, V1 Suppl 1, pS119; Sutradhar Rinku, 2007, BMC Proc, V1 Suppl 1, pS120; Waaijenborg Sandra, 2007, BMC Proc, V1 Suppl 1, pS122; Wang Yuanjia, 2007, BMC Proc, V1 Suppl 1, pS121	21	10	10	0	6	WILEY-BLACKWELL	HOBOKEN	111 RIVER ST, HOBOKEN 07030-5774, NJ USA	0741-0395	1098-2272		GENET EPIDEMIOL	Genet. Epidemiol.		2007	31			1			S103	S109		10.1002/gepi.20286		7	Genetics & Heredity; Public, Environmental & Occupational Health	Genetics & Heredity; Public, Environmental & Occupational Health	241RR	WOS:000251674000013	18046768	
S	Shimamura, T; Imoto, S; Yamaguchi, R; Miyano, S		Ng, SK; Mamitsuka, H; Wong, L		Shimamura, Teppei; Imoto, Seiya; Yamaguchi, Rui; Miyano, Satoru			Weighted lasso in graphical gaussian modeling for large gene network estimation based on microarray data	GENOME INFORMATICS 2007, VOL 19	Genome Informatics Series		English	Proceedings Paper	18th International Conference on Genome Informatics	DEC 03-05, 2007	Biopolis, SINGAPORE	Natl Univ Singapore, Sch Comp, Natl Univ Singapore, Bioinformat Programme, World Sci, Wiley, Taylor & Francis		lasso; L-1-regularization; empirical bayes; graphical modeling; large gene network; DNA microarray data	ISOPRENOID BIOSYNTHESIS; VARIABLE SELECTION; ORACLE PROPERTIES; INFERENCE; PATHWAY	We propose a statistical method based on graphical Gaussian models for estimating large gene networks from DNA microarray data. In estimating large gene networks, the number of genes is larger than the number of samples, we need to consider some restrictions for model building. We propose weighted lasso estimation for the graphical Gaussian models as a model of large gene networks. In the proposed method, the structural learning for gene networks is equivalent to the selection of the regularization parameters included in the weighted lasso estimation. We investigate this problem from a Bayes approach and derive an empirical Bayesian information criterion for choosing them. Unlike Bayesian network approach, our method can find the optimal network structure and does not require to use heuristic structural learning algorithm. We conduct Monte Carlo simulation to show the effectiveness of the proposed method. We also analyze Arabidopsis thaliana microarray data and estimate gene networks.	[Shimamura, Teppei; Imoto, Seiya; Yamaguchi, Rui; Miyano, Satoru] Univ Tokyo, Ctr Human Genome, Inst Med Sci, Minato Ku, Tokyo 1088639, Japan	Shimamura, T (reprint author), Univ Tokyo, Ctr Human Genome, Inst Med Sci, Minato Ku, 4-6-1 Shirokanedai, Tokyo 1088639, Japan.						Akaike H., 1980, BAYESIAN STAT, P143; KASS RE, 1995, J AM STAT ASSOC, V90, P773, DOI 10.1080/01621459.1995.10476572; Barabasi AL, 1999, SCIENCE, V286, P509, DOI 10.1126/science.286.5439.509; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; Lichtenthaler HK, 1997, FEBS LETT, V400, P271, DOI 10.1016/S0014-5793(96)01404-4; Yuan M, 2007, J ROY STAT SOC B, V69, P143, DOI 10.1111/j.1467-9868.2007.00581.x; Jeong H, 2000, NATURE, V407, P651; Arigoni D, 1997, P NATL ACAD SCI USA, V94, P10600, DOI 10.1073/pnas.94.20.10600; Laule O, 2003, P NATL ACAD SCI USA, V100, P6866, DOI 10.1073/pnas.1031755100; TIERNEY L, 1986, J AM STAT ASSOC, V81, P82, DOI 10.2307/2287970; Zou H, 2006, J AM STAT ASSOC, V101, P1418, DOI 10.1198/016214506000000735; Meinshausen N, 2006, ANN STAT, V34, P1436, DOI 10.1214/009053606000000281; Maslov S, 2002, SCIENCE, V296, P910, DOI 10.1126/science.1065103; Dobra A, 2004, J MULTIVARIATE ANAL, V90, P196, DOI 10.1016/j.jmva.2004.02.009; Gustafsson M, 2005, IEEE ACM T COMPUT BI, V2, P254, DOI 10.1109/TCBB.2005.35; Konishi S, 2004, BIOMETRIKA, V91, P27, DOI 10.1093/biomet/91.1.27; Leng CL, 2006, STAT SINICA, V16, P1273; Rodriguez-Concepcion M, 2002, PLANT PHYSIOL, V130, P1079, DOI 10.1104/pp.007138; Schafer J, 2005, BIOINFORMATICS, V21, P754, DOI 10.1093/bioinformatics/bti062; Toh H, 2002, BIOINFORMATICS, V18, P287, DOI 10.1093/bioinformatics/18.2.287; Wille A., 2004, GENOME BIOL, V5, P1	22	19	19	0	0	IMPERIAL COLLEGE PRESS	COVENT GARDEN	57 SHELTON STREET, COVENT GARDEN WC2H 9HE, ENGLAND	0919-9454		978-1-86094-984-5	GENOME INFORM SER			2007	19						142	153				12	Biochemistry & Molecular Biology; Mathematical & Computational Biology	Biochemistry & Molecular Biology; Mathematical & Computational Biology	BHJ32	WOS:000253521200013	18546512	
J	Musani, SK; Shriner, D; Liu, NJ; Feng, R; Coffey, CS; Yi, NJ; Tiwari, HK; Allison, DB				Musani, Solomon K.; Shriner, Daniel; Liu, Nianjun; Feng, Rui; Coffey, Christopher S.; Yi, Nengjun; Tiwari, Hemant K.; Allison, David B.			Detection of gene x gene interactions in genome-wide association studies of human population data	HUMAN HEREDITY			English	Review						epistasis; genome-wide association; computational burden; overfitting; data sparsity; methodological issues	MULTIFACTOR-DIMENSIONALITY REDUCTION; FALSE DISCOVERY RATE; ARTIFICIAL NEURAL-NETWORK; QUANTITATIVE TRAIT LOCI; LOGISTIC-REGRESSION; COMPLEX DISEASES; BREAST-CANCER; PAI-1-4G/5G POLYMORPHISMS; COMPENSATORY EVOLUTION; LINKAGE DISEQUILIBRIUM	Empirical evidence supporting the commonality of gene x gene interactions, coupled with frequent failure to replicate results from previous association studies, has prompted statisticians to develop methods to handle this important subject. Nonparametric methods have generated intense interest because of their capacity to handle high-dimensional data. Genome-wide association analysis of large-scale SNP data is challenging mathematically and computationally. In this paper, we describe major issues and questions arising from this challenge, along with methodological implications. Data reduction and pattern recognition methods seem to be the new frontiers in efforts to detect gene x gene interactions comprehensively. Currently, there is no single method that is recognized as the 'best' for detecting, characterizing, and interpreting gene x gene interactions. Instead, a combination of approaches with the aim of balancing their specific strengths may be the optimal approach to investigate gene x gene interactions in human data. Copyright (c) 2007 S. Karger AG, Basel.	Univ Alabama, Sect Stat Genet, Dept Biostat, Birmingham, AL 35294 USA; Univ Alabama, Sect Res Methods & Clin Trials, Dept Biostat, Birmingham, AL 35294 USA; Univ Alabama, Clin Nutr Res Ctr, Birmingham, AL 35294 USA	Musani, SK (reprint author), Univ Alabama, Sect Stat Genet, Dept Biostat, Ryals Publ Hlth Bldg Suite 517A, Birmingham, AL 35294 USA.	SMusani@ms.soph.uab.edu		Allison, David/0000-0003-3566-9399			Agresti A., 2002, CATEGORICAL DATA ANA; Anderson U, 1998, AUDITING-J PRACT TH, V17, P1; Andrew AS, 2006, CARCINOGENESIS, V27, P1030, DOI 10.1093/carcin/bgi284; Li WT, 2000, HUM HERED, V50, P334, DOI 10.1159/000022939; Ritchie MD, 2001, AM J HUM GENET, V69, P138, DOI 10.1086/321276; Allison DB, 2002, COMPUT STAT DATA AN, V39, P1, DOI 10.1016/S0167-9473(01)00046-9; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Mackay TFC, 2001, ANNU REV GENET, V35, P303, DOI 10.1146/annurev.genet.35.102401.090633; Moore JH, 2003, HUM HERED, V56, P73, DOI 10.1159/000073735; Carrasquillo MM, 2002, NAT GENET, V32, P237, DOI 10.1038/ng998; CHEVERUD JM, 1995, GENETICS, V139, P1455; Ritchie MD, 2003, GENET EPIDEMIOL, V24, P150, DOI 10.1002/gepi.10218; Millstein J, 2006, AM J HUM GENET, V78, P15, DOI 10.1086/498850; RUBIN DB, 1984, ANN STAT, V12, P1151, DOI 10.1214/aos/1176346785; Marchini J, 2005, NAT GENET, V37, P413, DOI 10.1038/ng1537; Gelman A, 1996, STAT SINICA, V6, P733; Segre D, 2005, NAT GENET, V37, P77, DOI 10.1038/ng1489; Hirschhorn JN, 2005, NAT REV GENET, V6, P95, DOI 10.1038/nrg1521; Benjamini Y, 2001, ANN STAT, V29, P1165; Cordell HJ, 2002, HUM MOL GENET, V11, P2463, DOI 10.1093/hmg/11.20.2463; Gauderman WJ, 2002, AM J EPIDEMIOL, V155, P478, DOI 10.1093/aje/155.5.478; Moore JH, 2004, JAMA-J AM MED ASSOC, V291, P1642, DOI 10.1001/jama.291.13.1642; Wang WYS, 2005, NAT REV GENET, V6, P109, DOI 10.1038/nrg1522; HOLM S, 1979, SCAND J STAT, V6, P65; Ma DQ, 2005, AM J HUM GENET, V77, P377, DOI 10.1086/433195; BENJAMINI Y, 1995, J ROY STAT SOC B MET, V57, P289; Hoh J, 2003, NAT REV GENET, V4, P701, DOI 10.1038/nrg1155; Hoh J, 2001, GENOME RES, V11, P2115, DOI 10.1101/gr.204001; Ioannidis JPA, 2001, NAT GENET, V29, P306, DOI 10.1038/ng749; Carlborg O, 2004, NAT REV GENET, V5, P618, DOI 10.1038/nrg1407; Hahn LW, 2003, BIOINFORMATICS, V19, P376, DOI 10.1093/bioinformatics/btf869; Storey JD, 2003, P NATL ACAD SCI USA, V100, P9440, DOI 10.1073/pnas.1530509100; Nakagawa S, 2004, BEHAV ECOL, V15, P1044, DOI 10.1093/beheco/arh107; Risch N, 1996, SCIENCE, V273, P1516, DOI 10.1126/science.273.5281.1516; Nelson MR, 2001, GENOME RES, V11, P458, DOI 10.1101/gr.172901; Curtis D, 2001, ANN HUM GENET, V65, P95, DOI 10.1046/j.1469-1809.2001.6510095.x; Florez JC, 2003, ANNU REV GENOM HUM G, V4, P257, DOI 10.1146/annurev.genom.4.070802.110436; Maisnier-Patin S, 2004, RES MICROBIOL, V155, P360, DOI 10.1016/j.resmic.2004.01.019; Hirschhorn JN, 2002, GENET MED, V4, P45, DOI 10.1097/00125817-200203000-00002; Storey JD, 2004, J ROY STAT SOC B, V66, P187, DOI 10.1111/j.1467-9868.2004.00439.x; Verhoeven KJF, 2005, OIKOS, V108, P643, DOI 10.1111/j.0030-1299.2005.13727.x; Yekutieli D, 1999, J STAT PLAN INFER, V82, P171, DOI 10.1016/S0378-3758(99)00041-5; Aston CE, 2005, HUM GENET, V116, P208, DOI 10.1007/s00439-004-1206-7; Badano JL, 2006, NATURE, V439, P326, DOI 10.1038/nature04370; Barnes KC, 2001, GENOMICS, V71, P246, DOI 10.1006/geno.2000.6430; Bateson W, 1909, MENDELS PRINCIPLES H; Becker T, 2005, GENET EPIDEMIOL, V29, P313, DOI 10.1002/gepi.20096; BOERWINKLE E, 1987, ANN HUM GENET, V51, P211, DOI 10.1111/j.1469-1809.1987.tb00874.x; BRADLEY JV, 1968, DISTRIBUTION FREE ST, P15; Bush WS, 2005, LECT NOTES COMPUT SC, V3449, P44; Cardon LR, 2001, NAT REV GENET, V2, P91, DOI 10.1038/35052543; Chen Y, 1999, GENES GENET SYST, V74, P271, DOI 10.1266/ggs.74.271; Cho JH, 1998, P NATL ACAD SCI USA, V95, P7502, DOI 10.1073/pnas.95.13.7502; Cho YM, 2004, DIABETOLOGIA, V47, P549, DOI 10.1007/s00125-003-1321-3; Clark AG, 2000, EVOL BIOL, V32, P205; COCKERHAM CC, 1954, GENETICS, V39, P859; Coffey CS, 2004, BMC BIOINFORMATICS, V5, DOI 10.1186/1471-2105-5-49; CONCATO J, 1993, ANN INTERN MED, V118, P201; Congdon C.B., 1993, P WORKSH ART INT GEN, P107; CORDELL HJ, 1995, AM J HUM GENET, V57, P920; Cox NJ, 1999, NAT GENET, V21, P213, DOI 10.1038/6002; Culverhouse R, 2004, GENET EPIDEMIOL, V27, P141, DOI 10.1002/gepi.20006; Culverhouse R, 2002, AM J HUM GENET, V70, P461, DOI 10.1086/338759; Daly MJ, 2004, INFLAMM BOWEL DIS, V10, P312, DOI 10.1097/00054725-200405000-00020; Daly MJ, 2005, NAT GENET, V37, P337, DOI 10.1038/ng0405-337; Dong CH, 2003, AM J HUM GENET, V72, P115, DOI 10.1086/345648; Dudbridge F, 2004, AM J HUM GENET, V75, P424, DOI 10.1086/423738; Evans DM, 2006, PLOS GENET, V2, P1424, DOI 10.1371/journal.pgen.0020157; Fisher R. A., 1919, Transactions of the Royal Society of Edinburgh, V52; Frankel WN, 1996, NAT GENET, V14, P371, DOI 10.1038/ng1296-371; FRAWLEY W, 1992, KNOWLEDGE DISCOVERY, P213; Greenland S., 1998, MODERN EPIDEMIOLOGY, Vsecond, P329; Hand D., 2001, PRINCIPLES DATA MINI; HEIDEMA AG, BMC GENET; Hoh J, 2000, ANN HUM GENET, V64, P413, DOI 10.1046/j.1469-1809.2000.6450413.x; Holm P, 2004, DIABETES, V53, P1584, DOI 10.2337/diabetes.53.6.1584; HOWARD CG, 1994, INT C PATT RECOG, P826; Ide A, 2004, J AUTOIMMUN, V22, P73, DOI 10.1016/j.jaut.2003.10.001; IONITA I, 2006, BMC GENET; Kim JH, 2001, GENOMICS, V74, P273, DOI 10.1006/geno.2001.6569; KIRKPATRICK S, 1983, SCIENCE, V220, P671, DOI 10.1126/science.220.4598.671; Kooperberg C, 2005, GENET EPIDEMIOL, V28, P157, DOI 10.1002/gepi.20042; Kooperberg C, 2001, GENET EPIDEMIOL, V21, pS626; Koza J. R., 1991, GENETIC GENERATION B, VII; Krupa R, 2004, J EXP CLIN CANC RES, V23, P285; Lai HC, 2005, GYNECOL ONCOL, V99, P113, DOI 10.1016/j.ygyno.2005.05.010; LI J, 2005, HEREDITY, P1; Lohmueller KE, 2003, NAT GENET, V33, P177, DOI 10.1038/ng1071; Martin ER, 2006, GENET EPIDEMIOL, V30, P111, DOI 10.1002/gepi.20128; Moore J, 2002, PAC S BIOCOMPUT, V7, P53; Moore JH, 2004, EXPERT REV MOL DIAGN, V4, P795, DOI 10.1586/14737159.4.6.795; Moore JH, 2002, ANN MED, V34, P88, DOI 10.1080/07853890252953473; Moore JH, 2002, CLIN GENET, V62, P74, DOI 10.1034/j.1399-0004.2002.620110.x; Moore JH, 2002, CLIN GENET, V62, P53, DOI 10.1034/j.1399-0004.2002.620107.x; MOORE JH, J THEOR BIOL; Motsinger AA, 2006, LECT NOTES COMPUT SC, V3907, P103; MOTSINGER AA, 2006, GENET EPIDEMIOL; Motsinger AA, 2006, BMC BIOINFORMATICS, V7, DOI 10.1186/1471-2105-7-39; North BV, 2005, HUM HERED, V59, P79, DOI 10.1159/000085222; Page GP, 2003, AM J HUM GENET, V73, P711, DOI 10.1086/378900; Peduzzi P, 1996, J CLIN EPIDEMIOL, V49, P1373, DOI 10.1016/S0895-4356(96)00236-3; Putt W, 2004, HUM MOL GENET, V13, P1587, DOI 10.1093/hmg/ddh168; Qin SY, 2005, EUR J HUM GENET, V13, P807, DOI 10.1038/sj.ejhg.5201418; Redden DT, 2006, PLOS GENET, V2, P1254, DOI 10.1371/journal.pgen.0020137; RITCHIE MD, 2003, BMC BIOINFORMATICS, V28, P4; Ritchie MD, 2004, LECT NOTES COMPUT SC, V3102, P438; Routman EJ, 1997, EVOLUTION, V51, P1654, DOI 10.2307/2411217; Searle SR, 1987, LINEAR MODELS UNBALA; Sherriff A, 2001, ADV GENET, V42, P287, DOI 10.1016/S0065-2660(01)42029-3; Shimomura K, 2001, GENOME RES, V11, P959, DOI 10.1101/gr.171601; SING CF, 1987, CIBA F SYMP, V130, P99; SING CF, 2003, CIBA F SYMP, V1997, P211; Smith TR, 2003, CANCER LETT, V190, P183, DOI 10.1016/S0304-3835(02)00595-5; Soares ML, 2005, HUM MOL GENET, V14, P543, DOI 10.1093/hmg/ddi051; Templeton AR, 2000, EPISTASIS EVOLUTIONA; Thornton-Wells TA, 2004, TRENDS GENET, V20, P640, DOI 10.1016/j.tig.2004.09.007; Todd JA, 2006, NAT GENET, V38, P731, DOI 10.1038/ng0706-731; Tomita Y, 2004, BMC BIOINFORMATICS, V5, DOI 10.1186/1471-2105-5-120; Tsai CT, 2004, CIRCULATION, V109, P1640, DOI 10.1161/01.CIR.0000124487.36586.26; UTANS J, 1991, C P 1 INT C ART INT; Wang T, 2006, BMC GENET, V7, DOI 10.1186/1471-2156/7/9; Warden CH, 2004, MAMM GENOME, V15, P460, DOI 10.1007/s00335-004-2353-9; Ways JA, 2002, GENOMICS, V80, P13, DOI 10.1006/geno.2002.6797; WEANG S, 2003, AM J EPIDEMIOL, P899; Westfall Peter H, 2002, Methods Mol Biol, V184, P143; Wille A, 2003, GENET EPIDEMIOL, V25, P350, DOI 10.1002/gepi.10263; Williams SM, 2004, BIOESSAYS, V26, P170, DOI 10.1002/bies.10401; WITTE J, 1998, ENCY BIOSTATISTICS, P1613; Xiong MM, 2002, HUM HERED, V53, P158, DOI 10.1159/000064978; Xu JF, 2004, HUM GENET, V115, P255, DOI 10.1007/s00439-004-1144-4; Yang WS, 2003, DIABETOLOGIA, V46, P977, DOI 10.1007/s00125-003-1136-2; Ye S, 2003, HEART, V89, P1195, DOI 10.1136/heart.89.10.1195; Zee R. Y. L., 2002, Pharmacogenomics Journal, V2, P197, DOI 10.1038/sj.tpj.6500101; Zerba KE, 2000, HUM GENET, V107, P466, DOI 10.1007/s004390000394; Zubenko GS, 2001, MOL PSYCHIATR, V6, P413, DOI 10.1038/sj.mp.4000900	135	98	102	3	12	KARGER	BASEL	ALLSCHWILERSTRASSE 10, CH-4009 BASEL, SWITZERLAND	0001-5652			HUM HERED	Hum. Hered.		2007	63	2					67	84		10.1159/000099179		18	Genetics & Heredity	Genetics & Heredity	136XD	WOS:000244256500002	17283436	
S	Balakrishnan, S; Madigan, D		Ramakrishnan, N; Zaiane, OR; Shi, Y; Clifton, CW; Wu, XD		Balakrishnan, Suhrid; Madigan, David			Finding predictive runs with LAPS	ICDM 2007: PROCEEDINGS OF THE SEVENTH IEEE INTERNATIONAL CONFERENCE ON DATA MINING	IEEE International Conference on Data Mining		English	Proceedings Paper	7th IEEE International Conference on Data Mining	OCT 28-31, 2007	Omaha, NE	IEEE, Microsoft adCenter Labs, Univ Nebraska Med Ctr, Univ Nebraska Omaha, Thomson CORP, Web Splashes, In The Details Events, IBM, IEEE Comp Soc, Henry Doorly Zoo, Mutual Omaha, CAS Res Ctr Fictitious Econ & Data Sci, First Natl Bank Omaha, Peter Kiewit Inst			REGRESSION; SELECTION; LASSO	We present an extension to the Lasso [6] for binary classification problems with ordered attributes. Inspired by the Fused Lasso [5] and the Group Lasso [7, 3] models, we aim to both discover and model runs (contiguous subgroups of the variables) that are highly predictive. We call the extended model LAPS (the Lasso with Attribute Partition Search). Such problems commonly arise in financial and medical domains, where predictors are time series variables, for example. This paper outlines the formulation of the problem, an algorithm to obtain the model coefficients and experiments showing applicability to practical problems of this type.	[Balakrishnan, Suhrid] Rutgers State Univ, Dept Comp Sci, Piscataway, NJ 08854 USA	Balakrishnan, S (reprint author), Rutgers State Univ, Dept Comp Sci, Piscataway, NJ 08854 USA.						Yuan M, 2006, J ROY STAT SOC B, V68, P49, DOI 10.1111/j.1467-9868.2005.00532.x; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Tibshirani R, 2005, J ROY STAT SOC B, V67, P91, DOI 10.1111/j.1467-9868.2005.00490.x; CONSONNI G, 1995, J AM STAT ASSOC, V90, P935, DOI 10.2307/2291328; Genkin A., 2004, LARGE SCALE BAYESIAN; MEIER L, 2006, GROUP LASSO LOGISTIC; SAITO N, 1994, LOCAL FEATURE EXTRAC	7	1	1	0	1	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA	1550-4786		978-0-7695-3018-5	IEEE DATA MINING			2007							415	420				6	Computer Science, Artificial Intelligence; Computer Science, Software Engineering	Computer Science	BHI56	WOS:000253429400043		
B	Liu, ZQ		Wani, MA; Kantardzic, MM; Li, T; Liu, Y; Kurgan, L; Ye, J; Ogihara, M; Sagiroglu, S; Chen, XW; Peterson, L; Hafeez, K		Liu, Zhenqiu			Cox's proportional hazards model with L-p penalty for biomarker identification and survival prediction	ICMLA 2007: SIXTH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS, PROCEEDINGS			English	Proceedings Paper	6th International Conference on Machine Learning and Applications	DEC 13-15, 2007	Cincinnati, OH	Assoc Machine Learning & Applicat, IEEE			REGRESSION; LASSO	Advances in high throughput technology provide massive high dimensional data. It is very important and challenging to study the association of genes with various clinical outcomes. Due to large variability in time to certain clinical event among patients, studying possibly censored survival data can be more informative than classification. We proposed the Cox's proportional hazards model with LP penalty method for simultaneous feature (gene) selection and survival prediction. Lp penalty shrinks coefficients and produces some coefficients that are exactly zero. It has been shown that Lp (p < 1) regularization performs better than L, in the regression and classification framework (Knight & Fu 2000, Liu et al. 2007). Experimental results with different data demonstrate that the proposed procedures can be used for identifying important genes (features) that are related to time to death due to cancer and for building parsimonious model for predicting the survival of future patients.	Univ Maryland, Greenebaum Canc Ctr, Div Biostat, Baltimore, MD 21201 USA	Liu, ZQ (reprint author), Univ Maryland, Greenebaum Canc Ctr, Div Biostat, 22 S Greene St, Baltimore, MD 21201 USA.						Knight K, 2000, ANN STAT, V28, P1356; Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010; Gui J, 2005, BIOINFORMATICS, V21, P3001, DOI 10.1093/bioinformatics/bti422; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Krishnapuram B, 2005, IEEE T PATTERN ANAL, V27, P957, DOI 10.1109/TPAMI.2005.127; Alizadeh AA, 2000, NATURE, V403, P503, DOI 10.1038/35000501; Heagerty PJ, 2005, BIOMETRICS, V61, P92, DOI 10.1111/j.0006-341X.2005.030814.x; Cox D. R., 1972, J R STAT SOC B, V34; INZA B, 2002, J INTELLIGENT FUZZY; LIU Z, 2007, J STAT APPL GENETICS, V6; Ma SG, 2007, BMC BIOINFORMATICS, V8, DOI 10.1186/1471-2105-8-192; Monari G, 2000, NEUROCOMPUTING, V35, P195, DOI 10.1016/S0925-2312(00)00325-8; Rivals I., 2003, Journal of Machine Learning Research, V3, DOI 10.1162/153244303322753724; TIBSHIRANI R, 1998, LASSO METHOD VARIABL; Wolberg WH, 1999, CLIN CANCER RES, V5, P3542	15	0	0	3	4	IEEE COMPUTER SOC	LOS ALAMITOS	10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA			978-0-7695-3069-7				2007							624	628				5	Computer Science, Artificial Intelligence; Computer Science, Theory & Methods; Engineering, Electrical & Electronic	Computer Science; Engineering	BHF84	WOS:000252793400101		
J	Fuchs, JJ; Maria, S				Fuchs, Jean-Jacques; Maria, Sebastien			A new approach to variable selection using the TLS approach	IEEE TRANSACTIONS ON SIGNAL PROCESSING			English	Article						least squares (LS) problem; matrix perturbation; stepwise regression; student test; subset selection; total least squares (TLS) problem	SUBSET-SELECTION; REGRESSION; MODEL	The problem of variable selection is one of the most important model selection problems in statistical applications. It is also known as the subset selection problem and arises when one wants to explain the observations or data adequately by a subset of possible explanatory variables. The objective is to identify factors of importance and to include only variables that contribute significantly to the reduction of the prediction error. Numerous selection procedures have been proposed in the classical multiple linear regression model. We extend one of the most popular methods developed in this context, the backward selection procedure, to a more general class of models. In the basic linear regression model, errors are present on the observations only, if errors are present on the regressors as well, one gets the errors-in-variables model which for Gaussian noise becomes the total-least-squares (TLS) model, this is the context considered here.	Univ Rennes 1, IRISA, F-35042 Rennes, France	Fuchs, JJ (reprint author), Univ Rennes 1, IRISA, F-35042 Rennes, France.	fuchs@irisa.fr; smaria@irisa.fr					AKAIKE H, 1974, IEEE T AUTOMAT CONTR, VAC19, P716, DOI 10.1109/TAC.1974.1100705; ANHUFFEL S, 1987, LINEAR ALGEBRA ITS A, V8, P695; Rao BD, 2003, IEEE T SIGNAL PROCES, V51, P760, DOI 10.1109/TSP.2002.808076; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Vorobyov SA, 2003, IEEE T SIGNAL PROCES, V51, P313, DOI 10.1109/TSP.2002.806865; Zou H, 2005, J ROY STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x; SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136; MALLOWS CL, 1973, TECHNOMETRICS, V15, P661, DOI 10.2307/1267380; Draper N, 1981, APPL REGRESSION ANAL; Dunne BE, 2003, IEEE T SIGNAL PROCES, V51, P386, DOI 10.1109/TSP.2002.806980; Fuchs JJ, 1996, IEEE T SIGNAL PROCES, V44, P2377, DOI 10.1109/78.539022; Fuller W. A., 1987, MEASUREMENT ERRORS M; George EI, 2000, BIOMETRIKA, V87, P731, DOI 10.1093/biomet/87.4.731; GLESER LJ, 1981, ANN STAT, V9, P24, DOI 10.1214/aos/1176345330; Golub G. H., 1983, MATRIX COMPUTATIONS; GOLUB GH, 1980, SIAM J NUMER ANAL, V17, P883, DOI 10.1137/0717073; Lutkepohl H., 1996, HDB MATRICES; Miller A. J., 1990, SUBSET SELECTION REG; Nafie M, 2002, IEEE T SIGNAL PROCES, V50, P1591, DOI 10.1109/TSP.2002.1011200; Scharf L. L., 1991, STAT SIGNAL PROCESSI; SEBER GAF, 2003, LINEAR REGRESSION; Stewart G.W., 1990, MATRIX PERTURBATION; THOMPSON ML, 1978, INT STAT REV, V46, P1, DOI 10.2307/1402505; Van Huffel S., 1991, TOTAL LEAST SQUARES; WILKINSON J. H., 1965, ALGEBRAIC EIGENVALUE	25	5	5	3	4	IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC	PISCATAWAY	445 HOES LANE, PISCATAWAY, NJ 08855 USA	1053-587X			IEEE T SIGNAL PROCES	IEEE Trans. Signal Process.	JAN	2007	55	1					10	19		10.1109/TSP.2006.882105		10	Engineering, Electrical & Electronic	Engineering	123FH	WOS:000243281200002		
S	Clemmensen, LH; Gomez, DD; Ersboll, BK		Ersboll, BK; Pedersen, KS		Clemmensen, Line H.; Gomez, David D.; Ersboll, Bjarne K.			Individual discriminative face recognition models based on subsets of features	Image Analysis, Proceedings	LECTURE NOTES IN COMPUTER SCIENCE		English	Proceedings Paper	15th Scandinavian Conference on Image Analysis	JUN 10-14, 2007	Aalborg, DENMARK	Aalborg Univ, Tech Univ Denmark, Univ Copenhagen, Dept Comp Sci, IT Univ Copenhagen			REGRESSION; SELECTION	The accuracy of data classification methods depends considerably on the data representation and on the selected features. In this work, the elastic net model selection is used to identify meaningful and important features in face recognition. Modelling the characteristics which distinguish one person from another using only subsets of features will both decrease the computational cost and increase the generalization capacity of the face recognition algorithm. Moreover, identifying which are the features that better discriminate between persons will also provide a deeper understanding of the face recognition problem. The elastic net model is able to select a subset of features with low computational effort compared to other state-of-the-art feature selection methods. Furthermore, the fact that the number of features usually is larger than the number of images in the data base makes feature selection techniques such as forward selection or lasso regression become inadequate. In the experimental section, the performance of the elastic net model is compared with geometrical and color based algorithms widely used in face recognition such as Procrustes nearest neighbor, Eigerifaces, or Fisher-faces. Results show that the elastic net is capable of selecting a set of discriminative features and hereby obtain higher classification rates.	Tech Univ Denmark, DK-2800 Lyngby, Denmark	Clemmensen, LH (reprint author), Tech Univ Denmark, DK-2800 Lyngby, Denmark.		Clemmensen, Line/E-9703-2011	Clemmensen, Line/0000-0001-5527-5798			Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Cevikalp H, 2005, IEEE T PATTERN ANAL, V27, P4, DOI 10.1109/TPAMI.2005.9; DAUGMAN JG, 1993, IEEE T PATTERN ANAL, V15, P1148, DOI 10.1109/34.244676; Phillips PJ, 2000, IEEE T PATTERN ANAL, V22, P1090, DOI 10.1109/34.879790; HOERL AE, 1970, TECHNOMETRICS, V12, P55; Zou H, 2005, J ROY STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x; Efron B, 2004, ANN STAT, V32, P407; Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228; Craw I, 1999, IEEE T PATTERN ANAL, V21, P725, DOI 10.1109/34.784286; Daugman J., 2002, P 2002 INT C IM PROC, V1; DRYDEN IL, 1998, STAT SHAPE ANAL WILE; GOLDSTEI.AJ, 1971, PR INST ELECTR ELECT, V59, P748, DOI 10.1109/PROC.1971.8254; Messer K., 1999, P 2 C AUD VID BAS BI; Shi J, 2006, COMPUT VIS IMAGE UND, V102, P117, DOI 10.1016/j.cviu.2005.10.002; Turk M. A., 1991, IEEE C COMP VIS PATT; Viola P., 2001, P IEEE WORKSH STAT C	16	1	1	0	1	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		978-3-540-73039-2	LECT NOTES COMPUT SC			2007	4522						61	71				11	Computer Science, Artificial Intelligence; Computer Science, Theory & Methods; Imaging Science & Photographic Technology	Computer Science; Imaging Science & Photographic Technology	BGI81	WOS:000247364000007		
S	Yang, Z; Laaksonen, J		Ersboll, BK; Pedersen, KS		Yang, Zhirong; Laaksonen, Jorma			Regularized neighborhood component analysis	IMAGE ANALYSIS, PROCEEDINGS	Lecture Notes in Computer Science		English	Proceedings Paper	15th Scandinavian Conference on Image Analysis	JUN 10-14, 2007	Aalborg, DENMARK	Aalborg Univ, Tech Univ Denmark, Univ Copenhagen, Dept Comp Sci, IT Univ Copenhagen				Discriminative feature extraction is one of the fundamental problems in pattern recognition and signal processing. It was recently proposed that maximizing the class prediction by neighboring samples in the transformed space is an effective objective for learning a low-dimensional linear embedding of labeled data. The associated methods, Neighborhood Component Analysis (NCA) and Relevant Component Analysis (RCA), have been proven to be useful preprocessing techniques for discriminative information visualization and classification. We point out here that NCA and RCA are prone to overfitting and therefore regularization is required. NCA and RCA's failure for high-dimensional data is demonstrated in this paper by experiments in facial image processing. We also propose to incorporate a Gaussian prior into the NCA objective and obtain the Regularized Neighborhood Component Analysis (RNCA). The empirical results show that the generalization can be significantly enhanced by using the proposed regularization method.	Helsinki Univ Technol, Lab Comp & informat Sci, FI-02015 Espoo, Finland	Yang, Z (reprint author), Helsinki Univ Technol, Lab Comp & informat Sci, POB 5400, FI-02015 Espoo, Finland.	zhirong.yang@tkk.fi; jorma.laaksonenl@tkk.fi					Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Phillips PJ, 2000, IEEE T PATTERN ANAL, V22, P1090, DOI 10.1109/34.879790; GIROSI F, 1995, NEURAL COMPUT, V7, P219, DOI 10.1162/neco.1995.7.2.219; Chen Z, 2002, NEURAL COMPUT, V14, P2791, DOI 10.1162/089976602760805296; Cristianini N., 2000, INTRO SUPPORT VECTOR; FISHER RA, 1963, ANN EUGENICS, V7; Flynn PJ, 2003, LECT NOTES COMPUT SC, V2688, P44; Goldberger J., 2004, NIPS 2004; Peltonen J, 2005, IEEE T NEURAL NETWOR, V16, P68, DOI 10.1109/TNN.2004.836194; Scholkopf B., 2002, LEARNING KERNELS; Yang ZR, 2005, LECT NOTES COMPUT SC, V3687, P216	11	1	1	4	4	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		978-3-540-73039-2	LECT NOTES COMPUT SC			2007	4522						253	262				10	Computer Science, Artificial Intelligence; Computer Science, Theory & Methods; Imaging Science & Photographic Technology	Computer Science; Imaging Science & Photographic Technology	BGI81	WOS:000247364000026		
S	Jordaan, JA; Ukil, A		Yin, H; Tino, P; Corchado, E; Byrne, W; Yao, X		Jordaan, J. A.; Ukil, A.			Load forecasting with support vector machines and semi-parametric method	INTELLIGENT DATA ENGINEERING AND AUTOMATED LEARNING - IDEAL 2007	Lecture Notes in Computer Science		English	Proceedings Paper	8th International Conference on Intelligent Data Engineering and Automated Learning	DEC 16-19, 2007	Birmingham, ENGLAND				NEURAL-NETWORKS	A new approach to short-term electrical load forecasting is investigated in this paper. As electrical load data are highly non-linear in nature, in the proposed approach, we first separate out the linear and the non-linear parts, and then forecast using the non-linear part only. Semi-parametric spectral estimation method is used to decompose a load data signal into a harmonic linear signal model and a non-linear trend. A support vector machine is then used to predict the non-linear trend. The final predicted signal is then found by adding the support vector machine predicted trend and the linear signal part. The performance of the proposed method seems to be more robust than using only the raw load data. This is due to the fact that the proposed method is intended to be more focused on the non-linear part rather than a diluted mixture of the linear and the non-linear parts as done usually.	[Jordaan, J. A.] Tshwane Univ Technol, ZA-0001 Pretoria, South Africa	Jordaan, JA (reprint author), Tshwane Univ Technol, Staatsartukkerue Rd, ZA-0001 Pretoria, South Africa.	jordaan.jaco@gmail.com; abhiukil@yahoo.com					[Anonymous], 2002, MATHW MATLAB DOC NEU; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; BIALKOWSKI SE, 1989, ANAL CHEM, V61, P1308, DOI 10.1021/ac00186a028; BITZER B, 1997, UPEC 1997 U POW ENG; CASARCORREDERA J, 1985, CH211888500000796 IE, P796; Draper N, 1981, APPL REGRESSION ANAL; GORRY PA, 1990, ANAL CHEM, V62, P570, DOI 10.1021/ac00205a007; JORDAAN J, 2004, T S AFRICAN I ELECT, V94, P171; Jordaan J. A., 2004, T S AFRICAN I ELECT, V95, P35; Pai PF, 2005, ELECTR POW SYST RES, V74, P417, DOI 10.1016/j.epsr.2005.01.006; Papadakis SE, 1998, IEEE T POWER SYST, V13, P480, DOI 10.1109/59.667372; PELCKMANS K, 2003, TOOLBOX USERS GUIDE; Piras A, 1996, IEEE T POWER SYST, V11, P397, DOI 10.1109/59.486124; Suykens J. A. K., 2002, LEAST SQUARES SUPPOR; Ukil A, 2006, LECT NOTES COMPUT SC, V4233, P974; Yang JF, 2006, ELECTR POW SYST RES, V76, P880, DOI 10.1016/j.epsr.2005.11.007; Zivanovic R, 2004, IEEE T POWER DELIVER, V19, P1085, DOI 10.1109/TPWRD.2004.829945; ZIVANOVIC R, 2005, POW TECH2005 C STPET; ZIVANOVIC R, 2004, 8 INT C DEV POW SYST	19	1	1	0	1	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		978-3-540-77225-5	LECT NOTES COMPUT SC			2007	4881						258	267				10	Computer Science, Hardware & Architecture; Computer Science, Information Systems; Computer Science, Theory & Methods	Computer Science	BHE04	WOS:000252394900027		
B	Omar, MK			ISCA	Omar, Mohamed Kamal			Regularized Feature-Based Maximum Likelihood Linear Regression for Speech Recognition	INTERSPEECH 2007: 8TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION, VOLS 1-4			English	Proceedings Paper	Interspeech Conference 2007	AUG 27-31, 2007	Antwerp, BELGIUM					In many automatic speech recognition (ASR) applications, maximum likelihood linear regression (MLLR), and feature-based maximum likelihood linear regression (FMLLR) are used for speaker adaptation. This paper investigates a possible generalization of FMLLR which addresses the degradation in the performance of ASR systems due to small -possibly time-varying- perturbations of the training and the testing data. We formulate the problem as a regularized maximum likelihood linear regression problem. Based on this formulation, we describe a computationally efficient algorithm for estimating the linear regression parameters which maximize the sum of the log likelihood and the negative of a measure of the sensitivity of the estimated likelihood to these perturbations. This approach does not make any assumptions about the noise model during training and testing. We present several large vocabulary speech recognition experiments that show significant recognition accuracy improvement compared to using the speaker-adapted baseline models.	IBM Corp, Thomas J Watson Res Ctr, Yorktown Hts, NY 10598 USA	Omar, MK (reprint author), IBM Corp, Thomas J Watson Res Ctr, Yorktown Hts, NY 10598 USA.	mkomar@us.ibm.com					Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Amari S, 1998, NEURAL COMPUT, V10, P251, DOI 10.1162/089976698300017746; Friedman JH, 2007, J STAT PLAN INFER, V137, P669, DOI 10.1016/j.jspi.2006.06.002; Gales M. J. F., 1997, MAXIMUM LIKELIHOOD L; Papoulis A., 1991, PROBABILITY RANDOM V; SIOHAN O, 1999, WORKSH ROB METH SPEE; ZHAN P, 1997, IEEE P ICASSP MUN GE	7	0	0	0	0	ISCA-INST SPEECH COMMUNICATION ASSOC	BAIXAS	C/O EMMANUELLE FOXONET, 4 RUE DES FAUVETTES, LIEU DIT LOUS TOURILS, BAIXAS, F-66390, FRANCE			978-1-60560-316-2				2007							2456	2459				4	Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications	Computer Science	BLE20	WOS:000269998601242		
J	Park, T				Park, Trevor			Alternative penalty functions for penalized likelihood principal components	JOURNAL OF APPLIED STATISTICS			English	Article						interpretation; Lasso penalty; multivariate exploratory analysis; principal component rotation; varimax	SELECTION; LASSO	The penalized likelihood principal component method of Park ( 2005) offers flexibility in the choice of the penalty function. This flexibility allows the method to be tailored to enhance interpretation in special cases. Of particular interest is a penalty function in the style of the Lasso that can be used to produce exactly zero loadings. Also of interest is a penalty function for cases in which interpretability is best represented by alignment with orthogonal subspaces, rather than with axis directions. In each case, a data example is presented.	Univ Florida, Dept Stat, Gainesville, FL 32611 USA	Park, T (reprint author), Univ Florida, Dept Stat, Trevor Pk,103 Griffin Floyd Hall ,POB 118545, Gainesville, FL 32611 USA.	tpark@stat.ufl.edu					Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Jolliffe IT, 2003, J COMPUT GRAPH STAT, V12, P531, DOI 10.1198/1061860032148; Zou H, 2005, J ROY STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x; Brockwell P. J., 1991, TIME SERIES THEORY M; Chipman HA, 2005, J APPL STAT, V32, P969, DOI 10.1080/02664760500168648; Davenport Michael, 1972, APPL STAT, V21, P324, DOI 10.2307/2346281; Filzmoser P, 2000, PSYCHOMETRIKA, V65, P363, DOI 10.1007/BF02296151; Jolliffe I. T., 2002, PRINCIPAL COMPONENT; JOLLIFFE IT, 1989, APPL STAT-J ROY ST C, V38, P139, DOI 10.2307/2347688; Jolliffe IT, 2000, J COMPUT GRAPH STAT, V9, P689, DOI 10.2307/1391088; Park T, 2005, J COMPUT GRAPH STAT, V14, P867, DOI 10.1198/106186005X78134; Ramsay J. O., 2002, APPL FUNCTIONAL DATA; Ramsay J. O., 1997, FUNCTIONAL DATA ANAL; VINES SK, 2000, APPL STAT, V49, P441; Zou H, 2006, J COMPUT GRAPH STAT, V15, P265, DOI 10.1198/106186006X113430	15	1	1	0	3	ROUTLEDGE JOURNALS, TAYLOR & FRANCIS LTD	ABINGDON	4 PARK SQUARE, MILTON PARK, ABINGDON OX14 4RN, OXFORDSHIRE, ENGLAND	0266-4763			J APPL STAT	J. Appl. Stat.		2007	34	7					767	777		10.1080/02664760701239859		11	Statistics & Probability	Mathematics	207OV	WOS:000249261300001		
J	Wang, HS; Li, GD; Tsai, CL				Wang, Hansheng; Li, Guodong; Tsai, Chih-Ling			Regression coefficient and autoregressive order shrinkage and selection via the lasso	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES B-STATISTICAL METHODOLOGY			English	Article						autoregression with exogenous variables; lasso; oracle estimator; regression model with autoregressive errors	NONCONCAVE PENALIZED LIKELIHOOD; MODEL SELECTION; VARIABLE SELECTION	The least absolute shrinkage and selection operator ('lasso') has been widely used in regression shrinkage and selection. We extend its application to the regression model with autoregressive errors. Two types of lasso estimators are carefully studied. The first is similar to the traditional lasso estimator with only two tuning parameters (one for regression coefficients and the other for autoregression coefficients). These tuning parameters can be easily calculated via a data-driven method, but the resulting lasso estimator may not be fully efficient. To overcome this limitation, we propose a second lasso estimator which uses different tuning parameters for each coefficient. We show that this modified lasso can produce the estimator as efficiently as the oracle. Moreover, we propose an algorithm for tuning parameter estimates to obtain the modified lasso estimator. Simulation studies demonstrate that the modified estimator is superior to the traditional estimator. One empirical example is also presented to illustrate the usefulness of lasso estimators. The extension of the lasso to the autoregression with exogenous variables model is briefly discussed.	Peking Univ, Guanghua Sch Management, Beijing 100871, Peoples R China; Univ Hong Kong, Hong Kong, Hong Kong, Peoples R China; Univ Calif Davis, Davis, CA 95616 USA	Wang, HS (reprint author), Peking Univ, Guanghua Sch Management, Beijing 100871, Peoples R China.	hansheng@gsm.pku.edu.cn	Li, Guodong/A-2741-2010	Li, Guodong/0000-0003-3137-8471			Akaike H., 1973, P 2 INT S INF THEOR, P267; Fu WJJ, 1998, J COMPUT GRAPH STAT, V7, P397, DOI 10.2307/1390712; Knight K, 2000, ANN STAT, V28, P1356; HURVICH CM, 1990, AM STAT, V44, P214, DOI 10.2307/2685338; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; Zou H, 2005, J ROY STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x; Efron B, 2004, ANN STAT, V32, P407; SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136; Box G. E. P., 1994, TIME SERIES ANAL; Breiman L, 1996, ANN STAT, V24, P2350; Brockwell P. J., 1991, TIME SERIES THEORY M; Cai JW, 2005, BIOMETRIKA, V92, P303, DOI 10.1093/biomet/92.2.303; Choi B., 1992, ARMA MODEL IDENTIFIC; COCHRANE D, 1949, J AM STAT ASSOC, V44, P32, DOI 10.2307/2280349; Fan JQ, 2004, ANN STAT, V32, P928, DOI 10.1214/009053604000000256; Fan JQ, 2002, ANN STAT, V30, P74; Gourieroux C, 1997, ARCH MODELS FINANCIA; Greene WH, 2003, ECONOMETRIC ANAL; Hamilton J, 1994, TIME SERIES ANAL; Harvey A. C., 1981, ECONOMETRIC ANAL TIM; Leng CL, 2006, STAT SINICA, V16, P1273; McQuarrie D.R., 1998, REGRESSION TIME SERI; RAMANATHAN R, 1989, INTRO ECONOMETRICS A; Shao J, 1997, STAT SINICA, V7, P221; Shi PD, 2004, J TIME SER ANAL, V25, P923, DOI 10.1111/j.1467-9892.2004.00385.x; Shumway RH, 2000, TIME SERIES ANAL ITS; TSAY RS, 1984, J AM STAT ASSOC, V79, P118; ZOU H, 2004, DEGREES FREEDOM LASS	29	59	62	5	24	BLACKWELL PUBLISHING	OXFORD	9600 GARSINGTON RD, OXFORD OX4 2DQ, OXON, ENGLAND	1369-7412			J ROY STAT SOC B	J. R. Stat. Soc. Ser. B-Stat. Methodol.		2007	69		1				63	78				16	Statistics & Probability	Mathematics	125YO	WOS:000243479600004		
J	Yuan, M; Lin, Y				Yuan, Ming; Lin, Yi			On the non-negative garrotte estimator	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES B-STATISTICAL METHODOLOGY			English	Article						elastic net; Lasso; least angle regression selection; non-negative garrotte; path consistency; piecewise linear solution path	BAYES VARIABLE SELECTION; MODEL SELECTION; ORACLE PROPERTIES; REGRESSION; LASSO	We study the non-negative garrotte estimator from three different aspects: consistency, computation and flexibility. We argue that the non-negative garrotte is a general procedure that can be used in combination with estimators other than the original least squares estimator as in its original form. In particular, we consider using the lasso, the elastic net and ridge regression along with ordinary least squares as the initial estimate in the non-negative garrotte. We prove that the non-negative garrotte has the nice property that, with probability tending to 1, the solution path contains an estimate that correctly identifies the set of important variables and is consistent for the coefficients of the important variables, whereas such a property may not be valid for the initial estimators. In general, we show that the non-negative garrotte can turn a consistent estimate into an estimate that is not only consistent in terms of estimation but also in terms of variable selection. We also show that the non-negative garrotte has a piecewise linear solution path. Using this fact, we propose an efficient algorithm for computing the whole solution path for the non-negative garrotte. Simulations and a real example demonstrate that the non-negative garrotte is very effective in improving on the initial estimator in terms of variable selection and estimation accuracy.	Georgia Inst Technol, Sch Ind & Syst Engn, Atlanta, GA 30332 USA; Univ Wisconsin, Madison, WI USA	Yuan, M (reprint author), Georgia Inst Technol, Sch Ind & Syst Engn, 755 Ferst Dr NW, Atlanta, GA 30332 USA.	myuan@isye.gatech.edu					BREIMAN L, 1995, TECHNOMETRICS, V37, P373, DOI 10.2307/1269730; Shen XT, 2002, J AM STAT ASSOC, V97, P210, DOI 10.1198/016214502753479356; Yuan M, 2006, J ROY STAT SOC B, V68, P49, DOI 10.1111/j.1467-9868.2005.00532.x; Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; Zou H, 2005, J ROY STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x; Friedman J, 2004, ANN STAT, V32, P102; GEORGE EI, 1993, J AM STAT ASSOC, V88, P881, DOI 10.2307/2290777; Zou H, 2006, J AM STAT ASSOC, V101, P1418, DOI 10.1198/016214506000000735; Efron B, 2004, ANN STAT, V32, P407; Meinshausen N, 2006, ANN STAT, V34, P1436, DOI 10.1214/009053606000000281; Breiman L, 1996, ANN STAT, V24, P2350; FOSTER DP, 1994, ANN STAT, V22, P1947, DOI 10.1214/aos/1176325766; George EI, 2000, BIOMETRIKA, V87, P731, DOI 10.1093/biomet/87.4.731; Osborne MR, 2000, J COMPUT GRAPH STAT, V9, P319, DOI 10.2307/1390657; Stamey T., 1989, J UROLOGY, V16, P1076; Yuan M, 2005, J AM STAT ASSOC, V100, P1215, DOI 10.1198/016214505000000367; ZHAO P, 2006, J MACH LEARN RES, V7, P2481	19	82	87	1	8	BLACKWELL PUBLISHING	OXFORD	9600 GARSINGTON RD, OXFORD OX4 2DQ, OXON, ENGLAND	1369-7412			J ROY STAT SOC B	J. R. Stat. Soc. Ser. B-Stat. Methodol.		2007	69		2				143	161		10.1111/j.1467-9868.2007.00581.x		19	Statistics & Probability	Mathematics	141RR	WOS:000244596900002		
J	Yuan, M; Ekici, A; Lu, ZS; Monteiro, R				Yuan, Ming; Ekici, Ali; Lu, Zhaosong; Monteiro, Renato			Dimension reduction and coefficient estimation in multivariate linear regression	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES B-STATISTICAL METHODOLOGY			English	Article						conic programming; dimension reduction; group variable selection; Ky Fan norm; penalized likelihood	MODEL SELECTION; VARIABLE SELECTION; RESPONSES	We introduce a general formulation for dimension reduction and coefficient estimation in the multivariate linear model. We argue that many of the existing methods that are commonly used in practice can be formulated in this framework and have various restrictions. We continue to propose a new method that is more flexible and more generally applicable. The method proposed can be formulated as a novel penalized least squares estimate. The penalty that we employ is the coefficient matrix's Ky Fan norm. Such a penalty encourages the sparsity among singular values and at the same time gives shrinkage coefficient estimates and thus conducts dimension reduction and coefficient estimation simultaneously in the multivariate linear model. We also propose a generalized cross-validation type of criterion for the selection of the tuning parameter in the penalized least squares. Simulations and an application in financial econometrics demonstrate competitive performance of the new method. An extension to the non-parametric factor model is also discussed.	Georgia Inst Technol, Sch Ind & Syst Engn, Atlanta, GA 30332 USA; Carnegie Mellon Univ, Pittsburgh, PA 15213 USA	Yuan, M (reprint author), Georgia Inst Technol, Sch Ind & Syst Engn, 755 Ferst Dr N W, Atlanta, GA 30332 USA.	myuan@isye.gatech.edu	Ekici, Ali/A-5388-2015	Ekici, Ali/0000-0002-9627-4780			Anderson T. W., 2003, INTRO MULTIVARIATE S; ANDERSON TW, 1951, ANN MATH STAT, V22, P327, DOI 10.1214/aoms/1177729580; Yuan M, 2006, J ROY STAT SOC B, V68, P49, DOI 10.1111/j.1467-9868.2005.00532.x; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; MASSY WF, 1965, J AM STAT ASSOC, V60, P234, DOI 10.2307/2283149; Brown PJ, 2002, J R STAT SOC B, V64, P519, DOI 10.1111/1467-9868.00348; FRANK IE, 1993, TECHNOMETRICS, V35, P109, DOI 10.2307/1269656; Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321; Eilers PHC, 1996, STAT SCI, V11, P89, DOI 10.1214/ss/1038425655; GOLUB GH, 1979, TECHNOMETRICS, V21, P215, DOI 10.1080/00401706.1979.10489751; Turlach BA, 2005, TECHNOMETRICS, V47, P349, DOI 10.1198/004017005000000139; Tutuncu RH, 2003, MATH PROGRAM, V95, P189, DOI 10.1007/s10107-002-0347-5; Johnstone IM, 2001, ANN STAT, V29, P295, DOI 10.1214/aos/1009210544; Bakin S, 1999, THESIS AUSTR NATL U; BEDRICK EJ, 1994, BIOMETRICS, V50, P226, DOI 10.2307/2533213; Breiman L, 1996, ANN STAT, V24, P2350; Breiman L, 1997, J ROY STAT SOC B MET, V59, P3, DOI 10.1111/1467-9868.00054; BROOKS R, 1994, J AM STAT ASSOC, V89, P1374, DOI 10.2307/2290999; Brown PJ, 1998, J R STAT SOC B, V60, P627, DOI 10.1111/1467-9868.00144; Brown PJ, 1999, BIOMETRIKA, V86, P635; Fujikoshi Y, 1997, BIOMETRIKA, V84, P707, DOI 10.1093/biomet/84.3.707; Hastie T., 1990, GENERALIZED ADDITIVE; Horn R. A., 1991, TOPICS MATRIX ANAL; Hotelling H, 1935, J EDUC PSYCHOL, V26, P139, DOI 10.1037/h0058165; Izenman A. J., 1975, Journal of Multivariate Analysis, V5, DOI 10.1016/0047-259X(75)90042-1; Lutz RW, 2006, STAT SINICA, V16, P471; Nemirovski A., 2001, LECT MODERN CONVEX O; Reinsel G., 1998, MULTIVARIATE REDUCED; Reinsel G. C., 1997, ELEMENTS MULTIVARIAT, V2nd; RUPPERT D., 1997, PENALIZED REGRESSION; SMITH H, 1962, BIOMETRICS, V18, P22, DOI 10.2307/2527708; Wold H., 1975, PERSPECTIVES PROBABI	32	50	50	2	10	BLACKWELL PUBLISHING	OXFORD	9600 GARSINGTON RD, OXFORD OX4 2DQ, OXON, ENGLAND	1369-7412			J ROY STAT SOC B	J. R. Stat. Soc. Ser. B-Stat. Methodol.		2007	69		3				329	346		10.1111/j.1467-9868.2007.00591.x		18	Statistics & Probability	Mathematics	171CW	WOS:000246713500003		
J	Nott, DJ; Kuk, AYC				Nott, David J.; Kuk, Anthony Y. C.			Coefficient sign prediction methods for model selection	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES B-STATISTICAL METHODOLOGY			English	Article						Bayesian variable selection; linear mixed model; variance component testing; Zheng-Loh procedure	VARIABLE SELECTION; LINEAR-MODELS; REGRESSION; CHOICE; NORMALITY	We consider a Bayesian model selection strategy based on predicting the signs of the coefficients in a regression model, i.e. we consider identification of coefficients in a full or encompassing model for which we can confidently predict whether they are positive or negative. This is useful when our main purpose in doing model selection is interpretation, since the sign of a coefficient is often of primary importance for this task. In the case of a linear model with standard non-informative prior, we connect our sign coefficient prediction approach to the classical Zheng-Loh procedure for model selection. One advantage of our approach is that only specification of a prior on the full model is required, unlike standard Bayesian variable selection approaches which require specification of prior distributions on parameters in all submodels, and specification of a prior on the model itself. We consider applying our method with proper hierarchical shrinkage priors, which makes the procedure more useful in 'large p, small n' regression problems with more predictors than observations and in problems involving multicollinearity. In these problems we may wish to do prediction by using shrinkage methods in the full model, but interpreting which variables are important is also of interest. We compare selection by using our coefficient sign prediction approach with the recently proposed elastic net procedure of Zou and Hastie and observe that our method shares some of the features of the elastic net such as a group selection property. The method can be extended to more complex model selection problems such as selection on variance components in random-effects models. For selection on variance components where the parameter of interest is non-negative and hence prediction of the sign of the parameter not the appropriate way to proceed, we consider instead prediction of the sign of the score component for the parameter at zero, obtaining a method that is related to classical score tests on variance components.	Natl Univ Singapore, Dept Stat & Appl Probabil, Singapore 117546, Singapore	Nott, DJ (reprint author), Natl Univ Singapore, Dept Stat & Appl Probabil, Singapore 117546, Singapore.	standj@nus.edu.sg					Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Raftery AE, 1997, J AM STAT ASSOC, V92, P179, DOI 10.2307/2291462; Lonnstedt I, 2002, STAT SINICA, V12, P31; Haario H, 2005, COMPUTATION STAT, V20, P265, DOI 10.1007/BF02789703; Brown PJ, 2002, J R STAT SOC B, V64, P519, DOI 10.1111/1467-9868.00348; Zou H, 2005, J ROY STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x; GEORGE EI, 1993, J AM STAT ASSOC, V88, P881, DOI 10.2307/2290777; Fernandez C, 2001, J ECONOMETRICS, V100, P381, DOI 10.1016/S0304-4076(00)00076-2; [Anonymous], 2005, BIOINFORMATICS COMPU, DOI DOI 10.1007/0-387-29362-0_23; Kohn R, 2001, STAT COMPUT, V11, P313, DOI 10.1023/A:1011916902934; Brown PJ, 1999, BIOMETRIKA, V86, P635; CHEN CF, 1985, J ROY STAT SOC B MET, V47, P540; Chen Z, 2003, BIOMETRICS, V59, P762, DOI 10.1111/j.0006-341X.2003.00089.x; Cotsapas C, 2003, COLD SPRING HARB SYM, V68, P109, DOI 10.1101/sqb.2003.68.109; Draper D, 2000, J GLOBAL OPTIM, V18, P399, DOI 10.1023/A:1026504402220; Dupuis JA, 2003, J STAT PLAN INFER, V111, P77, DOI 10.1016/S0378-3758(02)00286-0; Gelfand AE, 1998, BIOMETRIKA, V85, P1, DOI 10.1093/biomet/85.1.1; George EI, 2000, BIOMETRIKA, V87, P731, DOI 10.1093/biomet/87.4.731; HEYDE CC, 1979, J ROY STAT SOC B MET, V41, P184; LAUD PW, 1995, J ROY STAT SOC B MET, V57, P247; Lin XH, 1997, BIOMETRIKA, V84, P309, DOI 10.1093/biomet/84.2.309; LINDLEY DV, 1995, J R STAT SOC B, V57, P57; LINDLEY DV, 1968, J R STAT SOC B, V30, P31; Marriott JM, 2001, SCAND J STAT, V28, P87, DOI 10.1111/1467-9469.00225; Nguyen DV, 2002, BIOMETRICS, V58, P701, DOI 10.1111/j.0006-341X.2002.00701.x; Smith M, 1996, J ECONOMETRICS, V75, P317, DOI 10.1016/0304-4076(95)01763-1; Smyth GK, 2004, STAT APPL GENET MOL, V3, P1; Spiegelhalter DJ, 2002, J ROY STAT SOC B, V64, P583, DOI 10.1111/1467-9868.00353; ZASLAVSKY AM, 2005, ANN STAT, V33, P1; Zellner A, 1986, BAYESIAN INFERENCE D, P233; ZHENG XD, 1995, J AM STAT ASSOC, V90, P151	31	2	2	2	3	WILEY-BLACKWELL	HOBOKEN	111 RIVER ST, HOBOKEN 07030-5774, NJ USA	1369-7412			J R STAT SOC B	J. R. Stat. Soc. Ser. B-Stat. Methodol.		2007	69		3				447	461		10.1111/j.1467-9868.2007.00597.x		15	Statistics & Probability	Mathematics	171CW	WOS:000246713500009		
J	Park, MY; Hastie, T				Park, Mee Young; Hastie, Trevor			L-1-regularization path algorithm for generalized linear models	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES B-STATISTICAL METHODOLOGY			English	Article						generalized linear model; lasso; path algorithm; predictor-corrector method; regularization; variable selection	VARIABLE SELECTION; LOGISTIC-REGRESSION; GENE-EXPRESSION; REGULARIZATION; CANCER; LASSO	We introduce a path following algorithm for L-1-regularized generalized linear models. The L-1-regularization procedure is useful especially because it, in effect, selects variables according to the amount of penalization on the L-1-norm of the coefficients, in a manner that is less greedy than forward selection-backward deletion. The generalized linear model path algorithm efficiently computes solutions along the entire regularization path by using the predictor-corrector method of convex optimization. Selecting the step length of the regularization parameter is critical in controlling the overall accuracy of the paths; we suggest intuitive and flexible strategies for choosing appropriate values. We demonstrate the implementation with several simulated and real data sets.	Google Inc, Mountain View, CA 94043 USA; Stanford Univ, Stanford, CA 94305 USA	Park, MY (reprint author), Google Inc, Mee Young Pk,1600 Amphitheatre Pkwy, Mountain View, CA 94043 USA.	meeyoung@google.com					Allgower E. L., 1990, NUMERICAL CONTINUATI; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Heinze G, 2002, STAT MED, V21, P2409, DOI 10.1002/sim.1047; Osborne MR, 2000, IMA J NUMER ANAL, V20, P389, DOI 10.1093/imanum/20.3.389; Golub TR, 1999, SCIENCE, V286, P531, DOI 10.1126/science.286.5439.531; Zou H, 2005, J ROY STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x; Efron B, 2004, ANN STAT, V32, P407; Tibshirani R, 2002, P NATL ACAD SCI USA, V99, P6567, DOI 10.1073/pnas.082099299; COX DR, 1972, J R STAT SOC B, V34, P187; CROWLEY J, 1977, J AM STAT ASSOC, V72, P27, DOI 10.2307/2286902; FIRTH D, 1993, BIOMETRIKA, V80, P27, DOI 10.1093/biomet/80.1.27; GARCIA C. B., 1981, PATHWAYS SOLUTIONS F; Genkin A., 2004, LARGE SCALE BAYESIAN; Hastie T., 2001, ELEMENTS STAT LEARNI; Hastie T, 2004, J MACH LEARN RES, V5, P1391; Lokhorst J, 1999, LASSO GEN LINEAR MOD; McCullagh P., 1989, GEN LINEAR MODELS; Munkres J. R., 1991, ANAL MANIFOLDS; ROSSET S, 2004, NEURAL INFORMATION P; Rosset S, 2004, J MACH LEARN RES, V5, P941; Rosset S., 2003, PIECEWISE LINEAR REG; Shevade SK, 2003, BIOINFORMATICS, V19, P2246, DOI 10.1093/bioinformatics/btg308; STEIN CM, 1981, ANN STAT, V9, P1135, DOI 10.1214/aos/1176345632; Tibshirani R, 1997, STAT MED, V16, P385, DOI 10.1002/(SICI)1097-0258(19970228)16:4<385::AID-SIM380>3.0.CO;2-3; Zhao P., 2004, BOOSTED LASSO; Zhu J., 2003, NEURAL INFORM PROCES; ZHU J, 2004, BIOSTATISTICS, V46, P505; ZOU H, 2004, DEGREES FREEDOM LASS	28	241	246	5	18	BLACKWELL PUBLISHING	OXFORD	9600 GARSINGTON RD, OXFORD OX4 2DQ, OXON, ENGLAND	1369-7412			J ROY STAT SOC B	J. R. Stat. Soc. Ser. B-Stat. Methodol.		2007	69		4				659	677		10.1111/j.1467-9868.2007.00607.x		19	Statistics & Probability	Mathematics	207KM	WOS:000249250000008		
J	Kovac, A				Kovac, Arne			Discussion on the paper by ramsay, hooker, campbell and cao	JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES B-STATISTICAL METHODOLOGY			English	Editorial Material							POPULATION PHARMACOKINETIC MODEL; MAXIMUM-LIKELIHOOD-ESTIMATION; HUMAN CIRCADIAN PACEMAKER; DIFFERENTIAL-EQUATIONS; PARAMETER-ESTIMATION; TIME-SERIES; DERIVATIVE INFORMATION; MEASLES EPIDEMICS; DYNAMIC-MODELS; OPTIMIZATION		Univ Bristol, Bristol BS8 1TH, Avon, England; Univ London Imperial Coll Sci Technol & Med, London SW7 2AZ, England; Univ Virginia, Charlottesville, VA USA; Univ London London Sch Econ & Polit Sci, London WC2A 2AE, England; Queen Mary Univ London, London, England; Univ Alberta, Edmonton, AB, Canada; Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; MIT, Cambridge, MA 02139 USA; Harvard Univ, Sch Med, Boston, MA USA; Univ Notre Dame, Notre Dame, IN 46556 USA; Univ Paris 05, Paris, France; Dalhousie Univ, Halifax, NS, Canada; McMaster Univ, Hamilton, ON L8S 4L8, Canada; Cornell Univ, Ithaca, NY USA; Purdue Univ, W Lafayette, IN 47907 USA; Fred Hutchinson Canc Res Ctr, Seattle, WA 98104 USA; Georgia Inst Technol, Atlanta, GA 30332 USA; Texas A&M Univ, College Stn, TX USA; Univ Neuchatel, CH-2000 Neuchatel, Switzerland; Univ Michigan, Ann Arbor, MI 48109 USA; Univ Pittsburgh, Pittsburgh, PA 15260 USA; Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; Humboldt Univ, Berlin, Germany; Univ Western Ontario, London, ON, Canada; Univ Alberta, Edmonton, AB, Canada; Indiana Univ, Indianapolis, IN 46204 USA; Univ Geneva, CH-1211 Geneva 4, Switzerland; Univ Rochester, Rochester, NY 14627 USA	Kovac, A (reprint author), Univ Bristol, Bristol BS8 1TH, Avon, England.						Anderson R. M., 1991, INFECTIOUS DIS HUMAN; Anger G., 1990, INVERSE PROBLEMS DIF; FINE PEM, 1982, INT J EPIDEMIOL, V11, P5, DOI 10.1093/ije/11.1.5; Davies PL, 2001, ANN STAT, V29, P1, DOI 10.1214/aos/996986501; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Czeisler CA, 1999, SCIENCE, V284, P2177, DOI 10.1126/science.284.5423.2177; Gelman A, 1996, J AM STAT ASSOC, V91, P1400, DOI 10.2307/2291566; Liu YH, 2001, J COMPUT NEUROSCI, V10, P25, DOI 10.1023/A:1008916026143; Kennedy MC, 2001, J ROY STAT SOC B, V63, P425, DOI 10.1111/1467-9868.00294; Bjornstad ON, 2002, ECOL MONOGR, V72, P169, DOI 10.2307/3100023; Moles CG, 2003, GENOME RES, V13, P2467, DOI 10.1101/gr.1262503; Dushoff J, 2004, P NATL ACAD SCI USA, V101, P16915, DOI 10.1073/pnas.0407293101; Davidian M, 2003, J AGR BIOL ENVIR ST, V8, P387, DOI 10.1198/1085711032697; Ionides EL, 2006, P NATL ACAD SCI USA, V103, P18438, DOI 10.1073/pnas.0603181103; HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500; Earn DJD, 2000, SCIENCE, V287, P667, DOI 10.1126/science.287.5453.667; Berger JO, 1999, STAT SCI, V14, P1; Ascher U., 1998, COMPUTER METHODS ORD; Aster RC, 2005, INT GEOPHYS SER, V90, P1; Bauch CT, 2003, P ROY SOC B-BIOL SCI, V270, P1573, DOI 10.1098/rspb.2003.2410; BAYARRI M, 2007, IN PRESS ANN STAT; BERGSTRO.AR, 1966, ECONOMETRICA, V34, P173, DOI 10.2307/1909861; Beskos A, 2006, J ROY STAT SOC B, V68, P333, DOI 10.1111/j.1467-9868.2006.00552.x; BIEGLER LT, 2003, LECT NOTES COMPUTNL, V30; Bock H.G., 1983, NUMERICAL TREATMENT, P95; Boker S, 2004, MATH MODELL, V19, P151; BROWN EN, 1987, THESIS HARVARD U CAM; Brown EN, 2000, AM J PHYSIOL-ENDOC M, V279, pE669; Campbell D., 2007, THESIS MCGILL U MONT; CANDES E, 2005, DANTZIG SELECTOR STA; CASEY R, 2004, THESIS CORNELL U ITH; CHEN J, 2007, IN PRESS STAT SIN; Chorin AJ, 2004, P NATL ACAD SCI USA, V101, P15013, DOI 10.1073/pnas.0406222101; Coulson T, 2004, TRENDS ECOL EVOL, V19, P359, DOI 10.1016/j.tree.2004.05.008; Cressie NAC, 1991, STAT SPATIAL DATA; CZANNER G, 2004, THESIS U PITTSBURGH; CZANNER G, 2007, MAXIMUM LIKELHOOD ES; CZEISLER CA, 1989, SCIENCE, V244, P1328, DOI 10.1126/science.2734611; DEUFLHARD P, 2000, SCI COMPUT ORDINARY; De Valpine P, 2004, J AM STAT ASSOC, V99, P523, DOI 10.1198/016214504000000476; Diks C, 1999, NONLINEAR TIME SERIE; Dowd M, 2007, J MARINE SYST, V68, P439, DOI 10.1016/j.jmarsys.2007.01.007; Dowd M, 2006, ENVIRONMETRICS, V17, P435, DOI 10.1002/env.780; Durbin J., 2001, TIMES SERIES ANAL ST; Ellner S. P., 2006, DYNAMIC MODELS BIOL; Ellner SP, 2002, ECOLOGY, V83, P2256, DOI 10.2307/3072057; Ellner SP, 1998, AM NAT, V151, P425, DOI 10.1086/286130; Englezos P., 2001, APPL PARAMETER ESTIM; Evensen G., 2003, OCEAN DYNAM, V53, P343, DOI DOI 10.1007/S10236-003-0036-9; Fahrmeir L, 1994, MULTIVARIATE STAT MO; Finkenstadt BF, 2001, APPL STAT, V49, P187, DOI [10.1111/1467-9876.00187, DOI 10.1111/1467-9876.00187]; Geyer C.J., 1991, P 23 S INT, P156; Godsill SJ, 2004, J AM STAT ASSOC, V99, P156, DOI 10.1198/016214504000000151; HIGDON D, 2007, IN PRESS J AM STAT A; HOOKER G, 2007, IN PRESS FORCING FUN; Hotelling H, 1927, J AM STAT ASSOC, V22, P283, DOI 10.2307/2276800; Huang YX, 2006, J APPL STAT, V33, P155, DOI 10.1080/02664760500250552; Huang YX, 2006, BIOMETRICS, V62, P413, DOI 10.1111/j.1541-0420.2005.00447.x; Ito K., 1951, AM MATH SOC MEMOIRS, V4; JALMAN RE, 1960, ASME J BASIC ENG, V82, P35; Judd K, 2004, PHYSICA D, V196, P224, DOI 10.1016/j.physd.2004.03.020; Judd K, 2004, PHYSICA D, V190, P153, DOI 10.1016/j.physd.2003.10.011; JUDD K, 2007, PHYS REV E, V75; Kitagawa G., 1996, J COMPUTATIONAL GRAP, V5, P1, DOI DOI 10.2307/1390750; Koch C, 1999, BIOPHYSICS COMPUTATI; Kunsch HR, 2005, ANN STAT, V33, P1983, DOI 10.1214/009053605000000426; KURTZ T. G., 1980, LECT NOTES BIOMATH, V38, P449, DOI 10.1007/978-3-642-61850-5_39; Lande R., 2003, STOCHASTIC POPULATIO; Lawson C., 1995, SOLVING LEAST SQUARE; Lele SR, 2007, ECOL LETT, V10, P551, DOI 10.1111/j.1461-0248.2007.01047.x; Lewis J. M., 2006, DYNAMIC DATA ASSIMIL; Li L, 2004, BIOMETRICS, V60, P451, DOI 10.1111/j.0006-341X.2004.00190.x; Li L, 2002, BIOMETRICS, V58, P601, DOI 10.1111/j.0006-341X.2002.00601.x; Li ZF, 2005, IMA J NUMER ANAL, V25, P264, DOI 10.1093/imanum/drh016; LONDON WP, 1973, AM J EPIDEMIOL, V98, P453; McSharry PE, 2004, PHYSICA D, V192, P1, DOI 10.1016/j.physd.2004.01.003; MITCHELL T, 1994, J STAT PLAN INFER, V41, P377, DOI 10.1016/0378-3758(94)90030-2; Molenaar PCM, 2003, BRIT J MATH STAT PSY, V56, P199, DOI 10.1348/000711003770480002; Mood AM, 1940, ANN MATH STAT, V11, P367, DOI 10.1214/aoms/1177731825; MORRIS MD, 1993, TECHNOMETRICS, V35, P243, DOI 10.2307/1269517; Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5; O'Hagan A., 1992, BAYESIAN STAT, V4, P345; Pillai G, 2005, J PHARMACOKINET PHAR, V32, P161, DOI 10.1007/s10928-005-0062-y; Prinz AA, 2004, NAT NEUROSCI, V7, P1345, DOI 10.1038/nn1352; R Core Development Team, 2006, R LANG ENV STAT COMP; Ramsay JO, 1998, J ROY STAT SOC B, V60, P365, DOI 10.1111/1467-9868.00130; Robert CP, 1998, STAT COMPUT, V8, P145, DOI 10.1023/A:1008938201645; Ruppert D, 2005, SEMIPARAMETRIC REGRE; Sarkka S., 2006, P NONL STAT SIGN PRO; Schaffer W M, 1985, IMA J Math Appl Med Biol, V2, P221; SCHWARTZ IB, 1983, J MATH BIOL, V18, P233, DOI 10.1007/BF00276090; Singer H., 1993, J TIME SER ANAL, V14, P527, DOI 10.1111/j.1467-9892.1993.tb00162.x; Smith L. A., 2000, NONLINEAR DYNAMICS S, P31; SOLAK E, 2003, ADV NEURAL INFORM PR, V16; Stengel R, 1994, OPTIMAL CONTROL ESTI; Tanartkit P, 1996, IND ENG CHEM RES, V35, P1853, DOI 10.1021/ie950543v; TANARTKIT P, 1995, IND ENG CHEM RES, V34, P1253, DOI 10.1021/ie00043a029; Tarantola A., 2005, INVERSE PROBLEM THEO; Thompson KR, 2000, ENVIRONMETRICS, V11, P183, DOI 10.1002/(SICI)1099-095X(200003/04)11:2<183::AID-ENV401>3.3.CO;2-8; TIEN JH, 2007, THESIS CORNELL U ITH; TIEN JH, 2007, J COMPUTNL NEURSCI; TJOA IB, 1991, IND ENG CHEM RES, V30, P376, DOI 10.1021/ie00050a015; Turchin P., 2003, COMPLEX POPULATION D; VARAH JM, 1982, SIAM J SCI STAT COMP, V3, P28, DOI 10.1137/0903003; WAHBA G, 1978, J ROY STAT SOC B MET, V40, P364; Wahba G., 1990, SPLINE MODELS OBSERV; WAHBA G, 1990, COMMUN STAT THEORY, V19, P1685, DOI 10.1080/03610929008830285; Wallinga J, 2004, AM J EPIDEMIOL, V160, P509, DOI 10.1093/aje/kwh255; Wood S, 2006, GENERALIZED ADDITIVE; WU H, 2007, IN PRESS PARAMETER I; ZENKER S, 2006, J CRIT CARE, V21, P350, DOI 10.1016/j.jcrc.2006.10.013; Zimmer C, 1999, SCIENCE, V284, P83, DOI 10.1126/science.284.5411.83	112	0	0	1	3	WILEY-BLACKWELL	HOBOKEN	111 RIVER ST, HOBOKEN 07030-5774, NJ USA	1369-7412	1467-9868		J R STAT SOC B	J. R. Stat. Soc. Ser. B-Stat. Methodol.		2007	69		5				770	796				27	Statistics & Probability	Mathematics	229LW	WOS:000250806500002		
B	Arnold, A; Liu, Y; Abe, N		Berkhin, P; Caruana, R; Wu, X; Gaffney, S		Arnold, Andrew; Liu, Yan; Abe, Naoki			Temporal Causal Modeling with Graphical Granger Methods	KDD-2007 PROCEEDINGS OF THE THIRTEENTH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING			English	Proceedings Paper	13th International Conference on Knowledge Discovery and Data Mining	AUG 12-15, 2007	San Jose, CA	ACM SIGKDD, ACM SIGMOD		Graphical models; Causal modeling; time series data	SELECTION; LASSO	The need for mining causality, beyond mere statistical correlations, for real world problems has been recognized widely. Many of these applications naturally involve temporal data, which raises the challenge of how best to leverage the temporal information for causal modeling. Recently graphical modeling with the concept of "Granger causality", based on the intuition that a cause helps predict its effects in the future, has gained attention in many domains involving time series data analysis. With the surge of interest in model selection methodologies for regression, such as the Lasso, as practical alternatives to solving structural learning of graphical models, the question arises whether and how to combine these two notions into a practically viable approach for temporal causal modeling. In this paper, we examine a host of related algorithms that, loosely speaking, fall under the category of graphical Granger methods, and characterize their relative performance from multiple viewpoints. Our experiments show, for instance, that the Lasso algorithm exhibits consistent gain over the canonical pairwise graphical Granger method. We also characterize conditions under which these variants of graphical Granger methods perform well in comparison to other benchmark methods. Finally, we apply these methods to a real world data set involving key performance indicators of corporations, and present some concrete results.	[Arnold, Andrew] Carnegie Mellon Univ, Sch Comp Sci, Machine Learning Dept, Pittsburgh, PA 15213 USA	Arnold, A (reprint author), Carnegie Mellon Univ, Sch Comp Sci, Machine Learning Dept, Pittsburgh, PA 15213 USA.	aarnold@cs.cmu.edu; liuya@us.ibm.com; nabe@us.ibm.com					Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Silva R, 2006, J MACH LEARN RES, V7, P191; Meinshausen N, 2006, ANN STAT, V34, P1436, DOI 10.1214/009053606000000281; Chu T., 2004, DATA DRIVEN METHODS; CHU T, 2006, J MACHINE LEAR UNPUB; Coppersmith D., 1990, J SYMB COMPUT, V9; DEBRA A, 2003, J MULTIVARIATE ANAL, V90, P196; Drton M., 2004, 457 U WASH DEP STAT; Efron B., 2003, ANN STAT; Eichler M., 2006, GRAPHICAL MODELING M; Friedman N, 1999, UAI 99; GILBERT PD, 1995, J FORECASTING SPECIA, V14; GOLUB G, TECHNOMETRICS, V21, P215; Granger C.W.J., 1969, ECONOMETRICA, V37, P424, DOI DOI 10.2307/1912791; Heckerman D, 1999, LEARNING GRAPHICAL M; HOYER PO, 2006, P 3 EUR WORKSH PROB, P155; KALISCH M, 2005, 130 ETH; Kaplan R., 1992, HARVARD BUSINESS JAN, P71; Kaplan R. S., 1996, BALANCED SCORECARD T; Lauritzen S. L., 1996, GRAPHICAL MODELS; MEEK C, 1996, THESIS CARNEGIE MELL; MONETA A, 2006, P 5 INT C COMP INT E; OPGENRHEIN R, 2007, BMC BIOIN S IN PRESS; Pearl J., 2000, CAUSALITY; Roweis S, 1999, NEURAL COMPUT, V11, P305, DOI 10.1162/089976699300016674; Scheines R., 1994, TETRAD 2 TOOLS DISCO; Shimizu S, 2006, COMPUT STAT DATA AN, V50, P3278, DOI 10.1016/j.csda.2005.05.004; Spirtes P., 2000, CAUSATION PREDICTION; Valdes-Sosa PA, 2005, PHILOS T ROY SOC B, V360, P969, DOI 10.1098/rstb.2005.1654	29	12	12	0	1	ASSOC COMPUTING MACHINERY	NEW YORK	1515 BROADWAY, NEW YORK, NY 10036-9998 USA			978-1-59593-609-7				2007							66	75				10	Computer Science, Artificial Intelligence	Computer Science	BJK16	WOS:000266628300007		
B	Bell, RM; Koren, Y; Volinsky, C		Berkhin, P; Caruana, R; Wu, X; Gaffney, S		Bell, Robert M.; Koren, Yehuda; Volinsky, Chris			Modeling Relationships at Multiple Scales to Improve Accuracy of Large Recommender Systems	KDD-2007 PROCEEDINGS OF THE THIRTEENTH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING			English	Proceedings Paper	13th International Conference on Knowledge Discovery and Data Mining	AUG 12-15, 2007	San Jose, CA	ACM SIGKDD, ACM SIGMOD		collaborative filtering; recommender systems		The collaborative filtering approach to recommender systems predicts user preferences for products or services by learning past user-item relationships. In this work, we propose novel algorithms for predicting user ratings of items by integrating complementary models that focus on patterns at different scales. At a local scale, we use a neghborhood-based technique that infers ratings from observed ratings by similar users or of similar items. Unlike previous local approaches, our method is based on a formal model that accounts for interactions within the neighborhood, leading to improved estimation quality. At a higher, regional, scale, we use SVD-like matrix factorization for recovering the major structural patterns in the user-item rating matrix. Unlike previous approaches that require imputations in order to fill in the unknown matrix entries, our new iterative algorithm avoids imputation. Because the models involve estimation of millions, or even billions, of parameters, shrinkage of estimated values to account for sampling variability proves crucial to prevent overfitting. Both the local and the regional approaches, and in particular their combination through a unifying model, compare favorably with other approaches and deliver substantially better results than the commercial Netflix Cinematch recommender system on a large publicly available data set.	[Bell, Robert M.; Koren, Yehuda; Volinsky, Chris] AT&T Labs Res, Florham Pk, NJ 07932 USA	Bell, RM (reprint author), AT&T Labs Res, 180 Pk Ave, Florham Pk, NJ 07932 USA.	rbell@research.att.com; yehuda@research.att.com; volinsky@research.att.com					Adomavicius G., 2005, IEEE T KNOWL DATA EN, V17, P634; DEERWESTER S, 1990, J AM SOC INFORM SCI, V41, P391, DOI 10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9; Linden G, 2003, IEEE INTERNET COMPUT, V7, P76, DOI 10.1109/MIC.2003.1167344; GOLDBERG D, 1992, COMMUN ACM, V35, P61, DOI 10.1145/138859.138867; BELL R, 2007, IMPROVED NEIGH UNPUB; Goldberg K, 2001, INFORM RETRIEVAL, V4, P133, DOI 10.1023/A:1011419012209; Golub G., 1996, MATRIX COMPUTATIONS; Herlocker J. L., 1999, Proceedings of SIGIR '99. 22nd International Conference on Research and Development in Information Retrieval, DOI 10.1145/312624.312682; Kim D, 2005, EXPERT SYST APPL, V28, P823, DOI 10.1016/j.eswa.2004.12.037; Konstan JA, 1997, COMMUN ACM, V40, P77, DOI 10.1145/245108.245126; NOCEDAL J., 1999, NUMERICAL OPTIMIZATI; Roweis S, 1998, ADV NEUR IN, V10, P626; Sarwar B, 2001, P 10 INT C WORLD WID, P285, DOI DOI 10.1145/371920.372071; SARWAR BM, APPL DIMENSIONALITY; Tibshirani R, 1996, J ROYAL STAT SOC B, V58; Wang J., 2006, Proceedings of the Twenty-Ninth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, DOI 10.1145/1148170.1148257	16	35	35	1	2	ASSOC COMPUTING MACHINERY	NEW YORK	1515 BROADWAY, NEW YORK, NY 10036-9998 USA			978-1-59593-609-7				2007							95	104				10	Computer Science, Artificial Intelligence	Computer Science	BJK16	WOS:000266628300010		
B	Li, P		Berkhin, P; Caruana, R; Wu, X; Gaffney, S		Li, Ping			Very Sparse Stable Random Projections for Dimension Reduction in l(alpha) (0 < alpha <= 2) Norm	KDD-2007 PROCEEDINGS OF THE THIRTEENTH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING			English	Proceedings Paper	13th International Conference on Knowledge Discovery and Data Mining	AUG 12-15, 2007	San Jose, CA	ACM SIGKDD, ACM SIGMOD		Dimension Reductions; Random Projections; Stable Distributions; Sparse Projections; Asymptotic Analysis; Convergence	SUPPORT VECTOR MACHINES; JOHNSON-LINDENSTRAUSS; FINDING MOTIFS; DISTRIBUTIONS; RETRIEVAL; CLASSIFICATION; INFORMATION; REGRESSION	The method of stable random projections is a useful tool for efficiently computing the l(alpha) (0 < alpha <= 2) norms and distances in lmassive data in one pass. Consider a data matrix A is an element of R(nxD). If we multiply A with a projection matrix R is an element of R(DXk) (k << D), whose entries are i.i.d. samples of an a-stable distribution, then the projected matrix B = A x R is an element of R(nXk) contains enough information to approximately recover the l(alpha) properties in A. We propose very sparse stable random projections, by replacing the a-stable distribution with a (much simpler) mixture of a symmetric alpha-Pareto distribution (with probability beta, 0 < beta <= 1) and a point mass at the origin (with probability 1 - beta). This leads to a significant 1/beta-fold speedup for small beta when computing B = A x R and a 1/beta-fold cost reduction in storing R. By analyzing the convergence, we show that in "reasonable" datasets beta often can be very small (e.g., D(-1/2)) without hurting the estimation accuracy. Some numerical evaluations are conducted, on synthetic data, Web crawl data, and gene expression microarray data.	Stanford Univ, Dept Stat, Stanford, CA 94305 USA	Li, P (reprint author), Stanford Univ, Dept Stat, Stanford, CA 94305 USA.	pingli@cs.stanford.edu					Achlioptas D, 2001, P ACM S PRINC DAT SY, P274, DOI DOI 10.1109/TIT.2006.885507; Achlioptas D., 2001, NIPS 2001, P335; Aggarwal C., 2007, DATA STREAMS MODELS; Ailon N., 2006, STOC'06. Proceedings of the 38th Annual ACM Symposium on Theory of Computing; Alon N., 1996, Proceedings of the Twenty-Eighth Annual ACM Symposium on the Theory of Computing, DOI 10.1145/237814.237823; Achlioptas D, 2003, J COMPUT SYST SCI, V66, P671, DOI 10.1016/S0022-0000(03)00025-4; Bhattacharjee A, 2001, P NATL ACAD SCI USA, V98, P13790, DOI 10.1073/pnas.191502998; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Lee JR, 2004, GEOM FUNCT ANAL, V14, P745, DOI 10.1007/s00039-004-0473-8; MCCULLOCH JH, 1986, COMMUN STAT SIMULAT, V15, P1109, DOI 10.1080/03610918608812563; Chapelle O, 1999, IEEE T NEURAL NETWOR, V10, P1055, DOI 10.1109/72.788646; Dasgupta S, 2003, RANDOM STRUCT ALGOR, V22, P60, DOI 10.1002/rsa.10073; Indyk P, 2006, J ACM, V53, P307, DOI 10.1145/1147954.1147955; Efron B, 2004, ANN STAT, V32, P407; Brinkman B, 2005, J ACM, V52, P766, DOI 10.1145/1089023.1089026; Arriaga RI, 2006, MACH LEARN, V63, P161, DOI 10.1007/s10994-006-6265-7; Babcock B., 2002, P 21 ACM SIGACT SIGM, P1; Balcan MF, 2004, LECT NOTES ARTIF INT, V3244, P194; Bingham E., 2001, KDD-2001. Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining; Brinkman B., 2003, FOCS, P514; Broder AZ, 1997, COMPRESSION COMPLEXI, P21; Buhler J, 2002, J COMPUT BIOL, V9, P225, DOI 10.1089/10665270252935430; Charikar M. S., 2002, STOC, P380; Cormode G, 2003, IEEE T KNOWL DATA EN, V15, P529, DOI 10.1109/TKDE.2003.1198388; Cormode G., 2003, ESA, P148; Cormode G., 2002, VLDB, P335, DOI 10.1016/B978-155860869-6/50037-8; Donoho D. L., 1993, Applied and Computational Harmonic Analysis, V1, DOI 10.1006/acha.1993.1008; DUMAIS ST, 1991, BEHAV RES METH INSTR, V23, P229, DOI 10.3758/BF03203370; Durrett R., 1995, PROBABILITY THEORY E, Vsecond; Faloutsos M., 1999, SIGCOMM, P251; FAMA EF, 1968, J AM STAT ASSOC, V63, P817, DOI 10.2307/2283875; FAMA EF, 1971, J AM STAT ASSOC, V66, P331, DOI 10.2307/2283932; Feigenbaum J., 1999, FOCS, P501; FERECATU M, 2004, ACM SIGMM INT WORKSH, P23; Fern Xiaoli Zhang, 2003, ICML, P186; FONG JH, 2001, DISCRETE MATH THEOR, V4, P301; Fradkin D., 2003, KDD 03, P517; FRANKL P, 1988, J COMB THEORY B, V44, P355, DOI 10.1016/0095-8956(88)90043-3; GEOL N, 2005, SPIE, P426; Gnedenko B. V., 1954, LIMIT DISTRIBUTION S; Gradshteyn I. S., 1994, TABLE INTEGRALS SERI; GREIFF WR, 2003, SIGIR, P11; HENZINGER MR, 1999, COMPUTING DATA STREA; Indyk P., 2001, FOCS, P10; Indyk P, 1998, STOC, P604; Indyk Piotr, 2000, FOCS, P189; Johnson W. B., 1984, CONT MATH, V26, P189; JOHNSON WB, 1982, ACTA MATH-DJURSHOLM, V149, P71, DOI 10.1007/BF02392350; KUSKE R, 2000, J APPL MATH, V61, P1308; LAN M, 2005, WWW 05, P1032; LELAND WE, 1994, IEEE ACM T NETWORK, V2, P1, DOI 10.1109/90.282603; Leopold E, 2002, MACH LEARN, V46, P423, DOI 10.1023/A:1012491419635; Leung HCM, 2005, J COMPUT BIOL, V12, P686, DOI 10.1089/cmb.2005.12.686; LI P, ESTIMATORS TAIL BOUN; LI P, 2007, COMPUTATIONAL LINGUI; Li P, 2006, KDD, P287; LI P, VERY SPARSE STABLE R; LI P, 2005, HLT EMNLP, P708; LI P, 2006, COLT, P635; LI P, 2007, COLT; Li P, 2007, NIPS; LI P, J MACHINE L IN PRESS; LIN J, 2003, SDM; LIU B, 2001, KDD 2001, P144; Liu K, 2006, IEEE T KNOWL DATA EN, V18, P92; Manning C., 1999, FDN STAT NATURAL LAN; Mark E, 1997, IEEE ACM T NETWORK, V5, P835; Newman MEJ, 2005, CONTEMP PHYS, V46, P232; Papadimitriou C. H., 1998, Proceedings of the Seventeenth ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems. PODS 1998, DOI 10.1145/275487.275505; Ravichandran D, 2005, ACL, P622; Rennie J. D., 2003, ICML, P616; Rosa I., 1999, FOCS, P616; SALTON G, 1988, INFORM PROCESS MANAG, V24, P513, DOI 10.1016/0306-4573(88)90021-0; SAMORODNITSKY, 1994, STABLE NONGAUSSIAN R; VEMPALA S, 1998, FOCS 98, P389; Vempala S. S., 2004, RANDOM PROJECTION ME; YU CT, 1982, J ACM, V29, P152, DOI 10.1145/322290.322300; Zhu J., 2003, NIPS; ZOLOTAREV VM, 1986, ONE DIMENSIONAL STAB	80	0	0	0	4	ASSOC COMPUTING MACHINERY	NEW YORK	1515 BROADWAY, NEW YORK, NY 10036-9998 USA			978-1-59593-609-7				2007							440	449				10	Computer Science, Artificial Intelligence	Computer Science	BJK16	WOS:000266628300045		
B	Teo, CH; Le, Q; Smola, A; Vishwanathan, SVN		Berkhin, P; Caruana, R; Wu, X; Gaffney, S		Teo, Choon Hui; Le, Quoc; Smola, Alex; Vishwanathan, S. V. N.			A Scalable Modular Convex Solver for Regularized Risk Minimization	KDD-2007 PROCEEDINGS OF THE THIRTEENTH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING			English	Proceedings Paper	13th International Conference on Knowledge Discovery and Data Mining	AUG 12-15, 2007	San Jose, CA	ACM SIGKDD, ACM SIGMOD		Support Vectors; Gaussian Processes; Training Algorithms; Large-Scale; Bundle Methods; Parallel Optimization	REGRESSION	A wide variety of machine learning problems can be described as minimizing a regularized risk functional, with different algorithms using different notions of risk and different regularizers. Examples include linear Support Vector Machines (SVMs), Logistic Regression, Conditional Random Fields (CRFs), and Lasso amongst others. This paper describes the theory and implementation of a highly scalable and modular convex solver which solves all these estimation problems. It can be parallelized on a cluster of workstations, allows for data-locality, and can deal with regularizers such as l(1) and l(2) penalties. At present, our solver implements 20 different estimation problems, can be easily extended, scales to millions of observations, and is tip to 10 times faster than specialized solvers for many applications. The open source code is freely available as part of the ELEFANT toolbox.	[Teo, Choon Hui; Le, Quoc; Smola, Alex; Vishwanathan, S. V. N.] NICTA, Canberra, ACT 2601, Australia	Teo, CH (reprint author), NICTA, Northbourne Ave 218, Canberra, ACT 2601, Australia.	ChoonHui.Teo@nicta.com.au; Quoc.Le@nicta.com.au; Alex.Smola@nicta.com.au; SVN.ViShwanathan@nicta.com.au					Keerthi SS, 2005, J MACH LEARN RES, V6, P341; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Scholkopf B, 2001, NEURAL COMPUT, V13, P1443, DOI 10.1162/089976601750264965; Tsochantaridis I, 2005, J MACH LEARN RES, V6, P1453; Candes EJ, 2005, IEEE T INFORM THEORY, V51, P4203, DOI 10.1109/TIT.2005.858979; Bakir G. H., 2007, PREDICTING STRUCTURE; BALAY S, 2006, ANL9511 ARG NAT LAB; BARNDORFFNIELSE.OE, 1978, INFORM EXPONENTIAL F; Bennett KP, 1992, OPTIMIZATION METHODS, V1, P23, DOI 10.1080/10556789208805504; BENSON S, 2004, ANLMCSTM242 ARG NAT; BYRD RH, 1995, SIAM J SCI COMPUT, V16, P1190, DOI 10.1137/0916069; Cai L., 2004, P 13 ACM INT C INF K, P78, DOI DOI 10.1145/1031171.1031186; Chang C., 2001, LIBSVM LIB SUPPORT V; CHAPELLE O, 2006, TR147 M PLANCK I BIO; CHU C, 2007, NIPS 19; Clifton C., 2002, ACM SIGKDD EXPLORATI, V4; Collins M., 2000, COMPUTATIONAL LEARNI, P158; Cowell R. G., 1999, PROBABILISTIC NETWOR; Crammer K, 2005, NEURAL COMPUT, V17, P145, DOI 10.1162/0899766052530848; Cressie N., 1993, STAT SPATIAL DATA; Fahrmeir L, 1994, MULTIVARIATE STAT MO; FINE S, 2001, JMLR; Fine S., 2000, EFFICIENT SVM TRAINI; GENTILE C, 1999, NIPS 11, P225; Herbrich R, 2000, ADV NEUR IN, P115; Hiriart-Urruty J., 1993, CONVEX ANAL MINIMIZA, VI, P305; Hiriart-Urruty J., 1993, CONVEX ANAL MINIMISA, VII, P306; Joachims T, 1999, ADVANCES IN KERNEL METHODS, P169; Joachims T., 2006, KDD; Joachims T., 2005, INT C MACH LEARN ICM, P377; Koenker R, 2005, QUANTILE REGRESSION; Lafferty J.D., 2001, IEEE INT C MACH LEAR, V18, P282; LE Q, 2007, JMLR UNPUB; MANGASAR.OL, 1965, OPER RES, V13, P444, DOI 10.1287/opre.13.3.444; Muller Klaus-Robert, 1997, ICANN 97 P 7 INT C A, P999; SCHOLKOPF B, 1999, TR87; Sha F., 2003, P HLT NAACL, p213 ; SHALEVSHWARTZ S, 2006, COLT; Sindhwani V., 2006, SIGIR 06, P477; TAKEUCHI I, 2006, JMLR; TASKAR B, 2004, NIPS, P25; Vapnik V., 1997, NIPS, P281; VISHWANATHAN SVN, 2003, NIPS, P569	43	18	18	0	0	ASSOC COMPUTING MACHINERY	NEW YORK	1515 BROADWAY, NEW YORK, NY 10036-9998 USA			978-1-59593-609-7				2007							727	736				10	Computer Science, Artificial Intelligence	Computer Science	BJK16	WOS:000266628300074		
S	Bunea, F; Tsybakov, AB; Wegkamp, MH		Bshouty, NH; Gentile, C		Bunea, Florentina; Tsybakov, Alexandre B.; Wegkamp, Marten H.			Sparse density estimation with l(1) penalties	Learning Theory, Proceedings	LECTURE NOTES IN COMPUTER SCIENCE		English	Proceedings Paper	20th Annual Conference on Learning Theory	JUN 13-15, 2007	San Diego, CA	Google, Machine Learning, IBM			SELECTION; KERNEL; LASSO	This paper studies oracle properties of l(1)-penalized estimators of a probability density. We show that the penalized least squares estimator satisfies sparsity oracle inequalities, i.e., bounds in terms of the number of non-zero components of the oracle vector. The results are valid even when the dimension of the model is (much) larger than the sample size. They are applied to estimation in sparse high-dimensional mixture models, to nonparametric adaptive density estimation and to the problem of aggregation of density estimators.	Florida State Univ, Tallahassee, FL 32306 USA	Bunea, F (reprint author), Florida State Univ, Tallahassee, FL 32306 USA.						Donoho DL, 2001, IEEE T INFORM THEORY, V47, P2845, DOI 10.1109/18.959265; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Chen SSB, 2001, SIAM REV, V43, P129, DOI 10.1137/S003614450037906X; Abramovich F, 2006, ANN STAT, V34, P584, DOI 10.1214/009053606000000074; DONOHO DL, 1995, IEEE T INFORM THEORY, V41, P613, DOI 10.1109/18.382009; Meinshausen N, 2006, ANN STAT, V34, P1436, DOI 10.1214/009053606000000281; Barron A, 1999, PROBAB THEORY REL, V113, P301, DOI 10.1007/s004400050210; Birge L., 1997, RES PAPERS PROBABILI, P55; BUNEA F, 2006, UNPUB SPARSITY ORACL; Bunea F, 2006, LECT NOTES ARTIF INT, V4005, P379, DOI 10.1007/11776420_29; BUNEA F, 2005, IN PRESS ANN STAT; DEVROYE L., 2000, COMBINATORIAL METHOD; DONOHO D, 2004, UNPUB STABLE RECOVER; Golubev G. K., 1992, Problems of Information Transmission, V28; Golubev G. K., 2002, Problems of Information Transmission, V38, DOI 10.1023/A:1020098307781; Greenshtein E, 2004, BERNOULLI, V10, P971, DOI 10.3150/bj/1106314846; Hall P, 1998, ANN STAT, V26, P922; Hardle W., 1998, LECT NOTES STAT, V129; Kerkyacharian G., 1996, BERNOULLI, V2, P229, DOI 10.3150/bj/1178291720; KOLTCHINSKII V, 2006, UNPUB SPARSITY PENAL; KOLTCHINSKII V, 2005, MATH FORSCHUNGSINSTI, V2, P2663; Loubes J.-M., 2002, STAT NEERL, V56, P453; NEMIROVSKI A, 2000, LECT NOTES MATH, V28; RIGOLLET P, 2004, LINEAR CONVEX AGGREG; RIGOLLET P, 2006, THESIS U PARIS 6; RUDEMO M, 1982, SCAND J STAT, V9, P65; TSYBAKOV AB, 2003, LNCS LNAI, V2777; VANDEGEER SA, 2006, SEM STAT ETH ZUR; Vapnik V., 1998, STAT LEARNING THEORY; Wegkamp MH, 1999, CAN J STAT, V27, P409, DOI 10.2307/3315649	30	11	11	0	1	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		978-3-540-72925-9	LECT NOTES COMPUT SC			2007	4539						530	543		10.1007/978-3-540-72927-3_38		14	Computer Science, Artificial Intelligence	Computer Science	BGI46	WOS:000247339600038		
S	Rosset, S; Swirszcz, G; Srebro, N; Zhu, J		Bshouty, NH; Gentile, C		Rosset, Saharon; Swirszcz, Grzegorz; Srebro, Nathan; Zhu, Ji			l(1) regularization in infinite dimensional feature spaces	Learning Theory, Proceedings	LECTURE NOTES IN COMPUTER SCIENCE		English	Proceedings Paper	20th Annual Conference on Learning Theory	JUN 13-15, 2007	San Diego, CA	Google, Machine Learning, IBM			REGRESSION; SELECTION	In this paper we discuss the problem of fitting l(1), regularized prediction models in infinite (possibly non-countable) dimensional feature spaces. Our main contributions are: a. Deriving a generalization of l(1) regularization based on measures which can be applied in non-countable feature spaces; b. Proving that the sparsity property of l(1) regularization is maintained in infinite dimensions; c. Devising a path-following algorithm that can generate the set of regularized solutions in "nice" feature spaces; and d. Presenting an example of penalized spline models where this path following algorithm is computationally feasible, and gives encouraging empirical results.	IBM TJ Watson Res Ctr, Yorktown Hts, NY 10549 USA	Rosset, S (reprint author), IBM TJ Watson Res Ctr, Yorktown Hts, NY 10549 USA.						Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Efron B, 2004, ANN STAT, V32, P407; BACH F, MULTIPLE KERNEL LEAR; Blake C.L., 1998, UCI REPOSITORY MACHI; Hastie TJ, 1990, GEN ADDITIVE MODELS; KOLTCHINSKI V, 2002, ANN STAT, V30; Mammen E, 1997, ANN STAT, V25, P387; Pace RK, 1997, STAT PROBABIL LETT, V33, P291; Rosset S, 2004, J MACH LEARN RES, V5, P941; ROSSET S, 2006, IN PRESS ANN STAT; Rudin W., 1991, FUNCTIONAL ANAL; SMOLA A, 2002, LEARNING KERNELS; Zhang HH, 2006, STAT SINICA, V16, P1021; Zhang T, 2002, J MACH LEARN RES, V2, P527, DOI 10.1162/153244302760200713; Zhu J., 2004, NEURAL INFORM PROCES, V16	16	4	4	0	4	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		978-3-540-72925-9	LECT NOTES COMPUT SC			2007	4539						544	558		10.1007/978-3-540-72927-3_39		15	Computer Science, Artificial Intelligence	Computer Science	BGI46	WOS:000247339600039		
S	Cheng, HB; Chen, HF; Jiang, GF; Yoshihira, K		Perner, P		Cheng, Haibin; Chen, Haifeng; Jiang, Guofei; Yoshihira, Kenji			Nonlinear feature selection by relevance feature vector machine	Machine Learning and Data Mining in Pattern Recognition, Proceedings	LECTURE NOTES IN ARTIFICIAL INTELLIGENCE		English	Proceedings Paper	5th International Conference on Machine Learning and Data Mining in Pattern Recognition	JUL 18-20, 2007	Leipzig, GERMANY					Support vector machine (SVM) has received much attention in feature selection recently because of its ability to incorporate kernels to discover nonlinear dependencies between features. However it is known that the number of support vectors required in SVM typically grows linearly with the size of the training data set. Such a limitation of SVM becomes more critical when we need to select a small subset of relevant features from a very large number of candidates. To solve this issue, this paper proposes a novel algorithm, called the 'relevance feature vector machine'(RFVM), for nonlinear feature selection. The RFVM algorithm utilizes a highly sparse learning algorithm, the relevance vector machine (RVM), and incorporates kernels to extract important features with both linear and nonlinear relationships. As a result, our proposed approach can reduce many false alarms, e.g. including irrelevant features, while still maintain good selection performance. We compare the performances between RFVM and other state of the art nonlinear feature selection algorithms in our experiments. The results confirm our conclusions.	Michigan State Univ, CSE Dept, E Lansing, MI 48824 USA	Cheng, HB (reprint author), Michigan State Univ, CSE Dept, E Lansing, MI 48824 USA.						Aizerman M.A., 1964, AUTOMAT REM CONTR, V25, P821; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Peng HC, 2005, IEEE T PATTERN ANAL, V27, P1226; Tipping ME, 2001, J MACH LEARN RES, V1, P211, DOI 10.1162/15324430152748236; Bellman R.E., 1961, ADAPTIVE CONTROL PRO; Bishop C. M., 1995, NEURAL NETWORKS PATT; BO TH, 2000, GENOME BIOL, V3, P11, DOI UNSP 0017.1-0017.11; Burges C.J.C., 1996, P 13 INT C MACH LEAR, P71; Faul AC, 2002, ADV NEUR IN, V14, P383; Figueiredo M. A. T., 2001, P IEEE INT C COMP VI, V1, P35; Figueiredo MAT, 2003, IEEE T PATTERN ANAL, V25, P1150, DOI 10.1109/TPAMI.2003.1227989; Guiasu S., 1977, INFORM THEORY APPL; Guyon I., 2003, Journal of Machine Learning Research, V3, DOI 10.1162/153244303322753616; HOCHREITER S, 2005, FEATURE EXTRACTION F; LI F, 2005, ADV NEURAL INFORM PR, V18; Reeves SJ, 1999, IEEE T SIGNAL PROCES, V47, P123, DOI 10.1109/78.738245; ROTH V, 2004, NATURE STAT LEARNING, P15; Smola A. J., 1998, NCTR98030 ROYAL HOLL; Vapnik V.N., 1995, NATURE STAT LEARNING	19	2	2	0	0	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		978-3-540-73498-7	LECT NOTES ARTIF INT			2007	4571						144	159				16	Computer Science, Artificial Intelligence	Computer Science	BGM77	WOS:000248523200011		
S	Schmidt, M; Fung, G; Rosales, R		Kok, JN; Koronacki, J; DeMantaras, RL; Matwin, S; Mladenic, D; Skowron, A		Schmidt, Mark; Fung, Glenn; Rosales, Romer			Fast optimization methods for L1 regularization: A comparative study and two new approaches	Machine Learning: ECML 2007, Proceedings	LECTURE NOTES IN ARTIFICIAL INTELLIGENCE		English	Proceedings Paper	18th European Conference on Machine Learning (ECML 2007)/11th European Conference on Principles and Practice of Knowledge Discovery in Databases (PKDD 2007)	SEP 17-21, 2007	Warsaw, POLAND	Warsaw Univ, Fac Math, Informat & Mech, Polish Acad Sci, Inst Comp Sci, European Off Aerosp Res & Dev, Air Force Off Sci Res, USAF Res Lab	Warsaw Univ		CONSTRAINED OPTIMIZATION; REGRESSION; SELECTION; LASSO	L1 regularization is effective for feature selection, but the resulting optimization is challenging due to the non-differentiability of the 1-norm. In this paper we compare state-of-the-art optimization techniques to solve this problem across several loss functions. Furthermore, we propose two new techniques. The first is based on a smooth (differentiable) convex approximation for the L1 regularizer that does not depend on any assumptions about the loss function used. The other technique is a new strategy that addresses the non-differentiability of the L1-regularizer by casting the problem as a constrained optimization problem that is then solved using a specialized gradient projection method. Extensive comparisons show that our newly proposed approaches consistently rank among the best in terms of convergence speed and efficiency by measuring the number of function evaluations required.	Univ British Columbia, Dept Comp Sci, Vancouver, BC V6T 1W5, Canada	Schmidt, M (reprint author), Univ British Columbia, Dept Comp Sci, Vancouver, BC V6T 1W5, Canada.						GAFNI EM, 1984, SIAM J CONTROL OPTIM, V22, P936, DOI 10.1137/0322061; Fu WJJ, 1998, J COMPUT GRAPH STAT, V7, P397, DOI 10.2307/1390712; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Lee YJ, 2001, COMPUT OPTIM APPL, V20, P5, DOI 10.1023/A:1011215321374; Efron B, 2004, ANN STAT, V32, P407; Chen C. H., 1996, COMPUTATIONAL OPTIMI, V5, P97; Chen S. S., 1999, SIAM J SCI COMPUT, V20, P33; Figueiredo MAT, 2003, IEEE T PATTERN ANAL, V25, P1150, DOI 10.1109/TPAMI.2003.1227989; Freund R.M., 1996, OPTIMA, V51, P1; Guyon I., 2003, Journal of Machine Learning Research, V3, DOI 10.1162/153244303322753616; Kumar S., 2003, ICCV; Lee S., 2006, AAAI; Ng A. Y., 2004, ICML ACM PRESS NEW Y, P78, DOI 10.1145/1015330.1015435; NOCEDAL J., 1999, NUMERICAL OPTIMIZATI; PALOMARES UMG, 1976, MATH PROGRAM, V11, P1, DOI 10.1007/BF01580366; Perkins S., 2003, Journal of Machine Learning Research, V3, DOI 10.1162/153244303322753698; Shevade SK, 2003, BIOINFORMATICS, V19, P2246, DOI 10.1093/bioinformatics/btg308; Weston J., 2003, Journal of Machine Learning Research, V3, DOI 10.1162/153244303322753751; ZHAO P, 2007, J MACH LEARN RES, V7, P2541	19	43	43	0	3	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		978-3-540-74957-8	LECT NOTES ARTIF INT			2007	4701						286	297				12	Computer Science, Artificial Intelligence; Computer Science, Theory & Methods; Mathematics, Applied	Computer Science; Mathematics	BGQ45	WOS:000249742300024		
S	Seeger, M; Gerwinn, S; Bethge, M		Kok, JN; Koronacki, J; DeMantaras, RL; Matwin, S; Mladenic, D; Skowron, A		Seeger, Matthias; Gerwinn, Sebastian; Bethge, Matthias			Bayesian inference for sparse generalized linear models	Machine Learning: ECML 2007, Proceedings	LECTURE NOTES IN ARTIFICIAL INTELLIGENCE		English	Proceedings Paper	18th European Conference on Machine Learning (ECML 2007)/11th European Conference on Principles and Practice of Knowledge Discovery in Databases (PKDD 2007)	SEP 17-21, 2007	Warsaw, POLAND	Warsaw Univ, Fac Math, Informat & Mech, Polish Acad Sci, Inst Comp Sci, European Off Aerosp Res & Dev, Air Force Off Sci Res, USAF Res Lab	Warsaw Univ			We present a framework for efficient, accurate approximate Bayesian inference in generalized linear models (QLMs), based on the expectation propagation (EP) technique. The parameters can be endowed with a factorizing prior distribution, encoding properties such as sparsity or non-negativity. The central role of posterior log-concavity in Bayesian GLMs is emphasized and related to stability issues in EP. In particular, we use our technique to infer the parameters of a point process model for neuronal spiking data from multiple electrodes, demonstrating significantly superior predictive performance when a sparsity assumption is enforced via a Laplace prior distribution.	Max Planck Inst Biol Cybernet, Tubingen, Germany	Seeger, M (reprint author), Max Planck Inst Biol Cybernet, Spemannstr 38, Tubingen, Germany.		Bethge, Matthias/B-1554-2008				Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Harris KD, 2003, NATURE, V424, P552, DOI 10.1038/nature01834; Tipping ME, 2001, J MACH LEARN RES, V1, P211, DOI 10.1162/15324430152748236; BERRY M, 1997, STRUCTURE PRECISION; Carandini M, 2005, J NEUROSCI, V25, P10577, DOI 10.1523/JNEUROSCI.3726-05.2005; GILKS WR, 1992, APPL STAT-J ROY ST C, V41, P337, DOI 10.2307/2347565; McCullach P., 1983, MONOGRAPHS STAT APPL, V1st; MINKA T, 2001, UNCERTAINY AI, V17; Minka T., 2005, MSRTR2005173; NODELMAN U, 2005, UNCERTAINY AI, V21, P431; Opper M, 2000, NEURAL COMPUT, V12, P2655, DOI 10.1162/089976600300014881; PANINSKI L, 2004, COMPUTATION NEURAL S, V15, P243; Park T., 2005, BAYESIAN LASSO; QI Y, 2004, P ICML, V21; RAJARAM S, 2005, AI STAT, V10; Rieke F., 1999, SPIKES EXPLORING NEU; SEEGER M, 2007, AI STAT, V11; Seeger M. W., 2005, EXPECTATION PROPAGAT; Simoncelli Eero P, 2004, COGNITIVE NEUROSCIEN; Snyder D, 1991, SPRINGER TEXTS ELECT; Wilkinson D.J., 2006, STOCHASTIC MODELLING; WIPF D, 2004, ADV NIPS, V16; Zeck GM, 2005, EUR J NEUROSCI, V22, P2016, DOI 10.1111/j.1460-9568.2005.04390.x	23	6	6	0	0	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		978-3-540-74957-8	LECT NOTES ARTIF INT			2007	4701						298	309				12	Computer Science, Artificial Intelligence; Computer Science, Theory & Methods; Mathematics, Applied	Computer Science; Mathematics	BGQ45	WOS:000249742300025		
S	Gao, JB; Kwan, PWH; Guo, Y		Zhang, TX; Nardell, C; Smith, D; Lu, HQ		Gao, Junbin; Kwan, Paul W. H.; Guo, Yi			Robust L1 PCA and application in image denoising	MIPPR 2007: AUTOMATIC TARGET RECOGNITION AND IMAGE ANALYSIS; AND MULTISPECTRAL IMAGE ACQUISITION, PTS 1 AND 2	Proceedings of SPIE		English	Proceedings Paper	5th International Symposium on Multispectral Image Processing and Pattern Recognition	NOV 15-17, 2007	Wuhan, PEOPLES R CHINA	SPIE, State Key Lab Multi Spectral Informat Proc Technol, Chinese Educ Minist Key Lab Image Proc & Intelligence Control, Huazhong Univ Sci & Technol	Wuhan Univ	robust L1 PCA; Bayesian learning; denoising	PRINCIPAL COMPONENT ANALYSIS	The so-called robust L1 PCA was introduced in our recent work [1] based on the L1 noise assumption. Due to the heavy tail characteristics of the L1 distribution, the proposed model has been proved much more robust against data outliers. In this paper, we further demonstrate how the learned robust L1 PCA model can be used to denoise image data.	[Gao, Junbin] Charles Sturt Univ, Sch Comp Sci, Bathurst, NSW 2795, Australia; [Kwan, Paul W. H.; Guo, Yi] Univ New England, Sch Sci & Technol, Armidale, NSW 2351, Australia	Gao, JB (reprint author), Charles Sturt Univ, Sch Comp Sci, Bathurst, NSW 2795, Australia.	jbgao@csu.edu.au					Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Baccini A, 1996, ST CLASS DAT ANAL, P359; Aminghafari M, 2006, COMPUT STAT DATA AN, V50, P2381, DOI 10.1016/j.csda.2004.12.010; De la Torre F, 2003, INT J COMPUT VISION, V54, P117, DOI 10.1023/A:1023709501986; ARCHAMBEAU C, 2005, THESIS U CATHOLIQUE; Archambeau C., 2006, P 23 INT C MACH LEAR; Braun M. L., 2007, ADV NEURAL INF PROCE, V19, P185; DELATORRE F, 2001, INT C COMPUTER VISIO, V52, P362; Ding C., 2006, P 23 INT C MACH LEAR; Duda R. O., 2001, PATTERN CLASSIFICATI; GOA J, 2007, IN PRESS NEURAL COMP, V20, pR30; Gruber P, 2006, NEUROCOMPUTING, V69, P1485, DOI 10.1016/j.neucom.2005.12.025; Gruber P, 2004, LECT NOTES COMPUT SC, V3195, P993; Ke Q., 2005, P CVPR, P739, DOI 10.1109/CVPR.2005.309; KHAN Z, 2004, GITGVU0411; Mika S., 1999, P ADV NEUR INF PROC, P536; Ng A. Y., 2004, P INT C MACH LEARN; Peel D, 2000, STAT COMPUT, V10, P339, DOI 10.1023/A:1008981510081; RIDDER DD, 2003, BMVC 2003, P319; RUYMGAART FH, 1981, J MULTIVARIATE ANAL, V11, P485, DOI 10.1016/0047-259X(81)90091-9; Scholkopf, 2002, LEARNING KERNELS; Tipping M, 1999, J ROYAL STAT SOC B, V61, P611; Tipping ME, 2005, NEUROCOMPUTING, V69, P123, DOI 10.1016/j.neucom.2005.02.016	23	0	0	1	1	SPIE-INT SOC OPTICAL ENGINEERING	BELLINGHAM	1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA	0277-786X		978-0-8194-6950-2	PROC SPIE			2007	6786		1-2						67860T	10.1117/12.774719		8	Remote Sensing; Optics; Imaging Science & Photographic Technology	Remote Sensing; Optics; Imaging Science & Photographic Technology	BHK04	WOS:000253700100028		
B	Mouridsen, K; Ostergaard, L		Filippi, M; Rovaris, M; Comi, G		Mouridsen, K.; Ostergaard, L.			Predictive models in multimodal imaging	Neurodegeneration in Multiple Sclerosis	TOPICS IN NEUROSCIENCE		English	Proceedings Paper	International Workshop on Neurodegeneration in Multiple Sclerosis	JUN   10, 2005	Milan, ITALY				MULTIPLE-SCLEROSIS; MAGNETIZATION-TRANSFER; RELATIVE IMPORTANCE; EARLIEST STAGE; WHITE-MATTER; IMAGES; ABNORMALITIES; EVOLUTION		Aarhus Univ Hosp, Dept Neuroradiol, CFIN, DK-8000 Aarhus, Denmark	Mouridsen, K (reprint author), Aarhus Univ Hosp, Dept Neuroradiol, CFIN, DK-8000 Aarhus, Denmark.		Bonefeld, Birgit/B-7936-2010; Ostergaard, Leif/A-9281-2008				ALTMAN DG, 1983, STATISTICIAN, V32, P307, DOI 10.2307/2987937; Agosta F, 2006, BRAIN, V129, P2620, DOI 10.1093/brain/aw1208; CHEVAN A, 1991, AM STAT, V45, P90, DOI 10.2307/2684366; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Worsley KJ, 1996, HUM BRAIN MAPP, V4, P58, DOI 10.1002/(SICI)1097-0193(1996)4:1&lt;58::AID-HBM4&gt;3.0.CO;2-O; EFRON B, 1983, J AM STAT ASSOC, V78, P316, DOI 10.2307/2288636; Fischl B, 1999, HUM BRAIN MAPP, V8, P272, DOI 10.1002/(SICI)1097-0193(1999)8:4<272::AID-HBM10>3.0.CO;2-4; KRUSKAL W, 1989, AM STAT, V43, P2, DOI 10.2307/2685157; NAGELKERKE NJD, 1991, BIOMETRIKA, V78, P691, DOI 10.1093/biomet/78.3.691; CRAGG JG, 1970, CAN J ECONOMICS, V3, P386, DOI 10.2307/133656; LAWLESS JF, 1978, BIOMETRICS, V34, P318, DOI 10.2307/2530022; COLLINS DL, 1994, J COMPUT ASSIST TOMO, V18, P192; Audoin B, 2004, J MAGN RESON IMAGING, V20, P765, DOI 10.1002/jmri.20178; Bakshi Rohit, 2005, NeuroRx, V2, P277, DOI 10.1007/BF03206672; Bowman A.W., 1997, APPL SMOOTHING TECHN; Box G. E. P., 1994, TIME SERIES ANAL; LECESSIE S, 1992, APPL STAT-J ROY ST C, V41, P191; EFRON B, 1993, INTRO BOOTSTRAP, P247; FAZEKAS F, 2002, MULT SCLER, V8, P879; Glantz S, 1990, PRIMER APPL REGRESSI; Gross J., 2003, LINEAR REGRESSION; Harrell FE, 2001, REGRESSION MODELING; HAUCK WW, 1977, J AM STAT ASSOC, V72, P851, DOI 10.2307/2286473; JEROME F, 1991, ANN STAT, V19, P1; Jones R. H., 1993, LONGITUDINAL DATA SE; KRUSKAL W, 1987, AM STAT, V41, P6, DOI 10.2307/2684310; Laule C, 2003, J NEUROL, V250, P924, DOI 10.1007/s00415-003-1115-z; Maddala G. S., 1983, LIMITED DEPENDENT QU; Mardia KV, 1979, MULTIVARIATE ANAL; BLAND JM, 1986, LANCET, V1, P307; Pike GB, 2000, RADIOLOGY, V215, P824; Pinheiro J. C., 2000, MIXED EFFECTS MODELS; Ranjeva JP, 2005, AM J NEURORADIOL, V26, P119; Rocca MA, 2000, NEUROLOGY, V55, P882; Santos AC, 2002, J NEUROL, V249, P662, DOI 10.1007/s00415-002-0686-4; Soofi ES, 2000, DECISION SCI, V31, P595, DOI 10.1111/j.1540-5915.2000.tb00936.x; Sun GW, 1996, J CLIN EPIDEMIOL, V49, P907, DOI 10.1016/0895-4356(96)00025-X; vanBuchem MA, 1996, MAGNET RESON MED, V36, P632, DOI 10.1002/mrm.1910360420; WORSLEY KJ, 1994, ADV APPL PROBAB, V26, P13, DOI 10.2307/1427576; Wu O, 2006, BRAIN, V129, P2384; Wu O, 2001, STROKE, V32, P933	41	0	0	0	5	SPRINGER-VERLAG ITALIA	MILAN	MILAN, ITALY			978-88-470-0390-3	TOPICS NEUROSCI			2007							127	149		10.1007/978-88-470-0391-0_12		23	Neurosciences; Neuroimaging	Neurosciences & Neurology	BGV03	WOS:000250747200012		
S	Liu, YF; Zhang, HH; Park, CW; Ahn, JY		Verducci, JS; Shen, X; Lafferty, J		Liu, Yufeng; Zhang, Hao Helen; Park, Cheolwoo; Ahn, Jeongyoun			The L-q support vector machine	Prediction and Discovery	CONTEMPORARY MATHEMATICS SERIES		English	Proceedings Paper	AMS/IMS/SIAM Joint Summer Research Conference on Machine and Statistical Learning - Prediction and Discovery	JUN 25-29, 2006	Snowbird, UT	AMS, IMS, SIAM		adaptive penalty; classification; shrinkage; support vector machine; variable selection	VARIABLE SELECTION; CLASSIFICATION; REGRESSION; LASSO; REGULARIZATION; SHRINKAGE	The standard Support Vector Machine (SVM) minimizes the hinge loss function subject to the L-2 penalty or the roughness penalty. Recently, the L, SVM was suggested for variable selection by producing sparse solutions [BM, ZHRT]. These learning methods are non-adaptive since their penalty forms are pre-determined before looking at data, and they often perform well only in a certain type of situation. For instance, the L2 SVM generally works well except when there are too many noise inputs, while the L-1 SVM is more preferred in the presence of many noise variables. In this article we propose and explore an adaptive learning procedure called the L-q SVM, where the best q > 0 is automatically chosen by data. Both two- and multi-class classification problems are considered. We show that, the new adaptive approach combines the benefit of a class of non-adaptive procedures and gives the best performance of this class across a variety of situations. Moreover, we observe that the proposed Lq penalty is more robust to noise variables than the L-1 and L-2 penalties. An iterative algorithm is suggested to solve the L-q SVM efficiently. Simulations and real data applications support the effectiveness of the proposed procedure.	Univ N Carolina, Dept Stat & Operat Res, Chapel Hill, NC 27599 USA	Liu, YF (reprint author), Univ N Carolina, Dept Stat & Operat Res, CB 3260, Chapel Hill, NC 27599 USA.						Fu WJJ, 1998, J COMPUT GRAPH STAT, V7, P397, DOI 10.2307/1390712; Knight K, 2000, ANN STAT, V28, P1356; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Antoniadis A, 2001, J AM STAT ASSOC, V96, P939, DOI 10.1198/016214501753208942; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; FRANK IE, 1993, TECHNOMETRICS, V35, P109, DOI 10.2307/1269656; Zou H, 2005, J ROY STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x; Lin Y, 2002, DATA MIN KNOWL DISC, V6, P259, DOI 10.1023/A:1015469627679; Lin Y, 2002, MACH LEARN, V46, P191, DOI 10.1023/A:1012406528296; Efron B, 2004, ANN STAT, V32, P407; DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425; BOSER B, 1992, 5 ANN C COMP LEARN T, P142; Bradley Paul S., 1998, ICML 98; Chen S. S., 1999, SIAM J SCI COMPUT, V20, P33; Crammer K., 2001, J MACHINE LEARNING R, V2, P265; Ikeda K, 2005, NEURAL COMPUT, V17, P2508, DOI 10.1162/0899766054796897; Lee YK, 2004, J AM STAT ASSOC, V99, P67, DOI 10.1198/016214504000000098; Vapnik V., 1998, STAT LEARNING THEORY; Wahba G., 1998, ADV KERNEL METHODS S, P125; Wang LF, 2006, STAT SINICA, V16, P617; Weston J., 1999, P 7 EUR S ART NEUR N; Zhu J., 2003, NEURAL INFORM PROCES, V16	22	1	1	0	2	AMER MATHEMATICAL SOC	PROVIDENCE	P.O. BOX 6248, PROVIDENCE, RI 02940 USA	0271-4132		978-0-8218-4195-2	CONTEMP MATH			2007	443						35	48				14	Mathematics, Applied; Mathematics	Mathematics	BGW56	WOS:000250954400004		
S	Owen, AB		Verducci, JS; Shen, X; Lafferty, J		Owen, Art B.			A robust hybrid of lasso and ridge regression	Prediction and Discovery	CONTEMPORARY MATHEMATICS SERIES		English	Proceedings Paper	AMS/IMS/SIAM Joint Summer Research Conference on Machine and Statistical Learning - Prediction and Discovery	JUN 25-29, 2006	Snowbird, UT	AMS, IMS, SIAM			SELECTION	Ridge regression and the lasso are regularized versions of least squares regression using L-2 and L-1 penalties respectively, on the coefficient vector. To make these regressions more robust we may replace least squares with Huber's criterion which is a hybrid of squared error (for relatively small errors) and absolute error (for relatively large ones). A reversed version of Huber's criterion can be used as a hybrid penalty function. Relatively small coefficients contribute their L-1 norm to this penalty while larger ones cause it to grow quadratically. This hybrid sets some coefficients to 0 (as lasso does) while shrinking the larger coefficients the way ridge regression does. Both the Huber and reversed Huber penalty functions employ a scale parameter. We provide an objective function that is jointly convex in the regression coefficient vector and these two scale parameters.	Stanford Univ, Dept Stat, Stanford, CA 94305 USA	Owen, AB (reprint author), Stanford Univ, Dept Stat, Sequoia Hall, Stanford, CA 94305 USA.						Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; HOERL AE, 1970, TECHNOMETRICS, V12, P55; Zou H, 2005, J ROY STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x; Efron B, 2004, ANN STAT, V32, P407; Donoho DL, 2003, P NATL ACAD SCI USA, V100, P2197, DOI 10.1073/pnas.0437847100; GRANT M, 2006, CVX USERS GUIDE; Huber P. J., 1981, ROBUST STAT; Vandenberghe L., 2004, CONVEX OPTIMIZATION; Zhao P., 2005, GROUPED HIERARCHICAL	9	8	8	0	0	AMER MATHEMATICAL SOC	PROVIDENCE	P.O. BOX 6248, PROVIDENCE, RI 02940 USA	0271-4132		978-0-8218-4195-2	CONTEMP MATH			2007	443						59	71				13	Mathematics, Applied; Mathematics	Mathematics	BGW56	WOS:000250954400006		
S	Kim, YD; Kim, YW; Kim, JS		Verducci, JS; Shen, X; Lafferty, J		Kim, Yongdai; Kim, Yuwon; Kim, Jinseog			A gradient descent algorithm for LASSO	Prediction and Discovery	CONTEMPORARY MATHEMATICS SERIES		English	Proceedings Paper	AMS/IMS/SIAM Joint Summer Research Conference on Machine and Statistical Learning - Prediction and Discovery	JUN 25-29, 2006	Snowbird, UT	AMS, IMS, SIAM		gradient; LASSO; optimization	VARIABLE SELECTION; REGRESSION; SHRINKAGE	In this paper, we propose a new optimization algorithm for LASSO-a regression method with the L-1 constraint, based on the gradient descent method. An important advantage of the proposed algorithm over the existing methods is that the former is stabler in particular when the dimension of the input is large. Here, "stabler" means that the proposed algorithm always yields an optimal solution while the existing methods may fail to do so. Simulation results for comparing the proposed algorithm with the QP based algorithm are also given.	Seoul Natl Univ, Dept Stat, Seoul 151742, South Korea	Kim, YD (reprint author), Seoul Natl Univ, Dept Stat, Seoul 151742, South Korea.						Yuan M, 2006, J ROY STAT SOC B, V68, P49, DOI 10.1111/j.1467-9868.2005.00532.x; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; Osborne MR, 2000, IMA J NUMER ANAL, V20, P389, DOI 10.1093/imanum/20.3.389; Zou H, 2005, J ROY STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x; Tibshirani R, 2005, J ROY STAT SOC B, V67, P91, DOI 10.1111/j.1467-9868.2005.00490.x; Efron B, 2004, ANN STAT, V32, P407; BAKIN S, 1999, THESIS AUSTR NAT U A; Chen S. S., 1999, SIAM J SCI COMPUT, V20, P33; Grandvalet Y, 1999, ADV NEUR IN, V11, P445; GUNN SR, 2002, MACH LEARN, V48, P115; Kim Y, 2006, STAT SINICA, V16, P375; Kim Y., 2004, P 21 INT C MACH LEAR, P473; KRISHNAPURAM B, 2004, LEARNING SPARSE CLAS; LOKHORST J, 1999, LASSO2 S PLUS LIB SO; Perkins S., 2003, Journal of Machine Learning Research, V3, DOI 10.1162/153244303322753698; Roth V, 2004, IEEE T NEURAL NETWOR, V15, P16, DOI 10.1109/TNN.2003.809398; ZHANG HH, 2004, J AM STAT ASSOC, V99, P467	18	0	0	0	0	AMER MATHEMATICAL SOC	PROVIDENCE	P.O. BOX 6248, PROVIDENCE, RI 02940 USA	0271-4132		978-0-8218-4195-2	CONTEMP MATH			2007	443						73	82				10	Mathematics, Applied; Mathematics	Mathematics	BGW56	WOS:000250954400007		
S	Wang, SJ; Zhu, J		Verducci, JS; Shen, X; Lafferty, J		Wang, Sijian; Zhu, Ji			Variable selection for model-based high-dimensional clustering	Prediction and Discovery	CONTEMPORARY MATHEMATICS SERIES		English	Proceedings Paper	AMS/IMS/SIAM Joint Summer Research Conference on Machine and Statistical Learning - Prediction and Discovery	JUN 25-29, 2006	Snowbird, UT	AMS, IMS, SIAM		EM algorithm; high dimension low sample size; Microarray; model-based clustering; regularization; variable selection	ORACLE PROPERTIES; CLASSIFICATION; LIKELIHOOD; PREDICTION; LASSO	Variable selection in high-dimensional clustering analysis is an important yet challenging problem. In this paper, we propose two methods that simultaneously separate data points into similar clusters and select informative variables that contribute to the clustering. Our methods are in the framework of penalized model-based clustering. Unlike the classical L-1-norm, penalization, the penalty terms that we propose make use of the fact that parameters belonging to one variable should be treated as a natural group. Numerical results indicate that the two new methods tend to remove non-informative variables more effectively and provide better clustering results than the L(1-)norm approach.	Univ Michigan, Dept Biostat, Ann Arbor, MI 48109 USA	Wang, SJ (reprint author), Univ Michigan, Dept Biostat, Ann Arbor, MI 48109 USA.						BREIMAN L, 1995, TECHNOMETRICS, V37, P373, DOI 10.2307/1269730; Shen XT, 2002, J AM STAT ASSOC, V97, P210, DOI 10.1198/016214502753479356; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; Khan J, 2001, NAT MED, V7, P673, DOI 10.1038/89044; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1; Bickel PJ, 2004, BERNOULLI, V10, P989, DOI 10.3150/bj/1106314847; Golub TR, 1999, SCIENCE, V286, P531, DOI 10.1126/science.286.5439.531; Yeung KY, 2001, BIOINFORMATICS, V17, P763, DOI 10.1093/bioinformatics/17.9.763; Fraley C, 2002, J AM STAT ASSOC, V97, P611, DOI 10.1198/016214502760047131; Zou H, 2006, J AM STAT ASSOC, V101, P1418, DOI 10.1198/016214506000000735; Turlach BA, 2005, TECHNOMETRICS, V47, P349, DOI 10.1198/004017005000000139; SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136; Chang W., 1983, APPL STAT-J ROY ST C, V32, P267, DOI 10.2307/2347949; Friedman JH, 2004, J ROY STAT SOC B, V66, P815, DOI 10.1111/j.1467-9868.2004.02059.x; Hoff PD, 2006, BAYESIAN ANAL, V1, P321; Liu J. S., 2003, BAYESIAN STAT, V7, P249; MARRN J, 2002, DISTANCE WEIGHTED DI; McLachlan G, 2002, FINITE MIXTURE MODEL; PAN W, 2006, PENALIZED MODEL BASE; Raftery AE, 2006, J AM STAT ASSOC, V101, P168, DOI 10.1198/016214506000000113; RAFTERY AE, 2003, BAYESIAN STAT, V7, P266; Tadesse MG, 2005, J AM STAT ASSOC, V100, P602, DOI 10.1198/0162145040000001565; ZHANG H, 2006, I STAT MIMEO SERIES, V2596; ZHANG HH, 2006, IN PRESS BIOMETRIKA; ZHAO P, 2006, IN PRESS J MACHINE L; Zhao P, 2006, GROUPED HIERARCHICAL; ZOU H, 2006, IN PRESS STAT SINICA	28	0	0	0	3	AMER MATHEMATICAL SOC	PROVIDENCE	P.O. BOX 6248, PROVIDENCE, RI 02940 USA	0271-4132		978-0-8218-4195-2	CONTEMP MATH			2007	443						177	192				16	Mathematics, Applied; Mathematics	Mathematics	BGW56	WOS:000250954400013		
B	Yang, ZR; Young, N		Gardner, JW		Yang, Zheng Rong; Young, Natasha			Regressional inhibitive crosstalk models	Proceedings of the Fifth IASTED International Conference on Biomedical Engineering			English	Proceedings Paper	5th IASTED International Conference on Biomedical Engineering	FEB 14-16, 2007	Innsbruck, AUSTRIA	Int Assoc Sci & Technol Dev, TCBE, World Modelling & Simulat Forum		multivariate models; crosstalk; systems biology	PROTEIN-KINASE CASCADE; SIGNALING PATHWAYS; ACTIVATION; NETWORKS; CELLS; P38; ERK	Scientists have often observed unexpected communication between signalling pathways or molecules in the cell since 1960s. As this activity can hardly be well explained using biology theory, it has been termed as crosstalk, unwanted conversation. Recently, crosstalk has been found as a critical component of biological system robustness. More and more biologists have been working towards the understanding of the underlying mechanism of crosstalk for various signalling pathways or networks. Many qualitative models have been developed for indirect analysis of crosstalk. However, few quantitative models have been developed for such investigations. This study has employed regression approaches for modeling crosstalk between different signalling components and the end signalling components of the MAPK kinases ERK, MEK and JNK based on the published experimental data. The simulation shows that the models can well explain the crosstalk consistent with biology.	Univ Exeter, Sch Engn Comp Sci & Math, Exeter EX4 4QJ, Devon, England	Yang, ZR (reprint author), Univ Exeter, Sch Engn Comp Sci & Math, Exeter EX4 4QJ, Devon, England.						Alberts B, 2002, MOL BIOL CELL; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Madrid LV, 2001, J BIOL CHEM, V276, P18934, DOI 10.1074/jbc.M101103200; Lu ZM, 2002, MOL CELL, V9, P945, DOI 10.1016/S1097-2765(02)00519-1; Huang CYF, 1996, P NATL ACAD SCI USA, V93, P10078, DOI 10.1073/pnas.93.19.10078; Kholodenko BN, 1999, J BIOL CHEM, V274, P30169, DOI 10.1074/jbc.274.42.30169; Berra E, 1998, J BIOL CHEM, V273, P10792, DOI 10.1074/jbc.273.17.10792; Bhalla US, 1999, SCIENCE, V283, P381, DOI 10.1126/science.283.5400.381; Duda R. O., 2002, PATTERN CLASSIFICATI; Ferrell JE, 1996, TRENDS BIOCHEM SCI, V21, P460, DOI 10.1016/S0968-0004(96)20026-X; HOTOKEZAKA H, 2002, J BIOL CHEM, V277; Kawakami Y, 1998, J IMMUNOL, V161, P1795; Kolch W, 2000, BIOCHEM J, V351, P289, DOI 10.1042/0264-6021:3510289; LAY D, 2003, LINEAR ALGEBRA ITS A; Lopez-Bergami P, 2005, MOL CELL, V19, P578, DOI 10.1016/j.molcel.2005.08.005; Olsson AK, 2001, EXP CELL RES, V265, P21, DOI 10.1006/excr.2001.5163; Sachs K, 2005, SCIENCE, V308, P523, DOI 10.1126/science.1105809; Taylor JE, 2004, J EXP BOT, V55, P147, DOI 10.1093/jxb/erh060; Weng GZ, 1999, SCIENCE, V284, P92, DOI 10.1126/science.284.5411.92; YOUNG N, 2006, P 7 INT C INT DAT EN	20	0	0	0	14	ACTA PRESS ANAHEIM	ANAHEIM	PO BOX 5124, ANAHEIM, CA 92814-5124 USA			978-0-88986-648-5				2007							423	428				6	Automation & Control Systems; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Engineering, Biomedical; Radiology, Nuclear Medicine & Medical Imaging	Automation & Control Systems; Computer Science; Engineering; Radiology, Nuclear Medicine & Medical Imaging	BGE36	WOS:000246299900078		
B	Xiong, T; Bi, JB; Rao, B; Cherkassky, V		Apte, C; Liu, B; Parthasarathy, S; Skillicorn, D		Xiong, Tao; Bi, Jinbo; Rao, Bharat; Cherkassky, Vladimir			Probabilistic Joint Feature Selection for Multi-task Learning	PROCEEDINGS OF THE SEVENTH SIAM INTERNATIONAL CONFERENCE ON DATA MINING			English	Proceedings Paper	7th SIAM International Conference on Data Mining	APR 26-28, 2007	Minneapolis, MN	Amer Stat Assoc			CLASSIFICATION; LUNG	We study the joint feature selection problem when learning multiple related classification or regression tasks. By imposing an automatic relevance determination prior on the hypothesis classes associated with each of the tasks and regularizing the variance of the hypothesis parameters, similar feature patterns across different tasks are encouraged and features that are relevant to all (or most) of the tasks are identified. Our analysis shows that the proposed probabilistic framework can be seen as a generalization of previous result from adaptive ridge regression to the multi-task learning setting. We provide a detailed description of the proposed algorithms for simultaneous model construction and justify the proposed algorithms in several aspects. Our experimental results show that this approach outperforms a regularized multi-task learning approach and the traditional methods where individual tasks are solved independently on synthetic data and the real-world data sets for lung cancer prognosis.	[Xiong, Tao; Cherkassky, Vladimir] Univ Minnesota, Dept Elect & Comp Engn, Minneapolis, MN 55455 USA	Xiong, T (reprint author), Univ Minnesota, Dept Elect & Comp Engn, Minneapolis, MN 55455 USA.	txiong@ece.umn.edu; jinbo.big@siemens.com; bharat.rao@siemens.com; cherkass@ece.umn.edu					Ando R. K., 2005, J MACHINE LEARNING R; Suzuki K, 2006, ANN THORAC SURG, V81, P413, DOI 10.1016/j.athoracsur.2005.07.058; Armato SG, 2001, MED PHYS, V28, P1552, DOI 10.1118/1.1387272; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; [Anonymous], 2004, P 21 INT C MACH LEAR; Provost F, 2001, MACH LEARN, V42, P203, DOI 10.1023/A:1007601015854; Tipping ME, 2001, J MACH LEARN RES, V1, P211, DOI 10.1162/15324430152748236; MACKAY DJC, 1995, NETWORK-COMP NEURAL, V6, P469, DOI 10.1088/0954-898X/6/3/011; Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734; Evgeniou T., 2004, P 10 C KNOWL DISC DA; GANDVALET Y, 1999, ADV NEURAL INFORM PR, V11; GANDVALET Y, 1998, P INT C ART NEUR NET, P201; GOODMAN J, 2004, P ANN M ASS COMP LIN; Greene W. H., 2002, ECONOMETRIC ANAL; Guyon I., 2003, Journal of Machine Learning Research, V3, DOI 10.1162/153244303322753616; Hastie T., 2001, ELEMENTS STAT LEARNI; HESKES T, 2000, P ICML; Lee S, 2006, P 21 NAT C ART INT A; Liu HA, 1998, APPL INTELL, V9, P217, DOI 10.1023/A:1008363719778; MALOUF R, P 6 C NAT LANG LEARN; MINKA TP, COMPARISON NUMERICAL; NAIDICH DP, 2004, P CARS 2004 COMP ASS, P902; Obozinski G., 2006, MULTITASK FEATURE SE; Tipping ME, 2000, ADV NEURAL INFORM PR, V12; Vandenberghe L., 2004, CONVEX OPTIMIZATION; XUE Y, 2005, WORKSH OP PROBL CHAL; Yu K., 2005, P 22 INT C MACH LEAR; Zhang J, 2006, THESIS CARNEGIE MELL; ZHANG T, INFORM RETRIEVAL, V4, P5	29	0	0	0	2	SIAM	PHILADELPHIA	3600 UNIV CITY SCIENCE CENTER, PHILADELPHIA, PA 19104-2688 USA			978-0-898716-30-6				2007							332	342				11	Computer Science, Artificial Intelligence; Computer Science, Information Systems; Mathematics, Applied	Computer Science; Mathematics	BUG50	WOS:000289220200030		
S	Renzullo, LJ; Blanchfield, AL; Powell, KS		Powell, KS; Trethowan, CJ		Renzullo, L. J.; Blanchfield, A. L.; Powell, K. S.			Insights into the early detection of grapevine Phylloxera from in situ hyperspectral data	Proceedings of the Third International Grapevine Phylloxera Symposium	ACTA HORTICULTURAE		English	Proceedings Paper	3rd International Grapevine Phylloxera Symposium	OCT 05-07, 2005	Fremantle, AUSTRALIA	Int Soc Horticultural Sci, Dept Primary Ind, Grape Ind Board S Australia		grapevine phylloxera; hyperspectral retlectance spectrometry	REFLECTANCE; STRESS; REGRESSION; SELECTION; PROSPECT	In the interests of characterising a "unique" spectral signature of phylloxera infestation, leaf-level reflectance spectrometry was employed in a glasshouse trial and at various vineyards throughout north eastern Victoria, Australia. These hyperspectral data were processed using statistical discriminant analysis to find narrow spectral features that may be exploited to separate phylloxera infestation from other stresses. Findings show that, at the leaf level, the method can differentiate phylloxera-infested vines from vines subjected to the other stresses investigated. There is, however, often confusion as to the spectral characteristics of the stress that may be uniquely attributed to phylloxera infestation. This paper describes the in situ hyperspectral reflectance data that were acquired over the course of a 2-year Cooperative Research Centre for Viticulture project on the early detection of phylloxera infestation in Australian vineyards. Focus is given to the methods of analysis used in the project. The results described, whilst applicable to the leaf-level reflectances, may not necessarily scale up to the canopy and therefore remotely-sensed signal; an area in need of further investigation.	Cooperat Res Ctr Viticulture, Glen Osmond, SA 5064, Australia	Renzullo, LJ (reprint author), Cooperat Res Ctr Viticulture, Glen Osmond, SA 5064, Australia.		Renzullo, Luigi/D-5797-2011	Renzullo, Luigi/0000-0003-3056-4109			Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; HOERL AE, 1970, TECHNOMETRICS, V12, P55; Zarco-Tejada PJ, 2002, J ENVIRON QUAL, V31, P1433; Zarco-Tejada PJ, 2003, REMOTE SENS ENVIRON, V84, P283, DOI 10.1016/S0034-4257(02)00113-X; HASTIE T, 1995, ANN STAT, V23, P73, DOI 10.1214/aos/1176324456; CARTER GA, 1993, AM J BOT, V80, P239, DOI 10.2307/2445346; Clark R. N., 2003, J GEOPHYS RES, V108, p[5131, 5, 5], DOI 10.1029/2002JE001847; EDWARDS J, 2003, THESIS ADELAIDE U AU, P56; Efron B, 1993, INTRO BOOTSTRAP; HELD A, 2001, GRAPE HLTH MONITORIN; HSU PH, 2003, THESIS NATL CHENG KU, P146; JACQUEMOUD S, 1990, REMOTE SENS ENVIRON, V34, P75, DOI 10.1016/0034-4257(90)90100-Z; Lobitz B., 1997, 112218 NASA; LONDON A, 2004, COMMUNICATION; PETERSON DL, 2000, GEOG INF SCI, V6, P181; POWELL KS, 2003, 003 DNR GWRDC, P165; RENZULLO L, 2004, NZ GRAP WIN 32 ANN T, P126; Renzullo LJ, 2006, IEEE T GEOSCI REMOTE, V44, P1986, DOI 10.1109/TGRS.2006.870441; Renzullo LJ, 2006, INT J REMOTE SENS, V27, P817, DOI 10.1080/01431160500239164; Tsai F, 1998, REMOTE SENS ENVIRON, V66, P41, DOI 10.1016/S0034-4257(98)00032-7; ZARCOTEJADA P, 2000, REMOTE SENS ENVIRON, V75, P582	21	2	2	0	8	INTERNATIONAL SOCIETY HORTICULTURAL SCIENCE	LEUVEN 1	PO BOX 500, 3001 LEUVEN 1, BELGIUM	0567-7572		978-90-6605-170-6	ACTA HORTIC			2007		733					59	74				16	Agronomy; Horticulture	Agriculture	BGD63	WOS:000246176500006		
S	Sharon, E; Segal, E		Speed, T; Huang, H		Sharon, Eilon; Segal, Eran			A feature-based approach to modeling protein-DNA interactions	Research in Computational Molecular Biology, Proceedings	LECTURE NOTES IN COMPUTER SCIENCE		English	Proceedings Paper	11th Annual International Conference on Research in Computational Molecular Biology	APR 21-25, 2007	Oakland, CA	AFFYMETRIX, BioNovo, Genentech, Int Soc Computat Biol, Helicos, Merck, Roche, Wyeth, ARIADNE, QB3 Calif Inst Quantitat Biomed Res		transcription factor binding sites; DNA sequence motifs; probabilistic graphical models; Markov networks; motif finder	TRANSCRIPTION FACTOR; SITES	Transcription factor (TF) binding to its DNA target site is a fundamental regulatory interaction. The most common model used to represent TF binding specificities is a position specific scoring matriX (PSSM), which assumes independence between binding positions. In many cases this simplifying assumption does not hold. Here, we present feature motif models (FMMs), a novel probabilistic method for modeling TF-DNA interactions, based on Markov networks. Our approach uses sequence features to represent TF binding specificities, where each feature may span multiple positions. We develop the mathematical formulation of our models, and devise an algorithm for learning their structural features from binding site data. We evaluate our approach on synthetic data, and then apply it to binding site and ChIP-chip data from yeast. We reveal sequence features that are present in the binding specificities of yeast TFs, and show that FMMs explain the binding data significantly better than PSSMs.	Weizmann Inst Sci, Dept Comp Sci, IL-76100 Rehovot, Israel	Sharon, E (reprint author), Weizmann Inst Sci, Dept Comp Sci, IL-76100 Rehovot, Israel.						Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; MacIsaac KD, 2006, BMC BIOINFORMATICS, V7, DOI 10.1186/1471-2105-7-113; Harbison CT, 2004, NATURE, V431, P99, DOI 10.1038/nature02800; BARASH Y, 2003, RECOMB; BULYK ML, 2006, CURRENT OPINION BIOT, V17, P1; DellaPietra S, 1997, IEEE T PATTERN ANAL, V19, P380, DOI 10.1109/34.588021; Elnitski L, 2006, GENOME RES, V16, P1455, DOI 10.1101/gr.4140006; LEEE SI, 2007, NIPS; Maerkl SJ, 2007, SCIENCE, V315, P233, DOI 10.1126/science.1131007; Minka T. P., 2001, 758 CARN MELL U; Ng AY, 2004, ICML; Perkins S., 2003, Journal of Machine Learning Research, V3, DOI 10.1162/153244303322753698; ROTHERMEL B, 1997, J BIOL CHEM, V272, P1901; SEGAL E, 2003, BIOINFORMATICS S1, V19, pS273; Yedidia J., 2000, NIPS, P689; Zeitlinger J, 2003, CELL, V113, P395, DOI 10.1016/S0092-8674(03)00301-5	16	3	5	1	1	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	0302-9743		978-3-540-71680-8	LECT NOTES COMPUT SC			2007	4453						77	91				15	Biochemical Research Methods; Biochemistry & Molecular Biology; Computer Science, Information Systems	Biochemistry & Molecular Biology; Computer Science	BGI34	WOS:000247321200006		
B	Jang, W		Babu, GJ; Feigelson, ED		Jang, Woncheol			Discussion on "Analyzing Data from Astronomical Surveys: Issues and Directions" by T. J. Loredo	STATISTICAL CHALLENGES IN MODERN ASTRONOMY IV	ASTRONOMICAL SOCIETY OF THE PACIFIC CONFERENCE SERIES		English	Proceedings Paper	4th Statistical Challenges in Modern Astronomy Conference	JUN 12-15, 2006	University Pk, PA		Penn State Univ		ASYMPTOTIC EQUIVALENCE; TRUNCATED DATA; WHITE-NOISE; REGRESSION; SELECTION		Univ Georgia, Dept Hlth Adm Biostat & Epidemiol, Coll Publ Hlth, Paul D Coverdell ctr, Athens, GA 30602 USA	Jang, W (reprint author), Univ Georgia, Dept Hlth Adm Biostat & Epidemiol, Coll Publ Hlth, Paul D Coverdell ctr, 129C, Athens, GA 30602 USA.						AKRITAS MG, 1997, STAT CHALLENGES ASTR, V2, P105; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; WOODROOFE M, 1985, ANN STAT, V13, P163, DOI 10.1214/aos/1176346584; Brown LD, 1996, ANN STAT, V24, P2384; Efron B, 1999, J AM STAT ASSOC, V94, P824, DOI 10.2307/2669997; LYNDENBE.D, 1971, MON NOT R ASTRON SOC, V155, P95; Nussbaum M, 1996, ANN STAT, V24, P2399; SCHAFER C, 2007, IN PRESS AM SOC PACI; Stefanski L, 1990, STATISTICS, V21, P169, DOI 10.1080/02331889008802238; SUN JY, 2007, IN PRESS NONPARAMETR; SUN JY, 2002, UNPUB MIXTURES BUMPS	11	0	0	0	2	ASTRONOMICAL SOC PACIFIC	SAN FRANCISCO	390 ASHTON AVE, SAN FRANCISCO, CA 94112 USA			978-1-58381-240-2	ASTR SOC P			2007	371						138	141				4	Astronomy & Astrophysics; Physics, Particles & Fields; Statistics & Probability	Astronomy & Astrophysics; Physics; Mathematics	BHB12	WOS:000252032000012		
B	Meinhausen, N; Rice, J		Babu, GJ; Feigelson, ED		Meinhausen, Nicolai; Rice, John			Discussion on "Periodicities in Variable Stars: A Few Issues" by C. Koen and "Periodicity in Gravitational Waves" by G. Woan	STATISTICAL CHALLENGES IN MODERN ASTRONOMY IV	ASTRONOMICAL SOCIETY OF THE PACIFIC CONFERENCE SERIES		English	Proceedings Paper	4th Statistical Challenges in Modern Astronomy Conference	JUN 12-15, 2006	University Pk, PA		Penn State Univ				[Meinhausen, Nicolai; Rice, John] Univ Calif Berkeley, Dept Stat, Berkeley, CA 94720 USA	Meinhausen, N (reprint author), Univ Calif Berkeley, Dept Stat, 367 Evans Hall, Berkeley, CA 94720 USA.						Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Jasra A, 2005, STAT SCI, V20, P50, DOI 10.1214/08834230500000016; GENOVESE C, 1993, GONG 1992 SEISMIC IN, V42, P221; GENTON MG, 2005, UNPUB STAT INFERENCE; Hall P, 2000, BIOMETRIKA, V87, P545, DOI 10.1093/biomet/87.3.545; MEINSHAUSEN N, 2005, 129 ETH ZUR	6	0	0	0	0	ASTRONOMICAL SOC PACIFIC	SAN FRANCISCO	390 ASHTON AVE, SAN FRANCISCO, CA 94112 USA			978-1-58381-240-2	ASTR SOC P			2007	371						354	357				4	Astronomy & Astrophysics; Physics, Particles & Fields; Statistics & Probability	Astronomy & Astrophysics; Physics; Mathematics	BHB12	WOS:000252032000034		
S	Zhu, J; Zou, H		Chen, K; Wang, L		Zhu, Ji; Zou, Hui			Variable selection for the linear support vector machine	Trends in Neural Computation	STUDIES IN COMPUTATIONAL INTELLIGENCE		English	Proceedings Paper	1st International Conference on Natural Computation (ICNC 2005)	AUG 27-29, 2005	Changsha, PEOPLES R CHINA	Xiangtang Univ, IEEE Circuits & Syst Soc, IEEE Computat Intelligence Soc, IEEE Control Syst Soc, Int Neural Network Soc, European Neural Network Soc, Chinese Assoc Artificial Intelligence, Japanese Neural Network Soc, Int Fuzzy Syst Assoc, Asia Pacific Neural Network Assembly, Fuzzy Math & Syst Assoc China, Hunan Comp Federat		SVM; variable selection; quadratic programming	REGRESSION; CLASSIFICATION	The standard L-2-norm support vector machine (SVM) is a widely used tool for the classification problem. The L-1-norm SVM is a variant of the standard L-2-norm SVM, that constrains the L(1)(-)norm of the fitted coefficients. Due to the nature of the L-1-norm, the L-1-norm SVM has the property of automatically selecting variables, not shared by the standard L-2-norm SVM. It has been argued that the L-1-norm SVM may have some advantage over the L-2-norm SVM, especially with high dimensional problems and when there are redundant noise variables. On the other hand, the L-1-norm SVM has two drawbacks: (1) when there are several highly correlated variables, the L-1-norm SVM tends to pick only a few of them; and remove the rest; (2) the number of selected variables is upper bounded by the size of the training data. In this chapter, we propose a doubly regularized support vector machine (DrSVM). The DrSVM uses the elastic-net penalty, a mixture of the L-2-norm and the L-1-norm penalties. By doing so, the DrSVM performs automatic variable selection in a way similar to the L-1-norm SVM. In addition; the DrSVM encourages highly correlated variables to be selected (or removed) together. We also develop efficient algorithms to compute the whole solution paths of the DrSVM.	Univ Michigan, Dept Stat, Ann Arbor, MI 48109 USA	Zhu, J (reprint author), Univ Michigan, Dept Stat, Ann Arbor, MI 48109 USA.						Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; MALLAT SG, 1993, IEEE T SIGNAL PROCES, V41, P3397, DOI 10.1109/78.258082; HOERL AE, 1970, TECHNOMETRICS, V12, P55; Guyon I, 2002, MACH LEARN, V46, P389, DOI 10.1023/A:1012487302797; Zou H, 2005, J ROY STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x; Friedman J, 2004, ANN STAT, V32, P102; Lin Y, 2002, DATA MIN KNOWL DISC, V6, P259, DOI 10.1023/A:1015469627679; Efron B, 2004, ANN STAT, V32, P407; Burges CJC, 1998, DATA MIN KNOWL DISC, V2, P121, DOI 10.1023/A:1009715923555; Bradley P., 1998, INT C MACH LEARN; DONOHO D, 1995, J ROY STAT SOC, V57, P201; EVGENIOU T, 1999, ADV LARGE MARGIN CLA; GOLUB T, 2000, SCIENCE, V286, P531; Hastie T., 2001, ELEMENTS STAT LEARNI; Hastie T., 1990, GENERALIZED ADDITIVE; Mukherjee S., 1999, 1677 AI MIT; NG A, 2004, INT C MACH LEARN BAN; Rosset S, 2004, J MACH LEARN RES, V5, P941; ROSSET S, 2004, 431 U MICH DEP STAT; Song MH, 2002, J CHEM INF COMP SCI, V42, P1347, DOI 10.1021/ci025580t; Vapnik V.N., 1995, NATURE STAT LEARNING; WANG L, 2006, IN PRESS STAT SINICA; Zhu J., 2004, NEURAL INFORM PROCES, V16; ZOU H, 2005, 646 U MINN SCH STAT	25	3	3	1	1	SPRINGER-VERLAG BERLIN	BERLIN	HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY	1860-949X		3-540-36121-9	STUD COMP INTELL			2007	35						35	59				25	Computer Science, Artificial Intelligence	Computer Science	BFN80	WOS:000243355000002		
S	Khalidov, I; Van de Ville, D; Fadili, J; Unser, M		VanDeVille, D; Goyal, VK; Papadakis, M		Khalidov, Ildar; Van de Ville, Dimitri; Fadili, Jalal; Unser, Michael			Activelets and sparsity: A new way to detect brain activation from fMRI data - art. no. 67010Y	WAVELETS XII, PTS 1 AND 2	PROCEEDINGS OF THE SOCIETY OF PHOTO-OPTICAL INSTRUMENTATION ENGINEERS (SPIE)		English	Proceedings Paper	Conference on Wavelets XII	AUG 26-29, 2007	San Diego, CA	SPIE		fMRI; wavelets; exponential-spline wavelets; sparse approximations	REGRESSION; SELECTION; NOISE	FMRI time course processing is traditionally performed using linear regression followed by statistical hypothesis testing. While this analysis method is robust against noise, it relies strongly on the signal model. In this paper, we propose a non-parametric framework that is based on two main ideas. First, we introduce a problem-specific type of wavelet basis, for which we coin the term "activelets". The design of these wavelets is inspired by the form of the canonical hemodynamic response function. Second, we take advantage of sparsity-pursuing search techniques to find the most compact representation for the BOLD signal under investigation. The non-linear optimization allows to overcome the sensitivity-specificity trade-off that limits most standard techniques. Remarkably, the activelet framework does not require the knowledge of stimulus onset times; this property can be exploited to answer to new questions in neuroscience.	[Khalidov, Ildar; Van de Ville, Dimitri; Unser, Michael] Ecole Polytech Fed Lausanne, Biomed Imaging Grp, CH-1015 Lausanne, Switzerland	Khalidov, I (reprint author), Ecole Polytech Fed Lausanne, Biomed Imaging Grp, CH-1015 Lausanne, Switzerland.		Unser, Michael/A-1550-2008; Van De Ville, Dimitri/A-5364-2008				Tropp JA, 2006, IEEE T INFORM THEORY, V52, P1030, DOI 10.1109/TIT.2005.864420; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Osborne MR, 2000, IMA J NUMER ANAL, V20, P389, DOI 10.1093/imanum/20.3.389; Friston KJ, 1998, NEUROIMAGE, V7, P30, DOI 10.1006/nimg.1997.0306; Efron B, 2004, ANN STAT, V32, P407; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P6, DOI 10.1109/TIT.2005.860430; Chen S. S., 1999, SIAM J SCI COMPUT, V20, P33; FADILI MJ, 2006, ASTRONOMICAL DATA AN, V4; Frackowiak R., 2003, HUMAN BRAIN FUNCTION; Friman O, 2003, NEUROIMAGE, V19, P837, DOI 10.1016/S1053-8119(03)00077-6; Khalidov I., 2006, IEEE Transactions on Signal Processing, V54, DOI 10.1109/TSP.2006.870544; Menz MM, 2006, NEUROIMAGE, V32, P1185, DOI 10.1016/j.neuroimage.2006.06.003	12	0	0	0	2	SPIE-INT SOC OPTICAL ENGINEERING	BELLINGHAM	1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA	0277-786X		978-0-8194-6849-9	P SOC PHOTO-OPT INS			2007	6701		1-2				Y7010	Y7010		10.1117/12.734706		8	Optics	Optics	BHC85	WOS:000252227400028		
J	Larsson, EG; Selen, Y				Larsson, Erik G.; Selen, Yngve			Linear regression with a sparse parameter vector	IEEE TRANSACTIONS ON SIGNAL PROCESSING			English	Article						basis selection; Bayesian inference; Lasso linear regression; minimum mean-square error (MMSE) estimation; model averaging; model selection; sparse models; variable selection	VARIABLE SELECTION; MODEL; LASSO	We consider linear regression under a model where the parameter vector is known to be sparse. Using a Bayesian framework, we derive the minimum mean-square error (MMSE) estimate of the parameter vector and a computationally efficient approximation of it. We also derive an empirical-Bayesian version of the estimator, which does not need any a priori information, nor does it need the selection of any user parameters. As a byproduct, we obtain a powerful model ("basis") selection tool for sparse models. The performance and robustness of our new estimators are illustrated via numerical examples.	Royal Inst Technol, Sch Elect Engn, Commun Theory Lab, SE-10044 Stockholm, Sweden; Uppsala Univ, Dept Informat Technol, Div Syst & Control, SE-75105 Uppsala, Sweden	Larsson, EG (reprint author), Royal Inst Technol, Sch Elect Engn, Commun Theory Lab, SE-10044 Stockholm, Sweden.	erik.larsson@ee.kth.se; yngve.selen@it.uu.se					[Anonymous], 2003, P80215 IEEE; Malioutov D, 2005, IEEE T SIGNAL PROCES, V53, P3010, DOI 10.1109/TSP.2005.850882; Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010; Rao BD, 2003, IEEE T SIGNAL PROCES, V51, P760, DOI 10.1109/TSP.2002.808076; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Wolfe PJ, 2004, J ROY STAT SOC B, V66, P575, DOI 10.1111/j.1467-9868.2004.02052.x; Wipf DP, 2004, IEEE T SIGNAL PROCES, V52, P2153, DOI 10.1109/TSP.2004.831016; Stoica P, 2004, IEEE SIGNAL PROC MAG, V21, P36, DOI 10.1109/MSP.2004.1311138; Rao BD, 1999, IEEE T SIGNAL PROCES, V47, P187, DOI 10.1109/78.738251; Efron B, 2004, ANN STAT, V32, P407; Fuchs JJ, 2004, IEEE T INFORM THEORY, V50, P1341, DOI 10.1109/TIT.2004.828141; George EI, 2000, BIOMETRIKA, V87, P731, DOI 10.1093/biomet/87.4.731; Homer J, 1998, IEEE T SIGNAL PROCES, V46, P2651, DOI 10.1109/78.720368; IHAKA R, 1996, J COMPUTATIONAL GRAP, V5, P299, DOI DOI 10.2307/1390807; Johnstone IM, 2005, J STAT SOFTW, V12, P1; Kay S., 1993, FUNDAMENTALS STAT SI; Kocic M., 1995, P MTS IEEE OCEANS, V3, P1417; Leng CL, 2006, STAT SINICA, V16, P1273; Mackay D. J. C., 2003, INFORM THEORY INFERE; OBRIEN MS, 1994, IEEE T SIGNAL PROCES, V42, P3353, DOI 10.1109/78.340772; SALEH AAM, 1987, IEEE J SEL AREA COMM, V5, P128, DOI 10.1109/JSAC.1987.1146527; SELEN Y, 2007, IN PRESS IEEE T WIRE; Stoica P., 2005, SPECTRAL ANAL SIGNAL	23	54	55	1	4	IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC	PISCATAWAY	445 HOES LANE, PISCATAWAY, NJ 08855 USA	1053-587X			IEEE T SIGNAL PROCES	IEEE Trans. Signal Process.	FEB	2007	55	2					451	460		10.1109/TSP.2006.887109		10	Engineering, Electrical & Electronic	Engineering	132OR	WOS:000243952600005		
J	Hong, X; Mitchell, RJ				Hong, X.; Mitchell, R. J.			Backward elimination model construction for regression and classification using leave-one-out criteria	INTERNATIONAL JOURNAL OF SYSTEMS SCIENCE			English	Article						classification; cross validation; forward regression; backward elimination; system identification	ALGORITHM; SELECTION; DECOMPOSITION	A fundamental principle in practical nonlinear data modeling is the parsimonious principle of constructing the minimal model that explains the training data well. Leave-one-out (LOO) cross validation is often used to estimate generalization errors by choosing amongst different network architectures (M. Stone, "Cross validatory choice and assessment of statistical predictions", J. R. Stast. Soc., Ser. B, 36, pp. 117-147, 1974). Based upon the minimization of LOO criteria of either the mean squares of LOO errors or the LOO misclassification rate respectively, we present two backward elimination algorithms as model post-processing procedures for regression and classification problems. The proposed backward elimination procedures exploit an orthogonalization procedure to enable the orthogonality between the subspace as spanned by the pruned model and the deleted regressor. Subsequently, it is shown that the LOO criteria used in both algorithms can be calculated via some analytic recursive formula, as derived in this contribution, without actually splitting the estimation data set so as to reduce computational expense. Compared to most other model construction methods, the proposed algorithms are advantageous in several aspects; (i) There are no tuning parameters to be optimized through an extra validation data set; (ii) The procedure is fully automatic without an additional stopping criteria; and (iii) The model structure selection is directly based on model generalization performance. The illustrative examples on regression and classification are used to demonstrate that the proposed algorithms are viable post-processing methods to prune a model to gain extra sparsity and improved generalization.	Univ Reading, Sch Syst Engn, Reading RG6 6AY, Berks, England	Hong, X (reprint author), Univ Reading, Sch Syst Engn, Reading RG6 6AY, Berks, England.	x.hong@reading.ac.uk					Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Muller KR, 2001, IEEE T NEURAL NETWOR, V12, P181, DOI 10.1109/72.914517; Chen SSB, 2001, SIAM REV, V43, P129, DOI 10.1137/S003614450037906X; ALLEN DM, 1974, TECHNOMETRICS, V16, P125, DOI 10.2307/1267500; Tipping ME, 2001, J MACH LEARN RES, V1, P211, DOI 10.1162/15324430152748236; Efron B, 2004, ANN STAT, V32, P407; Ratsch G, 2001, MACH LEARN, V42, P287, DOI 10.1023/A:1007618119488; CHEN S, 1989, INT J CONTROL, V50, P1873, DOI 10.1080/00207178908953472; Barlett PL, 1998, IEEE T INFORMATION T, V44, P525; Brown M., 1994, NEUROFUZZY ADAPTIVE; Chen S, 1999, IEEE T NEURAL NETWOR, V10, P1239, DOI 10.1109/72.788663; CHEN S, 2006, IN PRESS IEEE T NEUR; Chen S, 2004, IEEE T SYST MAN CY B, V34, P898, DOI 10.1109/TSMCB.2003.817107; Green P, 1994, NONPARAMETRIC REGRES; Harris CJ, 2002, ADAPTIVE MODELING ES; Hastie T., 2001, ELEMENTS STAT LEARNI; Hong X, 1997, IEE P-CONTR THEOR AP, V144, P381, DOI 10.1049/ip-cta:19971436; Hong X., 2004, INT J HYBRID INTELL, V1, P90; Hong X, 2003, IEEE T FUZZY SYST, V11, P528, DOI 10.1109/TFUZZ.2003.814842; Hong X, 2003, IEE P-CONTR THEOR AP, V150, P245, DOI 10.1049/ip-cta:20030311; HONG X, 2005, UNPUB INT J SYS SCI; Mao KZ, 2002, IEEE T NEURAL NETWOR, V13, P1211, DOI 10.1109/TNN.2002.1031953; Miller A. J., 1990, SUBSET SELECTION REG; Miller WF, 1990, NEURAL NETWORKS CONT; Murray-Smith R., 1997, MULTIPLE MODEL APPRO; ORR MJL, 1993, NEURAL COMPUT, V7, P954; Shevade SK, 2003, BIOINFORMATICS, V19, P2246, DOI 10.1093/bioinformatics/btg308; Stone M., 1974, J R STAT SOC B, V36, P117; Vapnik V., 1998, STAT LEARNING THEORY; Wahba G., 1990, SPLINE MODELS OBSERV	30	11	11	1	4	TAYLOR & FRANCIS LTD	ABINGDON	4 PARK SQUARE, MILTON PARK, ABINGDON OX14 4RN, OXON, ENGLAND	0020-7721			INT J SYST SCI	Int. J. Syst. Sci.	FEB	2007	38	2					101	113		10.1080/00207720601051463		13	Automation & Control Systems; Computer Science, Theory & Methods; Operations Research & Management Science	Automation & Control Systems; Computer Science; Operations Research & Management Science	140SL	WOS:000244526900002		
J	Ayers, S; Wright, DB; Wells, N				Ayers, Susan; Wright, Daniel B.; Wells, Nicola			Symptoms of post-traumatic stress disorder in couples after birth: association with the couple's relationship and parent-baby bond	JOURNAL OF REPRODUCTIVE AND INFANT PSYCHOLOGY			English	Article							CHILDBIRTH; PREVALENCE; FATHERS; TRAUMA; PSYCHOPATHOLOGY; POSTPARTUM; PREDICTORS; ANXIETY; MOTHERS; EVENT	Recent research suggests a proportion of women develop post-traumatic stress disorder (PTSD) after childbirth. To date, the effects of postnatal PTSD on the couple's relationship and the parent-baby bond have not been examined. In the present study, 64 couples completed questionnaires about the birth, symptoms of PTSD, the couple's relationship and parent-baby bond 9 weeks after childbirth. Results showed 5% of men and women had severe symptoms of PTSD. Symptoms were strongly associated within couples and were related to similar birth factors for men and women. PTSD symptoms were associated with neither the parent-baby bond nor couple's relationship. The mother-baby bond was not associated with any variables measured in this study. However, the father-baby bond was associated with the couple's relationship. It is concluded that men and women have comparable levels of PTSD symptoms 9 weeks after birth. Furthermore, these results suggest postnatal symptoms of PTSD have little association with the couple's relationship or the parent-baby bond in the short term. However, further research is needed to address methodological considerations.	Univ Sussex, Dept Psychol, Brighton BN1 9QH, E Sussex, England; Brunel Univ, Uxbridge UB8 3PH, Middx, England	Ayers, S (reprint author), Univ Sussex, Dept Psychol, Brighton BN1 9QH, E Sussex, England.	S.Ayers@sussex.ac.uk					Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Creedy DK, 2000, BIRTH-ISS PERINAT C, V27, P104, DOI 10.1046/j.1523-536x.2000.00104.x; Wenzel A, 2005, J ANXIETY DISORD, V19, P295, DOI 10.1016/j.janxdis.2004.04.001; Breslau N, 1997, ARCH GEN PSYCHIAT, V54, P1044; Ayers S, 2001, J PSYCHOSOM OBST GYN, V22, P91, DOI 10.3109/01674820109049959; HOROWITZ M, 1979, PSYCHOSOM MED, V41, P209; Ayers S, 2004, CLIN OBSTET GYNECOL, V47, P552, DOI 10.1097/01.grf.0000129919.00756.9c; Ayers S, 2001, BIRTH-ISS PERINAT C, V28, P111, DOI 10.1046/j.1523-536X.2001.00111.x; Bailham D., 2003, PSYCHOL HEALTH MED, V8, P159; BALLARD CG, 1995, BRIT J PSYCHIAT, V166, P525, DOI 10.1192/bjp.166.4.525; BALLARD CG, 1994, BRIT J PSYCHIAT, V164, P782, DOI 10.1192/bjp.164.6.782; Brockington Ian, 2004, World Psychiatry, V3, P89; COHEN J, 1992, PSYCHOL BULL, V112, P155, DOI 10.1037/0033-2909.112.1.155; Czarnocka J, 2000, BRIT J CLIN PSYCHOL, V39, P35, DOI 10.1348/014466500163095; Horowitz M. J., 1982, HDB STRESS THEORETIC, P711; NICHOLLS K, IN PRESS BRIT J HLTH; O'Driscoll M., 1994, BRIT J MIDWIFERY, V2, P39; Office of Population Censuses and Survey, 1990, STAND OCC CLASS, V1; PEARCE H, 2005, J REPROD INFANT PSYC, V23, P1; PHARES V, 1992, AM PSYCHOL, V47, P656, DOI 10.1037/0003-066X.47.5.656; Shalev AY, 1997, BRIT J PSYCHIAT, V170, P558, DOI 10.1192/bjp.170.6.558; Skari H, 2002, BJOG-INT J OBSTET GY, V109, P1154, DOI 10.1016/S1470-0328(02)00968-0; SLADE P, 1993, BRIT J CLIN PSYCHOL, V32, P469; Soderquist J, 2002, J PSYCHOSOM OBST GYN, V23, P31, DOI 10.3109/01674820209093413; Soet JE, 2003, BIRTH-ISS PERINAT C, V30, P36, DOI 10.1046/j.1523-536X.2003.00215.x; SPANIER GB, 1976, J MARRIAGE FAM, V38, P15, DOI 10.2307/350547; STEPTOE A, 1998, HLTH PSYCHOL; STEWART DE, 1982, CAN MED ASSOC J, V127, P713; Tedstone JE, 2003, CLIN PSYCHOL REV, V23, P409, DOI 10.1016/S0272-7358(03)00031-X; WEISAETH L, 1996, BAILLIERES CLIN PSYC, V2; Wijma K, 1997, J ANXIETY DISORD, V11, P587, DOI 10.1016/S0887-6185(97)00041-8	31	13	14	2	11	ROUTLEDGE JOURNALS, TAYLOR & FRANCIS LTD	ABINGDON	4 PARK SQUARE, MILTON PARK, ABINGDON OX14 4RN, OXFORDSHIRE, ENGLAND	0264-6838			J REPROD INFANT PSYC	J. Reprod. Infant Psychol.	FEB	2007	25	1					40	50		10.1080/02646830601117175		11	Psychology, Multidisciplinary	Psychology	143BD	WOS:000244694100004		
J	Kim, SP; Sanchez, JC; Principe, JC				Kim, S-P.; Sanchez, J. C.; Principe, J. C.			Real time input subset selection for linear time-variant MIMO systems	OPTIMIZATION METHODS & SOFTWARE			English	Article; Proceedings Paper	Conference on Systems Analysis, Data Mining and Optimization in Biomedicine	FEB 02-04, 2005	Gainesville, FL		Univ Florida	time-variant multi-input multi-output systems; on-line channel selection; least angle regression	ALGORITHM; PERFORMANCE; REGRESSION; LMS; RLS	In this paper we propose an approach for multi-input multi-output (MIMO) system identification when the statistical relationship between input and output varies in input space as well as in time; i.e. nonstationary in space and time. An on-line variable selection algorithm, which has been recently developed for selecting a subset of input variables in real time by modifying least angle regression (LAR) with recursive estimators, is extensively applied to the linear time-variant MIMO systems. In our approach, a subset of input channels relevant with output is selected at every time instance based on the correlation between the filtering outcome of individual input channels and desired output. The on-line variable selection algorithm performs channel selection with weights using this real-time correlation. The proposed model is compared with a typical linear model in which only the least mean squares (LMS) is used to update system parameters. Tracking performances of these two models are demonstrated in a computer simulation and in a real-world application for tracking a linear relationship between neural firing rates of a primate and synchronously recorded hand kinematics. In both cases, our model demonstrates superior tracking performance.	Univ Florida, Dept Elect & Comp Engn, Gainesville, FL 32611 USA; Univ Florida, Dept Pediat, Div Neurol, Gainesville, FL 32611 USA	Kim, SP (reprint author), Univ Florida, Dept Elect & Comp Engn, Gainesville, FL 32611 USA.	spkim@cs.brown.edu					Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Wessberg J, 2000, NATURE, V408, P361; Efron B, 2004, ANN STAT, V32, P407; Cotter SF, 2002, IEEE T COMMUN, V50, P374, DOI 10.1109/26.990897; EWEDA E, 1994, IEEE T SIGNAL PROCES, V42, P2937, DOI 10.1109/78.330354; Guyon I., 2003, Journal of Machine Learning Research, V3, DOI 10.1162/153244303322753616; HAJIVANDI M, 1990, IEEE T ACOUST SPEECH, V38, P1953, DOI 10.1109/29.103096; Hastie T., 2004, ELEMENTS STAT LEARNI; Haykin S., 1996, ADAPTIVE FILTER THEO; Haykin S., 1996, NEURAL NETWORKS COMP, V2nd; HYAKIN S, 1995, IEEE MILCOM 95, V2, P602; KIM SP, 2004, 2004 IEEE WORKSH MAC; MACCHI OM, 1991, IEEE T SIGNAL PROCES, V39, P583; Nishiyama K, 2004, IEEE T SIGNAL PROCES, V52, P1335, DOI 10.1109/TSP.2004.826156; Sanchez J, 2002, P NEUR NET SIG PROC, P139, DOI 10.1109/NNSP.2002.1030025; SANCHEZ JC, 2003, 25 INT C IEEE EMBS C; Weisberg S., 1980, APPL LINEAR REGRESSI	17	7	7	0	0	TAYLOR & FRANCIS LTD	ABINGDON	4 PARK SQUARE, MILTON PARK, ABINGDON OX14 4RN, OXON, ENGLAND	1055-6788			OPTIM METHOD SOFTW	Optim. Method Softw.	FEB	2007	22	1					83	98		10.1080/10556780600881886		16	Computer Science, Software Engineering; Operations Research & Management Science; Mathematics, Applied	Computer Science; Operations Research & Management Science; Mathematics	112TO	WOS:000242550300007		
J	Hodges, JS; Cui, Y; Sargent, DJ; Carlin, BP				Hodges, James S.; Cui, Yue; Sargent, Daniel J.; Carlin, Bradley P.			Smoothing balanced single-error-term analysis of variance	TECHNOMETRICS			English	Article						Bayesian analysis; degrees of freedom; masking; prior distribution; shrinkage; sub-group analysis	HIERARCHICAL-MODELS; SELECTION	We present an approach to smoothing balanced, single-error term analysis of variance (ANOVA), descended from Smith, that also allows spatial, temporal, or spatiotemporal smoothing. The approach addresses unreplicated designs, masked contrasts in effects with many degrees of freedom, and subgroup analysis, demonstrated using a study of denture-lining materials. Our approach is Bayesian but can be viewed as a way to generate frequentist procedures. A simulation experiment compares four priors, unsmoothed ANOVA, and dropping nonsignificant interactions. Three priors have advantages when some interactions are absent; dropping nonsignificant interactions has serious flaws. We contrast our approach with the approaches of Nobile-Green and Gelman.	Univ Minnesota, Div Biostat, Minneapolis, MN 55414 USA; Mayo Clin, Mayo Clin Canc Ctr, Rochester, MN 55905 USA	Hodges, JS (reprint author), Univ Minnesota, Div Biostat, Minneapolis, MN 55414 USA.	hodges@ccbr.umn.edu; yuecui@biostat.umn.edu; sargent@mayo.edu; carli002@umn.edu		Sargent, Daniel/0000-0002-2684-4741			GELFAND AE, 1990, J AM STAT ASSOC, V85, P398, DOI 10.2307/2289776; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; DIXON DO, 1991, BIOMETRICS, V47, P871, DOI 10.2307/2532645; Daniels MJ, 1999, CAN J STAT, V27, P567, DOI 10.2307/3316112; DERKSEN S, 1992, BRIT J MATH STAT PSY, V45, P265; FREEDMAN DA, 1983, AM STAT, V37, P152, DOI 10.2307/2685877; Gelman A, 2005, ANN STAT, V33, P1, DOI 10.1214/009053604000001048; Hodges JS, 2003, BIOMETRICS, V59, P317, DOI 10.1111/1541-0420.00038; Hodges JS, 1998, J ROY STAT SOC B, V60, P497, DOI 10.1111/1467-9868.00137; Hodges JS, 2001, BIOMETRIKA, V88, P367, DOI 10.1093/biomet/88.2.367; Leamer E. E., 1978, SPECIFICATION SEARCH; Lee Y, 1996, J ROY STAT SOC B MET, V58, P619; Nobile A, 2000, BIOMETRIKA, V87, P15, DOI 10.1093/biomet/87.1.15; Pesun IJ, 2002, J PROSTHET DENT, V87, P311, DOI 10.1067/mpr.2002.122162; Raftery A. E., 1993, 262 U WASH DEP STAT; Scheffe H., 1959, ANAL VARIANCE; SMITH AFM, 1973, BIOMETRIKA, V60, P319, DOI 10.1093/biomet/60.2.319; WHITTAKER J, 1998, J ROY STAT SOC B, V60, P533	18	12	12	0	1	AMER STATISTICAL ASSOC	ALEXANDRIA	1429 DUKE ST, ALEXANDRIA, VA 22314 USA	0040-1706			TECHNOMETRICS	Technometrics	FEB	2007	49	1					12	25		10.1198/004017006000000408		14	Statistics & Probability	Mathematics	132PK	WOS:000243954500002		
J	Steele, AD; Jackson, WS; King, OD; Lindquist, S				Steele, Andrew D.; Jackson, Walker S.; King, Oliver D.; Lindquist, Susan			The power of automated high-resolution behavior analysis revealed by its application to mouse models of Huntington's and prion diseases	PROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES OF THE UNITED STATES OF AMERICA			English	Article						home cage; neurodegeneration; prion protein; polyQ	CREUTZFELDT-JAKOB-DISEASE; EXPANDED CAG REPEAT; MICE; MUTATION; DEFICITS; GENE; ENVIRONMENT; PHENOTYPE; SCRAPIE; EXON-1	Automated analysis of mouse behavior will be vital for elucidating the genetic determinants of behavior, for comprehensive analysis of human disease models, and for assessing the efficacy of various therapeutic strategies and their unexpected side effects. We describe a video-based behavior-recognition technology to analyze home-cage behaviors and demonstrate its power by discovering previously unrecognized features of two already extensively characterized mouse models of neurodegenerative disease. The severe motor abnormalities in Huntington's disease mice manifested in our analysis by decreased hanging, jumping, stretching, and rearing. Surprisingly, behaviors such as resting and grooming were also affected. Unexpectedly, mice with infectious prion disease showed profound increases in activity at disease onset: rearing increased 2.5-fold, walking 10-fold and jumping 30-fold. Strikingly, distinct behaviors were altered specifically during day or night hours. We devised a systems approach for multiple-parameter phenotypic characterization and applied it to defining disease onset robustly and at early time points.	MIT, Whitehead Inst Biomed Res, Cambridge, MA 02142 USA	Lindquist, S (reprint author), MIT, Whitehead Inst Biomed Res, 77 Massachusetts Ave, Cambridge, MA 02142 USA.	lindquist_admin@wi.mit.edu					Carter RJ, 1999, J NEUROSCI, V19, P3248; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Vonsattel JPG, 1998, J NEUROPATH EXP NEUR, V57, P369; Morton AJ, 2005, J NEUROSCI, V25, P157, DOI 10.1523/JNEUROSCI.3842-04.2005; Crabbe JC, 1999, SCIENCE, V284, P1670, DOI 10.1126/science.284.5420.1670; Prusiner SB, 1998, P NATL ACAD SCI USA, V95, P13363, DOI 10.1073/pnas.95.23.13363; Mangiarini L, 1996, CELL, V87, P493, DOI 10.1016/S0092-8674(00)81369-0; Brown SDM, 2005, NAT GENET, V37, P1155, DOI 10.1038/ng1105-1155; Arndt SS, 2001, BEHAV BRAIN RES, V125, P39, DOI 10.1016/S0166-4328(01)00285-6; Auwerx J, 2004, NAT GENET, V36, P925, DOI 10.1038/ng0904-925; Chen D, 2005, SCIENCE, V310, P1641, DOI 10.1126/science.1118357; Cunningham C, 2005, NEUROBIOL DIS, V18, P258, DOI 10.1016/j.nbd.2004.08.015; Dell'Omo G, 2002, EUR J NEUROSCI, V16, P735, DOI 10.1046/j.1460-9568.2002.02128.x; Hickey MA, 2005, NEUROBIOL DIS, V20, P1, DOI 10.1016/j.nbd.2005.01.024; Hockly E, 2003, BRAIN RES BULL, V61, P469, DOI 10.1016/S0361-9230(03)00185-0; Hurlbert MS, 1999, DIABETES, V48, P649, DOI 10.2337/diabetes.48.3.649; Kafkafi N, 2005, P NATL ACAD SCI USA, V102, P4619, DOI 10.1073/pnas.0409554102; KINGSBURY DT, 1983, J IMMUNOL, V131, P491; Landolt HP, 2006, NEUROLOGY, V66, P1418, DOI 10.1212/01.wnl.0000210445.16135.56; Lione LA, 1999, J NEUROSCI, V19, P10428; MCCORMACK CE, 1980, AM J PHYSIOL, V239, pR450; Murphy KPSJ, 2000, J NEUROSCI, V20, P5115; MACDONALD ME, 1993, CELL, V72, P971, DOI 10.1016/0092-8674(93)90585-E	23	78	78	3	6	NATL ACAD SCIENCES	WASHINGTON	2101 CONSTITUTION AVE NW, WASHINGTON, DC 20418 USA	0027-8424			P NATL ACAD SCI USA	Proc. Natl. Acad. Sci. U. S. A.	FEB 6	2007	104	6					1983	1988		10.1073/pnas.0610779104		6	Multidisciplinary Sciences	Science & Technology - Other Topics	135BD	WOS:000244127900046	17261803	
J	Zhang, NR; Siegmund, DO				Zhang, Nancy R.; Siegmund, David O.			A modified Bayes information criterion with applications to the analysis of comparative genomic hybridization data	BIOMETRICS			English	Article						Bayes information criterion; change-point; comparative genomic hybridization; model selection	ARRAY CGH DATA; CHANGE-POINT; NUMBER; MICROARRAYS; SELECTION; MODEL	In the analysis of data generated by change-point processes, one critical challenge is to determine the number of change-points. The classic Bayes information criterion (BIC) statistic does not work well here because of irregularities in the likelihood function. By asymptotic approximation of the Bayes factor, we derive a modified BIC for the model of Brownian motion with changing drift. The modified BIC is similar to the classic BIC in the sense that the first term consists of the log likelihood, but it differs in the terms that penalize for model dimension. As an example of application, this new statistic is used to analyze array-based comparative genomic hybridization (array-CGH) data. Array-CGH measures the number of chromosome copies at each genome location of a cell sample, and is useful for finding the regions of genome deletion and amplification in tumor cells. The modified BIC performs well compared to existing methods in accurately choosing the number of regions of changed copy number. Unlike existing methods, it does not rely on tuning parameters or intensive computing. Thus it is impartial and easier to understand and to use.	Stanford Univ, Dept Stat, Stanford, CA 94305 USA	Zhang, NR (reprint author), Stanford Univ, Dept Stat, Stanford, CA 94305 USA.	nzhang@stanford.edu; dos@stat.stanford.edu					Albertson DG, 2003, NAT GENET, V34, P369, DOI 10.1038/ng1215; Lavielle M, 2005, SIGNAL PROCESS, V85, P1501, DOI 10.1016/j.sigpro.2005.01.012; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Snijders AM, 2001, NAT GENET, V29, P263, DOI 10.1038/ng754; Pollack JR, 1999, NAT GENET, V23, P41; GEORGE EI, 1993, J AM STAT ASSOC, V88, P881, DOI 10.2307/2290777; Pinkel D, 1998, NAT GENET, V20, P207, DOI 10.1038/2524; Olshen AB, 2004, BIOSTATISTICS, V5, P557, DOI 10.1093/biostatistics/kxh008; SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136; Birge L., 2001, J EUR MATH SOC, V3, P203, DOI 10.1007/s100970100031; Fridlyand J, 2004, J MULTIVARIATE ANAL, V90, P132, DOI 10.1016/j.jmva.2004.02.008; Green PJ, 1995, BIOMETRIKA, V82, P711, DOI 10.1093/biomet/82.4.711; Gu C, 2003, STAT SINICA, V13, P811; Hsu L, 2005, BIOSTATISTICS, V6, P211, DOI 10.1093/biostatistics/kxi004; JAMES B, 1987, BIOMETRIKA, V74, P71, DOI 10.1093/biomet/74.1.71; Li W., 2001, P 5 ANN INT C COMP B, P204, DOI 10.1145/369133.369202; Picard F, 2005, BMC BIOINFORMATICS, V6, DOI 10.1186/1471-2105-6-27; SIEGMUND D, 2004, BIOMETRIKA, V92, P785; SIEGMUND DO, 1992, PROBABILITY THEORY /, P147; Snijders AM, 2003, ONCOGENE, V22, P4370, DOI 10.1038/sj.onc.1206482; Wang P, 2005, BIOSTATISTICS, V6, P45, DOI 10.1093/biostatistics/kxh017; YAO YC, 1988, STAT PROBABIL LETT, V6, P181, DOI 10.1016/0167-7152(88)90118-6; ZHANG NR, 2005, THESIS STANFORD U ST	23	68	69	2	8	BLACKWELL PUBLISHING	OXFORD	9600 GARSINGTON RD, OXFORD OX4 2DQ, OXON, ENGLAND	0006-341X			BIOMETRICS	Biometrics	MAR	2007	63	1					22	32		10.1111/j.1541-0420.2006.00662.x		11	Biology; Mathematical & Computational Biology; Statistics & Probability	Life Sciences & Biomedicine - Other Topics; Mathematical & Computational Biology; Mathematics	142KA	WOS:000244647100003	17447926	
J	Datta, S; Le-Rademacher, J; Datta, S				Datta, Susmita; Le-Rademacher, Jennifer; Datta, Somnath			Predicting patient survival from microarray data by accelerated failure time modeling using partial least squares and LASSO	BIOMETRICS			English	Article						cancer; gene expression; partial least squares; right censoring; survival	GENE-EXPRESSION DATA; RIGHT-CENSORED DATA; INVERSE-PROBABILITY; LINEAR-REGRESSION; SELECTION	We consider the problem of predicting survival times of cancer patients from the gene expression profiles of their turner samples via linear regression modeling of log-transformed failure times. The partial least squares (PLS) and least absolute shrinkage and selection operator (LASSO) methodologies are used for this purpose where we first modify the data to account for censoring. Three approaches of handling right censored data-reweighting, mean imputation, and multiple irnputation-are considered. Their performances are examined in a detailed simulation study and compared with that of full data PLS and LASSO had there been no censoring. A major objective of this article is to investigate the performances of PLS and LASSO in the context of microarray data where the number of covariates is very large and there are extremely few samples. We demonstrate that LASSO outperforms PLS in terms of prediction error when the list of covariates includes a moderate to large percentage of useless or noise variables; otherwise, PLS may outperform LASSO. For a moderate sample size (100 with 10,000 covariates), LASSO performed better than a no covariate model (or noise-based prediction). The mean imputation method appears to best track the performance of the full data PLS or LASSO. The mean imputation scheme is used on an existing data set on lung cancer. This reanalysis using the mean imputed PLS and LASSO identifies a number of genes that were known to be related to cancer or tumor activities from previous studies.	Univ Louisville, Dept Bioinformat & Biostat, Louisville, KY 40202 USA; Univ Georgia, Dept Stat, Athens, GA 30602 USA	Datta, S (reprint author), Univ Louisville, Dept Bioinformat & Biostat, Louisville, KY 40202 USA.	somnath.datta@louisville.edu					Knight K, 2000, ANN STAT, V28, P1356; Nguyen DV, 2002, BIOINFORMATICS, V18, P39, DOI 10.1093/bioinformatics/18.1.39; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Robins JM, 2000, BIOMETRICS, V56, P779, DOI 10.1111/j.0006-341X.2000.00779.x; Bullinger L, 2004, NEW ENGL J MED, V350, P1605, DOI 10.1056/NEJMoa031046; FRANK IE, 1993, TECHNOMETRICS, V35, P109, DOI 10.2307/1269656; BENJAMINI Y, 1995, J ROY STAT SOC B MET, V57, P289; Beer DG, 2002, NAT MED, V8, P816, DOI 10.1038/nm733; Efron B, 2004, ANN STAT, V32, P407; BUCKLEY J, 1979, BIOMETRIKA, V66, P429; Bair E, 2004, PLOS BIOL, V2, P511, DOI 10.1371/journal.pbio.00200108; Brown P. J., 1993, MEASUREMENT REGRESSI; Datta S, 2001, GENE EXPRESSION, V9, P249; Datta S., 2005, STAT METHODOL, V2, P65, DOI 10.1016/j.stamet.2004.11.003; GELADI P, 1992, CHEMOMETR INTELL LAB, V15, pR7, DOI 10.1016/0169-7439(92)80021-U; Hastie T., 2001, ELEMENTS STAT LEARNI; Huang J, 2005, BIOMETRICS, V61, P17, DOI 10.1111/j.0006-341X.2005.040304.x; Kiss H, 2002, EUR J HUM GENET, V10, P52, DOI 10.1038/sj/ejhg/5200758; KOUL H, 1981, ANN STAT, V9, P1276, DOI 10.1214/aos/1176345644; Li HZ, 2004, BIOINFORMATICS, V20, P208, DOI 10.1093/bioinformatics/bth900; LOSSOS IS, 2004, NEW ENGL J MED, V350, P1814; Martens H., 1989, MULTIVARIATE CALIBRA; Marx BD, 1996, TECHNOMETRICS, V38, P374, DOI 10.2307/1271308; Nguyen DV, 2002, BIOINFORMATICS, V18, P1625, DOI 10.1093/bioinformatics/18.12.1625; Park PJ, 2002, BIOINFORMATICS, V18, P120; Prentice RL, 1980, STAT ANAL FAILURE TI; Purohit PV, 2003, PROTEOMICS, V3, P1699, DOI 10.1002/pmic.200300518; Robins J. M., 1992, AIDS EPIDEMIOLOGY ME, P297; Satten GA, 2001, STAT PROBABIL LETT, V54, P397, DOI 10.1016/S0167-7152(01)00113-4; Satten GA, 2001, AM STAT, V55, P207, DOI 10.1198/000313001317098185; Tibshirani R, 1997, STAT MED, V16, P385, DOI 10.1002/(SICI)1097-0258(19970228)16:4<385::AID-SIM380>3.0.CO;2-3; Tobias R.D., 1997, INTRO PARTIAL LEAST; Wold H., 1966, MULTIVARIATE ANAL, P391; Wold H., 1975, PERSPECTIVES PROBABI, P117	34	31	34	1	4	WILEY-BLACKWELL	HOBOKEN	111 RIVER ST, HOBOKEN 07030-5774, NJ USA	0006-341X			BIOMETRICS	Biometrics	MAR	2007	63	1					259	271		10.1111/j.1541-0420.2006.00660.x		13	Biology; Mathematical & Computational Biology; Statistics & Probability	Life Sciences & Biomedicine - Other Topics; Mathematical & Computational Biology; Mathematics	142KA	WOS:000244647100029	17447952	
J	Yuan, M; Lin, Y				Yuan, Ming; Lin, Yi			Model selection and estimation in the Gaussian graphical model	BIOMETRIKA			English	Article						covariance selection; lasso; maxdet algorithm; nonnegative garrote; penalized likelihood	LASSO	We propose penalized likelihood methods for estimating the concentration matrix in the Gaussian graphical model. The methods lead to a sparse and shrinkage estimator of the concentration matrix that is positive definite, and thus conduct model selection and estimation simultaneously. The implementation of the methods is nontrivial because of the positive definite constraint on the concentration matrix, but we show that the computation can be done effectively by taking advantage of the efficient maxdet algorithm developed in convex optimization. We propose a BIC-type criterion for the selection of the tuning parameter in the penalized likelihood methods. The connection between our methods and existing methods is illustrated. Simulations and real examples demonstrate the competitive performance of the new methods.	Georgia Inst Technol, Sch Ind & Syst Engn, Atlanta, GA 30332 USA; Univ Wisconsin, Dept Stat, Madison, WI 53706 USA	Yuan, M (reprint author), Georgia Inst Technol, Sch Ind & Syst Engn, 755 Ferst Dr SW, Atlanta, GA 30332 USA.	myuan@isye.gatech.edu; yilin@stat.wisc.edu					BREIMAN L, 1995, TECHNOMETRICS, V37, P373, DOI 10.2307/1269730; Knight K, 2000, ANN STAT, V28, P1356; Drton M, 2004, BIOMETRIKA, V91, P591, DOI 10.1093/biomet/91.3.591; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Li HZ, 2006, BIOSTATISTICS, V7, P302, DOI 10.1093/biostatistics/kxj008; Vandenberghe L, 1998, SIAM J MATRIX ANAL A, V19, P499, DOI 10.1137/S0895479896303430; Meinshausen N, 2006, ANN STAT, V34, P1436, DOI 10.1214/009053606000000281; Boyd S., 2003, CONVEX OPTIMIZATION; Breiman L, 1996, ANN STAT, V24, P2350; Cox D.R., 1996, MULTIVARIATE DEPENDE; Dempster A., 1972, BIOMETRIKA, V32, P95; Edwards D., 2000, INTRO GRAPHICAL MODE; Lauritzen S. L., 1996, GRAPHICAL MODELS; Mardia KV, 1979, MULTIVARIATE ANAL; RAO CR, 1948, BIOMETRIKA, V35, P58, DOI 10.2307/2332629; Whittaker J, 1990, GRAPHICAL MODELS APP	16	245	250	5	19	OXFORD UNIV PRESS	OXFORD	GREAT CLARENDON ST, OXFORD OX2 6DP, ENGLAND	0006-3444			BIOMETRIKA	Biometrika	MAR	2007	94	1					19	35		10.1093/biomet/asm018		17	Biology; Mathematical & Computational Biology; Statistics & Probability	Life Sciences & Biomedicine - Other Topics; Mathematical & Computational Biology; Mathematics	145BH	WOS:000244839800002		
J	Avalos, M; Grandvalet, Y; Ambroise, C				Avalos, Marta; Grandvalet, Yves; Ambroise, Christophe			Parsimonious additive models	COMPUTATIONAL STATISTICS & DATA ANALYSIS			English	Article						model selection; supervised learning; nonparametric regression; function estimation; splines; smoothing; variable selection; lasso; penalization; interpretable models	NONPARAMETRIC REGRESSION; VARIABLE SELECTION; LASSO; LIKELIHOOD	A new method for function estimation and variable selection, specifically designed for additive models fitted by cubic splines is proposed. This new method involves regularizing additive models using the l(1)-norm, which generalizes the lasso to the nonparametric setting. As in the linear case, it shrinks coefficients and produces some coefficients that are exactly zero. It gives parsimonious models, selects significant variables, and reveals nonlinearities in the effects of predictors. Two strategies for finding a parsimonious additive model solution are proposed. Both algorithms are based on a fixed point algorithm, combined with a singular value decomposition that considerably reduces computation. The empirical behavior of parsimonious additive models is compared to the adaptive backfitting BRUTO algorithm. The results allow to characterize the domains in which our approach is effective: it performs significantly better than BRUTO when model estimation is challenging. An implementation of this method is illustrated using real data from the Cophar 1 ANRS 102 trial. Parsimonious additive models are applied to predict the indinavir plasma concentration in HIV patients. Results suggest that this new method is a promising technique for the research and application areas. (c) 2006 Elsevier B.V. All rights reserved.	Univ Victor Segalen Bordeaux, Inst Sante Publ Epidemiol & Dev, INSERM EO338, Equipe Biostat, F-33076 Bordeaux, France; Univ Technol Compiegne, HeuDiaSyc, UMR 6599, F-60205 Compiegne, France	Avalos, M (reprint author), Univ Victor Segalen Bordeaux, Inst Sante Publ Epidemiol & Dev, INSERM EO338, Equipe Biostat, 146 Rue Leo Saignat, F-33076 Bordeaux, France.	marta.avalos@isped.u-bordeaux2.fr					Fu WJJ, 1998, J COMPUT GRAPH STAT, V7, P397, DOI 10.2307/1390712; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Beck N, 1998, AM J POLIT SCI, V42, P596, DOI 10.2307/2991772; Shively TS, 1999, J AM STAT ASSOC, V94, P777, DOI 10.2307/2669990; Efron B, 2004, ANN STAT, V32, P407; AVALOS M, 2003, LECT NOTES COMPUTER, P509; Bacchetti P, 2002, BIOMETRICS, V58, P443, DOI 10.1111/j.0006-341X.2002.00443.x; Bakin S, 1999, THESIS AUSTR NATL U; Breiman L, 1996, ANN STAT, V24, P2350; Brenner N, 2002, CITIES, V19, P3, DOI 10.1016/S0264-2751(01)00042-7; Brumback BA, 1999, J AM STAT ASSOC, V94, P794, DOI 10.2307/2669991; Cantoni E, 2002, BIOMETRIKA, V89, P251, DOI 10.1093/biomet/89.2.251; CHAMBERS JM, 1993, COMPUTER SCI SERIES; CHEN R, 1996, STAT THEORY COMPUTAT, P247; Chen S., 1995, 479 STANF U DEP STAT; CHEN ZH, 1993, J ROY STAT SOC B MET, V55, P473; Dominici F, 2002, AM J EPIDEMIOL, V156, P193, DOI 10.1093/aje/kwf062; Gonzalez-Manteiga W, 2002, STAT PROBABIL LETT, V57, P259, DOI 10.1016/S0167-7152(02)00056-1; GRANDVALET Y, 1998, ADV NEURAL INFORMATI, V11, P445; Grandvalet Y., 1998, ICANN 98, V1, P201; GU C, 1991, SIAM J SCI STAT COMP, V12, P383, DOI 10.1137/0912021; GUNN SR, 2002, MACH LEARN, V10, P581; Guyon I., 2003, Journal of Machine Learning Research, V3, DOI 10.1162/153244303322753616; Hardle W, 1996, BIOMETRIKA, V83, P541, DOI 10.1093/biomet/83.3.541; Hastie T., 2001, SPRINGER SERIES STAT; Hastie T. J., 1990, MONOGRAPHS STAT APPL, V43; Hurvich CM, 1998, J ROY STAT SOC B, V60, P271, DOI 10.1111/1467-9868.00125; LIN Y, 2002, 1072R U WINS; Marx BD, 1998, COMPUT STAT DATA AN, V28, P193, DOI 10.1016/S0167-9473(98)00033-4; Opsomer JD, 2000, J MULTIVARIATE ANAL, V73, P166, DOI 10.1006/jmva.1999.1868; Osborne MR, 2000, J COMPUT GRAPH STAT, V9, P319, DOI 10.2307/1390657; Roth V, 2001, LECT NOTES COMPUT SC, V2130, P339; Rulipert D., 2003, SEMIPARAMETRIC REGRE, V12; Shi P, 1999, J STAT PLAN INFER, V77, P119, DOI 10.1016/S0378-3758(98)00175-X; Smith M, 1996, J ECONOMETRICS, V75, P317, DOI 10.1016/0304-4076(95)01763-1; STONE M, 1974, J R STAT SOC B, V36, P111; Wahba G., 1990, REGIONAL C SERIES AP, V59; Walker E, 2002, J QUAL TECHNOL, V34, P118; Wood SN, 2000, J ROY STAT SOC B, V62, P413, DOI 10.1111/1467-9868.00240; Zhang HH, 2004, J AM STAT ASSOC, V99, P659, DOI 10.1198/016214504000000593	40	7	7	1	3	ELSEVIER SCIENCE BV	AMSTERDAM	PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS	0167-9473			COMPUT STAT DATA AN	Comput. Stat. Data Anal.	MAR 1	2007	51	6					2851	2870		10.1016/j.csda.2006.10.007		20	Computer Science, Interdisciplinary Applications; Statistics & Probability	Computer Science; Mathematics	142DX	WOS:000244630500009		
J	Rezek, I; Roberts, SJ; Conradt, R				Rezek, Iead; Roberts, Stephen J.; Conradt, Regina			Increasing the depth of anesthesia assessment - Better higher-order statistics with generative polyspectral models	IEEE ENGINEERING IN MEDICINE AND BIOLOGY MAGAZINE			English	Article							BISPECTRAL INDEX; REGRESSION; RECOGNITION; NETWORKS; ENTROPY		Univ Oxford, Dept Engn Sci, Oxford OX1 3PJ, England; Univ Oxford, Pattern Anal & Machine Learning Res Grp, Oxford OX1 2JD, England; Univ Oxford, Dept Primary Hlth Care, Oxford OX1 2JD, England	Rezek, I (reprint author), Univ Oxford, Dept Engn Sci, Parks Rd, Oxford OX1 3PJ, England.	irezek@robots.ox.ac.uk					Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Barr G., 1999, BRIT J ANAESTH, V82, P8227; BRETTHORST GL, 1989, BAYESIAN SPECTRUM AN; Godsill SJ, 1998, IEEE T SPEECH AUDI P, V6, P352, DOI 10.1109/89.701365; Hall JD, 1998, BRIT J ANAESTH, V80, P342; HEMMINGS HC, 2005, FDN ANESTHESIA; Holt M, 1998, IEE P-SCI MEAS TECH, V145, P307, DOI 10.1049/ip-smt:19982324; Huber P. J., 2003, ROBUST STAT; Jaakkola T., 2000, ADV MEAN FIELD METHO; Johansen JW, 2000, ANESTHESIOLOGY, V93, P1336, DOI 10.1097/00000542-200011000-00029; KAY SM, 1981, P IEEE, V69, P1380, DOI 10.1109/PROC.1981.12184; KRKIC M, 1996, IEE P AI METH BIOS A; KUIZENGA K, 2003, BRIT J ANAESTH, V86, P354; Linkens DA, 1997, ARTIF INTELL MED, V11, P155, DOI 10.1016/S0933-3657(97)00028-6; MACKAY DJC, 1992, NEURAL COMPUT, V4, P720, DOI 10.1162/neco.1992.4.5.720; MACSKASSY S, 2004, P 1 WORKSH ROC AN AI; MAYNARD DE, 1984, ANAESTHESIA, V39, P678, DOI 10.1111/j.1365-2044.1984.tb06477.x; Miller A., 2004, BRIT J ANAESTH, V93, P596; Morioka N, 1997, ANESTHESIOLOGY, V87, pA502, DOI 10.1097/00000542-199709001-00502; MYLES PS, 2004, LANCET, V363, P9423; MYLES R, 2000, BRIT J ANAESTHESIA, V84; Nabney Ian, 2002, NETLAB ALGORITHMS PA; Ortolani O, 2002, BRIT J ANAESTH, V88, P644, DOI 10.1093/bja/88.5.644; Papoulis A., 1991, PROBABILITY RANDOM V; Pschyrembel W., 2002, KLIN WORTERBUCH; RAGHUVEER MR, 1985, IEEE T ACOUST SPEECH, V33, P1213, DOI 10.1109/TASSP.1985.1164679; Rampil IJ, 1998, ANESTHESIOLOGY, V89, P671, DOI 10.1097/00000542-199809000-00017; Rampil I.J, 1998, ANESTHESIOLOGY, V89, P981; REHMAN HU, 1993, COMPUT METH PROG BIO, V40, P227, DOI 10.1016/0169-2607(93)90009-A; REZEK R, 2005, INT C MACH LEARN APP; Ripley B. D., 2000, PATTERN RECOGNITION; Roberts SJ, 2002, IEEE T SIGNAL PROCES, V50, P2245, DOI 10.1109/TSP.2002.801921; STOELTING RK, 2006, HDB PHARM PHYS ANEST; Stuart A, 1994, DISTRIBUTION THEORY, V1; Swami A., 2000, HIGHER ORDER SPECTRA; Tempe D K., 2004, BRIT J ANAESTH, V29, P1; Thornton C, 1998, BRIT J ANAESTH, V81, P771; TOUGH RJA, 1994, J PHYS A-MATH GEN, V27, P7883, DOI 10.1088/0305-4470/27/23/031; Vanluchene ALG, 2004, ANESTHESIOLOGY, V101, P34, DOI 10.1097/00000542-200407000-00008; Viertio-Oja H, 2004, ACTA ANAESTH SCAND, V48, P154, DOI 10.1111/j.0001-5172.2004.00322.x; Webb AR, 2002, STAT PATTERN RECOGNI; WEST M, 1984, J ROY STAT SOC B MET, V46, P431	42	10	11	0	0	IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC	PISCATAWAY	445 HOES LANE, PISCATAWAY, NJ 08855 USA	0739-5175			IEEE ENG MED BIOL	IEEE Eng. Med. Biol. Mag.	MAR-APR	2007	26	2					64	73		10.1109/MEMB.2007.335582		10	Engineering, Biomedical; Medical Informatics	Engineering; Medical Informatics	151QZ	WOS:000245307000012	17441610	
J	Wu, YJ; Boos, DD; Stefanski, LA				Wu, Yujun; Boos, Dennis D.; Stefanski, Leonard A.			Controlling variable selection by the addition of pseudovariables	JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION			English	Article						false selection rate; forward selection; model error; model selection; subset selection	MODEL SELECTION; REGRESSION; LIKELIHOOD; BOOTSTRAP; CRITERION	We propose a new approach to variable selection designed to control the false selection rate (FSR), defined as the proportion of uninformative variables included in selected models. The method works by adding a known number of pseudovariables to the real dataset, running a variable selection procedure, and monitoring the proportion of pseudovariables falsely selected. Information obtained from bootstrap-like replications of this process is used to estimate the proportion of falsely selected real variables and to tune the selection procedure to control the FSR.	Univ Med & Dent New Jersey, Div Biometr, Dept Biostat, Sch Publ Hlth, Piscataway, NJ 08854 USA; N Carolina State Univ, Dept Stat, Raleigh, NC 27695 USA	Wu, YJ (reprint author), Univ Med & Dent New Jersey, Div Biometr, Dept Biostat, Sch Publ Hlth, Piscataway, NJ 08854 USA.	wuy5@umdnj.edu; boos@stat.ncsu.edu; stefanski@stat.ncsu.edu					AKAIKE H, 1973, BIOMETRIKA, V60, P255, DOI 10.1093/biomet/60.2.255; Luo XH, 2006, TECHNOMETRICS, V48, P165, DOI 10.1198/004017005000000319; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; BREIMAN L, 1992, J AM STAT ASSOC, V87, P738, DOI 10.2307/2290212; Gammaitoni L, 1998, REV MOD PHYS, V70, P223, DOI 10.1103/RevModPhys.70.223; GEORGE EI, 1993, J AM STAT ASSOC, V88, P881, DOI 10.2307/2290777; SHAO J, 1993, J AM STAT ASSOC, V88, P486, DOI 10.2307/2290328; Efron B, 2004, ANN STAT, V32, P407; SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136; MALLOWS CL, 1973, TECHNOMETRICS, V15, P661, DOI 10.2307/1267380; Berg A, 2004, J BUS ECON STAT, V22, P107, DOI 10.1198/073500103288619430; Berger J. O., 2001, I MATH STAT LECT NOT, V38, P135; BERK KN, 1978, TECHNOMETRICS, V20, P1, DOI 10.2307/1268153; BREIMAN L, 1992, INT STAT REV, V60, P291, DOI 10.2307/1403680; Chipman H., 2001, IMS LECT NOTES MONOG, V38, P65; Foster DP, 2004, J AM STAT ASSOC, V99, P303, DOI 10.1198/016214504000000287; HOCKING RR, 1976, BIOMETRICS, V32, P1, DOI 10.2307/2529336; Krishnaiah PR, 1977, APPL STAT, P27; Miller A, 2002, SUBSET SELECTION REG, V2nd; RAO C. R., 2001, IMS LECT NOTES MONOG, V38, P1; Shao J, 1996, J AM STAT ASSOC, V91, P655, DOI 10.2307/2291661; Spiegelhalter DJ, 2002, J ROY STAT SOC B, V64, P583, DOI 10.1111/1467-9868.00353; THOMPSON ML, 1978, INT STAT REV, V46, P1, DOI 10.2307/1402505; Tibshirani R, 1999, J ROY STAT SOC B, V61, P529, DOI 10.1111/1467-9868.00191; WU Y, 2004, CONTROLLING VARIABLE	26	22	22	3	5	AMER STATISTICAL ASSOC	ALEXANDRIA	1429 DUKE ST, ALEXANDRIA, VA 22314 USA	0162-1459			J AM STAT ASSOC	J. Am. Stat. Assoc.	MAR	2007	102	477					235	243		10.1198/016214506000000843		9	Statistics & Probability	Mathematics	138KH	WOS:000244361000022		
J	Micchelli, CA; Pontil, M				Micchelli, Charles A.; Pontil, Massimiliano			Feature space perspectives for learning the kernel	MACHINE LEARNING			English	Article						Banach space regularization; convex optimization; learning the kernels; kernel methods; sparsity	SELECTION	In this paper, we continue our study of learning an optimal kernel in a prescribed convex set of kernels (Micchelli & Pontil, 2005). We present a reformulation of this problem within a feature space environment. This leads us to study regularization in the dual space of all continuous functions on a compact domain with values in a Hilbert space with a mix norm. We also relate this problem in a special case to L(p) regularization.	UCL, Dept Comp Sci, London WC1E, England; SUNY Albany, Dept Math & Stat, Albany, NY 12222 USA	Pontil, M (reprint author), UCL, Dept Comp Sci, Gower St, London WC1E, England.	m.pontil@cs.ucl.ac.uk					Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Micchelli CA, 2005, J MACH LEARN RES, V6, P1099; Micchelli CA, 2005, NEURAL COMPUT, V17, P177, DOI 10.1162/0899766052530802; Lanckriet GRG, 2004, J MACH LEARN RES, V5, P27; ARONSZAJN N, 1950, T AM MATH SOC, V68, P337; ARGYRIOU A, 2005, P 18 ANN C LEARN THE; BACH FR, 2004, P INT C MACH LEARN I; Bousquet O., 2003, ADV NEURAL INFORM PR, V15; Chen Y, 1998, NONCON OPTIM ITS APP, V20, P1; Elissedff A., 2002, ADV NEURAL INFORM PR, V14; Fung GM, 2004, COMPUT OPTIM APPL, V28, P185, DOI 10.1023/B:COAP.0000026884.66338.df; Gunn SR, 2002, MACH LEARN, V48, P137, DOI 10.1023/A:1013903804720; HERBSTER M, 2004, P 15 INT C ALG LEARN; LEE Y, 2004, 743 OH STAT U DEP ST; Lewis A., 2000, CONVEX ANAL NONLINEA; LIN Y, 2003, I STAT MIMEO SERIES, V2556; MICCHELLI CA, 2004, P 17 ANN C LEARN THE; MICCHELLI CA, 1994, RENDICONTI MATEMAT 7, V14, P37; MICCHELLI CA, 2005, 0509 U COLL LOND DEP; MICCHELLI CA, 1992, ESAIM-MATH MODEL NUM, V26, P77; Ong C. S., 2003, ADV NEURAL INFORM PR, V15; ROYDEN HL, 1964, REAL ANAL; Wahba G., 1990, SERIES APPL MATH, V59; WU Q, IN PRESS J COMPLEXIT	24	16	16	0	0	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0885-6125			MACH LEARN	Mach. Learn.	MAR	2007	66	2-3					297	319		10.1007/s10994-006-0679-0		23	Computer Science, Artificial Intelligence	Computer Science	142ZQ	WOS:000244690200009		
J	Jones, IM; Thomas, CB; Xi, T; Mohrenweiser, HW; Nelson, DO				Jones, Irene M.; Thomas, Cynthia B.; Xi, Tina; Mohrenweiser, Harvey W.; Nelson, David O.			Exploration of methods to identify polymorphisms associated with variation in DNA repair capacity phenotypes	MUTATION RESEARCH-FUNDAMENTAL AND MOLECULAR MECHANISMS OF MUTAGENESIS			English	Article						polymorphism; DNA repair; phenotype; genotype	ACID SUBSTITUTION VARIANTS; GENETIC-VARIATION; RISK; CANCER; HUMANS; DAMAGE; SUSCEPTIBILITY; HERITABILITY; REGRESSION	Elucidating the relationship between polymorphic sequences and risk of common disease is a challenge. For example, although it is clear that variation in DNA repair genes is associated with familial cancer, aging and neurological disease, progress toward identifying polymorphisms associated with elevated risk of sporadic disease has been slow. This is partly due to the complexity of the genetic variation, the existence of large numbers of mostly low frequency variants and the contribution of many genes to variation in susceptibility. There has been limited development of methods to find associations between genotypes having many polymorphisms and pathway function or health outcome. We have explored several statistical methods for identifying polymorphisms associated with variation in DNA repair phenotypes. The model system used was 80 cell lines that had been resequenced to identify variation; 191 single nucleotide substitution polymorphisms (SNPs) are included, of which 172 are in 31 base excision repair pathway genes, 19 in 5 anti-oxidation genes, and DNA repair phenotypes based on single strand breaks measured by the alkaline Comet assay. Univariate analyses were of limited value in identifying SNPs associated with phenotype variation. Of the multivariable model selection methods tested: the easiest that provided reduced error of prediction of phenotype was simple counting of the variant alleles predicted to encode proteins with reduced activity, which led to a genotype including 52 SNPs; the best and most parsimonious model was achieved using a two-step analysis without regard to potential functional relevance: first SNPs were ranked by importance determined by random forests regression (RFR), followed by cross-validation in a second round of RFR modeling that included ever more SNPs in declining order of importance. With this approach six SNPs were found to minimize prediction error. The results should encourage research into utilization of multivariate analytical methods for epidemiological studies of the association of genetic variation in complex genotypes with risk of common diseases. (c) 2006 Elsevier B.V. All rights reserved.	Lawrence Livermore Natl Lab, Livermore, CA 94550 USA; Oregon Hlth Sci Univ, Portland, OR 97201 USA	Jones, IM (reprint author), Lawrence Livermore Natl Lab, Livermore, CA 94550 USA.	jones20@llnl.gov					Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324; Goode EL, 2002, CANCER EPIDEM BIOMAR, V11, P1513; Ng PC, 2003, NUCLEIC ACIDS RES, V31, P3812, DOI 10.1093/nar/gkg509; Collins FS, 1998, GENOME RES, V8, P1229; Shen MR, 1998, CANCER RES, V58, P604; BENJAMINI Y, 1995, J ROY STAT SOC B MET, V57, P289; Efron B, 2004, ANN STAT, V32, P407; Hirschhorn JN, 2002, GENET MED, V4, P45, DOI 10.1097/00125817-200203000-00002; Comings DE, 2003, CANCER, V97, P2160, DOI 10.1002/cncr.11340; Berwick M, 2000, J NATL CANCER I, V92, P874, DOI 10.1093/jnci/92.11.874; BERWICK M, 2002, BIOMARKERS ENV ASS D, P84; Bureau A, 2005, GENET EPIDEMIOL, V28, P171, DOI 10.1002/gepi.20041; Cloos J, 1999, J NATL CANCER I, V91, P1125, DOI 10.1093/jnci/91.13.1125; FRIEDMAN JH, 2003, RECENT ADV PREDICTIV; KENT CRH, 1995, INT J RADIAT BIOL, V67, P655, DOI 10.1080/09553009514550771; Lindahl T, 1997, CURR OPIN GENET DEV, V7, P158, DOI 10.1016/S0959-437X(97)80124-4; Millikan RC, 2006, CARCINOGENESIS, V27, P610, DOI 10.1093/carcin/bgi252; Mohrenweiser HW, 2002, CANCER EPIDEM BIOMAR, V11, P1054; Mohrenweiser HW, 2003, MUTAT RES-FUND MOL M, V526, P93, DOI 10.1016/S0027-5107(03)00049-6; Roberts SA, 1999, AM J HUM GENET, V65, P784, DOI 10.1086/302544; SINGH NP, 1988, EXP CELL RES, V175, P184, DOI 10.1016/0014-4827(88)90265-0; Sokhansanj BA, 2006, CANCER EPIDEM BIOMAR, V15, P1000, DOI 10.1158/1055-9965.EPI-05-0817; Wilson DM, 1997, P NATL ACAD SCI USA, V94, P12754, DOI 10.1073/pnas.94.24.12754; Wu XF, 2006, CANCER RES, V66, P5993, DOI 10.1158/0008-5472.CAN-06-1007; XI T, 2004, GENOMICS, V83, P973	26	10	10	0	1	ELSEVIER SCIENCE BV	AMSTERDAM	PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS	0027-5107			MUTAT RES-FUND MOL M	Mutat. Res.-Fundam. Mol. Mech. Mutagen.	MAR 1	2007	616	1-2					213	220		10.1016/j.mrfmmm.2006.11.005		8	Biotechnology & Applied Microbiology; Genetics & Heredity; Toxicology	Biotechnology & Applied Microbiology; Genetics & Heredity; Toxicology	143YC	WOS:000244761300025	17145065	
J	Welsch, RE; Zhou, XF				Welsch, Roy E.; Zhou, Xinfeng			APPLICATION OF ROBUST STATISTICS TO ASSET ALLOCATION MODELS	REVSTAT-STATISTICAL JOURNAL			English	Article						robust statistics; asset allocation; FAST-MCD; bivariate Winsorization; penalization		Many strategies for asset allocation involve the computation of the expected value and the covariance matrix of the returns of financial instruments. How much of each instrument to own is determined by an attempt to minimize risk - the variance of linear combinations of investments in these financial assets - subject to various constraints such as a given level of return, concentration limits, etc. The covariance matrix contains many parameters to estimate and two main problems arise. First, the data will very likely have outliers that will seriously affect the covariance matrix. Second, with so many parameters to estimate, a large number of return observations are required and the nature of markets may change substantially over such a long period. In this paper we discuss using robust covariance procedures, FAST-MCD, Iterated Bivariate Winsorization and Fast 2-D Winsorization, to address the first problem and penalization methods for the second. When back-tested on market data, these methods are shown to be effective in improving portfolio performance. Robust asset allocation methods have great potential to improve risk-adjusted portfolio returns and therefore deserve further exploration in investment management research.	[Welsch, Roy E.] MIT, Alfred P Sloan Sch Management, Cambridge, MA 02139 USA; [Zhou, Xinfeng] Barclays Global Investors, Global Index & Markets Grp, San Francisco, CA USA	Welsch, RE (reprint author), MIT, Alfred P Sloan Sch Management, Cambridge, MA 02139 USA.	rwelsch@mit.edu; xinfeng.zhou@barclaysglobal.com			Singapore-MIT Alliance; MIT Center for Computational Research in Economics and Management Science	This research was supported, in part, by the Singapore-MIT Alliance and the MIT Center for Computational Research in Economics and Management Science.	ALQALLAF FA, 2002, P 7 ACM SIGKDD INT C; ALQALLAF FA, 2005, MODEL CONTAMINATION; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; MARONNA RA, 1976, ANN STAT, V4, P51, DOI 10.1214/aos/1176343347; Ledoit O, 2004, J MULTIVARIATE ANAL, V88, P365, DOI 10.1016/S0047-259X(03)00096-4; Tse YK, 2000, J ECONOMETRICS, V98, P107, DOI 10.1016/S0304-4076(99)00080-9; MERTON RC, 1972, J FINANC QUANT ANAL, V7, P1851, DOI 10.2307/2329621; Efron B, 2004, ANN STAT, V32, P407; Rousseeuw PJ, 1999, TECHNOMETRICS, V41, P212, DOI 10.2307/1270566; BALVERS RJ, 1990, J FINANC, V45, P1109, DOI 10.2307/2328717; BRUNELLI R, 1995, PATTERN RECOGN, V28, P833, DOI 10.1016/0031-3203(94)00170-Q; CHILSON J, 2004, P ACM SIGKDD, P533, DOI 10.1145/1014052.1014115; DEMELO BV, 2003, COPPEAD WORKING PAPE, V355; DeMiguel V., 2006, PORTFOLIO SELECTION; DICKENSO.JP, 1974, STATISTICIAN, V23, P5, DOI 10.2307/2987261; Dueck A, 2005, BIOMETRICS, V61, P162, DOI 10.1111/j.0006-341X.2005.030151.x; JORION P, 1985, J BUS, V58, P259, DOI 10.1086/296296; KHAN JA, 2002, ROBUST LINEAR MODEL; KLEIN RW, 1976, J FINANC ECON, V3, P215, DOI 10.1016/0304-405X(76)90004-0; Knez PJ, 1997, J FINANC, V52, P1355, DOI 10.1111/j.1540-6261.1997.tb01113.x; LAUPRETE GJ, 2001, THESIS MIT; Lauprete GJ, 2002, METRIKA, V55, P139, DOI 10.1007/s001840200193; Markowitz H, 1952, J FINANC, V7, P77, DOI 10.2307/2975974; Markowitz H., 1959, PORTFOLIO SELECTION; Maronna R. A., 2006, ROBUST STAT THEORY M; Maronna RA, 2002, TECHNOMETRICS, V44, P307, DOI 10.1198/004017002188618509; Martin RD, 2003, FINANC ANAL J, V59, P56, DOI 10.2469/faj.v59.n5.2564; Michaud R. O., 1989, FINANCIAL ANAL J, V45, P31, DOI DOI 10.2469/FAJ.V45.N1.31; Scherer B., 2005, INTRO MODERN PORTFOL; Tse YK, 2002, J BUS ECON STAT, V20, P351, DOI 10.1198/073500102288618496; ZHOU X, 2006, THESIS MIT	31	15	16	1	2	INST NACIONAL ESTATISTICA-INE	LISBON	AV ANTONIO JOSE ALMEIDA, 2, LISBON, 1000-043, PORTUGAL	1645-6726			REVSTAT-STAT J	REVSTAT-Stat. J.	MAR	2007	5	1			SI		97	114				18	Statistics & Probability	Mathematics	V10UQ	WOS:000207489300006		
J	Liao, J; Warmuth, MK; Govindarajan, S; Ness, JE; Wang, RP; Gustafsson, C; Minshull, J				Liao, Jun; Warmuth, Manfred K.; Govindarajan, Sridhar; Ness, Jon E.; Wang, Rebecca P.; Gustafsson, Claes; Minshull, Jeremy			Engineering proteinase K using machine learning and synthetic genes	BMC BIOTECHNOLOGY			English	Article							LOCAL FITNESS LANDSCAPE; SUPPORT VECTOR MACHINES; DIRECTED EVOLUTION; MULTIVARIATE APPROACH; DNA-SYNTHESIS; AMINO-ACIDS; DESIGN; SELECTION; SEQUENCE; PEPTIDE	Background: Altering a protein's function by changing its sequence allows natural proteins to be converted into useful molecular tools. Current protein engineering methods are limited by a lack of high throughput physical or computational tests that can accurately predict protein activity under conditions relevant to its final application. Here we describe a new synthetic biology approach to protein engineering that avoids these limitations by combining high throughput gene synthesis with machine learning-based design algorithms. Results: We selected 24 amino acid substitutions to make in proteinase K from alignments of homologous sequences. We then designed and synthesized 59 specific proteinase K variants containing different combinations of the selected substitutions. The 59 variants were tested for their ability to hydrolyze a tetrapeptide substrate after the enzyme was first heated to 68 C for 5 minutes. Sequence and activity data was analyzed using machine learning algorithms. This analysis was used to design a new set of variants predicted to have increased activity over the training set, that were then synthesized and tested. By performing two cycles of machine learning analysis and variant design we obtained 20-fold improved proteinase K variants while only testing a total of 95 variant enzymes. Conclusion: The number of protein variants that must be tested to obtain significant functional improvements determines the type of tests that can be performed. Protein engineers wishing to modify the property of a protein to shrink tumours or catalyze chemical reactions under industrial conditions have until now been forced to accept high throughput surrogate screens to measure protein properties that they hope will correlate with the functionalities that they intend to modify. By reducing the number of variants that must be tested to fewer than 100, machine learning algorithms make it possible to use more complex and expensive tests so that only protein properties that are directly relevant to the desired application need to be measured. Protein design algorithms that only require the testing of a small number of variants represent a significant step towards a generic, resource-optimized protein engineering process.	DNA 20, Menlo Pk, CA 94025 USA; Univ Calif Santa Cruz, Dept Comp Sci, Santa Cruz, CA 95064 USA	Minshull, J (reprint author), DNA 20, 1430 O Brien Dr,Suite E, Menlo Pk, CA 94025 USA.	liaojun@soe.ucsc.edu; manfred@cse.ucsc.edu; sgovindarajan@dna20.com; sgovindarajan@dna20.com; rwang@dna20.com; cgustafsson@dna20.com; jminshull@dna20.com		Gustafsson, Claes/0000-0003-2181-3126			Aita T, 2002, BIOPOLYMERS, V64, P95, DOI 10.1002/bip.10126; Aita T, 2001, PROTEIN ENG, V14, P633, DOI 10.1093/protein/14.9.633; Almog O, 2002, J BIOL CHEM, V277, P27553, DOI 10.1074/jbc.M111777200; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Gustafsson C, 2004, TRENDS BIOTECHNOL, V22, P346, DOI 10.1016/j.tibtech.2004.04.006; Korkegian A, 2005, SCIENCE, V308, P857, DOI 10.1126/science.1107387; HOERL AE, 1970, TECHNOMETRICS, V12, P55; Strom MB, 2002, BIOCHEM CELL BIOL, V80, P65, DOI 10.1139/o01-236; Venter JC, 2004, SCIENCE, V304, P66, DOI 10.1126/science.1093857; THOMPSON JD, 1994, NUCLEIC ACIDS RES, V22, P4673, DOI 10.1093/nar/22.22.4673; Aita T, 2000, BIOPOLYMERS, V54, P64, DOI 10.1002/(SICI)1097-0282(200007)54:1<64::AID-BIP70>3.0.CO;2-R; Atkinson A. C., 1992, OXFORD STAT SCI SERI; Bryan PN, 2000, BBA-PROTEIN STRUCT M, V1543, P203, DOI 10.1016/S0167-4838(00)00235-1; Bucht G, 1999, BBA-PROTEIN STRUCT M, V1431, P471, DOI 10.1016/S0167-4838(99)00079-5; CASARI G, 1995, NAT STRUCT BIOL, V2, P171, DOI 10.1038/nsb0295-171; Cello J, 2002, SCIENCE, V297, P1016, DOI 10.1126/science.1072266; CHEN KQ, 1991, BIO-TECHNOL, V9, P1073, DOI 10.1038/nbt1191-1073; CICCARELLI RB, 1991, NUCLEIC ACIDS RES, V19, P6007, DOI 10.1093/nar/19.21.6007; Crameri A, 1998, NATURE, V391, P288; Dayhoff MO, 1968, ATLAS PROTEIN SEQUEN, P33; DELMAR EG, 1980, BIOCHEMISTRY-US, V19, P468, DOI 10.1021/bi00544a011; Demiriz A, 2002, MACH LEARN, V46, P225, DOI 10.1023/A:1012470815092; Drucker Harris, 1997, NEURAL INFORM PROCES, P155; Dwyer MA, 2004, SCIENCE, V304, P1967, DOI 10.1126/science.1098432; ERIKSSON L, 1990, ACTA CHEM SCAND, V44, P50, DOI 10.3891/acta.chem.scand.44-0050; Fang JW, 2006, J BIOMOL SCREEN, V11, P138, DOI 10.1177/1087057105284334; Goldberg SMD, 2006, P NATL ACAD SCI USA, V103, P11240, DOI 10.1073/pnas.0604351103; Govindarajan S, 2003, J MOL BIOL, V328, P1061, DOI 10.1016/S0022-2836(03)00357-7; GUNKEL FA, 1989, EUR J BIOCHEM, V179, P185, DOI 10.1111/j.1432-1033.1989.tb14539.x; HELLBERG S, 1987, J MED CHEM, V30, P1126, DOI 10.1021/jm00390a003; HELLBERG S, 1986, ACTA CHEM SCAND B, V40, P1335; HELMBOLD DP, 1995, ADV NEURAL INFORM PR, V8, P309; Henaut A., 1996, ESCHERICHIA COLI SAL, V2, P2047; JONSSON J, 1993, NUCLEIC ACIDS RES, V21, P733, DOI 10.1093/nar/21.3.733; KASAFIREK E, 1976, EUR J BIOCHEM, V69, P1, DOI 10.1111/j.1432-1033.1976.tb10852.x; Kodumal SJ, 2004, P NATL ACAD SCI USA, V101, P15573, DOI 10.1073/pnas.0406911101; Kretz KA, 2004, METHOD ENZYMOL, V388, P3; Lam Raymond L H, 2004, Methods Mol Biol, V275, P301; Lehmann M, 2000, PROTEIN ENG, V13, P49, DOI 10.1093/protein/13.1.49; LIAO J, 2005, THESIS U SATA CRUZ; Mee RP, 1997, J PEPT RES, V49, P89; Minshull J, 2005, CURR OPIN CHEM BIOL, V9, P202, DOI 10.1016/j.cbpa.2005.02.003; Mitra P, 2004, IEEE T PATTERN ANAL, V26, P413, DOI 10.1109/TPAMI.2004.1262340; NAMBIAR KP, 1984, SCIENCE, V223, P1299, DOI 10.1126/science.6322300; Ness JE, 2002, NAT BIOTECHNOL, V20, P1251, DOI 10.1038/nbt754; Ness JE, 2005, ACS SYM SER, V900, P37; Norinder U, 1997, J PEPT RES, V49, P155; Roberts RW, 1999, CURR OPIN CHEM BIOL, V3, P268, DOI 10.1016/S1367-5931(99)80042-8; Ryu DDY, 2000, BIOTECHNOL PROGR, V16, P2, DOI 10.1021/bp088059d; SANDBERG M, 1997, DEPT ORGANIC CHEM UM; Santos CF, 2002, CAN J PHYSIOL PHARM, V80, P42, DOI 10.1139/Y02-004; Schoch GA, 2003, EUR J BIOCHEM, V270, P3684, DOI 10.1046/j.1432-1033.2003.03739.x; Smola A. J., 1998, NEURAL COMPUTATIONAL; STEMMER WPC, 1994, P NATL ACAD SCI USA, V91, P10747, DOI 10.1073/pnas.91.22.10747; TAGUCHI G, 2004, TAGUCHIS QUALITY RNG; Taguchi Genichi, 1986, INTRO QUALITY ENG; Tian JD, 2004, NATURE, V432, P1050, DOI 10.1038/nature03151; Tobin MB, 2000, CURR OPIN STRUC BIOL, V10, P421, DOI 10.1016/S0959-440X(00)00109-3; Van Regenmortel MHV, 2000, J MOL RECOGNIT, V13, P1; Warmuth MK, 2003, J CHEM INF COMP SCI, V43, P667, DOI 10.1021/ci025620t; Wold H., 1966, MULTIVARIATE ANAL, P391; Xiong AS, 2004, NUCLEIC ACIDS RES, V32, DOI 10.1093/nar/gnh094; Young L, 2004, NUCLEIC ACIDS RES, V32, DOI 10.1093/nar/gnh058	63	25	28	4	8	BIOMED CENTRAL LTD	LONDON	MIDDLESEX HOUSE, 34-42 CLEVELAND ST, LONDON W1T 4LB, ENGLAND	1472-6750			BMC BIOTECHNOL	BMC Biotechnol.	MAR 26	2007	7								16	10.1186/1472-6750-7-16		19	Biotechnology & Applied Microbiology	Biotechnology & Applied Microbiology	154BK	WOS:000245481600001	17386103	
J	Huo, XM; Ni, XL				Huo, Xiaoming; Ni, Xuelei			When do stepwise algorithms meet subset selection criteria?	ANNALS OF STATISTICS			English	Article						subset selection; model selection; stepwise algorithms; convex optimization; concurrent optimal subset	NONCONCAVE PENALIZED LIKELIHOOD; LEAST ANGLE REGRESSION; VARIABLE SELECTION; SPARSE REPRESENTATIONS; BASES; SHRINKAGE; BRANCH; LASSO; NOISE	Recent results in homotopy and solution paths demonstrate that certain well-designed greedy algorithms, with a range of values of the algorithmic parameter, can provide solution paths to a sequence of convex optimization problems. On the other hand, in regression many existing criteria in subset selection (including C-p, AIC, BIC, MDL, RIC, etc.) involve optimizing an objective function that contains a counting measure. The two optimization problems are formulated as (P1) and (P0) in the present paper. The latter is generally combinatoric and has been proven to be NP-hard. We study the conditions under which the two optimization problems have common solutions. Hence, in these situations a stepwise algorithm can be used to solve the seemingly unsolvable problem. Our main result is motivated by recent work in sparse representation, while two others emerge from different angles: a direct analysis of sufficiency and necessity and a condition on the mostly correlated covariates. An extreme example connected with least angle regression is of independent interest.	Georgia Inst Technol, Atlanta, GA 30332 USA	Huo, XM (reprint author), Georgia Inst Technol, Atlanta, GA 30332 USA.	xiaoming@isye.gatech.edu; xni@isye.gatech.edu					Akaike H., 1973, 2 INT S INF THEOR, P267; Tropp JA, 2006, IEEE T INFORM THEORY, V52, P1030, DOI 10.1109/TIT.2005.864420; Donoho DL, 2001, IEEE T INFORM THEORY, V47, P2845, DOI 10.1109/18.959265; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; FURNIVAL GM, 1974, TECHNOMETRICS, V16, P499, DOI 10.2307/1267601; Jain A, 1997, IEEE T PATTERN ANAL, V19, P153, DOI 10.1109/34.574797; Gribonval R, 2003, IEEE T INFORM THEORY, V49, P3320, DOI 10.1109/TIT.2003.820031; Chen SSB, 2001, SIAM REV, V43, P129, DOI 10.1137/S003614450037906X; Osborne MR, 2000, IMA J NUMER ANAL, V20, P389, DOI 10.1093/imanum/20.3.389; George EI, 2000, J AM STAT ASSOC, V95, P1304, DOI 10.2307/2669776; Tropp JA, 2004, IEEE T INFORM THEORY, V50, P2231, DOI 10.1109/TIT.2004.834793; Efron B, 2004, ANN STAT, V32, P407; SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136; MALLOWS CL, 1973, TECHNOMETRICS, V15, P661, DOI 10.2307/1267380; Elad M, 2002, IEEE T INFORM THEORY, V48, P2558, DOI 10.1109/TIT.2002.801410; Kohavi R, 1997, ARTIF INTELL, V97, P273, DOI 10.1016/S0004-3702(97)00043-X; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P6, DOI 10.1109/TIT.2005.860430; BURNHAM KP, 2002, PRACTICAL INFORM THE; Chen S.S., 1995, THESIS STANFORD U; DeVore RA, 1996, ADV COMPUT MATH, V5, P173, DOI 10.1007/BF02124742; Donoho DL, 2003, P NATL ACAD SCI USA, V100, P2197, DOI 10.1073/pnas.0437847100; Donoho DL, 1995, J AM STAT ASSOC, V90, P1200, DOI 10.2307/2291512; Fan J., 2006, INT C MATH, VIII, P595; Fan JQ, 2004, ANN STAT, V32, P928, DOI 10.1214/009053604000000256; FOSTER DP, 1994, ANN STAT, V22, P1947, DOI 10.1214/aos/1176325766; Fuchs JJ, 2004, IEEE T INFORM THEORY, V50, P1341, DOI 10.1109/TIT.2004.828141; Gatu C, 2006, J COMPUT GRAPH STAT, V15, P139, DOI 10.1198/106186006X100290; Gilmour SG, 1996, STATISTICIAN, V45, P49, DOI 10.2307/2348411; GOLUB G. H., 1996, MATRIX COMPUTATIONS; GRIBONVAL R, 2005, P IEEE C AC SPEECH S, V5, P717, DOI 10.1109/ICASSP.2005.1416404; Hastie T, 2004, J MACH LEARN RES, V5, P1391; Hastie T., 2001, ELEMENTS STAT LEARNI; HUO X, 2005, STEPWISE ALGORITHMS; Malioutov D., 2005, P IEEE INT C AC SPEE, V5, P733, DOI 10.1109/ICASSP.2005.1416408; Miller A., 2002, SUBSET SELECTION REG; NARENDRA P, 1977, IEEE T COMPUT, V26, P917; NATARAJAN BK, 1995, SIAM J COMPUT, V24, P227, DOI 10.1137/S0097539792240406; NI XS, 2005, INFORM QSR STUDENT P; NI XS, 2006, REGRESSIONS ENHANCED; NI XS, 2006, THESIS GEORGIA I TEC; Osborne MR, 2000, J COMPUT GRAPH STAT, V9, P319, DOI 10.2307/1390657; OSBORNE MR, 1985, FINITE ALGORITHMS OP, DOI UNSP MR0817717; RIDOUT MS, 1988, APPL STAT-J ROY ST C, V37, P139, DOI 10.2307/2347512; ROBERTS SJ, 1984, APPL STAT, V33, P236, DOI 10.2307/2347457; Rockaffellar R. T., 1970, CONVEX ANAL; SANTOSA F, 1986, SIAM J SCI STAT COMP, V7, P1307, DOI 10.1137/0907087; Turlach BA, 2004, ANN STAT, V32, P481; WU CFJ, 1993, BIOMETRIKA, V80, P661, DOI 10.1093/biomet/80.3.661; ZOU H, 2005, UNPUB MANUSCRIPT	50	5	5	0	1	INST MATHEMATICAL STATISTICS	BEACHWOOD	PO BOX 22718, BEACHWOOD, OH 44122 USA	0090-5364			ANN STAT	Ann. Stat.	APR	2007	35	2					870	887		10.1214/009053606000001334		18	Statistics & Probability	Mathematics	203PX	WOS:000248987600017		
J	Park, MY; Hastie, T; Tibshirani, R				Park, Mee Young; Hastie, Trevor; Tibshirani, Robert			Averaged gene expressions for regression	BIOSTATISTICS			English	Article						averaging; hierarchical clustering; Lasso; variance reduction	VARIABLE SELECTION; ONTOLOGY; LASSO	Although averaging is a simple technique, it plays an important role in reducing variance. We use this essential property of averaging in regression of the DNA microarray data, which poses the challenge of having far more features than samples. In this paper, we introduce a two-step procedure that combines (1) hierarchical clustering and (2) Lasso. By averaging the genes within the clusters obtained from hierarchical clustering, we define supergenes and use them to fit regression models, thereby attaining concise interpretation and accuracy. Our methods are supported with theoretical justifications and demonstrated on simulated and real data sets.	Google Inc, Mountain View, CA 94043 USA; Stanford Univ, Dept Stat, Stanford, CA 94305 USA; Stanford Univ, Dept Hlth Res & Policy, Stanford, CA 94305 USA	Park, MY (reprint author), Google Inc, 1600 Amphitheatre Pkwy, Mountain View, CA 94043 USA.	meeyoung@google.com					Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; HOERL AE, 1970, TECHNOMETRICS, V12, P55; Ashburner M, 2000, NAT GENET, V25, P25; Zou H, 2005, J ROY STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x; Eisen MB, 1998, P NATL ACAD SCI USA, V95, P14863, DOI 10.1073/pnas.95.25.14863; Efron B, 2004, ANN STAT, V32, P407; Boyle EI, 2004, BIOINFORMATICS, V20, P3710, DOI 10.1093/bioinformatics/bth456; BAIR E, 2004, J AM STAT ASSOC, V101, P119; *GO CONS, 2004, NUCLEIC ACIDS RES, V32, P235; Hastie T, 2001, GENOME BIOL, V2; Hastie T., 2001, ELEMENTS STAT LEARNI; Park M., 2006, L1 REGULARIZATION PA; Tibshirani R, 1997, STAT MED, V16, P385, DOI 10.1002/(SICI)1097-0258(19970228)16:4<385::AID-SIM380>3.0.CO;2-3; van't Veer LJ, 2002, NATURE, V415, P530, DOI 10.1038/415530a; VERWEIJ PJM, 1993, STAT MED, V12, P2305, DOI 10.1002/sim.4780122407; YU X, 2005, THESIS STANFORD U ST, P23	16	53	55	1	7	OXFORD UNIV PRESS	OXFORD	GREAT CLARENDON ST, OXFORD OX2 6DP, ENGLAND	1465-4644			BIOSTATISTICS	Biostatistics	APR	2007	8	2					212	227		10.1093/biostatistics/kxl002		16	Mathematical & Computational Biology; Statistics & Probability	Mathematical & Computational Biology; Mathematics	154MP	WOS:000245512000004	16698769	
J	Wang, P; Tang, H; Fitzgibbon, MP; McIntosh, M; Coram, M; Zhang, H; Yi, E; Aebersold, R				Wang, Pei; Tang, Hua; Fitzgibbon, Matthew P.; McIntosh, Martin; Coram, Marc; Zhang, Hui; Yi, Eugene; Aebersold, Ruedi			A statistical method for chromatographic alignment of LC-MS data	BIOSTATISTICS			English	Article						alignment; LC-MS; regression; retention time	MASS-SPECTROMETRY; QUANTIFICATION; REGRESSION; PROTEOMICS; SELECTION; PROTEINS	Integrated liquid-chromatography mass-spectrometry (LC-MS) is becoming a widely used approach for quantifying the protein composition of complex samples. The output of the LC-MS system measures the intensity of a peptide with a specific mass-charge ratio and retention time. In the last few years, this technology has been used to compare complex biological samples across multiple conditions. One challenge for comparative proteomic profiling with LC-MS is to match corresponding peptide features from different experiments. In this paper, we propose a new method-Peptide Element Alignment (PETAL) that uses raw spectrum data and detected peak to simultaneously align features from multiple LC-MS experiments. PETAL creates spectrum elements, each of which represents the mass spectrum of a single peptide in a single scan. Peptides detected in different LC-MS data are aligned if they can be represented by the same elements. By considering each peptide separately, PETAL enjoys greater flexibility than time warping methods. While most existing methods process multiple data sets by sequentially aligning each data set to an arbitrarily chosen template data set, PETAL treats all experiments symmetrically and can analyze all experiments simultaneously. We illustrate the performance of PETAL on example data sets.	Univ Chicago, Dept Stat, Chicago, IL 60637 USA; Fred Hutchinson Canc Res Ctr, Seattle, WA 98104 USA; Inst Syst Biol, Seattle, WA USA	Wang, P (reprint author), Univ Chicago, Dept Stat, Chicago, IL 60637 USA.	pwang@fhcrc.org					Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Aebersold R, 2003, NATURE, V422, P198, DOI 10.1038/nature01511; Zhang H, 2003, NAT BIOTECHNOL, V21, P660, DOI 10.1038/nbt827; Zou H, 2005, J ROY STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x; Nielsen NPV, 1998, J CHROMATOGR A, V805, P17, DOI 10.1016/S0021-9673(98)00021-1; Wang WX, 2003, ANAL CHEM, V75, P4818, DOI 10.1021/ac026468x; Tibshirani R, 2004, BIOINFORMATICS, V20, P3034, DOI 10.1093/bioinformatics/bth357; Efron B, 2004, ANN STAT, V32, P407; BELLEW M, 2006, BIOINFORMATICS  0728; Bylund D, 2002, J CHROMATOGR A, V961, P237, DOI 10.1016/S0021-9673(02)00588-5; Fang RH, 2006, MOL CELL PROTEOMICS, V5, P714, DOI 10.1074/mcp.M500301-MCP200; Katajamaa M, 2005, BMC BIOINFORMATICS, V6, DOI 10.1186/1471-2105-6-179; Li XJ, 2005, MOL CELL PROTEOMICS, V4, P1328, DOI 10.1074/mcp.M500141-MCP200; Liebler D., 2002, INTRO PROTEOMICS; Listgarten J., 2005, ADV NEURAL INFORM PR; Prakash A, 2006, MOL CELL PROTEOMICS, V5, P423, DOI 10.1074/mcp.M500133-MCP200; Radulovic D, 2004, MOL CELL PROTEOMICS, V3, P984, DOI 10.1074/mcp.M400061-MCP200; RANDOLPH TW, 2004, BIOMETRICS, V62, P589; Zhang H, 2005, MOL CELL PROTEOMICS, V4, P144, DOI 10.1074/mcp.M400090-MCP200	19	39	40	0	2	OXFORD UNIV PRESS	OXFORD	GREAT CLARENDON ST, OXFORD OX2 6DP, ENGLAND	1465-4644			BIOSTATISTICS	Biostatistics	APR	2007	8	2					357	367		10.1093/biostatistics/kxl015		11	Mathematical & Computational Biology; Statistics & Probability	Mathematical & Computational Biology; Mathematics	154MP	WOS:000245512000015	16880200	
J	Xu, SZ; Jia, ZY				Xu, Shizhong; Jia, Zhenyu			Genomewide analysis of epistatic effects for quantitative traits in barley	GENETICS			English	Article							GENETIC VARIANCE; LOCI; MODEL; SELECTION; QTL; SHRINKAGE; EVOLUTION; MICE	The doubled-haploid (DH) barley population (Harrington x TR306) developed by the North American Barley Genome Mapping Project (NABGMP) for QTL mapping consisted of 145 lines and 127 markers covering a total genome length of 1270 cM. These DH lines were evaluated in similar to 25 environments for seven quantitative traits: heading, height, kernel weight, lodging, maturity, test weight, and yield. We applied an empirical Bayes method that simultaneously estimates 127 main effects for all markers and 127(127 - 1)/2 = 8001 interaction effects for all marker pairs in a single model. We found that the largest main-effect QTL (single marker) and the largest epistatic effect (single pair of markers) explained similar to 18 and 2.6% of the phenotypic variance, respectively. On average, the sum of all significant main effects and the sum of all significant epistatic effects contributed 35 and 6% of the total phenotypic variance, respectively. Epistasis seems to be negligible for all the seven traits. We also found that whether two loci interact does not depend on whether or not the loci have individual main effects. This invalidates the common practice of epistatic analysis in which epistatic effects are estimated only for pairs of loci of which both have main effects.	Univ Calif Riverside, Dept Bot & Plant Sci, Riverside, CA 92521 USA	Xu, SZ (reprint author), Univ Calif Riverside, Dept Bot & Plant Sci, 900 Univ Ave, Riverside, CA 92521 USA.	xu@genetics.ucr.edu					LINDLEY DV, 1972, J ROY STAT SOC B, V34, P1; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; HARTLEY HO, 1967, BIOMETRIKA, V54, P93, DOI 10.2307/2333854; Wright S, 1931, GENETICS, V16, P0097; Goodnight CJ, 2000, HEREDITY, V84, P587, DOI 10.1046/j.1365-2540.2000.00698.x; CHEVERUD JM, 1995, GENETICS, V139, P1455; GOODNIGHT CJ, 1988, EVOLUTION, V42, P441, DOI 10.2307/2409030; LANDER ES, 1989, GENETICS, V121, P185; Carlborg O, 2000, GENETICS, V155, P2003; Melchinger AE, 1998, GENETICS, V149, P383; SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136; Xu SZ, 2003, GENETICS, V165, P2259; Beavis W.D., 1994, P 49 ANN CORN SORGH, P250; Carlborg O, 2005, MAMM GENOME, V16, P481, DOI 10.1007/s00335-004-2425-4; Carlin BP, 1996, BAYES EMPIRICAL BAYE; COCKERHAM CC, 1954, GENETICS, V39, P859; Fisher R. A., 1919, Transactions of the Royal Society of Edinburgh, V52; George EI, 1993, J AM STAT ASSOC, V91, P883; GEORGE EI, 1995, PRACTICAL MARKOV CHA, P203; Holland JB, 1998, J HERED, V89, P374, DOI 10.1093/jhered/89.4.374; Jannink JL, 2003, CROP SCI, V43, P489; Kao CH, 2002, GENETICS, V160, P1243; Malmberg RL, 2005, GENETICS, V171, P2013, DOI 10.1534/genetics.105.046078; Malmberg RL, 2005, GENET RES, V86, P89, DOI 10.1017/S0016672305007780; Robinson G. K., 1991, STAT SCI, V6, P15, DOI DOI 10.1214/SS/1177011926; Sillanpaa MJ, 1998, GENETICS, V148, P1373; Tinker NA, 1996, CROP SCI, V36, P1053; Wang H, 2005, GENETICS, V170, P465, DOI 10.1534/genetics.104.039354; XU S, 2007, IN PRESS BIOMETRICS; Xu SZ, 2003, GENETICS, V163, P789; Yi NJ, 2003, GENETICS, V165, P867; Yi NJ, 2005, GENETICS, V170, P1333, DOI 10.1534/genetics.104.040386; Yi NJ, 2003, GENETICS, V164, P1129; Yi NJ, 2006, GENET RES, V87, P45, DOI 10.1017/S0016672306007944; Zeng ZB, 2005, GENETICS, V169, P1711, DOI 10.1534/genetics.104.035857; Zhang M, 2005, GENETICS, V169, P2305, DOI 10.1534/genetics.104.034181	36	70	73	3	8	GENETICS	BALTIMORE	428 EAST PRESTON ST, BALTIMORE, MD 21202 USA	0016-6731			GENETICS	Genetics	APR	2007	175	4					1955	1963		10.1534/genetics.106.066571		9	Genetics & Heredity	Genetics & Heredity	167JU	WOS:000246448800035	17277367	
J	Nott, DJ; Yu, ZM; Chan, E; Cotsapas, C; Cowley, MJ; Pulvers, J; Williams, R; Little, P				Nott, David J.; Yu, Zeming; Chan, Eva; Cotsapas, Chris; Cowley, Mark J.; Pulvers, Jeremy; Williams, Rohan; Little, Peter			Hierarchical Bayes variable selection and microarray experiments	JOURNAL OF MULTIVARIATE ANALYSIS			English	Article						Bayesian model selections; hierarchical Bayes; microarrays	GENE-EXPRESSION; MODEL; NORMALIZATION; REGRESSION	Hierarchical and empirical Bayes approaches to inference are attractive for data arising from microarray gene expression studies because of their ability to borrow strength across genes in making inferences. Here we focus on the simplest case where we have data from replicated two colour arrays which compare two samples and where we wish to decide which genes are differentially expressed and obtain estimates of operating characteristics such as false discovery rates. The purpose of this paper is to examine the frequentist performance of Bayesian variable selection approaches to this problem for different prior specifications and to examine the effect on inference of commonly used empirical Bayes approximations to hierarchical Bayes procedures. The paper makes three main contributions. First, we describe how the log odds of differential expression can usually be computed analytically in the case where a double tailed exponential prior is used for gene effects rather than a normal prior, which gives an alternative to the commonly used B-statistic for ranking genes in simple comparative experiments. The second contribution of the paper is to compare empirical Bayes procedures for detecting differential expression with hierarchical Bayes methods which account for uncertainty in prior hyperparameters to examine how much is lost in using the commonly employed empirical Bayes approximations. Third, we describe an efficient MCMC scheme for carrying out the computations required for the hierarchical Bayes procedures. Comparisons are made via simulation studies where the simulated data are obtained by fitting models to some real microarray data sets. The results have implications for analysis of microarray data using parametric hierarchical and empirical Bayes methods for more complex experimental designs: generally we find that the empirical Bayes methods work well, which supports their use in the analysis of more complex experiments when a full hierarchical Bayes analysis would impose heavy computational demands. (c) 2006 Elsevier Inc. All rights reserved.	Univ New S Wales, Dept Stat, Sydney, NSW 2052, Australia; Univ New S Wales, Sch Biotechnol & Biomol Sci, Sydney, NSW 2052, Australia	Nott, DJ (reprint author), Univ New S Wales, Dept Stat, Sydney, NSW 2052, Australia.	D.Nott@unsw.edu.au	Simon, Simon/D-5128-2012; Cowley, Mark/D-4504-2012				Akaike H., 1973, 2 INT S INF THEOR, P267; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Kendziorski CM, 2003, STAT MED, V22, P3899, DOI 10.1002/sim.1548; Lonnstedt I, 2002, STAT SINICA, V12, P31; BENJAMINI Y, 1995, J ROY STAT SOC B MET, V57, P289; Baldi P, 2001, BIOINFORMATICS, V17, P509, DOI 10.1093/bioinformatics/17.6.509; SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136; Newton MA, 2004, BIOSTATISTICS, V5, P155, DOI 10.1093/biostatistics/5.2.155; Broet P, 2002, J COMPUT BIOL, V9, P671, DOI 10.1089/106652702760277381; Cotsapas C, 2003, COLD SPRING HARB SYM, V68, P109, DOI 10.1101/sqb.2003.68.109; Dudoit S., 2003, ANAL GENE EXPRESSION, P73, DOI 10.1007/0-387-21679-0_3; Fan JQ, 2005, J AM STAT ASSOC, V100, P781, DOI 10.1198/016214504000001781; FOSTER DP, 1994, ANN STAT, V22, P1947, DOI 10.1214/aos/1176325766; George EI, 2000, BIOMETRIKA, V87, P731, DOI 10.1093/biomet/87.4.731; Huang J, 2005, J AM STAT ASSOC, V100, P814, DOI 10.1198/016214504000002032; Ibrahim JG, 2002, J AM STAT ASSOC, V97, P88, DOI 10.1198/016214502753479257; Kauermann G, 2004, BIOMETRICS, V60, P376, DOI 10.1111/j.0006-341X.2004.00182.x; LEWIN A, 2006, BIOMETRICS, V62, P10, DOI 10.1111/j.1541-0420.2005.00394.x; Liu J. S., 2001, MONTE CARLO STRATEGI; LONNSTEDT I, 2003, MICROARRAY ANAL 2 IN; NEWTON MA, 2003, ANAL GENE EXPRESSION; Smyth GK, 2004, STAT APPL GENET MOL, V3, P1	22	3	3	1	3	ELSEVIER INC	SAN DIEGO	525 B STREET, STE 1900, SAN DIEGO, CA 92101-4495 USA	0047-259X			J MULTIVARIATE ANAL	J. Multivar. Anal.	APR	2007	98	4					852	872		10.1016/j.jmva.2006.10.001		21	Statistics & Probability	Mathematics	149YN	WOS:000245182900011		
J	Liu, ZQ; Jiang, F; Tian, GL; Wang, SN; Sato, F; Meltzer, SJ; Tan, M				Liu, Zhenqiu; Jiang, Feng; Tian, Guoliang; Wang, Suna; Sato, Fumiaki; Meltzer, Stephen J.; Tan, Ming			Sparse logistic regression with Lp penalty for biomarker identification	STATISTICAL APPLICATIONS IN GENETICS AND MOLECULAR BIOLOGY			English	Article						sparse logistic regression; Lp penalty; feature selection; microarry analysis	GENE-EXPRESSION; MICROARRAY DATA; VARIABLE SELECTION; STATISTICAL-INFERENCE; LASSO; VARIANCE; ARRAYS; MODEL	In this paper, we propose a novel method for sparse logistic regression with non-convex regularization Lp (p < 1). Based on smooth approximation, we develop several fast algorithms for learning the classifier that is applicable to high dimensional dataset such as gene expression. To the best of our knowledge, these are the first algorithms to perform sparse logistic regression with an Lp and elastic net (Le) penalty. The regularization parameters are decided through maximizing the area under the ROC curve (AUC) of the test data. Experimental results on methylation and microarray data attest the accuracy, sparsity, and efficiency of the proposed algorithms. Biomarkers identified with our methods are compared with that in the literature. Our computational results show that Lp Logistic regression (p < 1) outperforms the L1 logistic regression and SCAD SVM. Software is available upon request from the first author.	Univ Maryland, Sch Med, Baltimore, MD 21201 USA; Johns Hopkins Univ, Sch Med, Baltimore, MD USA; Univ Maryland, Greenebaum Canc Ctr, College Pk, MD 20742 USA	Liu, ZQ (reprint author), Univ Maryland, Sch Med, Baltimore, MD 21201 USA.	zliu@umm.edu; fjiang@som.umaryland.edu; gtian2@umm.edu; swang@medicine.umaryland.edu; fsato1@jhmi.edu; smeltzer@jhmi.edu; mtan@umm.edu					Malioutov D, 2005, IEEE T SIGNAL PROCES, V53, P3010, DOI 10.1109/TSP.2005.850882; Knight K, 2000, ANN STAT, V28, P1356; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; Krishnapuram B, 2005, IEEE T PATTERN ANAL, V27, P957, DOI 10.1109/TPAMI.2005.127; Alon U, 1999, P NATL ACAD SCI USA, V96, P6745, DOI 10.1073/pnas.96.12.6745; Zhang HH, 2006, BIOINFORMATICS, V22, P88, DOI 10.1093/bioinformatics/bti736; Ambroise C, 2002, P NATL ACAD SCI USA, V99, P6562, DOI 10.1073/pnas.102102699; Segal MR, 2003, J COMPUT BIOL, V10, P961, DOI 10.1089/106652703322756177; FRANK IE, 1993, TECHNOMETRICS, V35, P109, DOI 10.2307/1269656; Zou H, 2005, J ROY STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x; Tipping ME, 2001, J MACH LEARN RES, V1, P211, DOI 10.1162/15324430152748236; Newton MA, 2001, J COMPUT BIOL, V8, P37, DOI 10.1089/106652701300099074; Bo T, 2002, GENOME BIOL, V3; Bradley PS, 1998, INT C MACH LEARN, P82; Chen Y, 1998, NONCON OPTIM ITS APP, V20, P1; CHOW ML, 2001, PHYSIOL GENOMICS, V5, P99111; Donoho DL, 2003, P NATL ACAD SCI USA, V100, P2197, DOI 10.1073/pnas.0437847100; FAWCETT T, 2004, 1143 HP LAB; Hastie T., 2001, ELEMENTS STAT LEARNI; INZA B, 2002, J INTELLIGENT FUZZY; John G., 1998, FEATURE SELECTION KN, P33; Kerr MK, 2000, J COMPUT BIOL, V7, P819, DOI 10.1089/10665270050514954; LING X, 2003, P IJCAI; Liu Zhenqiu, 2005, Int J Bioinform Res Appl, V1, P169, DOI 10.1504/IJBRA.2005.007576; Long AD, 2001, J BIOL CHEM, V276, P19937, DOI 10.1074/jbc.M010192200; Monari G, 2000, NEUROCOMPUTING, V35, P195, DOI 10.1016/S0925-2312(00)00325-8; Pavlidis P., 2001, GENOME BIOL, V2; RAKOTOMAMONJY, 2004, P EUR C ART INT WORK; Rivals I., 2003, Journal of Machine Learning Research, V3, DOI 10.1162/153244303322753724; Shevade SK, 2003, BIOINFORMATICS, V19, P2246, DOI 10.1093/bioinformatics/btg308; Siegmund KD, 2004, BIOINFORMATICS, V20, P1896, DOI 10.1093/bioinformatics/bth176; Tibshirani R, 1997, STAT MED, V16, P385, DOI 10.1002/(SICI)1097-0258(19970228)16:4<385::AID-SIM380>3.0.CO;2-3; Vapnik V.N., 1995, NATURE STAT LEARNING; Virmani AK, 2002, CANCER EPIDEM BIOMAR, V11, P291; Wang S, 2006, ONCOGENE, V25, P3346, DOI 10.1038/sj.onc.1209357; Yu JS, 2005, BIOINFORMATICS, V21, pI487, DOI 10.1093/bioinformatics/bti1030	37	21	22	3	4	WALTER DE GRUYTER GMBH	BERLIN	GENTHINER STRASSE 13, D-10785 BERLIN, GERMANY	2194-6302	1544-6115		STAT APPL GENET MOL	Stat. Appl. Genet. Mol. Biol.	APR 2	2007	6								6	10.2202/1544-6115.1248		22	Biochemistry & Molecular Biology; Mathematical & Computational Biology; Statistics & Probability	Biochemistry & Molecular Biology; Mathematical & Computational Biology; Mathematics	152BN	WOS:000245335600005		
J	Wang, SJ; Zhu, J				Wang, Sijian; Zhu, Ji			Improved centroids estimation for the nearest shrunken centroid classifier	BIOINFORMATICS			English	Article							GENE-EXPRESSION; CLASS PREDICTION; SELECTION; MICROARRAYS; CANCER; LASSO; MODEL	Motivation: The nearest shrunken centroid (NSC) method has been successfully applied in many DNA-microarray classification problems. The NSC uses 'shrunken' centroids as prototypes for each class and identifies subsets of genes that best characterize each class. Classification is then made to the nearest (shrunken) centroid. The NSC is very easy to implement and very easy to interpret, however, it has drawbacks. Results: We show that the NSC method can be interpreted in the framework of LASSO regression. Based on that, we consider two new methods, adaptive L-infinity-norm penalized NSC (ALP-NSC) and adaptive hierarchically penalized NSC (AHP-NSC), with two different penalty functions for microarray classification, which improve over the NSC. Unlike the L-1-norm penalty used in LASSO, the penalty terms that we consider make use of the fact that parameters belonging to one gene should be treated as a natural group. Numerical results indicate that the two new methods tend to remove irrelevant genes more effectively and provide better classification results than the L-1-norm approach.	Univ Michigan, Dept Biostat, Ann Arbor, MI 48109 USA; Univ Michigan, Dept Stat, Ann Arbor, MI 48109 USA	Zhu, J (reprint author), Univ Michigan, Dept Biostat, Ann Arbor, MI 48109 USA.	jizhu@umich.edu					BREIMAN L, 1995, TECHNOMETRICS, V37, P373, DOI 10.2307/1269730; Shen XT, 2002, J AM STAT ASSOC, V97, P210, DOI 10.1198/016214502753479356; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Khan J, 2001, NAT MED, V7, P673, DOI 10.1038/89044; Zhang HH, 2006, BIOINFORMATICS, V22, P88, DOI 10.1093/bioinformatics/bti736; Bickel PJ, 2004, BERNOULLI, V10, P989, DOI 10.3150/bj/1106314847; Alizadeh AA, 2000, NATURE, V403, P503, DOI 10.1038/35000501; Golub TR, 1999, SCIENCE, V286, P531, DOI 10.1126/science.286.5439.531; Dudoit S, 2002, J AM STAT ASSOC, V97, P77, DOI 10.1198/016214502753479248; Pan W, 2002, BIOINFORMATICS, V18, P546, DOI 10.1093/bioinformatics/18.4.546; Dabney AR, 2005, BIOINFORMATICS, V21, P4148, DOI 10.1093/bioinformatics/bti681; Eisen MB, 1998, P NATL ACAD SCI USA, V95, P14863, DOI 10.1073/pnas.95.25.14863; Tibshirani R, 2003, STAT SCI, V18, P104, DOI 10.1214/ss/1056397488; Liu YF, 2006, J AM STAT ASSOC, V101, P500, DOI 10.1198/016214505000000781; Zou H, 2006, J AM STAT ASSOC, V101, P1418, DOI 10.1198/016214506000000735; Tibshirani R, 2002, P NATL ACAD SCI USA, V99, P6567, DOI 10.1073/pnas.082099299; Hastie T, 2001, GENOME BIOL, V2; MARRON J, 2002, DISTANCE WEIGHTED DI; Shen RL, 2006, BIOINFORMATICS, V22, P2635, DOI 10.1093/bioinformatics/btl442; Wu BL, 2006, BIOINFORMATICS, V22, P472, DOI 10.1093/bioinformatics/bti827; ZHANG H, 2006, I STAT MIMEO SERIES, V2596; Zhao P, 2006, GROUPED HIERARCHICAL; ZOU H, 2007, IN PRESS STAT SIN	23	25	25	1	2	OXFORD UNIV PRESS	OXFORD	GREAT CLARENDON ST, OXFORD OX2 6DP, ENGLAND	1367-4803			BIOINFORMATICS	Bioinformatics	APR 15	2007	23	8					972	979		10.1093/bioinformatics/btm046		8	Biochemical Research Methods; Biotechnology & Applied Microbiology; Computer Science, Interdisciplinary Applications; Mathematical & Computational Biology; Statistics & Probability	Biochemistry & Molecular Biology; Biotechnology & Applied Microbiology; Computer Science; Mathematical & Computational Biology; Mathematics	165GS	WOS:000246293000008	17384429	
J	Yi, K; Choi, H; Kim, J; Kim, Y				Yi, Kwangkeun; Choi, Hosik; Kim, Jaehwang; Kim, Yongdai			An empirical study on classification methods for alarms from a bug-finding static C analyzer	INFORMATION PROCESSING LETTERS			English	Article						static analysis; abstract interpretation; statistical post analysis; classification methods; program correctness	ALGORITHMS		Seoul Natl Univ, Seoul, South Korea	Yi, K (reprint author), Seoul Natl Univ, Seoul, South Korea.	kwang@ropas.snu.ac.kr					Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451; [Anonymous], R PROJECT STAT COMPU; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324; Bauer E, 1999, MACH LEARN, V36, P105, DOI 10.1023/A:1007515423169; Kremenek T, 2003, LECT NOTES COMPUT SC, V2694, P295; Lee JW, 2005, COMPUT STAT DATA AN, V48, P869, DOI 10.1016/j.csda.2004.03.017; Jung YB, 2005, LECT NOTES COMPUT SC, V3672, P203; Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.1023/A:1018054314350; Bush WR, 2000, SOFTWARE PRACT EXPER, V30, P775, DOI 10.1002/(SICI)1097-024X(200006)30:7<775::AID-SPE309>3.0.CO;2-H; Cousot P., 1979, P 6 ACM S PRINC PROG, P269, DOI DOI 10.1145/567752.567778; Cousot P., 1977, P 4 ACM S PRINC PROG, P238, DOI DOI 10.1145/512950.512973; COUSOT P, 1992, LECT NOTES COMPUT SC, V631, P269; Fawcett T., 2003, HPL20034; HASITE T, 2001, ELEMENTS STAT LEARNI; McDonald RA, 2003, LECT NOTES COMPUT SC, V2709, P35; Vapnik V., 1998, STAT LEARNING THEORY; Wu BL, 2003, BIOINFORMATICS, V19, P1636, DOI 10.1093/bioinformatics/btg210	18	3	4	0	5	ELSEVIER SCIENCE BV	AMSTERDAM	PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS	0020-0190			INFORM PROCESS LETT	Inf. Process. Lett.	APR 30	2007	102	2-3					118	123		10.1016/j.ipl.2006.11.004		6	Computer Science, Information Systems	Computer Science	147HN	WOS:000244994100015		
J	Zhang, H; Zhang, H; Li, ZH; Zheng, G				Zhang, Han; Zhang, Hong; Li, Zhaohai; Zheng, Gang			Statistical methods for haplotype-based matched case-control association studies	GENETIC EPIDEMIOLOGY			English	Article						bias; estimation; haplotype-analysis; matching; Wald test	UNPHASED GENOTYPE DATA; UNRELATED INDIVIDUALS; GENETIC INFORMATION; LINKAGE PHASE; INFERENCE; TRAITS; POLYMORPHISMS; DISEASE; DESIGN; CANCER	Kraft et al. [2005] proposed a method for matched haplotype-based association studies and compared the performances of six analytic strategies for estimating the odds ratio parameters using a conditional likelihood function. Zhang et al. [2006] modified the conditional likelihood and proposed a new method for matched haplotype-based association studies. The main assumptions of Zhang et al. were that the disease was rare, the population was in Hardy-Weinberg equilibrium (HWE), and the haplotypes were independent of the covariates and matching variable(s). In this article, we modify the estimation procedure proposed by Zhang et al. and introduce a fixation index so that the assumption of HWE is relaxed. Using the Wald test, we compare the current modified method with the procedure developed by Kraft et al. through simulations. The results show that the modified method is uniformly more powerful than that described in Kraft et al. Furthermore, the results indicate that the modified method is quite robust to the rare disease assumption.	Yale Univ, Sch Med, Dept Epidemiol & Publ Hlth, New Haven, CT 06510 USA; Univ Sci & Technol China, Dept Stat & Finance, Hefei, Peoples R China; George Washington Univ, Dept Stat, Washington, DC 20052 USA; NCI, Biostat Branch, Div Canc Epidemiol & Genet, Bethesda, MD 20892 USA; NHLBI, Off Biostat Res, Bethesda, MD 20892 USA	Zhang, H (reprint author), Yale Univ, Sch Med, Dept Epidemiol & Publ Hlth, 60 Coll St, New Haven, CT 06510 USA.	zhangh@ustc.edu.cn					Baughman RP, 1999, J CLIN EPIDEMIOL, V52, P1173; Akaike H., 1972, P 2 INT S INF THEOR, P267; Cordell HJ, 2002, AM J HUM GENET, V70, P124, DOI 10.1086/338007; Zaykin DV, 2002, HUM HERED, V53, P79, DOI 10.1159/000057986; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Schaid DJ, 2002, AM J HUM GENET, V70, P425, DOI 10.1086/338688; Epstein MP, 2003, AM J HUM GENET, V73, P1316, DOI 10.1086/380204; Chee M, 1996, SCIENCE, V274, P610, DOI 10.1126/science.274.5287.610; Wang DG, 1998, SCIENCE, V280, P1077, DOI 10.1126/science.280.5366.1077; Botstein D, 2003, NAT GENET, V33, P228, DOI 10.1038/ng1090; Breslow N. E., 1980, STAT METHODS CANC RE; Chatterjee N, 2005, BIOMETRIKA, V92, P399, DOI 10.1093/biomet/92.2.399; COX DR, 1972, J R STAT SOC B, V34, P187; De Vivo I, 2002, P NATL ACAD SCI USA, V99, P12263, DOI 10.1073/pnas.192172299; Heinonen OP, 1998, J NATL CANCER I, V90, P440, DOI 10.1093/jnci/90.6.440; Kraft P, 2005, GENET EPIDEMIOL, V28, P261, DOI 10.1002/gepi.20061; Lake SL, 2003, HUM HERED, V55, P56, DOI 10.1159/000071811; LIDDELL FDK, 1977, J ROY STAT SOC A STA, V140, P469, DOI 10.2307/2345280; Lin DY, 2006, J AM STAT ASSOC, V101, P89, DOI 10.1198/016214505000000808; PIKE MC, 1980, INT J EPIDEMIOL, V9, P89, DOI 10.1093/ije/9.1.89; Risch NJ, 2000, NATURE, V405, P847, DOI 10.1038/35015718; RISH N, 1996, SCIENCE, V273, P1516; Satten GA, 2004, GENET EPIDEMIOL, V27, P192, DOI 10.1002/gepi.20020; Schaid DJ, 2004, GENET EPIDEMIOL, V27, P348, DOI 10.1002/gepi.20037; SCHWARTZ G, 1986, ANN STAT, V6, P461; Spinka C, 2005, GENET EPIDEMIOL, V29, P108, DOI 10.1002/gepi.20085; Stram DO, 2003, HUM HERED, V55, P179, DOI 10.1159/000073202; Stram DO, 2003, HUM HERED, V55, P27, DOI 10.1159/000071807; Valle T, 1998, DIABETES CARE, V21, P949, DOI 10.2337/diacare.21.6.949; Zhang H, 2006, BIOMETRICS, V62, P1124, DOI 10.1111/j.1541-0420.2006.00568.x; Zhao LP, 2003, AM J HUM GENET, V72, P1231, DOI 10.1086/375140	31	5	6	0	0	WILEY-BLACKWELL	HOBOKEN	111 RIVER ST, HOBOKEN 07030-5774, NJ USA	0741-0395			GENET EPIDEMIOL	Genet. Epidemiol.	MAY	2007	31	4					316	326		10.1002/gepi.20212		11	Genetics & Heredity; Public, Environmental & Occupational Health	Genetics & Heredity; Public, Environmental & Occupational Health	158VW	WOS:000245824200005	17285622	
J	Pelckmans, K; Suykens, JAK; De Moor, B				Pelckmans, K.; Suykens, J. A. K.; De Moor, B.			A convex approach to validation-based learning of the regularization constant	IEEE TRANSACTIONS ON NEURAL NETWORKS			English	Article						convex optimization; model selection; regularization	SUPPORT VECTOR MACHINES; CROSS-VALIDATION; REGRESSION	This letter investigates a tight convex relaxation to the problem of tuning the regularization constant with respect to a validation based criterion. A number of algorithms is covered including ridge regression, regularization networks, smoothing splines, and least squares support vector machines (LS-SVMs) for regression. This convex approach allows the application of reliable and efficient tools, thereby improving computational cost and automatization of the learning method. It is shown that all solutions of the relaxation allow an interpretation in terms of a solution to a weighted LS-SVM.	Katholieke Univ Leuven, ESAT, SCD SISTA, B-3001 Louvain, Belgium	Pelckmans, K (reprint author), Katholieke Univ Leuven, ESAT, SCD SISTA, B-3001 Louvain, Belgium.	kristiaan.pelckmans@esat.kuleuven.be	Pelckmans, Kristiaan/A-3118-2013; Suykens, Johan/C-9781-2014				Suykens JAK, 2002, NEUROCOMPUTING, V48, P85, DOI 10.1016/S0925-2312(01)00644-0; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Chapelle O, 2002, MACH LEARN, V46, P131, DOI 10.1023/A:1012450327387; Lanckriet GRG, 2004, J MACH LEARN RES, V5, P27; HOERL AE, 1970, TECHNOMETRICS, V12, P55; Cawley GC, 2004, NEURAL NETWORKS, V17, P1467, DOI 10.1016/j.neunet.2004.07.002; GOLUB GH, 1979, TECHNOMETRICS, V21, P215, DOI 10.1080/00401706.1979.10489751; Efron B, 2004, ANN STAT, V32, P407; GIROSI F, 1995, NEURAL COMPUT, V7, P219, DOI 10.1162/neco.1995.7.2.219; DEVROYE L, 1996, PROBABILISITC THEORY; DUDLEY RM, 1987, ANN PROBAB, V15, P1306, DOI 10.1214/aop/1176991978; Hastie T., 2001, ELEMENTS STAT LEARNI; PELCKMAN SK, 2005, CONVEX APPROACH LEAR; PELCKMANS K, 2005, THESIS KU LEUVEN LEU; Pelckmans K, 2006, MACH LEARN, V62, P217, DOI 10.1007/s10994-005-5315-x; Saunders C, 1998, P 15 INT C MACH LEAR, P515; Shawe-Taylor J., 2004, KERNEL METHODS PATTE; Suykens J. A. K., 2002, LEAST SQUARES SUPPOR; Tikhonov AN, 1977, SOLUTION ILL POSED P; Vapnik V., 1998, STAT LEARNING THEORY; von Luxburg U., 2005, ADV NEURAL INFORM PR, V17; Wahba G., 1990, SPLINE MODELS OBSERV	22	2	2	2	4	IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC	PISCATAWAY	445 HOES LANE, PISCATAWAY, NJ 08855 USA	1045-9227			IEEE T NEURAL NETWOR	IEEE Trans. Neural Netw.	MAY	2007	18	3					917	920		10.1109/TNN.2007.891187		4	Computer Science, Artificial Intelligence; Computer Science, Hardware & Architecture; Computer Science, Theory & Methods; Engineering, Electrical & Electronic	Computer Science; Engineering	167AS	WOS:000246423400027	17526357	
J	Pan, W				Pan, Wei			Penalized model-based clustering with application to variable selection	JOURNAL OF MACHINE LEARNING RESEARCH			English	Article						BIC; EM; mixture model; penalized likelihood; soft-thresholding; shrinkage	GENE-EXPRESSION DATA; HIGH-DIMENSIONAL DATA; MICROARRAY EXPERIMENTS; MAXIMUM-LIKELIHOOD; CLASS PREDICTION; MIXTURE MODEL; EM ALGORITHM; CLASSIFICATION; ANNOTATION; REGRESSION	Variable selection in clustering analysis is both challenging and important. In the context of model-based clustering analysis with a common diagonal covariance matrix, which is especially suitable for "high dimension, low sample size" settings, we propose a penalized likelihood approach with an L-1 penalty function, automatically realizing variable selection via thresholding and delivering a sparse solution. We derive an EM algorithm to fit our proposed model, and propose a modified BIC as a model selection criterion to choose the number of components and the penalization parameter. A simulation study and an application to gene function prediction with gene expression profiles demonstrate the utility of our method.	Univ Minnesota, Sch Publ Hlth, Div Biostat, Minneapolis, MN 55455 USA; Univ Minnesota, Sch Stat, Minneapolis, MN 55455 USA	Pan, W (reprint author), Univ Minnesota, Sch Publ Hlth, Div Biostat, Minneapolis, MN 55455 USA.	WEIP@BIOSTAT.UMN.EDU					ALEXANDRIDIS R, 2004, BIOINFORMATICS, V20, P2546; Zhou XH, 2002, P NATL ACAD SCI USA, V99, P12783, DOI 10.1073/pnas.192159399; Shen XT, 2002, J AM STAT ASSOC, V97, P210, DOI 10.1198/016214502753479356; Fraley C, 1998, COMPUT J, V41, P578, DOI 10.1093/comjnl/41.8.578; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1; Bickel PJ, 2004, BERNOULLI, V10, P989, DOI 10.3150/bj/1106314847; McLachlan GJ, 2003, COMPUT STAT DATA AN, V41, P379, DOI 10.1016/S0167-9473(02)00183-4; Golub TR, 1999, SCIENCE, V286, P531, DOI 10.1126/science.286.5439.531; Yeung KY, 2001, BIOINFORMATICS, V17, P763, DOI 10.1093/bioinformatics/17.9.763; Pan W, 2002, BIOINFORMATICS, V18, P546, DOI 10.1093/bioinformatics/18.4.546; Fraley C, 2002, J AM STAT ASSOC, V97, P611, DOI 10.1198/016214502760047131; Eisen MB, 1998, P NATL ACAD SCI USA, V95, P14863, DOI 10.1073/pnas.95.25.14863; Tibshirani R, 2003, STAT SCI, V18, P104, DOI 10.1214/ss/1056397488; Jasra A, 2005, STAT SCI, V20, P50, DOI 10.1214/08834230500000016; Efron B, 2004, ANN STAT, V32, P407; SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136; Brown MPS, 2000, P NATL ACAD SCI USA, V97, P262, DOI 10.1073/pnas.97.1.262; Richardson S, 1997, J ROY STAT SOC B MET, V59, P731, DOI 10.1111/1467-9868.00095; Chang W., 1983, APPL STAT-J ROY ST C, V32, P267, DOI 10.2307/2347949; Ciuperca G, 2003, SCAND J STAT, V30, P45, DOI 10.1111/1467-9469.00317; Efron B, 2004, J AM STAT ASSOC, V99, P619, DOI 10.1198/016214504000000692; Fraley C., 2005, 486 U WASH DEP STAT; Friedman JH, 2004, J ROY STAT SOC B, V66, P815, DOI 10.1111/j.1467-9868.2004.02059.x; Ghosh D, 2002, BIOINFORMATICS, V18, P275, DOI 10.1093/bioinformatics/18.2.275; GREEN PJ, 1990, J ROY STAT SOC B MET, V52, P443; H Li, 2001, GENOME BIOL, V2; Hastie T., 2001, ELEMENTS STAT LEARNI; Hoff PD, 2006, BAYESIAN ANAL, V1, P321; Hoff PD, 2005, BIOMETRICS, V61, P1027, DOI 10.1111/j.1541-0420.2005.00381.x; HOFF PD, 2004, J R STAT SOC B, V66, P845; HUGHES RT, 2000, CELL, V102, P109; Kim S, 2006, BIOMETRIKA, V93, P877, DOI 10.1093/biomet/93.4.877; Liu J. S., 2003, BAYESIAN STAT, V7, P249; Mangasarian O. L., 2004, P SIAM INT C DAT MIN, P23; McLachlan G, 2002, FINITE MIXTURE MODEL; McLachlan GJ, 2002, BIOINFORMATICS, V18, P413, DOI 10.1093/bioinformatics/18.3.413; Mewes HW, 2004, NUCLEIC ACIDS RES, V32, pD41, DOI 10.1093/nar/gkh092; Pan W, 2006, BIOINFORMATICS, V22, P2388, DOI 10.1093/bioinformatics/btl393; Raftery AE, 2006, J AM STAT ASSOC, V101, P168, DOI 10.1198/016214506000000113; RAFTERY AE, 2003, BAYESIAN STAT, V7, P266; Tadesse MG, 2005, J AM STAT ASSOC, V100, P602, DOI 10.1198/0162145040000001565; Thomas JG, 2001, GENOME RES, V11, P1227, DOI 10.1101/gr.165101; Vapnik V., 1998, STAT LEARNING THEORY; Wu LF, 2002, NAT GENET, V31, P255, DOI 10.1038/ng906; Xiao Guanghua, 2005, Journal of Bioinformatics and Computational Biology, V3, P1371, DOI 10.1142/S0219720005001612; Yeung KY, 2001, BIOINFORMATICS, V17, P977, DOI 10.1093/bioinformatics/17.10.977; ZOU H, 2004, DEGREES FREEDOM LASS	50	49	51	1	5	MICROTOME PUBLISHING	BROOKLINE	31 GIBBS STREET, BROOKLINE, MA 02446 USA	1532-4435			J MACH LEARN RES	J. Mach. Learn. Res.	MAY	2007	8						1145	1164				20	Automation & Control Systems; Computer Science, Artificial Intelligence	Automation & Control Systems; Computer Science	194NS	WOS:000248351700009		
J	Fleisher, AS; Sowell, BB; Taylor, C; Gamst, AC; Petersen, RC; Thal, LJ				Fleisher, A. S.; Sowell, B. B.; Taylor, C.; Gamst, A. C.; Petersen, R. C.; Thal, L. J.		Alzheimers Disease Cooperative	Clinical predictors of progression to Alzheimer disease in amnestic mild cognitive impairment	NEUROLOGY			English	Article							E EPSILON-4 ALLELE; MEMORY IMPAIRMENT; NEUROPSYCHOLOGICAL PREDICTION; QUESTIONABLE DEMENTIA; EPISODIC MEMORY; OLDER PERSONS; RISK-FACTORS; SCALE; TRIALS; MCI	Objective: To investigate the neurocognitive measures that best predict progression from amnestic mild cognitive impairment (aMCI) to Alzheimer disease (AD). Methods: We evaluated 539 participants with aMCI from the Alzheimer's Disease Cooperative Study clinical drug trial of donepezil, vitamin E, or placebo. During the study period of 36 months, 212 aMCI participants progressed to AD. Using progression from aMCI to AD within 36 months as the dependent variable, a generalized linear model was fit to the data using the least absolute shrinkage and selection operator. Independent variables included in this analysis were age, sex, education, APOE-e4 (APOE4) status, family history of dementia, Mini-Mental State Examination score, Digits Backwards (Wechsler Memory Scale), Maze Time and Errors, Number Cancellation, Delayed Recall of Alzheimer's Disease Assessment Scale Word List, New York University Paragraph Recall Test (Immediate and Delayed), Boston Naming Test, Category Fluency, Clock Drawing Test, and the Alzheimer's Disease Assessment Scale-Cognitive subscale (ADAS-cog). Results: The model that best predicted progression from aMCI to AD over 36 months included APOE4 status, the Symbol Digit Modalities Test, Delayed 10-Word List Recall, New York University Paragraph Recall Test (Delayed), and the ADAS-cog total score. When APOE4 was removed from the analysis the resulting model had a similar estimated predictive accuracy as the full model. As determined by cross-validation, the estimated predictive accuracy of the final model was 80%. Conclusion: Progression from amnestic mild cognitive impairment to Alzheimer disease in this cohort was best determined by combining four common, easily administered, cognitive measures.	Univ Calif San Diego, Dept Neurosci, La Jolla, CA 92093 USA; Univ Calif San Diego, Dept Family & Prevent Med, La Jolla, CA 92093 USA; Mayo Clin & Mayo Fdn, Mayo Neurol, Rochester, MN 55905 USA	Fleisher, AS (reprint author), Alzheimers Dis Cooperat Study, 8950 Villa La Jolla Dr,Suite C227, La Jolla, CA 92037 USA.	afleisher@ucsd.edu					Aggarwal NT, 2005, J NEUROL NEUROSUR PS, V76, P1479, DOI 10.1136/jnnp.2004.053561; Aggarwal NT, 2005, NEUROCASE, V11, P3, DOI 10.1080/13554790490903038; Albert MS, 2001, J INT NEUROPSYCH SOC, V7, P631, DOI 10.1017/S1355617701755105; Amieva H, 2004, DEMENT GERIATR COGN, V18, P87, DOI 10.1159/000077815; MCKHANN G, 1984, NEUROLOGY, V34, P939; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Palmer K, 2002, AM J PSYCHIAT, V159, P436, DOI 10.1176/appi.ajp.159.3.436; MORRIS JC, 1993, NEUROLOGY, V43, P2412; Petersen RC, 2001, NEUROLOGY, V56, P1133; Solfrizzi V, 2004, NEUROLOGY, V63, P1882; ROSEN WG, 1984, AM J PSYCHIAT, V141, P1356; Tierney MC, 1996, NEUROLOGY, V46, P661; Petersen RC, 1999, ARCH NEUROL-CHICAGO, V56, P303, DOI 10.1001/archneur.56.3.303; PETERSEN RC, 1995, JAMA-J AM MED ASSOC, V273, P1274, DOI 10.1001/jama.273.16.1274; Jorm AF, 1998, NEUROLOGY, V51, P728; STERN Y, 1994, JAMA-J AM MED ASSOC, V271, P1004, DOI 10.1001/jama.271.13.1004; Bennett DA, 2002, NEUROLOGY, V59, P198; Morris JC, 2005, J ALZHEIMERS DIS, V7, P235; Petersen RC, 2005, NEW ENGL J MED, V352, P2379, DOI 10.1056/NEJMoa050151; Artero S, 2003, ACTA PSYCHIAT SCAND, V107, P390, DOI 10.1034/j.1600-0447.2003.00081.x; Baudic S, 2006, ARCH CLIN NEUROPSYCH, V21, P15, DOI 10.1016/j.acn.2005.07.002; Bennett DA, 2003, NEUROLOGY, V60, P246; Busse A, 2003, BRIT J PSYCHIAT, V182, P449, DOI 10.1192/bjp.182.5.449; Cooper B, 1996, PSYCHOL MED, V26, P411; Devanand DP, 1997, J AM GERIATR SOC, V45, P321; Devanand DP, 2005, ARCH NEUROL-CHICAGO, V62, P975, DOI 10.1001/archneur.62.6.975; Fleisher A, 2005, ARCH NEUROL-CHICAGO, V62, P953, DOI 10.1001/archneur.62.6.953; Folstein MF, 1975, J PSYCHIATR RES, V12, P198; Galasko D, 1997, ALZ DIS ASSOC DIS, V11, pS33; Gao S, 1998, ARCH GEN PSYCHIAT, V55, P809, DOI 10.1001/archpsyc.55.9.809; Greene JDW, 1996, NEUROPSYCHOLOGIA, V34, P537, DOI 10.1016/0028-3932(95)00151-4; Griffith HR, 2006, J INT NEUROPSYCH SOC, V12, P166, DOI 10.1017/S1355617706060267; Grundman M, 2004, ARCH NEUROL-CHICAGO, V61, P59, DOI 10.1001/archneur.61.1.59; HANNINEN T, 1995, J AM GERIATR SOC, V43, P1007; Kaplan E. F., 1983, BOSTON NAMING TEST; Kluger A, 1999, J GERIATR PSYCH NEUR, V12, P168, DOI 10.1177/089198879901200402; Kryscio RJ, 2006, NEUROLOGY, V66, P828, DOI 10.1212/01.wnl.0000203264.71880.45; Mohs R, 1996, ALZHEIMER DIS MOL BI, P407; Mohs R C, 1996, Int Psychogeriatr, V8, P195, DOI 10.1017/S1041610296002578; Mohs RC, 1997, ALZ DIS ASSOC DIS, V11, pS13; MONSCH AU, 1992, ARCH NEUROL-CHICAGO, V49, P1253; Rapp MA, 2005, AM J GERIAT PSYCHIAT, V13, P134, DOI 10.1176/appi.ajgp.13.2.134; REISBERG B, 1982, AM J PSYCHIAT, V139, P1136; Smith A., 1982, SYMBOL DIGIT MODALIT; Storandt M, 2006, NEUROLOGY, V67, P467, DOI 10.1212/01.wnl.0000228231.26111.6e; Tian J, 2003, J NEUROL NEUROSUR PS, V74, P433, DOI 10.1136/jnnp.74.4.433; Tierney MC, 1996, NEUROLOGY, V46, P149; Tierney MC, 2005, NEUROLOGY, V64, P1853, DOI 10.1212/01.WNL.0000163773.21794.0B; Visser PJ, 2000, INT J GERIATR PSYCH, V15, P363, DOI 10.1002/(SICI)1099-1166(200004)15:4<363::AID-GPS129>3.0.CO;2-4; Visser PJ, 2002, J NEUROL, V249, P312, DOI 10.1007/s004150200011; WECHSLER D, 1987, WMSR WECHSLER MEMORY; Wechsler D., 1981, MANUAL WECHSLER ADUL; WHITE L, 1994, J CLIN EPIDEMIOL, V47, P363, DOI 10.1016/0895-4356(94)90157-0	53	90	94	4	12	LIPPINCOTT WILLIAMS & WILKINS	PHILADELPHIA	530 WALNUT ST, PHILADELPHIA, PA 19106-3621 USA	0028-3878			NEUROLOGY	Neurology	MAY 8	2007	68	19					1588	1595		10.1212/01.wnl.0000258542.58725.4c		8	Clinical Neurology	Neurosciences & Neurology	164ZW	WOS:000246274600008	17287448	
J	Xie, Y; Pan, W; Jeong, KS; Khodursky, A				Xie, Yang; Pan, Wei; Jeong, Kyeong S.; Khodursky, Arkady			Incorporating prior information via shrinkage: a combined analysis of genome-wide location data and gene expression data	STATISTICS IN MEDICINE			English	Article						integrative analysis; microarray; statistical power; permutation	FALSE DISCOVERY RATE; MICROARRAY EXPERIMENTS; VARIABLE SELECTION; ESCHERICHIA-COLI; TRANSCRIPTION; REGRESSION; MODEL; PERMUTATION; ADAPTATION; LRP	Transcriptional control is a critical step in regulation of gene expression. Understanding such a control on a genomic level involves deciphering the mechanisms and structures of regulatory programmes and networks. A difficulty arises due to the weak signal and high noise in various sources of data while most current approaches are limited to analysis of a single source of data. A natural alternative is to improve statistical efficiency and power by a combined analysis of multiple sources of data. Here we propose a shrinkage method to combine genome-wide location data and gene expression data to detect the binding sites or target genes of a transcription factor. Specifically, a prior 'non-target' gene list is generated by analysing the expression data, and then this information is incorporated into the subsequent binding data analysis via a shrinkage method. There is a Bayesian justification for this shrinkage method. Both simulated and real data were used to evaluate the proposed method and compare it with analysing binding data alone. In simulation studies, the proposed method gives higher sensitivity and lower false discovery rate (FDR) in detecting the target genes. In real data example, the proposed method can reduce the estimated FDR and increase the power to detect the previously known target genes of a broad transcription regulator, leucine responsive regulatory protein (Lrp) in Escherichia coli. This method can also be used to incorporate other information, such as gene ontology (GO), to microarray data analysis to detect differentially expressed genes. Copyright (c) 2006 John Wiley & Sons, Ltd.	Univ Minnesota, Sch Publ Hlth, Div Biostat, Minneapolis, MN 55455 USA; Univ Minnesota, Dept Biochem Mol Biol & Biophys, Minneapolis, MN 55455 USA	Pan, W (reprint author), Mayo A460,MMC 303, Minneapolis, MN 55455 USA.	weip@biostat.umn.edu					Ren B, 2000, SCIENCE, V290, P2306, DOI 10.1126/science.290.5500.2306; Cui XG, 2005, BIOSTATISTICS, V6, P59, DOI 10.1093/biostatistics/kxh018; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; Wright GW, 2003, BIOINFORMATICS, V19, P2448, DOI 10.1093/bioinformatics/btg345; Tusher VG, 2001, P NATL ACAD SCI USA, V98, P5116, DOI 10.1073/pnas.091062498; Zou H, 2005, J ROY STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x; BENJAMINI Y, 1995, J ROY STAT SOC B MET, V57, P289; Tibshirani R, 2003, STAT SCI, V18, P104, DOI 10.1214/ss/1056397488; Storey JD, 2003, P NATL ACAD SCI USA, V100, P9440, DOI 10.1073/pnas.1530509100; Baldi P, 2001, BIOINFORMATICS, V17, P509, DOI 10.1093/bioinformatics/17.6.509; DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425; Efron B, 2001, J AM STAT ASSOC, V96, P1151, DOI 10.1198/016214501753382129; DONOHO DL, 1995, IEEE T INFORM THEORY, V41, P613, DOI 10.1109/18.382009; Yang YH, 2002, NUCLEIC ACIDS RES, V30, DOI 10.1093/nar/30.4.e15; Broet P, 2004, BIOINFORMATICS, V20, P2562, DOI 10.1093/bioinformatics/bth285; Buck MJ, 2004, GENOMICS, V83, P349, DOI 10.1016/j.ygeno.2003.11.004; Hastie T., 2001, ELEMENTS STAT LEARNI; Huang XH, 2003, BIOINFORMATICS, V19, P2072, DOI 10.1093/bioinformatics/btg283; Iyer VR, 2001, NATURE, V409, P533, DOI 10.1038/35054095; Louis T. A., 2000, BAYES EMPIRICAL BAYE; Mathew E, 1996, J BACTERIOL, V178, P7234; Pan Wei, 2003, Functional & Integrative Genomics, V3, P117; Pan W, 2003, BIOINFORMATICS, V19, P1333, DOI 10.1093/bioinformatics/btg167; Shannon MF, 2002, SCIENCE, V296, P666, DOI 10.1126/science.1062936; Simon I, 2001, CELL, V106, P697, DOI 10.1016/S0092-8674(01)00494-9; Tani TH, 2002, P NATL ACAD SCI USA, V99, P13471, DOI 10.1073/pnas.212510999; Xie Y, 2005, BIOINFORMATICS, V21, P4280, DOI 10.1093/bioinformatics/bti685; Xu XL, 2002, HUM MOL GENET, V11, P1977, DOI 10.1093/hmg/11.17.1977; ZHENG M, 443 UCLA; Zhi JZ, 1999, MOL MICROBIOL, V32, P29, DOI 10.1046/j.1365-2958.1999.01314.x	31	5	6	0	0	JOHN WILEY & SONS LTD	CHICHESTER	THE ATRIUM, SOUTHERN GATE, CHICHESTER PO19 8SQ, W SUSSEX, ENGLAND	0277-6715			STAT MED	Stat. Med.	MAY 10	2007	26	10					2258	2275		10.1002/sim.2703		18	Mathematical & Computational Biology; Public, Environmental & Occupational Health; Medical Informatics; Medicine, Research & Experimental; Statistics & Probability	Mathematical & Computational Biology; Public, Environmental & Occupational Health; Medical Informatics; Research & Experimental Medicine; Mathematics	160TM	WOS:000245965200009	16958153	
J	Kraker, JJ; Hawkins, DM; Basak, SC; Natarajan, R; Mills, D				Kraker, Jessica J.; Hawkins, Douglas M.; Basak, Subhash C.; Natarajan, Ranianathan; Mills, Denise			Quantitative Structure-Activity Relationship (QSAR) modeling of juvenile hormone activity: Comparison of validation procedures	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS			English	Article; Proceedings Paper	Chemometrics VII Conference	AUG 28-31, 2005	Hajduszoboszlo, HUNGARY	Czech Chem Soc		model validation; ridge regression; marginal soft threshold; modified Gram-Schmidt orthogonalization; elastic net	COMPUTED MOLECULAR DESCRIPTORS; NONORTHOGONAL PROBLEMS; MIMETIC COMPOUNDS; RIDGE REGRESSION; CROSS-VALIDATION; VAPOR-PRESSURE; SELECTION; FEATURES; BLOOD	Four regression methods are used to fit QSAR models for a large set of juvenile hormone mimetic compounds, using a diverse set of descriptors. The proper application of cross-validation is summarized and applied to both the model selection and verification steps, with comparison to the use of a holdout sample. Implementation of the evaluation of a model's predictive ability at the correct point in the procedure is emphasized. A recent regression methodology, the elastic net, is shown to produce a reduced set of predictors while retaining predictive ability. (C) 2006 Elsevier B.V. All rights reserved.	Univ Minnesota, Nat Resources Res Inst, Ctr Water & Environm, Duluth, MN 55811 USA; Univ Minnesota, Sch Stat, Minneapolis, MN 55455 USA	Basak, SC (reprint author), Univ Minnesota, Nat Resources Res Inst, Ctr Water & Environm, 5013 Miller Trunk Hwy, Duluth, MN 55811 USA.	sbasak@nrri.umn.edu	Natarajan, Ramanathan/A-5851-2008				RANDIC M, 1975, J AM CHEM SOC, V97, P6609, DOI 10.1021/ja00856a001; Basak SC, 2003, RISK ANAL, V23, P1173, DOI 10.1111/j.0272-4332.2003.00390.x; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Tropsha A, 2003, QSAR COMB SCI, V22, P69, DOI 10.1002/qsar.200390007; Hawkins DM, 2003, J CHEM INF COMP SCI, V43, P579, DOI 10.1021/ci025626i; Karelson M, 1996, CHEM REV, V96, P1027, DOI 10.1021/cr950202r; Hawkins DM, 2004, J CHEM INF COMP SCI, V44, P1, DOI 10.1021/ci0342472; HOERL AE, 1970, TECHNOMETRICS, V12, P55; Zou H, 2005, J ROY STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x; CARHART RE, 1985, J CHEM INF COMP SCI, V25, P64, DOI 10.1021/ci00046a002; SHAO J, 1993, J AM STAT ASSOC, V88, P486, DOI 10.2307/2290328; WIENER H, 1947, J AM CHEM SOC, V69, P17, DOI 10.1021/ja01193a005; Tibshirani R, 2002, P NATL ACAD SCI USA, V99, P6567, DOI 10.1073/pnas.082099299; Golbraikh A, 2002, J MOL GRAPH MODEL, V20, P269, DOI 10.1016/S1093-3263(01)00123-1; AUER CM, 1994, SAR QSAR ENVIRON RES, V2, P29, DOI 10.1080/10629369408028838; BALABAN AT, 1982, CHEM PHYS LETT, V89, P399, DOI 10.1016/0009-2614(82)80009-2; Basak S. C., 1993, APPROBE; BASAK SC, 1988, POLLY; Basak SC, 1979, P 2 INT C MATH MOD, P851; Basak SC, 2006, J CHEM INF MODEL, V46, P65, DOI 10.1021/ci050215y; Basak S.C., 1999, TOPOLOGICAL INDICES, P675; Basak SC, 2005, SAR QSAR ENVIRON RES, V16, P581, DOI 10.1080/10659360500468526; Basak SC, 2005, ARKIVOC, P308; Basak SC, 2004, ENVIRON TOXICOL PHAR, V16, P45, DOI 10.1016/j.etap.2003.09.002; Basak S. C., 2003, QUANTITATIVE STRUCTU, P207; Basak SC, 2001, J CHEM INF COMP SCI, V41, P692, DOI 10.1021/ci000165r; BONCHEV D, 1977, J CHEM PHYS, V67, P4517, DOI 10.1063/1.434593; *CAMBR SOFT CORP, 2000, CHEM 3D ULTR V 8 0; Filip PA, 1987, J MATH CHEM, V1, P61, DOI 10.1007/BF01205338; GENTLEMAN R, 1996, VERSION 2 0 1; *HALL ASS CONS, 2000, MOLC Z VERS 3 5; Hawkins DM, 2001, J CHEM INF COMP SCI, V41, P663, DOI 10.1021/ci0001177; HAYASHI T, 1990, J AGR FOOD CHEM, V38, P965; HAYASHI T, 1991, J AGR FOOD CHEM, V39, P2039, DOI 10.1021/jf00011a032; HAYASHI T, 1991, J AGR FOOD CHEM, V39, P2029, DOI 10.1021/jf00011a031; HOERL AE, 1970, TECHNOMETRICS, V12, P69, DOI 10.2307/1267352; Kier L. B., 1999, MOL STRUCTURE DESCRI; Kier L.B., 1986, MOL CONNECTIVITY STR; Kier LB, 1999, TOPOLOGICAL INDICES, P455; Netzeva T., 2005, ATLA-ALTERN LAB ANIM, V33, P1; NIWA A, 1989, J AGR FOOD CHEM, V37, P462, DOI 10.1021/jf00086a042; NIWA A, 1989, J AGR FOOD CHEM, V37, P467; NIWA A, 1989, J AGR FOOD CHEM, V37, P378; NIWA A, 1990, J AGR FOOD CHEM, V38, P514, DOI 10.1021/jf00092a040; SAS Institute, 1988, SAS STAT US GUID REL; Thisted R. A., 1988, ELEMENTS STAT COMPUT; Todeschini R., 2000, HDB MOL DESCRIPTORS; WOLD S, 1993, TECHNOMETRICS, V35, P136, DOI 10.2307/1269657	48	29	30	1	11	ELSEVIER SCIENCE BV	AMSTERDAM	PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS	0169-7439			CHEMOMETR INTELL LAB	Chemometrics Intell. Lab. Syst.	MAY 15	2007	87	1					33	42		10.1016/j.chemolab.2006.03.001		10	Automation & Control Systems; Chemistry, Analytical; Computer Science, Artificial Intelligence; Instruments & Instrumentation; Mathematics, Interdisciplinary Applications; Statistics & Probability	Automation & Control Systems; Chemistry; Computer Science; Instruments & Instrumentation; Mathematics	178LK	WOS:000247222000005		
J	Leitenstorfer, F; Tutz, G				Leitenstorfer, Florian; Tutz, Gerhard			Knot selection by boosting techniques	COMPUTATIONAL STATISTICS & DATA ANALYSIS			English	Article						nonparametric regression; knot selection; radial basis functions; boosting; surface fitting	NONPARAMETRIC REGRESSION; SPLINES; PENALTIES; SHRINKAGE; LASSO; MODEL	A novel concept for estimating smooth functions by selection techniques based on boosting is developed. It is suggested to put radial basis functions with different spreads at each knot and to perform selection and estimation simultaneously by a componentwise boosting algorithm. The methodology of various other smoothing and knot selection procedures (e.g. stepwise selection) is summarized. They are compared to the proposed approach by extensive simulations for various unidimensional settings, including varying spatial variation and heteroskedasticity, as well as on a real world data example. Finally, an extension of the proposed method to surface fitting is evaluated numerically on both, simulation and real data. The proposed knot selection technique is shown to be a strong competitor to existing methods for knot selection. (c) 2006 Elsevier B.V. All rights reserved.	Univ Munich, D-80799 Munich, Germany	Leitenstorfer, F (reprint author), Univ Munich, Akad Str 1, D-80799 Munich, Germany.	leiten@stat.uni-muenchen.de; tutz@stat.uni-muenchen.de					Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451; Hansen MH, 2001, J AM STAT ASSOC, V96, P746, DOI 10.1198/016214501753168398; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; JOHNSON ME, 1990, J STAT PLAN INFER, V26, P131, DOI 10.1016/0378-3758(90)90122-B; HOERL AE, 1970, TECHNOMETRICS, V12, P55; Buhlmann P, 2003, J AM STAT ASSOC, V98, P324, DOI 10.1198/016214503000125; Denison DGT, 1998, J ROY STAT SOC B, V60, P333, DOI 10.1111/1467-9868.00128; Lang S, 2004, J COMPUT GRAPH STAT, V13, P183, DOI 10.1198/1061860043010; Eilers PHC, 1996, STAT SCI, V11, P89, DOI 10.1214/ss/1038425655; DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425; SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136; Buhlmann P, 2006, ANN STAT, V34, P559, DOI 10.1214/009053606000000092; Buhlmann P, 2006, J MACH LEARN RES, V7, P1001; Cleveland W. S., 1991, STAT COMPUT, V1, P47, DOI 10.1007/BF01890836; CRAINCEANU CM, 2006, 103 J HOPK U DEP BIO; CRAINICEANU CM, 2004, 61 J HOPK U DEP BIOS; de Boor C., 1978, PRACTICAL GUIDE SPLI; Eubank R.L., 1988, SPLINE SMOOTHING NON; FRIEDMAN JH, 1991, ANN STAT, V19, P1, DOI 10.1214/aos/1176347963; Gijbels I., 1996, LOCAL POLYNOMIAL MOD; GREEN DJ, 1994, NONPARAMETRIC REGRES; HARDLE W, 1984, ROBUST NONLINEAR TIM, P163; Hastie TJ, 1990, GEN ADDITIVE MODELS; He X., 1994, J NONPARAMETR STAT, V3, P299, DOI DOI 10.1080/10485259408832589; He XM, 1999, COMPUTATION STAT, V14, P315, DOI 10.1007/s001800050019; Holst U, 1996, ENVIRONMETRICS, V7, P401, DOI 10.1002/(SICI)1099-095X(199607)7:4<401::AID-ENV221>3.0.CO;2-D; Hurvich CM, 1998, J ROY STAT SOC B, V60, P271, DOI 10.1111/1467-9868.00125; KOENKER R, 1994, BIOMETRIKA, V81, P673; Leung DHY, 2005, ANN STAT, V33, P2291, DOI 10.1214/009053605000000499; Loader C., 1999, LOCAL REGRESSION LIK; LOADER C, 2006, LOCFIT LOCAL REGRESS; Lokhorst J, 1999, LASSO GEN LINEAR MOD; Nychka D, 2005, FIELDS TOOLS SPATIAL; Osborne MR, 1998, COMP SCI STAT, V30, P44; R Development Core Team, 2006, LANG ENV STAT COMP; RIDGEWAY G, 1999, MODELS PREDICTIONS C, V31, P172; Ripley B. D., 1996, PATTERN RECOGNITION; Ruppert D, 1997, STAT ENVIRON, V3, P155; Ruppert D., 2003, SEMIPARAMETRIC REGRE; Ruppert D, 2000, AUST NZ J STAT, V42, P205, DOI 10.1111/1467-842X.00119; SCHAPIRE RE, 1990, MACH LEARN, V5, P197, DOI 10.1023/A:1022648800760; Smith M, 1997, J AM STAT ASSOC, V92, P1522, DOI 10.2307/2965423; Smith M, 1996, J ECONOMETRICS, V75, P317, DOI 10.1016/0304-4076(95)01763-1; SMITH PL, 1982, 166034 NASA; Stone CJ, 1997, ANN STAT, V25, P1371; TUTZ G, 2005, 418 LMU MUNCH; TUTZ G, 2006, IN PRESS J COMPUT GR; TUTZ G, 2006, IN PRESS BIOMETRICS; Wahba G., 1990, SPLINE MODELS OBSERV; Wand MP, 2000, COMPUTATION STAT, V15, P443, DOI 10.1007/s001800000047; Wood S, 2006, GENERALIZED ADDITIVE; Wood SN, 2003, J ROY STAT SOC B, V65, P95, DOI 10.1111/1467-9868.00374	52	8	8	1	2	ELSEVIER SCIENCE BV	AMSTERDAM	PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS	0167-9473			COMPUT STAT DATA AN	Comput. Stat. Data Anal.	MAY 15	2007	51	9					4605	4621		10.1016/j.csda.2006.08.008		17	Computer Science, Interdisciplinary Applications; Statistics & Probability	Computer Science; Mathematics	169QF	WOS:000246606000037		
J	Blei, DM; Lafferty, JD				Blei, David M.; Lafferty, John D.			A CORRELATED TOPIC MODEL OF SCIENCE	ANNALS OF APPLIED STATISTICS			English	Article						Hierarchical models; approximate posterior inference; variational methods; text analysis	DISTRIBUTIONS; SELECTION; LASSO	Topic models. such as latent Dirichlet allocation (LDA), call he useful tools for the statistical analysis of document collections and other discrete data. The LDA model assumes that the words of each document arise from a mixture of topics, each of which is it distribution over the vocabulary. A limitation of LDA is the inability to model topic correlation even though, for example, a document about genetics is more likely to also be about disease than X-ray astronomy. This limitation Sterns from the use of the Dirichlet distribution to model the variability among the topic proportions. In this paper we develop the correlated topic model (CTM), where the topic proportions exhibit correlation via the logistic normal distribution [J. Roy. Statist. Soc. Ser. B 44 (1982) 139-177]. We derive a fast variational inference algorithm for approximate posterior inference in this model. which is complicated by the fact that the logistic normal is not conjugate to the multinomial. We apply the CTM to the articles from Science Published from 1990-1999, a data set that comprises 57M words. The CTM gives a better fit of the data than LDA, and we demonstrate its Use as an exploratory tool of large document collections.	[Blei, David M.] Princeton Univ, Dept Comp Sci, Princeton, NJ 08540 USA; [Lafferty, John D.] Carnegie Mellon Univ, Dept Comp Sci, Machine Learning DEpt, Pittsburgh, PA 15213 USA	Blei, DM (reprint author), Princeton Univ, Dept Comp Sci, Princeton, NJ 08540 USA.	blei@cs.princcton.edu; lafferty@cs.cmu.edu			NSF [IIS-0312814, IIS-0427206]; DARPA CALO; Google	Supported in part by NSF Grants IIS-0312814 and IIS-0427206, the DARPA CALO project and a grant from Google.	Airoldi EM, 2007, LECT NOTES COMPUT SC, V4503, P57; AITCHISON J, 1982, J ROY STAT SOC B MET, V44, P139; AITCHISON J, 1980, BIOMETRIKA, V67, P261, DOI 10.2307/2335470; AITCHISON J, 1985, J ROY STAT SOC B MET, V47, P136; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; [Anonymous], 2005, IEEE C COMP VIS PATT; Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178; Pritchard JK, 2000, GENETICS, V155, P945; Saul LK, 1996, J ARTIF INTELL RES, V4, P61; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Griffiths TL, 2004, P NATL ACAD SCI USA, V101, P5228, DOI 10.1073/pnas.0307752101; Meinshausen N, 2006, ANN STAT, V34, P1436, DOI 10.1214/009053606000000281; Bishop C. M., 2003, ADV NEURAL INFORM PR, V15, P777; Blei David M., 2006, ADV NEURAL INFORM PR, V18; Blei D.M., 2005, J BAYESIAN ANAL, V1, P121; Blei D.M., 2003, P 26 ANN INT ACM SIG, P127, DOI DOI 10.1145/860435.860460; EROSHEVA E, 2007, ANN APPL ST IN PRESS; Erosheva E., 2004, P NATL ACAD SCI USA, V97, P11885; EROSHEVA E. A., 2002, THESIS CARNEGIE MELL; Girolami M, 2004, ADV NEUR IN, V16, P9; Griffiths TL, 2005, ADV NEURAL INFORM PR, V17, P537; Marlin B., 2004, THESIS U TORONTO; Mccallum A., 2004, AUTHOR RECIPIENT TOP; MOSTELLER F, 1964, INFERENCE DISPUTED A; Robert CP, 2004, MONTE CARLO STAT MET; Rosen-Zvi M., 2004, AUAI 04, P487; Sivic J, 2005, DISCOVERING OBJECT C; Teh Y., 2006, J AM STAT ASSOC, V101, P1566; Wainwright M. J., 2003, 649 UC BERK DEP STAT; WEI GCG, 1990, J AM STAT ASSOC, V85, P699, DOI 10.2307/2290005; Xing E.P., 2003, P 19 ANN C UNC ART I, P583	31	121	136	6	35	INST MATHEMATICAL STATISTICS	CLEVELAND	3163 SOMERSET DR, CLEVELAND, OH 44122 USA	1932-6157			ANN APPL STAT	Ann. Appl. Stat.	JUN	2007	1	1					17	35		10.1214/07-AOAS114		19	Statistics & Probability	Mathematics	374NL	WOS:000261050400002		
J	Rosset, S; Zhu, J				Rosset, Saharon; Zhu, Ji			Piecewise linear regularized solution paths	ANNALS OF STATISTICS			English	Article						l(1)-norm penalty; polynomial splines; regularization; solution paths; sparsity; total variation	NONCONCAVE PENALIZED LIKELIHOOD; REGRESSION; LASSO; SHRINKAGE; SELECTION; SPLINES	We consider the generic regularized optimization problem (beta) over cap(lambda) = arg min(beta) L (y, X beta) + lambda J (beta). Efron, Hastie, Johnstone and Tibshirani [Ann. Statist. 32 (2004) 407-499] have shown that for the LASSO-that is, if L is squared error loss and J(beta) = vertical bar vertical bar beta vertical bar vertical bar(1) is the if l(1) norm of beta-the optimal coefficient path is piecewise linear, that is, is piecewise constant. We derive a general characterization of the properties of (loss L, penalty J) pairs which give piecewise linear coefficient paths. Such pairs allow for efficient generation of the full regularized coefficient paths. We investigate the nature of efficient path following algorithms which arise. We use our results to suggest robust versions of the LASSO for regression and classification, and to develop new, efficient algorithms for existing problems in the literature, including Mammen and van de Geer's locally adaptive regression splines.	IBM Corp, Thomas J Watson Res Ctr, Predict Modeling Grp, Yorktown Hts, NY 10598 USA; Univ Michigan, Dept Stat, Ann Arbor, MI 48109 USA	Rosset, S (reprint author), IBM Corp, Thomas J Watson Res Ctr, Predict Modeling Grp, Yorktown Hts, NY 10598 USA.	srosset@us.ibm.com; jizhu@umich.edu					Davies PL, 2001, ANN STAT, V29, P1, DOI 10.1214/aos/996986501; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; HOERL AE, 1970, TECHNOMETRICS, V12, P55; HUBER PJ, 1964, ANN MATH STAT, V35, P73, DOI 10.1214/aoms/1177703732; Tibshirani R, 2005, J ROY STAT SOC B, V67, P91, DOI 10.1111/j.1467-9868.2005.00490.x; Efron B, 2004, ANN STAT, V32, P407; DONOHO DL, 1995, J ROY STAT SOC B MET, V57, P301; Fan JQ, 2004, ANN STAT, V32, P928, DOI 10.1214/009053604000000256; Freund Y., 1996, P 13 INT C MACH LEAR, P148; Hastie T., 2001, ELEMENTS STAT LEARNI; Hastie T, 2004, J MACH LEARN RES, V5, P1391; KOENKER R, 1994, BIOMETRIKA, V81, P673; Koenker R, 2005, QUANTILE REGRESSION; Mammen E, 1997, ANN STAT, V25, P387; Osborne MR, 2000, J COMPUT GRAPH STAT, V9, P319, DOI 10.2307/1390657; Rosset S, 2004, J MACH LEARN RES, V5, P941; Shen XT, 2003, J AM STAT ASSOC, V98, P724, DOI 10.1198/016214503000000639; Tsuda K, 2005, IEEE T IMAGE PROCESS, V14, P737, DOI 10.1109/TIP.2005.846029; Vapnik V.N., 1995, NATURE STAT LEARNING; Zhu J., 2003, ADV NEURAL INFORM PR, V16	21	130	136	2	9	INST MATHEMATICAL STATISTICS	BEACHWOOD	PO BOX 22718, BEACHWOOD, OH 44122 USA	0090-5364			ANN STAT	Ann. Stat.	JUN	2007	35	3					1012	1030		10.1214/009053606000001370		19	Statistics & Probability	Mathematics	199JS	WOS:000248692700004		
J	Leng, CL; Ma, SG				Leng, Chenlei; Ma, Shuangge			Accelerated failure time models with nonlinear covariates effects	AUSTRALIAN & NEW ZEALAND JOURNAL OF STATISTICS			English	Article						accelerated failure time; component smoothing and selection operator; stute estimator; variable selection	LINEAR RANK-TESTS; CENSORED-DATA; VARIABLE SELECTION; REGRESSION-ANALYSIS; LARGE-SAMPLE; SPLINE; LASSO	As a flexible alternative to the Cox model, the accelerated failure time (AFF) model assumes that the event time of interest depends on the covariates through a regression function. The AFT model with non-parametric covariate effects is investigated, when variable selection is desired along with estimation. Formulated in the framework of the smoothing spline analysis of variance model, the proposed method based on the Stute estimate (Stute, 1993 [Consistent estimation under random censorship when covariables are present, J. Multivariate Anal. 45, 89-103]) can achieve a sparse representation of the functional decomposition, by utilizing a reproducing kernel Hilbert norm penalty. Computational algorithms and theoretical properties of the proposed method are investigated. The finite sample size performance of the proposed approach is assessed via simulation studies. The primary biliary cirrhosis data is analyzed for demonstration.	Natl Univ Singapore, Dept Stat & Appl Probabil, Singapore 117546, Singapore; Yale Univ, Dept Epidemiol & Publ Hlth, New Haven, CT 06520 USA	Leng, CL (reprint author), Natl Univ Singapore, Dept Stat & Appl Probabil, Singapore 117546, Singapore.	stalc@nus.edu.sg; shuangge.ma@yale.edu					BREIMAN L, 1995, TECHNOMETRICS, V37, P373, DOI 10.2307/1269730; Gui J, 2005, BIOINFORMATICS, V21, P3001, DOI 10.1093/bioinformatics/bti422; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; Lin Y, 2006, ANN STAT, V34, P2272, DOI 10.1214/009053606000000722; ARONSZAJN N, 1950, T AM MATH SOC, V68, P337; CRAVEN P, 1979, NUMER MATH, V31, P377; BUCKLEY J, 1979, BIOMETRIKA, V66, P429; Cai JW, 2005, BIOMETRIKA, V92, P303, DOI 10.1093/biomet/92.2.303; COX DR, 1972, J R STAT SOC B, V34, P187; DICKSON ER, 1989, HEPATOLOGY, V10, P1, DOI 10.1002/hep.1840100102; Fan J., 2005, CONT MULTIVARIATE AN, P315; Fan JQ, 2002, ANN STAT, V30, P74; Fleming T. R., 1991, COUNTING PROCESSES S; Gu C., 2002, SMOOTHING SPLINE ANO; Hastie T., 2001, ELEMENTS STAT LEARNI; Hastie T, 1986, STAT SCI, V1, P297, DOI DOI 10.1214/SS/1177013604; Hastie TJ, 1990, GEN ADDITIVE MODELS; Huang J, 2006, BIOMETRICS, V62, P813, DOI 10.1111/j.1541-0420.2006.00562.x; Huang JHZ, 2000, ANN STAT, V28, P961; Jin ZZ, 2003, BIOMETRIKA, V90, P341, DOI 10.1093/biomet/90.2.341; LAI TL, 1991, ANN STAT, V19, P1370, DOI 10.1214/aos/1176348253; LENG C, 2005, NONPARAMETRIC MODEL; LENG C, 2006, PATH CONSIST MODEL S; MILLER RG, 1976, BIOMETRIKA, V63, P449, DOI 10.1093/biomet/63.3.449; Prentice R. L., 2002, STAT ANAL FAILURE TI; PRENTICE RL, 1978, BIOMETRIKA, V65, P167, DOI 10.1093/biomet/65.1.167; REID N, 1994, STAT SCI, V9, P439, DOI 10.1214/ss/1177010394; RITOV Y, 1990, ANN STAT, V18, P303, DOI 10.1214/aos/1176347502; Shen XT, 2000, J AM STAT ASSOC, V95, P842, DOI 10.2307/2669468; STUTE W, 1993, J MULTIVARIATE ANAL, V45, P89, DOI 10.1006/jmva.1993.1028; Stute W, 1999, STAT SINICA, V9, P1089; STUTE W, 1993, ANN STAT, V21, P1591, DOI 10.1214/aos/1176349273; Tibshirani R, 1997, STAT MED, V16, P385, DOI 10.1002/(SICI)1097-0258(19970228)16:4<385::AID-SIM380>3.0.CO;2-3; TSIATIS AA, 1990, ANN STAT, V18, P354, DOI 10.1214/aos/1176347504; van de Geer S, 2000, EMPIRICAL PROCESSES; van de Geer S., 2001, MATH METH STAT, V10, P355; WAHBA G, 1983, J ROY STAT SOC B MET, V45, P133; Wahba G., 1990, CBMS NSF REGIONAL C, V59; Wellner J., 2000, WEAK CONVERGENCE EMP; YING ZL, 1993, ANN STAT, V21, P76, DOI 10.1214/aos/1176349016	41	0	0	2	3	BLACKWELL PUBLISHING	OXFORD	9600 GARSINGTON RD, OXFORD OX4 2DQ, OXON, ENGLAND	1369-1473			AUST NZ J STAT	Aust. N. Z. J. Stat.	JUN	2007	49	2					155	172		10.1111/j.1467-842X.2007.00470.x		18	Statistics & Probability	Mathematics	179BV	WOS:000247265200003		
J	Xu, SZ				Xu, Shizhong			An empirical Bayes method for estimating epistatic effects of quantitative trait loci	BIOMETRICS			English	Article						LASSO; maximum likelihood; mixed model; QTL mapping; shrinkage	VARIABLE SELECTION; ENTIRE GENOME; MODEL; REGRESSION; SEARCH; INFORMATION; SHRINKAGE; VARIANCE; MARKERS	The genetic variance of a quantitative trait is often controlled by the segregation of multiple interacting loci. Linear model regression analysis is usually applied to estimating and testing effects of these quantitative trait loci (QTL). Including all the main effects and the effects of interaction (epistatic effects), the dimension of the linear model can be extremely high. Variable selection via stepwise regression or stochastic search variable selection (SSVS) is the common procedure for epistatic effect QTL analysis. These methods are computationally intensive, yet they may not be optimal. The LASSO (least absolute shrinkage and selection operator) method is computationally more efficient than the above methods. As a result, it has been widely used in regression analysis for large models. However, LASSO has never been applied to genetic mapping for epistatic QTL, where the number of model effects is typically many times larger than the sample size. In this study, we developed an empirical Bayes method (E-BAYES) to map epistatic QTL under the mixed model framework. We also tested the feasibility of using LASSO to estimate epistatic effects, examined the fully Bayesian SSVS, and reevaluated the penalized likelihood (PENAL) methods in mapping epistatic QTL. Simulation studies showed that all the above methods performed satisfactorily well. However, E-BAYES appears to outperform all other methods in terms of minimizing the mean-squared error (MSE) with relatively short computing time. Application of the new method to real data was demonstrated using a barley dataset.	Univ Calif Riverside, Dept Bot & Plant Sci, Riverside, CA 92521 USA	Xu, SZ (reprint author), Univ Calif Riverside, Dept Bot & Plant Sci, Riverside, CA 92521 USA.	xu@genetics.ucr.edu					Akaike H., 1973, 2 INT S INF THEOR, P267; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Hobert JP, 1996, J AM STAT ASSOC, V91, P1461, DOI 10.2307/2291572; HOERL AE, 1970, TECHNOMETRICS, V12, P55; Efron B, 2004, ANN STAT, V32, P407; SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136; Bernstein DS, 2000, SIAM J MATRIX ANAL A, V22, P145, DOI 10.1137/S0895479898333636; Bogdan M, 2004, GENETICS, V167, P989, DOI 10.1534/genetics.103.021683; Chipman HA, 1998, J AM STAT ASSOC, V93, P935, DOI 10.2307/2669832; Gelman A, 2005, ANN STAT, V33, P1, DOI 10.1214/009053604000001048; George EI, 1993, J AM STAT ASSOC, V91, P883; George EI, 2000, BIOMETRIKA, V87, P731, DOI 10.1093/biomet/87.4.731; Golub G.H., 1997, MATRIX COMPUTATIONS; HARVILLE DA, 1977, J AM STAT ASSOC, V72, P320, DOI 10.2307/2286796; HENDERSON CR, 1953, BIOMETRICS, V9, P226, DOI 10.2307/3001853; HENDERSON CR, 1950, ANN MATH STAT, V21, P309; PATTERSO.HD, 1971, BIOMETRIKA, V58, P545, DOI 10.2307/2334389; SAS Institute, 1989, SAS IML SOFTW US REF; Sen S, 2001, GENETICS, V159, P371; ter Braak CJF, 2005, GENETICS, V170, P1435, DOI 10.1534/genetics.105.040469; Tinker NA, 1996, CROP SCI, V36, P1053; Wang H, 2005, GENETICS, V170, P465, DOI 10.1534/genetics.104.039354; Xu SZ, 2003, GENETICS, V163, P789; Yi NJ, 2003, GENETICS, V165, P867; Yi NJ, 2003, GENETICS, V164, P1129; Yuan M, 2005, J AM STAT ASSOC, V100, P1215, DOI 10.1198/016214505000000367; Zhang Y. M., 2005, RECENT RES DEV GENET, V2, P1; Zhang YM, 2005, HEREDITY, V95, P96, DOI 10.1038/sj.hdy.6800702	28	70	72	4	15	BLACKWELL PUBLISHING	OXFORD	9600 GARSINGTON RD, OXFORD OX4 2DQ, OXON, ENGLAND	0006-341X			BIOMETRICS	Biometrics	JUN	2007	63	2					513	521		10.1111/j.1541-0420.2006.00711.x		9	Biology; Mathematical & Computational Biology; Statistics & Probability	Life Sciences & Biomedicine - Other Topics; Mathematical & Computational Biology; Mathematics	172ZS	WOS:000246843900023	17688503	
J	Shao, YW; Cook, RD; Weisberg, S				Shao, Yongwu; Cook, R. Dennis; Weisberg, Sanford			Marginal tests with sliced average variance estimation	BIOMETRIKA			English	Article						marginal coordinate test; sufficient dimension reduction	SUFFICIENT DIMENSION REDUCTION; INVERSE REGRESSION; BINARY RESPONSE	We present a new computationally feasible test for the dimension of the central subspace in a regression problem based on sliced average variance estimation. We also provide a marginal coordinate test. Under the null hypothesis, both the test of dimension and the marginal coordinate test involve test statistics that asymptotically have chi-squared distributions given normally distributed predictors, and have a distribution that is a linear combination of chi-squared distributions in general.	Univ Minnesota, Sch Stat, Minneapolis, MN 55455 USA	Shao, YW (reprint author), Univ Minnesota, Sch Stat, Minneapolis, MN 55455 USA.	ywshao@stat.umn.edu; dennis@stat.umn.edu; sandy@stat.umn.edu					Bura E, 2001, J AM STAT ASSOC, V96, P996, DOI 10.1198/016214501753208979; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Bentler PM, 2000, STAT PROBABIL LETT, V47, P381, DOI 10.1016/S0167-7152(99)00182-0; Bura E, 2003, BIOINFORMATICS, V19, P1252, DOI 10.1093/bioinformatics/btg150; Cook RD, 2005, J AM STAT ASSOC, V100, P410, DOI 10.1198/016214504000001501; COOK RD, 1991, J AM STAT ASSOC, V86, P328, DOI 10.2307/2290564; Cook RD, 1996, J AM STAT ASSOC, V91, P983, DOI 10.2307/2291717; Cook RD, 2000, J AM STAT ASSOC, V95, P781, DOI 10.2307/2669462; Cook RD, 2004, ANN STAT, V32, P1062, DOI 10.1214/009053604000000292; Cook RD, 1999, J AM STAT ASSOC, V94, P1187, DOI 10.2307/2669934; EATON ML, 1986, J MULTIVARIATE ANAL, V20, P272, DOI 10.1016/0047-259X(86)90083-7; Flury B., 1988, MULTIVARIATE STAT, P107; LI KC, 1991, J AM STAT ASSOC, V86, P316, DOI 10.2307/2290563; Schott J. R., 1997, MATRIX ANAL STAT; TYLER DE, 1981, ANN STAT, V9, P725, DOI 10.1214/aos/1176345514; Weisberg S., 2005, APPL LINEAR REGRESSI; Zhu M, 2003, J COMPUT GRAPH STAT, V12, P101, DOI 10.1198/1061860031220	17	20	20	0	1	OXFORD UNIV PRESS	OXFORD	GREAT CLARENDON ST, OXFORD OX2 6DP, ENGLAND	0006-3444			BIOMETRIKA	Biometrika	JUN	2007	94	2					285	296		10.1093/biomet/asm021		12	Biology; Mathematical & Computational Biology; Statistics & Probability	Life Sciences & Biomedicine - Other Topics; Mathematical & Computational Biology; Mathematics	181HD	WOS:000247426900003		
J	Elad, M; Milanfar, P; Rubinstein, R				Elad, Michael; Milanfar, Peyman; Rubinstein, Ron			Analysis versus synthesis in signal priors	INVERSE PROBLEMS			English	Article							SPARSE REPRESENTATION; IMAGES; DICTIONARIES; NOISE	The concept of prior probability for signals plays a key role in the successful solution of many inverse problems. Much of the literature on this topic can be divided between analysis-based and synthesis-based priors. Analysis-based priors assign probability to a signal through various forward measurements of it, while synthesis-based priors seek a reconstruction of the signal as a combination of atom signals. The algebraic similarity between the two suggests that they could be strongly related; however, in the absence of a detailed study, contradicting approaches have emerged. While the computationally intensive synthesis approach is receiving ever-increasing attention and is notably preferred, other works hypothesize that the two might actually be much closer, going as far as to suggest that one can approximate the other. In this paper we describe the two prior classes in detail, focusing on the distinction between them. We show that although in the simpler complete and undercomplete formulations the two approaches are equivalent, in their overcomplete formulation they depart. Focusing on the L-1 case, we present a novel approach for comparing the two types of priors based on high-dimensional polytopal geometry. We arrive at a series of theoretical and numerical results establishing the existence of an unbridgeable gap between the two.	Technion Israel Inst Technol, Dept Comp Sci, IL-32000 Haifa, Israel; Univ Calif Santa Cruz, Baskin Sch Engn, Elect Engn Dept, Santa Cruz, CA 95064 USA	Elad, M (reprint author), Technion Israel Inst Technol, Dept Comp Sci, IL-32000 Haifa, Israel.	ronrubin@cs.technion.ac.il	Milanfar, Peyman/B-2551-2009; 	Elad, Michael/0000-0001-8131-6928			Blomgren P, 1998, IEEE T IMAGE PROCESS, V7, P304, DOI 10.1109/83.661180; Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0; Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199; RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Candes EJ, 1999, PHILOS T R SOC A, V357, P2495; Chen SSB, 2001, SIAM REV, V43, P129, DOI 10.1137/S003614450037906X; Do MN, 2005, IEEE T IMAGE PROCESS, V14, P2091, DOI 10.1109/TIP.2005.859376; Tropp JA, 2004, IEEE T INFORM THEORY, V50, P2231, DOI 10.1109/TIT.2004.834793; Elad M, 2002, IEEE T IMAGE PROCESS, V11, P1141, DOI 10.1109/TIP.2002.801126; Elad M, 2002, IEEE T INFORM THEORY, V48, P2558, DOI 10.1109/TIT.2002.801410; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P6, DOI 10.1109/TIT.2005.860430; Bouman C, 1993, IEEE Trans Image Process, V2, P296, DOI 10.1109/83.236536; Candes E., 1999, CURVES SURFACES; CHAMBOLLE A, 1998, IEEE T IMAGE PROCESS, V7, P320; Donoho DL, 2003, P NATL ACAD SCI USA, V100, P2197, DOI 10.1073/pnas.0437847100; DONOHO DL, 2005, HIGH DIMENSIONAL CEN; Elad M, 2005, APPL COMPUT HARMON A, V19, P340, DOI 10.1016/j.acha.2005.03.005; Farsiu S, 2006, IEEE T IMAGE PROCESS, V15, P141, DOI 10.1109/TIP.2005.860336; Higdon DM, 1997, IEEE T MED IMAGING, V16, P516, DOI 10.1109/42.640741; SCHULTZ RR, 1994, IEEE T IMAGE PROCESS, V3, P233, DOI 10.1109/83.287017; Simoncelli E.P., 1999, LECT NOTES STAT, V141, P291; STROHMER T, 2003, MATHFA031135	23	193	201	2	11	IOP PUBLISHING LTD	BRISTOL	DIRAC HOUSE, TEMPLE BACK, BRISTOL BS1 6BE, ENGLAND	0266-5611			INVERSE PROBL	Inverse Probl.	JUN	2007	23	3					947	968		10.1088/0266-5611/23/3/007		22	Mathematics, Applied; Physics, Mathematical	Mathematics; Physics	172FI	WOS:000246789100007		
J	Foster, SD; Verbyla, AP; Pitchford, WS				Foster, Scott D.; Verbyla, Arunas P.; Pitchford, Wayne S.			Incorporating LASSO effects into a mixed model for quantitative trait loci detection	JOURNAL OF AGRICULTURAL BIOLOGICAL AND ENVIRONMENTAL STATISTICS			English	Article						adjusted scores; partial laplace approximation; quantitative trail loci; restricted likelihood; subset selection	REGRESSION; SELECTION; LIKELIHOOD; SHRINKAGE; CROSSES; MARKERS; GENOME; RIDGE	The identification of quantitative trait loci (QTL) can be viewed as a subset selection problem. In a simulation study the least absolute selection and shrinkage operator (LASSO) is shown to be a useful and powerful tool for QTL identification. LASSO effects are embedded into a mixed model allowing simultaneous modeling of genetic and experimental effects. This provides the flexibility to model the experiment in conjunction with the power of LASSO QTL identification. Estimation is performed using an approximation to the restricted likelihood and modified Gaussian elimination. The extended mixed model is used to analyze a cattle gene mapping dataset.	Univ Adelaide, Sch Agr Food & Wine, Glen Osmond, SA 5064, Australia; CSIRO Math & Informat Sci, Hobart, Tas 7001, Australia	Foster, SD (reprint author), Univ Adelaide, Sch Agr Food & Wine, PMB 1, Glen Osmond, SA 5064, Australia.	scott.foster@csiro.au	Verbyla, Arunas/A-5032-2009; Whitford, Linda/C-2470-2009; foster, scott/E-9311-2010				Afolayan RA, 2002, ASIAN AUSTRAL J ANIM, V15, P1371; Knott SA, 1996, THEOR APPL GENET, V93, P71, DOI 10.1007/BF00225729; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Whittaker JC, 2000, GENET RES, V75, P249, DOI 10.1017/S0016672399004462; LANDER ES, 1989, GENETICS, V121, P185; Kao CH, 1999, GENETICS, V152, P1203; ZENG ZB, 1994, GENETICS, V136, P1457; Broman KW, 2002, J R STAT SOC B, V64, P641, DOI 10.1111/1467-9868.00354; Foster S, 2006, THESIS U ADELAIDE; FOSTER SD, 2007, IN PRESS COMPUTATION; Gianola D, 2003, GENETICS, V163, P347; Gilmour AR, 1995, BIOMETRICS, V51, P1440, DOI 10.2307/2533274; HALEY CS, 1992, HEREDITY, V69, P315; HENDERSON CR, 1950, ANN MATH STAT, V21, P309; JANSEN RC, 1993, GENETICS, V135, P205; Kao CH, 2000, GENETICS, V156, P855; MCCULLAGH P, 1990, J ROY STAT SOC B MET, V52, P325; Miller A., 2002, MONOGRAPHS STAT APPL, V95; MORRIS CA, 2003, P ASS ADV AN BREED G, V15, P400; Osborne M R, 1985, WILEY SERIES PROBABI; Osborne MR, 2000, J COMPUT GRAPH STAT, V9, P319, DOI 10.2307/1390657; Patterson HD, 1971, BIOMETRIKA, V31, P100; Satagopan JM, 1996, GENETICS, V144, P805; Seaton G, 2002, BIOINFORMATICS, V18, P339, DOI 10.1093/bioinformatics/18.2.339; Sen S, 2001, GENETICS, V159, P371; Taylor JD, 2006, AUST NZ J STAT, V48, P465, DOI 10.1111/j.1467-842X.2006.00451.x; Verbyla A. P., 1990, AUSTR J STATISTICS, V32, P227; Wang H, 2005, GENETICS, V170, P465, DOI 10.1534/genetics.104.039354; Xu SZ, 2003, GENETICS, V163, P789; Yi NJ, 2005, GENETICS, V170, P1333, DOI 10.1534/genetics.104.040386	30	14	14	3	7	AMER STATISTICAL ASSOC & INTERNATIONAL BIOMETRIC SOC	WASHINGTON	1444 I ST NW, STE 700, WASHINGTON, DC 20005 USA	1085-7117			J AGR BIOL ENVIR ST	J. Agric. Biol. Environ. Stat.	JUN	2007	12	2					300	314		10.1198/108571107X200396		15	Biology; Mathematical & Computational Biology; Statistics & Probability	Life Sciences & Biomedicine - Other Topics; Mathematical & Computational Biology; Mathematics	173KA	WOS:000246870700009		
J	Dudik, M; Phillips, SJ; Schapire, RE				Dudik, Miroslav; Phillips, Steven J.; Schapire, Robert E.			Maximum entropy density estimation with generalized regularization and an application to species distribution modeling	JOURNAL OF MACHINE LEARNING RESEARCH			English	Article						maximum entropy; density estimation; regularization; iterative scaling; species distribution modeling	GEOGRAPHIC DISTRIBUTIONS; LOGISTIC-REGRESSION; BIODIVERSITY; BOUNDS; PREDICTION; SHRINKAGE; SELECTION; FISH	We present a unified and complete account of maximum entropy density estimation subject to constraints represented by convex potential functions or, alternatively, by convex regularization. We provide fully general performance guarantees and an algorithm with a complete convergence proof. As special cases, we easily derive performance guarantees for many known regularization types, including l(1), l(2), l(2)(2), and l(1) + l(2)(2) style regularization. We propose an algorithm solving a large and general subclass of generalized maximum entropy problems, including all discussed in the paper, and prove its convergence. Our approach generalizes and unifies techniques based on information geometry and Bregman divergences as well as those based more directly on compactness. Our work is motivated by a novel application of maximum entropy to species distribution modeling, an important problem in conservation biology and ecology. In a set of experiments on real-world data, we demonstrate the utility of maximum entropy in this setting. We explore effects of different feature types, sample sizes, and regularization levels on the performance of maxent, and discuss interpretability of the resulting models.	Princeton Univ, Dept Comp Sci, Princeton, NJ 08540 USA; AT&T Labs Res, Florham Pk, NJ 07932 USA	Dudik, M (reprint author), Princeton Univ, Dept Comp Sci, 35 Olden St, Princeton, NJ 08540 USA.	MDUDIK@CS.PRINCETON.EDU; PHILLIPS@RESEARCH.ATT.COM; SCHAPIRE@CS.PRINCETON.EDU					ALTUN Y, 2006, P 19 ANN C LEARN THE; Anderson RP, 2004, BIOL CONSERV, V116, P167, DOI 10.1016/S0006-3207(03)00187-3; DARROCH JN, 1972, ANN MATH STAT, V43, P1470, DOI 10.1214/aoms/1177692379; WILLIAMS PM, 1995, NEURAL COMPUT, V7, P117, DOI 10.1162/neco.1995.7.1.117; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Krishnapuram B, 2005, IEEE T PATTERN ANAL, V27, P957, DOI 10.1109/TPAMI.2005.127; Leathwick JR, 2006, MAR ECOL PROG SER, V321, P267, DOI 10.3354/meps321267; Berger AL, 1996, COMPUT LINGUIST, V22, P39; Zou H, 2005, J ROY STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x; Collins M, 2002, MACH LEARN, V48, P253, DOI 10.1023/A:1013912006537; Ponder WF, 2001, CONSERV BIOL, V15, P648, DOI 10.1046/j.1523-1739.2001.015003648.x; Graham CH, 2006, P NATL ACAD SCI USA, V103, P632, DOI 10.1073/pnas.0505754103; JAYNES ET, 1957, PHYS REV, V106, P620, DOI 10.1103/PhysRev.106.620; Moisen GG, 2002, ECOL MODEL, V157, P209, DOI 10.1016/S0304-3800(02)00197-7; DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425; Phillips SJ, 2006, ECOL MODEL, V190, P231, DOI 10.1016/j.ecolmodel.2005.03.026; Elith J, 2006, ECOGRAPHY, V29, P129, DOI 10.1111/j.2006.0906-7590.04596.x; HUTCHINSON GE, 1957, COLD SPRING HARB SYM, V22, P415; Bernstein S. N., 1946, THEORY PROBABILITY, V4th; Bregman L. M., 1967, USSR COMP MATH MATH, V7, P200, DOI 10.1016/0041-5553(67)90040-7; CESABIANCHI N, 1994, IEEE T INFORM THEORY, V40, P1215, DOI 10.1109/18.335953; Chen Yong-Lin, 2000, Entomological Knowledge, V37, P50; DEKEL O, 2003, P 16 ANN C COMP LEAR, P433; Della Pietra S., 2001, CMUCS01109; Della Pietra SA, 1997, IEEE T PATTERN ANAL, V19, P1; DEVROYE L, 1982, J MULTIVARIATE ANAL, V12, P72, DOI 10.1016/0047-259X(82)90083-5; Donoho DL, 2003, P NATL ACAD SCI USA, V100, P2197, DOI 10.1073/pnas.0437847100; Dudik M., 2005, ADV NEURAL INFORM PR, V17, P323; Dudik M., 2006, P 19 ANN C LEARN THE, P123; Elith Jane, 2002, P39; Ferrier S, 2002, BIODIVERS CONSERV, V11, P2309, DOI 10.1023/A:1021374009951; Goodman J., 2002, P 40 ANN M ASS COMP, P9; GOODMAN J, 2004, C N AM CHAPT ASS COM; HANNAH L, 2005, BIOSCIENCE, V55; HOERL AE, 1970, TECHNOMETRICS, V12, P55, DOI DOI 10.1080/00401706.1970.10488634; Jedynak BM, 2005, NEURAL COMPUT, V17, P1508, DOI 10.1162/0899766053723078; KAZAMA J, 2003, C EMP METH NAT LANG, P137; Khudanpur S. P., 1995, P J HOPK U LANG MOD, P1; LAU R, 1994, THESIS MIT DEP ENG C; Leathwick JR, 2005, FRESHWATER BIOL, V50, P2034, DOI 10.1111/j.1365-2427.2005.01448.x; Lebanon G., 2001, CMUCS01144; Malouf R., 2002, P 6 C NAT LANG LEARN, P49; MCDIARMID C, 1989, LOND MATH S, V141, P148; New M, 1999, J CLIMATE, V12, P829, DOI 10.1175/1520-0442(1999)012<0829:RTCSTC>2.0.CO;2; NEWMAN WI, 1977, IEEE T INFORM THEORY, V23, P89, DOI 10.1109/TIT.1977.1055659; Ng A. Y., 2004, P 21 INT C MACH LEAR, P615; Peterson AT, 2003, INT J PARASITOL, V33, P919, DOI 10.1016/S0020-7519(03)00094-8; Peterson AT, 2001, CONDOR, V103, P599, DOI 10.1650/0010-5422(2001)103[0599:PSGDBO]2.0.CO;2; Phillips SJ, 2004, P 21 INT C MACH LEAR, P655, DOI DOI 10.1145/1015330.1015412; Rockafellar R.T., 1970, CONVEX ANAL; Rosenfeld R, 1996, COMPUT SPEECH LANG, V10, P187, DOI 10.1006/csla.1996.0011; ROSSET S, 2003, ADV NEURAL INFORMATI, V15, P641; SALAKHUTDINOV R, 2003, UNCERTAINTY ARTIFICI, V19, P509; SAUER J. R., 2001, N AM BREEDING BIRD S; Schapire R., 2002, MSRI WORKSH NONL EST; Stockwell DRB, 2002, PREDICTING SPECIES OCCURRENCES: ISSUES OF ACCURACY AND SCALE, P537; United States Geological Survey - USGS, 2001, HYDRO 1K ELEVATION D; Vandenberghe L., 2004, CONVEX OPTIMIZATION; Welk E, 2002, DIVERS DISTRIB, V8, P219, DOI 10.1046/j.1472-4642.2002.00144.x; WELLING M, 2003, ADV NEURAL INFORMATI, V15, P665; Zhang T., 2005, ADV NEURAL INFORM PR, V17, P1625	61	42	45	3	29	MICROTOME PUBL	BROOKLINE	31 GIBBS ST, BROOKLINE, MA 02446 USA	1532-4435			J MACH LEARN RES	J. Mach. Learn. Res.	JUN	2007	8						1217	1260				44	Automation & Control Systems; Computer Science, Artificial Intelligence	Automation & Control Systems; Computer Science	194NT	WOS:000248351800001		
J	Wang, LF; Shen, XT				Wang, Lifeng; Shen, Xiaotong			On L-1-norm multiclass support vector machines: Methodology and theory	JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION			English	Article						high-dimension but low sample size; margin classification; regularization; sparsity; variable selection	GENE-EXPRESSION DATA; FEATURE-SELECTION; CLASSIFICATION; REGULARIZATION; REGRESSION; CANCER; PATH	Binary support vector machines (SVMs) have been proven to deliver high performance. In multiclass classification, however, issues remain with respect to variable selection. One challenging issue is classification and variable selection in the presence of variables in the magnitude of thousands, greatly exceeding the size of training sample. This often occurs in genomics classification. To meet the challenge, this article proposes a novel multiclass support vector machine, which performs classification and variable selection simultaneously through an L-1-norm penalized sparse representation. The proposed methodology, together with the developed regularization solution path, permits variable selection in such a situation. For the proposed methodology, a statistical learning theory is developed to quantify the generalization error in an attempt to gain insight into the basic structure of sparse learning, permitting the number of variables to greatly exceed the sample size. The operating characteristics of the methodology are examined through both simulated and benchmark data and are compared against some competitors in terms of accuracy of prediction. The numerical results suggest that the proposed methodology is highly competitive.	Univ Minnesota, Sch Stat, Minneapolis, MN 55455 USA		iamwlf@stat.umn.edu; xshen@stat.umn.edu	Wang, Lifeng/E-7746-2010				Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Golub TR, 1999, SCIENCE, V286, P531, DOI 10.1126/science.286.5439.531; Dudoit S, 2002, J AM STAT ASSOC, V97, P77, DOI 10.1198/016214502753479248; Zhang T, 2004, J MACH LEARN RES, V5, P1225; Liu YF, 2006, J AM STAT ASSOC, V101, P500, DOI 10.1198/016214505000000781; Lin Y, 2002, DATA MIN KNOWL DISC, V6, P259, DOI 10.1023/A:1015469627679; HOEFFDING W, 1963, J AM STAT ASSOC, V58, P13, DOI 10.2307/2282952; Bartlett PL, 2006, J AM STAT ASSOC, V101, P138, DOI 10.1198/016214505000000907; Bradley P. S., 1998, Machine Learning. Proceedings of the Fifteenth International Conference (ICML'98); Bredensteiner E.J., 1999, COMPUT OPTIM APPL, V12, P35; Cristianini N., 2000, INTRO SUPPORT VECTOR; Guermeur Y, 2002, PATTERN ANAL APPL, V5, P168, DOI 10.1007/s100440200015; Hastie T, 2004, J MACH LEARN RES, V5, P1391; Lee Y, 2003, BIOINFORMATICS, V19, P1132, DOI 10.1093/bioinformatics/btg102; Lee Y, 2006, BIOMETRIKA, V93, P555, DOI 10.1093/biomet/93.3.555; Lee YK, 2004, J AM STAT ASSOC, V99, P67, DOI 10.1198/016214504000000098; Lin Y, 2004, STAT PROBABIL LETT, V68, P73, DOI 10.1016/j.spl.2004.03.002; Liu Y, 2006, PATTERN RECOGN, V39, P1333, DOI 10.1016/j.patcog.2005.10.006; Shen XT, 2003, J AM STAT ASSOC, V98, P724, DOI 10.1198/016214503000000639; SHEN XT, 1994, ANN STAT, V22, P580, DOI 10.1214/aos/1176325486; SZEDMAK S, 2004, PATTERN RECOGNITION; Tarigan B, 2006, BERNOULLI, V12, P1045, DOI 10.3150/bj/1165269150; Vapnik V., 1998, STAT LEARNING THEORY; Wahba G, 1999, ADVANCES IN KERNEL METHODS, P69; Wang LF, 2006, STAT SINICA, V16, P617; Wellner J., 2000, WEAK CONVERGENCE EMP; Weston J., 1999, P 7 EUR S ART NEUR N, P219; ZHU J, 2004, NEURAL INFORMATION R, P16; Zhu J, 2005, J COMPUT GRAPH STAT, V14, P185, DOI 10.1198/106186005X25619; Zhu J, 2005, 430 U MICH DEP STAT	30	31	35	1	10	AMER STATISTICAL ASSOC	ALEXANDRIA	1429 DUKE ST, ALEXANDRIA, VA 22314 USA	0162-1459			J AM STAT ASSOC	J. Am. Stat. Assoc.	JUN	2007	102	478					583	594		10.1198/016214506000001383		12	Statistics & Probability	Mathematics	173FP	WOS:000246859200022		
J	Sakaguchi, Y; Ikeda, S				Sakaguchi, Yutaka; Ikeda, Shiro			Motor planning and sparse motor command representation	NEUROCOMPUTING			English	Article; Proceedings Paper	15th Annual Computational Neuroscience Meeting	JUL, 2006	Edinburgh, SCOTLAND			motor planning; sparse representation; parametric command representation	MUSCLE SYNERGIES; MOVEMENTS; MODEL	The present article proposes a novel computational approach to the motor planning. In this approach, each motor command is represented as a linear combination of prefixed basis patterns, and the command for a given task is designed by minimizing a two-termed criterion consisting of a task optimization term and a parameter preference (i.e., sparseness) term. The result of a computer simulation with a single-joint reaching task confirmed that our "representation-based" criterion for motor planning appropriately worked, together with showing that the resultant trajectory qualitatively replicated Fitts' law. (c) 2006 Elsevier B.V. All rights reserved.	Univ Electrocommun, Grad Sch Informat Syst, Chofu, Tokyo 182, Japan; Inst Stat Math, Minato Ku, Tokyo 106, Japan	Sakaguchi, Y (reprint author), Univ Electrocommun, Grad Sch Informat Syst, 1-5-1 Chofugaoka, Chofu, Tokyo 182, Japan.	sakaguchi@is.uec.ac.jp					Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; FLASH T, 1985, J NEUROSCI, V5, P1688; d'Avella A, 2003, NAT NEUROSCI, V6, P300, DOI 10.1038/nn1010; Olshausen BA, 1997, VISION RES, V37, P3311, DOI 10.1016/S0042-6989(97)00169-7; Harris CM, 1998, NATURE, V394, P780, DOI 10.1038/29528; Bell AJ, 1997, VISION RES, V37, P3327, DOI 10.1016/S0042-6989(97)00121-1; d'Avella A, 2005, P NATL ACAD SCI USA, V102, P3076, DOI 10.1073/pnas.0500199102; FITTS PM, 1954, J EXP PSYCHOL, V47, P381, DOI 10.1037/h0055392; GOTTLIEB GL, 1989, BEHAV BRAIN SCI, V12, P189; UNO Y, 1989, BIOL CYBERN, V61, P89	11	5	5	1	3	ELSEVIER SCIENCE BV	AMSTERDAM	PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS	0925-2312			NEUROCOMPUTING	Neurocomputing	JUN	2007	70	10-12			SI		1748	1752		10.1016/j.neucom.2006.10.120		5	Computer Science, Artificial Intelligence	Computer Science	178IV	WOS:000247215300030		
J	Wang, LF; Chen, G; Li, HZ				Wang, Lifeng; Chen, Guang; Li, Hongzhe			Group SCAD regression analysis for microarray time course gene expression data	BIOINFORMATICS			English	Article							CELL-CYCLE; SACCHAROMYCES-CEREVISIAE; TRANSCRIPTION FACTORS; SELECTION; YEAST; IDENTIFICATION; DISCOVERY; NETWORKS; MODEL	Motivation: Since many important biological systems or processes are dynamic systems, it is important to study the gene expression patterns over time in a genomic scale in order to capture the dynamic behavior of gene expression. Microarray technologies have made it possible to measure the gene expression levels of essentially all the genes during a given biological process. In order to determine the transcriptional factors (TFs) involved in gene regulation during a given biological process, we propose to develop a functional response model with varying coefficients in order to model the transcriptional effects on gene expression levels and to develop a group smoothly clipped absolute deviation (SCAD) regression procedure for selecting the TFs with varying coefficients that are involved in gene regulation during a biological process. Results: Simulation studies indicated that such a procedure is quite effective in selecting the relevant variables with time-varying coefficients and in estimating the coefficients. Application to the yeast cell cycle microarray time course gene expression data set identified 19 of the 21 known TFs related to the cell cycle process. In addition, we have identified another 52 TFs that also have periodic transcriptional effects on gene expression during the cell cycle process. Compared to simple linear regression (SLR) analysis at each time point, our procedure identified more known cell cycle related TFs. Conclusions: The proposed group SCAD regression procedure is very effective for identifying variables with time-varying coefficients, in particular, for identifying the TFs that are related to gene expression over time. By identifying the TFs that are related to gene expression variations over time, the procedure can potentially provide more insight into the gene regulatory networks.	Univ Penn, Sch Med, Genome Computat Biol Grad Grp, Philadelphia, PA 19104 USA; Univ Penn, Sch Med, Dept Biostat & Epidemiol, Philadelphia, PA 19104 USA; Univ Penn, Sch Med, Dept Bioengn, Philadelphia, PA 19104 USA	Li, HZ (reprint author), Univ Penn, Sch Med, Genome Computat Biol Grad Grp, Philadelphia, PA 19104 USA.	hli@cceb.upenn.edu	Wang, Lifeng/E-7746-2010				Banerjee N, 2003, NUCLEIC ACIDS RES, V31, P7024, DOI 10.1093/nar/gkg894; Yuan M, 2006, J ROY STAT SOC B, V68, P49, DOI 10.1111/j.1467-9868.2005.00532.x; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; Spellman PT, 1998, MOL BIOL CELL, V9, P3273; Conlon EM, 2003, P NATL ACAD SCI USA, V100, P3339, DOI 10.1073/pnas.0630591100; Tsai HK, 2005, P NATL ACAD SCI USA, V102, P13532, DOI 10.1073/pnas.0505874102; BENJAMINI Y, 1995, J ROY STAT SOC B MET, V57, P289; Efron B, 2004, ANN STAT, V32, P407; Lee TI, 2002, SCIENCE, V298, P799, DOI 10.1126/science.1075090; Bussemaker HJ, 2001, NAT GENET, V27, P167, DOI 10.1038/84792; CHEN G, 2007, GENOME BIOL, V8, P1; DAS D, 2006, MOL SYST BIOL, DOI UNSP 7SB410067-E1; FRIEDMAN J, 2001, ANN STAT, V19, P1; Gao F, 2004, BMC BIOINFORMATICS, V5, DOI 10.1186/1471-2105-5-31; Hong F, 2006, BIOMETRICS, V62, P534, DOI 10.1111/j.1541-0420.2005.00505.x; Keles S, 2002, BIOINFORMATICS, V18, P1167, DOI 10.1093/bioinformatics/18.9.1167; Luan YH, 2003, BIOINFORMATICS, V19, P474, DOI 10.1093/bioinformatics/btg014; Ma P, 2006, NUCLEIC ACIDS RES, V34, P1261, DOI 10.1093/nar/gkl013; Storey JD, 2005, P NATL ACAD SCI USA, V102, P12837, DOI 10.1073/pnas.0504609102; TAI YC, 2006, IN PRESS ANN STAT; YUAN M, 2006, IN PRESS J AM STAT A	22	59	62	0	3	OXFORD UNIV PRESS	OXFORD	GREAT CLARENDON ST, OXFORD OX2 6DP, ENGLAND	1367-4803			BIOINFORMATICS	Bioinformatics	JUN 15	2007	23	12					1486	1494		10.1093/bioinformatics/btm125		9	Biochemical Research Methods; Biotechnology & Applied Microbiology; Computer Science, Interdisciplinary Applications; Mathematical & Computational Biology; Statistics & Probability	Biochemistry & Molecular Biology; Biotechnology & Applied Microbiology; Computer Science; Mathematical & Computational Biology; Mathematics	193KA	WOS:000248271700007	17463025	
J	Kim, H; Park, H				Kim, Hyunsoo; Park, Haesun			Sparse non-negative matrix factorizations via alternating non-negativity-constrained least squares for microarray data analysis	BIOINFORMATICS			English	Article							GENE-EXPRESSION DATA; CLASS DISCOVERY; ONTO-EXPRESS; CLASSIFICATION; IDENTIFICATION; PREDICTION; REDUCTION; ALGORITHM; CANCER; PARTS	Motivation: Many practical pattern recognition problems require non-negativity constraints. For example, pixels in digital images and chemical concentrations in bioinformatics are non-negative. Sparse non-negative matrix factorizations (NMFs) are useful when the degree of sparseness in the non-negative basis matrix or the non-negative coefficient matrix in an NMF needs to be controlled in approximating high-dimensional data in a lower dimensional space. Results: In this article, we introduce a novel formulation of sparse NMF and show how the new formulation leads to a convergent sparse NMF algorithm via alternating non-negativity-constrained least squares. We apply our sparse NMF algorithm to cancer-class discovery and gene expression data analysis and offer biological analysis of the results obtained. Our experimental results illustrate that the proposed sparse NMF algorithm often achieves better clustering performance with shorter computing time compared to other existing NMF algorithms.	Georgia Inst Technol, Coll Comp, Atlanta, GA 30332 USA	Kim, H (reprint author), Georgia Inst Technol, Coll Comp, 266 Ferst Dr, Atlanta, GA 30332 USA.	hskim@cc.gatech.edu; hpark@acc.gatech.edu					Li SZ, 2001, PROC CVPR IEEE, P207; Pascual-Montano A, 2006, IEEE T PATTERN ANAL, V28, P403, DOI 10.1109/TPAMI.2006.60; Bro R, 1997, J CHEMOMETR, V11, P393, DOI 10.1002/(SICI)1099-128X(199709/10)11:5<393::AID-CEM483>3.0.CO;2-L; Maher EA, 2006, CANCER RES, V66, P11502, DOI 10.1158/0008-5472.CAN-06-2072; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Brunet JP, 2004, P NATL ACAD SCI USA, V101, P4164, DOI 10.1073/pnas.0308531101; Lee DD, 1999, NATURE, V401, P788; Carrasco DR, 2006, CANCER CELL, V9, P313, DOI 10.1016/j.ccr.2006.03.019; Golub TR, 1999, SCIENCE, V286, P531, DOI 10.1126/science.286.5439.531; Khatri P, 2002, GENOMICS, V79, P266, DOI 10.1006/geno.2002.6698; Gao Y, 2005, BIOINFORMATICS, V21, P3970, DOI 10.1093/bioinformatics/bti653; Grippo L, 2000, OPER RES LETT, V26, P127, DOI 10.1016/S0167-6377(99)00074-7; Pomeroy SL, 2002, NATURE, V415, P436, DOI 10.1038/415436a; Hoyer PO, 2004, J MACH LEARN RES, V5, P1457; BERRY M, 2006, IN PRESS COMPUT STAT; Berry MW, 1999, SIAM REV, V41, P335, DOI 10.1137/S0036144598347035; Carmona-Saez P, 2006, BMC BIOINFORMATICS, V7, DOI 10.1186/1471-2105-7-78; Chagoyen M, 2006, BMC BIOINFORMATICS, V7, DOI 10.1186/1471-2105-7-41; DING C, 2002, P 2 IEEE INT C DAT M; Draghici S, 2003, NUCLEIC ACIDS RES, V31, P3775, DOI 10.1093/nar/gkg624; Dueck D, 2005, BIOINFORMATICS, V21, pI144, DOI 10.1093/bioinformatics/bti1041; Gonzales E. F., 2005, ACCELERATING LEE SEU; Kim H, 2005, J MACH LEARN RES, V6, P37; Kim PM, 2003, GENOME RES, V13, P1706, DOI 10.1101/gr.903503; Lawson C. L., 1974, SOLVING LEAST SQUARE; Lee DD, 2000, P NIPS, ppp; Pauca V. P., 2004, P SIAM INT C DAT MIN; PAUCA VP, 2006, IN PRESS LINEAR ALGE; Pehkonen P, 2005, BMC BIOINFORMATICS, V6, DOI 10.1186/1471-2105-6-162; The Mathworks Inc., 1992, MATLAB US GUID; Van Benthem MH, 2004, J CHEMOMETR, V18, P441, DOI 10.1002/cem.889	31	139	145	5	14	OXFORD UNIV PRESS	OXFORD	GREAT CLARENDON ST, OXFORD OX2 6DP, ENGLAND	1367-4803			BIOINFORMATICS	Bioinformatics	JUN 15	2007	23	12					1495	1502		10.1093/bioinformatics/btm134		8	Biochemical Research Methods; Biotechnology & Applied Microbiology; Computer Science, Interdisciplinary Applications; Mathematical & Computational Biology; Statistics & Probability	Biochemistry & Molecular Biology; Biotechnology & Applied Microbiology; Computer Science; Mathematical & Computational Biology; Mathematics	193KA	WOS:000248271700008	17483501	
J	Kovac, A				Kovac, A.			Smooth functions and local extreme values	COMPUTATIONAL STATISTICS & DATA ANALYSIS			English	Article						nonparametric regression; modality; smoothness; total variation	SPATIAL ADAPTATION; REGRESSION; BANDWIDTH; SHRINKAGE; SELECTION	The problem of specifying a smooth and simple function that approximates noisy data is considered. A new automatic method is described that is based on solving a constrained optimisation problem. The target functional to be minimised is the sum of the squared residuals penalised by the curve length of the approximation. Multiresolution and monotonicity constraints provide a good approximation to the data with a small number of local extreme values. The new method can also be applied to density estimation. (c) 2006 Elsevier B.V. All rights reserved.	Univ Bristol, Dept Math, Bristol, Avon, England	Kovac, A (reprint author), Univ Bristol, Dept Math, Bristol, Avon, England.	A.Kovac@bristol.ac.uk					Davies PL, 2001, ANN STAT, V29, P1, DOI 10.1214/aos/996986501; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Antoniadis A, 2001, J AM STAT ASSOC, V96, P939, DOI 10.1198/016214501753208942; MARRON JS, 1992, ANN STAT, V20, P712, DOI 10.1214/aos/1176348653; DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425; SILVERMAN BW, 1985, J R STAT SOC B, V47, P1; BROCKMANN M, 1993, J AM STAT ASSOC, V88, P1302, DOI 10.2307/2291270; Davies P. L., 1995, STAT NEERL, V49, P185, DOI 10.1111/j.1467-9574.1995.tb01464.x; Davies PL, 2004, ANN STAT, V32, P1093, DOI 10.1214/009053604000000364; DONOHO D. L., 1995, J ROYAL STAT SOC B, V57, P371; FAN JQ, 1995, J ROY STAT SOC B MET, V57, P371; Gijbels I., 1996, LOCAL POLYNOMIAL MOD; Green PJ, 1994, MONOGRAPHS STAT APPL, V58; Huang JHZ, 2003, ANN STAT, V31, P1600, DOI 10.1214/aos/1065705120; KOVAC A, 2006, MINIMIZING TOTAL VAR; Nadaraya E. A., 1964, THEOR PROBAB APPL, V10, P186; Ruppert D, 1995, J AM STAT ASSOC, V90, P1257, DOI 10.2307/2291516; SEIFERT B, 1994, J COMPUTATIONAL GRAP, V3, P192, DOI 10.2307/1390668; van de Geer S., 2001, MATH METH STAT, V10, P355; Watson G., 1964, SANKHYA            A, V26, P101; Winkler G, 2002, J NONPARAMETR STAT, V14, P203, DOI 10.1080/10485250290003407	21	7	7	0	1	ELSEVIER SCIENCE BV	AMSTERDAM	PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS	0167-9473			COMPUT STAT DATA AN	Comput. Stat. Data Anal.	JUN 15	2007	51	10					5155	5171		10.1016/j.csda.2006.08.018		17	Computer Science, Interdisciplinary Applications; Statistics & Probability	Computer Science; Mathematics	170RK	WOS:000246681500030		
J	Shi, YX; Klustein, M; Simon, I; Mitchell, T; Bar-Joseph, Z				Shi, Yanxin; Klustein, Michael; Simon, Itamar; Mitchell, Tom; Bar-Joseph, Ziv			Continuous hidden process model for time series expression experiments	BIOINFORMATICS			English	Article; Proceedings Paper	15th Conference on Intelligent Systems for Molecular Biology/6th European Conference on Computational Biology	JUL 21-25, 2007	Vienna, AUSTRIA				FUNCTIONAL-ANALYSIS; MICROARRAY DATA; KNOWLEDGE	Motivation: When analyzing expression experiments, researchers are often interested in identifying the set of biological processes that are up-or down-regulated under the experimental condition studied. Current approaches, including clustering expression profiles and averaging the expression profiles of genes known to participate in specific processes, fail to provide an accurate estimate of the activity levels of many biological processes. Results: We introduce a probabilistic continuous hidden process Model (CHPM) for time series expression data. CHPM can simultaneously determine the most probable assignment of genes to processes and the level of activation of these processes over time. To estimate model parameters, CHPM uses multiple time series datasets and incorporates prior biological knowledge. Applying CHPM to yeast expression data, we show that our algorithm produces more accurate functional assignments for genes compared to other expression analysis methods. The inferred process activity levels can be used to study the relationships between biological processes. We also report new biological experiments confirming some of the process activity levels predicted by CHPM.	Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA; Hebrew Univ Jerusalem, Sch Med, Dept Mol Biol, IL-91120 Jerusalem, Israel	Bar-Joseph, Z (reprint author), Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA.			Bar-Joseph, Ziv/0000-0003-3430-6051			Gasch AP, 2000, MOL BIOL CELL, V11, P4241; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Khatri P, 2005, BIOINFORMATICS, V21, P3587, DOI 10.1093/bioinformatics/bti565; Coleman TF, 1996, SIAM J OPTIMIZ, V6, P418, DOI 10.1137/0806023; Huttenhower C, 2006, BIOINFORMATICS, V22, P2890, DOI 10.1093/informatics/btl492; Tan PK, 2003, NUCLEIC ACIDS RES, V31, P5676, DOI 10.1093/nar/gkg763; Shamir R, 2005, BMC BIOINFORMATICS, V6, DOI 10.1186/1471-2105-6-232; Gomes-Marcondes MCC, 2002, CANCER LETT, V180, P69, DOI 10.1016/S0304-3835(02)00006-X; Alter O, 2000, P NATL ACAD SCI USA, V97, P10101, DOI 10.1073/pnas.97.18.10101; Barutcuoglu Z, 2006, BIOINFORMATICS, V22, P830, DOI 10.1093/bioinformatics/btk048; Chang C., 2001, LIBSVM LIB SUPPORT V; Cheng Y, 2000, P 8 INT C INT SYST M, P93; Fang Z, 2006, J BIOMED INFORM, V39, P401, DOI 10.1016/j.jbi.2005.08.004; Gibbons FD, 2002, GENOME RES, V12, P1574, DOI 10.1101/gr.397002; Huang DS, 2006, BIOINFORMATICS, V22, P1259, DOI 10.1093/bioinformatics/btl065; HUTCHINSON R, 2006, P ICML; LIANG S, 1998, P PSB; Murphy K., 2002, THESIS U CALIFORNIA; Nachman I, 2004, BIOINFORMATICS, V20, P248, DOI 10.1093/bioinformatics/bth941; Newman JC, 2005, GENOME BIOL, V6, DOI 10.1186/gb-2005-6-9-r81; RAMAKRISHNAN N, 2005, P BIOONTOLOGIES SIG; Segal E., 2002, P INT C RES COMP MOL, P273, DOI 10.1145/565196.565232; Segal E., 2003, PAC S BIOC, V8, P89; Smid M, 2004, BIOINFORMATICS, V20, P2618, DOI 10.1093/bioinformatics/bth293; Tanay Amos, 2002, Bioinformatics, V18 Suppl 1, pS136	25	5	5	0	0	OXFORD UNIV PRESS	OXFORD	GREAT CLARENDON ST, OXFORD OX2 6DP, ENGLAND	1367-4803			BIOINFORMATICS	Bioinformatics	JUL 1	2007	23	13					I459	I467		10.1093/bioinformatics/btm218		9	Biochemical Research Methods; Biotechnology & Applied Microbiology; Computer Science, Interdisciplinary Applications; Mathematical & Computational Biology; Statistics & Probability	Biochemistry & Molecular Biology; Biotechnology & Applied Microbiology; Computer Science; Mathematical & Computational Biology; Mathematics	198IN	WOS:000248620400079	17646331	
J	Wipf, DP; Rao, BD				Wipf, David P.; Rao, Bhaskar D.			An empirical Bayesian strategy for solving the, simultaneous sparse approximation problem	IEEE TRANSACTIONS ON SIGNAL PROCESSING			English	Article						automatic relevance determination; empirical Bayes; multiple response models; simultaneous sparse approximation; sparse Bayesian learning; variable selection	NONNEGATIVE GARROTE; REPRESENTATIONS; DICTIONARIES; MINIMIZATION; SHRINKAGE; SELECTION; MODELS; BASES	Given a large overcomplete dictionary of basis vectors, the goal is to simultaneously represent L > 1 signal vectors using coefficient expansions marked by a common sparsity profile. This generalizes the standard sparse representation problem to the case where multiple responses exist that were putatively generated by the same small subset of features. Ideally, the associated sparse generating weights should be recovered, which can have physical significance in many applications (e.g., source localization). The generic solution to this problem is intractable and, therefore, approximate procedures are sought. Based on the concept of automatic relevance determination, this paper uses an empirical Bayesian prior to estimate a convenient posterior distribution over candidate basis vectors. This particular approximation enforces a common sparsity profile and consistently places its prominent posterior mass on the appropriate region of weight-space necessary for simultaneous sparse recovery. The resultant algorithm is then compared with multiple response extensions of matching pursuit, basis pursuit, FOCUSS, and Jeffreys prior-based Bayesian methods, finding that it often outperforms the others. Additional motivation for this particular choice of cost function is also provided, including the analysis of global and local minima and a variational derivation that highlights the similarities and differences between the proposed algorithm and previous approaches.	Univ Calif San Francisco, Biomagnet Imaging Lab, San Francisco, CA 94143 USA; Univ Calif San Diego, Dept Elect & Comp Engn, La Jolla, CA 92093 USA	Wipf, DP (reprint author), Univ Calif San Francisco, Biomagnet Imaging Lab, San Francisco, CA 94143 USA.	david.wipf@mrsc.ucsf.edu; brao@ucsd.edu					MACKAY DJC, 1992, NEURAL COMPUT, V4, P415, DOI 10.1162/neco.1992.4.3.415; BREIMAN L, 1995, TECHNOMETRICS, V37, P373, DOI 10.2307/1269730; Malioutov D, 2005, IEEE T SIGNAL PROCES, V53, P3010, DOI 10.1109/TSP.2005.850882; Donoho DL, 2001, IEEE T INFORM THEORY, V47, P2845, DOI 10.1109/18.959265; Gao HY, 1998, J COMPUT GRAPH STAT, V7, P469; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582; Rao BD, 2003, IEEE T SIGNAL PROCES, V51, P760, DOI 10.1109/TSP.2002.808076; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; HARVILLE DA, 1974, BIOMETRIKA, V61, P383, DOI 10.1093/biomet/61.2.383; Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178; Gribonval R, 2003, IEEE T INFORM THEORY, V49, P3320, DOI 10.1109/TIT.2003.820031; Phillips C, 2005, NEUROIMAGE, V24, P997, DOI 10.1016/j.neuroimage.2004.10.030; DEMPSTER AP, 1981, J AM STAT ASSOC, V76, P341, DOI 10.2307/2287835; MALLAT SG, 1993, IEEE T SIGNAL PROCES, V41, P3397, DOI 10.1109/78.258082; Tropp JA, 2004, IEEE T INFORM THEORY, V50, P2231, DOI 10.1109/TIT.2004.834793; Tipping ME, 2001, J MACH LEARN RES, V1, P211, DOI 10.1162/15324430152748236; Cotter SF, 2005, IEEE T SIGNAL PROCES, V53, P2477, DOI 10.1109/TSP.2005.849172; Attias H, 2000, ADV NEURAL INFORM PR, V12; Beal M., 2002, BAYESIAN STAT, V7; Berger J. O., 1985, STAT DECISION THEORY; CHEN J, 2005, P IEEE INT C ACC SPE, V4, P257; Chen S. S., 1999, SIAM J SCI COMPUT, V20, P33; Donoho DL, 2003, P NATL ACAD SCI USA, V100, P2197, DOI 10.1073/pnas.0437847100; Donoho D. L., 2004, MOST LARGE UNDERDETE; DONOHO DL, 1994, WAVELETS THEORY ALGO, P233; Fevrier IJ, 1999, IEEE T COMMUN, V47, P927, DOI 10.1109/26.771349; Figueiredo MAT, 2002, ADV NEUR IN, V14, P697; Fuchs JJ, 2004, IEEE T INFORM THEORY, V50, P1341, DOI 10.1109/TIT.2004.828141; Gorodetsky VR, 2000, GEMATOL TRANSFUZIOL, V45, P3; GORODNITSKY IF, 1995, ELECTROEN CLIN NEURO, V95, P231, DOI 10.1016/0013-4694(95)00107-A; HARVILLE DA, 1977, J AM STAT ASSOC, V72, P320, DOI 10.2307/2286796; Hastie T., 2001, ELEMENTS STAT LEARNI; HILLEBRAND A, 2000, P 12 INT C BIOM, P753; Jeffs B. D., 1998, P ICASSP, P1885, DOI 10.1109/ICASSP.1998.681832; Johnson R. A., 1992, APPL MULTIVARIATE ST; Kay S., 1993, FUNDAMENTALS STAT SI; MacKay D, 1994, ASHRAE T, V100, P1053; MOULIN P, 1999, IEEE T INF THEORY, V45; Neal R. M., 1996, BAYESIAN LEARNING NE; Palmer J. A., 2006, ADV NEURAL INFORM PR, V18; Phillips JW, 1997, IEEE T MED IMAGING, V16, P338, DOI 10.1109/42.585768; RAMIREZ R, 2006, HUM BRAIN MAPP 12 AN; RAMIREZ RR, 2005, THESIS NEW YORK U NE; RAO BD, 1998, P 32 AS C SIGN SYST, V1, P752; Rasmussen C. E., 1996, THESIS U TORONTO TOR; Ripley B. D., 1996, PATTERN RECOGNITION; Rockafellar R.T., 1970, CONVEX ANAL; SILVA JG, 2006, ADV NEURAL INFORMATI, V18; Stoica P., 1997, INTRO SPECTRAL ANAL; Tropp J. A., 2006, EURASIP J SIGNAL PRO, V86, P589; TROPP JA, 2006, EURASIP J SIGNAL PRO, V86, P572; WAKIN MB, 2006, ADV NEURAL INFORM PR, V18; Wipf D., 2005, ADV NEURAL INFORM PR, V17; Wipf D., 2004, ADV NEURAL INFORM PR, V16; Wipf D. P., 2006, THESIS U CALIFORNIA; WIPF DP, 2005, WORKSH SIGN PROC AD; WIPF DP, 2004, IEEE INT INT C ACC S	57	120	130	1	9	IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC	PISCATAWAY	445 HOES LANE, PISCATAWAY, NJ 08855 USA	1053-587X			IEEE T SIGNAL PROCES	IEEE Trans. Signal Process.	JUL	2007	55	7	2				3704	3716		10.1109/TSP.2007.894265		13	Engineering, Electrical & Electronic	Engineering	182EW	WOS:000247488900013		
J	Sugiyama, M				Sugiyama, Masashi			Generalization error estimation for non-linear learning methods	IEICE TRANSACTIONS ON FUNDAMENTALS OF ELECTRONICS COMMUNICATIONS AND COMPUTER SCIENCES			English	Article						supervised learning; generalization capability; model selection; subspace information criterion; non-linear learning	SUBSPACE INFORMATION CRITERION; SELECTION	Estimating the generalization error is one of the key ingredients of supervised learning since a good generalization error estimator can be used for model selection. An unbiased generalization error estimator called the subspace information criterion (SIC) is shown to be useful for model selection, but its range of application is limited to linear learning methods. In this paper, we extend SIC to be applicable to non-linear learning.	Tokyo Inst Technol, Dept Comp Sci, Tokyo 1528552, Japan	Sugiyama, M (reprint author), Tokyo Inst Technol, Dept Comp Sci, Tokyo 1528552, Japan.	sugi@cs.titech.ac.jp					EFRON B, 1979, ANN STAT, V7, P1, DOI 10.1214/aos/1176344552; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; GIROSI F, 1995, NEURAL COMPUT, V7, P219, DOI 10.1162/neco.1995.7.2.219; Blanchard G, 2006, J MACH LEARN RES, V7, P247; Chen Y, 1998, NONCON OPTIM ITS APP, V20, P1; Efron B, 1993, INTRO BOOTSTRAP; Evgeniou T, 2001, ADV COMPUTATIONAL MA, V13, P1; Huber P. J., 1981, ROBUST STAT; PELCKMANS K, 2004, MACH LEARN, V62, P217; Scholkopf B., 2002, LEARNING KERNELS; STEIN CM, 1981, ANN STAT, V9, P1135, DOI 10.1214/aos/1176345632; Sugiyama M, 2002, MACH LEARN, V48, P25, DOI 10.1023/A:1013995402903; SUGIYAMA M, 2004, LETT REV, V2, P33; Sugiyama M, 2001, NEURAL COMPUT, V13, P1863, DOI 10.1162/08997660152469387; Sugiyama M., 2005, STAT DECISIONS, V23, P249, DOI 10.1524/stnd.2005.23.4.249; Sugiyama M., 2002, J MACHINE LEARNING R, V3, P323; Vapnik V. N., 1998, STAT LEARNING; WILLIAMS TF, 1995, AGING-CLIN EXP RES, V7, P1	18	1	1	1	4	IEICE-INST ELECTRONICS INFORMATION COMMUNICATIONS ENG	TOKYO	KIKAI-SHINKO-KAIKAN BLDG, 3-5-8, SHIBA-KOEN, MINATO-KU, TOKYO, 105-0011, JAPAN	0916-8508	1745-1337		IEICE T FUND ELECTR	IEICE Trans. Fundam. Electron. Commun. Comput. Sci.	JUL	2007	E90A	7					1496	1499		10.1093/ietfec/e90-a.7.1496		4	Computer Science, Hardware & Architecture; Computer Science, Information Systems; Engineering, Electrical & Electronic	Computer Science; Engineering	187IG	WOS:000247840400030		
J	Hannachi, A; Jolliffe, IT; Stephenson, DB				Hannachi, A.; Jolliffe, I. T.; Stephenson, D. B.			Empirical orthogonal functions and related techniques in atmospheric science: A review	INTERNATIONAL JOURNAL OF CLIMATOLOGY			English	Review						empirical orthogonal functions; simplified EOFs; extended EOFs; complex EOFs; North Atlantic Oscillation; Madden Julian oscillation; quasi-biennial oscillation	PRINCIPAL COMPONENT ANALYSIS; MADDEN-JULIAN OSCILLATION; INTERANNUAL TIME SCALES; TRADE-WIND SYSTEM; SEA-SURFACE TEMPERATURE; NORTHERN-HEMISPHERE; UNITED-STATES; EXTRATROPICAL CIRCULATION; GEOPOTENTIAL HEIGHT; BIENNIAL VARIATIONS	Climate and weather constitute a typical example where high dimensional and complex phenomena meet. The atmospheric system is the result of highly complex interactions between many degrees of freedom or modes. In order to gain insight in understanding the dynamical/physical behaviour involved it is useful to attempt to understand their interactions in terms of a much smaller number of prominent modes of variability. This has led to the development by atmospheric researchers of methods that give a space display and a time display of large space-time atmospheric data. Empirical orthogonal functions (EOFs) were first used in meteorology in the late 1940s. The method, which decomposes a space-time field into spatial patterns and associated time indices, contributed much in advancing our knowledge of the atmosphere. However, since the atmosphere contains all sorts of features, e.g. stationary and propagating, EOFs are unable to provide a full picture. For example, EOFs tend, in general, to be difficult to interpret because of their geometric properties, such as their global feature, and their orthogonality in space and time. To obtain more localised features, modifications, e.g. rotated EOFs (REOFs), have been introduced. At the same time, because these methods cannot deal with propagating features, since they only use spatial correlation of the field, it was necessary to use both spatial and time information in order to identify such features. Extended and complex EOFs were introduced to serve that purpose. Because of the importance of EOFs and closely related methods in atmospheric science, and because the existing reviews of the subject are slightly out of date, there seems to be a need to update our knowledge by including new developments that could not be presented in previous reviews. This review proposes to achieve precisely this goal. The basic theory of the main types of EOFs is reviewed, and a wide range of applications using various data sets are also provided. Copyright (C) 2007 Royal Meteorological Society	Univ Reading, Dept Meteorol, Reading RG6 6BB, Berks, England	Hannachi, A (reprint author), King Abdul Aziz Univ, Dept Meteorol, PO Box 80208, Jeddah 21589, Saudi Arabia.	ahannachi@kau.edu.sa	Stephenson, David/A-9903-2011				Ambaum MHP, 2002, J CLIMATE, V15, P553, DOI 10.1175/1520-0442(2002)015<0553:C>2.0.CO;2; Ambaum MHP, 2001, J CLIMATE, V14, P3495, DOI 10.1175/1520-0442(2001)014<3495:AOONAO>2.0.CO;2; ANDERSON JR, 1983, J ATMOS SCI, V40, P1584, DOI 10.1175/1520-0469(1983)040<1584:TLHSOD>2.0.CO;2; Anderson TW, 1984, INTRO MULTIVARIATE S; ANGELL JK, 1964, J ATMOS SCI, V21, P479, DOI 10.1175/1520-0469(1964)021<0479:QBVITT>2.0.CO;2; Angstrom A, 1935, GEOGRAFISKA ANN, V17, P243; KAISER HF, 1958, PSYCHOMETRIKA, V23, P187, DOI 10.1007/BF02289233; ANDERSON TW, 1963, ANN MATH STAT, V34, P122, DOI 10.1214/aoms/1177704248; BLUMENTHAL MB, 1991, J CLIMATE, V4, P766, DOI 10.1175/1520-0442(1991)004<0766:POACOM>2.0.CO;2; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; FUKUNAGA K, 1970, IEEE T COMPUT, VC 19, P311, DOI 10.1109/T-C.1970.222918; [Anonymous], 2002, REV GEOPHYS, DOI DOI 10.1029/2000RG000092; FRAEDRICH K, 1986, J ATMOS SCI, V43, P419, DOI 10.1175/1520-0469(1986)043<0419:ETDOWA>2.0.CO;2; BJERKNES J, 1969, MON WEATHER REV, V97, P163, DOI 10.1175/1520-0493(1969)097<0163:ATFTEP>2.3.CO;2; BROOMHEAD DS, 1986, PHYSICA D, V20, P217, DOI 10.1016/0167-2789(86)90031-X; Kalnay E, 1996, B AM METEOROL SOC, V77, P437, DOI 10.1175/1520-0477(1996)077<0437:TNYRP>2.0.CO;2; VAUTARD R, 1992, PHYSICA D, V58, P95, DOI 10.1016/0167-2789(92)90103-T; Saji NH, 1999, NATURE, V401, P360, DOI 10.1038/43855; Kistler R, 2001, B AM METEOROL SOC, V82, P247, DOI 10.1175/1520-0477(2001)082<0247:TNNYRM>2.3.CO;2; Thompson DWJ, 2000, J CLIMATE, V13, P1018, DOI 10.1175/1520-0442(2000)013<1018:AMITEC>2.0.CO;2; VONSTORCH H, 1995, J CLIMATE, V8, P377, DOI 10.1175/1520-0442(1995)008<0377:POPAR>2.0.CO;2; KILADIS GN, 1992, MON WEATHER REV, V120, P1900, DOI 10.1175/1520-0493(1992)120<1900:CAAWTC>2.0.CO;2; Jolliffe IT, 2003, J COMPUT GRAPH STAT, V12, P531, DOI 10.1198/1061860032148; PLAUT G, 1994, J ATMOS SCI, V51, P210, DOI 10.1175/1520-0469(1994)051<0210:SOLFOA>2.0.CO;2; Thompson DWJ, 2000, J CLIMATE, V13, P1000, DOI 10.1175/1520-0442(2000)013<1000:AMITEC>2.0.CO;2; Hurrell JW, 1996, GEOPHYS RES LETT, V23, P665, DOI 10.1029/96GL00459; Hotelling H, 1933, J EDUC PSYCHOL, V24, P417, DOI 10.1037/h0071325; MADDEN RA, 1972, J ATMOS SCI, V29, P1109, DOI 10.1175/1520-0469(1972)029<1109:DOGSCC>2.0.CO;2; WALLACE JM, 1981, MON WEATHER REV, V109, P784, DOI 10.1175/1520-0493(1981)109<0784:TITGHF>2.0.CO;2; Hannachi A, 2007, INT J CLIMATOL, V27, P1, DOI 10.1002/joc.1375; Dommenget D, 2007, CLIM DYNAM, V28, P517, DOI 10.1007/s00382-006-0195-8; BRETHERTON CS, 1992, J CLIMATE, V5, P541, DOI 10.1175/1520-0442(1992)005<0541:AIOMFF>2.0.CO;2; MADDEN RA, 1971, J ATMOS SCI, V28, P702, DOI 10.1175/1520-0469(1971)028<0702:DOADOI>2.0.CO;2; Pearson K, 1901, PHILOS MAG, V2, P559; Baldwin MP, 2001, REV GEOPHYS, V39, P179, DOI 10.1029/1999RG000073; BARNETT TP, 1983, MON WEATHER REV, V111, P756, DOI 10.1175/1520-0493(1983)111<0756:IOTMAP>2.0.CO;2; BARNETT TP, 1984, MON WEATHER REV, V112, P2388, DOI 10.1175/1520-0493(1984)112<2388:IOTMAP>2.0.CO;2; BARNETT TP, 1984, MON WEATHER REV, V112, P2380, DOI 10.1175/1520-0493(1984)112<2380:IOTMAP>2.0.CO;2; BARNSTON AG, 1987, MON WEATHER REV, V115, P1083, DOI 10.1175/1520-0493(1987)115<1083:CSAPOL>2.0.CO;2; BASILEVSKY A, 1979, J AM STAT ASSOC, V74, P284, DOI 10.2307/2286324; BIBBY J, 1980, SANKHYA B, V42, P165; Bjornsson H., 1997, 971 CCGCR MCGILL U, P52; BLOOMFIELD P, 1994, INT J CLIMATOL, V14, P759, DOI 10.1002/joc.3370140706; Brillinger D. R., 1981, TIME SERIES DATA ANA; BRINK KH, 1986, J GEOPHYS RES-OCEANS, V91, P877, DOI 10.1029/JC091iC01p00877; Broomhead D. S., 1986, Nonlinear Phenomena and Chaos; Cahalan RF, 1996, J GEOPHYS RES-ATMOS, V101, P26309, DOI 10.1029/96JD01611; CARROLL JB, 1953, PSYCHOMETRIKA, V18, P23; CHATFIELD C, 1989, INTRO MULTIVARIATE A; CHEN JM, 1993, MON WEATHER REV, V121, P2631, DOI 10.1175/1520-0493(1993)121<2631:IOEEOF>2.0.CO;2; CHENG XH, 1995, J CLIMATE, V8, P1709, DOI 10.1175/1520-0442(1995)008<1709:ROLFCP>2.0.CO;2; Craddock JM, 1973, STATISCIAN, V22, P133, DOI 10.2307/2987365; Dommenget D, 2002, J CLIMATE, V15, P216, DOI 10.1175/1520-0442(2002)015<0216:ACNOTI>2.0.CO;2; EBDON RA, 1960, Q J ROY METEOR SOC, V86, P540, DOI 10.1002/qj.49708637011; ELSNER JB, 1996, SINGULAR SPECTRUM AN; Fukuoka A, 1951, GEOPHYS MAG, V22, P177; Gerber EP, 2005, J CLIMATE, V18, P2102, DOI 10.1175/JCLI3337.1; GIRSHICK MA, 1939, ANN MATH STAT, V43, P128; Golub G H, 1996, MATRIX COMPUTATION; Golyandina N., 2001, ANAL TIME SERIES STR; Graybill F.A., 1969, INTRO MATRICES APPL; GRAYSTONE P, 1959, METEOROL MAG, V88, P113; GREEN BF, 1977, MULTIVAR BEHAV RES, V12, P263, DOI 10.1207/s15327906mbr1203_1; Hannachi A, 2006, INT J CLIMATOL, V26, P7, DOI 10.1002/joc.1243; Hannachi A, 2001, Q J ROY METEOR SOC, V127, P939, DOI 10.1256/smsqj.57311; Hannan EJ, 1970, MULTIPLE TIME SERIES; HARDY DM, 1978, J APPL METEOROL, V17, P1153, DOI 10.1175/1520-0450(1978)017<1153:PCAOVW>2.0.CO;2; Harman H. H., 1976, MODERN FACTOR ANAL; Hausman R., 1982, OPTIMIZATION STAT, P137; HEINLEIN R., 1973, TIME ENOUGH LOVE; HENDON HH, 1994, J ATMOS SCI, V51, P2225, DOI 10.1175/1520-0469(1994)051<2225:TLCOTM>2.0.CO;2; Hirsch M.W., 1974, DIFFERENTIAL EQUATIO; Holton J. R., 1992, INTRO DYNAMIC METEOR; HOREL JD, 1981, MON WEATHER REV, V109, P2080, DOI 10.1175/1520-0493(1981)109<2080:ARPCAO>2.0.CO;2; HOREL JD, 1984, J CLIM APPL METEOROL, V23, P1660, DOI 10.1175/1520-0450(1984)023<1660:CPCATA>2.0.CO;2; Hotelling H, 1935, J EDUC PSYCHOL, V26, P139, DOI 10.1037/h0058165; Hsieh WW, 2001, TELLUS A, V53, P599, DOI 10.1034/j.1600-0870.2001.00251.x; Hurrell J. W., 2003, GEOPHYS MONOGR SER, V134, P1, DOI DOI 10.1029/134GM01; Hyvarinen A., 2000, NEURAL NETWORKS, V13, P4; Jackson J. E., 1991, USERS GUIDE PRINCIPA; JENKINS JM, 1968, SPECTRAL ANAL ITS AP; Jennrich RI, 2002, PSYCHOMETRIKA, V67, P7, DOI 10.1007/BF02294706; Jennrich RI, 2001, PSYCHOMETRIKA, V66, P289, DOI 10.1007/BF02294840; JOHNSON ES, 1993, J PHYS OCEANOGR, V23, P608, DOI 10.1175/1520-0485(1993)023<0608:SOIKWI>2.0.CO;2; Jolliffe I. T., 2002, PRINCIPAL COMPONENT; JOLLIFFE IT, 1987, J CLIMATOL, V7, P507; Jolliffe IT, 2002, CLIMATE RES, V20, P271, DOI 10.3354/cr020271; JOLLIFFE IT, 1995, J APPL STAT, V22, P29, DOI 10.1080/757584395; Kaplan D.T., 1995, UNDERSTANDING NONLIN; KIERS HAL, 1994, PSYCHOMETRIKA, V59, P567, DOI 10.1007/BF02294392; Kim KY, 1999, J CLIMATE, V12, P2076, DOI 10.1175/1520-0442(1999)012<2076:EBLPAE>2.0.CO;2; Kim KY, 1999, J CLIMATE, V12, P185, DOI 10.1175/1520-0442-12.1.185; Kim KY, 1996, J ATMOS SCI, V53, P1007, DOI 10.1175/1520-0469(1996)053<1007:EOODCT>2.0.CO;2; Kimoto M., 1991, P 8 C ATM OC WAV STA, P115; Kress R., 1970, Z ANGEW MATH MECH, V50, P61, DOI 10.1002/zamm.19700500125; Krishnamurti TN, 2003, Q J ROY METEOR SOC, V129, P2559, DOI 10.1256/qj.02.151; KNUTSON TR, 1987, MON WEATHER REV, V115, P1407, DOI 10.1175/1520-0493(1987)115<1407:DAOCLC>2.0.CO;2; Krzanowski W.J., 2000, PRINCIPLES MULTIVARI; KUNDU PK, 1976, J PHYS OCEANOGR, V6, P181, DOI 10.1175/1520-0485(1976)006<0181:STDCOL>2.0.CO;2; Kutzbach J. E., 1967, J APPL METEOROL, V6, P791, DOI 10.1175/1520-0450(1967)006<0791:EEOSLP>2.0.CO;2; LABITZKE K, 1999, STRATOSPHERE; LAWLEY DN, 1956, BIOMETRIKA, V43, P128; LEUNG LY, 1991, J CLIMATE, V4, P753, DOI 10.1175/1520-0442(1991)004<0753:AVOAZS>2.0.CO;2; Loeve M, 1978, PROBABILITY THEORY, V2; Lorenz E. N, 1970, J APPL METEOROL, V9, P325, DOI 10.1175/1520-0450(1970)009<lessthan>0325:CCAAMP<greaterthan>2.0.CO;2.; LORENZ EN, 1956, EMPIRICAL ORTHOGONAL, P49; MADDEN RA, 1994, MON WEATHER REV, V122, P814, DOI 10.1175/1520-0493(1994)122<0814:OOTDTO>2.0.CO;2; Magnus J. R., 1995, MATRIX DIFFERENTIAL; Mardia KV, 1979, MULTIVARIATE ANAL; MARUYAMA T, 1997, PAPERS METEOROLOGY G, V47, P1; Matthews AJ, 2000, Q J ROY METEOR SOC, V126, P2637, DOI 10.1002/qj.49712656902; MERRIFIELD MA, 1989, J GEOPHYS RES-OCEANS, V94, P18133, DOI 10.1029/JC094iC12p18133; MERRIFIELD MA, 1990, J PHYS OCEANOGR, V20, P1628, DOI 10.1175/1520-0485(1990)020<1628:DPSWCE>2.0.CO;2; Mestas-Nunez AM, 2000, INT J CLIMATOL, V20, P1509, DOI 10.1002/1097-0088(200010)20:12<1509::AID-JOC553>3.0.CO;2-Q; Monahan AH, 2001, J CLIMATE, V14, P219, DOI 10.1175/1520-0442(2001)013<0219:NPCATI>2.0.CO;2; MONAHAN AH, 1999, ATMOS OCEAN, V3, P241; Morrison DF, 1976, MULTIVARIATE STAT ME; NORTH GR, 1982, MON WEATHER REV, V110, P699, DOI 10.1175/1520-0493(1982)110<0699:SEITEO>2.0.CO;2; NORTH GR, 1984, J ATMOS SCI, V41, P879, DOI 10.1175/1520-0469(1984)041<0879:EOFANM>2.0.CO;2; Obukhov A. M., 1960, B ACAD SCI USSR GEOP, V1, P288; Obukhov A. M., 1947, USP MAT NAUK, V2, P196; OVERLAND JE, 1982, MON WEATHER REV, V110, P1, DOI 10.1175/1520-0493(1982)110<0001:ASTFPC>2.0.CO;2; PALMER CE, 1954, WEATHER, V9, P351; Panagiotopoulos F, 2005, J CLIMATE, V18, P1411, DOI 10.1175/JCLI3352.1; Pavan V, 2000, Q J ROY METEOR SOC, V126, P2125, DOI 10.1256/smsqj.56707; Peng SL, 1996, J CLIMATE, V9, P1824, DOI 10.1175/1520-0442(1996)009<1824:TCPBSL>2.0.CO;2; Preisendorfer R. W., 1988, PRINCIPAL COMPONENT; Priestley M., 1981, SPECTRAL ANAL TIME S; Ramsey J. O., 1997, FUNCTIONAL DATA ANAL; RASMUSSON EM, 1981, MON WEATHER REV, V109, P587, DOI 10.1175/1520-0493(1981)109<0587:BVISTO>2.0.CO;2; REED RJ, 1961, J GEOPHYS RES, V66, P813, DOI 10.1029/JZ066i003p00813; Reyment RA, 1996, APPL FACTOR ANAL NAT; RICHMAN MB, 1987, J CLIMATOL, V7, P511; RICHMAN MB, 1981, J APPL METEOROL, V20, P1145, DOI 10.1175/1520-0450(1981)020<1145:ORPCAI>2.0.CO;2; RICHMAN MB, 1986, J CLIMATOL, V6, P293; SALSTEIN DA, 1983, J ATMOS SCI, V40, P788, DOI 10.1175/1520-0469(1983)040<0788:MOVIAH>2.0.CO;2; SEAL HL, 1967, MULTIVARIATE STAT AN; SIMMONS AJ, 1983, J ATMOS SCI, V40, P1363, DOI 10.1175/1520-0469(1983)040<1363:BWPAIA>2.0.CO;2; SUN L, 2005, THESIS OPEN U MILTON; Takens F., 1981, LECT NOTES MATH, V898, P366, DOI DOI 10.1007/BFB0091924; THACKER WC, 1996, TELLUS A, V46, P584; THIIEBAUX HJ, 1984, J CLIM APPL METEOROL, V23, P800; Thomas J. B, 1969, INTRO STAT COMMUNICA; Thompson DWJ, 1998, GEOPHYS RES LETT, V25, P1297, DOI 10.1029/98GL00950; TRENBERTH KE, 1984, MON WEATHER REV, V112, P761, DOI 10.1175/1520-0493(1984)112<0761:QBFISL>2.0.CO;2; TRENBERTH KE, 1984, MON WEATHER REV, V112, P2359, DOI 10.1175/1520-0493(1984)112<2359:SEOFSS>2.0.CO;2; TRENDAFILOV NT, 2005, COMPUATIONAL STAT DA, V50, P242; van den Dool HM, 2000, J CLIMATE, V13, P1421, DOI 10.1175/1520-0442(2000)013<1421:EOT>2.0.CO;2; VINES SK, 2000, APPL STAT, V49, P441; vonStorch H, 1995, ANALYSIS OF CLIMATE VARIABILITY, P227; von Storch H., 1999, STAT ANAL CLIMATE RE; Wallace J. M., 1972, Journal of Applied Meteorology, V11, DOI 10.1175/1520-0450(1972)011<0893:EOROTS>2.0.CO;2; Wallace JM, 2002, J CLIMATE, V15, P1987, DOI 10.1175/1520-0442(2002)015<1987:TPCOAO>2.0.CO;2; Wallace J. M., 1972, Journal of Applied Meteorology, V11, DOI 10.1175/1520-0450(1972)011<0887:EOROTS>2.0.CO;2; WALSH JE, 1981, MON WEATHER REV, V109, P767, DOI 10.1175/1520-0493(1981)109<0767:SITABS>2.0.CO;2; WEARE BC, 1982, MON WEATHER REV, V110, P481, DOI 10.1175/1520-0493(1982)110<0481:EOEEOF>2.0.CO;2; WEBSTER PJ, 1988, J ATMOS SCI, V45, P803, DOI 10.1175/1520-0469(1988)045<0803:EEAAER>2.0.CO;2; WEIDEMAN JAC, 1995, MATH COMPUT, V64, P745, DOI 10.2307/2153449; Whittle P., 1951, HYPOTHESIS TESTING T; Wilks D. S., 2006, STAT METHODS ATMOSPH; Xinhua Cheng, 1995, Journal of Climate, V8	161	172	175	9	57	JOHN WILEY & SONS LTD	CHICHESTER	THE ATRIUM, SOUTHERN GATE, CHICHESTER PO19 8SQ, W SUSSEX, ENGLAND	0899-8418			INT J CLIMATOL	Int. J. Climatol.	JUL	2007	27	9					1119	1152		10.1002/joc.1499		34	Meteorology & Atmospheric Sciences	Meteorology & Atmospheric Sciences	191TC	WOS:000248153900001		
J	Wang, HS; Li, GD; Jiang, GH				Wang, Hansheng; Li, Guodong; Jiang, Guohua			Robust regression shrinkage and consistent variable selection through the LAD-lasso	JOURNAL OF BUSINESS & ECONOMIC STATISTICS			English	Article						LAD; LAD-lasso; Lasso; oracle property	MODEL SELECTION; ASYMPTOTIC THEORY; SMALL SAMPLES; ESTIMATORS; LIKELIHOOD; ARCH	The least absolute deviation (LAD) regression is a useful method for robust regression, and the least absolute shrinkage and selection operator (lasso) is a popular choice for shrinkage estimation and variable selection. In this article we combine these two classical ideas together to produce LAD-lasso. Compared with the LAD regression, LAD-lasso can do parameter estimation and variable selection simultaneously. Compared with the traditional lasso, LAD-lasso is resistant to heavy-tailed errors or outliers in the response. Furthermore, with easily estimated tuning parameters, the LAD-lasso estimator enjoys the same asymptotic efficiency as the unpenalized LAD estimator obtained under the true model (i.e., the oracle property). Extensive simulation studies demonstrate satisfactory finite-sample performance of LAD-lasso, and a real example is analyzed for illustration purposes.	Peking Univ, Guanghua Sch Management, Beijing 100871, Peoples R China; Univ Hong Kong, Dept Stat & Actuarial Sci, Hong Kong, Hong Kong, Peoples R China	Wang, HS (reprint author), Peking Univ, Guanghua Sch Management, Beijing 100871, Peoples R China.	hansheng@gsm.pku.edu.cn; ligd@hkusua.hku.hk; gjiang@gsm.pku.edu.cn	Li, Guodong/A-2741-2010	Li, Guodong/0000-0003-3137-8471			Akaike H., 1973, 2 INT S INF THEOR, P267; Knight K, 2000, ANN STAT, V28, P1356; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; HURVICH CM, 1989, BIOMETRIKA, V76, P297, DOI 10.1093/biomet/76.2.297; Tibshirani R, 2005, J ROY STAT SOC B, V67, P91, DOI 10.1111/j.1467-9868.2005.00490.x; SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136; BASSETT G, 1978, J AM STAT ASSOC, V73, P618, DOI 10.2307/2286611; Bloomfield P., 1983, LEAST ABSOLUTE DEVIA; Craven P, 1979, NUMER MATH, V31, P337; DAVIS RA, 1992, STOCH PROC APPL, V40, P145, DOI 10.1016/0304-4149(92)90142-D; HURVICH CM, 1990, STAT PROBABIL LETT, V9, P259, DOI 10.1016/0167-7152(90)90065-F; Knight K, 1998, ANN STAT, V26, P755; Koenker R, 1996, ECONOMET THEOR, V12, P793; Ling S., 2005, J ROYAL STAT SOC B, V67, P1; McQuarrie D.R., 1998, REGRESSION TIME SERI; Nissim D., 2001, REV ACCOUNT STUD, V6, P109, DOI DOI 10.1023/A:1011338221623; Peng L, 2003, BIOMETRIKA, V90, P967, DOI 10.1093/biomet/90.4.967; POLLARD D, 1991, ECONOMET THEOR, V7, P186; Shao J, 1997, STAT SINICA, V7, P221; Shi PD, 2002, J ROY STAT SOC B, V64, P237, DOI 10.1111/1467-9868.00335; Shi PD, 2004, J TIME SER ANAL, V25, P923, DOI 10.1111/j.1467-9892.2004.00385.x; ZOU H, 2004, DEGREES FREEDOM LASS	23	85	89	1	21	AMER STATISTICAL ASSOC	ALEXANDRIA	1429 DUKE ST, ALEXANDRIA, VA 22314 USA	0735-0015			J BUS ECON STAT	J. Bus. Econ. Stat.	JUL	2007	25	3					347	355		10.1198/073500106000000251		9	Economics; Social Sciences, Mathematical Methods; Statistics & Probability	Business & Economics; Mathematical Methods In Social Sciences; Mathematics	182GR	WOS:000247493600008		
J	Smyth, C; Coomans, D				Smyth, Christine; Coomans, Danny			Predictive weighting for cluster ensembles	JOURNAL OF CHEMOMETRICS			English	Article						post processing; cluster ensembles	MULTIVARIATE REGRESSION TREES; SELECTION	An ensemble of regression models predicts by taking a weighted average of the predictions made by individual models. Calculating the weights such that they reflect the accuracy of individual models (post processing the ensemble) has been shown to increase the ensemble's accuracy. However, post processing cluster ensembles has not received as much attention because of the inherent difficulty in assessing the accuracy of an individual cluster model. By enforcing the notion that clusters must be 'predictable', this paper suggests a means of implicitly post processing cluster ensembles by drawing analogies with regression post processing techniques. The product of the post processing procedure is an intelligently weighted co-occurrence matrix. A new technique, similarity-based k-means (SBK), is developed to split this matrix into clusters. The results using three real life datasets underpinned by chemical and biological phenomena show that splitting an intelligently weighted co-occurrence matrix gives accuracy that approaches supervised classification methods. Copyright (c) 2007 John Wiley & Sons, Ltd.	James Cook Univ N Queensland, Sch Math Phys & Informat Technol, Stat & Intelligent Data Anal Grp, Townsville, Qld 4811, Australia	Smyth, C (reprint author), James Cook Univ N Queensland, Sch Math Phys & Informat Technol, Stat & Intelligent Data Anal Grp, Townsville, Qld 4811, Australia.	Christine.Smyth@jcu.edu.au					Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Dudoit S, 2003, BIOINFORMATICS, V19, P1090, DOI 10.1093/bioinformatics/btg038; Yeung KY, 2001, BIOINFORMATICS, V17, P309, DOI 10.1093/bioinformatics/17.4.309; Breiman L., 1984, CLASSIFICATION REGRE; Coomans D., 1978, ANAL CHIM ACTA, V103, P409, DOI 10.1016/S0003-2670(01)83105-6; FERN XZ, 2006, CS063002 OR STAT U D; Fisher RA, 1936, ANN EUGENIC, V7, P179; Friedman J. H., 2003, IMPORTANCE SAMPLED L; GHOSH J, 2002, NSF WORKSH GEN DAT M; Greene D., 2004, ENSEMBLE CLUSTERING; Grotkjaer T, 2006, BIOINFORMATICS, V22, P58, DOI 10.1093/bioinformatics/bti746; HANCOCK T, 2006, THESIS JAMES COOK U; Hancock T, 2005, CHEMOMETR INTELL LAB, V76, P185, DOI 10.1016/j.chemolab.2004.11.001; Hastie T., 2001, ELEMENTS STAT LEARNI; Kaufman L, 1990, FINDING GROUPS DATA; Monti S, 2003, MACH LEARN, V52, P91, DOI 10.1023/A:1023949509487; NGUYEN HT, 1997, SPRUCE 4 INT C STAT; Qu YS, 2002, CLIN CHEM, V48, P1835; Questier F, 2005, CHEMOMETR INTELL LAB, V76, P45, DOI 10.1016/j.chemolab.2004.09.003; *R FDN STAT COMP, 2004, R A LANG ENV STAT CO; Satten GA, 2004, BIOINFORMATICS, V20, P3128, DOI 10.1093/bioinformatics/bth372; SEGAL MR, 1992, J AM STAT ASSOC, V87, P407, DOI 10.2307/2290271; Smyth C, 2006, PATTERN RECOGN, V39, P424, DOI 10.1016/j.patcog.2005.09.003; SMYTH C, 2006, 3 S INT STAT COMP SC; Smyth C, 2006, CHEMOMETR INTELL LAB, V80, P120, DOI 10.1016/j.chemolab.2005.09.001; Strehl A., 2002, J MACHINE LEARNING R, V3, P583, DOI DOI 10.1162/153244303321897735; Therneau TM, 2004, MULTIVARIATE PARTITI; Tibshirani R, 2005, J COMPUT GRAPH STAT, V14, P511, DOI 10.1198/106186005X59243	28	1	1	2	2	JOHN WILEY & SONS LTD	CHICHESTER	THE ATRIUM, SOUTHERN GATE, CHICHESTER PO19 8SQ, W SUSSEX, ENGLAND	0886-9383			J CHEMOMETR	J. Chemometr.	JUL-SEP	2007	21	7-9					364	375		10.1002/cem.1048		12	Automation & Control Systems; Chemistry, Analytical; Computer Science, Artificial Intelligence; Instruments & Instrumentation; Mathematics, Interdisciplinary Applications; Statistics & Probability	Automation & Control Systems; Chemistry; Computer Science; Instruments & Instrumentation; Mathematics	227DY	WOS:000250638600013		
J	Koh, KM; Kim, SJ; Boyd, S				Koh, Kwangmoo; Kim, Seung-Jean; Boyd, Stephen			An interior-point method for large-scale l(1)-regularized logistic regression	JOURNAL OF MACHINE LEARNING RESEARCH			English	Article						logistic regression; feature selection; l(1) regularization; regularization path; interiorpoint methods	VARIABLE SELECTION; MODEL SELECTION; LASSO; RECONSTRUCTION; DECONVOLUTION; MINIMIZATION; ALGORITHMS; SHRINKAGE; PATH	Logistic regression with l(1) regularization has been proposed as a promising method for feature selection in classification problems. In this paper we describe an efficient interior-point method for solving large-scale l(1)-regularized logistic regression problems. Small problems with up to a thousand or so features and examples can be solved in seconds on a PC; medium sized problems, with tens of thousands of features and examples, can be solved in tens of seconds (assuming some sparsity in the data). A variation on the basic method, that uses a preconditioned conjugate gradient method to compute the search step, can solve very large problems, with a million features and examples (e. g., the 20 Newsgroups data set), in a few minutes, on a PC. Using warm-start techniques, a good approximation of the entire regularization path can be computed much more efficiently than by solving a family of problems independently.	Stanford Univ, Dept Elect Engn, Informat Syst Lab, Stanford, CA 94305 USA	Koh, KM (reprint author), Stanford Univ, Dept Elect Engn, Informat Syst Lab, Stanford, CA 94305 USA.	DENEB1@STANFORD.EDU; SJKIM@STANFORD.EDU; BOYD@STANFORD.EDU					Allgower E., 1993, ACTA NUMERICA, V2, P1, DOI 10.1017/S0962492900002336; Tropp JA, 2006, IEEE T INFORM THEORY, V52, P1030, DOI 10.1109/TIT.2005.864420; Keerthi SS, 2005, J MACH LEARN RES, V6, P341; Yuan M, 2006, J ROY STAT SOC B, V68, P49, DOI 10.1111/j.1467-9868.2005.00532.x; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Krishnapuram B, 2005, IEEE T PATTERN ANAL, V27, P957, DOI 10.1109/TPAMI.2005.127; Alon U, 1999, P NATL ACAD SCI USA, V96, P6745, DOI 10.1073/pnas.96.12.6745; Zhao P, 2006, J MACH LEARN RES, V7, P2541; Zhu J, 2004, ADV NEUR IN, V16, P49; Candes EJ, 2006, IEEE T INFORM THEORY, V52, P489, DOI 10.1109/TIT.2005.862083; Osborne MR, 2000, IMA J NUMER ANAL, V20, P389, DOI 10.1093/imanum/20.3.389; Golub TR, 1999, SCIENCE, V286, P531, DOI 10.1126/science.286.5439.531; OLDENBURG DW, 1983, GEOPHYSICS, V48, P1318, DOI 10.1190/1.1441413; Hastie T, 2007, ELECTRON J STAT, V1, P1, DOI 10.1214/07-EJS004; Candes EJ, 2005, IEEE T INFORM THEORY, V51, P4203, DOI 10.1109/TIT.2005.858979; Zou H, 2006, J AM STAT ASSOC, V101, P1418, DOI 10.1198/016214506000000735; Tibshirani R, 2005, J ROY STAT SOC B, V67, P91, DOI 10.1111/j.1467-9868.2005.00490.x; Efron B, 2004, ANN STAT, V32, P407; BALAKRISHNAN S, 2006, UNPUB ALOGRITHMS SPA; Banerjee O., 2006, P 23 INT C MACH LEAR; Bertsekas D., 1999, NONLINEAR PROGRAMMIN; BHUSNURMATH A, 2007, MSCIS0710 CIS; Boyd S., 2001, Proceedings of ISPD'01. 2001 International Symposium on Physical Design, DOI 10.1145/369691.369734; BOYD S, 2006, IN PRESS OPTIMIZATIO; CANDES EJ, 2005, COMMUN PUR APPL MATH, V59, P1207; CHALONER K, 1989, J STAT PLAN INFER, V21, P191, DOI 10.1016/0378-3758(89)90004-9; Chen S., 1994, P 28 AS C SIGN SYST, V1, P41; Chen XY, 2001, PARENT-SCI PRACT, V1, P159, DOI 10.1207/S15327922PAR0103_01; CLAERBOU.JF, 1973, GEOPHYSICS, V38, P826, DOI 10.1190/1.1440378; Conn AR, 1992, LANCELOT FORTRAN PAC, V17; COX DR, 1972, J R STAT SOC B, V34, P187; DAHL J, 2005, UNPUB MAXIMUM LIKEIH; dAspermont A., 2005, ADV NEURAL INFORM PR, V17, P41; DEMBO RS, 1983, MATH PROGRAM, V26, P190, DOI 10.1007/BF02592055; Demmel J. W., 1997, APPL NUMERICAL LINEA; Donoho DL, 2003, P NATL ACAD SCI USA, V100, P2197, DOI 10.1073/pnas.0437847100; DONOHO DL, 1995, J ROY STAT SOC B MET, V57, P301; FRIEDMAN J, 2007, UNPUB PATHWISE COORD; Fu HY, 2006, SIAM J SCI COMPUT, V27, P1881, DOI 10.1137/040615079; GENKIN A, 2006, IN PRESS TECHNOMETRI; GEORGE A, 1981, COMPUTER SOLUTION LA; GILL P, 1986, 862 SOL STANF U OP R; GOLUB G, 1996, MATRIX COMPUTATIONS, V13; GOODMAN J, 2004, P ANN M ASS COMP LIN; HASSIBI A, 1999, P IEEE C DEC CONTR, P140; Hastie T., 2001, ELEMENTS STAT LEARNI; Hastie T, 2004, J MACH LEARN RES, V5, P1391; Jaakkola TS, 2000, STAT COMPUT, V10, P25, DOI 10.1023/A:1008932416310; Kim Y, 2006, STAT SINICA, V16, P375; KOMAREK P, 2004, THESIS CAMEGIE MELLO; Lang K., 1995, P 12 INT C MACH LEAR, P331; LEE S, 2003, P 21 NATL C ARTIFICI; LEVY S, 1981, GEOPHYSICS, V46, P1235, DOI 10.1190/1.1441261; Lewis A., 2000, CONVEX ANAL NONLINEA; LIN CJ, 2007, IN PRESS 24 INT C MA; LOBO M, 2005, ANN OPERATIONS RES; LOKHORST J, 1999, LASSO GENERALISED LI; Luenberger D. G., 1984, LINEAR NONLINEAR PRO; MCCALLUM A, 1996, BOW TOOLOKIT STAT LA; McCullagh P., 1989, GENERALIZED LINEAR M, VSecond; MINKA T, 2003, COMPARISON NUMERICAL; *MOSEK APS, 2002, MOSEK OPT TOOLS VERS; Nash SG, 2000, J COMPUT APPL MATH, V124, P45, DOI 10.1016/S0377-0427(00)00426-X; Nesterov Y. E., 1994, INTERIOR POINT POLYN; Newman D., 1998, UCI REPOSITORY MACHI; Ng A. Y., 2004, P 21 INT C MACH LEAR, P78, DOI DOI 10.1145/1015330.1015435; PARK MY, 2006, IN PRESS J ROYAL STA; PARK MY, 2006, UNPUB REGULARIZATION; Perkins S., 2003, P 20 INT C MACH LEAR, P592; Polyak B. T., 1987, INTRO OPTIMIZATION; Portugal LF, 2000, NETWORKS, V35, P91, DOI 10.1002/(SICI)1097-0037(200003)35:2<91::AID-NET1>3.0.CO;2-T; Rosset S, 2004, J MACH LEARN RES, V5, P941; ROSSET S, 2007, IN PRESS ANN STAT; Rosset S., 2005, ADV NEURAL INFORM PR, V17; Roth V, 2004, IEEE T NEURAL NETWOR, V15, P16, DOI 10.1109/TNN.2003.809398; Ruszczynski A., 2006, NONLINEAR OPTIMIZATI; Ryan T.P., 1997, MODERN REGRESSION ME; Shevade SK, 2003, BIOINFORMATICS, V19, P2246, DOI 10.1093/bioinformatics/btg308; Shor N. Z., 1985, SPRINGER SERIES COMP; TAYLOR HL, 1979, GEOPHYSICS, V44, P39, DOI 10.1190/1.1440921; Tibshirani R, 1997, STAT MED, V16, P385, DOI 10.1002/(SICI)1097-0258(19970228)16:4<385::AID-SIM380>3.0.CO;2-3; Vandenberghe L., 2004, CONVEX OPTIMIZATION; VANDENBERGHE L, 1995, MATH PROGRAM, V69, P205, DOI 10.1007/BF01585558; VANDERBEI R, 1997, LOQU USERS MANUAL VE; WAINWRIGHT M, 2007, IN PRESS ADV NEURAL, V19; Wright SJ, 1997, PRIMAL DUAL INTERIOR; Wright SJ, 1999, SPRINGER SERIES OPER; Ye Y., 1997, INTERIOR POINT ALGOR; ZHANG Z, 2004, P 21 INT C MACH LEAR, P927; ZHAO P, 2007, 703 STAT DEP; Zou H., 2006, J COMPUTATIONAL GRAP, V15, P262; ZOU H, 2007, IN PRESS ANN STAT	93	159	169	2	10	MICROTOME PUBLISHING	BROOKLINE	31 GIBBS STREET, BROOKLINE, MA 02446 USA	1532-4435			J MACH LEARN RES	J. Mach. Learn. Res.	JUL	2007	8						1519	1555				37	Automation & Control Systems; Computer Science, Artificial Intelligence	Automation & Control Systems; Computer Science	208XY	WOS:000249353700006		
J	Tan, M; Tian, GL; Fang, HB; Ng, KW				Tan, Ming; Tian, Guo-Liang; Fang, Hong-Bin; Ng, Kai Wang			A fast EM algorithm for quadratic optimization subject to convex constraints	STATISTICA SINICA			English	Article						bootstrap; Cholesky decomposition; constrained optimization; convergence rate; data augmentation; EM algorithm; latent variables; working parameter	MAXIMUM-LIKELIHOOD-ESTIMATION; DUAL BASES ALGORITHM; ECM ALGORITHM; REGRESSION; RESTRICTIONS; PARAMETERS; VARIABLES	Convex constraints (CCs) such as box constraints and linear inequality constraints appear frequently in statistical inference and in applications. The problems of quadratic optimization (QO) subject to CCs occur in isotonic regression, shape-restricted non-parametric regression, variable selection (via the lasso algorithm and bridge regression), limited dependent variables models, image reconstruction, and so on. Existing packages for QO are not generally applicable to CCs. Although EM-type algorithms may be applied to such problems (Tian, Ng and Tan (2005)), the convergence rate/speed of these algorithms is painfully slow, especially for high-dimensional data. This paper develops a fast EM algorithm for QO with CCs. We construct a class of data augmentation schemes indexed by a 'working parameter' r (r is an element of R), and then optimize r over R under several convergence criteria. In addition, we use Cholesky decomposition to reduce both the number of latent variables and the dimension, leading to further acceleration of the EM. Standard errors of the restricted estimators are calculated using a non-parametric bootstrapping procedure. Simulation and comparison are performed and a complex multinomial dataset is analyzed to illustrate the proposed methods.	Univ Maryland, Greenebaum Canc Ctr, Div Biostat, Baltimore, MD 21201 USA; Univ Hong Kong, Dept Stat & Actuarial Sci, Hong Kong, Hong Kong, Peoples R China	Tan, M (reprint author), Univ Maryland, Greenebaum Canc Ctr, Div Biostat, 22 S Greene St, Baltimore, MD 21201 USA.	mtan@umm.edu; gtian2@umm.edu; hfang@umm.edu; kw.ng@hkuspace.hku.hk	Ng, Kai Wang/D-3114-2009				BREIMAN L, 1995, TECHNOMETRICS, V37, P373, DOI 10.2307/1269730; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1; Meng XL, 1997, J ROY STAT SOC B MET, V59, P511, DOI 10.1111/1467-9868.00082; Coleman TF, 1996, SIAM J OPTIMIZ, V6, P1040, DOI 10.1137/S1052623494240456; DYKSTRA RL, 1983, J AM STAT ASSOC, V78, P837, DOI 10.2307/2288193; DYKSTRA RL, 1982, ANN STAT, V10, P708, DOI 10.1214/aos/1176345866; FRASER DAS, 1989, SCAND J STAT, V16, P65; Gill P. E., 1981, PRACTICAL OPTIMIZATI; GREEN PJ, 1990, J ROY STAT SOC B MET, V52, P443; Hajivassiliou VA, 1998, ECONOMETRICA, V66, P863, DOI 10.2307/2999576; HILDRETH C, 1954, J AM STAT ASSOC, V49, P598, DOI 10.2307/2281132; KIM DK, 1995, J AM STAT ASSOC, V90, P708, DOI 10.2307/2291083; Kuhn H. W., 1951, P 2 BERK S MATH STAT; Lange K., 1999, NUMERICAL ANAL STAT; Liu CH, 2000, J AM STAT ASSOC, V95, P109, DOI 10.2307/2669531; Liu CH, 1998, BIOMETRIKA, V85, P755, DOI 10.1093/biomet/85.4.755; MENG XL, 1993, BIOMETRIKA, V80, P267, DOI 10.2307/2337198; MENG XL, 1994, ANN STAT, V22, P326, DOI 10.1214/aos/1176325371; Meyer MC, 1999, J STAT PLAN INFER, V81, P13, DOI 10.1016/S0378-3758(99)00025-7; NYQUIST H, 1991, APPL STAT-J ROY ST C, V40, P133, DOI 10.2307/2347912; Robertson T, 1988, ORDER RESTRICTED STA; Shi NZ, 2005, J MULTIVARIATE ANAL, V92, P53, DOI 10.1016/S0047-259X(03)00134-9; SILVERMAN BW, 1990, J ROY STAT SOC B MET, V52, P271; TAN M, 2003, DEV MODERN STAT RELA, P53; TIAN GL, 2005, ANN I STAT MATH; TITTERINGTON DM, 1985, ASTRON ASTROPHYS, V144, P381; VARDI Y, 1993, J ROY STAT SOC B MET, V55, P569; WOLFE P, 1959, ECONOMETRICA, V27, P382, DOI 10.2307/1909468; WOLLAN PC, 1987, APPL STAT-J ROY ST C, V36, P234, DOI 10.2307/2347557	30	6	6	2	4	STATISTICA SINICA	TAIPEI	C/O DR H C HO, INST STATISTICAL SCIENCE, ACADEMIA SINICA, TAIPEI 115, TAIWAN	1017-0405			STAT SINICA	Stat. Sin.	JUL	2007	17	3					945	964				20	Statistics & Probability	Mathematics	202RT	WOS:000248921600008		
J	Zhang, JL; Lin, MT; Liu, JS; Chen, R				Zhang, Junni L.; Lin, Ming T.; Liu, Jun S.; Chen, Rong			Lookahead and piloting strategies for variable selection	STATISTICA SINICA			English	Article						AIC; Akaike information criterion; BIC; Bayesian information criterion; gene regulation; Gibbs sampler; microarray data; sequential Monte Carlo; TFBM; transcription factor binding-site motif	MONTE-CARLO METHODS; MULTIPLE-REGRESSION; FADING CHANNELS; MODEL SELECTION; DISCOVERY; CHOICE	The traditional variable selection problem has attracted renewed attention from statistical researchers due to the recent advances in data collection, especially in fields such as bioinformatics and marketing. In this paper, we formulate regression variable selection as an optimization problem, propose and study several deterministic and stochastic sequential optimization methods with lookahead. Using several synthetic examples, we show that the stochastic sequential method with lookahead robustly and significantly outperforms a few close competitors, including the popular stepwise methods. When applied to analyze a yeast amino acid starvation microarray experiment, this method can find many transcription factors that are known to be important for yeast to cope with stress and starvation.	Peking Univ, Guanghua Sch Management, Dept Business Stat & Econometr, Beijing 100871, Peoples R China; Univ Illinois, Coll Business Adm, Dept Informat & Decis Sci MC 294, Chicago, IL 60607 USA; Harvard Univ, Dept Stat, Cambridge, MA 02138 USA	Zhang, JL (reprint author), Peking Univ, Guanghua Sch Management, Dept Business Stat & Econometr, Beijing 100871, Peoples R China.	zjn@gsm.pku.edu.cn; linming@uic.edu; jliu@stat.harvard.edu; rongchen@uic.edu					AKAIKE H, 1981, J ECONOMETRICS, V16, P3, DOI 10.1016/0304-4076(81)90071-3; Gasch AP, 2000, MOL BIOL CELL, V11, P4241; CARLIN BP, 1995, J ROY STAT SOC B MET, V57, P473; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; FURNIVAL GM, 1974, TECHNOMETRICS, V16, P499, DOI 10.2307/1267601; Conlon EM, 2003, P NATL ACAD SCI USA, V100, P3339, DOI 10.1073/pnas.0630591100; HOERL AE, 1970, TECHNOMETRICS, V12, P55; Liu JS, 1998, J AM STAT ASSOC, V93, P1032, DOI 10.2307/2669847; GEORGE EI, 1993, J AM STAT ASSOC, V88, P881, DOI 10.2307/2290777; Efron B, 2004, ANN STAT, V32, P407; SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136; Liang FM, 2000, STAT SINICA, V10, P317; Berger JO, 1996, J AM STAT ASSOC, V91, P109, DOI 10.2307/2291387; MALLOWS CL, 1973, TECHNOMETRICS, V15, P661, DOI 10.2307/1267380; Fernandez C, 2001, J ECONOMETRICS, V100, P381, DOI 10.1016/S0304-4076(00)00076-2; ROSENBLUTH MN, 1955, J CHEM PHYS, V23, P356; BIAO X, 2004, U CALIFORNIA BERKELE; Blaiseau PL, 1997, MOL CELL BIOL, V17, P3640; Chen R, 2000, IEEE T INFORM THEORY, V46, P2079; EFFROYMSON MA, 1960, MATH METHODS DIGITAL; FOSTER DP, 1994, ANN STAT, V22, P1947, DOI 10.1214/aos/1176325766; Jensen ST, 2004, STAT SCI, V19, P188, DOI 10.1214/088342304000000107; KIRKPATRICK S, 1983, SCIENCE, V220, P671, DOI 10.1126/science.220.4598.671; LAUD PW, 1995, J ROY STAT SOC B MET, V57, P247; LINDLEY DV, 1968, J R STAT SOC B, V30, P31; Liu J. S., 2001, MONTE CARLO STRATEGI; Zhang JL, 2002, J CHEM PHYS, V117, P3492, DOI 10.1063/1.1494415; Liu XS, 2002, NAT BIOTECHNOL, V20, P835, DOI 10.1038/nbt717; MEIROVITCH H, 1985, PHYS REV A, V32, P3699, DOI 10.1103/PhysRevA.32.3699; MEIROVITCH H, 1982, J PHYS A, V15, P735; OHAGAN A, 1995, J ROY STAT SOC B MET, V57, P99; Wang XD, 2002, IEEE T SIGNAL PROCES, V50, P241	32	2	2	1	2	STATISTICA SINICA	TAIPEI	C/O DR H C HO, INST STATISTICAL SCIENCE, ACADEMIA SINICA, TAIPEI 115, TAIWAN	1017-0405			STAT SINICA	Stat. Sin.	JUL	2007	17	3					985	1003				19	Statistics & Probability	Mathematics	202RT	WOS:000248921600010		
J	Schumacher, M; Binder, H; Gerds, T				Schumacher, Martin; Binder, Harald; Gerds, Thomas			Assessment of survival prediction models based on microarray data	BIOINFORMATICS			English	Article							GENE-EXPRESSION DATA; B-CELL LYMPHOMA; CROSS-VALIDATION; COX REGRESSION; PROGNOSTIC CLASSIFICATION; ROC CURVES; ERROR RATE	Motivation: In the process of developing risk prediction models, various steps of model building and model selection are involved. If this process is not adequately controlled, overfitting may result in serious overoptimism leading to potentially erroneous conclusions. Methods: For right censored time-to-event data, we estimate the prediction error for assessing the performance of a risk prediction model (Gerds and Schumacher, 2006; Graf et al., 1999). Furthermore, resampling methods are used to detect overfitting and resulting overoptimism and to adjust the estimates of prediction error (Gerds and Schumacher, 2007). Results: We show how and to what extent the methodology can be used in situations characterized by a large number of potential predictor variables where overfitting may be expected to be overwhelming. This is illustrated by estimating the prediction error of some recently proposed techniques for fitting a multivariate Cox regression model applied to the data of a prognostic study in patients with diffuse large-B-cell lymphoma (DLBCC).	Univ Med Ctr Freiburg, Inst Med Biometry & Med Informat, Dep Med Biometry & Stat, Freiburg, Germany; Univ Freiburg, Freiburg Ctr Data Anal & Model Bldg, Freiburg, Germany	Schumacher, M (reprint author), Univ Med Ctr Freiburg, Inst Med Biometry & Med Informat, Dep Med Biometry & Stat, Freiburg, Germany.	sec@imbi.uni-freiburg.de	Binder, Harald/C-7413-2009	Binder, Harald/0000-0002-5666-8662			ANDERSEN PK, 1993, STAT BASED COUNTING; Gui J, 2005, BIOINFORMATICS, V21, P3001, DOI 10.1093/bioinformatics/bti422; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; EFRON B, 1983, J AM STAT ASSOC, V78, P316, DOI 10.2307/2288636; Graf E, 1999, STAT MED, V18, P2529; Molinaro AM, 2005, BIOINFORMATICS, V21, P3301, DOI 10.1093/bioinformatics/bti499; Simon R, 2005, J CLIN ONCOL, V23, P7332, DOI 10.1200/JCO.2005.02.8712; Simon R, 2003, J NATL CANCER I, V95, P14; Efron B, 1997, J AM STAT ASSOC, V92, P548, DOI 10.2307/2965703; HOERL AE, 1970, TECHNOMETRICS, V12, P55; KORN EL, 1991, AM STAT, V45, P201, DOI 10.2307/2684290; Gerds TA, 2006, BIOMETRICAL J, V48, P1029, DOI 10.1002/bimj.200610301; Efron B, 2004, ANN STAT, V32, P407; Heagerty PJ, 2005, BIOMETRICS, V61, P92, DOI 10.1111/j.0006-341X.2005.030814.x; Heagerty PJ, 2000, BIOMETRICS, V56, P337, DOI 10.1111/j.0006-341X.2000.00337.x; Rosenwald A, 2002, NEW ENGL J MED, V346, P1937, DOI 10.1056/NEJMoa012914; Braga-Neto UM, 2004, BIOINFORMATICS, V20, P374, DOI 10.1093/bioinformatics/btg419; Brier G.W., 1950, MONTHLY WEATHER REVI, V78, P1, DOI DOI 10.1175/1520-0493(1950)078<0001:VOFEIT>2.0.CO;2; COX DR, 1972, J R STAT SOC B, V34, P187; Fu WJJ, 2005, BIOINFORMATICS, V21, P1979, DOI 10.1093/bioinformatics/bti294; GARTHWAITE PH, 1994, J AM STAT ASSOC, V89, P122, DOI 10.2307/2291207; GERDS TA, 2007, BIOMETRICS; Gerds TA, 2001, BIOMETRIKA, V88, P572, DOI 10.1093/biomet/88.2.572; Kattan M, 2002, J CLIN ONCOL, V20, P885; LI H, 2004, BIOINFORMATICS S1, V20, P1208; PARK M. Y., 2006, REGULARIZATION PATH; Schumacher M, 2006, HDB STAT CLIN ONCOLO, P289; Segal MR, 2006, BIOSTATISTICS, V7, P268, DOI 10.1093/biostatistics/kxj006; VAN DER LAAN M. J., 2003, UNIFIED METHODS CENS; van Houwelingen HC, 2006, STAT MED, V25, P3201, DOI 10.1002/sim.2353; Wehberg S, 2004, BIOMETRICAL J, V46, P35, DOI 10.1002/bimj.20041001; Wold H., 1966, MULTIVARIATE ANAL, P391	32	50	52	1	5	OXFORD UNIV PRESS	OXFORD	GREAT CLARENDON ST, OXFORD OX2 6DP, ENGLAND	1367-4803			BIOINFORMATICS	Bioinformatics	JUL 15	2007	23	14					1768	1774		10.1093/bioinformatics/btm232		7	Biochemical Research Methods; Biotechnology & Applied Microbiology; Computer Science, Interdisciplinary Applications; Mathematical & Computational Biology; Statistics & Probability	Biochemistry & Molecular Biology; Biotechnology & Applied Microbiology; Computer Science; Mathematical & Computational Biology; Mathematics	207JV	WOS:000249248300008	17485430	
J	Tai, F; Pan, W				Tai, Feng; Pan, Wei			Incorporating prior knowledge of predictors into penalized classifiers with multiple penalty terms	BIOINFORMATICS			English	Article							GENE-EXPRESSION DATA; BREAST-CANCER; SHRUNKEN CENTROIDS; PROSTATE-CANCER; CLASSIFICATION; REGRESSION; MICROARRAYS; PHENOTYPES; SELECTION; LASSO	Motivation: In the context of sample (e.g. tumor) classifications with microarray gene expression data, many methods have been proposed. However, almost all the methods ignore existing biological knowledge and treat all the genes equally a priori. On the other hand, because some genes have been identified by previous studies to have biological functions or to be involved in pathways related to the outcome (e.g. cancer), incorporating this type of prior knowledge into a classifier can potentially improve both the predictive performance and interpretability of the resulting model. Results: We propose a simple and general framework to incorporate such prior knowledge into building a penalized classifier. As two concrete examples, we apply the idea to two penalized classifiers, nearest shrunken centroids (also called PAM) and penalized partial least squares (PPLS) Instead of treating all the genes equally a priori as in standard penalized methods, we group the genes according to their functional associations based on existing biological knowledge or data, and adopt group-specific penalty terms and penalization parameters. Simulated and real data examples demonstrate that, if prior knowledge on gene grouping is indeed informative, our new methods perform better than the two standard penalized methods. yielding higher predictive accuracy and screening out more irrelevant genes.	Univ Minnesota, Sch Publ Hlth, Div Biostat, Minneapolis, MN 55455 USA	Pan, W (reprint author), Univ Minnesota, Sch Publ Hlth, Div Biostat, A460 Mayo Bldg MMC 303, Minneapolis, MN 55455 USA.	weip@biostat.umn.edu					Al-Shahrour F, 2005, BIOINFORMATICS, V21, P2988, DOI 10.1093/bioinformatics/bti457; BREIMAN L, 1995, TECHNOMETRICS, V37, P373, DOI 10.2307/1269730; Gui J, 2005, BIOINFORMATICS, V21, P3001, DOI 10.1093/bioinformatics/bti422; Bhattacharjee A, 2001, P NATL ACAD SCI USA, V98, P13790, DOI 10.1073/pnas.191502998; Nguyen DV, 2002, BIOINFORMATICS, V18, P39, DOI 10.1093/bioinformatics/18.1.39; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324; Yuan M, 2007, J ROY STAT SOC B, V69, P143, DOI 10.1111/j.1467-9868.2007.00581.x; Welsh JB, 2001, CANCER RES, V61, P5974; Wang YX, 2005, LANCET, V365, P671, DOI 10.1016/S0140-6736(05)17947-1; Ambroise C, 2002, P NATL ACAD SCI USA, V99, P6562, DOI 10.1073/pnas.102102699; Golub TR, 1999, SCIENCE, V286, P531, DOI 10.1126/science.286.5439.531; Ashburner M, 2000, NAT GENET, V25, P25; Dabney AR, 2005, BIOINFORMATICS, V21, P4148, DOI 10.1093/bioinformatics/bti681; Huang E, 2003, LANCET, V361, P1590, DOI 10.1016/S0140-6736(03)13308-9; Tibshirani R, 2003, STAT SCI, V18, P104, DOI 10.1214/ss/1056397488; Zou H, 2006, J AM STAT ASSOC, V101, P1418, DOI 10.1198/016214506000000735; Singh D, 2002, CANCER CELL, V1, P203, DOI 10.1016/S1535-6108(02)00030-2; Cai TT, 1999, ANN STAT, V27, P898, DOI 10.1214/aos/1018031262; Tibshirani R, 2002, P NATL ACAD SCI USA, V99, P6567, DOI 10.1073/pnas.082099299; Broet P, 2006, BIOINFORMATICS, V22, P1477, DOI 10.1093/bioinformatics/btl110; Cheng Jill, 2004, J Biopharm Stat, V14, P687, DOI 10.1081/BIP-200025659; Dopazo J, 2006, OMICS, V10, P398, DOI 10.1089/omi.2006.10.398; FANG, 2006, J BIOMEDICAL INFORMA, V39, P401; Hastie T., 2001, ELEMENTS STAT LEARNI; Huang DS, 2006, BIOINFORMATICS, V22, P1259, DOI 10.1093/bioinformatics/btl065; Huang XH, 2003, BIOINFORMATICS, V19, P2072, DOI 10.1093/bioinformatics/btg283; Kanehisa M, 1996, SCI TECHNOLOGY JAPAN, V59; Lottaz C, 2005, BIOINFORMATICS, V21, P1971, DOI 10.1093/bioinformatics/bti292; Pan W, 2005, STAT APPL GENET MO B, V4; Pan W, 2006, BIOINFORMATICS, V22, P795, DOI 10.1093/bioinformatics/btl011; Pang H, 2006, BIOINFORMATICS, V22, P2028, DOI 10.1093/bioinformatics/btl344; Wold H., 1966, MULTIVARIATE ANAL, P391	33	27	27	2	3	OXFORD UNIV PRESS	OXFORD	GREAT CLARENDON ST, OXFORD OX2 6DP, ENGLAND	1367-4803			BIOINFORMATICS	Bioinformatics	JUL 15	2007	23	14					1775	1782		10.1093/bioinformatics/btm234		8	Biochemical Research Methods; Biotechnology & Applied Microbiology; Computer Science, Interdisciplinary Applications; Mathematical & Computational Biology; Statistics & Probability	Biochemistry & Molecular Biology; Biotechnology & Applied Microbiology; Computer Science; Mathematical & Computational Biology; Mathematics	207JV	WOS:000249248300009	17483507	
J	Kaban, A				Kaban, Ata			On Bayesian classification with Laplace priors	PATTERN RECOGNITION LETTERS			English	Article						Laplace prior; variational Bayes; sparsity; shrinkage effect; predictive features; microarray gene expressions	GENE SELECTION; REGRESSION; REGULARIZATION; LASSO	We present a new classification approach, using a variational Bayesian estimation of probit regression with Laplace priors. Laplace priors have been previously used extensively as a sparsity-inducing mechanism to perform feature selection simultaneously with classification or regression. However, contrarily to the 'myth' of sparse Bayesian learning with Laplace priors, we find that the sparsity effect is due to a property of the maximum a posteriori (MAP) parameter estimates only. The Bayesian estimates, in turn, induce a posterior weighting rather than a hard selection of features, and has different advantageous properties: (1) it provides better estimates of the prediction uncertainty; (2) it is able to retain correlated features favouring generalisation; (3) it is more stable with respect to the hyperparameter choice and (4) it produces a weight-based ranking of the features, suited for interpretation.. We analyse the behaviour of the Bayesian estimate in comparison with its MAP counterpart, as well as other related models, (a) through a graphical interpretation of the associated shrinkage and (b) by controlled numerical simulations in a range of testing conditions. The results pinpoint the situations when the advantages of Bayesian estimates are feasible to exploit. Finally, we demonstrate the working of our method in a gene expression classification task. (C) 2007 Elsevier B.V. All rights reserved.	Univ Birmingham, Sch Comp Sci, Birmingham B15 2TT, W Midlands, England	Kaban, A (reprint author), Univ Birmingham, Sch Comp Sci, Birmingham B15 2TT, W Midlands, England.	A.Kaban@cs.bham.ac.uk					Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Alon U, 1999, P NATL ACAD SCI USA, V96, P6745, DOI 10.1073/pnas.96.12.6745; Tusher VG, 2001, P NATL ACAD SCI USA, V98, P5116, DOI 10.1073/pnas.091062498; Guyon I, 2002, MACH LEARN, V46, P389, DOI 10.1023/A:1012487302797; Zou H, 2005, J ROY STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x; Tipping ME, 2001, J MACH LEARN RES, V1, P211, DOI 10.1162/15324430152748236; Yeung KY, 2005, BIOINFORMATICS, V21, P2394, DOI 10.1093/bioinformatics/bti319; Efron B, 2004, ANN STAT, V32, P407; Bernardo J. M., 1994, BAYESIAN THEORY; BISHOP CM, 2000, VARIATIONAL RELEVANC; CAWLEY GC, 2006, GENE SELECTION CANC; CHU W, 2005, BIOINFORMATICS; DIAZURIATE R, 2005, DATA ANAL VISUALISAT; Fawcett T., 2004, ROC GRAPHS NOTES PRA; Figueiredo M., 2003, IEEE T PATTERN ANAL, V25; Goutte C, 1997, NEURAL NETWORKS, V10, P1053, DOI 10.1016/S0893-6080(97)00027-0; JU WH, 2002, BAYESIAN LEARNING SP; Li Y, 2002, BIOINFORMATICS, V18, P1332, DOI 10.1093/bioinformatics/18.10.1332; Mackay D. J. C., 2003, INFORM THEORY INFERE; Moulin P, 1999, IEEE T INFORM THEORY, V45, P909, DOI 10.1109/18.761332; Neal R. M., 1996, LECT NOTES STAT, V1; PARK T, UNPUB BAYESIAN LASSO; QI Y, 2004, P INT C MACH LEARN; Roth V, 2004, IEEE T NEURAL NETWOR, V15, P16, DOI 10.1109/TNN.2003.809398; Shevade SK, 2003, BIOINFORMATICS, V19, P2246, DOI 10.1093/bioinformatics/btg308	25	18	19	0	2	ELSEVIER SCIENCE BV	AMSTERDAM	PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS	0167-8655			PATTERN RECOGN LETT	Pattern Recognit. Lett.	JUL 15	2007	28	10					1271	1282		10.1016/j.patrec.2007.02.010		12	Computer Science, Artificial Intelligence	Computer Science	178VX	WOS:000247249300018		
J	Bunea, F; Tsybakov, AB; Wegkamp, MH				Bunea, Florentina; Tsybakov, Alexandre B.; Wegkamp, Marten H.			Aggregation for gaussian regression	ANNALS OF STATISTICS			English	Article						aggregation; lasso estimator; minimax risk; model selection; model averaging; nonparametric regression; oracle inequalities; penalized least squares	MODEL SELECTION; NONPARAMETRIC REGRESSION; ADAPTIVE REGRESSION; ORACLE INEQUALITIES; COMPLEXITIES; ESTIMATORS; BOUNDS; LASSO	This paper studies statistical aggregation procedures in the regression setting. A motivating factor is the existence of many different methods of estimation, leading to possibly competing estimators. We consider here three different types of aggregation: model selection (MS) aggregation, convex (C) aggregation and linear (L) aggregation. The objective of (MS) is to select the optimal single estimator from the list; that of (C) is to select the optimal convex combination of the given estimators; and that of (L) is to select the optimal linear combination of the given estimators. we are interested in evaluating the rates of convergence of the excess risks of the estimators obtained by these procedures. Our approach is motivated by recently published minimax results [Nemirovski, A. (2000). Topics in non-parametric statistics. Lectures on Probability Theory and Statistics (Saint-Flour, 1998). Lecture Notes in Math. 1738 85-277. Springer, Berlin; Tsybakov, A. B. (2003). Optimal rates of aggregation. Learning Theory and Kernel Machines. Lecture Notes in Artificial Intelligence 2777 303-313. Springer, Heidelberg]. There exist competing aggregation procedures achieving optimal convergence rates for each of the (MS), (C) and (L) cases separately. Since these procedures are not directly comparable with each other, we suggest an alternative solution. We prove that all three optimal rates, as well as those for the newly introduced (S) aggregation (subset selection), are nearly achieved via a single "universal" aggregation procedure. The procedure consists of mixing the initial estimators with weights obtained by penalized least squares. Two different penalties are considered: one of them is of the BIC type, the second one is a data-dependent l(1)-type penalty.	Florida State Univ, Dept Stat, Tallahassee, FL 32306 USA; Univ Paris 06, Lab Probabil & Models Aleatoires, F-75252 Paris 05, France	Bunea, F (reprint author), Florida State Univ, Dept Stat, Tallahassee, FL 32306 USA.	bunea@stat.fsu.edu; tsybakov@ccrjussieu.fr; wegkamp@stat.fsu.edu					AKAIKE H, 1974, IEEE T AUTOMAT CONTR, VAC19, P716, DOI 10.1109/TAC.1974.1100705; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Antoniadis A, 2001, J AM STAT ASSOC, V96, P939, DOI 10.1198/016214501753208942; Chen SSB, 2001, SIAM REV, V43, P129, DOI 10.1137/S003614450037906X; Efron B, 2004, ANN STAT, V32, P407; SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136; MALLOWS CL, 1973, TECHNOMETRICS, V15, P661, DOI 10.2307/1267380; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P6, DOI 10.1109/TIT.2005.860430; Audibert JY, 2004, ANN I H POINCARE-PR, V40, P685, DOI 10.1016/j.anihpb.2003.11.006; Baraud Y, 2000, PROBAB THEORY REL, V117, P467, DOI 10.1007/PL00008731; Baraud Y., 2002, ESAIM-PROBAB STAT, V6, P127, DOI 10.1051/ps:2002007; Barron A, 1999, PROBAB THEORY REL, V113, P301, DOI 10.1007/s004400050210; BARRON AR, 1993, IEEE T INFORM THEORY, V39, P930, DOI 10.1109/18.256500; BARTLETT PL, 2000, P 13 ANN C COMP LEAR, P286; Birge L., 2001, J EUR MATH SOC, V3, P203, DOI 10.1007/s100970100031; Birge L, 2006, ANN I H POINCARE-PR, V42, P273, DOI 10.1016/j.anihpb.2005.04.004; BUNEA F, 2004, PREPUBLICATION, V948; Bunea F, 2004, ANN STAT, V32, P898, DOI 10.1214/009053604000000247; BUNEA F, 2005, M984 FLORIDA STATE U; Catoni O., 2004, LECT NOTES MATH, V1851; Cavalier L, 2002, ANN STAT, V30, P843; Devroye L, 1996, PROBABILISTIC THEORY; FOSTER DP, 1994, ANN STAT, V22, P1947, DOI 10.1214/aos/1176325766; GILBERT EN, 1952, AT&T TECH J, V31, P504; Gyorfi L, 2002, DISTRIBUTION FREE TH; HARDLE W, 1998, WAVELETS APPROXIMATI, V129; Juditsky A, 2000, ANN STAT, V28, P681; JUDITSKY A, 2005, LEARNING MIRROR AVEA; Juditsky A. B., 2005, Problems of Information Transmission, V41, DOI 10.1007/s11122-006-0005-2; KNEIP A, 1994, ANN STAT, V22, P835, DOI 10.1214/aos/1176325498; Koltchinskii V, 2006, ANN STAT, V34, P2593, DOI 10.1214/009053606000001019; Leung G, 2006, IEEE T INFORM THEORY, V52, P3396, DOI 10.1109/TIT.2006.878172; Loubes JM, 2002, STAT NEERL, V56, P454, DOI 10.1111/1467-9574.00212; Lugosi G, 1999, ANN STAT, V27, P1830; Nemirovski A., 2000, LECT NOTES MATH, V1738, P85; Osborne MR, 2000, J COMPUT GRAPH STAT, V9, P319, DOI 10.2307/1390657; Rao CR, 2001, MODEL SELECTION; SCHAPIRE RE, 1990, MACH LEARN, V5, P197, DOI 10.1023/A:1022648800760; Tsybakov AB, 2003, LECT NOTES ARTIF INT, V2777, P303, DOI 10.1007/978-3-540-45167-9_23; Tsybakov A.B., 2004, INTRO ESTIMATION NON; Wegkamp M, 2003, ANN STAT, V31, P252, DOI 10.1214/aos/1046294464; Yang Y, 2000, J MULTIVARIATE ANAL, V74, P135, DOI 10.1006/jmva.1999.1884; Yang YH, 2001, J AM STAT ASSOC, V96, P574, DOI 10.1198/016214501753168262; Yang YH, 2004, BERNOULLI, V10, P25, DOI 10.3150/bj/1077544602	44	99	99	0	7	INST MATHEMATICAL STATISTICS	BEACHWOOD	PO BOX 22718, BEACHWOOD, OH 44122 USA	0090-5364			ANN STAT	Ann. Stat.	AUG	2007	35	4					1674	1697		10.1214/009053606000001587		24	Statistics & Probability	Mathematics	212BG	WOS:000249568000013		
J	Wang, H; Li, R; Tsai, CL				Wang, Hansheng; Li, Runze; Tsai, Chih-Ling			Tuning parameter selectors for the smoothly clipped absolute deviation method	BIOMETRIKA			English	Article						AIC; BIC; generalized crossvalidation; least absolute shrinkage and selection operator; smoothly clipped absolute deviation	MODEL SELECTION; LINEAR-MODEL; SEMIPARAMETRIC REGRESSION; LIKELIHOOD	The penalized least squares approach with smoothly clipped absolute deviation penalty has been consistently demonstrated to be an attractive regression shrinkage and selection method. It not only automatically and consistently selects the important variables, but also produces estimators which are as efficient as the oracle estimator. However, these attractive features depend on appropriate choice of the tuning parameter. We show that the commonly used generalized crossvalidation cannot select the tuning parameter satisfactorily, with a nonignorable overfitting effect in the resulting model. In addition, we propose a BIC tuning parameter selector, which is shown to be able to identify the true model consistently. Simulation studies are presented to support theoretical findings, and an empirical example is given to illustrate its use in the Female Labor Supply data.	Peking Univ, Guanghua Sch Math, Beijing 100871, Peoples R China; Penn State Univ, Dept Stat, University Pk, PA 16802 USA; Penn State Univ, Methodol Ctr, University Pk, PA 16802 USA; Univ Calif Davis, Grad Sch Management, Davis, CA 95616 USA	Wang, HS (reprint author), Peking Univ, Guanghua Sch Math, Beijing 100871, Peoples R China.	hansheng@gsm.pku.edu.cn; rli@stat.psu.edu; cltsai@ucdavis.edu	Li, Runze/C-5444-2013	Li, Runze/0000-0002-0154-2202			Akaike H., 1973, P 2 INT S INF THEOR, P267; ENGLE RF, 1986, J AM STAT ASSOC, V81, P310, DOI 10.2307/2289218; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; Fan JQ, 2005, BERNOULLI, V11, P1031, DOI 10.3150/bj/1137421639; SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136; Breiman L, 1996, ANN STAT, V24, P2350; Bunea F, 2004, CAN J STAT, V32, P105, DOI 10.2307/3315936; Bunea F, 2004, ANN STAT, V32, P898, DOI 10.1214/009053604000000247; Craven P, 1979, NUMER MATH, V31, P337; Fan JQ, 1998, ANN STAT, V26, P943; Fan JQ, 2004, J AM STAT ASSOC, V99, P710, DOI 10.1198/0162145040000001060; Gijbels I., 1996, LOCAL POLYNOMIAL MOD; Hardle W, 2000, PARTIALLY LINEAR MOD; HECKMAN NE, 1986, J ROY STAT SOC B MET, V48, P244; Huang JHZ, 2004, J ROY STAT SOC B, V66, P463, DOI 10.1111/j.1369-7412.2004.05500.x; MACK YP, 1982, Z WAHRSCHEINLICHKEIT, V61, P405, DOI 10.1007/BF00539840; McQuarrie D.R., 1998, REGRESSION TIME SERI; ROBINSON PM, 1988, ECONOMETRICA, V56, P931, DOI 10.2307/1912705; Ruppert D, 1995, J AM STAT ASSOC, V90, P1257, DOI 10.2307/2291516; Shao J, 1997, STAT SINICA, V7, P221; Shi PD, 2002, J ROY STAT SOC B, V64, P237, DOI 10.1111/1467-9868.00335; Shi PD, 2004, J TIME SER ANAL, V25, P923, DOI 10.1111/j.1467-9892.2004.00385.x; SPECKMAN P, 1988, J ROY STAT SOC B MET, V50, P413; YANG Y, 2005, BIOMETRIKA, V92, P973; Yatchew A, 1997, ECON LETT, V57, P135, DOI 10.1016/S0165-1765(97)00218-8	26	208	223	3	12	OXFORD UNIV PRESS	OXFORD	GREAT CLARENDON ST, OXFORD OX2 6DP, ENGLAND	0006-3444			BIOMETRIKA	Biometrika	AUG	2007	94	3					553	568		10.1093/biomet/asm053		16	Biology; Mathematical & Computational Biology; Statistics & Probability	Life Sciences & Biomedicine - Other Topics; Mathematical & Computational Biology; Mathematics	216WK	WOS:000249909500004		
J	Li, LX				Li, Lexin			Sparse sufficient dimension reduction	BIOMETRIKA			English	Article						lasso; shrinkage sparse estimator; sufficient dimension reduction	SLICED INVERSE REGRESSION; PRINCIPAL HESSIAN DIRECTIONS; VISUALIZATION; SHRINKAGE; LASSO	Existing sufficient dimension reduction methods suffer from the fact that each dimension reduction component is a linear combination of all the original predictors, so that it is difficult to interpret the resulting estimates. We propose a unified estimation strategy, which combines a regression-type formulation of sufficient dimension reduction methods and shrinkage estimation, to produce sparse and accurate solutions. The method can be applied to most existing sufficient dimension reduction methods such as sliced inverse regression, sliced average variance estimation and principal Hessian directions. We demonstrate the effectiveness of the proposed method by both simulations and real data analysis.	N Carolina State Univ, Dept Stat, Raleigh, NC 27695 USA	Li, LX (reprint author), N Carolina State Univ, Dept Stat, Raleigh, NC 27695 USA.	li@stat.ncsu.edu					Akaike H., 1973, 2 INT S INF THEOR, P267; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; [Anonymous], 2004, ANN STAT, V32, P1061; Cook RD, 1998, J AM STAT ASSOC, V93, P84; Jolliffe IT, 2003, J COMPUT GRAPH STAT, V12, P531, DOI 10.1198/1061860032148; Efron B, 2004, ANN STAT, V32, P407; Chen CH, 1998, STAT SINICA, V8, P289; Chiaromonte F, 2002, ANN STAT, V30, P475, DOI 10.1214/aos/1021379862; COOK R. D., 1998, REGRESSION GRAPHICS; Cook RD, 2001, AUST NZ J STAT, V43, P147, DOI 10.1111/1467-842X.00164; Cook RD, 2005, J AM STAT ASSOC, V100, P410, DOI 10.1198/016214504000001501; COOK RD, 1991, J AM STAT ASSOC, V86, P328, DOI 10.2307/2290564; Cook RD, 1996, J AM STAT ASSOC, V91, P983, DOI 10.2307/2291717; Cook RD, 2000, J AM STAT ASSOC, V95, P781, DOI 10.2307/2669462; Cook RD, 2002, ANN STAT, V30, P455, DOI 10.1214/aos/1021379861; Fung WK, 2002, STAT SINICA, V12, P1093; LI KC, 1992, J AM STAT ASSOC, V87, P1025, DOI 10.2307/2290640; LI KC, 1991, J AM STAT ASSOC, V86, P316, DOI 10.2307/2290563; Li LX, 2006, TECHNOMETRICS, V48, P503, DOI 10.1198/004017006000000129; Ni LQ, 2005, BIOMETRIKA, V92, P242, DOI 10.1093/biomet/92.1.242; Setodji CM, 2004, TECHNOMETRICS, V46, P421, DOI 10.1198/004017004000000437; Ye ZS, 2003, J AM STAT ASSOC, V98, P968, DOI 10.1198/016214503000000927; Yin XR, 2002, J R STAT SOC B, V64, P159, DOI 10.1111/1467-9868.00330; Zou H, 2006, J COMPUT GRAPH STAT, V15, P265, DOI 10.1198/106186006X113430; ZOU H, 2007, IN PRESS ANN STAT	25	40	41	3	6	OXFORD UNIV PRESS	OXFORD	GREAT CLARENDON ST, OXFORD OX2 6DP, ENGLAND	0006-3444			BIOMETRIKA	Biometrika	AUG	2007	94	3					603	613		10.1093/biomet/asm044		11	Biology; Mathematical & Computational Biology; Statistics & Probability	Life Sciences & Biomedicine - Other Topics; Mathematical & Computational Biology; Mathematics	216WK	WOS:000249909500007		
J	Zhang, HH; Lu, WB				Zhang, Hao Helen; Lu, Wenbin			Adaptive lasso for Cox's proportional hazards model	BIOMETRIKA			English	Article						adaptive lasso; lasso; penalized partial likelihood; proportional hazards model; variable selection	BAYESIAN VARIABLE SELECTION; CENSORED SURVIVAL-DATA; REGRESSION-MODEL; WAVELET SHRINKAGE; LARGE SAMPLE; LIKELIHOOD	We investigate the variable selection problem for Cox's proportional hazards model, and propose a unified model selection and estimation procedure with desired theoretical properties and computational convenience. The new method is based on a penalized log partial likelihood with the adaptively weighted L-1 penalty on regression coefficients, providing what we call the adaptive Lasso estimator. The method incorporates different penalties for different coefficients: unimportant variables receive larger penalties than important ones, so that important variables tend to be retained in the selection process, whereas unimportant variables are more likely to be dropped. Theoretical properties, such as consistency and rate of convergence of the estimator, are studied. We also show that, with proper choice of regularization parameters, the proposed estimator has the oracle properties. The convex optimization nature of the method leads to an efficient algorithm. Both simulated and real examples show that the method performs competitively.	N Carolina State Univ, Dept Stat, Raleigh, NC 27695 USA	Zhang, HH (reprint author), N Carolina State Univ, Dept Stat, Raleigh, NC 27695 USA.	hzhang@stat.ncsu.edu; lu@stat.ncsu.edu					ANDERSEN PK, 1982, ANN STAT, V10, P1100, DOI 10.1214/aos/1176345976; Fu WJJ, 1998, J COMPUT GRAPH STAT, V7, P397, DOI 10.2307/1390712; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Antoniadis A, 2001, J AM STAT ASSOC, V96, P939, DOI 10.1198/016214501753208942; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; COX DR, 1975, BIOMETRIKA, V62, P269, DOI 10.1093/biomet/62.2.269; Efron B, 2004, ANN STAT, V32, P407; DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425; CRAVEN P, 1979, NUMER MATH, V31, P377; SAUERBREI W, 1992, STAT MED, V11, P2093, DOI 10.1002/sim.4780111607; TSIATIS AA, 1981, ANN STAT, V9, P93, DOI 10.1214/aos/1176345335; BRESLOW N, 1974, BIOMETRICS, V30, P89, DOI 10.2307/2529620; COX DR, 1972, J R STAT SOC B, V34, P187; DICKSON ER, 1989, HEPATOLOGY, V10, P1, DOI 10.1002/hep.1840100102; Donoho DL, 1998, ANN STAT, V26, P879; Fan JQ, 2002, ANN STAT, V30, P74; Faraggi D, 1998, BIOMETRICS, V54, P1475, DOI 10.2307/2533672; Ibrahim JG, 1999, CAN J STAT, V27, P701, DOI 10.2307/3316126; Therneau TM, 2000, MODELING SURVIVAL DA; Tibshirani R, 1997, STAT MED, V16, P385, DOI 10.1002/(SICI)1097-0258(19970228)16:4<385::AID-SIM380>3.0.CO;2-3; Vandenberghe L., 2004, CONVEX OPTIMIZATION	21	134	140	4	15	OXFORD UNIV PRESS	OXFORD	GREAT CLARENDON ST, OXFORD OX2 6DP, ENGLAND	0006-3444			BIOMETRIKA	Biometrika	AUG	2007	94	3					691	703		10.1093/biomet/asm037		13	Biology; Mathematical & Computational Biology; Statistics & Probability	Life Sciences & Biomedicine - Other Topics; Mathematical & Computational Biology; Mathematics	216WK	WOS:000249909500013		
J	Cui, YH; Fu, WJ; Sun, KL; Romero, R; Wu, RL				Cui, Yuehua; Fu, Wenjiang; Sun, Kelian; Romero, Roberto; Wu, Rongling			Mapping nucleotide sequences that encode complex binary disease traits with HapMap	CURRENT GENOMICS			English	Article						nucleotide sequence; complex disease; EM algorithm; logistic regression; haplotype	LINKAGE DISEQUILIBRIUM; UNRELATED INDIVIDUALS; GENOTYPE DATA; CANDIDATE GENE; HUMAN GENOME; ASSOCIATION; HAPLOTYPES; TESTS; COMPLICATIONS; DETERMINANTS	Detecting the patterns of DNA sequence variants across the human genome is a crucial step for unraveling the genetic basis of complex human diseases. The human HapMap constructed by single nucleotide polymorphisms (SNPs) provides efficient sequence variation information that can speed up the discovery of genes related to common diseases. In this article, we present a generalized linear model for identifying specific nucleotide variants that encode complex human diseases. A novel approach is derived to group haplotypes to form composite diplotypes, which largely reduces the model degrees of freedom for an association test and hence increases the power when multiple SNP markers are involved. An efficient two-stage estimation procedure based on the expectation-maximization (EM) algorithm is derived to estimate parameters. Non-genetic environmental or clinical risk factors can also be fitted into the model. Computer simulations show that our model has reasonable power and type I error rate with appropriate sample size. It is also suggested through simulations that a balanced design with approximately equal number of cases and controls should be preferred to maintain small estimation bias and reasonable testing power. To illustrate the utility, we apply the method to a genetic association study of large for gestational age (LGA) neonates. The model provides a powerful tool for elucidating the genetic basis of complex binary diseases.	[Cui, Yuehua] Michigan State Univ, Dept Stat & Probabil, E Lansing, MI 48824 USA; [Romero, Roberto] NIH, NICHD, Perinatol Res Branch, Detroit, MI 48201 USA; [Wu, Rongling] Univ Florida, Dept Stat, Gainesville, FL 32611 USA; [Fu, Wenjiang; Sun, Kelian] Michigan State Univ, Dept Epidemiol, E Lansing, MI 48824 USA	Cui, YH (reprint author), Michigan State Univ, Dept Stat & Probabil, E Lansing, MI 48824 USA.	cui@stt.msu.edu	Cui, Yuehua/A-3529-2008				Agresti A., 2002, CATEGORICAL DATA ANA; AKAIKE H, 1974, IEEE T AUTOMAT CONTR, VAC19, P716, DOI 10.1109/TAC.1974.1100705; Akey J, 2001, EUR J HUM GENET, V9, P291, DOI 10.1038/sj.ejhg.5200619; Zhao JH, 2000, HUM HERED, V50, P133, DOI 10.1159/000022901; Zaykin DV, 2002, HUM HERED, V53, P79, DOI 10.1159/000057986; Morris RW, 2002, GENET EPIDEMIOL, V23, P221, DOI 10.1002/gepi.10200; Hao K, 2004, HUM MOL GENET, V13, P683, DOI 10.1093/hmg/ddh091; Gabriel SB, 2002, SCIENCE, V296, P2225, DOI 10.1126/science.1069424; Whitaker RC, 1998, J PEDIATR-US, V132, P768, DOI 10.1016/S0022-3476(98)70302-6; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Benjamini Y, 2001, BEHAV BRAIN RES, V125, P279, DOI 10.1016/S0166-4328(01)00297-2; Barrett JC, 2005, BIOINFORMATICS, V21, P263, DOI 10.1093/bioinformatics/bth457; Schaid DJ, 2002, AM J HUM GENET, V70, P425, DOI 10.1086/338688; Epstein MP, 2003, AM J HUM GENET, V73, P1316, DOI 10.1086/380204; HOERL AE, 1970, TECHNOMETRICS, V12, P55; EXCOFFIER L, 1995, MOL BIOL EVOL, V12, P921; Dietz WH, 2004, NEW ENGL J MED, V350, P855, DOI 10.1056/NEJMp048008; Risch N, 1996, SCIENCE, V273, P1516, DOI 10.1126/science.273.5281.1516; Stephens M, 2001, AM J HUM GENET, V68, P978, DOI 10.1086/319501; Kramer MS, 2002, J PEDIATR-US, V141, P538, DOI 10.1067/mpg.2002.128029; Gibbs RA, 2003, NATURE, V426, P789, DOI 10.1038/nature02168; BONNEY GE, 1986, BIOMETRICS, V42, P611, DOI 10.2307/2531211; Botstein D, 2003, NAT GENET, V33, P228, DOI 10.1038/ng1090; Clark AG, 2004, GENET EPIDEMIOL, V27, P321, DOI 10.1002/gepi.20025; CUI YH, UNPUB ASSYMPTOTIC DI; Dawson E, 2002, NATURE, V418, P544, DOI 10.1038/nature00864; Dempster A. P., 1977, J R STAT SOC, V9, P1; DENG W, 2005, GENETICS, V172, P1349, DOI 10.1534/genetics.105.047241; Greenspan G., 2003, P 7 ANN INT C RES CO, P131, DOI 10.1145/640075.640092; Greenwald P, 1996, J Cell Biochem Suppl, V25, P29; Altshuler D, 2005, NATURE, V437, P1299, DOI 10.1038/nature04226; Ke K, 2003, BIOINFORMATICS, V19, P287; Lake SL, 2003, HUM HERED, V55, P56, DOI 10.1159/000071811; LAZER S, 1986, J REPROD MED, V31, P501; Lemeshow S., 1989, APPL LOGISTIC REGRES; Liu Tian, 2004, Genetics, V168, P503, DOI 10.1534/genetics.104.029603; Lou XY, 2003, GENETICS, V163, P1533; Lynch M., 1998, GENETICS ANAL QUANTI; McIntyre LM, 2001, GENET RES, V78, P79, DOI 10.1017/S0016672301005092; Meeuwisse G, 1998, Lakartidningen, V95, P5488; MESHARI AA, 1990, INT J GYNECOL OBSTET, V32, P215, DOI 10.1016/0020-7292(90)90348-O; OLSON JM, 1994, AM J HUM GENET, V55, P574; Patil N, 2001, SCIENCE, V294, P1719, DOI 10.1126/science.1065573; Risch NJ, 2000, NATURE, V405, P847, DOI 10.1038/35015718; Schaid DJ, 2004, GENET EPIDEMIOL, V27, P348, DOI 10.1002/gepi.20037; SPELLACY WN, 1985, OBSTET GYNECOL, V66, P158; SPINKA C, 2005, GENET EPIDEMIOL, V29, P649; Stram DO, 2003, HUM HERED, V55, P179, DOI 10.1159/000073202; STRITTMATTER WJ, 1995, P NATL ACAD SCI USA, V92, P4725, DOI 10.1073/pnas.92.11.4725; Tan QH, 2005, GENET RES, V86, P223, DOI 10.1017/S0016672305007792; Xu SZ, 1996, GENETICS, V143, P1417; Zhang K, 2002, P NATL ACAD SCI USA, V99, P7335, DOI 10.1073/pnas.102186799; Zhao JY, 2005, AM J HUM GENET, V77, P27, DOI 10.1086/431243	53	7	9	1	1	BENTHAM SCIENCE PUBL LTD	SHARJAH	EXECUTIVE STE Y26, PO BOX 7917, SAIF ZONE, 1200 BR SHARJAH, U ARAB EMIRATES	1389-2029			CURR GENOMICS	Curr. Genomics	AUG	2007	8	5					307	322		10.2174/138920207782446188		16	Biochemistry & Molecular Biology; Genetics & Heredity	Biochemistry & Molecular Biology; Genetics & Heredity	242OI	WOS:000251734700002	19384427	
J	Ribbing, J; Nyberg, J; Caster, O; Jonsson, EN				Ribbing, Jakob; Nyberg, Joakim; Caster, Ola; Jonsson, E. Niclas			The lasso - a novel method for predictive covariate model building in nonlinear mixed effects models	JOURNAL OF PHARMACOKINETICS AND PHARMACODYNAMICS			English	Article						least absolute shrinkage and selection operator; covariate analysis; model validation; model evaluation; data splitting; predictive modelling; mixed effects modelling	LOGISTIC-REGRESSION ANALYSIS; SMALL DATA SETS; POPULATION PHARMACOKINETICS; EXTERNAL VALIDATION; SELECTION; BOOTSTRAP; NONMEM; PERFORMANCE; BIAS; VARIABILITY	Covariate models for population pharmacokinetics and pharmacodynamics are often built with a stepwise covariate modelling procedure (SCM). When analysing a small dataset this method may produce a covariate model that suffers from selection bias and poor predictive performance. The lasso is a method suggested to remedy these problems. It may also be faster than SCM and provide a validation of the covariate model. The aim of this study was to implement the lasso for covariate selection within NONMEM and to compare this method to SCM. In the lasso all covariates must be standardised to have zero mean and standard deviation one. Subsequently, the model containing all potential covariate-parameter relations is fitted with a restriction: the sum of the absolute covariate coefficients must be smaller than a value, t. The restriction will force some coefficients towards zero while the others are estimated with shrinkage. This means in practice that when fitting the model the covariate relations are tested for inclusion at the same time as the included relations are estimated. For a given SCM analysis, the model size depends on the P-value required for selection. In the lasso the model size instead depends on the value of t which can be estimated using cross-validation. The lasso was implemented as an automated tool using PsN. The method was compared to SCM in 16 scenarios with different dataset sizes, number of investigated covariates and starting models for the covariate analysis. Hundred replicate datasets were created by resampling from a PK-dataset consisting of 721 stroke patients. The two methods were compared primarily on the ability to predict external data, estimate their own predictive performance (external validation), and on the computer run-time. In all 16 scenarios the lasso predicted external data better than SCM with any of the studied P-values (5%, 1% and 0.1%), but the benefit was negligible for large datasets. The lasso cross-validation provided a precise and nearly unbiased estimate of the actual prediction error. On a single processor, the lasso was faster than SCM. Further, the lasso could run completely in parallel whereas SCM must run in steps. In conclusion, the lasso is superior to SCM in obtaining a predictive covariate model on a small dataset or on small subgroups (e.g. rare genotype). Run in parallel the lasso could be much faster than SCM. Using cross-validation, the lasso provides a validation of the covariate model and does not require the user to specify a P-value for selection.	Uppsala Univ, Dept Pharmaceut Biosci, Div Pharmacokinet & Drug Therapy, S-75124 Uppsala, Sweden; F Hoffmann La Roche & Co Ltd, CH-4002 Basel, Switzerland	Ribbing, J (reprint author), Uppsala Univ, Dept Pharmaceut Biosci, Div Pharmacokinet & Drug Therapy, Box 591, S-75124 Uppsala, Sweden.	jakob.ribbing@farmbio.uu.se		Caster, Ola/0000-0002-2259-1716			*ADM FAD, 1999, GUID IND POP PHARM; ALTMAN DG, 1989, STAT MED, V8, P771, DOI 10.1002/sim.4780080702; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Green B, 2004, BRIT J CLIN PHARMACO, V58, P119, DOI 10.1111/j.1365-2125.2004.02157.x; Janmahasatian S, 2005, CLIN PHARMACOKINET, V44, P1051, DOI 10.2165/00003088-200544100-00004; Efron B, 1997, J AM STAT ASSOC, V92, P548, DOI 10.2307/2965703; Lindbom L, 2004, COMPUT METH PROG BIO, V75, P85, DOI 10.1016/j.cmpb.2003.11.003; Lindbom L, 2005, COMPUT METH PROG BIO, V79, P241, DOI 10.1016/j.cmpb.2005.04.005; Efron B, 2004, ANN STAT, V32, P407; Steyerberg EW, 1999, J CLIN EPIDEMIOL, V52, P935, DOI 10.1016/S0895-4356(99)00103-1; BIES RR, 2006, J PHARMACOKINET PHAR, V32, P195; Bleeker SE, 2003, J CLIN EPIDEMIOL, V56, P826, DOI 10.1016/S0895-4356(03)00207-5; BREIMAN L, 1992, INT STAT REV, V60, P291, DOI 10.2307/1403680; Brendel K, 2006, PHARM RES, V23, P2036, DOI 10.1007/s11095-006-9067-5; Bruno R, 1996, J PHARMACOKINET BIOP, V24, P153, DOI 10.1007/BF02353487; LECESSIE S, 1992, APPL STAT-J ROY ST C, V41, P191; HARRELL FE, 2001, REGRESSION MODELING, P90; Harrell Jr FE, 2001, REGRESSION MODELING, P207; Hastie T., 2001, ELEMENTS STAT LEARNI; Hastie T., 2001, ELEMENTS STAT LEARNI, P214; Hjorth J. S. U., 1994, COMPUTER INTENSIVE S; Holford NHG, 1996, CLIN PHARMACOKINET, V30, P329; Jonsson EN, 1998, PHARMACEUT RES, V15, P1463, DOI 10.1023/A:1011970125687; KARLSSON MO, 1993, J PHARMACOKINET BIOP, V21, P735, DOI 10.1007/BF01113502; Kowalski KG, 2001, J PHARMACOKINET PHAR, V28, P253, DOI 10.1023/A:1011579109640; MANDEMA JW, 1992, J PHARMACOKINET BIOP, V20, P511, DOI 10.1007/BF01061469; Matthews I, 2004, BRIT J CLIN PHARMACO, V58, P8, DOI 10.1111/j.1365-2125.2004.02114.x; Mentre F, 2006, J PHARMACOKINET PHAR, V33, P345, DOI 10.1007/s10928-005-0016-4; OLOFSEN E, 2006, USING LASSO SIMULTAN, P15; ONEILL R, 1971, J ROY STAT SOC B, V33, P218; PICARD RR, 1990, AM STAT, V44, P140, DOI 10.2307/2684155; RIBBING J, 2001, CROSS MODEL VALIDATI, P10; Ribbing J, 2004, J PHARMACOKINET PHAR, V31, P109, DOI 10.1023/B:JOPA.0000034404.86036.72; Rodgers JL, 1999, MULTIVAR BEHAV RES, V34, P441, DOI 10.1207/S15327906MBR3404_2; Sauerbrei W, 1999, J ROY STAT SOC C-APP, V48, P313, DOI 10.1111/1467-9876.00155; Shao J, 1996, J AM STAT ASSOC, V91, P655, DOI 10.2307/2291661; SHEINER LB, 1981, J PHARMACOKINET BIOP, V9, P503, DOI 10.1007/BF01060893; Steyerberg EW, 2000, STAT MED, V19, P1059, DOI 10.1002/(SICI)1097-0258(20000430)19:8<1059::AID-SIM412>3.3.CO;2-S; Steyerberg EW, 2003, J CLIN EPIDEMIOL, V56, P441, DOI 10.1016/S0895-4356(03)00047-7; VANHOUWELINGEN JC, 1990, STAT MED, V9, P1303, DOI 10.1002/sim.4780091109; WADE JR, 1994, J PHARMACOKINET BIOP, V22, P165, DOI 10.1007/BF02353542; Wahlby U, 2001, J PHARMACOKINET PHAR, V28, P231, DOI 10.1023/A:1011527125570; Wahlby U, 2002, AAPS PHARMSCI, V4; Yano Y, 2001, J PHARMACOKINET PHAR, V28, P171, DOI 10.1023/A:1011555016423; Zingmark PH, 2003, BRIT J CLIN PHARMACO, V56, P173, DOI 10.1046/j.0306-5251.2003.01850.x	45	15	16	2	5	SPRINGER/PLENUM PUBLISHERS	NEW YORK	233 SPRING ST, NEW YORK, NY 10013 USA	1567-567X			J PHARMACOKINET PHAR	J. Pharmacokinet. Pharmacodyn.	AUG	2007	34	4					485	517		10.1007/s10928-007-9057-1		33	Pharmacology & Pharmacy	Pharmacology & Pharmacy	189RS	WOS:000248006800003	17516152	
J	Schmolck, A; Everson, R				Schmolck, Alexander; Everson, Richard			Smooth relevance vector machine: a smoothness prior extension of the RVM	MACHINE LEARNING			English	Article						sparse regression; kernel regression; smoothness prior; relevance vector machine	WAVELET SHRINKAGE; REGRESSION	Enforcing sparsity constraints has been shown to be an effective and efficient way to obtain state-of-the-art results in regression and classification tasks. Unlike the support vector machine (SVM) the relevance vector machine (RVM) explicitly encodes the criterion of model sparsity as a prior over the model weights. However the lack of an explicit prior structure over the weight variances means that the degree of sparsity is to a large extent controlled by the choice of kernel (and kernel parameters). This can lead to severe overfitting or oversmoothing-possibly even both at the same time (e.g. for the multiscale Doppler data). We detail an efficient scheme to control sparsity in Bayesian regression by incorporating a flexible noise-dependent smoothness prior into the RVNI. We present an empirical evaluation of the effects of choice of prior structure on a selection of popular data sets and elucidate the link between Bayesian wavelet shrinkage and RVM regression. Our model encompasses the original RVM as a special case, but our empirical results show that we can surpass RVNI performance in terms of goodness of fit and achieved sparsity as well as computational performance in many cases. The code is freely available.	Univ Exeter, Sch Engn Comp Sci & Math, Exeter EX4 4QF, Devon, England	Everson, R (reprint author), Univ Exeter, Sch Engn Comp Sci & Math, Exeter EX4 4QF, Devon, England.	a.schmolck@gmail.com; r.m.everson@ex.ac.uk					Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; HOERL AE, 1970, TECHNOMETRICS, V12, P55; Tipping ME, 2001, J MACH LEARN RES, V1, P211, DOI 10.1162/15324430152748236; DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425; Chipman HA, 1997, J AM STAT ASSOC, V92, P1413, DOI 10.2307/2965411; Arfken G., 1985, MATH METHODS PHYS; Bernardo J. M., 1994, BAYESIAN THEORY; Bishop C., 2000, P 16 C UNC ART INT, P46; Bobin J., 2005, P SPARS05, P103; Clarkson E, 2001, OPT LETT, V26, P1253, DOI 10.1364/OL.26.001253; Daubechies I., 1992, 10 LECT WAVELETS; Denison D. G. T., 2002, BAYESIAN METHODS NON; DSOUZA A, 2004, P INT C MACH LEARN I; FAUL A, 2002, ADV NEURAL INFORM PR, V14; Figueiredo MAT, 2003, IEEE T PATTERN ANAL, V25, P1150, DOI 10.1109/TPAMI.2003.1227989; FOKOUE E, 2004, PRIOR CONSISTENT EST; GIROLAMI M, 2005, 22 INT C MACH LEARN, P241; Golub G., 1989, MATRIX COMPUTATIONS; Hastie TJ, 1990, GEN ADDITIVE MODELS; HOLMES CC, 1999, BAYESIAN STAT, V6, P769; Jansen M, 2001, NOISE REDUCTION WAVE; Lam EY, 2000, IEEE T IMAGE PROCESS, V9, P1661, DOI 10.1109/83.869177; MACKAY DJC, 1992, NEURAL COMPUT, V4, P720, DOI 10.1162/neco.1992.4.5.720; Mallat S., 1999, WAVELET TOUR SIGNAL; QUINONEROCANDEL.J, 2004, THESIS TU DENMARK LY; Roweis S., 1999, MATRIX IDENTITIES; SCHMOLCK A, 2005, WORKSH SIGN PROC AD; Scholkopf B., 2002, LEARNING KERNELS; Smola A. J., 2000, P 17 INT C MACH LEAR, P911; Teukolsky SA, 1992, NUMERICAL RECIPES C; Tipping ME, 2000, ADV NEUR IN, V12, P652; Tipping M. E., 2003, P 9 INT WORKSH ART I; Vidakovic B, 1998, J AM STAT ASSOC, V93, P173, DOI 10.2307/2669614; VIDAKOVIC B, 1998, PRACTICAL NONPARAMET, V133, P133; Wipf D., 2004, ADV NEURAL INFORM PR, V16; WIPF D, 2005, P SPARS05, P155	36	20	23	1	5	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	0885-6125			MACH LEARN	Mach. Learn.	AUG	2007	68	2					107	135		10.1007/s10994-007-5012-z		29	Computer Science, Artificial Intelligence	Computer Science	200IP	WOS:000248757400001		
J	Yu, B				Yu, Bin			Embracing statistical challenges in the information technology age	TECHNOMETRICS			English	Article							NONLINEAR DIMENSIONALITY REDUCTION; VARIABLE SELECTION; MODEL SELECTION; REGRESSION; LASSO; CONSISTENCY; EIGENMAPS; PARALLEL; FUTURE	This article examines the role of statistics in the age of information technology (IT). It begins by examining the current state of IT and of the cyberinfrastructure initiative aimed at integrating the technologies into science, engineering, and education to convert massive amounts of data into useful information. Selected applications from science and text processing are introduced to provide concrete examples of massive data sets and the statistical challenges that they pose. The thriving field of machine learning is reviewed as an example of current achievements driven by computations and IT. Ongoing challenges that we face in the IT revolution are also highlighted. The paper concludes that for the healthy future of our field, computer technologies have to be integrated into statistics, and statistical thinking in turn must be integrated into computer technologies.	Univ Calif Berkeley, Dept Stat, Berkeley, CA 94720 USA	Yu, B (reprint author), Univ Calif Berkeley, Dept Stat, Berkeley, CA 94720 USA.	binyu@stat.berkeley.edu					Aldous D. J., 1989, PROBABILITY APPROXIM; Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323; Yuan M, 2006, J ROY STAT SOC B, V68, P49, DOI 10.1111/j.1467-9868.2005.00532.x; TUKEY JW, 1962, ANN MATH STAT, V33, P1, DOI 10.1214/aoms/1177704711; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; [Anonymous], 1998, P 7 INT WORLD WID WE, DOI DOI 10.1016/S0169-7552(98)00110-X; Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319; Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888; Zhao P, 2006, J MACH LEARN RES, V7, P2541; Osborne MR, 2000, IMA J NUMER ANAL, V20, P389, DOI 10.1093/imanum/20.3.389; Breiman L, 2001, STAT SCI, V16, P199, DOI 10.1214/ss/1009213726; Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317; WEGMAN EJ, 1990, J AM STAT ASSOC, V85, P664, DOI 10.2307/2290001; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Zou H, 2005, J ROY STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x; Zou H, 2006, J AM STAT ASSOC, V101, P1418, DOI 10.1198/016214506000000735; Wainwright MJ, 2006, J MACH LEARN RES, V7, P1829; Efron B, 2004, ANN STAT, V32, P407; Jordan MI, 2004, STAT SCI, V19, P140, DOI 10.1214/088342304000000026; Genkin A, 2007, TECHNOMETRICS, V49, P291, DOI 10.1198/004017007000000245; Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467; Meinshausen N, 2006, ANN STAT, V34, P1436, DOI 10.1214/009053606000000281; Zhou DY, 2004, ADV NEUR IN, V16, P321; Atkins D. E., 2003, REVOLUTIONIZING SCI; BAN RJ, 2006, NAT RES COUNC COMM E, P112; Berk RA, 2002, STAT SCI, V17, P173, DOI 10.1214/ss/1030550860; BRAVERMAN A, 2006, 2 NASA DAT MIN WORKS; Buvaneswari A, 2007, TECHNOMETRICS, V49, P305, DOI 10.1198/004017007000000263; CANDES E, DANTZIG SELECTOR STA; Chambers JM, 2006, STAT SCI, V21, P463, DOI 10.1214/088342306000000583; Chapelle O., 2006, SEMISUPERVISED LEARN; CHEN S, 1994, BASIS PERSUIT; COLLINS M, 2000, ICML 2000; Dass SC, 2007, TECHNOMETRICS, V49, P262, DOI 10.1198/004017007000000272; Denby L, 2007, TECHNOMETRICS, V49, P318, DOI 10.1198/004017007000000290; Donoho DL, 2003, P NATL ACAD SCI USA, V100, P5591, DOI 10.1073/pnas.1031596100; Donoho D. L., 2004, MOST LARGE UNDERDETE; EMMOTT S, 2005, 2020 SCI; Faraway J, 2007, TECHNOMETRICS, V49, P277, DOI 10.1198/004017007000000281; FETTERLY D, 2004, SOFTWARE PRACTIC EXP, V1, P1; FISHER RA, 1922, PHILOS T R SOC A, V222, P309, DOI DOI 10.1098/RSTA.1922.0009; GAO J, 2002, EMNLP; GAO J, 2006, P 21 INT C COMP LING, P225, DOI 10.3115/1220175.1220204; Gilbert AC, 2007, TECHNOMETRICS, V49, P346, DOI 10.1198/004017007000000308; Greenshtein E, 2004, BERNOULLI, V10, P971, DOI 10.3150/bj/1106314846; HAM J, 2003, TR110 MAX PLANK I BI; Hastie T., 2001, ELEMENTS STAT LEARNI; HENZINGER MR, 2003, ACM 18 INT JOINT C A, P1573; Henzinger M.R., 2003, INTERNET MATH, V1, P115; Hofmann H, 2000, METRIKA, V51, P11, DOI 10.1007/s001840000041; Inselberg A, 1999, COMPUTATION STAT, V14, P53; JACOB JC, 2001, VIRT OBS FUT ASTR SO, P225; Johnson C, 2004, IEEE COMPUT GRAPH, V24, P13, DOI 10.1109/MCG.2004.20; Jurafsky D., 2000, SPEECH LANGUAGE PROC; Kim Y, 2006, STAT SINICA, V16, P375; Knuteson B, 2003, J COMPUT GRAPH STAT, V12, P808, DOI 10.1198/1061860032599; KUMASAKA N, 2007, UNPUN HIGH DIMENSION; LAFFERTY J, 2005, ADV NEURAL INFORM PR, P18; Lauritzen S. L., 1996, GRAPHICAL MODELS; LEVIT C, 2006, STAT INF TECHN INT 2; Lindsay BG, 2004, STAT SCI, V19, P387, DOI 10.1214/088342304000000404; Liu J. S., 2003, MONTE CARLO STRATEGI; Madianos M, 2004, EUR PSYCHIAT, V19, P408, DOI 10.1016/j.eurpsy.2004.06.028; Mallows C, 2006, TECHNOMETRICS, V48, P319, DOI 10.1198/004017006000000219; Manning C., 1999, FDN STAT NATURAL LAN; Manning C. D., 2007, INTRO INFORM RETRIEV; MEINSHAUSEN N, 2006, LASSO TYPE RECOVERY; Ng AY, 2002, ADV NEUR IN, V14, P849; NORVIG P, 2006, CITRIS DISTINGUISHED; Scholkopf B., 2002, LEARNING KERNELS SUP; SHI T, 2006, REMEOTE SENSING ENV, V107, P172; SHI T, 2006, DAYTIME ARCTIC CLOUD; SPEED TP, 2005, IMS B, V34, P16; TROPP J, 2004, 0404 ICES U TEX; Tukey J. W., 1970, EXPLORATORY DATA ANA, V1st; Unwin A, 2006, GRAPHICS LARGE DATAS; Vandenberghe L., 2004, CONVEX OPTIMIZATION; VANDERGEER S, 2006, SEM STAT ETH ZUR; WAINWRIGHT MJ, 2005, NEW DIRECTIONS STAT; WAINWRIGHT MJ, 2006, 709 U CAL BERK; WELLING J, 2001, VIRTUAL OBSERVATIONS, P225; WILKINSON L, 2005, GRAMMAR GRPAHICS; Wu YN, 2007, TECHNOMETRICS, V49, P249, DOI 10.1198/004017007000000236; YEDIDIA J, 2001, NIPS, V13, P689; ZHANG CH, 2006, MODEL SELECTION CONS; Zhao P., 2004, BOOSTED LASSO; ZHAO P, 2006, GROUPED HIERACHICAL	87	4	4	1	1	AMER STATISTICAL ASSOC	ALEXANDRIA	1429 DUKE ST, ALEXANDRIA, VA 22314 USA	0040-1706			TECHNOMETRICS	Technometrics	AUG	2007	49	3					237	248		10.1198/004017007000000254		12	Statistics & Probability	Mathematics	195QW	WOS:000248427700002		
J	Genkin, A; Lewis, DD; Madigan, D				Genkin, Alexander; Lewis, David D.; Madigan, David			Large-scale Bayesian logistic regression for text categorization	TECHNOMETRICS			English	Article						information retrieval; lasso; penalization; ridge regression; support vector classifier; variable selection	LEAST ANGLE REGRESSION; VARIABLE SELECTION; CLASSIFICATION; CONSISTENCY; ALGORITHM; MACHINE; BOUNDS; LASSO	Logistic regression analysis of high-dimensional data, such as natural language text, poses computational and statistical challenges. Maximum likelihood estimation often fails in these applications. We present a simple Bayesian logistic regression approach that uses a Laplace prior to avoid overfitting and produces sparse predictive models for text data. We apply this approach to a range of document classification problems and show that it produces compact predictive models at least as effective as those produced by support vector machine classifiers or ridge logistic regression combined with feature selection. We describe our model fitting algorithm, our open source implementations (BBR and BMR), and experimental results.	Rutgers State Univ, DIMACS, Piscataway, NJ 08854 USA; David D Lewis Consulting, Chicago, IL 60614 USA	Genkin, A (reprint author), Rutgers State Univ, DIMACS, Piscataway, NJ 08854 USA.	alexgenkin@iname.com; tmpaper06@DavidDLewis.com; dmadigan@rutgers.edu					Greenland S, 2000, AM J EPIDEMIOL, V151, P531; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Krishnapuram B, 2005, IEEE T PATTERN ANAL, V27, P957, DOI 10.1109/TPAMI.2005.127; Sebastiani F, 2002, ACM COMPUT SURV, V34, P1, DOI 10.1145/505282.505283; Zhao P, 2006, J MACH LEARN RES, V7, P2541; Lewis DD, 2004, J MACH LEARN RES, V5, P361; Schapire RE, 2000, MACH LEARN, V39, P135, DOI 10.1023/A:1007649029923; HOERL AE, 1970, TECHNOMETRICS, V12, P55; Zou H, 2005, J ROY STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x; Tipping ME, 2001, J MACH LEARN RES, V1, P211, DOI 10.1162/15324430152748236; Efron B, 2004, ANN STAT, V32, P407; Berry M.W., 2004, SURVEY TEXT MINING C; LECESSIE S, 1992, APPL STAT-J ROY ST C, V41, P191; Dayanik A., 2006, Proceedings of the Twenty-Ninth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, DOI 10.1145/1148170.1148255; DENNIS JE, 1989, OPTIMIZATION, P1; Figueiredo M. A. T., 2001, P IEEE INT C COMP VI, V1, P35; Figueiredo MAT, 2003, IEEE T PATTERN ANAL, V25, P1150, DOI 10.1109/TPAMI.2003.1227989; Forman G., 2003, Journal of Machine Learning Research, V3, DOI 10.1162/153244303322753670; Girosi F, 1998, NEURAL COMPUT, V10, P1455, DOI 10.1162/089976698300017269; Hadjicostas P, 2003, STAT PROBABIL LETT, V62, P293, DOI 10.1016/S0167-7152(03)00036-1; Hastie T., 2001, ELEMENTS STAT LEARNI; HASTIE TJ, 1992, STAT MODELS S, P377; Hauben Manfred, 2005, Expert Opin Drug Saf, V4, P929, DOI 10.1517/14740338.4.5.929; Hersh W., 1994, SIGIR '94. Proceedings of the Seventeenth Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval; Jin R, 2003, P 20 INT C MACH LEAR, P282; Joachims T., 1998, Machine Learning: ECML-98. 10th European Conference on Machine Learning. Proceedings; Joachims T., 2002, LEARNING CLASSIFY TE; Jurafsky D., 2000, SPEECH LANGUAGE PROC; Kittler J, 1986, HDB PATTERN RECOGNIT, P59; Kivinen J, 2001, MACH LEARN, V45, P301, DOI 10.1023/A:1017938623079; Li Fan, 2004, Genome Inform, V15, P131; LOWE HJ, 1994, JAMA-J AM MED ASSOC, V271, P1103, DOI 10.1001/jama.271.14.1103; Madigan D, 2004, ANN STAT, V32, P465; Mallick BK, 2005, J R STAT SOC B, V67, P219, DOI 10.1111/j.1467-9868.2005.00498.x; MARON ME, 1961, J ACM, V8, P404, DOI 10.1145/321075.321084; MITCHELL TJ, 1988, J AM STAT ASSOC, V83, P1023, DOI 10.2307/2290129; PIKE MC, 1980, INT J EPIDEMIOL, V9, P89, DOI 10.1093/ije/9.1.89; PORTER MF, 1980, PROGRAM-AUTOM LIBR, V14, P130, DOI 10.1108/eb046814; SALTON G, 1988, INFORM PROCESS MANAG, V24, P513, DOI 10.1016/0306-4573(88)90021-0; Shevade SK, 2003, BIOINFORMATICS, V19, P2246, DOI 10.1093/bioinformatics/btg308; Smith R.L., 1999, BAYESIAN STAT, V6, P589; Zhang T, 2001, INFORM RETRIEVAL, V4, P5, DOI 10.1023/A:1011441423217	42	147	151	4	20	AMER STATISTICAL ASSOC	ALEXANDRIA	1429 DUKE ST, ALEXANDRIA, VA 22314 USA	0040-1706			TECHNOMETRICS	Technometrics	AUG	2007	49	3					291	304		10.1198/004017007000000245		14	Statistics & Probability	Mathematics	195QW	WOS:000248427700006		
J	Bovelstad, HM; Nygard, S; Storvold, HL; Aldrin, M; Borgan, O; Frigessi, A; Lingjaerde, OC				Bovelstad, H. M.; Nygard, S.; Storvold, H. L.; Aldrin, M.; Borgan, O.; Frigessi, A.; Lingjaerde, O. C.			Predicting survival from microarray data - a comparative study	BIOINFORMATICS			English	Article							GENE-EXPRESSION DATA; B-CELL LYMPHOMA; BREAST-CANCER; CROSS-VALIDATION; RIDGE-REGRESSION; COX REGRESSION; SELECTION; SIGNATURE; LASSO; MODEL	Motivation: Survival prediction from gene expression data and other high-dimensional genomic data has been subject to much research during the last years. These kinds of data are associated with the methodological problem of having many more gene expression values than individuals. In addition, the responses are censored survival times. Most of the proposed methods handle this by using Cox's proportional hazards model and obtain parameter estimates by some dimension reduction or parameter shrinkage estimation technique. Using three well-known microarray gene expression data sets, we compare the prediction performance of seven such methods: univariate selection, forward stepwise selection, principal components regression (PCR), supervised principal components regression, partial least squares regression (PLS), ridge regression and the lasso. Results: Statistical learning from subsets should be repeated several times in order to get a fair comparison between methods. Methods using coefficient shrinkage or linear combinations of the gene expression values have much better performance than the simple variable selection methods. For our data sets, ridge regression has the overall best performance. Availability: Matlab and R code for the prediction methods are available at http://www.med.uio.no/imb/stat/bmms/software/microsurv/. Contact: hegembo@math.uio.no	Univ Oslo, Dept Math, Oslo, Norway; Univ Oslo, Dept Informat, Oslo, Norway; Univ Oslo, Norwegian Comp Ctr, Oslo, Norway; Univ Oslo, Inst Basic Med Sci, Dept Biostat, Oslo, Norway	Bovelstad, HM (reprint author), Univ Oslo, Dept Math, Oslo, Norway.	hegembo@math.uio.no	Nygard, Stale/H-2148-2012; Borgan, Ornulf/G-1039-2013				Aldrin M, 1997, COMPUT STAT DATA AN, V25, P377, DOI 10.1016/S0167-9473(97)00015-7; Chang HY, 2005, P NATL ACAD SCI USA, V102, P3738, DOI 10.1073/pnas.0409462102; Michiels S, 2005, LANCET, V365, P488, DOI 10.1016/S0140-6736(05)17866-0; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; HOERL AE, 1970, TECHNOMETRICS, V12, P55; Sorlie T, 2003, P NATL ACAD SCI USA, V100, P8418, DOI 10.1073/pnas.0932692100; FRANK IE, 1993, TECHNOMETRICS, V35, P109, DOI 10.2307/1269656; SHAO J, 1993, J AM STAT ASSOC, V88, P486, DOI 10.2307/2290328; Rosenwald A, 2002, NEW ENGL J MED, V346, P1937, DOI 10.1056/NEJMoa012914; Bair E, 2004, PLOS BIOL, V2, P511, DOI 10.1371/journal.pbio.00200108; Bair E, 2006, J AM STAT ASSOC, V101, P119, DOI 10.1198/016214505000000628; COX DR, 1972, J R STAT SOC B, V34, P187; Hastie T., 2001, ELEMENTS STAT LEARNI; Klein J., 2003, SURVIVAL ANAL TECHNI; Martens H., 1989, MULTIVARIATE CALIBRA; Marx BD, 1996, TECHNOMETRICS, V38, P374, DOI 10.2307/1271308; Nygard S., 2006, 52006 U OSL DEP MATH; PARK MP, 2006, 200614 STANF U DEP S; Parkhomenko YN, 2002, AD LA OP RE, V1, P127; Segal MR, 2006, BIOSTATISTICS, V7, P268, DOI 10.1093/biostatistics/kxj006; Tibshirani R, 1997, STAT MED, V16, P385, DOI 10.1002/(SICI)1097-0258(19970228)16:4<385::AID-SIM380>3.0.CO;2-3; van de Vijver MJ, 2002, NEW ENGL J MED, V347, P1999, DOI 10.1056/NEJMoa021967; van't Veer LJ, 2002, NATURE, V415, P530, DOI 10.1038/415530a; van Houwelingen HC, 2006, STAT MED, V25, P3201, DOI 10.1002/sim.2353; VERWEIJ PJM, 1993, STAT MED, V12, P2305, DOI 10.1002/sim.4780122407; VERWEIJ PJM, 1994, STAT MED, V13, P2427, DOI 10.1002/sim.4780132307	26	100	102	3	13	OXFORD UNIV PRESS	OXFORD	GREAT CLARENDON ST, OXFORD OX2 6DP, ENGLAND	1367-4803			BIOINFORMATICS	Bioinformatics	AUG 15	2007	23	16					2080	2087		10.1093/bioinformatics/btm305		8	Biochemical Research Methods; Biotechnology & Applied Microbiology; Computer Science, Interdisciplinary Applications; Mathematical & Computational Biology; Statistics & Probability	Biochemistry & Molecular Biology; Biotechnology & Applied Microbiology; Computer Science; Mathematical & Computational Biology; Mathematics	215OG	WOS:000249818300007	17553857	
J	Tutz, G; Binder, H				Tutz, Gerhard; Binder, Harald			Boosting ridge regression	COMPUTATIONAL STATISTICS & DATA ANALYSIS			English	Article						ridge regression; boosting; Lasso; mandatory variables; pseudo ROC curves	NONORTHOGONAL PROBLEMS; VARIABLE SELECTION; STATISTICAL VIEW; LASSO	There are several possible approaches to combining ridge regression with boosting techniques. In the simple or naive approach the ridge estimator is used to fit iteratively the current residuals yielding an alternative to the usual ridge estimator. In partial boosting only part of the regression parameters are reestimated within one step of the iterative procedure. The technique allows to distinguish between mandatory variables that are always included in the analysis and optional variables that are chosen only if relevant. The resulting procedure selects optional variables in a similar way as the Lasso, yielding a reduced set of influential variables, while allowing for regularized estimation of the mandatory parameters. The suggested procedures are investigated within the classical framework of continuous response variables as well as in the case of generalized linear models. The performance in terms of prediction and the identification of relevant variables is compared to several competitors as the Lasso and the more recently proposed elastic net. For the evaluation of the identification of relevant variables pseudo ROC curves are introduced. (C) 2006 Elsevier B.V. All rights reserved.	Univ Munich, Inst Stat, D-80799 Munich, Germany; Univ Freiburg Klinikum, Inst Med Biometrie & Med Informat, Freiburg, Germany	Tutz, G (reprint author), Univ Munich, Inst Stat, Akademiestr 1, D-80799 Munich, Germany.	tutz@stat.uni-muenchen.de; binderh@fdm.uni-freiburg.de	Binder, Harald/C-7413-2009	Binder, Harald/0000-0002-5666-8662			Fu WJJ, 1998, J COMPUT GRAPH STAT, V7, P397, DOI 10.2307/1390712; Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451; Friedman J, 2000, ANN STAT, V28, P337, DOI 10.1214/aos/1016218223; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Breiman L, 1999, NEURAL COMPUT, V11, P1493, DOI 10.1162/089976699300016106; HOERL AE, 1970, TECHNOMETRICS, V12, P55; Buhlmann P, 2003, J AM STAT ASSOC, V98, P324, DOI 10.1198/016214503000125; FRANK IE, 1993, TECHNOMETRICS, V35, P109, DOI 10.2307/1269656; Zou H, 2005, J ROY STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x; Efron B, 2004, ANN STAT, V32, P407; Buhlmann P, 2006, ANN STAT, V34, P559, DOI 10.1214/009053606000000092; LECESSIE S, 1992, APPL STAT-J ROY ST C, V41, P191; Hastie T., 2001, ELEMENTS STAT LEARNI; Hastie TJ, 1990, GEN ADDITIVE MODELS; HOERL AE, 1970, TECHNOMETRICS, V12, P69, DOI 10.2307/1267352; Lokhorst J, 1999, LASSO GEN LINEAR MOD; R Development Core Team, 2005, R LANG ENV STAT COMP; SCHAPIRE RE, 1990, MACH LEARN, V5, P197, DOI 10.1023/A:1022648800760; SEBER GAF, 1977, LINEAR REGRESSION AN; Stamey T., 1989, J UROLOGY, V16, P1076; Tutz G, 2006, BIOMETRICS, V62, P961, DOI 10.1111/j.1541-0420.2006.00578.x	21	36	36	5	14	ELSEVIER SCIENCE BV	AMSTERDAM	PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS	0167-9473			COMPUT STAT DATA AN	Comput. Stat. Data Anal.	AUG 15	2007	51	12					6044	6059		10.1016/j.csda.2006.11.041		16	Computer Science, Interdisciplinary Applications; Statistics & Probability	Computer Science; Mathematics	208JQ	WOS:000249316000047		
J	Liu, YF; Zhang, HH; Park, C; Ahn, J				Liu, Yufeng; Zhang, Hao Helen; Park, Cheolwoo; Ahn, Jeongyoun			Support vector machines with adaptive L-q penalty	COMPUTATIONAL STATISTICS & DATA ANALYSIS			English	Article						adaptive penalty; classification; shrinkage; support vector machine; variable selection	VARIABLE SELECTION; CLASSIFICATION; REGRESSION; LASSO; REGULARIZATION; SHRINKAGE	The standard support vector machine (SVM) minimizes the hinge loss function subject to the L-2 penalty or the roughness penalty. Recently, the L-1 SVM was suggested for variable selection by producing sparse solutions [Bradley, P., Mangasarian, O., 1998. Feature selection via concave minimization and support vector machines. In: Shavlik, J. (Ed.), ICML'98. Morgan Kaufmann, Los Altos, CA; Zhu, J., Hastie, T., Rosset, S., Tibshirani, R., 2003. 1-norm support vector machines. Neural Inform. Process. Systems 16]. These learning methods are non-adaptive since their penalty forms are pre-determined before looking at data, and they often perform well only in a certain type of situation. For instance, the L-2 SVM generally works well except when there are too many noise inputs, while the L-1 SVM is more preferred in the presence of many noise variables. In this article we propose and explore an adaptive learning procedure called the L-q SVM, Where the best q > 0 is automatically chosen by data. Both two- and multi-class classification problems are considered. We show that the new adaptive approach combines the benefit of a class of non-adaptive procedures and gives the best performance of this class across a variety of situations. Moreover, we observe that the proposed L-q penalty is more robust to noise variables than the L-1 and L-2 penalties. An iterative algorithm is suggested to solve the L-q SVM efficiently. Simulations and real data applications support the effectiveness of the proposed procedure. (C) 2007 Elsevier B.V. All rights reserved.	Univ N Carolina, Dept Stat & Operat Res, Carolina Ctr Genome Sci, Chapel Hill, NC 27515 USA; N Carolina State Univ, Dept Stat, Raleigh, NC 27695 USA; Univ Georgia, Dept Stat, Athens, GA 30602 USA	Liu, YF (reprint author), Univ N Carolina, Dept Stat & Operat Res, Carolina Ctr Genome Sci, Chapel Hill, NC 27515 USA.	yfliu@email.unc.edu					Fu WJJ, 1998, J COMPUT GRAPH STAT, V7, P397, DOI 10.2307/1390712; Knight K, 2000, ANN STAT, V28, P1356; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Antoniadis A, 2001, J AM STAT ASSOC, V96, P939, DOI 10.1198/016214501753208942; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; FRANK IE, 1993, TECHNOMETRICS, V35, P109, DOI 10.2307/1269656; Zou H, 2005, J ROY STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x; Lin Y, 2002, DATA MIN KNOWL DISC, V6, P259, DOI 10.1023/A:1015469627679; Lin Y, 2002, MACH LEARN, V46, P191, DOI 10.1023/A:1012406528296; Efron B, 2004, ANN STAT, V32, P407; DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425; BOSER B, 1992, 5 ANN C COMP LEARN T, P142; Bradley Paul S., 1998, ICML 98; Chen S. S., 1999, SIAM J SCI COMPUT, V20, P33; Crammer K., 2001, J MACHINE LEARNING R, V2, P265; Ikeda K, 2005, NEURAL COMPUT, V17, P2508, DOI 10.1162/0899766054796897; Lee YK, 2004, J AM STAT ASSOC, V99, P67, DOI 10.1198/016214504000000098; Vapnik V., 1998, STAT LEARNING THEORY; WAHBA G, 1995, COMPUTATIONAL LEARNI, V3, P127; Wahba G., 1998, ADV KERNEL METHODS S, P125; Wang LF, 2006, STAT SINICA, V16, P617; Weston J., 1999, P 7 EUR S ART NEUR N; Zhu J., 2003, NEURAL INFORM PROCES, V16	23	22	26	0	5	ELSEVIER SCIENCE BV	AMSTERDAM	PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS	0167-9473			COMPUT STAT DATA AN	Comput. Stat. Data Anal.	AUG 15	2007	51	12					6380	6394		10.1016/j.csda.2007.02.006		15	Computer Science, Interdisciplinary Applications; Statistics & Probability	Computer Science; Mathematics	208JQ	WOS:000249316000071		
J	Fujita, A; Sato, JR; Garay-Malpartida, HM; Yamaguchi, R; Miyano, S; Sogayar, MC; Ferreira, CE				Fujita, Andre; Sato, Joao R.; Garay-Malpartida, Humberto M.; Yamaguchi, Rui; Miyano, Satoru; Sogayar, Mari C.; Ferreira, Carlos E.			Modeling gene expression regulatory networks with the sparse vector autoregressive model	BMC SYSTEMS BIOLOGY			English	Article							NF-KAPPA-B; NONCONCAVE PENALIZED LIKELIHOOD; PROBABILISTIC BOOLEAN NETWORKS; BAYESIAN NETWORKS; MODULE NETWORKS; MM ALGORITHMS; P53; CANCER; REGRESSION; ACTIVATION	Background: To understand the molecular mechanisms underlying important biological processes, a detailed description of the gene products networks involved is required. In order to define and understand such molecular networks, some statistical methods are proposed in the literature to estimate gene regulatory networks from time-series microarray data. However, several problems still need to be overcome. Firstly, information flow need to be inferred, in addition to the correlation between genes. Secondly, we usually try to identify large networks from a large number of genes (parameters) originating from a smaller number of microarray experiments (samples). Due to this situation, which is rather frequent in Bioinformatics, it is difficult to perform statistical tests using methods that model large gene-gene networks. In addition, most of the models are based on dimension reduction using clustering techniques, therefore, the resulting network is not a gene-gene network but a module-module network. Here, we present the Sparse Vector Autoregressive model as a solution to these problems. Results: We have applied the Sparse Vector Autoregressive model to estimate gene regulatory networks based on gene expression profiles obtained from time-series microarray experiments. Through extensive simulations, by applying the SVAR method to artificial regulatory networks, we show that SVAR can infer true positive edges even under conditions in which the number of samples is smaller than the number of genes. Moreover, it is possible to control for false positives, a significant advantage when compared to other methods described in the literature, which are based on ranks or score functions. By applying SVAR to actual HeLa cell cycle gene expression data, we were able to identify well known transcription factor targets. Conclusion: The proposed SVAR method is able to model gene regulatory networks in frequent situations in which the number of samples is lower than the number of genes, making it possible to naturally infer partial Granger causalities without any a priori information. In addition, we present a statistical test to control the false discovery rate, which was not previously possible using other gene regulatory network models.	Univ Sao Paulo, Inst Math & Stat, BR-05508090 Sao Paulo, Brazil; Univ Sao Paulo, Inst Chem, BR-05513970 Sao Paulo, Brazil; Univ Sao Paulo, Sch Arts Sci & Humanities, BR-03828000 Sao Paulo, Brazil; Univ Tokyo, Inst Med Sci, Ctr Human Genome, Minato Ku, Tokyo 1088693, Japan	Ferreira, CE (reprint author), Univ Sao Paulo, Inst Math & Stat, Rua Matao 1010, BR-05508090 Sao Paulo, Brazil.	fujita@ime.usp.br; jsato@ime.usp.br; hmgaray@usp.br; ruiy@ims.u-tokyo.ac.jp; miyano@ims.u-tokyo.ac.jp; mcsoga@iq.usp.br; cef@ime.usp.br	Ferreira, Carlos/A-1156-2010; Garay-Malpartida, Humberto Miguel/D-3667-2012				Akutsu T, 2000, J COMPUT BIOL, V7, P331, DOI 10.1089/106652700750050817; BREIMAN L, 1995, TECHNOMETRICS, V37, P373, DOI 10.2307/1269730; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; [Anonymous], R PROJ STAT COMP; Jeong H, 2000, NATURE, V407, P651; Jing N, 2005, ANTI-CANCER DRUG, V16, P601, DOI 10.1097/00001813-200507000-00002; Yu H, 2007, NAT REV IMMUNOL, V7, P41, DOI 10.1038/nri1995; Chen F, 2001, AM J PATHOL, V159, P387, DOI 10.1016/S0002-9440(10)61708-7; Mukhopadhyay ND, 2007, BIOINFORMATICS, V23, P442, DOI 10.1093/bioinformatics/btl598; Soussi T, 2007, ONCOGENE, V26, P2145, DOI 10.1038/sj.onc.1210280; HOERL AE, 1970, TECHNOMETRICS, V12, P55; Albert R, 2000, NATURE, V406, P378, DOI 10.1038/35019019; Segal E, 2003, NAT GENET, V34, P166, DOI 10.1038/ng1165; BENJAMINI Y, 1995, J ROY STAT SOC B MET, V57, P289; Friedman N, 2004, SCIENCE, V303, P799, DOI 10.1126/science.1094068; Gartel AL, 2006, MOL CANCER THER, V5, P1385, DOI 10.1158/1535-7163.MCT-06-0163; Whitfield ML, 2002, MOL BIOL CELL, V13, P1977, DOI 10.1091/mbc.02-02-0030; Hunter DR, 2004, AM STAT, V58, P30, DOI 10.1198/0003130042836; BARABAS AL, 2000, SCIENCE, V286, P509; BARRERA J, 2004, DURHAM, P36; Brown RT, 1995, J BIOL CHEM, V270, P31129; BRYD RH, 1995, SIAM J COMPUT, V16, P1190; BUCKBINDER L, 1995, NATURE, V377, P646, DOI 10.1038/377646a0; CHEN M, 1999, ONCOGENE, V377, P6845; DAMERON KM, 1994, SCIENCE, V265, P1582, DOI 10.1126/science.7521539; De la Cueva E, 2006, ONCOGENE, V25, P4128, DOI 10.1038/sj.onc.1209432; di Bernardo D, 2005, NAT BIOTECHNOL, V23, P377, DOI 10.1038/nbt1075; Dojer N, 2006, BMC BIOINFORMATICS, V7, DOI 10.1186/1471-2105-7-249; Dreyfus DH, 2005, BMC IMMUNOL, V6, DOI 10.1186/1471-2172-6-12; Eichler M, 2005, PHILOS T ROY SOC B, V360, P953, DOI 10.1098/rstb.2005.1641; Erdos P., 1959, PUBL MATH-DEBRECEN, V6, P290; FAITH J, 2007, PLOS BIOL, V7, pE8; Fan JQ, 2004, ANN STAT, V32, P928, DOI 10.1214/009053604000000256; Fujita A, 2007, BIOINFORMATICS, V23, P1623, DOI 10.1093/bioinformatics/btm151; Fukushima Y, 1998, INT J ONCOL, V13, P967; Gardner TS, 2003, SCIENCE, V301, P102, DOI 10.1126/science.1081900; Granger C.W.J., 1969, ECONOMETRICA, V37, P424, DOI DOI 10.2307/1912791; HASTIE T, 1969, ECONOMETRICA, V37, P424; Hosoya Y, 2001, J TIME SER ANAL, V22, P537, DOI 10.1111/1467-9892.00240; Hunter DR, 2004, ANN STAT, V32, P384; Imoto Seiya, 2002, Pac Symp Biocomput, P175; Inoue J, 2007, CANCER SCI, V98, P268, DOI 10.1111/j.1349-7006.2007.00389.x; Jin SK, 2001, J CELL SCI, V114, P4139; KRIKOS A, 1992, J BIOL CHEM, V267, P17971; Masuda M, 2002, CANCER RES, V62, P3351; MESTL T, 1995, J THEOR BIOL, V176, P291, DOI 10.1006/jtbi.1995.0199; Niu GL, 2005, MOL CELL BIOL, V25, P7432, DOI 10.1128/MCB.25.17.7432-7440.2005; Pal R, 2005, BIOINFORMATICS, V21, P1211, DOI 10.1093/bioinformatics/bti131; Roninson IB, 2002, CANCER LETT, V179, P1, DOI 10.1016/S0304-3835(01)00847-3; Schafer J, 2005, BIOINFORMATICS, V21, P754, DOI 10.1093/bioinformatics/bti062; Shmulevich I, 2002, BIOINFORMATICS, V18, P1319, DOI 10.1093/bioinformatics/18.10.1319; Storz P, 2005, BIOCHEM J, V387, P47, DOI 10.1042/BJ20041443; Tamada Y., 2003, BIOINFORMATICS, V19, pii227; Valdes-Sosa PA, 2005, PHILOS T ROY SOC B, V360, P969, DOI 10.1098/rstb.2005.1654; Vapnik V.N., 1995, NATURE STAT LEARNING; Wang HS, 2007, J ROY STAT SOC B, V69, P63; Werhli AV, 2007, STAT APPL GENET MOL, V6; Woolf PJ, 2000, PHYSIOL GENOMICS, V3, P9; Xiong MM, 2004, GENETICS, V166, P1037, DOI 10.1534/genetics.166.2.1037; Xu XJ, 2004, FEBS LETT, V578, P297, DOI 10.1016/j.febslet.2004.11.019; Yakovlev AG, 2004, J BIOL CHEM, V279, P28367, DOI 10.1074/jbc.M313526200; YAMAGUCHI R, 2007, IEEE SIGNAL PROCESSI; You MJ, 1997, MOL CELL BIOL, V17, P7328; HUMAN CELL CYCLE HEL	65	27	29	0	5	BIOMED CENTRAL LTD	LONDON	MIDDLESEX HOUSE, 34-42 CLEVELAND ST, LONDON W1T 4LB, ENGLAND	1752-0509			BMC SYST BIOL	BMC Syst. Biol.	AUG 30	2007	1								39	10.1186/1752-0509-1-39		11	Mathematical & Computational Biology	Mathematical & Computational Biology	230JY	WOS:000250874000001	17761000	
J	Jiang, P; Wu, H; Da, Y; Sang, F; Wei, IW; Sun, X; Lu, ZH				Jiang, Peng; Wu, Haonan; Da, Yao; Sang, Fei; Wei, Iiawei; Sun, Xiao; Lu, Zuhong			RFRCDB-siRNA: Improved design of siRNAs by random forest regression model coupled with database searching	COMPUTER METHODS AND PROGRAMS IN BIOMEDICINE			English	Article						siRNA efficacy; quantitative estimation; random forest regression (RFR)	SUPPORT VECTOR MACHINE; ARTIFICIAL NEURAL-NETWORK; DOUBLE-STRANDED-RNA; FUNCTIONAL SIRNAS; INTERFERING RNAS; PREDICTION; SEQUENCES; EFFICIENT; EFFICACY; CELLS	Although the observations concerning the factors which influence the siRNA efficacy give clues to the mechanism of RNAi, the quantitative prediction of the siRNA efficacy is still a challenge task. In this paper, we introduced a novel non-linear regression method: random forest regression (RFR), to quantitatively estimate siRNAs efficacy values. Compared with an alternative machine learning regression algorithm, support vector machine regression (SVR) and four other score-based algorithms [A. Reynolds, D. Leake, Q. Boese, S. Scaringe, W.S. Marshall, A. Khvorova, Rational siRNA design for RNA interference, Nat. Biotechnol. 22 (2004) 326-330; K. Ui-Tei, Y Naito, F. Takahashi, T. Haraguchi, H. Ohki-Hamazaki, A. Juni, R. Ueda, K. Saigo, Guidelines for the selection of highly effective siRNA sequences for mammalian and chick RNA interference, Nucleic Acids Res. 32 (2004) 936-948; A.C. Hsieh, R. Bo, J. Manola, F. Vazquez, O. Bare, A. Khvorova, S. Scaringe, W.R. Sellers, A library of siRNA duplexes targeting the phosphoinositide 3-kinase pathway: determinants of gene silencing for use in cell-based screens, Nucleic Acids Res. 32 (2004) 893-901; M. Amarzguioui, H. Prydz, An algorithm for selection of functional siRNA sequences, Biochem. Biophys. Res. Commun. 316 (2004) 1050-1058) our RFR model achieved the best performance of all. A web-server, RFRCDB-siRNA (http://vAvw.bioinf.seu.edu.cn/siRNA/index.htm), has been developed. RFRCDB-siRNA consists of two modules: a siRNA-centric database and a RFR prediction system. RFRCDB-siRNA works as follows: (1) instead of directly predicting the gene silencing activity of siRNAs, the service takes these siRNAs as queries to search against the siRNA-centric database. The matched sequences with the exceeding the user defined functionality value threshold are kept. (2) The mismatched sequences are then processed into the RFR prediction system for further analysis. (C) 2007 Elsevier Ireland Ltd. All rights reserved.	Southeast Univ, Dept Biol Sci & Med Engn, State Key Lab Bioelect, Nanjing 210096, Peoples R China	Lu, ZH (reprint author), Southeast Univ, Dept Biol Sci & Med Engn, State Key Lab Bioelect, Nanjing 210096, Peoples R China.	zhlu@seu.edu.cn	Lu, Zuhong/A-5448-2013				Amarzguioui M, 2004, BIOCHEM BIOPH RES CO, V316, P1050, DOI 10.1016/j.bbrc.2004.02.157; Bhasin M, 2005, FEBS LETT, V579, P4302, DOI 10.1016/j.febslet.2005.07.002; Harborth J, 2003, ANTISENSE NUCLEIC A, V13, P83, DOI 10.1089/108729003321629638; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324; Stark GR, 1998, ANNU REV BIOCHEM, V67, P227, DOI 10.1146/annurev.biochem.67.1.227; Fire A, 1998, NATURE, V391, P806, DOI 10.1038/35888; Schwarz DS, 2003, CELL, V115, P199, DOI 10.1016/S0092-8674(03)00759-1; Schubert S, 2005, J MOL BIOL, V348, P883, DOI 10.1016/j.jmb.2005.03.011; Holen T, 2002, NUCLEIC ACIDS RES, V30, P1757, DOI 10.1093/nar/30.8.1757; Svetnik V, 2003, J CHEM INF COMP SCI, V43, P1947, DOI 10.1021/ci034160g; Mathews DH, 2004, P NATL ACAD SCI USA, V101, P7287, DOI 10.1073/pnas.0401799101; Hammond SM, 2000, NATURE, V404, P293; Sing T, 2005, BIOINFORMATICS, V21, P3940, DOI 10.1093/bioinformatics/bti623; Zamore PD, 2000, CELL, V101, P25, DOI 10.1016/S0092-8674(00)80620-0; DIMITRIADOU E, 2006, MISC FUNCITON DEP ST; Fawcett T., 2003, ROC GRAPHS NOTE PRAC; Ge GT, 2005, BIOCHEM BIOPH RES CO, V336, P723, DOI 10.1016/j.bbrc.2005.08.147; Gong WM, 2006, BMC BIOINFORMATICS, V7, DOI 10.1186/1471-2105-7-516; Holen T, 2006, RNA, V12, P1620, DOI 10.1261/rna.81006; Hsieh AC, 2004, NUCLEIC ACIDS RES, V32, P893, DOI 10.1093/nar/gkh238; Huesken D, 2005, NAT BIOTECHNOL, V23, P995, DOI 10.1038/nbt1118; Jagla B, 2005, RNA, V11, P864, DOI 10.1261/rna.7275905; Jia PL, 2006, BMC BIOINFORMATICS, V7, DOI 10.1186/1471-2105-7-271; Katoh T, 2007, NUCLEIC ACIDS RES, V35, DOI 10.1093/nar/gkl1120; Khvorova A, 2003, CELL, V115, P209, DOI 10.1016/S0092-8674(03)00801-8; Ladunga I, 2007, NUCLEIC ACIDS RES, V35, P433, DOI 10.1093/nar/gkl1065; Liaw A., 2002, R NEWS, V2, P18, DOI DOI 10.1016/J.MEMSCI.2010.02.036; Liu W, 2006, BMC BIOINFORMATICS, V7, DOI 10.1186/1471-2105-7-182; Matveeva O, 2007, NUCLEIC ACIDS RES, V35, DOI 10.1093/nar/gkm088; Myasnikova E, 2002, Bioinformatics, V18 Suppl 1, pS87; Overhoff M, 2005, J MOL BIOL, V348, P871, DOI 10.1016/j.jmb.2005.03.012; Reynolds A, 2004, NAT BIOTECHNOL, V22, P326, DOI 10.1038/nbt936; Saetrom P, 2004, BIOCHEM BIOPH RES CO, V321, P247, DOI 10.1016/j.bbrc.2004.06.116; Shabalina SA, 2006, BMC BIOINFORMATICS, V7, DOI 10.1186/1471-2105-7-65; Takasaki S, 2004, CELL CYCLE, V3, P790; Teramoto R, 2005, FEBS LETT, V579, P2878, DOI 10.1016/j.febslet.2005.04.045; Truss M, 2005, NUCLEIC ACIDS RES, V33, pD108, DOI 10.1093/nar/gkil131; Ui-Tei K, 2004, NUCLEIC ACIDS RES, V32, P936, DOI 10.1093/nar/gkh247; Vapnik V.N., 1995, NATURE STAT LEARNING; Vert JP, 2006, BMC BIOINFORMATICS, V7, DOI 10.1186/1471-2105-7-520; Vickers TA, 2003, J BIOL CHEM, V278, P7108, DOI 10.1074/jbc.M210326200; Xia TB, 1998, BIOCHEMISTRY-US, V37, P14719, DOI 10.1021/bi9809425	43	15	16	0	2	ELSEVIER IRELAND LTD	CLARE	ELSEVIER HOUSE, BROOKVALE PLAZA, EAST PARK SHANNON, CO, CLARE, 00000, IRELAND	0169-2607			COMPUT METH PROG BIO	Comput. Meth. Programs Biomed.	SEP	2007	87	3					230	238		10.1016/j.cmpb.2007.06.001		9	Computer Science, Interdisciplinary Applications; Computer Science, Theory & Methods; Engineering, Biomedical; Medical Informatics	Computer Science; Engineering; Medical Informatics	209DK	WOS:000249369000006	17644215	
J	Naik, PA; Schultz, DE; Srinivasan, S				Naik, Prasad A.; Schultz, Don E.; Srinivasan, Shuba			Perils of using OLS to estimate multimedia communications effects	JOURNAL OF ADVERTISING RESEARCH			English	Article							OPTIMAL-CONTROL MODELS; SHRINKAGE; SELECTION; SALES	Companies invest millions of dollars in various forms of marketing communications to impact customers' awareness, attitudes, purchases, and, ultimately, profitability. An important question for marketers an d shareholders alike is: what effects do marketing investments have on market performance? To assess these effects, marketers estimate marketing-mix models by using regression analysis. However, we show that the estimation of marketing-mix models via regression analysis (i.e., ordinary least squares, OLS) yields severely biased estimates of marketing effects. To mitigate such severe biases, we present an alternative approach, called the Wiener-Kalman filter, that provides reasonable esti mates that are much closer to the true parameters than the corresponding OLS estimates. In addition, we analyze Corolla brand's multimedia campaign and furnish results based on marketplace data that corroborate the simulation findings. Finally, we discuss both the implications of these results for brand managers and the opportunities that lie ahead for advertising researchers.	Univ Calif Davis, Davis, CA 95616 USA; Northwestern Univ, Evanston, IL USA; Univ Calif Riverside, A Gary Anderson Grad Sch Management, Riverside, CA 92521 USA	Naik, PA (reprint author), Univ Calif Davis, Davis, CA 95616 USA.	panaik@ucdavis.edu; dschultz@northwestern.edu; shuba.srinivasan@ucr.edu					AMBLER T, 2000, MARKETING BOTTON LIN; Naik PA, 2003, J MARKETING RES, V40, P375, DOI 10.1509/jmkr.40.4.375.19385; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; MEINHOLD RJ, 1983, AM STAT, V37, P123, DOI 10.2307/2685871; Smith A, 2006, J ECONOMETRICS, V134, P553, DOI 10.1016/j.jeconom.2005.07.005; DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425; BLAIR MH, 2000, J ADVERTISING RES, V46, P95; Bollen Kenneth A., 1989, STRUCTURAL EQUATIONS; Bradburn N, 2004, ASKING QUESTIONS; Carroll R, 1995, MEASUREMENT ERROR NO; CLARKE DG, 1976, J MARKETING RES, V13, P345, DOI 10.2307/3151017; Davidson R, 2004, EC THEORY METHODS; FEICHTINGER G, 1994, MANAGE SCI, V40, P195, DOI 10.1287/mnsc.40.2.195; GATIGNON H, 1987, J MARKETING RES, V24, P247, DOI 10.2307/3151635; GATIGNON H, 1993, HDB OR MS, V5; GOPALAKRISHNA S, 1992, J MARKETING RES, V29, P189, DOI 10.2307/3172569; Greene W. H., 1993, ECONOMETRIC ANAL; Harvey A. C., 1994, FORECASTING STRUCTUR; Leone RP, 1995, MARKET SCI, V14, P141; Little JDC, 1986, MARKET SCI, V5, P107, DOI 10.1287/mksc.5.2.107; Mantrala M. K., 2002, HDB MARKETING; MURTHY P, 2005, MARKET LETT, V6, P19; Naik PA, 2000, J MARKETING RES, V37, P113, DOI 10.1509/jmkr.37.1.113.18717; Naik PA, 1998, MARKET SCI, V17, P214, DOI 10.1287/mksc.17.3.214; NAIK PA, 2005, J AM STAT ASSOC, V469, P204; Naik PA, 2005, MARKET SCI, V24, P25, DOI 10.1287/mksc.1040.0083; Pauwels K, 2004, J MARKETING, V68, P142, DOI 10.1509/jmkg.68.4.142.42724; SCHULTZ DE, 2004, 3 ANN ESOMAR ARF WOR; SETHI SP, 1977, SIAM REV, V19, P685, DOI 10.1137/1019106; Smith TM, 2004, INT J RES MARK, V21, P61, DOI 10.1016/j.ijresmar.2003.04.003; ULLLAH A, 2004, FINITE SAMPLE ECONOM; Wiener N., 1949, EXTRAPOLATION INTERP	32	0	0	3	8	ADVERTISING RES FOUNDATION	NEW YORK	641 LEXINGTON AVE, NEW YORK, NY 10022 USA	0021-8499			J ADVERTISING RES	J. Advert. Res.	SEP	2007	47	3					257	269		10.2501/S0021849907070298		13	Business; Communication	Business & Economics; Communication	223EJ	WOS:000250352100005		
J	Shao, J; Chow, SC				Shao, Jun; Chow, Shein-Chung			Variable screening in predicting clinical outcome with high-dimensional microarrays	JOURNAL OF MULTIVARIATE ANALYSIS			English	Article						variable selection; genes; ridge regression; penalizing parameter	REGRESSION; SHRINKAGE; SELECTION	Statistical modeling is an important area of biomarker research of important genes for new drug targets, drug candidate validation, disease diagnoses, personalized treatment, and prediction of clinical outcome of a treatment. A widely adopted technology is the use of microarray data that are typically very high dimensional. After screening chromosomes for relative genes using methods such as quantitative trait locus mapping, there: may still be a few thousands of genes related to the clinical outcome of interest. On the other hand, the: sample size (the number of subjects) in a clinical study is typically much smaller. Under the assumption that only a few important genes are actually related to the clinical outcome, we propose a variable screening procedure to eliminate genes having negligible effects on the clinical outcome. Once the dimension of rnicroarray data is reduced to a manageable number relative to the sample size, one can select a final set of genes via a well-known variable selection method such as the cross-validation. We establish the asymptotic consistency of the proposed variable screening procedure. Some simulation results are also presented. (c) 2005 Elsevier Inc. All rights reserved.	Univ Wisconsin, Dept Stat, Madison, WI 53706 USA; Duke Univ, Dept Biostat & Bioinformat, Durham, NC 27705 USA	Shao, J (reprint author), Univ Wisconsin, Dept Stat, 1210 W Dayton St, Madison, WI 53706 USA.	shao@stat.wisc.edu					Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Almasy L, 1998, AM J HUM GENET, V62, P1198, DOI 10.1086/301844; HOERL AE, 1970, TECHNOMETRICS, V12, P55; CHEN JJ, 2003, ENCY BIOPHARMACEUTIC, P599; *FDA, 2003, DRAFT GUID IND FDA R; GEORGE K, 2000, SCIENCE, V290, P1771; van Houwelingen JC, 2001, STAT NEERL, V55, P17, DOI 10.1111/1467-9574.00154; Whittle P., 1960, THEOR PROBAB APPL, V5, P302, DOI 10.1137/1105028; Zheng XD, 1997, STAT SINICA, V7, P311	9	7	7	0	1	ELSEVIER INC	SAN DIEGO	525 B STREET, STE 1900, SAN DIEGO, CA 92101-4495 USA	0047-259X			J MULTIVARIATE ANAL	J. Multivar. Anal.	SEP	2007	98	8					1529	1538		10.1016/j.jmva.2004.12.004		10	Statistics & Probability	Mathematics	222QF	WOS:000250311300001		
J	Khalili, A; Chen, JH				Khalili, Abbas; Chen, Jiahua			Variable selection in finite mixture of regression models	JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION			English	Article						EM algorithm; LASSO; mixture model; penalty method; SCAD	MAXIMUM-LIKELIHOOD; CROSS-VALIDATION	In the applications of finite mixture of regression (FMR) models, often many covariates are used, and their contributions to the response variable vary from one component to another of the mixture model. This creates a complex variable selection problem. Existing methods, such as the Akaike information criterion and the Bayes information criterion, are computationally expensive as the number of covariates and components in the mixture model increases. In this article we introduce a penalized likelihood approach for variable selection in FMR models. The new method introduces penalties that depend on the size of the regression coefficients and the mixture structure. The new method is shown to be consistent for variable selection. A data-adaptive method for selecting tuning parameters and an EM algorithm for efficient numerical computations are developed. Simulations show that the method performs very well and requires much less computing power than existing methods. The new method is illustrated by analyzing two real data sets.	Ohio State Univ, Dept Stat, Columbus, OH 43210 USA; Univ British Columbia, Dept Stat, Vancouver, BC V6T 1Z2, Canada	Khalili, A (reprint author), Ohio State Univ, Dept Stat, Columbus, OH 43210 USA.	khalili@stat.ohio-state.edu; jhchen@stat.ubc.ca	Chen, Jiahua/C-5040-2008	Chen, Jiahua/0000-0001-8064-4444			Akaike H., 1973, 2 INT S INF THEOR, P267; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1; SHAO J, 1993, J AM STAT ASSOC, V88, P486, DOI 10.2307/2290328; SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136; CRAVEN P, 1979, NUMER MATH, V31, P377; Fan JQ, 2002, ANN STAT, V30, P74; Hennig C, 2000, J CLASSIF, V17, P273, DOI 10.1007/s003570000022; Hunter DR, 2005, ANN STAT, V33, P1617, DOI 10.1214/009053605000000200; Jacobs R. A., 1991, Neural Computation, V3, DOI 10.1162/neco.1991.3.1.79; James LF, 2001, ANN STAT, V29, P1281; Jiang WX, 1999, ANN STAT, V27, P987; Keribin C., 2000, SANKHYA A, V62, P49, DOI [10.2307/25051289, DOI 10.2307/25051289]; KHALILI A, 2005, VARIABLE SELECTION F; Leeb H, 2003, ECONOMET THEOR, V19, P100, DOI 10.1017/S0266466603191050; Peel D, 2000, FINITE MIXTURE MODEL; Rabe-Hesketh S., 2004, GENERALIZED LATENT V; STONE M, 1974, J R STAT SOC B, V36, P111; TITTERINGTON DM, 1985, STAT ANAL FINITE MIX; Wang PM, 1996, BIOMETRICS, V52, P381, DOI 10.2307/2532881; WATNIK MR, 1998, J STAT ED; WEDEL M, 2000, MARKET SEGMENTTATION	23	36	37	1	3	AMER STATISTICAL ASSOC	ALEXANDRIA	1429 DUKE ST, ALEXANDRIA, VA 22314 USA	0162-1459			J AM STAT ASSOC	J. Am. Stat. Assoc.	SEP	2007	102	479					1025	1038		10.1198/016214507000000590		14	Statistics & Probability	Mathematics	214QD	WOS:000249752300029		
J	Wang, HS; Leng, CL				Wang, Hansheng; Leng, Chenlei			Unified LASSO estimation by least squares approximation	JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION			English	Article						adaptive LASSO; Bayes information criterion; LASSO; least angle regression; least squares approximation; microarray data; oracle property; solution path	VARIABLE SELECTION; ORACLE PROPERTIES; MODEL SELECTION; CENSORED-DATA; REGRESSION; CLASSIFICATION; ASYMPTOTICS; SHRINKAGE	We propose a method of least squares approximation (LSA) for unified yet simple LASSO estimation. Our general theoretical framework includes ordinary least squares, generalized linear models, quantile regression, and many others as special cases. Specifically, LSA can transfer many different types of LASSO objective functions into their asymptotically equivalent least squares problems. Thereafter, the standard asymptotic theory can be established and the LARS algorithm can be applied. In particular, if the adaptive LASSO penalty and a Bayes information criterion-type tuning parameter selector are used, the resulting LSA estimator can be as efficient as the oracle. Extensive numerical studies confirm our theory.	Peking Univ, Grad Sch Management, Beijing 100871, Peoples R China; Natl Univ Singapore, Dept Stat & Appl Probabil, Singapore 117548, Singapore	Wang, HS (reprint author), Peking Univ, Grad Sch Management, Beijing 100871, Peoples R China.	hansheng@gsm.pku.edu.cn; stalc@nus.edu.sg					Knight K, 2000, ANN STAT, V28, P1356; KOENKER R, 1978, ECONOMETRICA, V46, P33, DOI 10.2307/1913643; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; Yuan M, 2007, J ROY STAT SOC B, V69, P143, DOI 10.1111/j.1467-9868.2007.00581.x; Osborne MR, 2000, IMA J NUMER ANAL, V20, P389, DOI 10.1093/imanum/20.3.389; Golub TR, 1999, SCIENCE, V286, P531, DOI 10.1126/science.286.5439.531; Dudoit S, 2002, J AM STAT ASSOC, V97, P77, DOI 10.1198/016214502753479248; Zou H, 2005, J ROY STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x; Zou H, 2006, J AM STAT ASSOC, V101, P1418, DOI 10.1198/016214506000000735; Efron B, 2004, ANN STAT, V32, P407; BUCKLEY J, 1979, BIOMETRIKA, V66, P429; COX DR, 1972, J R STAT SOC B, V34, P187; Fan JQ, 2002, ANN STAT, V30, P74; HUBER P, 1981, ROBUSTY ESTIMATION; Hunter DR, 2005, ANN STAT, V33, P1617, DOI 10.1214/009053605000000200; HURVICH CM, 1990, STAT PROBABIL LETT, V9, P259, DOI 10.1016/0167-7152(90)90065-F; LAI TL, 1991, ANN STAT, V19, P1370, DOI 10.1214/aos/1176348253; Leeb H, 2006, ECONOMET THEOR, V22, P69, DOI 10.1017/S0266466606060038; LEEB H, 2005, SPARSE ESTIMATORS OR; Leng CL, 2006, STAT SINICA, V16, P1273; McCullagh P., 1989, GENERALIZED LINEAR M, VSecond; PARK MY, 2006, PENALIZED LOGISTIC R; PARK MY, 2006, AN L1 REGULARIZATION; POLLARD D, 1991, ECONOMET THEOR, V7, P186; ROSSET S, 2004, NIPS; Tibshirani R, 1997, STAT MED, V16, P385, DOI 10.1002/(SICI)1097-0258(19970228)16:4<385::AID-SIM380>3.0.CO;2-3; Venables W.N., 1999, MODERN APPL STAT SPL; WANG H, 2007, IN PRESS J BUSINESS; WANG H, 2007, IN PRESS BIOMETRIKA; ZHAN P, 2004, 678 LASSO; ZOU H, 2004, DEGREES FREEDOM LASS	32	82	84	1	18	AMER STATISTICAL ASSOC	ALEXANDRIA	732 N WASHINGTON ST, ALEXANDRIA, VA 22314-1943 USA	0162-1459			J AM STAT ASSOC	J. Am. Stat. Assoc.	SEP	2007	102	479					1039	1048		10.1198/016214507000000509		10	Statistics & Probability	Mathematics	214QD	WOS:000249752300030		
J	Xue, XN; Kim, MMY; Shore, RE				Xue, Xiaonan; Kim, Mimi Y.; Shore, Roy E.			Cox regression analysis in presence of collinearity: an application to assessment of health risks associated with occupational radiation exposure	LIFETIME DATA ANALYSIS			English	Article						ridge regression; collinearity; Cox proportional hazards model; occupational exposure	OAK-RIDGE; IONIZING-RADIATION; FOLLOW-UP; PENALIZED LIKELIHOOD; NUCLEAR INDUSTRY; CANCER-MORTALITY; HANFORD SITE; WORKERS; SELECTION; SIMULATION	This paper considers the analysis of time to event data in the presence of collinearity between covariates. In linear and logistic regression models, the ridge regression estimator has been applied as an alternative to the maximum likelihood estimator in the presence of collinearity. The advantage of the ridge regression estimator over the usual maximum likelihood estimator is that the former often has a smaller total mean square error and is thus more precise. In this paper, we generalized this approach for addressing collinearity to the Cox proportional hazards model. Simulation studies were conducted to evaluate the performance of the ridge regression estimator. Our approach was motivated by an occupational radiation study conducted at Oak Ridge National Laboratory to evaluate health risks associated with occupational radiation exposure in which the exposure tends to be correlated with possible confounders such as years of exposure and attained age. We applied the proposed methods to this study to evaluate the association of radiation exposure with all-cause mortality.	Albert Einstein Coll Med, Dept Epidemiol & Populat Hlth, Div Biostat, Bronx, NY 10461 USA; Radiat Effects Res Fdn, Minami Ku, Hiroshima 7320815, Japan	Xue, XN (reprint author), Albert Einstein Coll Med, Dept Epidemiol & Populat Hlth, Div Biostat, 1300 Morris Pk Ave,Belfer 1303C, Bronx, NY 10461 USA.	xxue@aecom.yu.edu; mikim@aecom.yu.edu; shore@rerf.or.jp					Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; SCHAEFER RL, 1984, COMMUN STAT-THEOR M, V13, P99, DOI 10.1080/03610928408828664; HOERL AE, 1970, TECHNOMETRICS, V12, P55; Cardis E, 2005, BRIT MED J, V331, P77, DOI 10.1136/bmj.38499.599861.E0; Barker L, 2001, STAT MED, V20, P1431, DOI 10.1002/sim.680; CARDIS E, 1995, RADIAT RES, V142, P117, DOI 10.2307/3579020; COX DR, 1972, J R STAT SOC B, V34, P187; CRIVELLI A, 1995, COMMUN STAT SIMULAT, V24, P631, DOI 10.1080/03610919508813264; Efron B, 1993, INTRO BOOTSTRAP; Frome EL, 1997, RADIAT RES, V148, P64, DOI 10.2307/3579540; GIBBONS DG, 1981, J AM STAT ASSOC, V76, P131, DOI 10.2307/2287058; GILBERT ES, 1993, RADIAT RES, V136, P408, DOI 10.2307/3578555; GILBERT ES, 1993, HEALTH PHYS, V64, P577, DOI 10.1097/00004032-199306000-00001; HOERL AE, 1970, COMMUN STAT, V4, P1105; Holford TR, 2000, INT J EPIDEMIOL, V29, P975, DOI 10.1093/ije/29.6.975; Huang J, 2002, BIOMETRICS, V58, P781, DOI 10.1111/j.0006-341X.2002.00781.x; LAGAKOS SW, 1988, STAT MED, V7, P257, DOI 10.1002/sim.4780070126; Lustbader E., 1986, MODERN STAT METHODS; Marsh JL, 2002, BRIT MED J, V325, P327, DOI 10.1136/bmj.325.7359.327; Richardson DB, 1999, INT J EPIDEMIOL, V28, P428, DOI 10.1093/ije/28.3.428; SHORE RE, 1990, HEALTH PHYS, V59, P63, DOI 10.1097/00004032-199007000-00007; SMITH KR, 1991, J CLIN EPIDEMIOL, V44, P715, DOI 10.1016/0895-4356(91)90031-4; Tibshirani R, 1997, STAT MED, V16, P385, DOI 10.1002/(SICI)1097-0258(19970228)16:4<385::AID-SIM380>3.0.CO;2-3; van Houwelingen JC, 2001, STAT NEERL, V55, P17, DOI 10.1111/1467-9574.00154; VERWEIJ PJM, 1993, STAT MED, V12, P2305, DOI 10.1002/sim.4780122407; VERWEIJ PJM, 1994, STAT MED, V13, P2427, DOI 10.1002/sim.4780132307; WING S, 1993, AM J IND MED, V23, P265, DOI 10.1002/ajim.4700230204; WING S, 1991, JAMA-J AM MED ASSOC, V265, P1397, DOI 10.1001/jama.265.11.1397; Xue XN, 2004, HEALTH PHYS, V87, P397, DOI 10.1097/01.HP.0000133382.36334.ef	29	11	11	0	4	SPRINGER	DORDRECHT	VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS	1380-7870			LIFETIME DATA ANAL	Lifetime Data Anal.	SEP	2007	13	3					333	350		10.1007/s10985-007-9045-1		18	Mathematics, Interdisciplinary Applications; Statistics & Probability	Mathematics	210LY	WOS:000249458900003	17661175	
J	d'Aspremont, A; El Ghaoui, L; Jordan, MI; Lanckriet, GRG				d'Aspremont, Alexandre; El Ghaoui, Laurent; Jordan, Michael I.; Lanckriet, Gert R. G.			A direct formulation for sparse PCA using semidefinite programming	SIAM REVIEW			English	Article						principal component analysis; Karhunen-Loeve transform; factor analysis; semidefinite relaxation; Moreau-Yosida regularization; semidefinite programming	PRINCIPAL COMPONENTS; OPTIMIZATION; MATRIX; LASSO	Given a covariance matrix, we consider the problem of maximizing the variance explained by a particular linear combination of the input variables while constraining the number of nonzero coefficients in this combination. This problem arises in the decomposition of a covariance matrix into sparse factors or sparse principal component analysis (PCA), and has wide applications ranging from biology to finance. We use a modification of the classical variational representation of the largest eigenvalue of a symmetric matrix, where cardinality is constrained, and derive a semidefinite programming-based relaxation for our problem. We also discuss Nesterov's smooth minimization technique applied to the semidefinite program arising in the semidefinite relaxation of the sparse PCA problem. The method has complexity O(n(4) root log(n)/epsilon), where n is the size of the underlying covariance matrix and e is the desired absolute accuracy on the optimal value of the problem.	Princeton Univ, Dept Operat Res & Financial Engn, Princeton, NJ 08544 USA; Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA; Univ Calif Berkeley, Dept EECS & Stat, Berkeley, CA 94720 USA; Univ Calif San Diego, Dept Elect & Comp Engn, La Jolla, CA 92093 USA	d'Aspremont, A (reprint author), Princeton Univ, Dept Operat Res & Financial Engn, Princeton, NJ 08544 USA.	aspremon@princeton.edu; elghaoui@eecs.berkeley.edu; jordan@cs.berkeley.edu; gert@ece.ucsd.edu					Nesterov Y, 2005, MATH PROGRAM, V103, P127, DOI 10.1007/s10107-004-0552-5; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; ALIZADEH F, 1995, SIAM J OPTIMIZ, V5, P13, DOI 10.1137/0805002; Sturm JF, 1999, OPTIM METHOD SOFTW, V11-2, P625, DOI 10.1080/10556789908805766; Jolliffe IT, 2003, J COMPUT GRAPH STAT, V12, P531, DOI 10.1198/1061860032148; Moler C, 2003, SIAM REV, V45, P3, DOI 10.1137/S00361445024180; Ben-Tal A, 2005, MATH PROGRAM, V102, P407, DOI 10.1007/s10107-004-0553-4; CADIMA J, 1995, J APPL STAT, V22, P203, DOI 10.1080/757584614; DASPREMONT A, 2005, SMOOTH OPTIMIZATION; Donoho DL, 2005, P NATL ACAD SCI USA, V102, P9446, DOI 10.1073/pnas.0502269102; Fazel M., 2001, P AM CONTR C, V6, P4734, DOI 10.1109/ACC.2001.945730; Helmberg C, 2000, SIAM J OPTIMIZ, V10, P673, DOI 10.1137/S1052623497328987; Jeffers JNR, 1967, APPLIED STATISTICS, V16, P225, DOI 10.2307/2985919; JOLLIFFE IT, 1995, J APPL STAT, V22, P29, DOI 10.1080/757584395; Kolda TG, 2000, ACM T MATH SOFTWARE, V26, P415, DOI 10.1145/358407.358424; Lemarechal C, 1997, SIAM J OPTIMIZ, V7, P367; Lemarechal C., 1999, 3710 INRIA; Lovasz L, 1991, SIAM J OPTIMIZ, V1, P166, DOI 10.1137/0801013; Nemirovski A, 2004, SIAM J OPTIMIZ, V15, P229, DOI 10.1137/S1052623403425629; Nesterov Y., 1983, SOV MATH DOKL, V27, P372; Nesterov Y., 2004, INTRO LECT CONVEX OP; Nesterov Y., 2007, SMOOTHING TECHNIQUE, V110, P245; SIDJE RB, 1998, ACM T MATH SOFTWARE, V24, P120; Toh KC, 1999, OPTIM METHOD SOFTW, V11-2, P545, DOI 10.1080/10556789908805762; Vandenberghe L, 1996, SIAM REV, V38, P49, DOI 10.1137/1038003; Vandenberghe L., 2004, CONVEX OPTIMIZATION; VINES SK, 2000, APPL STAT, V49, P441; ZHANG Z., 2004, SIAM J MATRIX ANAL A, V25, P901, DOI 10.1137/S0895479801394477; Zhang ZY, 2002, SIAM J MATRIX ANAL A, V23, P706, DOI 10.1137/S0895479899359631; Zou H, 2006, J COMPUT GRAPH STAT, V15, P265, DOI 10.1198/106186006X113430	30	144	146	6	19	SIAM PUBLICATIONS	PHILADELPHIA	3600 UNIV CITY SCIENCE CENTER, PHILADELPHIA, PA 19104-2688 USA	0036-1445			SIAM REV	SIAM Rev.	SEP	2007	49	3					434	448		10.1137/050645506		15	Mathematics, Applied	Mathematics	202RR	WOS:000248921400005		
J	Leng, CL; Ma, SG				Leng, Chenlei; Ma, Shuangge			Path consistent model selection in additive risk model via Lasso	STATISTICS IN MEDICINE			English	Article						additive risk model; Lasso; oracle properties; variable selection	SURVIVAL-DATA; VARIABLE SELECTION; REGRESSION; ASYMPTOTICS	As a flexible alternative to the Cox model, the additive risk model assumes that the hazard function is the sum of the baseline hazard and a regression function of covariates. For right censored survival data when variable selection is needed along with model estimation, we propose a path consistent model selector using a modified Lasso approach, under the additive risk model assumption. We show that the proposed estimator possesses the oracle variable selection and estimation property. Applications of the proposed approach to three right censored survival data sets show that the proposed modified Lasso yields parsimonious models with satisfactory estimation and prediction results. Copyright (c) 2007 John Wiley & Sons, Ltd.	Yale Univ, Dept Epidemiol & Publ Hlth, Div Biostat, New Haven, CT 06520 USA; Natl Univ Singapore, Dept Stat & Appl Probabil, Singapore 117548, Singapore	Ma, SG (reprint author), Yale Univ, Dept Epidemiol & Publ Hlth, Div Biostat, New Haven, CT 06520 USA.	shuanage.ma@yale.edu					Aalen O., 1980, LECT NOTES STAT, V2; ANDERSEN PK, 1982, ANN STAT, V10, P1100, DOI 10.1214/aos/1176345976; BREIMAN L, 1995, TECHNOMETRICS, V37, P373, DOI 10.2307/1269730; LIN DY, 1994, BIOMETRIKA, V81, P61, DOI 10.1093/biomet/81.1.61; Knight K, 2000, ANN STAT, V28, P1356; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; Efron B, 2004, ANN STAT, V32, P407; Heagerty PJ, 2000, BIOMETRICS, V56, P337, DOI 10.1111/j.0006-341X.2000.00337.x; Breiman L, 1996, ANN STAT, V24, P2350; COX DR, 1972, J R STAT SOC B, V34, P187; Day N. E., 1987, STAT METHODS CANC RE; DICKSON ER, 1989, HEPATOLOGY, V10, P1, DOI 10.1002/hep.1840100102; Fleming T. R., 1991, COUNTING PROCESSES S; GEYER C, 1996, UNPUB; GEYER CJ, 1994, ANN STAT, V22, P1993, DOI 10.1214/aos/1176325768; Gijbels I., 1996, LOCAL POLYNOMIAL MOD; Hosmer DW, 1999, APPL SURVIVAL ANAL; Huang J, 2006, BIOMETRICS, V62, P813, DOI 10.1111/j.1541-0420.2006.00562.x; Leng CL, 2006, STAT SINICA, V16, P1273; Li HZ, 2004, BIOINFORMATICS, V20, P208, DOI 10.1093/bioinformatics/bth900; MA S, 2005, 347 U IOW DEP STAT A; Ma S, 2006, BIOMETRICS, V62, P202, DOI 10.1111/j.1541-0420.2005.00405.x; MCKEAGUE IW, 1994, BIOMETRIKA, V81, P501; Miller A. J., 1990, SUBSET SELECTION REG; Moeschberger M.L., 1997, SURVIVAL ANAL TECHNI; Nguyen DV, 2002, BIOINFORMATICS, V18, P1625, DOI 10.1093/bioinformatics/18.12.1625; Oakes D., 1984, ANAL SURVIVAL DATA; ROSSET S, 2004, UNPUB; Thomas D. C., 1986, MODERN STAT METHODS; Tibshirani R, 1997, STAT MED, V16, P385, DOI 10.1002/(SICI)1097-0258(19970228)16:4<385::AID-SIM380>3.0.CO;2-3; Wahba G., 1990, CBMS NSF REG C SER A, V59; Wang H. S., 2005, UNPUB; YUAN M, 2005, 200525 GEORG I TECHN; ZHAO P, 2005, UNPUB MODEL SELECTIO; ZOU H, 2005, ADAPTIVE LASSO ITS O	36	10	10	0	1	WILEY-BLACKWELL	MALDEN	COMMERCE PLACE, 350 MAIN ST, MALDEN 02148, MA USA	0277-6715			STAT MED	Stat. Med.	SEP 10	2007	26	20					3753	3770		10.1002/sim.2834		18	Mathematical & Computational Biology; Public, Environmental & Occupational Health; Medical Informatics; Medicine, Research & Experimental; Statistics & Probability	Mathematical & Computational Biology; Public, Environmental & Occupational Health; Medical Informatics; Research & Experimental Medicine; Mathematics	204GU	WOS:000249032700005	17309043	
J	Lu, WB; Zhang, HH				Lu, Wenbin; Zhang, Hao H.			Variable selection for proportional odds model	STATISTICS IN MEDICINE			English	Article						marginal likelihood; proportional odds model; variable selection; shrinkage estimate	LIKELIHOOD-ESTIMATION; LASSO; REGRESSION	In this paper we study the problem of variable selection for the proportional odds model, which is a useful alternative to the proportional hazards model and might be appropriate when the proportional hazards assumption is not satisfied. We propose to fit the proportional odds model by maximizing the marginal likelihood subject to a shrinkage-type penalty, which encourages sparse solutions and hence facilitates the process of variable selection. Two types of shrinkage penalties are considered: the LASSO and the adaptive-LASSO (ALASSO) penalty. In the ALASSO penalty, different weights are imposed on different coefficients such that important variables are more protectively retained in the final model while unimportant ones are more likely to be shrunk to zeros. We further provide an efficient computation algorithm to implement the proposed methods, and demonstrate their performance through simulation studies and an application to real data. Numerical results indicate that both methods can produce accurate and interpretable models, and the ALASSO tends to work better than the usual LASSO. Copyright (c) 2007 John Wiley & Sons, Ltd.	N Carolina State Univ, Dept Stat, Raleigh, NC 27695 USA	Lu, WB (reprint author), N Carolina State Univ, Dept Stat, Raleigh, NC 27695 USA.	lu@stat.ncsu.edu					AKAIKE H, 1973, BIOMETRIKA, V60, P255, DOI 10.1093/biomet/60.2.255; Fu WJJ, 1998, J COMPUT GRAPH STAT, V7, P397, DOI 10.2307/1390712; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; DABROWSKA DM, 1988, J AM STAT ASSOC, V83, P744, DOI 10.2307/2289300; Zou H, 2006, J AM STAT ASSOC, V101, P1418, DOI 10.1198/016214506000000735; SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136; MALLOWS CL, 1973, TECHNOMETRICS, V15, P661, DOI 10.2307/1267380; Bennett S, 1983, Stat Med, V2, P273, DOI 10.1002/sim.4780020223; Cai JW, 2005, BIOMETRIKA, V92, P303, DOI 10.1093/biomet/92.2.303; COX DR, 1972, J R STAT SOC B, V34, P187; FAN J, 2001, J AM STAT ASSOC, V99, P1348; Fan JQ, 2002, ANN STAT, V30, P74; HOCKING RR, 1976, BIOMETRICS, V32, P1, DOI 10.2307/2529336; Hunter DR, 2005, ANN STAT, V33, P1617, DOI 10.1214/009053605000000200; Kalbfleish JD, 2002, STAT ANAL FAILURE TI; Krishnaiah PR, 1977, APPL STAT, P27; Lam KF, 2001, LIFETIME DATA ANAL, V7, P39, DOI 10.1023/A:1009673026121; Lawless J. F., 1982, STAT MODELS METHODS; MALLOWS CL, 1995, TECHNOMETRICS, V37, P362, DOI 10.2307/1269729; Miller A. J., 1990, SUBSET SELECTION REG; Monahan J.F., 2001, NUMERICAL METHODS ST; Murphy SA, 1997, J AM STAT ASSOC, V92, P968, DOI 10.2307/2965560; PETTITT AN, 1982, J ROY STAT SOC B MET, V44, P234; PETTITT AN, 1984, APPL STAT-J ROY ST C, V33, P169, DOI 10.2307/2347443; Tibshirani R, 1997, STAT MED, V16, P385, DOI 10.1002/(SICI)1097-0258(19970228)16:4<385::AID-SIM380>3.0.CO;2-3; WAHBA G, 1990, CBMS NSF REG C SER A, V59; WANG H, 2007, IN PRESS J BUSINESS; ZHANG HH, 2007, IN PRESS BIOMETRIKA	28	17	17	2	4	JOHN WILEY & SONS LTD	CHICHESTER	THE ATRIUM, SOUTHERN GATE, CHICHESTER PO19 8SQ, W SUSSEX, ENGLAND	0277-6715			STAT MED	Stat. Med.	SEP 10	2007	26	20					3771	3781		10.1002/sim.2833		11	Mathematical & Computational Biology; Public, Environmental & Occupational Health; Medical Informatics; Medicine, Research & Experimental; Statistics & Probability	Mathematical & Computational Biology; Public, Environmental & Occupational Health; Medical Informatics; Research & Experimental Medicine; Mathematics	204GU	WOS:000249032700006	17266170	
J	Saigo, H; Uno, T; Tsuda, K				Saigo, Hiroto; Uno, Takeaki; Tsuda, Koji			Mining complex genotypic features for predicting HIV-1 drug resistance	BIOINFORMATICS			English	Article							REVERSE-TRANSCRIPTASE; INHIBITORS; OPTIMIZATION; PHENOTYPE; NETWORKS; ACCURATE; THERAPY	Motivation: Human immunodeficiency virus type 1 (HIV-1) evolves in human body, and its exposure to a drug often causes mutations that enhance the resistance against the drug. To design an effective pharmacotherapy for an individual patient, it is important to accurately predict the drug resistance based on genotype data. Notably, the resistance is not just the simple sum of the effects of all mutations. Structural biological studies suggest that the association of mutations is crucial: even if mutations A or B alone do not affect the resistance, a significant change might happen when the two mutations occur together. Linear regression methods cannot take the associations into account, while decision tree methods can reveal only limited associations. Kernel methods and neural networks implicitly use all possible associations for prediction, but cannot select salient associations explicitly. Results: Our method, itemset boosting, performs linear regression in the complete space of power sets of mutations. It implements a forward feature selection procedure where, in each iteration, one mutation combination is found by an efficient branch- and- bound search. This method uses all possible combinations, and salient associations are explicitly shown. In experiments, our method worked particularly well for predicting the resistance of nucleotide reverse transcriptase inhibitors ( NRTIs). Furthermore, it successfully recovered many mutation associations known in biological literature.	Max Planck Inst Biol Cybernet, D-72076 Tubingen, Germany; Natl Inst Informat, Tokyo, Japan	Tsuda, K (reprint author), Max Planck Inst Biol Cybernet, Spemannstr 38, D-72076 Tubingen, Germany.	koji.tsuda@tuebingen.mpg.de					Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Sturm JF, 1999, OPTIM METHOD SOFTW, V11-2, P625, DOI 10.1080/10556789908805766; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Dietterich TG, 2000, MACH LEARN, V40, P139, DOI 10.1023/A:1007607513941; Beerenwinkel N, 2002, P NATL ACAD SCI USA, V99, P8271, DOI 10.1073/pnas.112177799; Beerenwinkel N, 2003, NUCLEIC ACIDS RES, V31, P3850, DOI 10.1093/nar/gkg575; BRINZ D, 2006, INT C IEEE ENG MED B, P5802; Danziger SA, 2006, IEEE-ACM T COMPUT BI, V3, P114, DOI 10.1109/TCBB.2006.22; Deforche K, 2006, BIOINFORMATICS, V22, P2975, DOI 10.1093/bioinformatics/btl508; Demiriz A, 2002, MACH LEARN, V46, P225, DOI 10.1023/A:1012470815092; DiRienzo AG, 2003, STAT MED, V22, P2785, DOI 10.1002/sim.1516; FOULKES AS, 2002, BIOMETRICS, V58, P146; Iversen AKN, 1996, J VIROL, V70, P1086; Kozal M, 2004, AIDS PATIENT CARE ST, V18, P199, DOI 10.1089/108729104323038874; Kudo T., 2005, ADV NEURAL INFORM PR, V17, P729; Le Quoc V., 2006, P 23 INT C MACH LEAR, P521, DOI 10.1145/1143844.1143910; Lengauer T, 2006, NAT REV MICROBIOL, V4, P790, DOI 10.1038/nrmicro1477; MORISHITA S, 2001, DISCOVERY SCI, P471; NAKAYA A, 2000, PAC S BIOC, P364; Rabinowitz M, 2006, BIOINFORMATICS, V22, P541, DOI 10.1093/bioinformatics/btk011; Rhee SY, 2003, NUCLEIC ACIDS RES, V31, P298, DOI 10.1093/nar/gkg100; Rhee SY, 2006, P NATL ACAD SCI USA, V103, P17355, DOI 10.1073/pnas.0607274103; SARDANA VV, 1992, J BIOL CHEM, V267, P17526; Scholkopf B., 2002, LEARNING KERNELS SUP; SING T, 2005, 9 EUR C PRINC PRACT, P285; SING T, 2007, ADV NEURAL INFORM PR, V19, P1297; Uno T, 2005, OSDM 05, P77; Vert JP, 2006, BMC BIOINFORMATICS, V7, DOI 10.1186/1471-2105-7-520; Vivet-Boudou V, 2006, CELL MOL LIFE SCI, V63, P163, DOI 10.1007/s00018-005-5367-x; Wang DC, 2003, J INFECT DIS, V188, P653, DOI 10.1086/377453; ZHU Z, 2005, GENOME RES, V15, P845	31	17	17	3	5	OXFORD UNIV PRESS	OXFORD	GREAT CLARENDON ST, OXFORD OX2 6DP, ENGLAND	1367-4803			BIOINFORMATICS	Bioinformatics	SEP 15	2007	23	18					2455	2462		10.1093/bioinformatics/btm353		8	Biochemical Research Methods; Biotechnology & Applied Microbiology; Computer Science, Interdisciplinary Applications; Mathematical & Computational Biology; Statistics & Probability	Biochemistry & Molecular Biology; Biotechnology & Applied Microbiology; Computer Science; Mathematical & Computational Biology; Mathematics	215OK	WOS:000249818700014	17698858	
J	Hofmann, M; Gatu, C; Kontoghiorghes, EJ				Hofmann, Marc; Gatu, Cristian; Kontoghiorghes, Erricos John			Efficient algorithms for computing the best subset regression models for large-scale problems	COMPUTATIONAL STATISTICS & DATA ANALYSIS			English	Article						best-subset regression; regression tree; branch-and-bound algorithm	QR DECOMPOSITION; BOUND ALGORITHMS; SELECTION	Several strategies for computing the best subset regression models are proposed. Some of the algorithms are modified versions of existing regression-tree methods, while others are new. The first algorithm selects the best subset models within a given size range. It uses a reduced search space and is found to outperform computationally the existing branch-and-bound algorithm. The properties and computational aspects of the proposed algorithm are discussed in detail. The second new algorithm preorders the variables inside the regression tree. A radius is defined in order to measure the distance of a node from the root of the tree. The algorithm applies the preordering to all nodes which have a smaller distance than a certain radius that is given a priori. An efficient method of preordering the variables is employed. The experimental results indicate that the algorithm performs best when preordering is employed on a radius of between one quarter and one third of the number of variables. The algorithm has been applied with such a radius to tackle large-scale subset-selection problems that are considered to be computationally infeasible by conventional exhaustive-selection methods. A class of new heuristic strategies is also proposed. The most important of these is one that assigns a different tolerance value to each subset model size. This strategy with different kind of tolerances is equivalent to all exhaustive and heuristic subset-selection strategies. In addition the strategy can be used to investigate submodels having noncontiguous size ranges. Its implementation provides a flexible tool for tackling large scale models. (c) 2007 Elsevier B.V. All rights reserved.	Univ Neuchatel, Inst Informat, CH-2000 Neuchatel, Switzerland; Univ Cyprus, Dept Publ & Business Adm, Nicosia, Cyprus; Univ London, Birkbeck Coll, Sch Comp Sci & Informat Syst, London WC1E 7HU, England; Alexandru Ioan Cuza Univ, Fac Comp Sci, Iasi, Romania	Hofmann, M (reprint author), Univ Neuchatel, Inst Informat, CH-2000 Neuchatel, Switzerland.	marc.hofmann@unine.ch; cristian.gatu@unine.ch; erricos@ucy.ac.cy	Gatu, Cristian/C-4814-2011; Cuza, UAIC/D-2604-2009				BREIMAN L, 1995, TECHNOMETRICS, V37, P373, DOI 10.2307/1269730; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; FURNIVAL GM, 1974, TECHNOMETRICS, V16, P499, DOI 10.2307/1267601; CLARKE MRB, 1981, APPL STATIST, V30, P198, DOI 10.2307/2346398; GATU C, 2005, COMPUTATIONAL MANAGE, V2, P253, DOI 10.1007/s10287-004-0021-x; Gatu C, 2006, J COMPUT GRAPH STAT, V15, P139, DOI 10.1198/106186006X100290; Gatu C, 2007, COMPUT STAT DATA AN, V52, P799, DOI 10.1016/j.csda.2007.02.018; Gatu C, 2003, PARALLEL COMPUT, V29, P505, DOI 10.1016/S0167-8191(03)00019-X; Golub G., 1996, MATRIX COMPUTATIONS; Hastie T., 2001, SPRINGER SERIES STAT; HOCKING RR, 1976, BIOMETRICS, V32, P1, DOI 10.2307/2529336; Hofmann M, 2006, PARALLEL COMPUT, V32, P222, DOI 10.1016/j.parco.2005.11.001; KONTAGHIORGHES EJ, 2000, ADV COMPUTATIONAL EC, V15; Miller A, 2002, SUBSET SELECTION REG, V95; Narendra P. M., 1997, IEEE T COMPUT, V26, P917; R DEVELOPMENT CORE TEAM, 2005, R FDN STAT COMP; ROBERTS SJ, 1984, APPL STAT, V33, P236, DOI 10.2307/2347457; Searle SR., 1971, LINEAR MODELS; Sen A. K., 1990, REGRESSION ANAL THEO; SMITH DM, 1989, COMPUT STAT DATA AN, V7, P217, DOI 10.1016/0167-9473(89)90023-6; Sober G., 1977, LINEAR REGRESSION AN; Somol P, 2004, IEEE T PATTERN ANAL, V26, P900, DOI 10.1109/TPAMI.2004.28	23	27	27	2	4	ELSEVIER SCIENCE BV	AMSTERDAM	PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS	0167-9473	1872-7352		COMPUT STAT DATA AN	Comput. Stat. Data Anal.	SEP 15	2007	52	1					16	29		10.1016/j.csda.2007.03.017		14	Computer Science, Interdisciplinary Applications; Statistics & Probability	Computer Science; Mathematics	216XN	WOS:000249912400003		
J	McCann, L; Welsch, RE				McCann, Lauren; Welsch, Roy E.			Robust variable selection using least angle regression and elemental set sampling	COMPUTATIONAL STATISTICS & DATA ANALYSIS			English	Article						robust regression; variable selection; LARS; outliers; elemental sets	LINEAR-MODEL SELECTION; CROSS-VALIDATION	The problem of selecting variables or features in a regression model in the presence of both additive (vertical) and leverage outliers is addressed. Since variable selection and the detection of anomalous data are not separable problems, the focus is on methods that select variables and outliers simultaneously. For selection, the fast forward selection algorithm, least angle regression (LARS), is used, but it is not robust. To achieve robustness to additive outliers, a dummy variable identity matrix is appended to the design matrix allowing both real variables and additive outliers to be in the selection set. For leverage outliers, these selection methods are used on samples of elemental sets in a manner similar to that used in high breakdown robust estimation. These results are compared to several other selection methods of varying computational complexity and robustness. The extension of these methods to situations where the number of variables exceeds the number of observations is discussed. (c) 2007 Elsevier B.V. All rights reserved.	MIT, Sloan Sch Management, Cambridge, MA 02139 USA; GlaxoSmithKline Inc, Collegeville, PA 19426 USA	Welsch, RE (reprint author), MIT, Sloan Sch Management, 77 Massachusetts Ave,Rm E53-383, Cambridge, MA 02139 USA.	futed@alum.mit.edu; rwelsch@mit.edu					Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Raftery AE, 1997, J AM STAT ASSOC, V92, P179, DOI 10.2307/2291462; Zou H, 2005, J ROY STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x; SHAO J, 1993, J AM STAT ASSOC, V88, P486, DOI 10.2307/2290328; Efron B, 2004, ANN STAT, V32, P407; Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.1023/A:1018054314350; HASTIE T, 2001, ELEMENTS STAT LEARNI, P58; KHAN J, 2005, ROBUST LINEAR MODEL; MCCANN L, 2006, THESIS SLOAN SCH MAN; MEINSHAUSEN N, 2005, LASSO RELAXATION; Morgenthaler S, 2004, STAT IND TECHNOL, P195; Muller S, 2005, J AM STAT ASSOC, V100, P1297, DOI 10.1198/016214505000000529; Ronchetti E, 1997, J AM STAT ASSOC, V92, P1017, DOI 10.2307/2965566; ROUSSEEUW PJ, 2000, DATA ANAL SCI MODELI, P335; SALIBIANBARRERA M, 2006, IN PRESS J COMPUT GR	15	8	8	1	7	ELSEVIER SCIENCE BV	AMSTERDAM	PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS	0167-9473			COMPUT STAT DATA AN	Comput. Stat. Data Anal.	SEP 15	2007	52	1					249	257		10.1016/j.csda.2007.01.012		9	Computer Science, Interdisciplinary Applications; Statistics & Probability	Computer Science; Mathematics	216XN	WOS:000249912400021		
J	Bock, HH; Vichi, M				Bock, Hans-Hermann; Vichi, Maurizio			Statistical learning methods including dimensionality reduction	COMPUTATIONAL STATISTICS & DATA ANALYSIS			English	Editorial Material							VARIABLE SELECTION; REGRESSION; MODEL; CLASSIFICATION		Rhein Westfal TH Aachen, Inst Stat, D-52056 Aachen, Germany; Univ Roma La Sapienza, Dept Stat Probabil & Appl Stat, I-00185 Rome, Italy	Bock, HH (reprint author), Rhein Westfal TH Aachen, Inst Stat, D-52056 Aachen, Germany.	bock@stochastik.rwth-aachen.de; maurizio.vichi@uniromal.it					Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Louw N, 2006, COMPUT STAT DATA AN, V51, P2043, DOI 10.1016/j.csda.2005.12.018; BANFIELD JD, 1993, BIOMETRICS, V49, P803, DOI 10.2307/2532201; Avalos M, 2007, COMPUT STAT DATA AN, V51, P2851, DOI 10.1016/j.csda.2006.10.007; Bocci L, 2006, COMPUT STAT DATA AN, V50, P1625, DOI 10.1016/j.csda.2005.02.007; BOUVEYRON C, 2007, COMPUT STAT DATA ANA; CALO DG, 2007, COMPUT STAT DATA ANA; CELEUX G, 1995, PATTERN RECOGN, V28, P781, DOI 10.1016/0031-3203(94)00125-6; CUMMING JA, 2007, COMPUT STAT DATA ANA; GALIMBERTI G, 2007, COMPUT STAT DATA ANA; Lee TH, 2006, COMPUT STAT DATA AN, V51, P659, DOI 10.1016/j.csda.2006.02.015; LOMBARDO R, 2007, COMPUT STAT DATA ANA; MCCANN L, 2007, COMPUT STAT DATA ANA; MEINSHAUSEN N, 2007, COMPUT STAT DATA ANA; PLASSE M, 2007, COMPUT STAT DATA ANA; QUEVEDO JR, 2007, COMPUT STAT DATA ANA; Sauerbrei W, 2006, COMPUT STAT DATA AN, V50, P3464, DOI 10.1016/j.csda.2005.07.015; SCRUCCA L, 2007, COMPUT STAT DATA ANA; SIMILA T, 2007, COMPUT STAT DATA ANA; STROBL C, 2007, COMPUT STAT DATA ANA; TAKANE Y, 2007, COMPUT STAT DATA ANA; TEBBENS JD, 2007, COMPUT STAT DATA ANA; TENENHAUS A, 2007, COMPUT STAT DATA ANA; Trendafilov NT, 2006, COMPUT STAT DATA AN, V50, P242, DOI 10.1016/j.csda.2004.07.017; TRENDAFILOV NT, 2007, COMPUT STAT DATA ANA; Turlach BA, 2006, COMPUT STAT DATA AN, V50, P642, DOI 10.1016/j.csda.2004.09.007; TUTZ G, 2007, COMPUT STAT DATA ANA; VANMECHELEN I, 2007, COMPUT STAT DATA ANA; Yang H, 2003, PATTERN RECOGN, V36, P563; ZHANG C, 2007, COMPUT STAT DATA ANA	30	2	2	0	1	ELSEVIER SCIENCE BV	AMSTERDAM	PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS	0167-9473			COMPUT STAT DATA AN	Comput. Stat. Data Anal.	SEP 15	2007	52	1					370	373		10.1016/j.csda.2007.04.012		4	Computer Science, Interdisciplinary Applications; Statistics & Probability	Computer Science; Mathematics	216XN	WOS:000249912400031		
J	Meinshausen, N				Meinshausen, Nicolai			Relaxed lasso	COMPUTATIONAL STATISTICS & DATA ANALYSIS			English	Article						high dimensionality; bridge estimation; lasso; l(q)-norm penalisation; dimensionality reduction	NONCONCAVE PENALIZED LIKELIHOOD; VARIABLE SELECTION; REGRESSION; ASYMPTOTICS	The Lasso is an attractive regularisation method for high-dimensional regression. It combines variable selection with an efficient computational procedure. However, the rate of convergence of the Lasso is slow for some sparse high-dimensional data, where the number of predictor variables is growing fast with the number of observations. Moreover, many noise variables are selected if the estimator is chosen by cross-validation. It is shown that the contradicting demands of an efficient computational procedure and fast convergence rates of the l(2)-loss can be overcome by a two-stage procedure, termed the relaxed Lasso. For orthogonal designs, the relaxed Lasso provides a continuum of solutions that include both soft- and hard-thresholding of estimators. The relaxed Lasso solutions include all regular Lasso solutions and computation of all relaxed Lasso solutions is often identically expensive as computing all regular Lasso solutions. Theoretical and numerical results demonstrate that the relaxed Lasso produces sparser models with equal or lower prediction loss than the regular Lasso estimator for high-dimensional data. (c) 2007 Elsevier B. V. All rights reserved.	Univ Calif Berkeley, Dept Stat, Berkeley, CA 94720 USA	Meinshausen, N (reprint author), Univ Calif Berkeley, Dept Stat, 367 Evans Hall, Berkeley, CA 94720 USA.	nicolai@stat.berkeley.edu					BREIMAN L, 1995, TECHNOMETRICS, V37, P373, DOI 10.2307/1269730; HUBER PJ, 1973, ANN STAT, V1, P799, DOI 10.1214/aos/1176342503; Knight K, 2000, ANN STAT, V28, P1356; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; FRANK IE, 1993, TECHNOMETRICS, V35, P109, DOI 10.2307/1269656; Efron B, 2004, ANN STAT, V32, P407; Meinshausen N, 2006, ANN STAT, V34, P1436, DOI 10.1214/009053606000000281; Fan JQ, 2004, ANN STAT, V32, P928, DOI 10.1214/009053604000000256; McCullagh P., 1989, GEN LINEAR MODELS; Osborne MR, 2000, J COMPUT GRAPH STAT, V9, P319, DOI 10.2307/1390657; Tsybakov AB, 2005, ANN STAT, V33, P1203, DOI 10.1214/009053604000001066; Van De Geer SA, 2004, BERNOULLI, V10, P939, DOI 10.3150/bj/1106314843; ZHAO P, 2004, 678 U CAL	14	99	102	4	11	ELSEVIER SCIENCE BV	AMSTERDAM	PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS	0167-9473			COMPUT STAT DATA AN	Comput. Stat. Data Anal.	SEP 15	2007	52	1					374	393		10.1016/j.csda.2006.12.019		20	Computer Science, Interdisciplinary Applications; Statistics & Probability	Computer Science; Mathematics	216XN	WOS:000249912400032		
J	Simila, T; Tikka, J				Simila, Timo; Tikka, Jarkko			Input selection and shrinkage in multiresponse linear regression	COMPUTATIONAL STATISTICS & DATA ANALYSIS			English	Article						subset selection; variable selection; constrained regression; multivariate regression; convex optimization; cone programming	MULTIVARIATE REGRESSION; VARIABLE SELECTION; MODEL SELECTION; ALGORITHMS; RESPONSES	The regression problem of modeling several response variables using the same set of input variables is considered. The model is linearly parameterized and the parameters are estimated by minimizing the error sum of squares subject to a sparsity constraint. The constraint has the effect of eliminating useless inputs and constraining the parameters of the remaining inputs in the model. Two algorithms for solving the resulting convex cone programming problem are proposed. The first algorithm gives a pointwise solution, while the second one computes the entire path of solutions as a function of the constraint parameter. Based on experiments with real data sets, the proposed method has a similar performance to existing methods. In simulation experiments, the proposed method is competitive both in terms of prediction accuracy and correctness of input selection. The advantages become more apparent when many correlated inputs are available for model construction. (c) 2007 Elsevier B.V. All rights reserved.	Helsinki Univ Technol, Lab Comp & Informat Sci, FI-02015 Helsinki, Finland	Simila, T (reprint author), Helsinki Univ Technol, Lab Comp & Informat Sci, PO Box 5400, FI-02015 Helsinki, Finland.	timo.simila@hut.fi; tikka@cis.hut.fi					Abraham B, 2005, COMPUT STAT DATA AN, V48, P5, DOI 10.1016/j.csda.2003.11.021; Allgower E., 1993, ACTA NUMERICA, V2, P1, DOI 10.1017/S0962492900002336; ANDERSON R.L., 1952, STAT THEORY RES; RAKOWSKA J, 1991, STRUCT OPTIMIZATION, V3, P29, DOI 10.1007/BF01743487; Malioutov D, 2005, IEEE T SIGNAL PROCES, V53, P3010, DOI 10.1109/TSP.2005.850882; Yuan M, 2006, J ROY STAT SOC B, V68, P49, DOI 10.1111/j.1467-9868.2005.00532.x; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Alizadeh F, 2003, MATH PROGRAM, V95, P3, DOI 10.1007/s10107-002-0339-5; Osborne MR, 2000, IMA J NUMER ANAL, V20, P389, DOI 10.1093/imanum/20.3.389; Lin Y, 2006, ANN STAT, V34, P2272, DOI 10.1214/009053606000000722; Sturm JF, 1999, OPTIM METHOD SOFTW, V11-2, P625, DOI 10.1080/10556789908805766; Brown PJ, 2002, J R STAT SOC B, V64, P519, DOI 10.1111/1467-9868.00348; Tropp JA, 2006, SIGNAL PROCESS, V86, P589, DOI 10.1016/j.sigpro.2005.05.031; Turlach BA, 2005, TECHNOMETRICS, V47, P349, DOI 10.1198/004017005000000139; Efron B, 2004, ANN STAT, V32, P407; Cotter SF, 2005, IEEE T SIGNAL PROCES, V53, P2477, DOI 10.1109/TSP.2005.849172; BACH FR, 2005, ADV NEURAL INFORM PR, P73; Bakin S, 1999, THESIS AUSTR NATL U; BARRY MJ, 1994, CURR OPIN UROL, V4, P3, DOI 10.1097/00042307-199401000-00002; BEDRICK EJ, 1994, BIOMETRICS, V50, P226, DOI 10.2307/2533213; Breiman L, 1996, ANN STAT, V24, P2350; Breiman L, 1997, J ROY STAT SOC B MET, V59, P3, DOI 10.1111/1467-9868.00054; BROWN PJ, 1980, ANN STAT, V8, P64, DOI 10.1214/aos/1176344891; Burnham AJ, 1999, CHEMOMETR INTELL LAB, V48, P167, DOI 10.1016/S0169-7439(99)00018-0; DERKSEN S, 1992, BRIT J MATH STAT PSY, V45, P265; Hiriart-Urruty J., 1996, CONVEX ANAL MINIMIZA, VI; Kim Y, 2006, STAT SINICA, V16, P375; Lutz RW, 2006, STAT SINICA, V16, P471; MEIER L, 2006, GROUP LASSO LOGISTIC; PARK M. Y., 2006, REGULARIZATION PATH; Reinsel G., 1998, MULTIVARIATE REDUCED; Rencher AC., 2002, METHODS MULTIVARIATE; Rosset S, 2005, ADV NEURAL INFORM PR, V1153, P1153; Simila T, 2006, IEEE IJCNN, P1908; SPARKS RS, 1985, COMMUN STAT-THEOR M, V14, P1569, DOI 10.1080/03610928508828996; Srivastava M. S., 2002, METHODS MULTIVARIATE; Srivastava MS, 2003, COMMUN STAT-SIMUL C, V32, P389, DOI 10.1081/SAC-120017497; Turlach B., 2006, JOINT STAT M SEATTL; TURLACH BA, 2005, S OPT DAT AN CANB AU; Vandenberghe L., 2004, CONVEX OPTIMIZATION; Zhao P, 2006, GROUPED HIERARCHICAL; ZOU H, 2005, F ALPHA NORM SUPPORT	42	21	22	0	5	ELSEVIER SCIENCE BV	AMSTERDAM	PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS	0167-9473			COMPUT STAT DATA AN	Comput. Stat. Data Anal.	SEP 15	2007	52	1					406	422		10.1016/j.csda.2007.01.025		17	Computer Science, Interdisciplinary Applications; Statistics & Probability	Computer Science; Mathematics	216XN	WOS:000249912400034		
J	Zou, H; Hastie, T; Tibshirani, R				Zou, Hui; Hastie, Trevor; Tibshirani, Robert			On the "degrees of freedom" of the lasso	ANNALS OF STATISTICS			English	Article						degrees of freedom; LARS algorithm; lasso; model selection; SURE; unbiased estimate	NONCONCAVE PENALIZED LIKELIHOOD; ADAPTIVE MODEL SELECTION; VARIABLE SELECTION; REGRESSION; SHRINKAGE	We study the effective degrees of freedom of the lasso in the framework of Stein's unbiased risk estimation (SURE). We show that the number of nonzero coefficients is an unbiased estimate for the degrees of freedom of the lasso-a conclusion that requires no special assumption on the predictors. In addition, the unbiased estimator is shown to be asymptotically consistent. With these results on hand, various model selection criteria-C-p, AIC and BIC-are available, which, along with the LARS algorithm, provide a principled and efficient approach to obtaining the optimal lasso fit with the computational effort of a single ordinary least-squares fit.	Univ Minnesota, Sch Stat, Minneapolis, MN 55455 USA; Stanford Univ, Dept Stat, Stanford, CA 94305 USA	Zou, H (reprint author), Univ Minnesota, Sch Stat, Minneapolis, MN 55455 USA.	hzou@stat.umn.edu; hastie@stanford.edu; tibs@stanford.edu	Yang, Ying/G-4302-2012				AKAIKE H, 1973, Patent No. 0483125; Shen XT, 2002, J AM STAT ASSOC, V97, P210, DOI 10.1198/016214502753479356; Knight K, 2000, ANN STAT, V28, P1356; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; Shen XT, 2006, J AM STAT ASSOC, V101, P554, DOI 10.1198/016214505000001078; Osborne MR, 2000, IMA J NUMER ANAL, V20, P389, DOI 10.1093/imanum/20.3.389; Efron B, 2004, ANN STAT, V32, P407; SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136; MALLOWS CL, 1973, TECHNOMETRICS, V15, P661, DOI 10.2307/1267380; Buhlmann P., 2005, BOOSTING MODEL SELEC; Donoho DL, 1995, J AM STAT ASSOC, V90, P1200, DOI 10.2307/2291512; Efron B, 2004, J AM STAT ASSOC, V99, P619, DOI 10.1198/016214504000000692; FAN J, 2006, Patent No. 2275698; Fan JQ, 2004, ANN STAT, V32, P928, DOI 10.1214/009053604000000256; Gunter L, 2007, NEURAL COMPUT, V19, P1633, DOI 10.1162/neco.2007.19.6.1633; HASTIE T, 2001, Patent No. 1851606; HASTIE T, 1990, Patent No. 1082147; Meyer M, 2000, ANN STAT, V28, P1083; Shao J, 1997, STAT SINICA, V7, P221; Shen XT, 2004, TECHNOMETRICS, V46, P306, DOI 10.1198/004017004000000338; STEIN CM, 1981, ANN STAT, V9, P1135, DOI 10.1214/aos/1176345632; Yang YH, 2005, BIOMETRIKA, V92, P937, DOI 10.1093/biomet/92.4.937; Ye JM, 1998, J AM STAT ASSOC, V93, P120, DOI 10.2307/2669609; Zhao P, 2006, GROUPED HIERARCHICAL; ZOU H, 2005, THESIS STANFORD U	26	219	222	2	18	INST MATHEMATICAL STATISTICS	BEACHWOOD	PO BOX 22718, BEACHWOOD, OH 44122 USA	0090-5364			ANN STAT	Ann. Stat.	OCT	2007	35	5					2173	2192		10.1214/009053607000000127		20	Statistics & Probability	Mathematics	233MX	WOS:000251096100013		
J	Abramovich, F; Grinshtein, V; Pensky, M				Abramovich, Felix; Grinshtein, Vadim; Pensky, Marianna			On optimality of Bayesian testimation in the normal means problem	ANNALS OF STATISTICS			English	Article						adaptivity; complexity penalty; maximum a posteriori rule; minimax estimation; sequence estimation; sparsity; thresholding	FALSE DISCOVERY RATE; INFLATION CRITERION; VARIABLE SELECTION; REGRESSION; SHRINKAGE; MODEL; RISK	We consider a problem of recovering a high-dimensional vector mu observed in white noise, where the unknown vector g is assumed to be sparse. The objective of the paper is to develop a Bayesian formalism which gives rise to a family of l(0)-type penalties. The penalties are associated with various choices of the prior distributions pi(n)(center dot) on the number of nonzero entries of mu and, hence, are easy to interpret. The resulting Bayesian estimators lead to a general thresholding rule which accommodates many of the known thresholding and model selection procedures as particular cases corresponding to specific choices of pi(n)(center dot). Furthermore, they achieve optimality in a rather general setting under very mild conditions on the prior. We also specify the class of priors pi(n)(center dot) for which the resulting estimator is adaptively optimal (in the minimax sense) for a wide range of sparse sequences and consider several examples of such priors.	Tel Aviv Univ, Dept Stat & Operat Res, IL-69978 Tel Aviv, Israel; Open Univ, Dept Math, IL-43107 Raanana, Israel; Univ Cent Florida, Dept Math, Orlando, FL 32816 USA	Abramovich, F (reprint author), Tel Aviv Univ, Dept Stat & Operat Res, IL-69978 Tel Aviv, Israel.	fe1ix@post.tau.ac.il; vadimg@oumail.openu.ac.il; mpensky@pegasus.cc.ucf.edu					Abramovich F, 1996, COMPUT STAT DATA AN, V22, P351, DOI 10.1016/0167-9473(96)00003-5; Abramovich F., 1995, LECT NOTES STAT, V103, P5; Abramovich F., 2006, SANKHYA, V68, P436; Akaike H., 1973, 2 INT S INF THEOR, P267; HOCHBERG Y, 1988, BIOMETRIKA, V75, P800, DOI 10.1093/biomet/75.4.800; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Antoniadis A, 2001, J AM STAT ASSOC, V96, P939, DOI 10.1198/016214501753208942; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; Abramovich F, 2006, ANN STAT, V34, P584, DOI 10.1214/009053606000000074; FRANK IE, 1993, TECHNOMETRICS, V35, P109, DOI 10.2307/1269656; HOLM S, 1979, SCAND J STAT, V6, P65; BENJAMINI Y, 1995, J ROY STAT SOC B MET, V57, P289; DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425; SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136; Birge L., 2001, J EUR MATH SOC, V3, P203, DOI 10.1007/s100970100031; DONOHO D L, 1996, BERNOULLI, V2, P39, DOI 10.3150/bj/1193758789; DONOHO DL, 1994, PROBAB THEORY REL, V99, P277, DOI 10.1007/BF01199026; DONOHO DL, 1992, J ROY STAT SOC B MET, V54, P41; Foster DP, 1999, IEEE T INFORM THEORY, V45, P1289, DOI 10.1109/18.761287; FOSTER DP, 1994, ANN STAT, V22, P1947, DOI 10.1214/aos/1176325766; Hunter DR, 2005, ANN STAT, V33, P1617, DOI 10.1214/009053605000000200; JOHNSTONE IM, 2002, UNPUB FUNCTION ESTIM; Johnstone I.M., 1994, STATISTICAL DECISION, VV, P303; Johnstone IM, 2004, ANN STAT, V32, P1594, DOI 10.1214/009053604000000030; Sarkar SK, 2002, ANN STAT, V30, P239, DOI 10.1214/aos/1015362192; Tibshirani R, 1999, J ROY STAT SOC B, V61, P529, DOI 10.1111/1467-9868.00191	26	18	18	0	1	INST MATHEMATICAL STATISTICS	BEACHWOOD	PO BOX 22718, BEACHWOOD, OH 44122 USA	0090-5364			ANN STAT	Ann. Stat.	OCT	2007	35	5					2261	2286		10.1214/009053607000000226		26	Statistics & Probability	Mathematics	233MX	WOS:000251096100017		
J	Xu, SH				Xu, Shizhong			Derivation of the shrink-age estimates of quantitative trait locus effects	GENETICS			English	Article							VARIABLE SELECTION	The shrinkage estimate of a quantitative trait locus (QTL) effect is the posterior mean of the QTL effect when a normal prior distribution is assigned to the QTL. This note gives the derivation of the shrinkage estimate under the multivariate linear model. An important lemma regarding the posterior mean of a normal likelihood combined with a normal prior is introduced. The lemma is then used to derive the Bayesian shrinkage estimates of the QTL effects.	Univ Calif Riverside, Dept Bot & Plant Sci, Riverside, CA 92521 USA	Xu, SH (reprint author), Univ Calif Riverside, Dept Bot & Plant Sci, 900 Univ Ave, Riverside, CA 92521 USA.	xu@genetics.ucr.edu					LINDLEY DV, 1972, J ROY STAT SOC B, V34, P1; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Box GEP, 1973, BAYESIAN INFERENCE S; Gelman A, 2006, BAYESIAN ANAL, V1, P515; Gelman A, 2005, ANN STAT, V33, P1, DOI 10.1214/009053604000001048; George EI, 1993, J AM STAT ASSOC, V91, P883; GIRI NC, 1996, MULTIVARIATE STAT AN; Ishwaran H, 2005, ANN STAT, V33, P730, DOI 10.1214/009053604000001147; Robinson G. K., 1991, STAT SCI, V6, P15, DOI DOI 10.1214/SS/1177011926; Wang H, 2005, GENETICS, V170, P465, DOI 10.1534/genetics.104.039354; Xu SZ, 2007, BIOMETRICS, V63, P513, DOI 10.1111/j.1541-0420.2006.00711.x; Xu SZ, 2003, GENETICS, V163, P789; Yang RQ, 2007, GENETICS, V176, P1169, DOI 10.1534/genetics.106.064279; Yi NJ, 2003, GENETICS, V164, P1129	14	11	11	0	1	GENETICS	BALTIMORE	428 EAST PRESTON ST, BALTIMORE, MD 21202 USA	0016-6731			GENETICS	Genetics	OCT	2007	177	2					1255	1258		10.1534/genetics.107.077487		4	Genetics & Heredity	Genetics & Heredity	227LI	WOS:000250657800052	17720913	
J	Li, P; Hastie, TJ; Church, KW				Li, Ping; Hastie, Trevor J.; Church, Kenneth W.			Nonlinear estimators and tail bounds for dimension reduction in l(1) using Cauchy random projections	JOURNAL OF MACHINE LEARNING RESEARCH			English	Article						dimension reduction; l(1) norm; Johnson-Lindenstrauss (JL) lemma; Cauchy random projections	INVERSE GAUSSIAN DISTRIBUTIONS; SYMMETRIC STABLE DISTRIBUTIONS; MAXIMUM LIKELIHOOD ESTIMATORS; SCALE PARAMETERS; STATISTICAL PROPERTIES; JOHNSON-LINDENSTRAUSS; LOCATION; REGRESSION	For(1) dimension reduction in the l(1) norm, the method of Cauchy random projections multiplies the original data matrix A epsilon R-nxD with a random matrix R epsilon R-Dxk (k<<D) whose entries are i.i.d. samples of the standard Cauchy C (0, 1). Because of the impossibility result, one can not hope to recover the pairwise l(1) distances in A from B = AxR epsilon R-nxk, using linear estimators without incurring large errors. However, nonlinear estimators are still useful for certain applications in data stream computations, information retrieval, learning, and data mining. We study three types of nonlinear estimators: the sample median estimators, the geometric mean estimators, and the maximum likelihood estimators (MLE). We derive tail bounds for the geometric mean estimators and establish that k = O (logn/epsilon(2)) suffices with the constants explicitly given. Asymptotically (as k ->infinity), both the sample median and the geometric mean estimators are about 80% efficient compared to the MLE. We analyze the moments of the MLE and propose approximating its distribution of by an inverse Gaussian.	[Li, Ping] Cornell Univ, Fac Computing & Informat Sci, Dept Stat Sci, Ithaca, NY 14853 USA; [Hastie, Trevor J.] Stanford Univ, Dept Stat, Stanford, CA 94305 USA; [Church, Kenneth W.] Microsoft Corp, Microsoft Res, Redmond, WA 98052 USA	Li, P (reprint author), Cornell Univ, Fac Computing & Informat Sci, Dept Stat Sci, Ithaca, NY 14853 USA.	PINGLI@CORNELL.EDU; HASTIE@STANFORD.EDU; CHURCH@MICROSOFT.COM	Church, Kenneth/G-3167-2010				Achlioptas D, 2001, P ACM S PRINC DAT SY, P274, DOI DOI 10.1109/TIT.2006.885507; Aggarwal CC, 1999, SIGMOD RECORD, VOL 28, NO 2 - JUNE 1999, P407; Ailon N., 2006, STOC'06. Proceedings of the 38th Annual ACM Symposium on Theory of Computing; Achlioptas D, 2003, J COMPUT SYST SCI, V66, P671, DOI 10.1016/S0022-0000(03)00025-4; Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Lee JR, 2004, GEOM FUNCT ANAL, V14, P745, DOI 10.1007/s00039-004-0473-8; MCCULLOCH JH, 1986, COMMUN STAT SIMULAT, V15, P1109, DOI 10.1080/03610918608812563; Chapelle O, 1999, IEEE T NEURAL NETWOR, V10, P1055, DOI 10.1109/72.788646; CHERNOFF H, 1952, ANN MATH STAT, V23, P493, DOI 10.1214/aoms/1177729330; Dasgupta S, 2003, RANDOM STRUCT ALGOR, V22, P60, DOI 10.1002/rsa.10073; Indyk P, 2006, J ACM, V53, P307, DOI 10.1145/1147954.1147955; Efron B, 2004, ANN STAT, V32, P407; Brinkman B, 2005, J ACM, V52, P766, DOI 10.1145/1089023.1089026; ANTLE CE, 1969, SIAM REV, V11, P251, DOI 10.1137/1011039; Arriaga RI, 2006, MACH LEARN, V63, P161, DOI 10.1007/s10994-006-6265-7; Babcock B., 2002, P 21 ACM SIGACT SIGM, P1; BARTLETT MS, 1953, BIOMETRIKA, V40, P306; BHATTACHARYA RN, 1978, ANN STAT, V6, P434, DOI 10.1214/aos/1176344134; Brinkman B., 2003, FOCS, P514; Chhikara R. S., 1989, INVERSE GAUSSIAN DIS; CORMODE G, 2001, VLDB, P335; Cormode G, 2003, IEEE T KNOWL DATA EN, V15, P529, DOI 10.1109/TKDE.2003.1198388; Cormode G., 2003, ESA, P148; CYSNEIROS FJD, 2001, BRAZILIAN J PROBABIL, V15, P85; Dhillon IS, 2001, MACH LEARN, V42, P143, DOI 10.1023/A:1007612920971; FAMA EF, 1968, J AM STAT ASSOC, V63, P817, DOI 10.2307/2283875; FAMA EF, 1971, J AM STAT ASSOC, V66, P331, DOI 10.2307/2283932; Feigenbaum J., 1999, FOCS, P501; Feller W., 1971, INTRO PROBABILITY TH, V2; Ferrari SLP, 1996, STAT PROBABIL LETT, V30, P339, DOI 10.1016/S0167-7152(95)00237-5; Fisher RA, 1934, P R SOC LOND A-CONTA, V144, P0285, DOI 10.1098/rspa.1934.0050; FRANKL P, 1988, J COMB THEORY B, V44, P355, DOI 10.1016/0095-8956(88)90043-3; Gerber H. U., 1991, INSUR MATH ECON, V10, P303; Gradshteyn I. S., 1994, TABLE INTEGRALS SERI; HAAS G, 1970, BIOMETRIKA, V57, P403, DOI 10.2307/2334849; HENZINGER MR, 1999, COMPUTING DATA STREA; HINKLEY DV, 1978, BIOMETRIKA, V65, P253, DOI 10.1093/biomet/65.2.253; HOUGAARD P, 1986, BIOMETRIKA, V73, P387, DOI 10.1093/biomet/73.2.387; Huber P. J., 1981, ROBUST STAT; Indyk P., 2001, FOCS, P10; Indyk P, 1998, STOC, P604; Indyk Piotr, 2000, FOCS, P189; Jensen J., 1995, SADDLEPOINT APPROXIM; Johnson W. B., 1984, CONT MATH, V26, P189; JOHNSON WB, 1982, ACTA MATH-DJURSHOLM, V149, P71, DOI 10.1007/BF02392350; LAWLESS JF, 1972, BIOMETRIKA, V59, P377, DOI 10.2307/2334582; LI P, 2007, NIPS, P873; Li P, 2007, COMPUT LINGUIST, V33, P305, DOI 10.1162/coli.2007.33.3.305; Li P, 2006, KDD, P287; LI P, 2005, HLT EMNLP, P708; LI P, 2006, COLT, P635; LI P, 2007, COLT; LI P, 2007, KDD; Li P., 2008, SODA; Li P, 2006, IEEE T INFORM THEORY, V52, P271, DOI 10.1109/TIT.2005.860466; Lugosi G., 2004, LECT NOTES; PHILIPS TK, 1995, AM STAT, V49, P175, DOI 10.2307/2684633; Rosa I., 1999, FOCS, P616; Seshadri V., 1993, INVERSE GAUSSIAN DIS; Severini T.A., 2000, LIKELIHOOD METHODS S; Shakhnarovich G., 2005, NEAREST NEIGHBOR MET; SHENTON LR, 1963, J ROY STAT SOC B, V25, P305; Strehl A., 2000, High Performance Computing - HiPC 2000. 7th International Conference. Proceedings (Lecture Notes in Computer Science Vol.1970); TWEEDIE MCK, 1957, ANN MATH STAT, V28, P362, DOI 10.1214/aoms/1177706964; TWEEDIE MCK, 1957, ANN MATH STAT, V28, P696, DOI 10.1214/aoms/1177706881; Vempala S. S., 2004, RANDOM PROJECTION ME; Zhu J., 2003, NIPS; ZOLOTAREV VM, 1986, ONE DIMENSIONAL STAB	69	10	10	0	2	MICROTOME PUBL	BROOKLINE	31 GIBBS ST, BROOKLINE, MA 02446 USA	1532-4435			J MACH LEARN RES	J. Mach. Learn. Res.	OCT	2007	8						2497	2532				36	Automation & Control Systems; Computer Science, Artificial Intelligence	Automation & Control Systems; Computer Science	256QN	WOS:000252744800010		
J	Sjostrand, K; Hansen, MS; Larsson, HB; Larsen, R				Sjostrand, Karl; Hansen, Michael Sass; Larsson, Henrik B.; Larsen, Rasmus			A path algorithm for the support vector domain description and its application to medical imaging	MEDICAL IMAGE ANALYSIS			English	Article; Proceedings Paper	9th International Conference on Computing and Computer-Assisted Intervention	OCT 01-06, 2006	Copenhagen, DENMARK	AstraZeneca, Ctr Clin & Basic Res, Claron, Elsevier, GE, Medtronic, No Digital Inc, Siemens Corp Res, Springer, Visiopharm		support vector domain description; novelty detection; outlier detection; classification; path algorithm; corpus callosum; myocardial perfusion; ischemic segment detection	REGULARIZATION; CLASSIFICATION; REGRESSION; SELECTION; MRI	The support vector domain description is a one-class classification method that estimates the distributional support of a data set. A flexible closed boundary function is used to separate trustworthy data on the inside from outliers on the outside. A single regularization parameter determines the shape of the boundary and the proportion of observations that are regarded as outliers. Picking an appropriate amount of regularization is crucial in most applications but is, for computational reasons, commonly limited to a small collection of parameter values. This paper presents an algorithm where the solutions for all possible values of the regularization parameter are computed at roughly the same computational complexity previously required to obtain a single solution. Such a collection of solutions is known as a regularization path. Knowledge of the entire regularization path not only aids model selection, but may also provide new information about a, data set. We illustrate this potential of the method in two applications; one where we establish a sensible ordering among a set of corpora callosa outlines, and one where ischemic segments of the myocardium are detected in patients with acute myocardial infarction. (C) 2007 Elsevier B.V. All rights reserved.	Tech Univ Denmark, Informat & Math Modelling, Lyngby, Denmark; Glostrup Cty Hosp, Dept Clin Physiol & Nucl Med, Glostrup, Denmark	Larsen, R (reprint author), Tech Univ Denmark, Informat & Math Modelling, Lyngby, Denmark.	kas@imm.dtu.dk; rl@imm.dtu.dk					Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Tax DMJ, 1999, PATTERN RECOGN LETT, V20, P1191, DOI 10.1016/S0167-8655(99)00087-2; Scholkopf B, 2001, NEURAL COMPUT, V13, P1443, DOI 10.1162/089976601750264965; Zhu J, 2004, ADV NEUR IN, V16, P49; Banerjee A, 2006, IEEE T GEOSCI REMOTE, V44, P2282, DOI 10.1109/TGRS.2006.873019; Vanderbei RJ, 1999, OPTIM METHOD SOFTW, V11-2, P451, DOI 10.1080/10556789908805759; Zou H, 2005, J ROY STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x; Efron B, 2004, ANN STAT, V32, P407; Tax DMJ, 2004, MACH LEARN, V54, P45, DOI 10.1023/B:MACH.0000008084.60811.49; Ban T, 2006, IEEE IJCNN, P327; Ben-Hur A., 2001, J MACHINE LEARNING R, V2, P125; CHOI J, 2006, LECT NOTES COMPUTER, V3971, P991; Dong X, 2001, P 2001 IEEE SIGN PRO, P481; HANSEN M, 2007, INT S MED IM 2007 SA; Hastie T., 2001, ELEMENTS STAT LEARNI; Hastie T, 2004, J MACH LEARN RES, V5, P1391; Lai C, 2004, INT J PATTERN RECOGN, V18, P867, DOI 10.1142/S0218001404003459; Larsson HBW, 1996, MAGNET RESON MED, V35, P716, DOI 10.1002/mrm.1910350513; Lee D, 2007, PATTERN RECOGN, V40, P41, DOI 10.1016/j.patcog.2006.06.008; Lee K, 2007, IEEE T NEURAL NETWOR, V18, P284, DOI 10.1109/TNN.2006.884673; Lee SW, 2006, PATTERN RECOGN, V39, P1809, DOI 10.1016/j.patcog.2006.04.033; OLAFSDOTTIR H, 2004, LNCS, V36, P1060; Pantoni L, 2005, NEUROEPIDEMIOLOGY, V24, P51, DOI 10.1159/000081050; PARK M. Y., 2006, REGULARIZATION PATH; Platt J. C., 1999, FAST TRAINING SUPPOR, P185; Rosset S, 2007, ANN STAT, V35, P1012, DOI 10.1214/009053606000001370; SEO J, 2004, IEEE INT C ACOUST SP, V5, P729; Stegmann MB, 2005, MED IMAGE ANAL, V9, P394, DOI 10.1016/j.media.2004.10.002; Tax DMJ, 1999, LECT NOTES COMPUT SC, V1642, P415; TSANG E, 2005, LECT NOTES COMPUTER, V3749, P572; Vapnik V.N., 1995, NATURE STAT LEARNING	31	9	11	1	7	ELSEVIER SCIENCE BV	AMSTERDAM	PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS	1361-8415			MED IMAGE ANAL	Med. Image Anal.	OCT	2007	11	5					417	428		10.1016/j.media.2007.07.008		12	Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Engineering, Biomedical; Radiology, Nuclear Medicine & Medical Imaging	Computer Science; Engineering; Radiology, Nuclear Medicine & Medical Imaging	223UV	WOS:000250398800002	17822947	
J	Lee, W; Tillo, D; Bray, N; Morse, RH; Davis, RW; Hughes, TR; Nislow, C				Lee, William; Tillo, Desiree; Bray, Nicolas; Morse, Randall H.; Davis, Ronald W.; Hughes, Timothy R.; Nislow, Corey			A high- resolution atlas of nucleosome occupancy in yeast	NATURE GENETICS			English	Article							SACCHAROMYCES-CEREVISIAE; IN-VIVO; EUKARYOTIC GENOME; RNA-POLYMERASE; BUDDING YEAST; Z-DNA; CHROMATIN; TRANSCRIPTION; BINDING; PROMOTER	We present the first complete high-resolution map of nucleosome occupancy across the whole Saccharomyces cerevisiae genome, identifying over 70,000 positioned nucleosomes occupying 81% of the genome. On a genome-wide scale, the persistent nucleosome-depleted region identified previously in a subset of genes demarcates the transcription start site. Both nucleosome occupancy signatures and overall occupancy correlate with transcript abundance and transcription rate. In addition, functionally related genes can be clustered on the basis of the nucleosome occupancy patterns observed at their promoters. A quantitative model of nucleosome occupancy indicates that DNA structural features may account for much of the global nucleosome occupancy.	Stanford Univ, Sch Med, Dept Genet, Stanford, CA 94305 USA; Stanford Genome Technol, Palo Alto, CA 94304 USA; Univ Toronto, Dept Mol & Med Genet, Toronto, ON M5S 1A8, Canada; New York State Dept Hlth, Wadsworth Ctr, Albany, NY 12201 USA; Terrence Donnelly Ctr Cellular & Biomol Res, Toronto, ON M5S 3E1, Canada; Banting & Best Dept Med Res, Toronto, ON M5S 1L6, Canada	Nislow, C (reprint author), Stanford Univ, Sch Med, Dept Genet, Stanford, CA 94305 USA.	corey.nislow@utoronto.ca		Lee, William/0000-0001-9582-4413			Albert I, 2007, NATURE, V446, P572, DOI 10.1038/nature05632; Martinez-Campa C, 2004, MOL CELL, V15, P69, DOI 10.1016/j.molcel.2004.05.022; MacIsaac KD, 2006, BIOINFORMATICS, V22, P423, DOI 10.1093/bioinformatics/bti815; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Sekinger EA, 2005, MOL CELL, V18, P735, DOI 10.1016/j.molcel.2005.05.003; NOLL M, 1977, J MOL BIOL, V109, P393, DOI 10.1016/S0022-2836(77)80019-3; Richmond TJ, 2003, NATURE, V423, P145, DOI 10.1038/nature01595; MacIsaac KD, 2006, BMC BIOINFORMATICS, V7, DOI 10.1186/1471-2105-7-113; David L, 2006, P NATL ACAD SCI USA, V103, P5320, DOI 10.1073/pnas.0601091103; Carrozza MJ, 2005, CELL, V123, P581, DOI 10.1016/j.cell.2005.10.023; Ashburner M, 2000, NAT GENET, V25, P25; SATCHWELL SC, 1986, J MOL BIOL, V191, P659, DOI 10.1016/0022-2836(86)90452-3; Yuan GC, 2005, SCIENCE, V309, P626, DOI 10.1126/science.1112178; Jenuwein T, 2001, SCIENCE, V293, P1074, DOI 10.1126/science.1063127; Raisner RM, 2005, CELL, V123, P233, DOI 10.1016/j.cell.2005.10.002; Efron B, 2004, ANN STAT, V32, P407; Ozsolak F, 2007, NAT BIOTECHNOL, V25, P244, DOI 10.1038/nbt1279; Huh WK, 2003, NATURE, V425, P686, DOI 10.1038/nature02026; Harbison CT, 2004, NATURE, V431, P99, DOI 10.1038/nature02800; Segal E, 2006, NATURE, V442, P772, DOI 10.1038/nature04979; Bernstein BE, 2004, GENOME BIOL, V5, DOI 10.1186/gb-2004-5-9-r62; Champ PC, 2004, NUCLEIC ACIDS RES, V32, P6501, DOI 10.1093/nar/gkh988; ElHassan MA, 1996, J MOL BIOL, V259, P95, DOI 10.1006/jmbi.1996.0304; FEDOR MJ, 1988, J MOL BIOL, V204, P109, DOI 10.1016/0022-2836(88)90603-1; Hogan GJ, 2006, PLOS GENET, V2, P1433, DOI 10.1371/journal.pgen.0020158; Holstege FCP, 1998, CELL, V95, P717, DOI 10.1016/S0092-8674(00)81641-4; Ioshikhes IP, 2006, NAT GENET, V38, P1210, DOI 10.1038/ng1878; Johnson SM, 2006, GENOME RES, V16, P1505, DOI 10.1101/gr.5560806; Kampa D, 2004, GENOME RES, V14, P331, DOI 10.1011/gr.2094104; Kasten MM, 1997, MOL GEN GENET, V256, P376; Keogh MC, 2005, CELL, V123, P593, DOI 10.1016/j.cell.2005.10.025; KORNBERG RD, 1988, NUCLEIC ACIDS RES, V16, P6677, DOI 10.1093/nar/16.14.6677; Lascaris RF, 2000, NUCLEIC ACIDS RES, V28, P1390, DOI 10.1093/nar/28.6.1390; Lee CK, 2004, NAT GENET, V36, P900, DOI 10.1038/ng1400; Liu CL, 2005, PLOS BIOL, V3, P1753, DOI 10.1371/journal.pbio.0030328; Liu X, 2006, GENOME RES, V16, P1517, DOI 10.1101/gr.5655606; LOHR D, 1995, J BIOL CHEM, V270, P27671; Moreira JMA, 1998, EMBO J, V17, P6028, DOI 10.1093/emboj/17.20.6028; Morse RH, 2003, BIOCHEM CELL BIOL, V81, P101, DOI 10.1139/003-039; Ponomarenko JV, 1999, BIOINFORMATICS, V15, P654, DOI 10.1093/bioinformatics/15.7.654; Roth FP, 1998, NAT BIOTECHNOL, V16, P939, DOI 10.1038/nbt1098-939; Sivolob A, 2000, J MOL BIOL, V295, P55, DOI 10.1006/jmbi.1999.3302; STRAKA C, 1991, EMBO J, V10, P361; Studitsky VM, 1997, SCIENCE, V278, P1960, DOI 10.1126/science.278.5345.1960; Suter B, 2000, NUCLEIC ACIDS RES, V28, P4083, DOI 10.1093/nar/28.21.4083; WANG YH, 1994, SCIENCE, V265, P669, DOI 10.1126/science.8036515; Wong B, 2007, P NATL ACAD SCI USA, V104, P2229, DOI 10.1073/pnas.0611447104; Yarragudi A, 2004, MOL CELL BIOL, V24, P9152, DOI 10.1128/MCB.24.20.9152-9164.2004; Zanton SJ, 2006, GENE DEV, V20, P2250, DOI 10.1101/gad.1437506	49	491	501	5	25	NATURE PUBLISHING GROUP	NEW YORK	75 VARICK STREET, 9TH FLOOR, NEW YORK, NY 10013-1917 USA	1061-4036			NAT GENET	Nature Genet.	OCT	2007	39	10					1235	1244		10.1038/ng2117		10	Genetics & Heredity	Genetics & Heredity	214KW	WOS:000249737400019	17873876	
J	Cao, N; Nehorai, A; Jacob, M				Cao, Nannan; Nehorai, Arye; Jacob, Mathews			Image reconstruction for diffuse optical tomography using sparsity regularization and expectation-maximization algorithm	OPTICS EXPRESS			English	Article							SIGNAL RECONSTRUCTION; BOUNDARY-CONDITIONS; EM ALGORITHM; SHRINKAGE; PRIORS; LIGHT	We present an image reconstruction method for diffuse optical tomography (DOT) by using the sparsity regularization and expectation-maximization (EM) algorithm. Typical image reconstruction approaches in DOT employ Tikhonov-type regularization, which imposes restrictions on the L-2 norm of the optical properties (absorption/scattering coefficients). It tends to cause a blurring effect in the reconstructed image and works best when the unknown parameters follow a Gaussian distribution. In reality, the abnormality is often localized in space. Therefore, the vector corresponding to the change of the optical properties compared with the background would be sparse with only a few elements being nonzero. To incorporate this information and improve the performance, we propose an image reconstruction method by regularizing the L-1 norm of the unknown parameters and solve it iteratively using the expectation-maximization algorithm. We verify our method using simulated 3D examples and compare the reconstruction performance of our approach with the level-set algorithm, Tikhonov regularization, and simultaneous iterative reconstruction technique (SIRT). Numerical results show that our method provides better resolution than the Tikhonov-type regularization and is also efficient in estimating two closely spaced abnormalities. (c) 2007 Optical Society of America.	Washington Univ, Dept Elect & Syst Engn, St Louis, MO 63130 USA; Univ Rochester, Dept Biomed Engn, Rochester, NY 14642 USA	Nehorai, A (reprint author), Washington Univ, Dept Elect & Syst Engn, Bryan Hall,Room 201,Campus Box 1127,One Brookings, St Louis, MO 63130 USA.	nehorai@ese.wustl.edu	Jacob, Mathews/F-9205-2011; Nehorai, Arye/G-1661-2011				Dehghani H, 2003, APPL OPTICS, V42, P3117, DOI 10.1364/AO.42.003117; Malioutov D, 2005, IEEE T SIGNAL PROCES, V53, P3010, DOI 10.1109/TSP.2005.850882; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Pogue BW, 1999, APPL OPTICS, V38, P2950, DOI 10.1364/AO.38.002950; HASKELL RC, 1994, J OPT SOC AM A, V11, P2727, DOI 10.1364/JOSAA.11.002727; Charbonnier P, 1997, IEEE T IMAGE PROCESS, V6, P298, DOI 10.1109/83.551699; Villringer A, 1997, TRENDS NEUROSCI, V20, P435, DOI 10.1016/S0166-2236(97)01132-6; PATTERSON MS, 1989, APPL OPTICS, V28, P2331, DOI 10.1364/AO.28.002331; Huang MX, 2006, NEUROIMAGE, V31, P1025, DOI 10.1016/j.neuroimage.2006.01.029; Strangman G, 2002, BIOL PSYCHIAT, V52, P679, DOI 10.1016/S0006-3223(02)01550-0; Gorodnitsky IF, 1997, IEEE T SIGNAL PROCES, V45, P600, DOI 10.1109/78.558475; Cetin M, 2001, IEEE T IMAGE PROCESS, V10, P623, DOI 10.1109/83.913596; DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425; DONOHO DL, 1995, IEEE T INFORM THEORY, V41, P613, DOI 10.1109/18.382009; Figueiredo MAT, 2003, IEEE T IMAGE PROCESS, V12, P906, DOI 10.1109/TIP.2003.814255; ARONSON R, 1995, J OPT SOC AM A, V12, P2532, DOI 10.1364/JOSAA.12.002532; Arridge S. R., 1999, INVERSE PROBL, V15, P41, DOI DOI 10.1088/0266-5611/15/2/022; BOVERMAN G, 2006, P IEEE INT S BIOM IM, P1132; Bradley P. S., 1998, INFORMS Journal on Computing, V10, DOI 10.1287/ijoc.10.2.209; DORN O, 2006, P IEEE INT S BIOM IM, P1015; Douiri A, 2007, MEAS SCI TECHNOL, V18, P87, DOI 10.1088/0957-0233/18/1/011; Figueiredo MAT, 2003, IEEE T PATTERN ANAL, V25, P1150, DOI 10.1109/TPAMI.2003.1227989; Fuchs JJ, 2004, IEEE T INFORM THEORY, V50, P1341, DOI 10.1109/TIT.2004.828141; Gaudette RJ, 2000, PHYS MED BIOL, V45, P1051, DOI 10.1088/0031-9155/45/4/318; Grosenick D, 2003, APPL OPTICS, V42, P3170, DOI 10.1364/AO.42.003170; HOLBOKE MJ, 2000, J BIOMED OPT, V5; Intes X, 2003, MED PHYS, V30, P1039, DOI 10.1118/1.1573791; JACOB M, 2006, J BIOMED OPT, V11, P12; Kilmer ME, 2000, OPT EXPRESS, V7, P481; Kilmer ME, 2003, APPL OPTICS, V42, P3129, DOI 10.1364/AO.42.003129; KIM SJ, 2007, IEEE T SELECTED TOPI; Li A, 2005, APPL OPTICS, V44, P1948, DOI 10.1364/AO.44.001948; Matsuura K, 1997, IEEE T BIO-MED ENG, V44, P720, DOI 10.1109/10.605428; MATSUURA K, 1995, IEEE T BIO-MED ENG, V42, P608, DOI 10.1109/10.387200; McLachlan G.J., EM ALGORITHM EXTENSI; Moulin P, 1999, IEEE T INFORM THEORY, V45, P909, DOI 10.1109/18.761332; Paulsen K. D., 1995, MED PHYS, V22, P619; Paulsen KD, 1996, APPL OPTICS, V35, P3447, DOI 10.1364/AO.35.003447; POGUE BW, 1995, PHYS MED BIOL, V40, P1709, DOI 10.1088/0031-9155/40/10/011; TIKHONOV AN, SOLUTIONS ILLPOSED P; WU CFJ, 1983, ANN STAT, V11, P95, DOI 10.1214/aos/1176346060	41	45	45	1	6	OPTICAL SOC AMER	WASHINGTON	2010 MASSACHUSETTS AVE NW, WASHINGTON, DC 20036 USA	1094-4087			OPT EXPRESS	Opt. Express	OCT 17	2007	15	21					13695	13708		10.1364/OE.15.013695		14	Optics	Optics	235HA	WOS:000251223100026	19550641	
J	Corcoran, J; Higgs, G; Brunsdon, C; Ware, A; Norman, P				Corcoran, Jonathan; Higgs, Gary; Brunsdon, Chris; Ware, Andrew; Norman, Paul			The use of spatial analytical techniques to explore patterns of fire incidence: A South Wales case study	COMPUTERS ENVIRONMENT AND URBAN SYSTEMS			English	Article						GIS; fire incidence; spatial statistics; regression; visualisation	REGRESSION; DEPRIVATION; LONDON	The application of mapping and spatial analytical techniques to explore geographical patterns of crime incidence is well established. In contrast, the analysis of operational incident data routinely collected by fire brigades has received relatively less research attention, certainly in the UK academic literature. The aim of this paper is to redress this balance through the application of spatial analytical techniques that permit an exploration of the spatial dynamics of fire incidents and their relationships with socio-economic variables. By examining patterns for different fire incident types, including household fires, vehicle fires, secondary fires and malicious false alarms in relation to 2001 Census of Population data for an area of South Wales, we demonstrate the potential of such techniques to reveal spatial patterns that may be worthy of further contextual study. Further research is needed to establish how transferable these findings are to other geographical settings and how replicable the findings are at different geographical scales. The paper concludes by drawing attention to the current gaps in knowledge in analysing trends in fire incidence and proposes an agenda to advance such research using spatial analytical techniques. (c) 2007 Elsevier Ltd. All rights reserved.	[Higgs, Gary; Ware, Andrew] Univ Glamorgan, Fac Adv Technol, Pontypridd CF37 1DL, M Glam, Wales; [Corcoran, Jonathan] Univ Queensland, Sch Geog Planning & Architecture, CR SURF, Brisbane, Qld, Australia; [Brunsdon, Chris] Univ Leicester, Dept Geog, Leicester LE1 7RH, Leics, England; [Norman, Paul] Univ Leeds, Sch Geog, Leeds LS2 9JT, W Yorkshire, England	Higgs, G (reprint author), Univ Glamorgan, Fac Adv Technol, Pontypridd CF37 1DL, M Glam, Wales.	jj.corcoran@uq.edu.au; ghiggs@glam.ac.uk; cb179@leicester.ac.uk; jaware@glam.ac.uk; p.d.norman@leeds.ac.uk	Corcoran, Jonathan/A-4631-2010; Brunsdon, Chris/G-6700-2011; 	Norman, Paul/0000-0002-6211-1625; Brunsdon, Chris/0000-0003-4254-1780			*AGI, 1997, EM SERV GI SURV; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; TOWNSEND P, 1987, J SOC POLICY, V16, P125; Kulldorff M, 2001, J ROY STAT SOC A STA, V164, P61, DOI 10.1111/1467-985X.00186; Duncanson M, 2002, FIRE SAFETY J, V37, P165, DOI 10.1016/S0379-7112(01)00033-9; HOERL AE, 1970, TECHNOMETRICS, V12, P55; BOWMAN AW, 1984, BIOMETRIKA, V71, P353; RUNYAN CW, 1992, NEW ENGL J MED, V327, P859, DOI 10.1056/NEJM199209173271207; Jennings CR, 1999, FIRE TECHNOL, V35, P7, DOI 10.1023/A:1015330931387; Holborn PG, 2003, FIRE SAFETY J, V38, P1, DOI 10.1016/S0379-7112(02)00049-8; Brunsdon C, 1996, GEOGR ANAL, V28, P281; Bailey T. C., 1995, INTERACTIVE SPATIAL; Boba R., 2005, CRIME ANAL CRIME MAP; Bowman A.W., 1997, APPL SMOOTHING TECHN; BROWN P, 1999, EUR REG SCI ASS REG; BRUNSDON C, 2007, ENV URBAN SYSTEMS, V31, P52; CAMBELL HJ, 1995, GIS ORG; CHAINEY S, 2002, 6 ANN INT CRIM MAPP; Chainey Spencer, 2005, GIS CRIME MAPPING; Chandler S, 1984, FIRE PREVENTION, V172, P15; Coleman C., 1996, UNDERSTANDING CRIME; COOMBES M, 1994, ENVIRON PLANN C, V12, P53, DOI 10.1068/c120053; CORCORAN J, 2003, SURVEY GIS UPTAKE UK; Craglia M, 2000, URBAN STUD, V37, P711, DOI 10.1080/00420980050003982; DODGE M, 1996, JOINT EUR C EXH GEOG; Goldsmith V., 2000, ANAL CRIME PATTERNS; Gunther Paul., 1981, FIRE J, V75, P52; Harris K. D., 1999, MAPPING CRIME PRINCI; HIRSCHFIELD A., 2001, MAPPING ANAL CRIME D; Krisp JM, 2005, GEOINFORMATION DISAS, P1283, DOI 10.1007/3-540-27468-5_89; Lapidus G., 1998, GEOGRAPHIC INFORM SY, P103; Leipnik MR, 2003, GIS LAW ENFORCEMENT; Levine N., 2004, CRIMESTAT; Malczewski J, 2005, PROF GEOGR, V57, P516, DOI 10.1111/j.1467-9272.2005.00496.x; Merrall S., 2001, MAPPING ANAL CRIME D, P156; NEELY J, 2002, 2 INT C FIR SERV DEP; *ODPM, 2006, FIR HOM FIND 200404; *ODPM, 2005, INT RISK MAN PLANN G; *ODPM, 2004, ARS CONTR FOR SOC EX; ORMSBY D, 2005, FIGHTING PREVENTING, P34; Ratcliffe JH, 2000, INT J GEOGR INF SCI, V14, P669, DOI 10.1080/136588100424963; RUSHTON R, 1998, GIS HLTH, P63; Shai D, 2003, PUBLIC HEALTH REP, V118, P115, DOI 10.1016/S0033-3549(04)50226-1; Silverman B. M., 1986, DENSITY ESTIMATION S; van Oosterom P., 2005, GEOINFORMATION DISAS; Weisburd D., 1995, CRIME PLACE CRIME PR; Wold H., 1966, MULTIVARIATE ANAL, P391; Yang LL, 2003, LECT NOTES ARTIF INT, V2718, P644	48	18	18	1	4	ELSEVIER SCI LTD	OXFORD	THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND	0198-9715			COMPUT ENVIRON URBAN	Comput. Environ. Urban Syst.	NOV	2007	31	6					623	647		10.1016/j.compenvurbsys.2007.01.002		25	Computer Science, Interdisciplinary Applications; Engineering, Environmental; Environmental Studies; Geography; Operations Research & Management Science	Computer Science; Engineering; Environmental Sciences & Ecology; Geography; Operations Research & Management Science	244GE	WOS:000251853600002		
J	Keerthi, SS; Shevade, S				Keerthi, S. Sathiya; Shevade, Shirish			A fast tracking algorithm for generalized LARS/LASSO	IEEE TRANSACTIONS ON NEURAL NETWORKS			English	Article						generalized Least Angle Regression (LARS); Least Absolute Shrinkage and Selection Operator (LASSO); sparse logistic regression	LEAST ANGLE REGRESSION; LOGISTIC-REGRESSION; LASSO; SELECTION	This letter gives an efficient algorithm for tracking the solution curve of sparse logistic regression With respect to the L I regularization parameter. The algorithm is based on approximating the logistic regression loss by a piecewise quadratic function, using Rosset and Zhu's path tracking algorithm on the approximate problem, and then applying a correction to get to the true path. Application of the algorithm to text classification and sparse kernel logistic regression shows that the algorithm is efficient.	Yahoo Res, Burbank, CA 91504 USA; Indian Inst Sci, Dept Comp Sci & Automat, Bangalore 560012, Karnataka, India	Keerthi, SS (reprint author), Yahoo Res, Media Studios N, Burbank, CA 91504 USA.	selvarak@yahoo-inc.com; shifish@csa.iisc.ernet.in					Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Efron B, 2004, ANN STAT, V32, P407; BACH FR, 2005, NEURAL INFORM PROCES, V17; Genkin A., 2004, LARGE SCALE BAYESIAN; Keerthi S. S., 2005, P 22 INT C MACH LEAR, P417, DOI 10.1145/1102351.1102404; Madigan D, 2004, ANN STAT, V32, P465; Osborne MR, 2000, J COMPUT GRAPH STAT, V9, P319, DOI 10.2307/1390657; RATSCH TG, 1998, BENCHMARK REPOSITORY; Rosset S., 2005, ADV NEURAL INFORM PR, V17; ROSSET S, 2004, PIECEWISE LINEAR REG; Roth V, 2004, IEEE T NEURAL NETWOR, V15, P16, DOI 10.1109/TNN.2003.809398; Shevade SK, 2003, BIOINFORMATICS, V19, P2246, DOI 10.1093/bioinformatics/btg308; Zho H., 2005, J R STAT SOC B, V67, P301; Zhu J, 2005, J COMPUT GRAPH STAT, V14, P185, DOI 10.1198/106186005X25619	14	6	6	2	7	IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC	PISCATAWAY	445 HOES LANE, PISCATAWAY, NJ 08855 USA	1045-9227			IEEE T NEURAL NETWOR	IEEE Trans. Neural Netw.	NOV	2007	18	6					1826	1830		10.1109/TNN.2007.900229		5	Computer Science, Artificial Intelligence; Computer Science, Hardware & Architecture; Computer Science, Theory & Methods; Engineering, Electrical & Electronic	Computer Science; Engineering	229FS	WOS:000250789100020		
J	Hirao, T; Okumura, M; Yasuda, N; Isozaki, H				Hirao, Tsutomu; Okumura, Manabu; Yasuda, Norihito; Isozaki, Hideki			Supervised automatic evaluation for summarization with voted regression model	INFORMATION PROCESSING & MANAGEMENT			English	Article						automatic summarization; automatic evaluation; text summarization challenge; document understanding conference; regression model		The high quality evaluation of generated summaries is needed if we are to improve automatic summarization systems. Although human evaluation provides better results than automatic evaluation methods, its cost is huge and it is difficult to reproduce the results. Therefore, we need an automatic method that simulates human evaluation if we are to improve our summarization system efficiently. Although automatic evaluation methods have been proposed, they are unreliable when used for individual summaries. To solve this problem, we propose a supervised automatic evaluation method based on a new regression model called the voted regression model (VRM). VRM has two characteristics: (1) model selection based on 'corrected AIC' to avoid multicollinearity, (2) voting by the selected models to alleviate the problem of overfitting. Evaluation results obtained for TSC3 and DUC2004 show that our method achieved error reductions of about 17-51% compared with conventional automatic evaluation methods. Moreover, our method obtained the highest correlation coefficients in several different experiments. (C) 2007 Elsevier Ltd. All rights reserved.	NTT Corp, NTT Commun Sci Labs, Seika, Kyoto 6190237, Japan; Tokyo Inst Technol, Precis & Intelligence Labs, Midori Ku, Kanagawa 2268503, Japan	Hirao, T (reprint author), NTT Corp, NTT Commun Sci Labs, 2-4 Hikaridai, Seika, Kyoto 6190237, Japan.	hirao@csiab.kecl.ntt.co.jp	Okumura, Manabu/E-3878-2014				Akaike H., 1973, P 2 INT S INF THEOR, P267; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136; Burnham K. P., 2002, MODEL SELECTION MULT; Cancedda N, 2003, J MACH LEARN RES, V3, P1059, DOI 10.1162/153244303322533197; DAUME H, 2005, P WORKSH MULT SUMM E; Doddington G., 2002, P 2 INT C HUM LANG T, P138, DOI 10.3115/1289189.1289273; Galliers J. R., 1996, EVALUATING NATURAL L; HIRAO T, 2005, P HUM LANG TECHN EMP, P145, DOI 10.3115/1220575.1220594; HIRAO T, 2004, WORK NOT 4 NTCIR WOR, P407; Hirao T., 2001, P NAACL 01, P61; Hoerl AE, 1970, TECHNOMETRICS, V12, P56; Lin C. - Y., 2004, P WORKSH TEXT SUMM B, P74; Lin C.-Y., 2004, P 42 ANN M ACL, P606; LIN CY, 2003, P ANN M N AM ASS COM, P150; Mani I., 2002, NAT LANG ENG, V8, P43; Morris A., 1992, INFORM SYST RES, V3, P17, DOI 10.1287/isre.3.1.17; Papineni K., 2002, P 40 ANN M ASS COMP, P311, DOI DOI 10.3115/1073083.1073135; SORICUT R, 2004, P 42 ANN M ASS COMP, P614; SUGIURA N, 1978, COMMUN STAT A-THEOR, V7, P13, DOI 10.1080/03610927808827599	20	8	8	0	1	PERGAMON-ELSEVIER SCIENCE LTD	OXFORD	THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND	0306-4573			INFORM PROCESS MANAG	Inf. Process. Manage.	NOV	2007	43	6					1521	1535		10.1016/j.ipm-2007.01.012		15	Computer Science, Information Systems; Information Science & Library Science	Computer Science; Information Science & Library Science	214MO	WOS:000249742500008		
J	Buhlmann, P; Hothorn, T				Buehlmann, Peter; Hothorn, Torsten			Boosting algorithms: Regularization, prediction and model fitting	STATISTICAL SCIENCE			English	Article						generalized linear models; generalized additive models; gradient boosting; survival analysis; variable selection; software	ADDITIVE LOGISTIC-REGRESSION; GENE-EXPRESSION DATA; VARIABLE SELECTION; STATISTICAL VIEW; MONOTONIC REGRESSION; TUMOR CLASSIFICATION; GRADIENT DESCENT; LINEAR-MODELS; LASSO; CONSISTENCY	We present a statistical perspective on boosting. Special emphasis is given to estimating potentially complex parametric or nonparametric models, including generalized linear and additive models as well as regression models for survival analysis. Concepts of degrees of freedom and corresponding Akaike or Bayesian information criteria, particularly useful for regularization and variable selection in high-dimensional covariate spaces, are discussed as well. The practical aspects of boosting procedures for fitting statistical models are illustrated by means of the dedicated open-source software package mboost. This package implements functions which can be used for model fitting, prediction and variable selection. It is flexible, allowing for the implementation of new boosting algorithms optimizing user-specified loss functions.	[Buehlmann, Peter] ETH, CH-8092 Zurich, Switzerland; [Hothorn, Torsten] Univ Munich, Inst Stat, D-80539 Munich, Germany	Buhlmann, P (reprint author), ETH, CH-8092 Zurich, Switzerland.	buhlmann@stat.math.ethz.ch; Torsten.Hothorn@R-project.org	Hothorn, Torsten/A-3639-2010; Buhlmann, Peter/A-2107-2013	Hothorn, Torsten/0000-0001-8301-0471; 			Koltchinskii V, 2002, ANN STAT, V30, P1; BREIMAN L, 1995, TECHNOMETRICS, V37, P373, DOI 10.2307/1269730; Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451; Friedman J, 2000, ANN STAT, V28, P337, DOI 10.1214/aos/1016218223; Hansen MH, 2001, J AM STAT ASSOC, V96, P746, DOI 10.1198/016214501753168398; McCaffrey DF, 2004, PSYCHOL METHODS, V9, P403, DOI 10.1037/1082-989X.9.4.403; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Jiang WX, 2004, ANN STAT, V32, P13; Zhang T, 2005, ANN STAT, V33, P1538, DOI 10.1214/009053605000000255; Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324; Park MY, 2007, J ROY STAT SOC B, V69, P659, DOI 10.1111/j.1467-9868.2007.00607.x; Breiman L, 1998, ANN STAT, V26, P801; Breiman L, 1999, NEURAL COMPUT, V11, P1493, DOI 10.1162/089976699300016106; Zhao P, 2006, J MACH LEARN RES, V7, P2541; Osborne MR, 2000, IMA J NUMER ANAL, V20, P389, DOI 10.1093/imanum/20.3.389; MALLAT SG, 1993, IEEE T SIGNAL PROCES, V41, P3397, DOI 10.1109/78.258082; Schapire RE, 2000, MACH LEARN, V39, P135, DOI 10.1023/A:1007649029923; Buhlmann P, 2003, J AM STAT ASSOC, V98, P324, DOI 10.1198/016214503000125; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Mease D, 2007, J MACH LEARN RES, V8, P409; Zou H, 2006, J AM STAT ASSOC, V101, P1418, DOI 10.1198/016214506000000735; Efron B, 2004, ANN STAT, V32, P407; Amit Y, 1997, NEURAL COMPUT, V9, P1545, DOI 10.1162/neco.1997.9.7.1545; Ratsch G, 2001, MACH LEARN, V42, P287, DOI 10.1023/A:1007618119488; West M, 2001, P NATL ACAD SCI USA, V98, P11462, DOI 10.1073/pnas.201162998; Meinshausen N, 2006, ANN STAT, V34, P1436, DOI 10.1214/009053606000000281; Schapire RE, 1998, ANN STAT, V26, P1651; AUDRINO F, 2005, COMPUTATIONAL MANAGE, V2, P87, DOI 10.1007/s10287-004-0028-3; Audrino F, 2005, J BANK FINANC, V29, P959, DOI 10.1016/j.jbankfin.2004.08.008; Audrino F., 2003, J COMPUT FINANC, V6, P65; BARTLETT P, 2003, P 13 IFAC S SYST ID; Bartlett PL, 2006, J AM STAT ASSOC, V101, P138, DOI 10.1198/016214505000000907; Bartlett PL, 2007, J MACH LEARN RES, V8, P2347; BENNER A, 2002, P COMP STAT COMPSTAT, P171; BINDER H, 2006, GAMBOOST GEN ADDITIV; Bissantz N, 2007, SIAM J NUMER ANAL, V45, P2610, DOI 10.1137/060651884; Blake C.L., 1998, UCI REPOSITORY MACHI; BLANCHARD G, 2003, MACHINE LEARNING RES, V4, P861; Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.1023/A:1018054314350; Buhlmann P, 2006, ANN STAT, V34, P559, DOI 10.1214/009053606000000092; Buhlmann P, 2006, J MACH LEARN RES, V7, P1001; Buhlmann P, 2006, FRONTIERS IN STATISTICS: DEDICATED TO PETER JOHN BICKEL IN HONOR OF HIS 65TH BIRTHDAY, P209, DOI 10.1142/9781860948886_0010; BUHLMANN P, 2007, TWIN BOOSTING IMPROV; Buhlmann P, 2000, ANN STAT, V28, P377; BUJA A., 2005, LOSS FUNCTIONS BINAR; Dettling M, 2004, BIOINFORMATICS, V20, P3583, DOI 10.1093/bioinformatics/bth447; Dettling M, 2003, BIOINFORMATICS, V19, P1061, DOI 10.1093/bioinformatics/btf867; DIMARZIO M, 2008, IN PRESS J STAT PLAN; Freund Y., 1996, P 13 INT C MACH LEAR; Freund Y, 1995, P 2 EUR C COMP LEARN; Garcia AL, 2005, OBES RES, V13, P626, DOI 10.1038/oby.2005.67; Gentleman RC, 2004, GENOME BIOL, V5, DOI 10.1186/gb-2004-5-10-r80; Green P, 1994, NONPARAMETRIC REGRES; Greenshtein E, 2004, BERNOULLI, V10, P971, DOI 10.3150/bj/1106314846; Hastie T., 2001, ELEMENTS STAT LEARNI; Hastie T, 1986, STAT SCI, V1, P297, DOI DOI 10.1214/SS/1177013604; HASTIE T, 2004, LARS LEAST ANGLE REG; Hastie TJ, 1990, GEN ADDITIVE MODELS; Hothorn T, 2006, J COMPUT GRAPH STAT, V15, P651, DOI 10.1198/106186006X133933; HOTHORN T, 2007, MBOOST MODEL BASED B; Hothorn T, 2006, BIOINFORMATICS, V22, P2828, DOI 10.1093/bioinformatics/btl462; Hothorn T, 2006, BIOSTATISTICS, V7, P355, DOI 10.1093/biostatistics/kxj011; Hothorn T, 2006, PARTY LAB RECURSIVE; HUANG J, 2008, IN PRESS STAT SINICA; Hurvich CM, 1998, J ROY STAT SOC B, V60, P271, DOI 10.1111/1467-9868.00125; IYER R, 2000, P CIKM 00 9 ACM INT; Jiang W., 2004, ANN STAT, V32, P85; KEARNS M, 1994, J ACM, V41, P67, DOI 10.1145/174644.174647; LEITENSTORFER F, 2007, COMPUT STAT DATA AN, V54, P4605; LEITENSTORFER F, 2006, P COMP STAT COMPSTAT; Leitenstorfer F, 2007, BIOSTATISTICS, V8, P654, DOI 10.1093/biostatistics/kxl036; Lozano A. C., 2006, ADV NEURAL INFORM PR, V18; Lugosi G, 2004, ANN STAT, V32, P30; LUGOSI G, 2004, ANN STAT, V32, P85; Lutz RW, 2006, STAT SINICA, V16, P471; Mannor S., 2003, J MACHINE LEARNING R, V4, P713; Mason L, 2000, ADV NEUR IN, P221; Meir R, 2002, LECT NOTES ARTIF INT, V2600, P118; R Development Core Team, 2006, R LANG ENV STAT COMP; Ridgeway G, 2006, GBM GEN BOOSTED REGR; Ridgeway G, 1999, COMPUTING SCI STAT, V31, P172; Ridgeway G, 2000, ANN STAT, V28, P393; Ridgeway G, 2002, COMPUT STAT DATA AN, V38, P379, DOI 10.1016/S0167-9473(01)00066-4; Schapire R., 2002, LECT NOTES STAT, V171, P149; SCHAPIRE RE, 1990, MACH LEARN, V5, P197, DOI 10.1023/A:1022648800760; Southwell R.V., 1946, RELAXATION METHODS T; STREET WN, 1995, P 12 INT C MACH LEAR; Temlyakov VN, 2000, ADV COMPUT MATH, V12, P213, DOI 10.1023/A:1018917218956; Tukey J. W., 1977, EXPLORATORY DATA ANA; Tutz G, 2007, STAT MED, V26, P2872, DOI 10.1002/sim.2738; Tutz G, 2007, J COMPUT GRAPH STAT, V16, P165, DOI 10.1198/106186007X180949; Tutz G, 2005, J STAT COMPUT SIM, V75, P391, DOI 10.1080/00949650410001729481; Tutz G, 2006, BIOMETRICS, V62, P961, DOI 10.1111/j.1541-0420.2006.00578.x; Tutz G, 2007, COMPUT STAT DATA AN, V51, P6044, DOI 10.1016/j.csda.2006.11.041; VAN DER LAAN M. J., 2003, UNIFIED METHODS CENS; Yao Y, 2007, CONSTR APPROX, V26, P289, DOI 10.1007/s00365-006-0663-2; Zhao P, 2007, J MACH LEARN RES, V8, P2701; Zhu J., 2005, MULTICLASS ADABOOST	98	172	178	4	29	INST MATHEMATICAL STATISTICS	CLEVELAND	3163 SOMERSET DR, CLEVELAND, OH 44122 USA	0883-4237			STAT SCI	Stat. Sci.	NOV	2007	22	4					477	505		10.1214/07-STS242		29	Statistics & Probability	Mathematics	295CD	WOS:000255451700001		
J	Yuan, M; Joseph, VR; Lin, Y				Yuan, Ming; Joseph, V. Roshan; Lin, Yi			An efficient variable selection approach for analyzing designed experiments	TECHNOMETRICS			English	Article						effect heredity; least angle regression; variable selection	LEAST ANGLE REGRESSION; MODEL SELECTION; LINEAR-MODELS; SUPERSATURATED DESIGNS; LASSO	The analysis of experiments in which numerous potential variables are examined is driven by the principles of effect sparsity, effect hierarchy, and effect heredity. We propose an efficient variable selection strategy to specifically address the unique challenges faced by such analysis. The proposed methods are natural extensions of the LARS general-purpose variable selection algorithm. They can be computed very rapidly and can find sparse models that better satisfy the goals of experiments. Simulations and real examples are used to illustrate the wide applicability of the proposed methods.	Georgia Inst Technol, Sch Ind & Syst Engn, Atlanta, GA 30332 USA; Univ Wisconsin, Dept Stat, Madison, WI 53706 USA	Yuan, M (reprint author), Georgia Inst Technol, Sch Ind & Syst Engn, Atlanta, GA 30332 USA.						BREIMAN L, 1995, TECHNOMETRICS, V37, P373, DOI 10.2307/1269730; Shen XT, 2002, J AM STAT ASSOC, V97, P210, DOI 10.1198/016214502753479356; Yuan M, 2006, J ROY STAT SOC B, V68, P49, DOI 10.1111/j.1467-9868.2005.00532.x; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; GEORGE EI, 1993, J AM STAT ASSOC, V88, P881, DOI 10.2307/2290777; Efron B, 2004, ANN STAT, V32, P407; Breiman L, 1996, ANN STAT, V24, P2350; Breiman L., 1984, CLASSIFICATION REGRE; Chipman H, 1996, CAN J STAT, V24, P17, DOI 10.2307/3315687; Chipman H, 1997, TECHNOMETRICS, V39, P372, DOI 10.2307/1271501; Cormen T. H., 1990, INTRO ALGORITHMS; FOSTER DP, 1994, ANN STAT, V22, P1947, DOI 10.1214/aos/1176325766; George EI, 2000, BIOMETRIKA, V87, P731, DOI 10.1093/biomet/87.4.731; HAMADA M, 1992, J QUAL TECHNOL, V24, P130; Jeff Wu CF, 2000, EXPT PLANNING ANAL P; Joseph VR, 2006, TECHNOMETRICS, V48, P219, DOI 10.1198/004017005000000652; Li RZ, 2002, STAT PROBABIL LETT, V59, P135, DOI 10.1016/S0167-7152(02)00140-2; McCullagh P., 1989, GEN LINEAR MODELS; Meyer RD, 1996, TECHNOMETRICS, V38, P303, DOI 10.2307/1271297; NELDER JA, 1977, J ROY STAT SOC A STA, V140, P48, DOI 10.2307/2344517; NELDER JA, 1994, STAT COMPUT, V4, P221, DOI 10.1007/BF00156745; Nelder JA, 1998, AM STAT, V52, P315, DOI 10.2307/2685433; Osborne MR, 2000, J COMPUT GRAPH STAT, V9, P319, DOI 10.2307/1390657; Raghavarao D, 2003, METRIKA, V58, P185, DOI 10.1007/s001840200236; Turlach BA, 2004, ANN STAT, V32, P481; Westfall PH, 1998, STAT SINICA, V8, P101; Yuan M, 2005, J AM STAT ASSOC, V100, P1215, DOI 10.1198/016214505000000367	28	23	23	1	5	AMER STATISTICAL ASSOC	ALEXANDRIA	1429 DUKE ST, ALEXANDRIA, VA 22314 USA	0040-1706			TECHNOMETRICS	Technometrics	NOV	2007	49	4					430	439		10.1198/004017007000000173		10	Statistics & Probability	Mathematics	230NM	WOS:000250883200007		
J	Steinke, F; Seeger, M; Tsuda, K				Steinke, Florian; Seeger, Matthias; Tsuda, Koji			Experimental design for efficient identification of gene regulatory networks using sparse Bayesian models	BMC SYSTEMS BIOLOGY			English	Article							EXPRESSION DATA; REGRESSION; PERTURBATIONS	Background: Identifying large gene regulatory networks is an important task, while the acquisition of data through perturbation experiments (e. g., gene switches, RNAi, heterozygotes) is expensive. It is thus desirable to use an identification method that effectively incorporates available prior knowledge-such as sparse connectivity-and that allows to design experiments such that maximal information is gained from each one. Results: Our main contributions are twofold: a method for consistent inference of network structure is provided, incorporating prior knowledge about sparse connectivity. The algorithm is time efficient and robust to violations of model assumptions. Moreover, we show how to use it for optimal experimental design, reducing the number of required experiments substantially. We employ sparse linear models, and show how to perform full Bayesian inference for these. We not only estimate a single maximum likelihood network, but compute a posterior distribution over networks, using a novel variant of the expectation propagation method. The representation of uncertainty enables us to do effective experimental design in a standard statistical setting: experiments are selected such that the experiments are maximally informative. Conclusion: Few methods have addressed the design issue so far. Compared to the most well-known one, our method is more transparent, and is shown to perform qualitatively superior. In the former, hard and unrealistic constraints have to be placed on the network structure for mere computational tractability, while such are not required in our method. We demonstrate reconstruction and optimal experimental design capabilities on tasks generated from realistic non-linear network simulators.	[Steinke, Florian; Seeger, Matthias; Tsuda, Koji] Max Planck Inst Biol Cybernet, D-72076 Tubingen, Germany	Seeger, M (reprint author), Max Planck Inst Biol Cybernet, Spemannstr 38, D-72076 Tubingen, Germany.	steinke@tuebingen.mpg.de; seeger@tuebingen.mpg.de; tsuda@tuebingen.mpg.de					Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Watts DJ, 1998, NATURE, V393, P440, DOI 10.1038/30918; Chaloner K, 1995, STAT SCI, V10, P273, DOI 10.1214/ss/1177009939; Yeung MKS, 2002, P NATL ACAD SCI USA, V99, P6163, DOI 10.1073/pnas.092576199; Fire A, 1998, NATURE, V391, P806, DOI 10.1038/35888; Friedman N, 2000, J COMPUT BIOL, V7, P601, DOI 10.1089/106652700750050961; Rogers S, 2005, BIOINFORMATICS, V21, P3131, DOI 10.1093/bioinformatics/bti487; Tipping ME, 2001, J MACH LEARN RES, V1, P211, DOI 10.1162/15324430152748236; Daw ND, 2006, NATURE, V441, P876, DOI 10.1038/nature04766; Shmulevich I, 2002, BIOINFORMATICS, V18, P261, DOI 10.1093/bioinformatics/18.2.261; Gardner TS, 2000, NATURE, V403, P339; COKUS SJ, 2006, BMC BIOINFORMATICS, V7; Hartemink AJ, 2002, IEEE INTELL SYST, V17, P37, DOI 10.1109/MIS.2002.999218; Ideker T E, 2000, Pac Symp Biocomput, P305; Kholodenko BN, 2002, P NATL ACAD SCI USA, V99, P12841, DOI 10.1073/pnas.192442699; Ljung L, 1999, SYSTEM IDENTIFICATIO; Minka T., 2001, UNCERTAINTY ARTIFICI, V17; O'Hagan A., 1994, BAYESIAN INFERENCE B, V2B; OPPER M, 2000, NEURAL COMPUT, V12, P26555; PARK T, 2005, TECH REP; PEETERS R, 2004, P 16 INT S MATH THEO; Schmidt H, 2005, FEBS J, V272, P2141, DOI 10.1111/j.1742-4658.2005.04605.x; SEEGER M, 2007, WORKSH ART INT STAT, V11; SEEGER M, 2006, TECH REP; SEEGER M, 2005, TECH REP; Sontag E, 2004, BIOINFORMATICS, V20, P1877, DOI 10.1093/bioinformatics/bth173; Tegner J, 2003, P NATL ACAD SCI USA, V100, P5944, DOI 10.1073/pnas.0933416100; Vert JP, 2006, BMC BIOINFORMATICS, V7, DOI 10.1186/1471-2105-7-520; von Dassow G, 2000, NATURE, V406, P188, DOI 10.1038/35018085; Yoo Changwon, 2003, AMIA Annu Symp Proc, P733	30	12	12	0	1	BIOMED CENTRAL LTD	LONDON	236 GRAYS INN RD, FLOOR 6, LONDON WC1X 8HL, ENGLAND	1752-0509			BMC SYST BIOL	BMC Syst. Biol.	NOV 16	2007	1								51	10.1186/1752-0509-1-51		15	Mathematical & Computational Biology	Mathematical & Computational Biology	261AH	WOS:000253051000001	18021391	
J	Vandenhende, F; Eilers, P; Ledent, E; Renard, D; Tibaldi, E				Vandenhende, F.; Eilers, P.; Ledent, E.; Renard, D.; Tibaldi, E.			Joint detection of important biomarkers and optimal dose-response model using penalties	STATISTICS IN MEDICINE			English	Article; Proceedings Paper	5th International Meeting on Statistical Methods in Biopharmacy	SEP 26-27, 2006	Paris, FRANCE	French Soc Stat		biomarkers; penalty; dose-response	REGRESSION	We propose a method to jointly detect influential biomarkers and estimate how they change with dose. The assessment is made in dose-ranging trials collecting multiple outcomes for efficacy, safety, pharmacokinetics or pharmacodynamics. We regress all these outcomes versus a non-parametric transformation of the dose. The regression coefficients and the parameters from the dose-response model are simultaneously estimated using a penalized alternating least-squares method. We illustrate the technique with a phase I clinical trial and a metabonomic experiment in rats. (C) 2007 John Wiley & Sons, Ltd.	Eli Lilly & Co, Global Stat, Mont St Guibert, Belgium; Leiden Univ, Dept Med Stat, NL-2300 RA Leiden, Netherlands	Vandenhende, F (reprint author), Clinbay, Rue Banterley 80, B-1471 Genappe, Belgium.	francois@clinbay.com					Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Lindon JC, 2003, TOXICOL APPL PHARM, V187, P137, DOI 10.1016/S0041-008X(02)00079-0; HOERL AE, 1970, TECHNOMETRICS, V12, P55; Eilers PHC, 1996, STAT SCI, V11, P89, DOI 10.1214/ss/1038425655; Atkinson AJ, 2001, CLIN PHARMACOL THER, V69, P89, DOI 10.1067/mcp.2000.113989; Bloom J. C., 2003, BIOMARKERS CLIN DRUG, P526; BOLLAERTS K, 2005, 0535 IAP STAT NETW T; DAVISON AC, 1999, BOOTSTRAP METHODS AP; DELEEUW J, 1976, PSYCHOMETRIKA, V41, P471; Hastie T., 2000, ELEMENTS STAT LEARNI; Ruppert D., 2003, SEMIPARAMETRIC REGRE; Verbyla A. P., 1999, APPL STAT, V48, P269, DOI DOI 10.1111/1467-9876.00154; WAHABA G, 1990, SPLINE MODELS OBSERV	13	3	3	1	2	JOHN WILEY & SONS LTD	CHICHESTER	THE ATRIUM, SOUTHERN GATE, CHICHESTER PO19 8SQ, W SUSSEX, ENGLAND	0277-6715			STAT MED	Stat. Med.	NOV 30	2007	26	27					4876	4888		10.1002/sim.2960		13	Mathematical & Computational Biology; Public, Environmental & Occupational Health; Medical Informatics; Medicine, Research & Experimental; Statistics & Probability	Mathematical & Computational Biology; Public, Environmental & Occupational Health; Medical Informatics; Research & Experimental Medicine; Mathematics	228IU	WOS:000250724400004	17579922	
J	Friedman, J; Hastie, T; Hofling, H; Tibshirani, R				Friedman, Jerome; Hastie, Trevor; Hoefling, Holger; Tibshirani, Robert			PATHWISE COORDINATE OPTIMIZATION	ANNALS OF APPLIED STATISTICS			English	Article						Coordinate descent; lasso; convex optimization	REGRESSION; SELECTION; LASSO; SMOOTHNESS; SHRINKAGE; SPARSITY; INVERSE	We consider "one-at-a-time" coordinate-wise descent algorithms for a class of convex optimization problems. An algorithm of this kind has been proposed for the L-1-penalized regression (lasso) in the literature, but it seems to have been largely ignored. Indeed. it seems (hat coordinate-wise algorithms are not often Used in convex optimization. We show that this algorithm is very competitive with the well-known LARS (or homotopy) procedure in large lasso problems, and that it call be applied to related methods such as the garotte and elastic net. It turns out that coordinate-wise descent does not work in the "Fused lasso." however. so we derive a generalized algorithm that yields the solution in much less time that a standard convex optimizer. Finally. we generalize the procedure to the two-dimensional fused lasso, and demonstrate its performance oil some image smoothing problems.	[Friedman, Jerome; Hastie, Trevor; Hoefling, Holger; Tibshirani, Robert] Stanford Univ, Dept Stat, Stanford, CA 94305 USA; [Hastie, Trevor; Tibshirani, Robert] Stanford Univ, Dept Hlth Res & Policy, Stanford, CA 94305 USA	Friedman, J (reprint author), Stanford Univ, Dept Stat, Stanford, CA 94305 USA.	jhf@stanford.edu; hastie@stanford.edu; hhoeflin@gmail.com; tibs@stanford.edu			NSF [DMS-97-64411, DMS-05-50670, DMS-99-71405]; NIH [2R01 CA 72028-07, N01-HV-28183]; Albion Walter Hewlett Stanford Graduate Fellowship	Supported in part by NSF Grant DMS-97-644112Supported in part by NSF Grant DMS-05-50670 and NIH Grant 2R01 CA 72028-07.3Supported by all Albion Walter Hewlett Stanford Graduate Fellowship.4Supported in part by NSF Grant DMS-99-71405 and NIH Contract N01-HV-28183.	Fu WJJ, 1998, J COMPUT GRAPH STAT, V7, P397, DOI 10.2307/1390712; Yuan M, 2006, J ROY STAT SOC B, V68, P49, DOI 10.1111/j.1467-9868.2005.00532.x; RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Osborne MR, 2000, IMA J NUMER ANAL, V20, P389, DOI 10.1093/imanum/20.3.389; Tibshirani R, 2005, J ROY STAT SOC B, V67, P91, DOI 10.1111/j.1467-9868.2005.00490.x; Efron B, 2004, ANN STAT, V32, P407; Daubechies I, 2004, COMMUN PUR APPL MATH, V57, P1413, DOI 10.1002/cpa.20042; Bertsekas D., 1999, NONLINEAR PROGRAMMIN; Breiman L., 1995, TECHNOMETRICS, V37, P738; Chen S. S., 1998, SIAM J SCI COMPUT, P33; Donoho DL, 1995, J AM STAT ASSOC, V90, P1200, DOI 10.2307/2291512; Friedlander MP, 2007, ANN STAT, V35, P2385, DOI 10.1214/00905360700000479; GILL P, 1999, USERS GUIDE SQOPT 5; LI Y, 2004, URASIP J APPL SIGNAL, P1762; Owen A. B., 2006, ROBUST HYBRID LASSO; SCHLEGEL P, 1970, MATH COMPUT, V24, P665, DOI 10.2307/2004842; TIBSHIRANI R, 2007, BIOSTATISTICS; Tseng P., 2001, J OPT THEORY APPL, V109, P474; Tseng P., 1988, 1840 LIDSP MIT; Van Der Kooij A., 2007, PREDICTION ACCURACY; WANG H, 2006, J BUSINESS EC STAT, V11, P1; Zhou H, 2005, J ROYAL STAT SOC B, V67, P301	23	436	447	10	29	INST MATHEMATICAL STATISTICS	CLEVELAND	3163 SOMERSET DR, CLEVELAND, OH 44122 USA	1932-6157			ANN APPL STAT	Ann. Appl. Stat.	DEC	2007	1	2					302	332		10.1214/07-AOAS131		31	Statistics & Probability	Mathematics	374QA	WOS:000261057600003		
J	Jensen, ST; Chen, G; Stoeckert, CJ				Jensen, Shane T.; Chen, Guang; Stoeckert, Christian J., Jr.			BAYESIAN VARIABLE SELECTION AND DATA INTEGRATION FOR BIOLOGICAL REGULATORY NETWORKS	ANNALS OF APPLIED STATISTICS			English	Article						Regulatory networks; Bayesian variable selection; data integration; transcription factors	GENE-EXPRESSION DATA; REGRESSION-MODELS; PRIOR ELICITATION; CELL-CYCLE; MODULES; RECONSTRUCTION; DISTRIBUTIONS; DISCOVERY; PROFILES; YEAST	A substantial focus of research in molecular biology are gene regulatory networks: the set of transcription factors and target genes which control C the involvement of different biological processes in living cells. Previous statistical approaches for identifying gene regulatory networks have Used expression data. Chip binding data or promoter sequence data. but each of these resources provides only partial information. We present a Bayesian hierarchical model that integrates all three data types in a principled variable selection framework. The gene expression data are modeled as a function of the Unknown gene regulatory network which has all informed prior distribution based Upon both Chip binding and promoter sequence data. We also present a variable weighing, methodology for the principled balancing of multiple S our procedure 10 the discovery of sources of prior information. We apply gene regulatory relationships in Saccharomyces cerevisiae (Yeast) for which we call use several external sources of information to validate our results. Our inferred relationships show greater biological relevance on the external validation measures than previous data integration methods. Our model also estimates synergistic and antagonistic interactions between transcription factors many of which are validated by previous studies. We also evaluate the results from our procedures for the weighting for multiple sources of prior information. Finally, we discuss our methodology in the context of previous approaches to data integration and Bayesian variable selection.	[Jensen, Shane T.] Univ Penn, Wharton Sch, Dept Stat, Philadelphia, PA 19104 USA; [Chen, Guang] Univ Penn, Dept Bioengn, Ctr Bioinformat, Philadelphia, PA 19104 USA; [Stoeckert, Christian J., Jr.] Univ Penn, Dept Genet, Ctr Bioinformat, Philadelphia, PA 19104 USA	Jensen, ST (reprint author), Univ Penn, Wharton Sch, Dept Stat, Philadelphia, PA 19104 USA.	stjensen@wharton.upenn.edu; ggchen@pcbi.upenn.edu; stoeckrt@pcbi.upenn.edu			University of Pennsylvania Research Foundation; NIH [U01-DK50947]	2 Supported in part by grant from the University of Pennsylvania Research Foundation. Supported in part by NIH Grant U01-DK50947.	Banerjee N, 2003, NUCLEIC ACIDS RES, V31, P7024, DOI 10.1093/nar/gkg894; Bar-Joseph Z, 2003, NAT BIOTECHNOL, V21, P1337, DOI 10.1038/nbt890; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; George EI, 2000, J AM STAT ASSOC, V95, P1304, DOI 10.2307/2669776; Segal E, 2003, NAT GENET, V34, P166, DOI 10.1038/ng1165; Hughes TR, 2000, CELL, V102, P109, DOI 10.1016/S0092-8674(00)00015-5; Matys V, 2003, NUCLEIC ACIDS RES, V31, P374, DOI 10.1093/nar/gkg108; Efron B, 2004, ANN STAT, V32, P407; Berry DA, 1999, J STAT PLAN INFER, V82, P215, DOI 10.1016/S0378-3758(99)00044-0; Boulesteix AL, 2005, THEOR BIOL MED MODEL, V2, DOI 10.1186/1742-4682-2-23; Bussemaker HJ, 2001, NAT GENET, V27, P167, DOI 10.1038/84792; Chen G, 2007, GENOME BIOL, V8, DOI 10.1186/gb-2007-8-r4; Chen MH, 1999, J ROY STAT SOC B, V61, P223, DOI 10.1111/1467-9868.00173; Chen MH, 2003, J STAT PLAN INFER, V111, P57, DOI 10.1016/S0378-3758(02)00285-9; GAO L, 2004, BMC GENOMICS, V5, P1; Garthwaite PH, 1996, CHEMOMETR INTELL LAB, V35, P1, DOI 10.1016/S0169-7439(96)00035-4; GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721; George E.I., 1996, MARKOV CHAIN MONTE C, P203; Ibrahim JG, 2000, STAT SCI, V15, P46; Kloster M, 2005, BIOINFORMATICS, V21, P1172, DOI 10.1093/bioinformatics/bti096; Lee T. I., 2002, SCIENCE, V298, P763; Lemmens K, 2006, GENOME BIOL, V7, DOI 10.1186/gb-2006-7-5-r37; Liao JC, 2003, P NATL ACAD SCI USA, V100, P15522, DOI 10.1073/pnas.2136632100; Mewes HW, 2002, NUCLEIC ACIDS RES, V30, P31, DOI 10.1093/nar/30.1.31; Sabatti C, 2005, BIOINFORMATICS, V21, P922, DOI 10.1093/bioinformatics/bti083; Segal Eran, 2001, BIOINFORMATICS, V1, P1; *SGD PROJ, 2005, SACCH GEN DAT; Tadesse MG, 2004, BIOINFORMATICS, V20, P2553, DOI 10.1093/bioinformatics/bth282; Xing B, 2005, J COMPUT BIOL, V12, P229, DOI 10.1089/cmb.2005.12.229; Yang YL, 2005, BMC GENOMICS, V6, DOI 10.1186/1471-2164-6-90	30	10	10	1	3	INST MATHEMATICAL STATISTICS	CLEVELAND	3163 SOMERSET DR, CLEVELAND, OH 44122 USA	1932-6157			ANN APPL STAT	Ann. Appl. Stat.	DEC	2007	1	2					612	633		10.1214/07-AOAS130		22	Statistics & Probability	Mathematics	374QA	WOS:000261057600016		
J	Candes, E; Tao, T				Candes, Emmanuel; Tao, Terence			The Dantzig selector: Statistical estimation when p is much larger than n	ANNALS OF STATISTICS			English	Article						statistical linear model; model selection; ideal estimation; oracle inequalities; sparse solutions to underdetermined systems; l(1)-minimization; linear programming; restricted orthonormality; geometry in high dimensions; random matrices	UNCERTAINTY PRINCIPLES; SIGNAL RECOVERY; MODEL SELECTION; REGRESSION; BASES; RECONSTRUCTION; SHRINKAGE; SYSTEMS	In many important statistical applications, the number of variables or parameters p is much larger than the number of observations n. Suppose then that we have observations y = X beta + z, where beta epsilon R-p is a parameter vector of interest, X is a data matrix with possibly far fewer rows than columns, n << p, and the z(i)'s are i.i.d. N(0, sigma(2)). Is it possible to estimate beta reliably based on the noisy data y? To estimate beta, we introduce a new estimator-we call it the Dantzig selector-which is a solution to the l(1)-regularization problem (min) ((beta) over tilde epsilon Rp) parallel to(beta) over tilde parallel to l(1) subject to parallel to X*r parallel to l(infinity) <= (1 + t(-1)) root 2 log p.sigma, where r is the residual vector y - X (beta) over tilde and t is a positive scalar. We show that if X obeys a uniform uncertainty principle (with unit-normed columns) and if the true parameter vector beta is sufficiently sparse (which here roughly guarantees that the model is identifiable), then with very large probability, parallel to(beta) over cap-beta parallel to(2)(l2) <= C-2 . 2 log p . (sigma(2) + Sigma(i) min(beta(2)(i), sigma(2))). Our results are nonasymptotic and we give values for the constant C. Even though n may be much smaller than p, our estimator achieves a loss within a logarithmic factor of the ideal mean squared error one would achieve with an oracle which would supply perfect information about which coordinates are nonzero, and which were above the noise level. In multivariate regression and from a model selection viewpoint, our result says that it is possible nearly to select the best subset of variables by solving a very simple convex program, which, in fact, can easily be recast as a convenient linear program (LP).	[Candes, Emmanuel] CALTECH, Pasadena, CA 91125 USA; [Tao, Terence] Univ Calif Los Angeles, Dept Math, Los Angeles, CA 90095 USA	Candes, E (reprint author), CALTECH, MC 217-50, Pasadena, CA 91125 USA.	emmanuel@acm.caltech.edu; tao@math.ucla.edu	Tao, Terence/M-1837-2015				AKAIKE H, 1974, IEEE T AUTOMAT CONTR, VAC19, P716, DOI 10.1109/TAC.1974.1100705; Donoho DL, 2001, IEEE T INFORM THEORY, V47, P2845, DOI 10.1109/18.959265; Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582; RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Antoniadis A, 2001, J AM STAT ASSOC, V96, P939, DOI 10.1198/016214501753208942; Haupt J, 2006, IEEE T INFORM THEORY, V52, P4036, DOI 10.1109/TIT.2006.880031; Candes EJ, 2006, IEEE T INFORM THEORY, V52, P489, DOI 10.1109/TIT.2005.862083; Candes EJ, 2006, IEEE T INFORM THEORY, V52, P5406, DOI 10.1109/TIT.2006.885507; DONOHO DL, 1994, CR ACAD SCI I-MATH, V319, P1317; Candes EJ, 2005, IEEE T INFORM THEORY, V51, P4203, DOI 10.1109/TIT.2005.858979; Donoho DL, 2006, COMMUN PUR APPL MATH, V59, P797, DOI 10.1002/cpa.20132; DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425; SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136; Candes EJ, 2006, COMMUN PUR APPL MATH, V59, P1207, DOI 10.1002/cpa.20124; MALLOWS CL, 1973, TECHNOMETRICS, V15, P661, DOI 10.2307/1267380; Elad M, 2002, IEEE T INFORM THEORY, V48, P2558, DOI 10.1109/TIT.2002.801410; Baraud Y, 2000, PROBAB THEORY REL, V117, P467, DOI 10.1007/PL00008731; Barron A, 1999, PROBAB THEORY REL, V113, P301, DOI 10.1007/s004400050210; BARRON AR, 1991, IEEE T INFORM THEORY, V37, P1034, DOI 10.1109/18.86996; Birge L., 1997, FESTSCHRIFT LUCIEN C, P55, DOI 10.1007/978-1-4612-1880-7_4; Birge L., 2001, J EUR MATH SOC, V3, P203, DOI 10.1007/s100970100031; CANDES E, 2005, P 46 ANN IEEE S FDN, P295; CANDES EJ, 2005, COMPUTATIONAL IMAG 3, V1, P76; Daniel BL, 1998, RADIOLOGY, V209, P499; DAUBECHIES I, 2005, COMMUNICATION; DONOHO DL, 1995, UNPUB EMPIRICAL ATOM; Fan JQ, 2004, ANN STAT, V32, P928, DOI 10.1214/009053604000000256; FOSTER DP, 1994, ANN STAT, V22, P1947, DOI 10.1214/aos/1176325766; Fuchs JJ, 2004, IEEE T INFORM THEORY, V50, P1341, DOI 10.1109/TIT.2004.828141; Greenshtein E, 2004, BERNOULLI, V10, P971, DOI 10.3150/bj/1106314846; KETTENRING J, 2003, STAT CHALLENGES OPPO; NATARAJAN BK, 1995, SIAM J COMPUT, V24, P227, DOI 10.1137/S0097539792240406; Peters DC, 2000, MAGNET RESON MED, V43, P91, DOI 10.1002/(SICI)1522-2594(200001)43:1<91::AID-MRM11>3.0.CO;2-4; Sardy S, 2000, J COMPUT GRAPH STAT, V9, P361, DOI 10.2307/1390659; Szarek S. J., 1991, Journal of Complexity, V7, DOI 10.1016/0885-064X(91)90002-F; Vandenberghe L., 2004, CONVEX OPTIMIZATION	38	732	767	11	47	INST MATHEMATICAL STATISTICS	BEACHWOOD	PO BOX 22718, BEACHWOOD, OH 44122 USA	0090-5364			ANN STAT	Ann. Stat.	DEC	2007	35	6					2313	2351		10.1214/009053606000001523		39	Statistics & Probability	Mathematics	261KP	WOS:000253077800001		
J	Efron, B; Hastie, T; Tibshiran, R				Efron, Bradley; Hastie, Trevor; Tibshiran, Robert			Discussion: The Dantzig selector: Statistical estimation when p is much larger than n	ANNALS OF STATISTICS			English	Editorial Material							REGRESSION		[Efron, Bradley; Hastie, Trevor; Tibshiran, Robert] Stanford Univ, Dept Stat & Hlth Res & Policy, Stanford, CA 94305 USA	Efron, B (reprint author), Stanford Univ, Dept Stat & Hlth Res & Policy, Stanford, CA 94305 USA.	efron@stanford.edu; hastie@stanford.edu; tibs@stanford.edu					Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Osborne MR, 2000, IMA J NUMER ANAL, V20, P389, DOI 10.1093/imanum/20.3.389; Efron B, 2004, ANN STAT, V32, P407; DONOHO D, 2006, FAST SOLUTION LAMBDA; Rosset S, 2007, ANN STAT, V35, P1012, DOI 10.1214/009053606000001370	6	22	24	2	7	INST MATHEMATICAL STATISTICS	BEACHWOOD	PO BOX 22718, BEACHWOOD, OH 44122 USA	0090-5364			ANN STAT	Ann. Stat.	DEC	2007	35	6					2358	2364		10.1214/009053607000000433		7	Statistics & Probability	Mathematics	261KP	WOS:000253077800003		
J	Cai, TT; Lv, J				Cai, T. Tony; Lv, Jinchi			Discussion: The Dantzig selector: Statistical estimation when p is much larger than n	ANNALS OF STATISTICS			English	Editorial Material							NONCONCAVE PENALIZED LIKELIHOOD; VARIABLE SELECTION; ORACLE PROPERTIES; REGRESSION; SHRINKAGE; LASSO		[Cai, T. Tony] Univ Penn, Wharton Sch, Dept Stat, Philadelphia, PA 19104 USA; [Lv, Jinchi] Princeton Univ, Dept Math, Princeton, NJ 08544 USA	Cai, TT (reprint author), Univ Penn, Wharton Sch, Dept Stat, Philadelphia, PA 19104 USA.	tcai@wharton.upenn.edu; jlv@princeton.edu	Lv, Jinchi/D-2295-2012				BREIMAN L, 1995, TECHNOMETRICS, V37, P373, DOI 10.2307/1269730; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; Candes EJ, 2006, IEEE T INFORM THEORY, V52, P5406, DOI 10.1109/TIT.2006.885507; Candes EJ, 2005, IEEE T INFORM THEORY, V51, P4203, DOI 10.1109/TIT.2005.858979; Zou H, 2006, J AM STAT ASSOC, V101, P1418, DOI 10.1198/016214506000000735; Efron B, 2004, ANN STAT, V32, P407; DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425; Donoho DL, 2003, P NATL ACAD SCI USA, V100, P2197, DOI 10.1073/pnas.0437847100; FAN J, 2006, UNPUB SURE INDEPENDE; Fan JQ, 2004, ANN STAT, V32, P928, DOI 10.1214/009053604000000256; Hunter DR, 2005, ANN STAT, V33, P1617, DOI 10.1214/009053605000000200	12	16	16	1	3	INST MATHEMATICAL STATISTICS	BEACHWOOD	PO BOX 22718, BEACHWOOD, OH 44122 USA	0090-5364			ANN STAT	Ann. Stat.	DEC	2007	35	6					2365	2369		10.1214/009053607000000442		5	Statistics & Probability	Mathematics	261KP	WOS:000253077800004		
J	Meinshausen, N; Rocha, G; Yu, B				Meinshausen, N.; Rocha, G.; Yu, B.			Discussion: A tale of three cousins: Lasso, L2Boosting and Dantzig	ANNALS OF STATISTICS			English	Editorial Material							MODEL SELECTION; REGRESSION; NOISE		[Meinshausen, N.] Univ Oxford, Dept Stat, Oxford OX1 3TG, England; [Rocha, G.; Yu, B.] Univ Calif Berkeley, Dept Stat, Berkeley, CA 94720 USA	Meinshausen, N (reprint author), Univ Oxford, Dept Stat, 1 S Pk Rd, Oxford OX1 3TG, England.	meinshausen@stats.ox.ac.uk; gvrocha@stat.berkeley.edu; binyu@stat.berkeley.edu					Tropp JA, 2006, IEEE T INFORM THEORY, V52, P1030, DOI 10.1109/TIT.2005.864420; Knight K, 2000, ANN STAT, V28, P1356; Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Zhao P, 2006, J MACH LEARN RES, V7, P2541; Zou H, 2006, J AM STAT ASSOC, V101, P1418, DOI 10.1198/016214506000000735; Efron B, 2004, ANN STAT, V32, P407; Meinshausen N, 2006, ANN STAT, V34, P1436, DOI 10.1214/009053606000000281; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P6, DOI 10.1109/TIT.2005.860430; Bertsekas DP, 1995, NONLINEAR PROGRAMMIN; Buhlmann P, 2006, ANN STAT, V34, P559, DOI 10.1214/009053606000000092; BUNEA F, 2006, SPARASITY ORACLE INE; Candes E. J., 2007, L1 MAGIC; Donoho DL, 2006, COMMUN PUR APPL MATH, V59, P907, DOI 10.1002/cpa.20131; Greenshtein E, 2004, BERNOULLI, V10, P971, DOI 10.3150/bj/1106314846; Leng CL, 2006, STAT SINICA, V16, P1273; LI Y, 2006, L1 NORM QUANTILE REG; MEINSHAUSEN N, 2007, IN PRESS ANN STAT; Osborne MR, 2000, J COMPUT GRAPH STAT, V9, P319, DOI 10.2307/1390657; Rosset S, 2007, ANN STAT, V35, P1012, DOI 10.1214/009053606000001370; VANDEGEER S, 2006, 133 ETH ZUR; Wainwright M., 2006, SHARP THRESHOLDS HIG; WAINWRIGHT MJ, 2007, 725 U CAL DEP STAT; ZHANG CH, 2006, 003 RUTG U DEP STAT	24	23	24	0	1	INST MATHEMATICAL STATISTICS	BEACHWOOD	PO BOX 22718, BEACHWOOD, OH 44122 USA	0090-5364			ANN STAT	Ann. Stat.	DEC	2007	35	6					2373	2384		10.1214/009053607000000460		12	Statistics & Probability	Mathematics	261KP	WOS:000253077800006		
J	Friedlander, MP; Saunders, MA				Friedlander, Michael P.; Saunders, Michael A.			Discussion: The Dantzig selector: Statistical estimation when p is much larger than n	ANNALS OF STATISTICS			English	Editorial Material							ATOMIC DECOMPOSITION; BASIS PURSUIT; REGRESSION; LASSO		[Friedlander, Michael P.] Univ British Columbia, Dept Comp Sci, Vancouver, BC V6K 2C6, Canada; [Saunders, Michael A.] Stanford Univ, Dept Management Sci & Engn, Stanford, CA 94305 USA	Friedlander, MP (reprint author), Univ British Columbia, Dept Comp Sci, Vancouver, BC V6K 2C6, Canada.	mpf@cs.ubc.ca; saunders@stanford.edu					Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Chen SSB, 2001, SIAM REV, V43, P129, DOI 10.1137/S003614450037906X; Osborne MR, 2000, IMA J NUMER ANAL, V20, P389, DOI 10.1093/imanum/20.3.389; Efron B, 2004, ANN STAT, V32, P407; Candes E. J., 2007, L1 MAGIC; Donoho D., 2006, FAST SOLUTION L1 NOR; *ILOG CPLEX, 2007, MATH PROGR SYST; Osborne MR, 2000, J COMPUT GRAPH STAT, V9, P319, DOI 10.2307/1390657; ROMBERG J, 2005, MATLAB SOLVER DS PRO; SAUNDERS MA, 2005, PDCO MATLAB SOFTWARE; TOMLIN JA, 1975, 7512 SOL STANF U DEP	12	12	12	1	4	INST MATHEMATICAL STATISTICS	BEACHWOOD	PO BOX 22718, BEACHWOOD, OH 44122 USA	0090-5364			ANN STAT	Ann. Stat.	DEC	2007	35	6					2385	2391		10.1214/00905360700000479		7	Statistics & Probability	Mathematics	261KP	WOS:000253077800007		
J	Ahmed, SE; Doksum, KA; Hossain, S; You, J				Ahmed, S. Ejaz; Doksum, Kjell A.; Hossain, S.; You, Jinhong			Shrinkage, pretest and absolute penalty estimators in partially linear models	AUSTRALIAN & NEW ZEALAND JOURNAL OF STATISTICS			English	Article						asymptotic risk; hard thresholding; kernel smoothing; regression model; semiparametric least squares; simulation; smooth thresholding	SEMIPARAMETRIC REGRESSION; CONVERGENCE-RATES; COMPONENTS; SELECTION; LIKELIHOOD; BIASES; TESTS	We consider a partially linear model in which the vector of coefficients beta in the linear part can be partitioned as (beta(1), beta(2)), where beta(1) is the coefficient vector for main effects (e.g. treatment effect, genetic effects) and beta(2) is a vector for 'nuisance' effects (e.g. age, laboratory). In this situation, inference about beta(1) may benefit from moving the least squares estimate for the full model in the direction of the least squares estimate without the nuisance variables (Steinian shrinkage), or from dropping the nuisance variables if there is evidence that they do not provide useful information (pretesting). We investigate the asymptotic properties of Stein-type and pretest semiparametric estimators under quadratic loss and show that, under general conditions, a Stein-type semiparametric estimator improves on the full model conventional semiparametric least squares estimator. The relative performance of the estimators is examined using asymptotic analysis of quadratic risk functions and it is found that the Stein-type estimator outperforms the full model estimator uniformly. By contrast, the pretest estimator dominates the least squares estimator only in a small part of the parameter space, which is consistent with the theory. We also consider an absolute penaltytype estimator for partially linear models and give a Monte Carlo simulation comparison of shrinkage, pretest and the absolute penalty-type estimators. The comparison shows that the shrinkage method performs better than the absolute penalty-type estimation method when the dimension of the beta(2) parameter space is large.	[Ahmed, S. Ejaz; Hossain, S.] Univ Windsor, Dept Math & Stat, Windsor, ON N9B 3P4, Canada; [Doksum, Kjell A.] Univ Wisconsin, Dept Stat, Madison, WI 53706 USA; [You, Jinhong] Univ N Carolina, Dept Biostat, Chapel Hill, NC 27599 USA	Ahmed, SE (reprint author), Univ Windsor, Dept Math & Stat, Windsor, ON N9B 3P4, Canada.	seahmed@uwindsor.ca					AHMED S. E., 1999, J JAPAN STAT SOC, V29, P55; Ahmed SE, 2006, J NONPARAMETR STAT, V18, P401, DOI 10.1080/10485250601046752; Ahmed S.E., 2001, LECT NOTES STAT, V148, P103; ENGLE RF, 1986, J AM STAT ASSOC, V81, P310, DOI 10.2307/2289218; Cui XG, 2005, BIOSTATISTICS, V6, P59, DOI 10.1093/biostatistics/kxh018; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Hoeting JA, 1999, STAT SCI, V14, P382; Claeskens G, 2003, J AM STAT ASSOC, V98, P900, DOI 10.1198/016214503000000819; Bancroft TA, 1944, ANN MATH STAT, V15, P190, DOI 10.1214/aoms/1177731284; Hamilton SA, 1997, J MULTIVARIATE ANAL, V60, P1, DOI 10.1006/jmva.1996.1642; BICKEL PJ, 1984, ANN STAT, V12, P864, DOI 10.1214/aos/1176346707; Bowman A.W., 1997, APPL SMOOTHING TECHN; Bunea F, 2004, ANN STAT, V32, P898, DOI 10.1214/009053604000000247; BURMAN P, 1992, 243 U CAL DAV DIV ST; Burnham K. P., 2002, MODEL SELECTION MULT; CHEN H, 1988, ANN STAT, V16, P136, DOI 10.1214/aos/1176350695; CHEN H, 1994, ANN STAT, V22, P211, DOI 10.1214/aos/1176325366; Chen S., 1994, BASIS PURSUIT; Chen S. S., 1999, SIAM J SCI COMPUT, V20, P33; DONALD SG, 1994, J MULTIVARIATE ANAL, V50, P30, DOI 10.1006/jmva.1994.1032; Donoho DL, 1998, ANN STAT, V26, P879; EUBANK RL, 1990, J MULTIVARIATE ANAL, V32, P70, DOI 10.1016/0047-259X(90)90072-P; Fan JQ, 1998, ANN STAT, V26, P943; Gao J, 1997, COMMUN STAT-THEOR M, V26, P787, DOI 10.1080/03610929708831950; GAO JT, 1995, STAT PROBABIL LETT, V25, P153, DOI 10.1016/0167-7152(94)00217-V; GAO JT, 1995, COMMUN STAT THEORY, V24, P1985, DOI 10.1080/03610929508831598; Hardle W, 2000, PARTIALLY LINEAR MOD; HECKMAN NE, 1986, J ROY STAT SOC B MET, V48, P244; Hoeting JA, 2002, J COMPUT GRAPH STAT, V11, P485, DOI 10.1198/106186002501; IHAKA R, 1996, J COMPUTATIONAL GRAP, V5, P299, DOI DOI 10.2307/1390807; Judge GG, 2004, J AM STAT ASSOC, V99, P479, DOI 10.1198/0161214501000000430; Liang H, 2004, J AM STAT ASSOC, V99, P357, DOI 10.1198/016214504000000421; Liang H, 1999, COMMUN STAT-THEOR M, V28, P2025, DOI 10.1080/03610929908832403; RICE J, 1986, STAT PROBABIL LETT, V4, P203, DOI 10.1016/0167-7152(86)90067-2; ROBINSON PM, 1988, ECONOMETRICA, V56, P931, DOI 10.2307/1912705; Schick A, 1998, J TIME SER ANAL, V19, P575, DOI 10.1111/1467-9892.00109; SCHICK A, 1994, STAT PROBABIL LETT, V21, P371, DOI 10.1016/0167-7152(94)00034-4; Schick A, 1996, STOCH PROC APPL, V61, P339, DOI 10.1016/0304-4149(95)00093-3; SCOLVE SL, 1972, ANN MATH STAT, V43, P1481; Shi J, 2000, J MULTIVARIATE ANAL, V72, P132, DOI 10.1006/jmva.1999.1866; SHI PD, 1995, STATISTICS, V26, P27, DOI 10.1080/02331889508802465; SPECKMAN P, 1988, J ROY STAT SOC B MET, V50, P413; STEIN C, 1956, ANN MATH STAT, V27, P616, DOI 10.1214/aoms/1177728171; Wang QH, 2004, J AM STAT ASSOC, V99, P334, DOI 10.1198/016214504000000449; Xue HQ, 2004, J AM STAT ASSOC, V99, P346, DOI 10.1198/016214504000000313	45	21	21	14	25	BLACKWELL PUBLISHING	OXFORD	9600 GARSINGTON RD, OXFORD OX4 2DQ, OXON, ENGLAND	1369-1473			AUST NZ J STAT	Aust. N. Z. J. Stat.	DEC	2007	49	4					435	454		10.1111/j.1467-842X.2007.00493.x		20	Statistics & Probability	Mathematics	248KL	WOS:000252152100008		
J	Tai, F; Pan, W				Tai, Feng; Pan, Wei			Incorporating prior knowledge of gene functional groups into regularized discriminant analysis of microarray data	BIOINFORMATICS			English	Article							EXPRESSION DATA; LINEAR-REGRESSION; BREAST-CANCER; CLASSIFICATION; PREDICTORS; CENTROIDS; SELECTION; MODELS	Motivation: Discriminant analysis for high-dimensional and low-sample-sized data has become a hot research topic in bioinformatics, mainly motivated by its importance and challenge in applications to tumor classifications for high-dimensional microarray data. Two of the popular methods are the nearest shrunken centroids, also called predictive analysis of microarray (PAM), and shrunken centroids regularized discriminant analysis (SCRDA). Both methods are modifications to the classic linear discriminant analysis (LDA) in two aspects tailored to high-dimensional and low-sample-sized data: one is the regularization of the covariance matrix, and the other is variable selection through shrinkage. In spite of their usefulness, there are potential limitations with each method. The main concern is that both PAM and SCRDA are possibly too extreme: the covariance matrix in the former is restricted to be diagonal while in the latter there is barely any restriction. Based on the biology of gene functions and given the feature of the data, it may be beneficial to estimate the covariance matrix as an intermediate between the two; furthermore, more effective shrinkage schemes may be possible. Results: We propose modified LDA methods to integrate biological knowledge of gene functions (or variable groups) into classification of microarray data. Instead of simply treating all the genes independently or imposing no restriction on the correlations among the genes, we group the genes according to their biological functions extracted from existing biological knowledge or data, and propose regularized covariance estimators that encourages between-group gene independence and within-group gene correlations while maintaining the flexibility of any general covariance structure. Furthermore, we propose a shrinkage scheme on groups of genes that tends to retain or remove a whole group of the genes altogether, in contrast to the standard shrinkage on individual genes. We show that one of the proposed methods performed better than PAM and SCRDA in a simulation study and several real data examples.	Univ Minnesota, Sch Publ Hlth, Div Biostat, Minneapolis, MN 55454 USA	Pan, W (reprint author), Univ Minnesota, Sch Publ Hlth, Div Biostat, A460 Mayo Bldg, Minneapolis, MN 55454 USA.						Yuan M, 2006, J ROY STAT SOC B, V68, P49, DOI 10.1111/j.1467-9868.2005.00532.x; Gui J, 2005, BIOINFORMATICS, V21, P3001, DOI 10.1093/bioinformatics/bti422; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Wang YX, 2005, LANCET, V365, P671, DOI 10.1016/S0140-6736(05)17947-1; Guo YQ, 2007, BIOSTATISTICS, V8, P86, DOI 10.1093/biostatistics/kxj035; Ashburner M, 2000, NAT GENET, V25, P25; Huang E, 2003, LANCET, V361, P1590, DOI 10.1016/S0140-6736(03)13308-9; Tibshirani R, 2003, STAT SCI, V18, P104, DOI 10.1214/ss/1056397488; Singh D, 2002, CANCER CELL, V1, P203, DOI 10.1016/S1535-6108(02)00030-2; Cai TT, 1999, ANN STAT, V27, P898, DOI 10.1214/aos/1018031262; Gordon GJ, 2002, CANCER RES, V62, P4963; ARMSTRONG, 2001, NAT GENET, V30, P41; Hastie T., 2001, ELEMENTS STAT LEARNI; Huang DS, 2006, BIOINFORMATICS, V22, P1259, DOI 10.1093/bioinformatics/btl065; Huang XH, 2003, BIOINFORMATICS, V19, P2072, DOI 10.1093/bioinformatics/btg283; Kanehisa M, 1996, SCI TECHNOLOGY JAPAN, V59; Lottaz C, 2005, BIOINFORMATICS, V21, P1971, DOI 10.1093/bioinformatics/bti292; Pan W, 2005, STAT APPL GENET MO B, V4; Pan W, 2006, BIOINFORMATICS, V22, P795, DOI 10.1093/bioinformatics/btl011; Pang H, 2006, BIOINFORMATICS, V22, P2028, DOI 10.1093/bioinformatics/btl344; Srivastava M., 2007, J JAPAN STAT SOC, V37, P123; Tai F, 2007, BIOINFORMATICS, V23, P1775, DOI 10.1093/bioinformatics/btm234; TAI F, 2007, 2008020 RES REP U MI; Vapnik V., 1998, STAT LEARNING THEORY; Wang SJ, 2007, BIOINFORMATICS, V23, P972, DOI 10.1093/bioinformatics/btm046; Wei Z, 2007, BIOSTATISTICS, V8, P265, DOI 10.1093/biostatistics/kxl007; Wu BL, 2006, BIOINFORMATICS, V22, P472, DOI 10.1093/bioinformatics/bti827	27	20	21	3	5	OXFORD UNIV PRESS	OXFORD	GREAT CLARENDON ST, OXFORD OX2 6DP, ENGLAND	1367-4803			BIOINFORMATICS	Bioinformatics	DEC 1	2007	23	23					3170	3177		10.1093/bioinformatics/btm488		8	Biochemical Research Methods; Biotechnology & Applied Microbiology; Computer Science, Interdisciplinary Applications; Mathematical & Computational Biology; Statistics & Probability	Biochemistry & Molecular Biology; Biotechnology & Applied Microbiology; Computer Science; Mathematical & Computational Biology; Mathematics	236WT	WOS:000251334800009	17933851	
J	Houseman, EA; Marsit, C; Karagas, M; Ryan, LM				Houseman, E. Andres; Marsit, Carmen; Karagas, Margaret; Ryan, Louise M.			Penalized item response theory models: Application to epigenetic alterations in bladder cancer	BIOMETRICS			English	Article						Bayesian hierarchical models; bladder cancer; constrained estimation; latent trait models; methylation silencing; penalized likelihood; Rasch models; ridge regression	LATENT VARIABLE MODELS; TUMOR-TISSUES; PARAMETER-ESTIMATION; ARSENIC EXPOSURE; DRINKING-WATER; VOIDED URINE; GENES; HYPERMETHYLATION; REGRESSION; METHYLATION	Increasingly used in health-related applications, latent variable models provide an appealing framework for handling high-dimensional exposure and response data. Item response theory (IRT) models, which have gained widespread popularity, were originally developed for use in the context of educational testing, where extremely large sample sizes permitted the estimation of a moderate-to-large number of parameters. In the context of public health applications, smaller sample sizes preclude large parameter spaces. Therefore, we propose a penalized likelihood approach to reduce mean square error and improve numerical stability. We present a continuous family of models, indexed by a tuning parameter, that range between the Rasch model and the HIT model. The tuning parameter is selected by cross validation or approximations such as Akaike Information Criterion. While our approach can be placed easily in a Bayesian context, we find that our frequentist approach is more computationally efficient. We demonstrate our methodology on a study of methylation silencing of gene expression in bladder tumors. We obtain similar results using both frequentist and Bayesian approaches, although the frequentist approach is less computationally demanding. In particular, we find high correlation of methylation silencing among 16 loci in bladder tumors, that methylation is associated with smoking and also with patient survival.	Univ Massachusetts Lowell, Dept Work Environm, Lowell, MA 01854 USA; Harvard Univ, Sch Publ Hlth, Dept Genet & Complex Dis, Boston, MA 02115 USA; Dartmouth Hitchcock Med Ctr, Dept Community & Family Med, Lebanon, NH 03756 USA	Houseman, EA (reprint author), Univ Massachusetts Lowell, Dept Work Environm, Kitson Hall,202E,One Univ Ave, Lowell, MA 01854 USA.	andres_houseman@uml.edu	Ryan, Louise/A-4562-2009; 	Ryan, Louise/0000-0001-5957-2490; Marsit, Carmen/0000-0003-4566-150X			Adams RJ, 1997, J EDUC BEHAV STAT, V22, P47, DOI 10.3102/10769986022001047; AKAIKE H, 1974, IEEE T AUTOMAT CONTR, VAC19, P716, DOI 10.1109/TAC.1974.1100705; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; DiCiccio TJ, 1996, STAT SCI, V11, P189; Jemal A, 2005, CA-CANCER J CLIN, V55, P10; Jones PA, 1999, NAT GENET, V21, P163, DOI 10.1038/5947; Sammel MD, 1997, J ROY STAT SOC B MET, V59, P667, DOI 10.1111/1467-9868.00090; BATES MN, 1995, AM J EPIDEMIOL, V141, P523; Blozis SA, 1999, J EDUC BEHAV STAT, V24, P245, DOI 10.2307/1165324; Budtz-Jorgensen E, 2003, ENVIRONMETRICS, V14, P105, DOI 10.1002/env.569; Chan MWY, 2002, CLIN CANCER RES, V8, P464; Chan MWY, 2003, INT J CANCER, V104, P611, DOI 10.1002/ijc.10971; Dulaimi E, 2004, CLIN CANCER RES, V10, P1887, DOI 10.1158/1078-0432.CCR-03-0127; Eaves L, 2005, BEHAV GENET, V35, P765, DOI 10.1007/s10519-005-7284-z; Esrig D, 1997, Semin Urol Oncol, V15, P154; ESRIG D, 1994, NEW ENGL J MED, V331, P1259, DOI 10.1056/NEJM199411103311903; Friedrich MG, 2004, CLIN CANCER RES, V10, P7457, DOI 10.1158/1078-0432.CCR-04-0930; Gao FR, 2005, APPL MEAS EDUC, V18, P351, DOI 10.1207/s15324818ame1804_2; GELFAND AE, 1995, BIOMETRIKA, V82, P479, DOI 10.1093/biomet/82.3.479; HAMBLETON RK, 1977, J EDUC MEAS, V14, P75, DOI 10.1111/j.1745-3984.1977.tb00030.x; Hastie T., 2001, ELEMENTS STAT LEARNI; Houseman EA, 2006, BIOMETRICS, V62, P1062, DOI 10.1111/j.1541-0420.2006.00566.x; IBRAHIM JG, 2001, BAYESIAN SURVIVAL AN; Jones PA, 1998, EUR UROL, V33, P7, DOI 10.1159/000052251; Karagas MR, 1998, ENVIRON HEALTH PERSP, V106, P1047, DOI 10.2307/3434150; Karagas MR, 2004, CANCER CAUSE CONTROL, V15, P465, DOI 10.1023/B:CACO.0000036452.55199.a3; Kennedy W. J., 1980, STAT COMPUTING; Larsen K, 2005, BIOMETRICS, V61, P1049, DOI 10.1111/j.1541-0420.2005.00374.x; Legler JM, 1997, J AM STAT ASSOC, V92, P13, DOI 10.2307/2291445; LINDSAY B, 1991, J AM STAT ASSOC, V86, P96, DOI 10.2307/2289719; LORD FM, 1986, J EDUC MEAS, V23, P157, DOI 10.1111/j.1745-3984.1986.tb00241.x; Marsit CJ, 2005, CANCER RES, V65, P7081, DOI 10.1058/0008-5472.CAN-05-0267; Masters JRW, 2003, J PATHOL, V200, P74, DOI 10.1002/path.1293; May M, 2004, UROL INT, V72, P103, DOI 10.1159/000075962; Rasch G, 1960, PROBABILISTIC MODELS; RASCH G, 1966, BRIT J MATH STAT PSY, V19, P49; Sammel MD, 1996, BIOMETRICS, V52, P650, DOI 10.2307/2532903; Sanchez BN, 2005, J AM STAT ASSOC, V100, P1443, DOI 10.1198/016214505000001005; Sathyanarayana UG, 2004, CANCER RES, V64, P1425, DOI 10.1158/0008-5472.CAN-03-0701; Scott SL, 2002, J AM STAT ASSOC, V97, P409, DOI 10.1198/016214502760046961; Silverman DT, 1996, CANC EPIDEMIOLOGY PR, V2nd, P1156; Sinharay S, 2005, J EDUC MEAS, V42, P375, DOI 10.1111/j.1745-3984.2005.00021.x; Skrondal A., 2004, GEN LATENT VARIABLE; Spiegelhalter D, 2003, WINBUGS USER MANUAL; Spiegelhalter D., 1996, BAYESIAN STAT, V5, P407; Vineis P, 2001, CANCER EPIDEM BIOMAR, V10, P1249; Vineis P, 2000, MUTAT RES-REV MUTAT, V463, P103; Wang CY, 2000, BIOMETRICS, V56, P487, DOI 10.1111/j.0006-341X.2000.00487.x	48	3	3	1	4	BLACKWELL PUBLISHING	OXFORD	9600 GARSINGTON RD, OXFORD OX4 2DQ, OXON, ENGLAND	0006-341X			BIOMETRICS	Biometrics	DEC	2007	63	4					1269	1277		10.1111/j.1541-0420.2007.00806.x		9	Biology; Mathematical & Computational Biology; Statistics & Probability	Life Sciences & Biomedicine - Other Topics; Mathematical & Computational Biology; Mathematics	239HB	WOS:000251508300033	17484774	
J	Yu, ZX; Schaid, DJ				Yu, Zhaoxia; Schaid, Daniel J.			Methods to impute missing genotypes for population data	HUMAN GENETICS			English	Article							GENOME-WIDE ASSOCIATION; EXPECTATION-MAXIMIZATION ALGORITHM; HAPLOTYPE INFERENCE; UNRELATED INDIVIDUALS; MAXIMUM-LIKELIHOOD; STATISTICAL-MODEL; LINKAGE PHASE; IMPUTATION; RECONSTRUCTION; TRAITS	For large-scale genotyping studies, it is common for most subjects to have some missing genetic markers, even if the missing rate per marker is low. This compromises association analyses, with varying numbers of subjects contributing to analyses when performing single-marker or multi-marker analyses. In this paper, we consider eight methods to infer missing genotypes, including two haplotype reconstruction methods (local expectation maximization-EM, and fastPHASE), two k-nearest neighbor methods (original k-nearest neighbor, KNN, and a weighted k-nearest neighbor, wtKNN), three linear regression methods (backward variable selection, LM.back, least angle regression, LM.lars, and singular value decomposition, LM.svd), and a regression tree, Rtree. We evaluate the accuracy of them using single nucleotide polymorphism (SNP) data from the HapMap project, under a variety of conditions and parameters. We find that fastPHASE has the lowest error rates across different analysis panels and marker densities. LM.lars gives slightly less accurate estimate of missing genotypes than fastPHASE, but has better performance than the other methods.	Mayo Clin, Coll Med, Dept Hlth Sci Res, Div Biostat, Rochester, MN 55905 USA; Univ Calif Irvine, Dept Stat, Irvine, CA 92697 USA	Schaid, DJ (reprint author), Mayo Clin, Coll Med, Dept Hlth Sci Res, Div Biostat, 200 First St,SW,Harwick 775, Rochester, MN 55905 USA.	schaid@mayo.edu					AKAIKE H, 1974, IEEE T AUTOMAT CONTR, VAC19, P716, DOI 10.1109/TAC.1974.1100705; Troyanskaya O, 2001, BIOINFORMATICS, V17, P520, DOI 10.1093/bioinformatics/17.6.520; Price AL, 2006, NAT GENET, V38, P904, DOI 10.1038/ng1847; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1; Schaid DJ, 2002, AM J HUM GENET, V70, P425, DOI 10.1086/338688; Stephens M, 2005, AM J HUM GENET, V76, P449, DOI 10.1086/428594; EXCOFFIER L, 1995, MOL BIOL EVOL, V12, P921; Stephens M, 2003, AM J HUM GENET, V73, P1162, DOI 10.1086/379378; Marchini J, 2007, NAT GENET, V39, P906, DOI 10.1038/ng2088; Stephens M, 2001, AM J HUM GENET, V68, P978, DOI 10.1086/319501; Marchini J, 2006, AM J HUM GENET, V78, P437, DOI 10.1086/500808; MALLOWS CL, 1973, TECHNOMETRICS, V15, P661, DOI 10.2307/1267380; Scheet P, 2006, AM J HUM GENET, V78, P629, DOI 10.1086/502802; Becker T, 2005, HUM HERED, V59, P185, DOI 10.1159/000086696; Chiano MN, 1998, ANN HUM GENET, V62, P55, DOI 10.1046/j.1469-1809.1998.6210055.x; Dai JY, 2006, GENET EPIDEMIOL, V30, P690, DOI 10.1002/gepi.20180; ENFRON B, 2004, ANN STAT, V32, P407; Fallin D, 2000, AM J HUM GENET, V67, P947, DOI 10.1086/303069; Hastie T., 2001, ELEMENTS STAT LEARNI; HAWLEY ME, 1995, J HERED, V86, P409; Hoti F, 2006, HEREDITY, V97, P4, DOI 10.1038/sj.hdy.6800817; Altshuler D, 2005, NATURE, V437, P1299, DOI 10.1038/nature04226; Lake SL, 2003, HUM HERED, V55, P56, DOI 10.1159/000071811; LEWONTIN RW, 1964, GENETICS, V120, P849; Lichten M, 1995, ANNU REV GENET, V29, P423, DOI 10.1146/annurev.ge.29.120195.002231; Lin S, 2004, GENOME RES, V14, P1624, DOI 10.1101/gr.2204604; Little Roderick, 1987, STAT ANAL MISSING DA; Liu NJ, 2006, GENET EPIDEMIOL, V30, P290, DOI 10.1002/gepi.20144; LONG JC, 1995, AM J HUM GENET, V56, P799; Nicolae DL, 2006, GENET EPIDEMIOL, V30, P718, DOI 10.1002/gepi.20182; Niu TH, 2002, AM J HUM GENET, V70, P157, DOI 10.1086/338446; Qin ZHS, 2002, AM J HUM GENET, V71, P1242, DOI 10.1086/344207; RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626; Servin B, 2007, PLOS GENET, V3, P1296, DOI 10.1371/journal.pgen.0030114; Souverein OW, 2006, ANN HUM GENET, V70, P372, DOI 10.1111/j.1469-1809.2005.00236.x; THERNEAU T, 1997, TECH REP, V61, P52	37	27	29	1	3	SPRINGER	NEW YORK	233 SPRING STREET, NEW YORK, NY 10013 USA	0340-6717			HUM GENET	Hum. Genet.	DEC	2007	122	5					495	504		10.1007/s00439-007-0427-y		10	Genetics & Heredity	Genetics & Heredity	234EQ	WOS:000251143900009	17851696	
J	Figueiredo, MAT; Nowak, RD; Wright, SJ				Figueiredo, Mario A. T.; Nowak, Robert D.; Wright, Stephen J.			Gradient Projection for Sparse Reconstruction: Application to Compressed Sensing and Other Inverse Problems	IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING			English	Article						Compressed sensing; convex optimization; deconvolution; gradient projection; quadratic programming; sparseness; sparse reconstruction	INTERIOR-POINT METHODS; WARM-START STRATEGIES; SIGNAL RECONSTRUCTION; LEAST-SQUARES; ALGORITHM; REPRESENTATIONS; RECOVERY; NOISE; DECONVOLUTION; MINIMIZATION	Many problems in signal processing and statistical inference involve finding sparse solutions to under-determined, or ill-conditioned, linear systems of equations. A standard approach consists in minimizing an objective function which includes a quadratic (squared l(2)) error term combined with a sparseness-inducing (l(1)) regularization term. Basis pursuit, the least absolute shrinkage and selection operator (LASSO), wavelet-based deconvolution, and compressed sensing are a few well-known examples of this approach. This paper proposes gradient projection (GP) algorithms for the bound-constrained quadratic programming (BCQP) formulation of these problems. We test variants of this approach that select the line search parameters in different ways, including techniques based on the Barzilai-Borwein method. Computational experiments show that these GP approaches perform well in a wide range of applications, often being significantly faster (in terms of computation time) than competing methods. Although the performance of GP methods tends to degrade as the regularization term is de-emphasized, we show how they can be embedded in a continuation scheme to recover their efficient practical performance.	[Figueiredo, Mario A. T.] Inst Telecommun, P-1049001 Lisbon, Portugal; [Figueiredo, Mario A. T.] Inst Super Tecn, Dept Elect & Comp Engn, P-1049001 Lisbon, Portugal; [Nowak, Robert D.] Univ Wisconsin, Dept Elect & Comp Engn, Madison, WI 53706 USA; [Wright, Stephen J.] Univ Wisconsin, Dept Comp Sci, Madison, WI 53706 USA	Figueiredo, MAT (reprint author), Inst Telecommun, P-1049001 Lisbon, Portugal.		Figueiredo, Mario/C-5428-2008	Figueiredo, Mario/0000-0002-0970-7745	NSF [CCF-0430504, CNS-0540147]; Fundacao para a Ciencia e Tecnologia [POSC/EEA-CPS/61271/2004]	This work was supported in part by the NSF under Grants CCF-0430504 and CNS-0540147 and by the Fundacao para a Ciencia e Tecnologia, POSC/FEDER under Grant POSC/EEA-CPS/61271/2004. The associate editor coordinating the review of this manuscript and approving it for publication was Y. Eldar.	ALLINEY S, 1994, IEEE T SIGNAL PROCES, V42, P618, DOI 10.1109/78.277854; Birgin EG, 2000, SIAM J OPTIMIZ, V10, P1196, DOI 10.1137/S1052623497330963; Tropp JA, 2006, IEEE T INFORM THEORY, V52, P1030, DOI 10.1109/TIT.2005.864420; Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582; RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Haupt J, 2006, IEEE T INFORM THEORY, V52, P4036, DOI 10.1109/TIT.2006.880031; Candes EJ, 2006, IEEE T INFORM THEORY, V52, P489, DOI 10.1109/TIT.2005.862083; Osborne MR, 2000, IMA J NUMER ANAL, V20, P389, DOI 10.1093/imanum/20.3.389; Chambolle A, 2004, J MATH IMAGING VIS, V20, P89; Candes EJ, 2006, IEEE T INFORM THEORY, V52, P5406, DOI 10.1109/TIT.2006.885507; Tropp JA, 2004, IEEE T INFORM THEORY, V50, P2231, DOI 10.1109/TIT.2004.834793; Dai YH, 2005, NUMER MATH, V100, P21, DOI 10.1007/s00211-004-0569-y; Efron B, 2004, ANN STAT, V32, P407; Candes EJ, 2006, COMMUN PUR APPL MATH, V59, P1207, DOI 10.1002/cpa.20124; Daubechies I, 2004, COMMUN PUR APPL MATH, V57, P1413, DOI 10.1002/cpa.20042; DONOHO DL, 1995, IEEE T INFORM THEORY, V41, P613, DOI 10.1109/18.382009; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P6, DOI 10.1109/TIT.2005.860430; Figueiredo MAT, 2003, IEEE T IMAGE PROCESS, V12, P906, DOI 10.1109/TIP.2003.814255; Hunter DR, 2004, AM STAT, V58, P30, DOI 10.1198/0003130042836; BARZILAI J, 1988, IMA J NUMER ANAL, V8, P141, DOI 10.1093/imanum/8.1.141; Bertsekas D., 1999, NONLINEAR PROGRAMMIN; Blumensath T., 2007, GRADIENT PURSUITS; Candes E., 2007, ANN STAT; CLAERBOU.JF, 1973, GEOPHYSICS, V38, P826, DOI 10.1190/1.1440378; COMBETTES PL, 2005, SIAM J MULTISCALE MO, V4, P1168; COTTLE R., 1993, LINEAR COMPLEMENTARI; Davis G, 1997, J CONSTR APPROX, V13, P57; Donoho D., 2006, FAST SOLUTION L1 NOR; Elad M., 2006, P IEEE COMP SOC C CO; Elad M, 2006, IEEE T INFORM THEORY, V52, P5559, DOI 10.1109/TIT.2006.885522; Figueiredo M., 2005, P IEEE INT C IM PROC; Frank M., 1956, NAVAL RES LOGIST QUA, V3, P95, DOI 10.1002/nav.3800030109; Fuchs JJ, 2004, IEEE T INFORM THEORY, V50, P1341, DOI 10.1109/TIT.2004.828141; Fuchs JJ, 1999, IEEE T SIGNAL PROCES, V47, P237, DOI 10.1109/78.738263; GONDZIO J, 2006, MS06005 U ED; HALE E, 2007, TR0707 RIC U SENS DE; Iusem AN, 2003, COMPUT APPL MATH, V22, P37, DOI 10.1590/S1807-03022003000100003; John E, 2008, COMPUT OPTIM APPL, V41, P151, DOI 10.1007/s10589-007-9096-y; Kim S., 2007, METHOD LARGE SCALE L; LEVY S, 1981, GEOPHYSICS, V46, P1235, DOI 10.1190/1.1441261; LUSTIG M, SPARSE MRI APPL COMP; Malioutov D., 2005, P IEEE INT C AC SPEE, V5, P733, DOI 10.1109/ICASSP.2005.1416408; Mallat S., 1998, WAVELET TOUR SIGNAL; Miller A, 2002, SUBSET SELECTION REG, V2nd; Moghaddam B., 2006, ADV NEURAL INFORM PR, V18, P915; More JJ, 1991, SIAM J OPTIMIZ, V1, P93, DOI 10.1137/0801008; Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5; NOWAK R, 2001, P 35 AS C SIGN SYST; PAIGE CC, 1982, ACM T MATH SOFTWARE, V8, P43, DOI 10.1145/355984.355989; Rockafellar R.T., 1970, CONVEX ANAL; SANTOSA F, 1986, SIAM J SCI STAT COMP, V7, P1307, DOI 10.1137/0907087; Serafini T., 2004, OPTIMIZATION METHODS, V20, P353, DOI 10.1080/10556780512331318182; TAYLOR HL, 1979, GEOPHYSICS, V44, P39, DOI 10.1190/1.1440921; Turlach B., 2005, TECHNOMETRICS, V27, P349; Turlach B. A., 2005, P AM STAT ASS STAT C, P2572; WRIGHT SJ, 1990, J OPTIMIZ THEORY APP, V65, P531, DOI 10.1007/BF00939565; Wright SJ, 1997, PRIMAL DUAL INTERIOR; Yildirim EA, 2002, SIAM J OPTIMIZ, V12, P782, DOI 10.1137/S1052623400369235; Yun S. W., 2009, PDCO PRIMAL DUAL INT, V117, P387	61	829	939	24	135	IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC	PISCATAWAY	445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA	1932-4553			IEEE J-STSP	IEEE J. Sel. Top. Signal Process.	DEC	2007	1	4					586	597		10.1109/JSTSP.2007.910281		12	Engineering, Electrical & Electronic	Engineering	437NT	WOS:000265494900006		
J	Kim, SJ; Koh, K; Lustig, M; Boyd, S; Gorinevsky, D				Kim, Seung-Jean; Koh, K.; Lustig, M.; Boyd, Stephen; Gorinevsky, Dimitry			An Interior-Point Method for Large-Scale l(1)-Regularized Least Squares	IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING			English	Article						Basis pursuit denoising; compressive sampling; compressed sensing; convex optimization; interior-point methods; least squares; preconditioned conjugate gradients; l(1) regularization	VARIABLE SELECTION; MODEL SELECTION; REGRESSION; LASSO; NOISE; REGULARIZATION; RECOVERY; REPRESENTATIONS; RECONSTRUCTION; ALGORITHM	Recently, a lot of attention has been paid to l(1) regularization based methods for sparse signal reconstruction (e.g., basis pursuit denoising and compressed sensing) and feature selection (e.g., the Lasso algorithm) in signal processing, statistics, and related fields. These problems can be cast as l(1)-regularized least-squares programs (LSPs), which can be reformulated as convex quadratic programs, and then solved by several standard methods such as interior-point methods, at least for small and medium size problems. In this paper, we describe a specialized interior-point method for solving large-scale, l(1)-regularized LSPs that uses the preconditioned conjugate gradients algorithm to compute the search direction. The interior-point method can solve large sparse problems, with a million variables and observations, in a few tens of minutes on a PC. It can efficiently solve large dense problems, that arise in sparse signal recovery with orthogonal transforms, by exploiting fast algorithms for these transforms. The method is illustrated on a magnetic resonance imaging data set.	[Kim, Seung-Jean; Koh, K.; Lustig, M.; Boyd, Stephen; Gorinevsky, Dimitry] Stanford Univ, Dept Elect Engn, Informat Syst Lab, Stanford, CA 94305 USA	Kim, SJ (reprint author), Stanford Univ, Dept Elect Engn, Informat Syst Lab, Stanford, CA 94305 USA.	sjkim@stanford.edu; deneb1@statiford.edu; mlustig@stanford.edu; boyd@stanford.edu; gorin@stanford.edu	Lustig, Michael/G-7081-2012		Focus Center Research Program Center for Circuit and System Solutions [2003-CT-888]; JPL [1291856]; Precourt Institute on Energy Efficiency; Army award [W911NF-07-1-0029]; NSF [ECS-0423905, 0529426]; DARPA [FA9550-06-1-0514]; AFOSR [FA9550-06-1-0312]	This work was supported by the Focus Center Research Program Center for Circuit and System Solutions award 2003-CT-888, by JPL award 1291856, by the Precourt Institute on Energy Efficiency, by Army award W911NF-07-1-0029, by NSF awards ECS-0423905 and 0529426, by DARPA award N66001-06-C-2021, by NASA award NNX07AEIIA, by AFOSR award FA9550-06-1-0514, and by AFOSR award FA9550-06-1-0312. The associate editor coordinating the review of this manuscript and approving it for publication was Dr. Yonina Eldar.	Tropp JA, 2006, IEEE T INFORM THEORY, V52, P1030, DOI 10.1109/TIT.2005.864420; Knight K, 2000, ANN STAT, V28, P1356; Yuan M, 2006, J ROY STAT SOC B, V68, P49, DOI 10.1111/j.1467-9868.2005.00532.x; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582; RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Zhao P, 2006, J MACH LEARN RES, V7, P2541; Chen SSB, 2001, SIAM REV, V43, P129, DOI 10.1137/S003614450037906X; Candes EJ, 2006, IEEE T INFORM THEORY, V52, P489, DOI 10.1109/TIT.2005.862083; Osborne MR, 2000, IMA J NUMER ANAL, V20, P389, DOI 10.1093/imanum/20.3.389; Hastie T, 2007, ELECTRON J STAT, V1, P1, DOI 10.1214/07-EJS004; Zou H, 2005, J ROY STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x; Zou H, 2006, J AM STAT ASSOC, V101, P1418, DOI 10.1198/016214506000000735; Tsaig Y, 2006, SIGNAL PROCESS, V86, P549, DOI 10.1016/j.sigpro.2005.05.029; Efron B, 2004, ANN STAT, V32, P407; Candes EJ, 2006, COMMUN PUR APPL MATH, V59, P1207, DOI 10.1002/cpa.20124; Daubechies I, 2004, COMMUN PUR APPL MATH, V57, P1413, DOI 10.1002/cpa.20042; Neumaier A, 1998, SIAM REV, V40, P636, DOI 10.1137/S0036144597321909; Meinshausen N, 2006, ANN STAT, V34, P1436, DOI 10.1214/009053606000000281; Lobo MS, 1998, LINEAR ALGEBRA APPL, V284, P193, DOI 10.1016/S0024-3795(98)10032-0; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P6, DOI 10.1109/TIT.2005.860430; BARLOW R. E., 1972, STAT INFERENCE ORDER; Candes E., 2006, P INT C MATH; CANDES E, 2006, I1 MAGIC COLLECTION; Demmel J. W., 1997, APPL NUMERICAL LINEA; DONOHO D, SPARSELAB SEEKING SP; DONOHO D, 2006, FAST SOLUTION UNPUB; Elad M., 2006, P IEEE COMP SOC C CO, V2, P1924; Figueiredo M., 2005, P IEEE INT C IM PROC, P782; Figueiredo M., 2007, IEEE J SELECT TOPICS; FRIEDMAN J, PATHWISE COORDINATE; Fuchs JJ, 2005, IEEE T INFORM THEORY, V51, P3601, DOI 10.1109/TIT.2005.855614; Gorinevsky D, 2004, P AMER CONTR CONF, P5394; GORINEVSKY D, 2007, IEEE T SIGNAL PROCES; HALE E, 2007, FIXED POINT CO UNPUB; Hastie T., 2001, SPRINGER SERIES STAT; Hastie T, 2004, J MACH LEARN RES, V5, P1391; Johnson CA, 2000, IEEE T MED IMAGING, V19, P271, DOI 10.1109/42.848179; Kelley C. T., 1995, FRONTIERS APPL MATH, V16; Koh KM, 2007, J MACH LEARN RES, V8, P1519; Lewis A., 2000, CONVEX ANAL NONLINEA; Luenberger D. G., 1984, LINEAR NONLINEAR PRO; *MOSEK, 2002, MOSEK OPT TOOLS VERS; NARKISS G, 2005, 559 CCIT; Nesterov Y., 2007, 200776 CORE; Nesterov Y.E., 1994, STUDIES APPL MATH, V13; OSHER S, 2005, SIAM J MULTISCALE MO, V4, P460, DOI DOI 10.1137/040605412; PAIGE CC, 1982, ACM T MATH SOFTWARE, V8, P43, DOI 10.1145/355984.355989; Polyak B.T., 1987, OPTIMIZATION SOFTWAR; Portugal LF, 2000, NETWORKS, V35, P91, DOI 10.1002/(SICI)1097-0037(200003)35:2<91::AID-NET1>3.0.CO;2-T; Robertson T, 1988, ORDER RESTRICTED STA; ROSSET S, ADV NEURAL INFORM PR, V17; Saad Y., 2003, ITERATIVE METHODS SP; SAMAR S, 2004, P 43 IEEE CDC, V3, P3115; SAUNDERS M. A., PDCO PRIMAL DUAL INT; Shor N. Z., 1985, SPRINGER SERIES COMP; Vandenberghe L., 2004, CONVEX OPTIMIZATION; VANDENBERGHE L, 1995, MATH PROGRAM, V69, P205, DOI 10.1007/BF01585558; WAINWRIGHT M, 2006, P 44 ANN ALL C COMM; Wright SJ, 1997, PRIMAL DUAL INTERIOR; Wright SJ, 1999, SPRINGER SERIES OPER; Ye Y., 1997, INTERIOR POINT ALGOR; YIN W, BREGMAN ITERATIVE AL	63	597	657	11	76	IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC	PISCATAWAY	445 HOES LANE, PISCATAWAY, NJ 08855 USA	1932-4553			IEEE J-STSP	IEEE J. Sel. Top. Signal Process.	DEC	2007	1	4					606	617		10.1109/JSTSP.2007.910971		12	Engineering, Electrical & Electronic	Engineering	437NT	WOS:000265494900008		
J	Figueiredo, MAT; Bioucas-Dias, JM; Nowak, RD				Figueiredo, Mario A. T.; Bioucas-Dias, Jose M.; Nowak, Robert D.			Majorization-minimization algorithms for wavelet-based image restoration	IEEE TRANSACTIONS ON IMAGE PROCESSING			English	Article						image deconvolution; image restoration; majorization-minimization (MM) algorithms; optimization; regularization; wavelets	EM ALGORITHM; TRANSMISSION TOMOGRAPHY; SCALE MIXTURES; DECONVOLUTION; SHRINKAGE; REPRESENTATIONS; REGULARIZATION; CONVERGENCE; LIKELIHOOD; REGRESSION	Standard formulations of image/signal deconvolution under wavelet-based priors/regularizers lead to very high-dimensional optimization problems involving the following difficulties: the non-Gaussian (heavy-tailed) wavelet priors lead to objective functions which are nonquadratic, usually nondifferentiable, and sometimes even nonconvex; the presence of the convolution operator destroys the separability which underlies the simplicity of wavelet-based denoising. This paper presents a unified view of several recently proposed algorithms for handling this class of optimization problems, placing them in a common majorization-minimization (MM) framework. One of the classes of algorithms considered (when using quadratic bounds on non-differentiable log-priors) shares the infamous "singularity issue" (SI) of "iteratively reweighted least squares" (IRLS) algorithms: the possibility of having to handle infinite weights, which may cause both numerical and convergence issues. In this paper, we prove several new results which strongly support the claim that the SI does not compromise the usefulness of this class of algorithms. Exploiting the unified MM perspective, we introduce a new algorithm, resulting from using l(1) bounds for nonconvex regularizers; the experiments confirm the superior performance of this method, when compared to the one based on quadratic majorization. Finally, an experimental comparison of the several algorithms, reveals their relative merits for different standard types of scenarios.	Univ Tecn Lisbon, Inst Telecomunicacoes, P-1049001 Lisbon, Portugal; Univ Tecn Lisbon, Inst Super Tecn, P-1049001 Lisbon, Portugal; Univ Wisconsin, Dept Elect & Comp Engn, Madison, WI 53706 USA	Figueiredo, MAT (reprint author), Univ Tecn Lisbon, Inst Telecomunicacoes, P-1049001 Lisbon, Portugal.	mario.figueiredo@lx.it.pt; jose.bioucas@lx.it.pt; nowak@ece.wisc.edu	Figueiredo, Mario/C-5428-2008; Bioucas-Dias, Jose/C-5479-2009; 	Figueiredo, Mario/0000-0002-0970-7745; Bioucas-Dias, Jose/0000-0002-0166-5149			ANDREWS DF, 1974, J ROY STAT SOC B MET, V36, P99; Andrews H., 1977, DIGITAL IMAGE RESTOR; Sendur L, 2002, IEEE SIGNAL PROC LET, V9, P438, DOI 10.1109/LSP.2002.806054; Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010; Schoenberg IJ, 1938, ANN MATH, V39, P811, DOI 10.2307/1968466; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Antoniadis A, 2001, J AM STAT ASSOC, V96, P939, DOI 10.1198/016214501753208942; Neelamani R, 2004, IEEE T SIGNAL PROCES, V52, P418, DOI 10.1109/TSP.2003.821103; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1; Crouse MS, 1998, IEEE T SIGNAL PROCES, V46, P886, DOI 10.1109/78.668544; Chan TF, 1999, SIAM J NUMER ANAL, V36, P354, DOI 10.1137/S0036142997327075; Efron B, 2004, ANN STAT, V32, P407; Portilla J, 2003, IEEE T IMAGE PROCESS, V12, P1338, DOI 10.1109/TIP.2003.818640; Daubechies I, 2004, COMMUN PUR APPL MATH, V57, P1413, DOI 10.1002/cpa.20042; Figueiredo MAT, 2003, IEEE T IMAGE PROCESS, V12, P906, DOI 10.1109/TIP.2003.814255; Hunter DR, 2004, AM STAT, V58, P30, DOI 10.1198/0003130042836; Axelsson O., 1996, ITERATIVE SOLUTION M; Banham MR, 1996, IEEE T IMAGE PROCESS, V5, P619, DOI 10.1109/83.491338; Belge M, 2000, IEEE T IMAGE PROCESS, V9, P597, DOI 10.1109/83.841937; Bioucas-Dias JM, 2006, IEEE T IMAGE PROCESS, V15, P937, DOI 10.1109/TIP.2005.863972; BOHNING D, 1988, ANN I STAT MATH, V40, P641, DOI 10.1007/BF00049423; Brimberg J., 2003, YUGOSLAV J OPER RES, V13, P199, DOI 10.2298/YJOR0302199B; Burrus C. S., 1998, INTRO WAVELETS WAVEL; Byrd R., 1979, 313 J HOPK U; Chaux C., 2006, EUR SIGN PROC C FLOR; Coifman R. R., 1995, WAVELETS STAT, P125; COMBETTES PL, 2005, SIAM J MULTISCALE MO, V4, P1168; DEPIERRO AR, 1995, IEEE T MED IMAGING, V14, P132, DOI 10.1109/42.370409; DONOHO D, 1995, J APPL COMPUT HARMON, V1, P100; Elad M, 2006, IEEE T INFORM THEORY, V52, P5559, DOI 10.1109/TIT.2006.885522; Elad M., 2006, IEEE COMP SOC C COMP; Erdogan H, 1999, IEEE T MED IMAGING, V18, P801, DOI 10.1109/42.802758; FIGUEIREDO M, 2005, IEEE INT C IM PROC G; FIGUEIREDO M, 2002, IEEE INT C AC SPEECH; Figueiredo MAT, 2001, IEEE T IMAGE PROCESS, V10, P1322, DOI 10.1109/83.941856; FUCHS JJ, 2003, P 13 IFAC IFORS S ID, V2, P1357; Fuchs JJ, 2004, IEEE T INFORM THEORY, V50, P1341, DOI 10.1109/TIT.2004.828141; GIROSI F, 1991, 66 MIT CTR BIOL COMP; GUERREROCOLON J, 2006, IEEE INT C IM PROC A; Hiriart-Urruty J.-B., 1993, CONVEX ANAL MINIMIZA; Huber P. J., 1981, ROBUST STAT; JALOBEANU A, 2001, IEEE INT C IM PROC T; Lang M, 1996, IEEE SIGNAL PROC LET, V3, P10, DOI 10.1109/97.475823; LANGE K, 1995, IEEE T IMAGE PROCESS, V4, P1430, DOI 10.1109/83.465107; LIU J, 1998, P IEEE INT C IM PROC, V1, P555; Mallat S., 1998, WAVELET TOUR SIGNAL; Mihcak MK, 1999, IEEE SIGNAL PROC LET, V6, P300; Moulin P, 1999, IEEE T INFORM THEORY, V45, P909, DOI 10.1109/18.761332; Simoncelli E. P., 1996, P IEEE INT C IM PROC, V1, P379, DOI DOI 10.1109/ICIP.1996.559512; Starck JL, 2003, SIGNAL PROCESS, V83, P2279, DOI 10.1016/S0165-1684(03)00150-6; Starck JL, 2003, ASTRON ASTROPHYS, V398, P785, DOI 10.1051/0004-6361:20021571; Weiszfeld E., 1937, TOHOKU MATHEMATICS J, V43, P355; WU CFJ, 1983, ANN STAT, V11, P95, DOI 10.1214/aos/1176346060	53	143	148	1	14	IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC	PISCATAWAY	445 HOES LANE, PISCATAWAY, NJ 08855 USA	1057-7149			IEEE T IMAGE PROCESS	IEEE Trans. Image Process.	DEC	2007	16	12					2980	2991		10.1109/TIP.2007.909318		12	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic	Computer Science; Engineering	236IE	WOS:000251295700011	18092597	
J	Sjostrand, K; Rostrup, E; Ryberg, C; Larsen, R; Studholme, C; Baezner, H; Ferro, J; Fazekas, F; Pantoni, L; Inzitari, D; Waldemar, G				Sjoestrand, Karl; Rostrup, Egill; Ryberg, Charlotte; Larsen, Rasmus; Studholme, Colin; Baezner, Hansjoerg; Ferro, Jose; Fazekas, Franz; Pantoni, Leonardo; Inzitari, Domenico; Waldemar, Gunhild			Sparse decomposition and modeling of anatomical shape variation	IEEE TRANSACTIONS ON MEDICAL IMAGING			English	Article						corpus callosum (CC); decomposition; Leukoaraiosis And DISability in the elderly (LADIS); principal component analysis (PCA); shape analysis; sparse	HUMAN CORPUS-CALLOSUM; SEXUAL-DIMORPHISM; COMPONENT ANALYSIS; MORPHOMETRY; REGRESSION; SELECTION; CLASSIFICATION; SURFACES; LASSO	Recent advances in statistics have spawned powerful methods for regression and data decomposition that promote sparsity, a property that facilitates interpretation of the results. Sparse models use a small subset of the available variables and may perform as well or better than their full counterparts if constructed carefully. In most medical applications, models are required to have both good statistical performance and a relevant clinical interpretation to be of value. Morphometry of the corpus callosum is one illustrative example. This paper presents a method for relating spatial features to clinical outcome data. A set of parsimonious variables is extracted using sparse principal component analysis, producing simple yet characteristic features. The relation of these variables with clinical data is then established using a regression model. The result may be visualized as patterns of anatomical variation related to clinical outcome. In the present application, landmark-based shape data of the corpus callosum is analyzed in relation to age, gender, and clinical tests of walking speed and verbal fluency. To put the data-driven sparse principal component method into perspective, we consider two alternative techniques, one where features are derived using a model-based wavelet approach, and one where the original variables are regressed directly on the outcome.	Tech Univ Denmark, Dept Informat & Math Modelling, DK-2800 Lyngby, Denmark; Univ Copenhagen Hosp, Danish Res Ctr Magnet Resonance, DK-2650 Hvidovre, Denmark; Univ Copenhagen Hosp, Dept Neurol, Memory Disorders Res Grp, DK-2100 Copenhagen, Denmark; Tech Univ Denmark, Dept Informat & Math Modelling, DK-2800 Lyngby, Denmark; Univ Calif San Francisco, Dept Radiol, San Francisco, CA 94143 USA; Univ Heidelberg, Klinikum Mannheim, Dept Neurol, D-68167 Mannheim, Germany; Hosp Santa Maria, Ctr Estudos Egas Moniz, Neurol Serv, P-1649 Lisbon, Portugal; Med Univ, Dept Neurol, A-8036 Graz, Austria; Univ Florence, Dept Neurol & Psychiat Sci, I-50139 Florence, Italy; Copenhagen Univ Hosp, Dept Neurol, Mem Disorders Res Grp, DK-2200 Copenhagen, Denmark	Sjostrand, K (reprint author), Tech Univ Denmark, Dept Informat & Math Modelling, DK-2800 Lyngby, Denmark.	kas@imm.dtu.dk		Ferro, Jose/0000-0002-2343-9097			ALLEN LS, 1991, J NEUROSCI, V11, P933; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Dubb A, 2003, NEUROIMAGE, V20, P512, DOI 10.1016/S1053-8119(03)00313-6; Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467; HOERL AE, 1970, TECHNOMETRICS, V12, P55; Jolliffe IT, 2003, J COMPUT GRAPH STAT, V12, P531, DOI 10.1198/1061860032148; Ashburner J, 1999, HUM BRAIN MAPP, V7, P254, DOI 10.1002/(SICI)1097-0193(1999)7:4<254::AID-HBM4>3.0.CO;2-G; Zou H, 2005, J ROY STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x; Efron B, 2004, ANN STAT, V32, P407; Stegmann MB, 2003, IEEE T MED IMAGING, V22, P1319, DOI 10.1109/TMI.2003.817780; Bermudez P, 2001, NEUROIMAGE, V13, P1121, DOI 10.1006/nimg.2001.0772; BOOKSTEIN FL, 1979, COMPUT VISION GRAPH, V11, P123, DOI 10.1016/0146-664X(79)90062-5; Bookstein FL, 1996, PROCEEDINGS OF THE IEEE WORKSHOP ON MATHEMATICAL METHODS IN BIOMEDICAL IMAGE ANALYSIS, P2, DOI 10.1109/MMBIA.1996.534052; Bookstein F L, 1997, Med Image Anal, V1, P225, DOI 10.1016/S1361-8415(97)85012-8; BORGEFORS G, 1986, COMPUT VISION GRAPH, V34, P344, DOI 10.1016/S0734-189X(86)80047-0; BRECHBUHLER C, 1995, COMPUT VIS IMAGE UND, V61, P154, DOI 10.1006/cviu.1995.1013; Chennubhotla C, 2001, P IEEE INT C COMP VI, P641, DOI 10.1109/ICCV.2001.937579; CLARKE S, 1989, J COMP NEUROL, V280, P213, DOI 10.1002/cne.902800205; COOTES TF, 1995, COMPUT VIS IMAGE UND, V61, P38, DOI 10.1006/cviu.1995.1004; DASPREMONT A, 2005, DIRECT FORMULATING S, V17, P41; Daubechies I., 1993, B AM MATH SOC, V28, P350; Davatzikos C, 2003, IEEE T MED IMAGING, V22, P414, DOI 10.1109/TMI.2003.809688; Davatzikos C, 1996, J COMPUT ASSIST TOMO, V20, P88, DOI 10.1097/00004728-199601000-00017; DAVIES R, P INF PROC MED IM IP, V18, P38; Davison A. C., 2003, BOOTSTRAP METHODS TH; Dryden IL, 1999, STAT SHAPE ANAL; Fan Y, 2007, IEEE T MED IMAGING, V26, P93, DOI 10.1109/TMI.2006.886812; Golland P, 2001, LECT NOTES COMPUTER, P517; Golland P., 2000, P IEEE C COMP VIS PA, V1, P10, DOI 10.1109/CVPR.2000.855792; Golland P, 2005, MED IMAGE ANAL, V9, P69, DOI 10.1016/j.media.2004.07.003; Golub G., 1996, MATRIX COMPUTATIONS; Hastie T., 2001, ELEMENTS STAT LEARNI; HAUSMAN R, 1982, STUDIES MANAGEMENT S, P137; JOKINEN H, 2006, J NEUROL NEUROSURGER; Joshi S, 2002, IEEE T MED IMAGING, V21, P538, DOI 10.1109/TMI.2002.1009389; Kelemen A., 1998, Proceedings. Workshop on Biomedical Image Analysis (Cat. No.98EX162), DOI 10.1109/BIA.1998.692374; KENDALL DG, 1977, ADV APPL PROBAB, V9, P428, DOI 10.2307/1426091; LEBRIQUER L, 1997, P IMPI 97, P477; LEVENTON ME, 2000, P C COMP VIS PATT RE, V1, P316, DOI 10.1109/CVPR.2000.855835; Machado AMC, 2004, ARTIF INTELL MED, V30, P97, DOI 10.1016/S0933-3657(03)00039-3; Machado AMC, 1998, P SOC PHOTO-OPT INS, V3338, P642, DOI 10.1117/12.310942; Moghaddam B., 2006, ADV NEURAL INFORM PR, V18, P915; Nichols T.E., 2001, HUM BRAIN MAPP, V15, P1, DOI DOI 10.1002/HBM.1058; Pantoni L, 2005, NEUROEPIDEMIOLOGY, V24, P51, DOI 10.1159/000081050; Peterson BS, 2001, HUM BRAIN MAPP, V12, P232; Reissell LM, 1996, GRAPH MODEL IM PROC, V58, P198, DOI 10.1006/gmip.1996.0017; Rousson V, 2004, J ROY STAT SOC C-APP, V53, P539, DOI 10.1111/j.1467-9876.2004.05359.x; RYBERG C, 2006, NEUROBIOL AGING; SJOSTRAND K, 2006, P INT SOC MAGN RES M; SJOSTRAND K, 2006, P INT S MED IM SAN D, V6144; STAIB LH, 1992, IEEE T PATTERN ANAL, V14, P1061, DOI 10.1109/34.166621; STEGMANN M, 2006, P INT S MED IM; STOECKEL J, 2005, P 5 IEEE INT C DAT M; Studholme C, 2004, NEUROIMAGE, V21, P1387, DOI 10.1016/j.neuroimage.2003.12.009; UZUMEU M, 2003, P INT S MED IM, V5032; Vines SK, 2000, J ROY STAT SOC C-APP, V49, P441, DOI 10.1111/1467-9876.00204; WITELSON SF, 1989, BRAIN, V112, P799, DOI 10.1093/brain/112.3.799; Yushkevich P, 2003, LECT NOTES COMPUT SC, V2732, P114; Zaidel E., 2003, PARALLEL BRAIN COGNI; Zou H, 2006, J COMPUT GRAPH STAT, V15, P265, DOI 10.1198/106186006X113430	60	17	17	1	12	IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC	PISCATAWAY	445 HOES LANE, PISCATAWAY, NJ 08855 USA	0278-0062			IEEE T MED IMAGING	IEEE Trans. Med. Imaging	DEC	2007	26	12					1625	1635		10.1109/TMI.2007.898808		11	Computer Science, Interdisciplinary Applications; Engineering, Biomedical; Engineering, Electrical & Electronic; Imaging Science & Photographic Technology; Radiology, Nuclear Medicine & Medical Imaging	Computer Science; Engineering; Imaging Science & Photographic Technology; Radiology, Nuclear Medicine & Medical Imaging	237LX	WOS:000251376500002	18092733	
J	Fornasier, M				Fornasier, Massimo			Domain decomposition methods for linear inverse problems with sparsity constraints	INVERSE PROBLEMS			English	Article							OPERATOR-EQUATIONS; VARIATIONAL-PROBLEMS; IMAGE-RESTORATION; ITERATIVE METHODS; SIGNAL RECOVERY; CONVERGENCE; PROJECTIONS; REGRESSION; SHRINKAGE; ALGORITHM	Quantities of interest appearing in concrete applications often possess sparse expansions with respect to a preassigned frame. Recently, there were introduced sparsity measures which are typically constructed on the basis of weighted l(1) norms of frame coefficients. One can model the reconstruction of a sparse vector from noisy linear measurements as the minimization of the functional defined by the sum of the discrepancy with respect to the data and the weighted l(1)-norm of suitable frame coefficients. Thresholded Landweber iterations were proposed for the solution of the variational problem. Despite its simplicity which makes it very attractive to users, this algorithm converges slowly. In this paper, we investigate methods to accelerate significantly the convergence. We introduce and analyze sequential and parallel iterative algorithms based on alternating subspace corrections for the solution of the linear inverse problem with sparsity constraints. We prove their norm convergence to minimizers of the functional. We compare the computational cost and the behavior of these new algorithms with respect to the thresholded Landweber iterations.	[Fornasier, Massimo] Princeton Univ, Program Appl & Computat Math, Princeton, NJ 08544 USA	Fornasier, M (reprint author), Princeton Univ, Program Appl & Computat Math, Fine Hall,Washington Rd, Princeton, NJ 08544 USA.	mfornasi@math.princeton.edu					OPIAL Z, 1967, B AM MATH SOC, V73, P591, DOI 10.1090/S0002-9904-1967-11761-0; Cohen A, 2002, FOUND COMPUT MATH, V2, P203, DOI 10.1007/s102080010027; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Chambolle A, 1998, IEEE T IMAGE PROCESS, V7, P319; Candes EJ, 2006, IEEE T INFORM THEORY, V52, P489, DOI 10.1109/TIT.2005.862083; Candes EJ, 2006, IEEE T INFORM THEORY, V52, P5406, DOI 10.1109/TIT.2006.885507; Chambolle A, 1997, NUMER MATH, V76, P167, DOI 10.1007/s002110050258; Cohen A, 2003, SIAM J NUMER ANAL, V41, P1785, DOI 10.1137/S0036142902412269; Combettes PL, 2005, MULTISCALE MODEL SIM, V4, P1168, DOI 10.1137/050626090; Efron B, 2004, ANN STAT, V32, P407; Daubechies I, 2004, COMMUN PUR APPL MATH, V57, P1413, DOI 10.1002/cpa.20042; Figueiredo MAT, 2003, IEEE T IMAGE PROCESS, V12, P906, DOI 10.1109/TIP.2003.814255; Bauschke HH, 2003, T AM MATH SOC, V355, P3433, DOI 10.1090/S0002-9947-03-03136-2; BRAMBLE JH, 1991, MATH COMPUT, V57, P1; Chan T., 1994, ACTA NUMER, V3, P61, DOI 10.1017/S0962492900002427; Christensen O., 2003, INTRO FRAMES RIESZ B; Cohen LD, 1996, J MATH IMAGING VIS, V6, P59, DOI 10.1007/BF00127375; DAHLKE S, 2007, IMA J NUMER ANAL; Dahlke S, 2007, ADV COMPUT MATH, V27, P27, DOI 10.1007/s10444-005-7501-6; DAUBECHIES I, 2007, IN PRESS F FOURIER A; Daubechies I, 2007, INVERSE PROBL IMAG, V1, P29; Daubechies I, 2005, APPL COMPUT HARMON A, V19, P1, DOI 10.1016/j.acha.2004.12.004; Ekeland I., 1999, CONVEX ANAL VARIATIO; Elad M, 2005, APPL COMPUT HARMON A, V19, P340, DOI 10.1016/j.acha.2005.03.005; Engl H. W., 1996, REGULARIZATION INVER, V375; FORNASIER M, 2007, IN PRESS SIAM J NUME; FORNASIER M, 2007, ITERATIVE THRESHOLDI; FORNASIER M, 2007, IN PRESS J COMPUT AP; FORNASIER M, 2007, IN PRESS SIAM J APPL; LANGER U, LECT NOTES DOMAIN DE; LEE YJ, 2003, 14 INT C DOM DEC MET, P315; LIONS PL, 1988, P 1 INT S DOM DEC ME; Loris I, 2007, GEOPHYS J INT, V170, P359, DOI 10.1111/j.1365-246X.2007.03409.x; NABBEN R, 2005, 051103 TEMPL U DEP M; Quarteroni A., 1999, NUMERICAL MATH SCI C; Ramlau R, 2005, INVERSE PROBL, V21, P1571, DOI 10.1088/0266-5611/21/5/005; Ramlau R, 2006, NUMER MATH, V104, P177, DOI 10.1007/s00211-006-0016-3; Rockafellar R. T., 1998, GRUNDLEHREN MATH WIS, V317; Starck JL, 2003, SIGNAL PROCESS, V83, P2279, DOI 10.1016/S0165-1684(03)00150-6; Starck JL, 2003, ASTRON ASTROPHYS, V398, P785, DOI 10.1051/0004-6361:20021571; Stevenson R, 2003, SIAM J NUMER ANAL, V41, P1074, DOI 10.1137/S0036142902407988; Teschke G, 2007, APPL COMPUT HARMON A, V22, P43, DOI 10.1016/j.acha.2006.05.003; TROPP JA, 2007, IN PRESS APPL COMPUT; XU J, 2000, AM223 PENN STAT U DE; XU JC, 1992, SIAM REV, V34, P581, DOI 10.1137/1034116	46	27	28	0	0	IOP PUBLISHING LTD	BRISTOL	DIRAC HOUSE, TEMPLE BACK, BRISTOL BS1 6BE, ENGLAND	0266-5611			INVERSE PROBL	Inverse Probl.	DEC	2007	23	6					2505	2526		10.1088/0266-5611/23/6/014		22	Mathematics, Applied; Physics, Mathematical	Mathematics; Physics	243DC	WOS:000251774100023		
J	Agarwal, V; Gribok, AV; Abidi, MA				Agarwal, Vivek; Gribok, Andrei V.; Abidi, Mongi A.			Image restoration using L-1 norm penalty function	INVERSE PROBLEMS IN SCIENCE AND ENGINEERING			English	Article						image restoration; L-1 norm penalty functions; total variation regularization; LASSO regularization; adaptive ridge regression	ILL-POSED PROBLEMS	The process of estimating in original image from a given blurred and noisy image is known its image restoration. It is an ill-posed inverse problem, since one of the ways of solving it requires finding a solution to a Fredholm integral equation of convolution type in two-dimensional space. The focus of the article is to achieve it quality edge preserving image restoration using it less expensive (Fast) regularization technique with L-1 norm penalty function. L, norm based approaches do not penalize edges or high frequency contents in the restored image compared to L-2 norm based approaches. Total variation (TV) is ail established L-1 norm regularization approach that performs edge preserving image restoration, but at a high computational cost. TV regularization requires linearization of it highly nonlinear penalty term, which increases the restoration time considerably for large scale images. In order to reduce the computational cost, we extend least absolute shrinkage and selection operator (LASSO), ail L-1 norm minimization statistical modeling technique to image restoration. The penalty function of LASSO is in identify matrix so it is computationally fast. The metrics, like, residual error, peak signal to noise ratio (PSNR), restoration time, edge map of the restored image, and subjective visual evaluation are used to assess the performances of both methods. Based on our experimental results, we show that LASSO achieves similar quality of edge preserving restoration as TV regularization, and is approximately two times faster in computation compared to TV regularization oil the same set of images. We also analyze the impact of the different degree of blurring caused by point spread functions (PSFs) corrupted by different signal to noise ratios (SNRs) oil image restoration.	[Agarwal, Vivek; Gribok, Andrei V.; Abidi, Mongi A.] Univ Tennessee, IRIS Lab, Knoxville, TN 37996 USA	Agarwal, V (reprint author), Purdue Univ, AISL, 400 Cent Dr, W Lafayette, IN 47907 USA.	agarwall@purdue.edu		Agarwal, Vivek/0000-0003-1334-0509			RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Vogel CR, 1996, SIAM J SCI COMPUT, V17, P227, DOI 10.1137/0917016; FRANK IE, 1993, TECHNOMETRICS, V35, P109, DOI 10.2307/1269656; ACAR R, 1994, INVERSE PROBL, V10, P1217, DOI 10.1088/0266-5611/10/6/003; BIOUCASDAS J, 2006, P IEEE INT C AC SPEE; BREIMAN L, 1993, BETTER SUBSER SELECT; Chan R., 1995, P INT SOC PHOT INSTR, P314; Chan TF, 1998, SIAM J SCI COMPUT, V20, P1964; Engl H., 1996, REGULARIZATION INVER; Engl HW, 1993, SURV MATH IND, V3, P71; Figueiredo MAT, 2003, IEEE T PATTERN ANAL, V25, P1150, DOI 10.1109/TPAMI.2003.1227989; Gonzalez R. C., 2002, DIGITAL IMAGE PROCES; GRANDVALET Y, 1998, NEURAL INFORM PROCES, V11, P445; Grandvalet Y., 1998, LEAST ABSOLUTE SHRIN, P201; Groetsch C., 1993, INVERSE PROBLEMS MAT; GUO W, 1999, IEEE T SIGNAL PROCES, V6, P165; Hadamard J., 1923, LECT CAUCHYS PROBLEM; Hanke M., 1993, Surveys on Mathematics for Industry, V3; Hansen P., 1998, RANK DEFICIENT DISCR; IDIER J, 1999, P 3 INT C INV PROBL, P23; Banham MR, 1997, IEEE SIGNAL PROC MAG, V14, P24, DOI 10.1109/79.581363; Koenker R, 2004, J ROY STAT SOC B, V66, P145, DOI 10.1111/j.1467-9868.2004.00437.x; Molina R, 2003, IEEE T IMAGE PROCESS, V12, P1642, DOI 10.1109/TIP.2003.818015; NASHED MZ, 1981, IEEE T ANTENN PROPAG, V29, P220, DOI 10.1109/TAP.1981.1142564; Tikhonov AN, 1977, SOLUTION ILL POSED P; VOGEL CR, 1995, NONSMOOTH REGULARIZA; Vogel C.R., 2002, COMPUTATIONAL METHOD; YANG YY, 1995, IEEE T IMAGE PROCESS, V4, P896, DOI 10.1109/83.392332	29	4	4	0	1	TAYLOR & FRANCIS LTD	ABINGDON	4 PARK SQUARE, MILTON PARK, ABINGDON OX14 4RN, OXON, ENGLAND	1741-5977			INVERSE PROBL SCI EN	Inverse Probl. Sci. Eng.	DEC	2007	15	8					785	809		10.1080/17415970600971987		25	Engineering, Multidisciplinary; Mathematics, Interdisciplinary Applications	Engineering; Mathematics	255PG	WOS:000252669300002		
J	Goddard, ME; Hayes, BJ				Goddard, M. E.; Hayes, B. J.			Genomic selection	JOURNAL OF ANIMAL BREEDING AND GENETICS			English	Review						genomic selection; linkage disequilibrium; prior distribution	QUANTITATIVE TRAIT LOCI; MARKER-ASSISTED SELECTION; EFFECTIVE POPULATION-SIZE; LINKAGE; INFORMATION; PREDICTION	Genomic selection is a form of marker-assisted selection in which genetic markers covering the whole genome are used so that all quantitative trait loci (QTL) are in linkage disequilibrium with at least one marker. This approach has become feasible thanks to the large number of single nucleotide polymorphisms (SNP) discovered by genome sequencing and new methods to efficiently genotype large number of SNP. Simulation results and limited experimental results suggest that breeding values can be predicted with high accuracy using genetic markers alone but more validation is required especially in samples of the population different from that in which the effect of the markers was estimated. The ideal method to estimate the breeding value from genomic data is to calculate the conditional mean of the breeding value given the genotype of the animal at each QTL. This conditional mean can only be calculated by using a prior distribution of QTL effects so this should be part of the research carried out to implement genomic selection. In practice, this method of estimating breeding values is approximated by using the marker genotypes instead of the QTL genotypes but the ideal method is likely to be approached more closely as more sequence and SNP data is obtained. Implementation of genomic selection is likely to have major implications for genetic evaluation systems and for genetic improvement programmes generally and these are discussed.	Dept Primary Ind, Attwood, Vic 3049, Australia; Univ Melbourne, Attwood, Vic, Australia	Goddard, ME (reprint author), Dept Primary Ind, 475 Mickleham Rd, Attwood, Vic 3049, Australia.	mike.goddard@dpi.vic.gov.au					Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Parkes M, 2007, NAT GENET, V39, P830, DOI 10.1038/ng2061; Meuwissen THE, 2007, GENETICS, V176, P2551, DOI 10.1534/genetics.107.070953; Meuwissen THE, 2001, GENETICS, V157, P1819; Weller JI, 2005, GENET SEL EVOL, V37, P501, DOI 10.1051/gse:2005013; Armstrong DT, 1997, REPROD FERT DEVELOP, V9, P333, DOI 10.1071/R96080; Beavis WD, 1994, MOL DISSECTION COMPL, P145; BOICHARD D, 2006, 8 WORLD C GEN APPL L; CALUS MPL, 2008, IN PRESS GENETICS; CHAMBERLAIN AJ, 2007, GENETICS; Dekkers J C M, 2004, J Anim Sci, V82 E-Suppl, pE313; GODDARD ME, 2002, P 7 WORLD C GEN APPL, V33, P3; GODDARD ME, 1991, GENET SEL EVOL S1, V23, P1315; GODDARD ME, 1992, J DAIRY SCI, V75, P2902, DOI 10.3168/jds.S0022-0302(92)78052-7; GODDARD ME, 1998, P 6 WORLD C GEN APPL, V26, P33; GODDARD ME, 2006, 8 WORLD C GEN APPL L, V8, P22; Grapes L, 2004, GENETICS, V166, P1561, DOI 10.1534/genetics.166.3.1561; HAYES BJ, 2006, P 8 WORLD C GEN APPL, V8, P30; HAYES BJ, 2007, IN PRESS GENOME RES; MACLEOD IM, IN PRESS GENETICS; Meuwissen THE, 1999, GENET SEL EVOL, V31, P375, DOI 10.1051/gse:19990405; Meuwissen THE, 2002, GENETICS, V161, P373; SCHAEFFER LR, 2006, ANIM BREED GENET, V123, P218; Villanueva B, 2005, J ANIM SCI, V83, P1747; Whittaker JC, 1997, GENET RES, V69, P137, DOI 10.1017/S0016672397002711; Wray NR, 2007, GENOME RES, V17, P1520, DOI 10.1101/gr.6665407; Xu SZ, 2007, GENETICS, V175, P1955, DOI 10.1534/genetics.106.066571; Xu SZ, 2003, GENETICS, V163, P789; Zhao HH, 2007, GENETICS, V175, P1975, DOI 10.1534/genetics.106.066480	29	158	171	12	53	BLACKWELL PUBLISHING	OXFORD	9600 GARSINGTON RD, OXFORD OX4 2DQ, OXON, ENGLAND	0931-2668			J ANIM BREED GENET	J. Anim. Breed. Genet.	DEC	2007	124	6					323	330				8	Agriculture, Dairy & Animal Science	Agriculture	238FN	WOS:000251432800002	18076469	
J	Long, N; Gianola, D; Rosa, GJM; Weigel, KA; Avendano, S				Long, N.; Gianola, D.; Rosa, G. J. M.; Weigel, K. A.; Avendano, S.			Machine learning classification procedure for selecting SNPs in genomic selection: application to early mortality in broilers	JOURNAL OF ANIMAL BREEDING AND GENETICS			English	Article						filter-wrapper feature selection; genomic selection; machine learning; mortality; single nucleotide polymorphism	GENETIC VALUE; ASSISTED PREDICTION; ASSOCIATION; WRAPPERS; MODEL	Genome-wide association studies using single nucleotide polymorphisms (SNPs) can identify genetic variants related to complex traits. Typically thousands of SNPs are genotyped, whereas the number of phenotypes for which there is genomic information may be smaller. When predicting phenotypes, options for statistical model building range from incorporating all possible markers into the specification to including only sets of relevant SNPs (features). In the latter case, an efficient method of selecting influential features is required. A two-step feature selection method for binary traits was developed, which consisted of filtering (using information gain), and wrapping (using naive Bayesian classification). The filter reduces the large number of SNPs to a much smaller size, to facilitate the wrapper step. As the procedure is tailored for discrete outcomes, an approach based on discretization of phenotypic values was developed, to enable feature selection in a classification framework. The method was applied to chick mortality rates (0-14 days of age) on progeny from 201 sires in a commercial broiler line, with the goal of identifying SNPs (over 5000) related to progeny mortality. To mimic a case-control study, sires were clustered into two groups, low and high, according to two arbitrarily chosen mortality rate cut points. By varying these thresholds, 11 different 'case-control' samples were formed, and the SNP selection procedure was applied to each sample. To compare the 11 sets of chosen SNPs, predicted residual sum of squares (PRESS) from a linear model was used. The two-step method improved naive Bayesian classification accuracy over the case without feature selection (from around 50 to above 90% without and with feature selection in each case-control sample). The best case-control group (63 sires above or below the thresholds) had the smallest PRESS statistic among groups with model p-values below 0.003. The 17 SNPs selected using this group accounted for 31% of the variation in raw mortality rates between sire families.	Univ Wisconsin, Dept Anim Sci, Madison, WI 53706 USA; Univ Wisconsin, Dept Dairy Sci, Madison, WI 53706 USA; Aviagen Ltd, Newbridge EH28 8SZ, Midlothian, Scotland	Long, N (reprint author), Univ Wisconsin, Dept Anim Sci, Madison, WI 53706 USA.	nlong@wisc.edu	Rosa, Guilherme/G-3862-2011				Agresti A., 2002, CATEGORICAL DATA ANA; STRAM DO, 1994, BIOMETRICS, V50, P1171, DOI 10.2307/2533455; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Balding DJ, 2006, NAT REV GENET, V7, P781, DOI 10.1038/nrg1916; Hoh J, 2003, NAT REV GENET, V4, P701, DOI 10.1038/nrg1155; Meuwissen THE, 2001, GENETICS, V157, P1819; Kohavi R, 1997, ARTIF INTELL, V97, P273, DOI 10.1016/S0004-3702(97)00043-X; CARUANA R, 1994, P 11 INT MACH LEARN; Collett D., 1991, MODELLING BINARY DAT; Dekkers JCM, 2002, NAT REV GENET, V3, P22, DOI 10.1038/nrg701; Elkan C., 1997, BOOSTING NAIVE BAYES; Gianola D, 2006, GENETICS, V173, P1761, DOI 10.1534/genetics.105.049510; Gianola D, 2003, GENETICS, V163, P347; Guyon I., 2003, Journal of Machine Learning Research, V3, DOI 10.1162/153244303322753616; HARTL DL, 2005, GENETIC ANAL GENES G; HENDERSON CR, 1953, BIOMETRICS, V9, P226, DOI 10.2307/3001853; Hoh J, 2000, ANN HUM GENET, V64, P413, DOI 10.1046/j.1469-1809.2000.6450413.x; HOLMES G, 1995, FEATURE SELECTION VI; Kelemen A, 2003, IEEE IJCNN, P1769; Liu HQ, 2005, BIOINFORMATICS, V21, P3377, DOI 10.1093/bioinformatics/bti44; Mackay D. J. C., 2003, INFORM THEORY INFERE; Mitchell T., 1997, MACHINE LEARNING; NAKAMICHI R, 2004, 4 IEEE S BIOIN BIOEN, P73; RIPLEY BD, 1994, J ROY STAT SOC B MET, V56, P409; Ruppert D., 2003, SEMIPARAMETRIC REGRE; Russell S., 2002, ARTIFICIAL INTELLIGE; Sorensen D., 2002, LIKELIHOOD BAYESIAN; TSALENKO A, 2005, IEEE COMP SYST BIOIN, P135; Tsalenko Anya, 2003, Pac Symp Biocomput, P548; Witten I., 2005, DATA MINING PRACTICA; Xiong MM, 2001, GENOME RES, V11, P1878; Ye X, 2006, POULTRY SCI, V85, P1555; Yu L., 2004, P 10 ACM SIGKDD INT, P737, DOI 10.1145/1014052.1014149	33	48	52	0	7	WILEY-BLACKWELL	MALDEN	COMMERCE PLACE, 350 MAIN ST, MALDEN 02148, MA USA	0931-2668			J ANIM BREED GENET	J. Anim. Breed. Genet.	DEC	2007	124	6					377	389				13	Agriculture, Dairy & Animal Science	Agriculture	238FN	WOS:000251432800008	18076475	
J	Zhang, CH				Zhang, Cun-Hui			Continuous generalized gradient descent	JOURNAL OF COMPUTATIONAL AND GRAPHICAL STATISTICS			English	Article						classification; matrix differential equation; regression; regularized optimization	LOGISTIC-REGRESSION; SELECTION; MACHINE; LASSO	This article derives characterizations and computational algorithms for continuous general gradient descent trajectories in high-dimensional parameter spaces for statistical model selection, prediction, and classification. Examples include proportional gradient shrinkage as an extension of LASSO and LARS, threshold gradient descent with right-continuous variable selectors, threshold ridge regression, and many more with proper combinations of variable selectors and functional forms of a kernel. In all these problems, general gradient descent trajectories are continuous piecewise analytic vector-valued curves as solutions to matrix differential equations. We show the monotonicity and convergence of the proposed algorithms in the loss or negative likelihood functions. We prove that approximations of continuous solutions via infinite series expansions are computationally more efficient and accurate compared with discretization methods. We demonstrate the applicability of our algorithms through numerical experiments with real and simulated datasets.	Rutgers State Univ, Dept Stat, Piscataway, NJ 08854 USA	Zhang, CH (reprint author), Rutgers State Univ, Dept Stat, Busch Campus, Piscataway, NJ 08854 USA.	czhang@stat.rutgers.edu					Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451; Friedman J, 2000, ANN STAT, V28, P337, DOI 10.1214/aos/1016218223; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Osborne MR, 2000, IMA J NUMER ANAL, V20, P389, DOI 10.1093/imanum/20.3.389; HOERL AE, 1970, TECHNOMETRICS, V12, P55; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Efron B, 2004, ANN STAT, V32, P407; Genkin A, 2007, TECHNOMETRICS, V49, P291, DOI 10.1198/004017007000000245; Breiman L, 1996, ANN STAT, V24, P2350; Freund Y, 1996, EXPT NEW BOOSTING AL, P148; Friedman JH, 2004, GRADIENT DIRECTED RE; Hastie T, 2004, J MACH LEARN RES, V5, P1391; Osborne MR, 2000, J COMPUT GRAPH STAT, V9, P319, DOI 10.2307/1390657; PARK MY, 2007, J ROYAL STAT SOC B, P659; ROSSET S, 2004, TRACKING CURVED REGU; Rosset S, 2007, ANN STAT, V35, P1012, DOI 10.1214/009053606000001370; SCHAPIRE RE, 1990, MACH LEARN, V5, P197, DOI 10.1023/A:1022648800760; Vapnik V.N., 1996, NATURE STAT LEARNING; Wahba G, 2000, ADV NEUR IN, P297; Zhao P., 2004, BOOSTED LASSO	20	1	1	1	3	AMER STATISTICAL ASSOC	ALEXANDRIA	1429 DUKE ST, ALEXANDRIA, VA 22314 USA	1061-8600			J COMPUT GRAPH STAT	J. Comput. Graph. Stat.	DEC	2007	16	4					761	781		10.1198/106186007X238846		21	Statistics & Probability	Mathematics	246MG	WOS:000252010500001		
J	Liu, YF; Wu, YC				Liu, Yufeng; Wu, Yichao			Variable selection via a combination of the L-0 and L-1 penalties	JOURNAL OF COMPUTATIONAL AND GRAPHICAL STATISTICS			English	Article						mixed integer programming; regression; regularization; SVM; variable selection	REGRESSION; ASYMPTOTICS; SHRINKAGE; LASSO; TOOLS	Variable selection is an important aspect of high-dimensional statistical modeling, particularly in regression and classification. In the regularization framework, various penalty functions are used to perform variable selection by putting relatively large penalties on small coefficients. The L-1 penalty is a popular choice because of its convexity, but it produces biased estimates for the large coefficients. The L-0 penalty is attractive for variable selection because it directly penalizes the number of nonzero coefficients. However, the optimization involved is discontinuous and nonconvex, and therefore it is very challenging to implement. Moreover, its solution may not be stable. In this article, we propose a new penalty that combines the L-0 and L-1 penalties. We implement this new penalty by developing a global optimization algorithm using mixed integer programming (MIP). We compare this combined penalty with several other penalties via simulated examples as well as real applications. The results show that the new penalty outperforms both the L-0 and L-1 penalties in terms of variable selection while maintaining good prediction accuracy.	[Liu, Yufeng] Univ N Carolina, Dept Stat & Operat Res, Carolina Ctr Genome Sci, Chapel Hill, NC 27599 USA; [Wu, Yichao] Princeton Univ, Dept Operat Res & Financial Engn, Princeton, NJ 08544 USA	Liu, YF (reprint author), Univ N Carolina, Dept Stat & Operat Res, Carolina Ctr Genome Sci, Chapel Hill, NC 27599 USA.	yfliu@email.unc.edu; yichaowu@princeton.edu					Knight K, 2000, ANN STAT, V28, P1356; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; Liu YF, 2005, J COMPUT GRAPH STAT, V14, P219, DOI 10.1198/106186005X37238; HOERL AE, 1970, TECHNOMETRICS, V12, P55; FRANK IE, 1993, TECHNOMETRICS, V35, P109, DOI 10.2307/1269656; KIM JY, 1990, ANN STAT, V18, P191, DOI 10.1214/aos/1176347498; DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425; Bradley Paul S., 1998, ICML 98; CRISTANINI N, 1999, INTRO SUPPORT VECTOR; DONOHO DL, 1995, J ROYAL STAT SOC B, V35, P109; Garfinkel R.S., 1972, INTEGER PROGRAMMING; Kernighan B., 2003, AMPL MODELING LANGUA; Liu YF, 2006, STAT SINICA, V16, P441; Nemhauser G. L., 1999, INTEGER COMBINATORIA; SIGILLITO VG, 1989, J HOPKINS APL TECH D, V10, P262; Vandenberghe L., 2004, CONVEX OPTIMIZATION; Vapnik V., 1998, STAT LEARNING THEORY; Zhu J., 2003, NEURAL INFORM PROCES, P16	19	22	24	1	4	AMER STATISTICAL ASSOC	ALEXANDRIA	1429 DUKE ST, ALEXANDRIA, VA 22314 USA	1061-8600			J COMPUT GRAPH STAT	J. Comput. Graph. Stat.	DEC	2007	16	4					782	798		10.1198/106186007X255676		17	Statistics & Probability	Mathematics	246MG	WOS:000252010500002		
J	Schmidler, SC; Lucas, JE; Oas, TG				Schmidler, Scott C. y; Lucas, Joseph E.; Oas, Terrence G.			Statistical estimation of statistical mechanical models: Helix-coil theory and peptide helicity prediction	JOURNAL OF COMPUTATIONAL BIOLOGY			English	Article						Bayesian methods; empirical potentials; helical peptides; helix-coil theory; protein folding	SIDE-CHAIN INTERACTIONS; PROTEIN SECONDARY STRUCTURE; ALANINE-BASED PEPTIDES; ALPHA-HELICES; FOLDING PROBLEM; EMPIRICAL PARAMETERS; TRANSITION THEORY; AMINO-ACIDS; HYDROGEN-BONDS; SEQUENCE	Analysis of biopolymer sequences and structures generally adopts one of two approaches: use of detailed biophysical theoretical models of the system with experimentally-determined parameters, or largely empirical statistical models obtained by extracting parameters from large datasets. In this work, we demonstrate a merger of these two approaches using Bayesian statistics. We adopt a common biophysical model for local protein folding and peptide configuration, the helix-coil model. The parameters of this model are estimated by statistical fitting to a large dataset, using prior distributions based on experimental data. L-1-norm shrinkage priors are applied to induce sparsity among the estimated parameters, resulting in a significantly simplified model. Formal statistical procedures for evaluating support in the data for previously proposed model extensions are presented. We demonstrate the advantages of this approach including improved prediction accuracy and quantification of prediction uncertainty, and discuss opportunities for statistical design of experiments. Our approach yields a 39% improvement in mean-squared predictive error over the current best algorithm for this problem. In the process we also provide an efficient recursive algorithm for exact calculation of ensemble helicity including sidechain interactions, and derive an explicit relation between homo- and heteropolymer helix-coil theories and Markov chains and (non-standard) hidden Markov models respectively, which has not appeared in the literature previously.	[Schmidler, Scott C. y; Lucas, Joseph E.] Duke Univ, Inst Stat & Decis Sci, Durham, NC 27706 USA; [Oas, Terrence G.] Duke Univ, Med Ctr, Dept Biochem, Durham, NC USA	Schmidler, SC (reprint author), Duke Univ, Inst Stat & Decis Sci, 223 Old Chem Bldg, Box 90251, Durham, NC 27706 USA.	schmidler@stat.duke.edu	Oas, Terrence/F-8916-2011	Oas, Terrence/0000-0002-3067-2743			Andersen NH, 1997, PROTEIN SCI, V6, P1920; ANDREWS DF, 1974, J ROY STAT SOC B MET, V36, P99; Munoz V, 1997, BIOPOLYMERS, V41, P495, DOI 10.1002/(SICI)1097-0282(19970415)41:5<495::AID-BIP2>3.0.CO;2-H; Aurora R, 1998, PROTEIN SCI, V7, P21; SCHOLTZ JM, 1993, BIOCHEMISTRY-US, V32, P9668, DOI 10.1021/bi00088a019; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Thomas A, 2002, PROTEINS, V48, P635, DOI 10.1002/prot.10191; WILSON C, 1989, PROTEINS, V6, P193, DOI 10.1002/prot.340060208; ROST B, 1993, P NATL ACAD SCI USA, V90, P7558, DOI 10.1073/pnas.90.16.7558; Simons KT, 1997, J MOL BIOL, V268, P209, DOI 10.1006/jmbi.1997.0959; Tipping ME, 2001, J MACH LEARN RES, V1, P211, DOI 10.1162/15324430152748236; ZIMM BH, 1959, J CHEM PHYS, V31, P526, DOI 10.1063/1.1730390; Schmidler SC, 2000, J COMPUT BIOL, V7, P233, DOI 10.1089/10665270050081496; Luo PZ, 1999, P NATL ACAD SCI USA, V96, P4930, DOI 10.1073/pnas.96.9.4930; CHAKRABARTTY A, 1994, PROTEIN SCI, V3, P843; SCHOLTZ JM, 1991, BIOPOLYMERS, V31, P1463, DOI 10.1002/bip.360311304; MARK AE, 1994, J MOL BIOL, V240, P167, DOI 10.1006/jmbi.1994.1430; LAURITZEN SL, 1988, J ROY STAT SOC B MET, V50, P157; CREAMER TP, 1995, PROTEIN SCI, V4, P1305; Creighton T. E., 1993, PROTEINS STRUCTURES; DEPMSTER AP, 1977, J R STAT SOC B, V39, P1; Dill KA, 1997, J BIOL CHEM, V272, P701; DOIG AJ, 1994, BIOCHEMISTRY-US, V33, P3396, DOI 10.1021/bi00177a033; DOIG AJ, 1995, PROTEIN SCI, V4, P1325; Duan Y, 1998, SCIENCE, V282, P740, DOI 10.1126/science.282.5389.740; FernandezRecio J, 1997, J MOL BIOL, V267, P184, DOI 10.1006/jmbi.1996.0831; FROIMOWI.M, 1974, MACROMOLECULES, V7, P583, DOI 10.1021/ma60041a009; Garcia AE, 2002, P NATL ACAD SCI USA, V99, P2782, DOI 10.1073/pnas.042496899; GELMANA, 1995, BAYESIAN DATA ANAL; GILKS WR, 1996, MORKOV CHAIN M CARLO; Holley HHL, 1989, P NATL ACAD SCI USA, V86, P152; HUYGHUESDESPOINTES BMP, 1995, BIOCHEMISTRY-US, V34, P13267, DOI 10.1021/bi00041a001; Johnstone IM, 2004, ANN STAT, V32, P1594, DOI 10.1214/009053604000000030; JU WH, 2002, BAYESIAN LEARNING SP; KLINGLER TM, 1994, PROTEIN SCI, V3, P1847; Lacroix E, 1998, J MOL BIOL, V284, P173, DOI 10.1006/jmbi.1998.2145; Lathrop RH, 1996, J MOL BIOL, V255, P641, DOI 10.1006/jmbi.1996.0053; LIFSON S, 1961, J CHEM PHYS, V34, P1963, DOI 10.1063/1.1731802; LIU JS, 1994, J AM STAT ASSOC, V89, P958, DOI 10.2307/2290921; Liwo A, 1999, P NATL ACAD SCI USA, V96, P5482, DOI 10.1073/pnas.96.10.5482; LUCAS J, 2007, UNPUB BAYESIAN EXPT; McGaughey GB, 1998, J BIOL CHEM, V273, P15458, DOI 10.1074/jbc.273.25.15458; Misra GP, 1997, PROTEINS, V28, P344; MUNOZ V, 1995, J MOL BIOL, V245, P297, DOI 10.1006/jmbi.1994.0024; MUNOZ V, 1995, J MOL BIOL, V245, P275, DOI 10.1006/jmbi.1994.0023; MUNOZ V, 1994, NAT STRUCT BIOL, V1, P399, DOI 10.1038/nsb0694-399; PADMANABHAN S, 1994, PROTEIN SCI, V3, P1992; PADMANABHAN S, 1990, NATURE, V344, P268, DOI 10.1038/344268a0; Poland D., 1970, THEORY HELIX COIL TR; PRESTA LG, 1988, SCIENCE, V240, P1632, DOI 10.1126/science.2837824; Qian H, 1996, J MOL BIOL, V256, P663, DOI 10.1006/jmbi.1996.9999; QIAN H, 1992, J PHYS CHEM-US, V96, P3987, DOI 10.1021/j100189a015; Qian N, 1988, J MOL BIOL, V202, P865, DOI 10.1016/0022-2836(88)90564-5; RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626; RICHARDSON JS, 1988, SCIENCE, V240, P1648, DOI 10.1126/science.3381086; Richardson JM, 2004, J MOL BIOL, V335, P1029, DOI 10.1016/j.jmb.2003.11.027; Rosen JB, 2000, BIOPHYS J, V79, P2818; SALAMOV AA, 1995, J MOL BIOL, V247, P11, DOI 10.1006/jmbi.1994.0116; Scheraga HA, 2002, BIOPHYS CHEM, V101, P255, DOI 10.1016/S0301-4622(02)00175-8; SCHOLTZ JM, 1992, ANNU REV BIOPH BIOM, V21, P95, DOI 10.1146/annurev.bb.21.060192.000523; SHALONGO W, 1995, PROTEIN SCI, V4, P1161; Shalongo W, 1997, PROTEINS, V28, P467, DOI 10.1002/(SICI)1097-0134(199708)28:4<467::AID-PROT2>3.0.CO;2-7; SIPPL MJ, 1995, CURR OPIN STRUC BIOL, V5, P229, DOI 10.1016/0959-440X(95)80081-6; STAPLEY BJ, 1995, PROTEIN SCI, V4, P2383; Sun JK, 1998, PROTEIN SCI, V7, P2374; Thomas A, 2002, PROTEINS, V48, P628, DOI 10.1002/prot.10190; Thomas PD, 1996, P NATL ACAD SCI USA, V93, P11628, DOI 10.1073/pnas.93.21.11628; YI TM, 1993, J MOL BIOL, V232, P1117, DOI 10.1006/jmbi.1993.1464	68	4	4	1	6	MARY ANN LIEBERT INC	NEW ROCHELLE	140 HUGUENOT STREET, 3RD FL, NEW ROCHELLE, NY 10801 USA	1066-5277			J COMPUT BIOL	J. Comput. Biol.	DEC	2007	14	10					1287	1310		10.1089/cmb.2007.0008		24	Biochemical Research Methods; Biotechnology & Applied Microbiology; Computer Science, Interdisciplinary Applications; Mathematical & Computational Biology; Statistics & Probability	Biochemistry & Molecular Biology; Biotechnology & Applied Microbiology; Computer Science; Mathematical & Computational Biology; Mathematics	243HB	WOS:000251786300003	18047425	
J	Zhao, P; Yu, B				Zhao, Peng; Yu, Bin			Stagewise Lasso	JOURNAL OF MACHINE LEARNING RESEARCH			English	Article						backward step; boosting; convexity; Lasso; regularization path	VARIABLE SELECTION; ORACLE PROPERTIES; STATISTICAL VIEW; REGRESSION; APPROXIMATION; NOISE	Many statistical machine learning algorithms minimize either an empirical loss function as in Ad-aBoost, or a penalized empirical loss as in Lasso or SVM. A single regularization tuning parameter controls the trade-off between fidelity to the data and generalizability, or equivalently between bias and variance. When this tuning parameter changes, a regularization "path" of solutions to the minimization problem is generated, and the whole path is needed to select a tuning parameter to optimize the prediction or interpretation performance. Algorithms such as homotopy-Lasso or LARS-Lasso and Forward Stagewise Fitting (FSF) (aka e-Boosting) are of great interest because of their resulted sparse models for interpretation in addition to prediction. In this paper, we propose the BLasso algorithm that ties the FSF (e-Boosting) algorithm with the Lasso method that minimizes the L-1 penalized L-2 loss. BLasso is derived as a coordinate descent method with a fixed stepsize applied to the general Lasso loss function (L1 penalized convex loss). It consists of both a forward step and a backward step. The forward step is similar to e-Boosting or FSF, but the backward step is new and revises the FSF (or e-Boosting) path to approximate the Lasso path. In the cases of a finite number of base learners and a bounded Hessian of the loss function, the BLasso path is shown to converge to the Lasso path when the stepsize goes to zero. For cases with a larger number of base learners than the sample size and when the true model is sparse, our simulations indicate that the BLasso model estimates are sparser than those from FSF with comparable or slightly better prediction performance, and that the the discrete stepsize of BLasso and FSF has an additional regularization effect in terms of prediction and sparsity. Moreover, we introduce the Generalized BLasso algorithm to minimize a general convex loss penalized by a general convex function. Since the (Generalized) BLasso relies only on differences not derivatives, we conclude that it provides a class of simple and easy-to-implement algorithms for tracing the regularization or solution paths of penalized minimization problems.	[Zhao, Peng; Yu, Bin] Univ Calif Berkeley, Dept Stat, Berkeley, CA 94720 USA	Zhao, P (reprint author), Univ Calif Berkeley, Dept Stat, 367 Evans Hall, Berkeley, CA 94720 USA.	pengzhao@stat.berkeley.edu; binyu@stat.berkeley.edu					Allgower E. L., 1980, Numerical Solution of Highly Nonlinear Problems. Fixed Point Algorithms and Complementarity Problems; Tropp JA, 2006, IEEE T INFORM THEORY, V52, P1030, DOI 10.1109/TIT.2005.864420; Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451; Knight K, 2000, ANN STAT, V28, P1356; Friedman J, 2000, ANN STAT, V28, P337, DOI 10.1214/aos/1016218223; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; Breiman L, 1998, ANN STAT, V26, P801; Zhao P, 2006, J MACH LEARN RES, V7, P2541; Osborne MR, 2000, IMA J NUMER ANAL, V20, P389, DOI 10.1093/imanum/20.3.389; FRANK IE, 1993, TECHNOMETRICS, V35, P109, DOI 10.2307/1269656; Zou H, 2006, J AM STAT ASSOC, V101, P1418, DOI 10.1198/016214506000000735; Efron B, 2004, ANN STAT, V32, P407; Donoho DL, 2006, COMMUN PUR APPL MATH, V59, P797, DOI 10.1002/cpa.20132; Meinshausen N, 2006, ANN STAT, V34, P1436, DOI 10.1214/009053606000000281; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P6, DOI 10.1109/TIT.2005.860430; BUHLMANN P, 2003, J AM STAT ASS, V98; CANDES E, 2007, IN PRESS ANN STAT; CHEN S, 1994, BASIS PURSUIT TECHNI; Cristianini N., 2002, INTRO SUPPORT VECTOR; FREUND Y, 1995, INFORM COMPUT, V121, P256, DOI 10.1006/inco.1995.1136; Freund Y., 1996, Machine Learning. Proceedings of the Thirteenth International Conference (ICML '96); Fu WJ, 2001, J COMPUTATIONAL GRAP, V7, P397; GAO J, 2006, P 21 INT C COMP LING, P225, DOI 10.3115/1220175.1220204; GEDEON T, 2002, CANADIAN APPL MATH Q; Hastie T., 2001, ELEMENTS STAT LEARNI; HASTIE T, 2006, FORWARD STAGEWISE RE; Mason L, 1999, ADV LARGE MARGIN CLA; MEINSHAUSEN N, 2006, IN PRESS ANN STAT; Osborne MR, 2000, J COMPUT GRAPH STAT, V9, P319, DOI 10.2307/1390657; ROSSET S, 2004, NIPS; Rosset S, 2004, J MACH LEARN RES, V5, P941; SCHAPIRE RE, 1990, J MACHINE LEARNING, V5, P1997; Scholkopf B., 2002, LEARNING KERNELS SUP; Tishby N, 1999, 37 ANN ALL C COMM CO; Vandenberghe L., 2004, CONVEX OPTIMIZATION; Vapnik V.N., 1995, NATURE STAT LEARNING; WAINWRIGHT MJ, 2006, 709 STAT DEP; ZHANG CH, 2006, IN PRESS ANN STAT; Zhang T, 2003, IEEE T INFORM THEORY, V49, P682, DOI 10.1109/TIT.2002.808136; ZHU J, 2003, ADV NEURAL INFORM PR, P16	41	45	47	1	4	MICROTOME PUBL	BROOKLINE	31 GIBBS ST, BROOKLINE, MA 02446 USA	1532-4435			J MACH LEARN RES	J. Mach. Learn. Res.	DEC	2007	8						2701	2726				26	Automation & Control Systems; Computer Science, Artificial Intelligence	Automation & Control Systems; Computer Science	256QQ	WOS:000252745100002		
J	Khan, JA; Van Aelst, S; Zamar, RH				Khan, Jafar A.; Van Aelst, Stefan; Zamar, Ruben H.			Robust linear model selection based on least angle regression	JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION			English	Article						bootstrap; computational complexity; robust prediction; stepwise algorithm; Winsorization	INFERENCE; ESTIMATORS; LOCATION	In this article we consider the problem of building a linear prediction model when the number of candidate predictors is large and the data possibly contain anomalies that are difficult to visualize and clean. We want to predict the nonoutlying cases; therefore, we need a method that is simultaneously robust and scalable. We consider the stepwise least angle regression (LARS) algorithm which is computationally very efficient but sensitive to outliers. We introduce two different approaches to robustify LARS. The plug-in approach replaces the classical correlations in LARS by robust correlation estimates. The cleaning approach first transforms the data set by shrinking the outliers toward the bulk of the data (which we call multivariate Winsorization) and then applies LARS to the transformed data. We show that the plug in approach is time-efficient and scalable and that the bootstrap can be used to stabilize its results. We recommend using bootstrapped robustified LARS to sequence a number of candidate predictors to form a reduced set from which a more refined model can be selected.	[Khan, Jafar A.] Univ Dhaka, Dept Stat, Dhaka 1000, Bangladesh; [Van Aelst, Stefan] Univ Ghent, Dept Appl Math & Comp Sci, B-9000 Ghent, Belgium; [Zamar, Ruben H.] Univ British Columbia, Dept Stat, Vancouver, BC V6T 1Z2, Canada	Khan, JA (reprint author), Univ Dhaka, Dept Stat, Dhaka 1000, Bangladesh.	jkhan66@gmai1.com; Stefan.VanAelst@UGent.be; ruben@stat.ubc.ca	Van Aelst, Stefan/B-5201-2012				Alqallaf FA, 2002, P 7 ACM SIGKDD INT C, P14, DOI 10.1145/775047.775050; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; MARONNA RA, 1976, ANN STAT, V4, P51, DOI 10.1214/aos/1176343347; FRANK IE, 1993, TECHNOMETRICS, V35, P109, DOI 10.2307/1269656; Efron B, 2004, ANN STAT, V32, P407; Atkinson AC, 2002, BIOMETRIKA, V89, P939, DOI 10.1093/biomet/89.4.939; Breithaupt H, 2000, EMBO REP, V1, P5; Cantoni E, 2001, J AM STAT ASSOC, V96, P1022, DOI 10.1198/016214501753209004; Croux C, 2003, STAT COMPUT, V13, P23, DOI 10.1023/A:1021979409012; Dlaz-Uriarte R., 2006, BMC BIOINFORMATICS, V7, P3; Duan KB, 2005, IEEE T NANOBIOSCI, V4, P228, DOI 10.1109/TNB.2005.853657; Genton MG, 2003, J AM STAT ASSOC, V98, P67, DOI 10.1198/016214503388619102; GNANADES.R, 1972, BIOMETRICS, V28, P81, DOI 10.2307/2528963; Hastie T., 2001, ELEMENTS STAT LEARNI; Huber P. J., 1981, ROBUST STAT; Jiang WX, 2004, STAT SCI, V19, P239, DOI 10.1214/088342304000000152; Maronna R. A., 2006, ROBUST STAT THEORY M; Maronna RA, 2002, TECHNOMETRICS, V44, P307, DOI 10.1198/004017002188618509; Mendenhall W., 2003, 2 COURSE STAT REGRES; MORGENTHALER S, 2003, THEORY APPL RECENT R, P195; Muller S, 2005, J AM STAT ASSOC, V100, P1297, DOI 10.1198/016214505000000529; RONCHETTI E, 1994, J AM STAT ASSOC, V89, P550, DOI 10.2307/2290858; RONCHETTI E, 1985, STAT PROBABIL LETT, V3, P21, DOI 10.1016/0167-7152(85)90006-9; Ronchetti E, 2001, J ECONOMETRICS, V101, P37, DOI 10.1016/S0304-4076(00)00073-7; Ronchetti E, 1997, J AM STAT ASSOC, V92, P1017, DOI 10.2307/2965566; Rousseeuw P. J., 1987, ROBUST REGRESSION OU; Sommer S, 1996, APPL STAT-J ROY ST C, V45, P15, DOI 10.2307/2986219; TORKKOLA K, 2006, FEATURE EXTRACTION F, P297; Weisberg S., 1985, APPL LINEAR REGRESSI	29	37	37	2	5	AMER STATISTICAL ASSOC	ALEXANDRIA	1429 DUKE ST, ALEXANDRIA, VA 22314 USA	0162-1459			J AM STAT ASSOC	J. Am. Stat. Assoc.	DEC	2007	102	480					1289	1299		10.1198/016214507000000950		11	Statistics & Probability	Mathematics	243WY	WOS:000251829200023		
J	Verbyla, AP; Cullis, BR; Thompson, R				Verbyla, Arunas P.; Cullis, Brian R.; Thompson, Robin			The analysis of QTL by simultaneous use of the full linkage map	THEORETICAL AND APPLIED GENETICS			English	Article							QUANTITATIVE TRAIT LOCI; MARKER-ASSISTED SELECTION; ALTERNATIVE OUTLIER MODEL; MIXED-MODEL; FLANKING MARKERS; REGRESSION; WHEAT; INFORMATION; POPULATIONS; CROSSES	An extension of interval mapping is presented that incorporates all intervals on the linkage map simultaneously. The approach uses a working model in which the sizes of putative QTL for all intervals across the genome are random effects. An outlier detection method is used to screen for possible QTL. Selected QTL are subsequently fitted as fixed effects. This screening and selection approach is repeated until the variance component for QTL sizes is not statistically significant. A comprehensive simulation study is conducted in which map uncertainty is included. The proposed method is shown to be superior to composite interval mapping in terms of power of detection of QTL. There is an increase in the rate of false positive QTL detected when using the new approach, but this rate decreases as the population size increases. The new approach is much simpler computationally. The analysis of flour milling yield in a doubled haploid population illustrates the improved power of detection of QTL using the approach, and also shows how vital it is to allow for sources of non-genetic variation in the analysis.	CSIRO, Glen Osmond, SA 5064, Australia; Univ Adelaide, Sch Agr Food & Wine, Glen Osmond, SA 5064, Australia; Wagga Wagga Res Inst, Dept Primary Ind, Wagga Wagga, NSW 2650, Australia; Univ London, Queen Mary Coll, Sch Math Sci, London E1 4NS, England; Ctr Math & Computat Biol, Dept Biomath & Bioinformat, Harpenden AL5 2JQ, Herts, England	Verbyla, AP (reprint author), CSIRO, PMB 2, Glen Osmond, SA 5064, Australia.	Ari.Verbyla@adelaide.edu.au	Verbyla, Arunas/A-5032-2009; cullis, brian/E-7078-2010; thompson, robin/F-7946-2010				STRAM DO, 1994, BIOMETRICS, V50, P1171, DOI 10.2307/2533455; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Whittaker JC, 2000, GENET RES, V75, P249, DOI 10.1017/S0016672399004462; LANDER ES, 1989, GENETICS, V121, P185; ZENG ZB, 1994, GENETICS, V136, P1457; Broman KW, 2002, J R STAT SOC B, V64, P641, DOI 10.1111/1467-9868.00354; BROMAN KW, 2005, QTL TOOLS ANAL QTL E; Butler DG, 2007, ASREML R REFERENCE M; COOK RD, 1982, J ROY STAT SOC B MET, V44, P370; Crainiceanu CM, 2004, J ROY STAT SOC B, V66, P165, DOI 10.1111/j.1467-9868.2004.00438.x; DIGGLE PJ, 1990, TIME SERIES ANAL BIO; Eckermann PJ, 2001, AUST J AGR RES, V52, P1195, DOI 10.1071/AR01039; Foster SD, 2007, J AGR BIOL ENVIR ST, V12, P300, DOI 10.1198/108571107X200396; Gianola D, 2003, GENETICS, V163, P347; GILMOUR AR, 2007, ASREML USERS GUIDE; GOGEL BJ, 1997, THESIS U ADELAIDE; GOGEL BJ, 2001, P106 U AD; HALEY CS, 1992, HEREDITY, V69, P315; HENDERSON CR, 1950, ANN MATH STAT, V21, P309; JANSEN RC, 1994, GENETICS, V138, P871; KIIVERI HT, 2004, SCI STAT FESTCHRIFT, P127; LANDER ES, 1987, P NATL ACAD SCI USA, V84, P2363, DOI 10.1073/pnas.84.8.2363; Lehmensiek A, 2005, AUST J AGR RES, V56, P1347, DOI 10.1071/AR05126; Lehmensiek A, 2006, AUST J AGR RES, V57, P1115, DOI 10.1071/AR05375; MARTINEZ O, 1992, THEOR APPL GENET, V85, P480, DOI 10.1007/BF00222330; MARTINEZ O, 1994, HEREDITY, V73, P198, DOI 10.1038/hdy.1994.120; Moreau L, 1999, THEOR APPL GENET, V98, P234, DOI 10.1007/s001220051063; PATTERSO.HD, 1971, BIOMETRIKA, V58, P545, DOI 10.2307/2334389; Piepho HP, 2000, GENETICS, V156, P2043; R Development Core Team, 2006, R LANG ENV STAT COMP; Robinson G. K., 1991, STAT SCI, V6, P15, DOI DOI 10.1214/SS/1177011926; Smith AB, 2001, AUST J AGR RES, V52, P1207, DOI 10.1071/AR01058; Smith AB, 2006, J AGR SCI, V144, P393, DOI 10.1017/S0021859606006319; Thompson R, 2003, AUST NZ J STAT, V45, P445, DOI 10.1111/1467-842X.00297; THOMPSON R, 1985, J ROY STAT SOC B MET, V47, P53; Trow AH, 1913, J GENET, V2, P313, DOI 10.1007/BF02981557; Verbyla AP, 2003, AUST J AGR RES, V54, P1395, DOI 10.1071/AR02239; WELHAM SJ, 2006, THESIS U LONDON; Whittaker JC, 1996, HEREDITY, V77, P23, DOI 10.1038/hdy.1996.104; Yi NJ, 2003, GENETICS, V164, P1129	40	31	31	3	13	SPRINGER	NEW YORK	233 SPRING STREET, NEW YORK, NY 10013 USA	0040-5752			THEOR APPL GENET	Theor. Appl. Genet.	DEC	2007	116	1					95	111		10.1007/s00122-007-0650-x		17	Agronomy; Plant Sciences; Genetics & Heredity; Horticulture	Agriculture; Plant Sciences; Genetics & Heredity	234IH	WOS:000251154100010	17952402	
J	Dahinden, C; Parmigiani, G; Emerick, MC; Buhlmann, P				Dahinden, Corinne; Parmigiani, Giovanni; Emerick, Mark C.; Buehlmann, Peter			Penalized likelihood for sparse contingency tables with an application to full-length cDNA libraries	BMC BIOINFORMATICS			English	Article							HUMAN GENOME; HUMAN GENES; SELECTION; REGRESSION	Background: The joint analysis of several categorical variables is a common task in many areas of biology, and is becoming central to systems biology investigations whose goal is to identify potentially complex interaction among variables belonging to a network. Interactions of arbitrary complexity are traditionally modeled in statistics by log-linear models. It is challenging to extend these to the high dimensional and potentially sparse data arising in computational biology. An important example, which provides the motivation for this article, is the analysis of so-called full-length cDNA libraries of alternatively spliced genes, where we investigate relationships among the presence of various exons in transcript species. Results: We develop methods to perform model selection and parameter estimation in log-linear models for the analysis of sparse contingency tables, to study the interaction of two or more factors. Maximum Likelihood estimation of log-linear model coefficients might not be appropriate because of the presence of zeros in the table's cells, and new methods are required. We propose a computationally efficient l(I)-penalization approach extending the Lasso algorithm to this context, and compare it to other procedures in a simulation study. We then illustrate these algorithms on contingency tables arising from full-length cDNA libraries. Conclusion: We propose regularization methods that can be used successfully to detect complex interaction patterns among categorical variables in a broad range of biological problems involving categorical variables.	[Dahinden, Corinne; Buehlmann, Peter] ETH, CH-8092 Zurich, Switzerland; [Dahinden, Corinne; Buehlmann, Peter] ETH, Competence Ctr Syst Physiol & Metab Dis, CH-8093 Zurich, Switzerland; [Parmigiani, Giovanni] Johns Hopkins Univ, Sch Med & Publ Hlth, Dept Oncol & Biostat, Baltimore, MD USA; [Emerick, Mark C.] Johns Hopkins Univ, Sch Med, Dept Physiol, Baltimore, MD 21205 USA	Dahinden, C (reprint author), ETH, CH-8092 Zurich, Switzerland.	dahinden@stat.math.ethz.ch; gp@jhu.edu; memeri@jhmi.edu; buhlmann@stat.math.ethz.ch	Buhlmann, Peter/A-2107-2013				Yuan M, 2006, J ROY STAT SOC B, V68, P49, DOI 10.1111/j.1467-9868.2005.00532.x; Carninci P, 2005, SCIENCE, V309, P1559, DOI 10.1126/science.1112014; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Southan C, 2004, PROTEOMICS, V4, P1712, DOI 10.1002/pmic.200300700; Mironov AA, 1999, GENOME RES, V9, P1288, DOI 10.1101/gr.9.12.1288; Brett D, 2002, NAT GENET, V30, P29, DOI 10.1038/ng803; Brett D, 2000, FEBS LETT, V474, P83, DOI 10.1016/S0014-5793(00)01581-7; Christensen R, 1991, LINEAR MODELS MULTIV; Everitt B.S., 1992, MONOGRAPHS STAT APPL, V45; Imanishi T, 2004, PLOS BIOL, V2, P856, DOI 10.1371/journal.pbio.0020162; Collins FS, 2004, NATURE, V431, P931, DOI 10.1038/nature03001; Lander ES, 2001, NATURE, V409, P860, DOI 10.1038/35057062; Lauritzen Steffen L., 1996, OXFORD STAT SCI SERI, V17; Liang F, 2000, NAT GENET, V25, P239; MEINSHAUSEN N, IN PRESS COMPUTATION; Regan MR, 2005, PROTEINS, V59, P312, DOI 10.1002/prot.20225; Rosset S, 2005, ADV NEURAL INFORM PR, V1153, P1153; Zavolan M, 2002, GENOME RES, V12, P1377, DOI 10.1101/gr.191702	18	8	8	1	2	BIOMED CENTRAL LTD	LONDON	236 GRAYS INN RD, FLOOR 6, LONDON WC1X 8HL, ENGLAND	1471-2105			BMC BIOINFORMATICS	BMC Bioinformatics	DEC 11	2007	8								476	10.1186/1471-2105-8-476		11	Biochemical Research Methods; Biotechnology & Applied Microbiology; Mathematical & Computational Biology	Biochemistry & Molecular Biology; Biotechnology & Applied Microbiology; Mathematical & Computational Biology	259XB	WOS:000252973100002	18072965	
J	Ye, JJ; Liu, H; Kirmiz, C; Lebrilla, CB; Rocke, DM				Ye, Jingjing; Liu, Hao; Kirmiz, Crystal; Lebrilla, Carlito B.; Rocke, David M.			On the analysis of glycomics mass spectrometry data via the regularized area under the ROC curve	BMC BIOINFORMATICS			English	Article							DISEASE CLASSIFICATION; POTENTIAL BIOMARKERS; OVARIAN-CANCER; SERUM; IDENTIFICATION; DISCOVERY; SELECTION; SPECTRA; TUMORS	Background: Novel molecular and statistical methods are in rising demand for disease diagnosis and prognosis with the help of recent advanced biotechnology. High-resolution mass spectrometry (MS) is one of those biotechnologies that are highly promising to improve health outcome. Previous literatures have identified some proteomics biomarkers that can distinguish healthy patients from cancer patients using MS data. In this paper, an MS study is demonstrated which uses glycomics to identify ovarian cancer. Glycomics is the study of glycans and glycoproteins. The glycans on the proteins may deviate between a cancer cell and a normal cell and may be visible in the blood. High-resolution MS has been applied to measure relative abundances of potential glycan biomarkers in human serum. Multiple potential glycan biomarkers are measured in MS spectra. With the objection of maximizing the empirical area under the ROC curve (AUC), an analysis method was considered which combines potential glycan biomarkers for the diagnosis of cancer. Results: Maximizing the empirical AUC of glycomics MS data is a large-dimensional optimization problem. The technical difficulty is that the empirical AUC function is not continuous. Instead, it is in fact an empirical 0-1 loss function with a large number of linear predictors. An approach was investigated that regularizes the area under the ROC curve while replacing the 0-1 loss function with a smooth surrogate function. The constrained threshold gradient descent regularization algorithm was applied, where the regularization parameters were chosen by the cross-validation method, and the confidence intervals of the regression parameters were estimated by the bootstrap method. The method is called TGDR-AUC algorithm. The properties of the approach were studied through a numerical simulation study, which incorporates the positive values of mass spectrometry data with the correlations between measurements within person. The simulation proved asymptotic properties that estimated AUC approaches the true AUC. Finally, mass spectrometry data of serum glycan for ovarian cancer diagnosis was analyzed. The optimal combination based on TGDR-AUC algorithm yields plausible result and the detected biomarkers are confirmed based on biological evidence. Conclusion: The TGDR-AUC algorithm relaxes the normality and independence assumptions from previous literatures. In addition to its flexibility and easy interpretability, the algorithm yields good performance in combining potential biomarkers and is computationally feasible. Thus, the approach of TGDR-AUC is a plausible algorithm to classify disease status on the basis of multiple biomarkers.	[Liu, Hao] Baylor Coll Med, Dan L Canc Ctr, Div Biostat, Houston, TX 77030 USA; [Ye, Jingjing] Univ Calif Davis, Dept Stat, Davis, CA 95616 USA; [Kirmiz, Crystal; Lebrilla, Carlito B.] Univ Calif Davis, Dept Chem, Davis, CA 95616 USA; [Rocke, David M.] Univ Calif Davis, Div Biostat, Davis, CA 95616 USA	Ye, JJ (reprint author), Baylor Coll Med, Dan L Canc Ctr, Div Biostat, Houston, TX 77030 USA.	jingjingye@gmail.com; haol@bcm.tmc.edu; ckirmiz@ucdavis.edu; cblebrilla@ucdavis.edu; dmrocke@ucdavis.edu	Rocke, David/I-7044-2013	Rocke, David/0000-0002-3958-7318			Adam BL, 2002, CANCER RES, V62, P3609; An HJ, 2006, J PROTEOME RES, V5, P1626, DOI 10.1021/pr060010k; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Ball G, 2002, BIOINFORMATICS, V18, P395, DOI 10.1093/bioinformatics/18.3.395; Dudoit S, 2002, J AM STAT ASSOC, V97, P77, DOI 10.1198/016214502753479248; Pepe MS, 2001, J NATL CANCER I, V93, P1054, DOI 10.1093/jnci/93.14.1054; Geurts P, 2005, BIOINFORMATICS, V21, P3138, DOI 10.1093/bioinformatics/bti494; BENJAMINI Y, 1995, J ROY STAT SOC B MET, V57, P289; Apweiler R, 1999, BBA-GEN SUBJECTS, V1473, P4, DOI 10.1016/S0304-4165(99)00165-8; VARKI A, 1993, GLYCOBIOLOGY, V3, P97, DOI 10.1093/glycob/3.2.97; Baggerly KA, 2003, PROTEOMICS, V3, P1667, DOI 10.1002/pmic.200300522; Baggerly KA, 2004, BIOINFORMATICS, V20, P777, DOI 10.1093/bioinformatics/btg484; BAMBER D, 1975, J MATH PSYCHOL, V12, P387, DOI 10.1016/0022-2496(75)90001-2; Datta S., 2006, STAT METHODOLOGY, V3, P79, DOI 10.1016/j.stamet.2005.09.006; Friedman J., 2004, TECHNICAL REPORT; FUSHIKI T, 2006, BMC BIOINFORMATICS, V7; Gui J., 2005, PACIFIC S BIOCOMPUTI, V10, P272; LANCASHIRE LJ, 2003, 17 EUR SIM MULT NOTT, P131; Lilien RH, 2003, J COMPUT BIOL, V10, P925, DOI 10.1089/106652703322756159; Ma SG, 2005, BIOINFORMATICS, V21, P4356, DOI 10.1093/bioinformatics/bti724; McIntosh MW, 2002, BIOMETRICS, V58, P657, DOI 10.1111/j.0006-341X.2002.00657.x; Mian S, 2003, PROTEOMICS, V3, P1725, DOI 10.1002/pmic.200300526; Miketova P, 2003, J ANAL APPL PYROL, V67, P109, DOI 10.1016/S0165-2370(02)00019-0; Pepe MS, 2006, BIOMETRICS, V62, P221, DOI 10.1111/j.1541-0420.2005.00420.x; Pepe MS, 2000, BIOSTATISTICS, V1, P123, DOI DOI 10.1093/BI0STATISTICS/1.2.123; STONE M, 1974, J R STAT SOC B, V36, P111; SU JQ, 1993, J AM STAT ASSOC, V88, P1350; Teukolsky SA, 1992, NUMERICAL RECIPES C; Wagner M, 2003, PROTEOMICS, V3, P1692, DOI 10.1002/pmic.200300519; Wu BL, 2003, BIOINFORMATICS, V19, P1636, DOI 10.1093/bioinformatics/btg210; Xiong MM, 2001, GENOME RES, V11, P1878	31	9	9	0	0	BIOMED CENTRAL LTD	LONDON	236 GRAYS INN RD, FLOOR 6, LONDON WC1X 8HL, ENGLAND	1471-2105			BMC BIOINFORMATICS	BMC Bioinformatics	DEC 12	2007	8								477	10.1186/1471-2105-8-477		12	Biochemical Research Methods; Biotechnology & Applied Microbiology; Mathematical & Computational Biology	Biochemistry & Molecular Biology; Biotechnology & Applied Microbiology; Mathematical & Computational Biology	257HY	WOS:000252791000001	18076765	
J	Sauerbrei, W; Royston, P; Binder, H				Sauerbrei, Willi; Royston, Patrick; Binder, Harald			Selection of important variables and determination of functional form for continuous predictors in multivariable model building	STATISTICS IN MEDICINE			English	Article; Proceedings Paper	27th Annual Conference of the International-Society-for-Clinical-Biostatistics	AUG 27-31, 2006	Geneva, SWITZERLAND	Int Soc Clin Biostat		regression models; variable selection; functional form; fractional polynomials; splines	FRACTIONAL POLYNOMIAL-MODELS; REGRESSION; TRANSFORMATION; SHRINKAGE; TIME	In developing regression models, data analysts are often faced with many predictor variables that may influence an outcome variable. After more than half a century of research, the 'best' way of selecting a multivariable model is still unresolved. It is generally agreed that subject matter knowledge, when available, should guide model building. However, such knowledge is often limited, and data-dependent model building is required. We limit the scope of the modelling exercise to selecting important predictors and choosing interpretable and transportable functions for continuous predictors. Assuming linear functions, stepwise selection and all-subset strategies are discussed; the key tuning parameters are the nominal P-value for testing a variable for inclusion and the penalty for model complexity, respectively. We argue that stepwise procedures perform better than a literature-based assessment would suggest. Concerning selection of functional form for continuous predictors, the principal competitors are fractional polynomial functions and various types of spline techniques. We note that a rigorous selection strategy known as multivariable fractional polynomials (MFP) has been developed. No spline-based procedure for simultaneously selecting variables and functional forms has found wide acceptance. Results of FP and spline modelling are compared in two data sets. It is shown that spline modelling, while extremely flexible, can generate fitted curves with uninterpretable `wiggles', particularly when automatic methods for choosing the smoothness are employed. We give general recommendations to practitioners for carrying out variable and function selection. While acknowledging that further research is needed, we argue why MFP is our preferred approach for multivariable model building with continuous covariates. Copyright (C) 2007 John Wiley & Sons, Ltd.	[Sauerbrei, Willi; Binder, Harald] Univ Med Ctr Freiburg, Inst Med Biometry & Med Informat, D-79104 Freiburg, Germany; [Royston, Patrick] MRC, Clin Trials Unit, London NW1 2DA, England	Sauerbrei, W (reprint author), Univ Med Ctr Freiburg, Inst Med Biometry & Med Informat, Stefan Meier Str 26, D-79104 Freiburg, Germany.	wfs@imbi.uni-freiburg.de	Binder, Harald/C-7413-2009	Binder, Harald/0000-0002-5666-8662			ROYSTON P, 1994, APPL STAT-J ROY ST C, V43, P429, DOI 10.2307/2986270; Royston P, 2004, STAT MED, V23, P2509, DOI 10.1002/sim.1815; Ambler G, 2001, J STAT COMPUT SIM, V69, P89, DOI 10.1080/00949650108812083; Pocock SJ, 2006, EUR HEART J, V27, P65, DOI 10.1093/eurheartj/ehi555; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Steyerberg EW, 2001, MED DECIS MAKING, V21, P45; Sauerbrei W, 2007, BIOMETRICAL J, V49, P453, DOI 10.1002/bimj.200610328; Eilers PHC, 1996, STAT SCI, V11, P89, DOI 10.1214/ss/1038425655; Royston P, 2006, STAT MED, V25, P127, DOI 10.1002/sim.2331; SAUERBREI W, 1992, STAT MED, V11, P2093, DOI 10.1002/sim.4780111607; COPAS JB, 1983, J R STAT SOC B, V45, P311; Burnham K. P., 2002, MODEL SELECTION MULT; Chatfield C, 2002, J ROY STAT SOC D-STA, V51, P1, DOI 10.1111/1467-9884.00294; COPAS JB, 1991, STATISTICIAN, V40, P51, DOI 10.2307/2348223; Harrell FE, 2001, REGRESSION MODELING; Hastie TJ, 1990, GEN ADDITIVE MODELS; Hollander N, 2006, COMPUT STAT DATA AN, V50, P1131, DOI 10.1016/j.csda.2004.11.008; MANTEL N, 1970, TECHNOMETRICS, V12, P621, DOI 10.2307/1267207; Rosenberg PS, 2003, STAT MED, V22, P3369, DOI 10.1002/sim.1638; Royston P, 2008, MULTIVARIABLE MODELB; Royston P, 2003, STAT MED, V22, P639, DOI 10.1002/sim.1310; Royston P, 2007, COMPUT STAT DATA AN, V51, P4240, DOI 10.1016/j.csda.2006.05.006; Royston P, 2005, METHOD INFORM MED, V44, P561; Ruppert D., 2003, SEMIPARAMETRIC REGRE; SAUERBEI W, 2007, VARIABLE SELECTION M; SAUERBREI D, 2006, COMPUTATIONAL STAT D, V50, P3464; Sauerbrei W., 1993, EUROPAISCHE PERSPEKT, P108; Sauerbrei W, 2002, J ROY STAT SOC A STA, V165, P399, DOI 10.1111/1467-985X.02026; Sauerbrei W, 1999, APPL STAT, V48, P313; Wood SN, 2006, GEN ADDITIVE MODELS; WYATT JC, 1995, BRIT MED J, V311, P1539	31	92	93	1	14	JOHN WILEY & SONS LTD	CHICHESTER	THE ATRIUM, SOUTHERN GATE, CHICHESTER PO19 8SQ, W SUSSEX, ENGLAND	0277-6715			STAT MED	Stat. Med.	DEC 30	2007	26	30					5512	5528		10.1002/sim.3148		17	Mathematical & Computational Biology; Public, Environmental & Occupational Health; Medical Informatics; Medicine, Research & Experimental; Statistics & Probability	Mathematical & Computational Biology; Public, Environmental & Occupational Health; Medical Informatics; Research & Experimental Medicine; Mathematics	254OJ	WOS:000252595600014	18058845	
J	Friedlander, MP; Tseng, P				Friedlander, Michael P.; Tseng, Paul			EXACT REGULARIZATION OF CONVEX PROGRAMS	SIAM JOURNAL ON OPTIMIZATION			English	Article						convex program; conic program; linear program; regularization; exact penalization; Lagrange multiplier; degeneracy; sparse solutions; interior-point algorithms		The regularization of a convex program is exact if all solutions of the regularized problem are also solutions of the original problem for all values of the regularization parameter below some positive threshold. For a general convex program, we show that the regularization is exact if and only if a certain selection problem has a Lagrange multiplier. Moreover, the regularization parameter threshold is inversely related to the Lagrange multiplier. We use this result to generalize an exact regularization result of Ferris and Mangasarian [Appl. Math. Optim., 23 (1991), pp. 266-273] involving a linearized selection problem. We also use it to derive necessary and sufficient conditions for exact penalization, similar to those obtained by Bertsekas [Math. Programming, 9 (1975), pp. 87-99] and by Bertsekas, Nedic, and Ozdaglar [Convex Analysis and Optimization, Athena Scientific, Belmont, MA, 2003]. When the regularization is not exact, we derive error bounds on the distance from the regularized solution to the original solution set. We also show that existence of a "weak sharp minimum" is in some sense close to being necessary for exact regularization. We illustrate the main result with numerical experiments on the l(1) regularization of benchmark (degenerate) linear programs and semidefinite/second-order cone programs. The experiments demonstrate the usefulness of l(1) regularization in finding sparse solutions.	[Friedlander, Michael P.] Univ British Columbia, Dept Comp Sci, Vancouver, BC V6T 1Z4, Canada; [Tseng, Paul] Univ Washington, Dept Math, Seattle, WA 98195 USA	Friedlander, MP (reprint author), Univ British Columbia, Dept Comp Sci, Vancouver, BC V6T 1Z4, Canada.	mpf@cs.ubc.ca; tseng@math.washington.edu			National Science and Engineering Council of Canada; National Science Foundation [DMS-0511283]	Department of Computer Science, University of British Columbia, Vancouver V6T 1Z4, BC, Canada (mpf@cs.ubc.ca). This author's research was supported by the National Science and Engineering Council of Canada.Department of Mathematics, University of Washington, Seattle, WA 98195 (tseng@math.washington.edu). This author's research was supported by National Science Foundation grant DMS-0511283.	Altman A, 1999, OPTIM METHOD SOFTW, V11-2, P275, DOI 10.1080/10556789908805754; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; BURKE JV, 1991, SIAM J CONTROL OPTIM, V29, P968, DOI 10.1137/0329054; MANGASARIAN OL, 1988, OPER RES LETT, V7, P21, DOI 10.1016/0167-6377(88)90047-8; Kanzow C, 2003, J OPTIMIZ THEORY APP, V116, P333, DOI 10.1023/A:1022457904979; Chen SSB, 2001, SIAM REV, V43, P129, DOI 10.1137/S003614450037906X; Candes EJ, 2006, IEEE T INFORM THEORY, V52, P489, DOI 10.1109/TIT.2005.862083; MANGASARIAN OL, 1979, SIAM J CONTROL OPTIM, V17, P745, DOI 10.1137/0317052; MANGASARIAN OL, 1984, MATH PROGRAM STUD, V22, P206; Efron B, 2004, ANN STAT, V32, P407; Candes EJ, 2006, COMMUN PUR APPL MATH, V59, P1207, DOI 10.1002/cpa.20124; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P6, DOI 10.1109/TIT.2005.860430; BACH FR, 2005, ADV NEURAL INFORM PR, V17; Ben-Tal A., 2001, MPS SIAM SER OPTIM, V2; Bertsekas D., 2003, CONVEX ANAL OPTIMIZA; Bertsekas D. P., 1982, CONSTRAINED OPTIMIZA; BERTSEKAS DP, 1975, MATH PROGRAM, V9, P87, DOI 10.1007/BF01681332; Bertsekas DP, 1999, COMPUT OPTIM APPL, V12, P41, DOI 10.1023/A:1008659512824; BURKE JV, 1993, SIAM J CONTROL OPTIM, V31, P1340, DOI 10.1137/0331063; Burke JV, 2005, MATH PROGRAM, V104, P235, DOI 10.1007/s10107-005-0615-2; CONN A. R., 2000, MPS SIAM SER OPTIM, V1; Donoho DL, 2003, P NATL ACAD SCI USA, V100, P2197, DOI 10.1073/pnas.0437847100; Donoho DL, 2005, P NATL ACAD SCI USA, V102, P9446, DOI 10.1073/pnas.0502269102; FERRIS MC, 1991, APPL MATH OPT, V23, P263, DOI 10.1007/BF01442401; Fletcher R., 1985, NUMERICAL OPTIMIZATI, P26; Fletcher R., 1987, PRACTICAL METHODS OP; GONZAGA CC, 2003, GENERATION DEGENERAT; HAN SP, 1979, MATH PROGRAM, V17, P251, DOI 10.1007/BF01588250; Lucidi S., 1987, Optimization, V18, DOI 10.1080/02331938708843296; LUCIDI S, 1987, J OPTIMIZ THEORY APP, V55, P103, DOI 10.1007/BF00939047; LUO ZQ, 1994, MATH PROGRAM, V67, P1, DOI 10.1007/BF01582210; Luo ZQ, 2000, MATH PROGRAM, V88, P221; Mangasarian OL, 2004, J OPTIMIZ THEORY APP, V121, P1, DOI 10.1023/B:JOTA.0000026128.34294.77; MANGASARIAN OL, 1985, SIAM J CONTROL OPTIM, V23, P30, DOI 10.1137/0323003; NESTEROV Y., 1994, SIAM STUD APPL MATH, V13; PATAKI G, 2002, DIMACS LIB IN PRESS; RENEGAR J., 2001, MPS SIAM SER OPTIM, V3; ROBINSON SM, 1984, MATH PROGRAM STUD, V22, P217; Rockafellar R.T., 1970, CONVEX ANAL; Sardy S, 2004, J AM STAT ASSOC, V99, P191, DOI 10.1198/016214504000000188; Saunders MA, 1996, SIAM PROC S, P92; Sturm J. F., 2001, USING SEDUMI 1 02 MA; Tikhonov AN, 1977, SOLUTIONS ILL POSED; Vandenberghe L., 2004, CONVEX OPTIMIZATION; Wu ZL, 2002, MATH PROGRAM, V92, P301, DOI 10.1007/s1401070100278; Ye Y., 1997, INTERIOR POINT ALGOR; Zhao YB, 2002, SIAM J OPTIMIZ, V12, P893, DOI 10.1137/S1052623401386368	47	27	28	0	1	SIAM PUBLICATIONS	PHILADELPHIA	3600 UNIV CITY SCIENCE CENTER, PHILADELPHIA, PA 19104-2688 USA	1052-6234	1095-7189		SIAM J OPTIMIZ	SIAM J. Optim.		2008	18	4					1326	1350		10.1137/060675320		25	Mathematics, Applied	Mathematics	V17MD	WOS:000207940400010		
B	Ma, B; McLoone, S; Ringwood, J; Macgearailt, N			IEEE	Ma, Beibei; McLoone, Sean; Ringwood, John; Macgearailt, Niall			Selecting Signature Optical Emission Spectroscopy Variables Using Sparse Principal Component Analysis	2008 11TH INTERNATIONAL CONFERENCE ON COMPUTER AND INFORMATION TECHNOLOGY: ICCIT 2008, VOLS 1 AND 2			English	Proceedings Paper	11th International Conference on Computer and Information Technology	DEC 24-27, 2008	Khulna, BANGLADESH	IEEE			REGRESSION; LASSO	Principal component analysis (PCA) is a widely used technique in optical emission spectroscopy (OES) sensor data analysis for the low dimension representation of high dimensional datasets. While PCA produces a linear combination of all the variables in each loading, sparse principal component analysis (SPCA) focuses on using a subset of variables in each loading. Therefore, SPCA can be used as a key variable selection technique. This paper shows that, using SPCA to analyze 2046 variable OES data sets, the number of selected variables can be traded off against variance explained to identifying a subset of key wavelengths, with an acceptable level of variance explained. SPCA-related issues such as selection of the tuning parameter and the grouping effect are discussed.	[Ma, Beibei; McLoone, Sean; Ringwood, John] Natl Univ Ireland, Dept Elect Engn, Maynooth, Kildare, Ireland	Ma, B (reprint author), Natl Univ Ireland, Dept Elect Engn, Maynooth, Kildare, Ireland.	beibei.ma@eeng.nuim.ie; sean.mcloone@eeng.nuim.ie; john.ringwood@eeng.nuim.ie; niall.macgearailt2@mail.dcu.ie					KAISER HF, 1958, PSYCHOMETRIKA, V23, P187, DOI 10.1007/BF02289233; d'Aspremont A, 2007, SIAM REV, V49, P434, DOI 10.1137/050645506; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; GELADI P, 1986, ANAL CHIM ACTA, V185, P1, DOI 10.1016/0003-2670(86)80028-9; Jolliffe IT, 2003, J COMPUT GRAPH STAT, V12, P531, DOI 10.1198/1061860032148; Zou H, 2005, J ROY STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x; CADIMA J, 1995, J APPL STAT, V22, P203, DOI 10.1080/757584614; Gervini D, 2004, AM STAT, V58, P72, DOI 10.1198/0003130042863; Jeffers JNR, 1967, APPLIED STATISTICS, V16, P225, DOI 10.2307/2985919; SJOSTRAND K, 2006, P SPIE, V6144; VINES SK, 2000, APPL STAT, V49, P441; Zou H, 2006, J COMPUT GRAPH STAT, V15, P265, DOI 10.1198/106186006X113430	12	0	0	1	1	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA			978-1-4244-2135-0				2008							21	26				6	Computer Science, Information Systems; Engineering, Electrical & Electronic	Computer Science; Engineering	BKB39	WOS:000267674500004		
S	Winken, M; Schwarz, H; Wiegand, T			IEEE	Winken, Martin; Schwarz, Heiko; Wiegand, Thomas			JOINT RATE-DISTORTION OPTIMIZATION OF TRANSFORM COEFFICIENTS FOR SPATIAL SCALABLE VIDEO CODING USING SVC	2008 15TH IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOLS 1-5	IEEE International Conference on Image Processing (ICIP)		English	Proceedings Paper	15th IEEE International Conference on Image Processing (ICIP 2008)	OCT 12-15, 2008	San Diego, CA	IEEE Signal Proc Soc		video coding; Scalable Video Coding (SVC); spatial scalability; rate distortion optimization	H.264/AVC	The recently finalized Scalable Video Coding (SVC) extension of the H.264/AVC video coding standard supports encoding of video sequences at different spatial resolutions within one single scalable bit-stream. In order to achieve an improved coding efficiency compared to simulcast, SVC allows so-called "inter-layer prediction," where texture, motion, and residual data of the lower resolution layer are exploited for the encoding of the higher resolution layer. In this paper, we extend the ideas presented in our previous work [1], and describe an approach how the dependencies caused by the inter-layer residual prediction can be utilized in the selection of the transform coefficient values for the lower resolution layer. Our experimental results show gains of 0.5 dB luma PSNR compared to our previous approach [1] for the high resolution layer with no impact on the base layer coding efficiency.	[Winken, Martin; Schwarz, Heiko; Wiegand, Thomas] HHI, Fraunhofer Inst Telecommun, Image Proc Dept, D-10587 Berlin, Germany	Winken, M (reprint author), HHI, Fraunhofer Inst Telecommun, Image Proc Dept, Einsteinufer 37, D-10587 Berlin, Germany.	winken@hhi.fraunhofer.de; hschwarz@hhi.fraunhofer.de; wiegand@hhi.fraunhofer.de					Tropp JA, 2006, IEEE T INFORM THEORY, V52, P1030, DOI 10.1109/TIT.2005.864420; Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; EVERETT H, 1963, OPER RES, V11, P399, DOI 10.1287/opre.11.3.399; Sarwer MG, 2007, IEEE T CIRC SYST VID, V17, P1402, DOI 10.1109/TCSVT.2007.903787; SCHUMITSCH B, 2005, P IVCP 2005 SAN JOS; SCHWARZ H, 2005, P ICIP 2005 GEN IT S; Segall CA, 2007, IEEE T CIRC SYST VID, V17, P1121, DOI 10.1109/TCSVT.2007.906824; Vandenberghe L., 2004, CONVEX OPTIMIZATION; Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P688, DOI 10.1109/TCSVT.2003.815168; WINKEN M, 2007, P ICIP 2007 SAN ANT	11	2	2	1	1	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA	1522-4880		978-1-4244-1765-0	IEEE IMAGE PROC			2008							1220	1223		10.1109/ICIP.2008.4711981		4	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic; Imaging Science & Photographic Technology	Computer Science; Engineering; Imaging Science & Photographic Technology	BJH64	WOS:000265921400306		
S	Yang, J; Tang, H; Ma, Y; Huang, T			IEEE	Yang, Jianchao; Tang, Hao; Ma, Yi; Huang, Thomas			FACE HALLUCINATION VIA SPARSE CODING	2008 15TH IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOLS 1-5	IEEE International Conference on Image Processing (ICIP)		English	Proceedings Paper	15th IEEE International Conference on Image Processing (ICIP 2008)	OCT 12-15, 2008	San Diego, CA	IEEE Signal Proc Soc		Super-resolution; face hallucination; non-negative matrix factorization; sparse representation; sparse coding		In this paper, we address the problem of hallucinating a high resolution face given a low resolution input face. The problem is approached through sparse coding. To exploit the facial structure, Non-negative Matrix Factorization (NMF) [1] is first employed to learn a localized part-based subspace. This subspace is effective for super-resolving the incoming low resolution face under reconstruction constraints. To further enhance the detailed facial information, we propose a local patch method based on sparse representation with respect to coupled overcomplete patch dictionaries, which can be fast solved through linear programming. Experiments demonstrate that our approach can hallucinate high quality super-resolution faces.	[Yang, Jianchao; Tang, Hao; Ma, Yi; Huang, Thomas] Univ Illinois, Dept Elect & Comp Engn, Urbana, IL 61801 USA	Yang, J (reprint author), Univ Illinois, Dept Elect & Comp Engn, 1406 W Green St, Urbana, IL 61801 USA.	jyang29@uiuc.edu; haotang2@uiuc.edu; yima@uiuc.edu; huang@uiuc.edu					Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Lee DD, 1999, NATURE, V401, P788; Baker S., 2000, IEEE INT C AUT FAC G; Capel D. P., 2001, THESIS U OXFORD; CHANG H, 2004, SUPER RESOLUTION NEI; Donoho D. L., 2004, MOST LARGE UNDERDETE; Donoho D. L., 2006, COMM PURE APPL MATH, V59; Freeman William T., 2002, IEEE COMPUTER GRAPHI, V22; Freeman W.T., 2000, LEARNING LOW LEVEL V; HARDIE RC, 1997, JOINT MAP REGISTRATI; IRANI M, 1993, MOTION ANAL IMAGE EN; Liu C, 2007, INT J COMPUT VISION, V75, P115, DOI 10.1007/s11263-006-0029-5; LIU W, 2006, P CVPR; Phillips P. J., 2005, P CVPR; SUN J, 2003, P CVPR	15	25	26	0	3	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA	1522-4880		978-1-4244-1765-0	IEEE IMAGE PROC			2008							1264	1267		10.1109/ICIP.2008.4711992		4	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic; Imaging Science & Photographic Technology	Computer Science; Engineering; Imaging Science & Photographic Technology	BJH64	WOS:000265921400317		
S	Marcia, RE; Kim, C; Kim, J; Brady, DJ; Willett, RM			IEEE	Marcia, Roummel E.; Kim, Changsoon; Kim, Jungsang; Brady, David J.; Willett, Rebecca M.			FAST DISAMBIGUATION OF SUPERIMPOSED IMAGES FOR INCREASED FIELD OF VIEW	2008 15TH IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOLS 1-5	IEEE International Conference on Image Processing (ICIP)		English	Proceedings Paper	15th IEEE International Conference on Image Processing (ICIP 2008)	OCT 12-15, 2008	San Diego, CA	IEEE Signal Proc Soc		Image reconstruction; Image sampling; Video cameras		Many infrared optical systems in wide-ranging applications such as surveillance and security frequently require large fields of view. Often this necessitates a focal plane array (FPA) with a large number of pixels, which, in general, is very expensive. In this paper, we propose a method for increasing the field of view without increasing the pixel resolution of the FPA by superimposing the multiple subimages within a scene and disambiguating the observed data to reconstruct the original scene. This technique, in effect, allows each subimage of the scene to share a single FPA, thereby increasing the field of view without compromising resolution. To disambiguate the subimages, we develop wavelet regularized reconstruction methods which encourage sparsity in the solution. We present results from numerical experiments that demonstrate the effectiveness of this approach.	[Marcia, Roummel E.; Kim, Changsoon; Kim, Jungsang; Brady, David J.; Willett, Rebecca M.] Duke Univ, Dept Elect & Comp Engn, Durham, NC 27708 USA	Marcia, RE (reprint author), Duke Univ, Dept Elect & Comp Engn, Durham, NC 27708 USA.		Willett, Rebecca/G-6930-2012				Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Uttal WR, 2000, VISION RES, V40, P301, DOI 10.1016/S0042-6989(99)00177-7; Bobin J, 2006, IEEE SIGNAL PROC LET, V13, P409, DOI 10.1109/LSP.2006.873141; CANDES E, 2006, IEEE T INFO IN PRESS; Donoho D., 2006, FAST SOLUTION L1 NOR; FIGUEIREDO MAT, IEEE J SELE IN PRESS	7	1	1	1	1	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA	1522-4880		978-1-4244-1765-0	IEEE IMAGE PROC			2008							2620	2623		10.1109/ICIP.2008.4712331		4	Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic; Imaging Science & Photographic Technology	Computer Science; Engineering; Imaging Science & Photographic Technology	BJH64	WOS:000265921401172		
B	Bajwa, WU; Haupt, J; Raz, G; Nowak, R			IEEE	Bajwa, Waheed U.; Haupt, Jarvis; Raz, Gil; Nowak, Robert			Compressed channel sensing	2008 42ND ANNUAL CONFERENCE ON INFORMATION SCIENCES AND SYSTEMS, VOLS 1-3			English	Proceedings Paper	42nd Annual Conference on Information Sciences and Systems	MAR 19-21, 2008	Princeton, NJ	IEEE				Reliable wireless communications often requires accurate knowledge of the underlying multipath channel. This typically involves probing of the channel with a known training waveform and linear processing of the input probe and channel output to estimate the impulse response. Many real-world channels of practical interest tend to exhibit impulse responses characterized by a relatively small number of nonzero channel coefficients. Conventional linear channel estimation strategies, such as the least squares, are ill-suited to fully exploiting the inherent low-dimensionality of these sparse channels. In contrast, this paper proposes sparse channel estimation methods based on convex/linear programming. Quantitative error bounds for the proposed schemes are derived by adapting recent advances from the theory of compressed sensing. The bounds come within a logarithmic factor of the performance of an ideal channel estimator and reveal significant advantages of the proposed methods over the conventional channel estimation schemes.	[Bajwa, Waheed U.; Haupt, Jarvis; Nowak, Robert] Univ Wisconsin, Dept Elect & Comp Engn, Madison, WI 53706 USA	Bajwa, WU (reprint author), Univ Wisconsin, Dept Elect & Comp Engn, 1415 Johnson Dr, Madison, WI 53706 USA.						Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Molisch AF, 2005, IEEE T VEH TECHNOL, V54, P1528, DOI 10.1109/TVT.2005.856194; Bajwa W., 2007, P 14 IEEE SP WORKSH, P294; BELLO PA, 1963, IEEE T COMMUN, P360; BICKEL P, SIMULTANEOUS ANAL LA; CANDES E, 2007, ANN STAT         DEC, P2313; CARBONELLI C, 2007, IEEE T WIRELESS  MAY, P1743; Cotter SF, 2002, IEEE T COMMUN, V50, P374, DOI 10.1109/26.990897; CZINK N, 2007, IEEE T WIRELESS  APR, P1465; DONOHO D, 1994, BIOMETRIKA       DEC, P425; FIGUEIREDO MAT, 2007, IEEE J SELECT TO DEC, P586; Hajnal A., 1970, COMBINATORIAL THEORY, P601; HERMAN M, HIGH RESOLUTION RADA; Kilfoyle D., 2000, IEEE J OCEANIC E JAN, P4; PFANDER GE, IDENTIFICATION MATRI; Saleh A., 1987, IEEE J SEL AREA COMM, P128; Sayeed AM, 1999, IEEE T COMMUN, V47, P123, DOI 10.1109/26.747819; SELEN Y, 2007, IEEE T WIRELESS  SEP, P3175; VUOKKO L, 2007, IEEE T ANTENNAS  NOV, P3361; 2004, RECEIVER PERFORMANCE	20	32	32	1	6	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA			978-1-4244-2246-3				2008							5	10		10.1109/CISS.2008.4558485		6	Computer Science, Information Systems; Computer Science, Theory & Methods; Engineering, Electrical & Electronic	Computer Science; Engineering	BIE60	WOS:000258931600002		
B	Daubechies, I; DeVore, R; Fornasier, M; Gunturk, S			IEEE	Daubechies, Ingrid; DeVore, Ronald; Fornasier, Massimo; Guentuerk, Sinan			Iteratively Re-weighted Least Squares minimization: Proof of faster than linear rate for sparse recovery	2008 42ND ANNUAL CONFERENCE ON INFORMATION SCIENCES AND SYSTEMS, VOLS 1-3			English	Proceedings Paper	42nd Annual Conference on Information Sciences and Systems	MAR 19-21, 2008	Princeton, NJ	IEEE			SIGNAL RECOVERY; UNCERTAINTY PRINCIPLES	Given an m x N matrix Phi, with m N, the system of equations Phi x = y is typically underdetermined and has infinitely many solutions. Various forms of optimization can extract a "best" solution. One of the oldest is to select the one with minimal l(2) norm. It has been shown that in many applications a better choice is the minimal E, norm solution. This is the case in Compressive Sensing, when sparse solutions are sought. The minimal l(1) norm solution can be found by using linear programming; an alternative method is Iterative Re-weighted Least Squares (IRLS), which in some cases is numerically faster. The main step of IRLS finds, for a given weight w, the solution with smallest l(2)(w) norm; this weight is updated at every iteration step: if x((n)) is the solution at step n, then w((n)) is defined by w(i)((n)) := 1/vertical bar x(i)((n))vertical bar, i = 1,...,N. We give a specific recipe for updating weights that avoids technical shortcomings in other approaches, and for which we can prove convergence under certain conditions on the matrix Phi known as the Restricted Isometry Property. We also show that if there is a sparse solution, then the limit of the proposed algorithm is that sparse solution. It is also shown that whenever the solution at a given iteration is sufficiently close to the limit, then the remaining steps of the algorithm converge exponentially fast. In the standard version of the algorithm, designed to emulate l(1) minimization, the exponenital rate is linear; in adapted versions aimed at l(tau)-minimization with tau < 1, we prove faster than linear rate.	[Daubechies, Ingrid] Princeton Univ, Princeton, NJ 08544 USA	Daubechies, I (reprint author), Princeton Univ, Princeton, NJ 08544 USA.		Daubechies, Ingrid/B-5886-2012				Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Candes EJ, 2006, IEEE T INFORM THEORY, V52, P489, DOI 10.1109/TIT.2005.862083; Candes EJ, 2006, IEEE T INFORM THEORY, V52, P5406, DOI 10.1109/TIT.2006.885507; Chambolle A, 1997, NUMER MATH, V76, P167, DOI 10.1007/s002110050258; Candes EJ, 2005, IEEE T INFORM THEORY, V51, P4203, DOI 10.1109/TIT.2005.858979; Candes EJ, 2006, COMMUN PUR APPL MATH, V59, P1207, DOI 10.1002/cpa.20124; BARANIUK R, JOHNSONLIDENSTRAUSS; Baraniuk Richard G., 2007, COMPRESSIVE SENSING; Candes E. J., 2006, INT C MATH, V3, P1433; Candes EJ, ENHANCING SPARSITY R; Chen S, 1998, SIAM J SCI COMPUT, V1, P33; CLAERBOU.JF, 1973, GEOPHYSICS, V38, P826, DOI 10.1190/1.1440378; CLINE AK, 1972, MATH COMPUT, V26, P167, DOI 10.2307/2004726; Cohen A., COMPRESSED SENSING B; DeVore R. A., 1998, Acta Numerica, V7, DOI 10.1017/S0962492900002816; DOHOHO DL, 2006, COUNTING FACES RANDO; Donoho D., 2006, FAST SOLUTION L1 NOR; DONOHO DL, 1989, SIAM J APPL MATH, V49, P906, DOI 10.1137/0149053; Donoho DL, 2005, P NATL ACAD SCI USA, V102, P9446, DOI 10.1073/pnas.0502269102; Donoho D.L., 1992, SIAM J APPL MATH, V52, P557; Donoho DL, 2006, DISCRETE COMPUT GEOM, V35, P617, DOI 10.1007/s00454-005-1220-0; Fornasier M, 2007, SIAM J APPL MATH, V68, P437, DOI 10.1137/060671875; Gribonval R, 2007, APPL COMPUT HARMON A, V22, P335, DOI 10.1016/j.acha.2006.09.003; Lawson C L, 1961, THESIS U CALIFORNIA; Li YY, 1993, SIAM J OPTIMIZ, V3, P609, DOI 10.1137/0803030; OSBORNE M. R., 1985, FINITE ALGORITHMS OP; Pinkus A. M., 1989, L1 APPROXIMATION, V93; RUDELSON M, COMMUNICATI IN PRESS; SANTOSA F, 1986, SIAM J SCI STAT COMP, V7, P1307, DOI 10.1137/0907087; TAYLOR HL, 1979, GEOPHYSICS, V44, P39, DOI 10.1190/1.1440921	31	8	8	1	2	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA			978-1-4244-2246-3				2008							26	29		10.1109/CISS.2008.4558489		4	Computer Science, Information Systems; Computer Science, Theory & Methods; Engineering, Electrical & Electronic	Computer Science; Engineering	BIE60	WOS:000258931600006		
S	Yardibi, T; Li, J; Stoica, P		Matthews, MB		Yardibi, Tarik; Li, Jian; Stoica, Petre			Nonparametric and Sparse Signal Representations in Array Processing via Iterative Adaptive Approaches	2008 42ND ASILOMAR CONFERENCE ON SIGNALS, SYSTEMS AND COMPUTERS, VOLS 1-4	Conference Record of the Asilomar Conference on Signals Systems and Computers		English	Proceedings Paper	42nd Asilomar Conference on Signals, Systems and Computers	OCT 26-29, 2008	Pacific Grove, CA	Naval Postgrad Sch, ATK Mission Res, IEEE Signal Proc Soc			SELECTION; NOISE; MINIMIZATION; RELAX	This paper addresses the problem of source localization and waveform estimation in array processing applications. We present two nonparametric user parameter free algorithms, namely the iterative adaptive approach (IAA) and the maximum likelihood based IAA (IAA-ML). Both IAA and IAA-ML can work with arbitrary array geometries and uncorrelated as well as coherent sources. We extend IAA and IAA-ML to yield sparse results by, using the Bayesian information criterion (BIC). Further improvements in performance can be achieved by employing the last step of the parametric RELAX algorithm initialized by the estimates of IAA and IAA-ML with BIC. Our simulations demonstrate that IAA and IAA-ML show better performance than the most competitive sparse signal representation approaches in the literature: M-FOCUSS, M-SBL and l(1)-SVD.	[Yardibi, Tarik; Li, Jian] Univ Florida, Dept Elect & Comp Engn, Gainesville, FL 32611 USA	Li, J (reprint author), Univ Florida, Dept Elect & Comp Engn, Gainesville, FL 32611 USA.	ytarik@dsp.ufl.edu; li@dsp.ufl.edu; ps@it.uu.se					Tropp JA, 2006, IEEE T INFORM THEORY, V52, P1030, DOI 10.1109/TIT.2005.864420; Malioutov D, 2005, IEEE T SIGNAL PROCES, V53, P3010, DOI 10.1109/TSP.2005.850882; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582; Rao BD, 2003, IEEE T SIGNAL PROCES, V51, P760, DOI 10.1109/TSP.2002.808076; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Wipf DP, 2004, IEEE T SIGNAL PROCES, V52, P2153, DOI 10.1109/TSP.2004.831016; Li J, 1996, IEEE T SIGNAL PROCES, V44, P1469; CAPON J, 1969, P IEEE, V57, P1408, DOI 10.1109/PROC.1969.7278; Stoica P, 2004, IEEE SIGNAL PROC MAG, V21, P36, DOI 10.1109/MSP.2004.1311138; Baraniuk RG, 2007, IEEE SIGNAL PROC MAG, V24, P118, DOI 10.1109/MSP.2007.4286571; Tipping ME, 2001, J MACH LEARN RES, V1, P211, DOI 10.1162/15324430152748236; Wipf DP, 2007, IEEE T SIGNAL PROCES, V55, P3704, DOI 10.1109/TSP.2007.894265; Chen Y, 1998, NONCON OPTIM ITS APP, V20, P1; Cotter RJ, 2005, J MASS SPECTROM SOC, V53, P7; Donoho DL, 2003, P NATL ACAD SCI USA, V100, P2197, DOI 10.1073/pnas.0437847100; Figueiredo MAT, 2003, IEEE T PATTERN ANAL, V25, P1150, DOI 10.1109/TPAMI.2003.1227989; Fuchs JJ, 2005, IEEE T INFORM THEORY, V51, P3601, DOI 10.1109/TIT.2005.855614; Fuchs JJ, 2004, IEEE T INFORM THEORY, V50, P1341, DOI 10.1109/TIT.2004.828141; Li J, 2006, ROBUST ADAPTIVE BEAMFORMING, P1; Li J, 1997, IEEE T AERO ELEC SYS, V33, P1077; Li J, 1996, IEEE T SIGNAL PROCES, V44, P281; Kreutz-Delgado K, 2003, NEURAL COMPUT, V15, P349, DOI 10.1162/089976603762552951; Soderstrom T., 1989, SYSTEM IDENTIFICATIO; Stoica P., 2005, SPECTRAL ANAL SIGNAL; Stoica P, 1999, IEEE SIGNAL PROC LET, V6, P205, DOI 10.1109/97.774866; Van Trees H. L., 2002, OPTIMUM ARRAY PROC 4	26	1	1	1	1	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA	1058-6393		978-1-4244-2940-0	CONF REC ASILOMAR C			2008							278	282		10.1109/ACSSC.2008.5074408		5	Computer Science, Interdisciplinary Applications; Engineering, Electrical & Electronic; Telecommunications	Computer Science; Engineering; Telecommunications	BNH24	WOS:000274551000052		
S	Asif, MS; Romberg, J		Matthews, MB		Asif, M. Salman; Romberg, Justin			Streaming Measurements in Compressive Sensing: l(1) Filtering	2008 42ND ASILOMAR CONFERENCE ON SIGNALS, SYSTEMS AND COMPUTERS, VOLS 1-4	Conference Record of the ssilomar Conference on Signals Systems and Computers		English	Proceedings Paper	42nd Asilomar Conference on Signals, Systems and Computers	OCT 26-29, 2008	Pacific Grove, CA	Naval Postgrad Sch, ATK Mission Res, IEEE Signal Proc Soc		Homotopy; Lasso; BPDN; l1 decoding; compressed sensing; dynamic measurement update	LEAST-SQUARES; RECONSTRUCTION; REGRESSION; SELECTION	The central framework for signal recovery in compressive sensing is l(1) norm minimization. In recent years, tremendous progress has been made on algorithms, typically based on some kind of gradient descent or Newton iterations, for performing l(1) norm minimization. These algorithms, however, are for the most part "static": they focus on finding the solution for a fixed set of measurements. In this paper, we will present a method for quickly updating the solution to some l(1) norm minimization problems as new measurements are added. The result is an "l(1) filter" and can be implemented using standard techniques from numerical linear algebra. Our proposed scheme is homotopy based where we add new measurements in the system and instead of solving updated problem directly, we solve a series of simple (easy to solve) intermediate problems which lead to the desired solution.	[Asif, M. Salman; Romberg, Justin] Georgia Inst Technol, Sch Elect & Comp Engn, Atlanta, GA 30332 USA	Asif, MS (reprint author), Georgia Inst Technol, Sch Elect & Comp Engn, Atlanta, GA 30332 USA.	sasif@ece.gatech.edu; jrom@ece.gatech.edu					Kim SJ, 2007, IEEE J-STSP, V1, P606, DOI 10.1109/JSTSP.2007.910971; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Candes E, 2007, ANN STAT, V35, P2313, DOI 10.1214/009053606000001523; Candes EJ, 2006, IEEE T INFORM THEORY, V52, P489, DOI 10.1109/TIT.2005.862083; Osborne MR, 2000, IMA J NUMER ANAL, V20, P389, DOI 10.1093/imanum/20.3.389; Figueiredo MAT, 2007, IEEE J-STSP, V1, P586, DOI 10.1109/JSTSP.2007.910281; Candes EJ, 2005, IEEE T INFORM THEORY, V51, P4203, DOI 10.1109/TIT.2005.858979; Efron B, 2004, ANN STAT, V32, P407; Asif M., 2008, THESIS GEORGIA I TEC; Candes E., 2006, P INT C MATH, V3, P1433; Candes EJ, 2008, IEEE T INFORM THEORY, V54, P2829, DOI 10.1109/TIT.2008.924688; Chen S. S., 1999, SIAM J SCI COMPUT, V20, P33; Fuchs JJ, 2004, IEEE T INFORM THEORY, V50, P1341, DOI 10.1109/TIT.2004.828141; GARRIGUES PJ, 2008, NEURAL INFO IN PRESS, V21; Golub G., 1996, MATRIX COMPUTATIONS; Hayes M. H., 1996, STAT DIGITAL SIGNAL; Malioutov D. M., 2005, IEEE INT C AC SPEECH, VV, P733; Rudelson M., 2008, ADV MATH; Vandenberghe L., 2004, CONVEX OPTIMIZATION; Vanderbei RJ, 2001, LINEAR PROGRAMMING F	21	0	1	1	3	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA	1058-6393		978-1-4244-2940-0	CONF REC ASILOMAR C			2008							1051	1058		10.1109/ACSSC.2008.5074573		8	Computer Science, Interdisciplinary Applications; Engineering, Electrical & Electronic; Telecommunications	Computer Science; Engineering; Telecommunications	BNH24	WOS:000274551000200		
B	Obozinski, G; Wainwright, MJ; Jordan, MI			IEEE	Obozinski, Guillaume; Wainwright, Martin J.; Jordan, Michael I.			Union support recovery in high-dimensional multivariate regression	2008 46TH ANNUAL ALLERTON CONFERENCE ON COMMUNICATION, CONTROL, AND COMPUTING, VOLS 1-3			English	Proceedings Paper	46th Annual Allerton Conference on Communication, Control and Computing	SEP, 2008	Monticello, IL				ATOMIC DECOMPOSITION; VARIABLE SELECTION; LASSO	In the problem of multivariate regression, a K-dimensional response vector is regressed upon a common set of p covariates, with a matrix B* is an element of R(p x K) of regression coefficients. We study the behavior of the group Lasso using l(1)/l(2) regularization for the union support problem, meaning that the set of s rows for which B* is non-zero is recovered exactly. Studying this problem under high-dimensional scaling, we show that group Lasso recovers the exact row pattern with high probability over the random design and noise for scalings of (n, p, s) such that the sample complexity parameter given by theta(n, p, s) : = n/[2 psi(B*) log(p - s)] exceeds a critical threshold. Here n is the sample size, p is the ambient dimension of the regression model, s is the number of non-zero rows, and psi(B*) is a sparsity-overlap function that measures a combination of the sparsities and overlaps of the K-regression coefficient vectors that constitute the model. This sparsity-overlap unction reveals that, if the design is uncorrelated on the active rows, block l(1)/l(2) regularization for multivariate regression never harms performance relative to an ordinary Lasso approach, and can yield substantial improvements in sample complexity (up to a factor of K) when the regression vectors are suitably orthogonal. For more general designs, it is possible for the ordinary Lasso to outperform the group Lasso.	[Obozinski, Guillaume] Univ Calif Berkeley, Dept Stat, Berkeley, CA 94720 USA	Obozinski, G (reprint author), Univ Calif Berkeley, Dept Stat, Berkeley, CA 94720 USA.	gobo@stat.berkeley.edu; wainwrig@stat.berkeley.edu; jordan@stat.berkeley.edu					Tropp JA, 2006, IEEE T INFORM THEORY, V52, P1030, DOI 10.1109/TIT.2005.864420; Knight K, 2000, ANN STAT, V28, P1356; Donoho DL, 2001, IEEE T INFORM THEORY, V47, P2845, DOI 10.1109/18.959265; Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Zhao P, 2006, J MACH LEARN RES, V7, P2541; Candes E, 2007, ANN STAT, V35, P2313, DOI 10.1214/009053606000001523; Osborne MR, 2000, IMA J NUMER ANAL, V20, P389, DOI 10.1093/imanum/20.3.389; Meier L, 2008, J R STAT SOC B, V70, P53; Efron B, 2004, ANN STAT, V32, P407; Meinshausen N, 2006, ANN STAT, V34, P1436, DOI 10.1214/009053606000000281; Argyriou A., 2006, ADV NEURAL INFORM PR, V18; BACH F, 2008, CONSISTENCY GROUP LA; BACH FR, 2004, P INT C MACH LEARN I; Buldygin V. V., 2000, METRIC CHARACTERIZAT; LIU H, 2008, ARXIV08021517V1 CARN; MEINSHAUSEN N, 2008, ANN STAT; Obozinski G., 2007, 743 U CAL DEP STAT; Obozinski G., 2008, UNION SUPPORT RECOVE; RAVIKUMAR P, 2008, ARXIV07114555V2 CARN; Turlach B., 2005, TECHNOMETRICS, V27, P349; Vandenberghe L., 2004, CONVEX OPTIMIZATION; Wainwright M., 2006, 709 UC BERK DEP STAT; YUAN M, 2006, J ROYAL STAT SOC B, V1, P4967; ZHANG H, 2008, ELECTRON J STAT, V2, P1149; ZHAO P, 2007, 703 U CAL STAT DEP	26	0	0	1	2	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA			978-1-4244-2925-7				2008							21	26		10.1109/ALLERTON.2008.4797530		6	Automation & Control Systems; Computer Science, Interdisciplinary Applications; Engineering, Electrical & Electronic; Telecommunications	Automation & Control Systems; Computer Science; Engineering; Telecommunications	BKI79	WOS:000268229600004		
B	Xu, H; Mannor, S; Caramanis, C			IEEE	Xu, Huan; Mannor, Shie; Caramanis, Constantine			Sparse Algorithms are not Stable: A No-free-lunch Theorem	2008 46TH ANNUAL ALLERTON CONFERENCE ON COMMUNICATION, CONTROL, AND COMPUTING, VOLS 1-3			English	Proceedings Paper	46th Annual Allerton Conference on Communication, Control and Computing	SEP, 2008	Monticello, IL				SELECTION	We consider two widely used notions in machine learning, namely: sparsity and algorithmic stability. Both notions are deemed desirable in designing algorithms, and are believed to lead to good generalization ability. In this paper, we show that these two notions contradict each other. That is, a sparse algorithm can not be stable and vice versa. Thus, one has to tradeoff sparsity and stability in designing a learning algorithm. In particular, our general result implies that l(1)-regularized regression (Lasso) cannot be stable, while l(2)-regularized regression is known to have strong stability properties.	[Xu, Huan; Mannor, Shie] McGill Univ, Dept Elect & Comp Engn, Montreal, PQ, Canada	Xu, H (reprint author), McGill Univ, Dept Elect & Comp Engn, Montreal, PQ, Canada.	xuhuan@cim.mcgill.ca; shie.mannor@mcgill.ca; caramanis@mail.utexas.edu	Xu, Huan/M-5155-2014				d'Aspremont A, 2007, SIAM REV, V49, P434, DOI 10.1137/050645506; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Candes EJ, 2006, IEEE T INFORM THEORY, V52, P489, DOI 10.1109/TIT.2005.862083; MALLAT SG, 1993, IEEE T SIGNAL PROCES, V41, P3397, DOI 10.1109/78.258082; Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647; Bousquet O, 2002, J MACH LEARN RES, V2, P499, DOI 10.1162/153244302760200704; COIFMAN RR, 1992, IEEE T INFORM THEORY, V38, P713, DOI 10.1109/18.119732; Poggio T, 1998, NEURAL COMPUT, V10, P1445, DOI 10.1162/089976698300017250; Steinwart I, 2005, IEEE T INFORM THEORY, V51, P128, DOI 10.1109/TIT.2004.839514; Zhu J., 2003, ADV NEURAL INFORM PR, V16	11	3	3	0	2	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA			978-1-4244-2925-7				2008							1299	1303				5	Automation & Control Systems; Computer Science, Interdisciplinary Applications; Engineering, Electrical & Electronic; Telecommunications	Automation & Control Systems; Computer Science; Engineering; Telecommunications	BKI79	WOS:000268229600184		
B	Mendez-Vazquez, A; Gader, P			IEEE	Mendez-Vazquez, Andres; Gader, Paul			Learning fuzzy measure parameters by logistic LASSO	2008 ANNUAL MEETING OF THE NORTH AMERICAN FUZZY INFORMATION PROCESSING SOCIETY, VOLS 1 AND 2			English	Proceedings Paper	Annual Meeting of the North-American-Fuzzy-Information-Processing-Society	MAY 19-22, 2008	New York, NY	N Amer Fuzzy Informat Proc Soc		fuzzy measures; Choquet integral; logistic regression; laplacian distribution; Gibbs sampler	CLASSIFICATION; STATISTICS; NETWORKS; MINE	In this paper, a novel Bayesian hierarchical method is defined by the use of logistic distribution and a Laplacian prior to learn the parameters on fuzzy measures. The new algorithm goes beyond previously published NICE based approaches. It has the advantage that it is applicable to general measures, as opposed to only the Sugeno class of measures. In addition, the monotonicity constraints are handled easily with minimal time or storage requirements. This is made by the use of an alternated sampling to avoid favoring max-like operators or min-like operators. The use of the logistic distribution eliminates the necessity of using desired outputs, and the Laplacian prior regularize the parameters in the fuzzy measures. Results are given on synthetic and real data sets, the latter obtained from a landmine detection problem.	[Mendez-Vazquez, Andres; Gader, Paul] Univ Florida, Dept Comp & Informat Sci & Engn, Gainesville, FL 32611 USA	Mendez-Vazquez, A (reprint author), Univ Florida, Dept Comp & Informat Sci & Engn, POB 116120, Gainesville, FL 32611 USA.						Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Auephanwiriyakul S., 2002, Information Fusion, V3, DOI 10.1016/S1566-2535(01)00054-9; Casella G., 2002, STAT INFERENCE; Chiang JH, 1999, IEEE T FUZZY SYST, V7, P63; Choquet G., 1953, ANN I FOURIER GRENOB, V5, P131; Efron Bradley, 2004, LEAST ANGLE REGRESSI; Frigui H., 2006, P IEEE INT C FUZZ SY; FRIGUI H, 2005, EURASIP J APPL SIG P, P1867; Gader P, 2004, IEEE T GEOSCI REMOTE, V42, P2522, DOI 10.1109/TGRS.2004.837333; Gader P., 2004, IEEE INT C FUZZ SYST, P523; GADER PD, 2000, NEURO FUZZY PATTERN, P205; GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721; Gill P. E., 1981, PRACTICAL OPTIMIZATI; GRABISCH M, 1992, IEEE INTERNATIONAL CONFERENCE ON FUZZY SYSTEMS, P47, DOI 10.1109/FUZZY.1992.258678; Grabisch M, 2000, FUZZY MEASURES INTEG; Grabisch M, 1995, FUNDAMENTALS UNCERTA; Grabisch M., 1995, 4 IEEE INT C FUZZ SY, P145; GRABISCH M, 1994, FUZZY SET SYST, V65, P255, DOI 10.1016/0165-0114(94)90023-X; HO KC, IEEE T GEOS IN PRESS; Keller JM, 1996, INT J APPROX REASON, V15, P1, DOI 10.1016/0888-613X(95)00132-Z; KIM S, 2003, FUZZY INFORM PROCESS, P143; Kim Y., 2004, ICML 04, P60; Komarek P., 2004, THESIS CARNEGIE MELL; Lee WH, 2007, IEEE T GEOSCI REMOTE, V45, P389, DOI 10.1109/TGRS.2006.887018; MENDEZVAZQUEZ A, 2007, IEEE T IN PRESS  JAN; MENDEZVAZQUEZ A, 2007, THESIS U FLORIDA; Perkins S., 2003, Journal of Machine Learning Research, V3, DOI 10.1162/153244303322753698; Robert C. P., 2005, SPRINGER TEXTS STAT; SO Y, 1999, TUTORIAL LOGISTIC RE; Sugeno M., 1974, THESIS TOKYO I TECHN; WALSH B, 2004, M CHAIN M CARLO GIBB; Wang D., 1997, P NAFIPS 97 SYR NY S, P263; Wang J, 1997, NEURAL NETWORKS, V10, P183; Wang W, 1998, J INTELL FUZZY SYST, V6, P171; Wang Z., 1993, FUZZY MEASURE THEORY; WILSON JN, 2007, IEEE T GEOSCI REMOTE, P2560; Xu KB, 2003, IEEE T FUZZY SYST, V11, P187, DOI 10.1109/TFUZZ.2003.809891; Zhao YX, 2003, IEEE T GEOSCI REMOTE, V41, P1016, DOI 10.1109/TGRS.2003.811761	38	0	0	0	2	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA			978-1-4244-2351-4				2008							348	354				7	Computer Science, Artificial Intelligence; Computer Science, Information Systems; Computer Science, Theory & Methods; Engineering, Electrical & Electronic	Computer Science; Engineering	BIC06	WOS:000258322800063		
S	Zhang, X; Liang, L; Tang, XO; Shum, HY			IEEE	Zhang, Xiao; Liang, Lin; Tang, Xiaoou; Shum, Heung-Yeung			L-1 regularized projection pursuit for additive model learning	2008 IEEE CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, VOLS 1-12	PROCEEDINGS - IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION		English	Proceedings Paper	IEEE Conference on Computer Vision and Pattern Recognition	JUN 23-28, 2008	Anchorage, AK	IEEE Comp Soc			REGRESSION	In this paper, we present a L, regularized projection pursuit algorithm for additive model learning. Two new algorithms are developed for regression and classification respectively: sparse projection pursuit regression and sparse Jensen-Shannon Boosting. The introduced L, regularized projection pursuit encourages sparse solutions, thus our new algorithms are robust to overfitting and present better generalization ability especially in settings with many irrelevant input features and noisy data. To make the optimization with L, regularization more efficient, we develop an "informative feature first" sequential optimization algorithm. Extensive experiments demonstrate the effectiveness of our proposed approach.	[Zhang, Xiao] Tsinghua Univ, Ctr Adv Study, Beijing 100084, Peoples R China	Zhang, X (reprint author), Tsinghua Univ, Ctr Adv Study, Beijing 100084, Peoples R China.		Pan, Linqiang/E-4714-2011; Tang, Xiaoou/G-6509-2012	Pan, Linqiang/0000-0002-4554-455X; 			FRIEDMAN JH, 1981, J AM STAT ASSOC, V76, P817, DOI 10.2307/2287576; [Anonymous], 2001, CVPR, DOI DOI 10.1109/CVPR.2001.990517; Friedman J, 2000, ANN STAT, V28, P337, DOI 10.1214/aos/1016218223; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267; Gooch B, 2004, ACM T GRAPHIC, V23, P27, DOI 10.1145/966131.966133; HORNIK K, 1989, NEURAL NETWORKS, V2, P359, DOI 10.1016/0893-6080(89)90020-8; Donoho DL, 2006, COMMUN PUR APPL MATH, V59, P797, DOI 10.1002/cpa.20132; BRENNAN SE, 1985, LEONARDO, V18, P170, DOI 10.2307/1578048; CHEN H, 2002, ACM MULTIMEDIA, P171; Hastie T, 1986, STAT SCI, V1, P297, DOI DOI 10.1214/SS/1177013604; HASTIE T, 2001, ELEMENTS RESPONSE BE; HUANG X, 2005, CVPR, P144; Jain A., 2004, FGR, P159; KEGL B, 2004, NIPS; KIM Y, 2004, ICML, V69; KOSHIMIZU H, 1999, KANSEI FACIAL PROCES, V6, P294; Liang L., 2002, P 10 PAC C COMP GRAP, P386; LIU C, 2004, CVPR, P587; MARTINEZ A, 1998, 24 CVC UAB; MASON JBL, 1999, NIPS, P512; MOGHADDAM B, 2002, PAMI, V25, P707; Ng AY, 2004, ICML; Perkins S., 2003, Journal of Machine Learning Research, V3, DOI 10.1162/153244303322753698; Phillips PJ, 1998, IMAGE VISION COMPUT, V16, P295, DOI 10.1016/S0262-8856(97)00070-X; Ravikumar P., 2007, NIPS; VERSCHAE R, 2006, IB C PATT REC, P68; ZHONG H, 2005, EXAMPLE BASED FACE C	27	1	2	0	1	IEEE	NEW YORK	345 E 47TH ST, NEW YORK, NY 10017 USA	1063-6919		978-1-4244-2242-5	PROC CVPR IEEE			2008							117	124				8	Computer Science, Artificial Intelligence; Imaging Science & Photographic Technology	Computer Science; Imaging Science & Photographic Technology	BII46	WOS:000259736800016		
