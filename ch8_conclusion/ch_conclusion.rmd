
\chapter{ Out of the Data}
\label{ch:conclusion}



```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=12, fig.height=8, 
                      echo=FALSE, warning=FALSE, message=FALSE, dev='pdf')
library(RSQLite)
con <- dbConnect(RSQLite::SQLite(),'../ml_lit/all_refs.sqlite3')
res = dbGetQuery(con, statement ="select * from basic_refs limit 10;")
```

## overview

 - how many things I couldn't include in the book
 - what it felt to write it -- working with all the code and various other problems
 - how it remained a kind of handicraft, not a social labour
 - numbers, infrastructures, mathematical, abstraction, 

## todo

- add morton on hyperobjects
- add Golumbia on computationalism?
- add the inceptionism stuff -- unsettling to look at the world in this way? see http://www.cs.toronto.edu/~graves/handwriting.cgi?text=&style=&bias=0.75&samples=3
- inceptionism as the complement of kittydar?
- present what I've been doing as an account of a transformation in human-machine relations; something that transforms scale, agency, dispersion, etc;
- return to 'into' and 'out' of the data: so many attempts to get things out of the data; little effort to puts things into the data;
- what if this book was a model -- what kind of model would it be
- overview of the parts, and the chapters
- the media-science-government
- I use media as shorthand for media in general: they are things or infrastructures, which are also things, and things are media (Lash & Lury)
- why I haven't done critique of surveillance, etc? Or of the hype?
- what does it add to have this concrete account of abstraction?
- what does it add to have followed some of the infrastructures, implementations, etc 
- what happened to the affective/psychoanalytic/movement in thought?
- the praxiography -- writing code as practice that allows something about movement to be contoured and perhaps countered. 


## text from proposal

The conclusion draws together the main threads running through the previous chapter, and sets out a series of questions and provocations for thinking with data. Crucially, the conclusion will stand back from the much more hands-on approach to data and data practice adopted in the preceding chapters in order to think more about we -- social scientists, humanities scholars -- might invent or create in the midst of data. While this book has a critical angle to it (so many claims about and beliefs in data plainly deserve critique for their conservative and naive approach to things), it is principally concerned with conceptual invention through doing things with data. The work of learning about machine learning, and learning about it in a way that is deeply embodied or practically embodied, brings with it altered ways of thinking about, questioning and integrating what is happening to data more generally. It highlights the key argument that has run through the book about the plural dimensionality of data as it is aggregated, tabulated, summarised and modelled in contemporary data and signal processes, and as well as the extraordinary mobility or kinetic energy of generic machine learning methods. In discussing the shifting dimensionality of data, and the kinetics of methods, the conclusion will attempt to sketch out how some promising ways of thinking with data might proceed. 

## If this book were a predictive model, what would it predict?

What if this book worked machinically? The primary empirical material on which almost everything that precedes this draws on is not unusual. I've read articles and books, downloaded data and software libraries, watched Youtube lectures and presentations, configured and written bits of code and text, made plots and diagrams, and done much configuration work across various platforms (Github.com, linux, Google Compute, `R`, `python` and `ipython`).   There is no reason to assume that learning machine learning is something that a person does as a conscious subject. When we look at an equation, when we try to comply with the machine learning injunction to 'find a useful approximation $\hat(f)(x)$ to the function $f(x)$ that underlies the predictive relationship between input and output' [@Hastie_2009, 28], the 'learning' may not be entirely that of a proper human subject. What happens there might have an important material-semiotic dynamic that does not readily map onto the forms of subjectivity, sense-making and experience that typically take as anchor points for our life in the world. 

Learning machine learning,  then,  could be construed as a machine learning problem. Writing about that learning would be a practice of diagrammatically mapping that movement into the data. If writing  can have this function in relation to what happens, a book could be machinic. In some ways, this book is a diagram. In that case, the book itself in its couplings with the infrastructures, datasets, with its authors, and its readers, would be traversing a vector-space, discriminating and generating, cross-validating and regularizing, maximising expectations and multiplying chains of events, minimising loss or cost functions, estimating probabilities (joint, conditional, marginal, etc) and trading off between bias and variance. Most books are complicated semiotic entities no doubt, but books that concern the  especially the somewhat diagrammatic processes of contemporary capitalism face particular challenges in generating forms of movement that can traverse spaces that are not, as various recent thinkers have strenuously argued, not reducible to the domain of representation, ideology, the subject-object distinction or conventional forms of power and hegemony [@Lash_2007; @Thrift_2014]. The problem of movement for books today is that they need to address asignifying or operational processes, like  machine learning, that are not easy to subjectify oneself around. Most writing in the social sciences and humanities tends to adhere in its forms of movement to the constitution of experiential modalities that can be mapped onto subject forms. Writing that fails or tries not to do that risks either being incomprehensible or boring, or maybe other at the same time. The price of learning machine learning might be something rather problematic and undomesticated when translated into writing.  

Let me pose the problem more bluntly: could a book function as a predictive model, or as a classifier? The problem with machine learning in general is that it is often quite predictable. I say this now, after a couple of hundred weeks spending time with machine learning, some of which time has been mired in technical articles or the mathematics entries in Wikipedia, some of which has lost in a kind of dazzled and maybe naive immersion in technical configuration work, and some of which has been attempting to look to such techniques as ways of making unfamiliar sense of contemporary forms of knowledge, power, value and experience, without denigrating it too much? Maybe the distinction is too subtle, but it seems to me that machine learning is an uneasy mixture of massively repeated and familiar forms, and something that is not easily understood. On the one hand, the level of imitation, duplications, copying and reproduction associated with the techniques suggests that a process of remaking the world according to particular forms is in process (for instance, in Chapter 4, we saw how Naive Bayes classifiers are almost demonstrated on spam classification problems.) The scientific and engineering literature, with its really frequent variations on similar themes, suggests that imitation and copying are very much  at the heart of the movements I have been describing. This is nothing new. It would be strange of these techniques were not subject to imitation and emulation. That imitation is predictable, we expect it and can account for it sociologically.[^8.2] Some symptoms of these imitative fluxes can be found in the scientific and engineering literature.  As we have seen, work on image and video classification, on text and speech, on gene interaction prediction or above all, on predictions of relations or associations between people and things (usually commodities, but not always) is striking in its persevering homogeneity. Moreover, the powerful aspirations evident amongst large media platforms such as Baidu, Google  and Facebook to re-ground machine learning in the project of artificial intelligence amidst social media or web page-related data  in many ways seems to be a resumption of business as usual for computer scientists [@Gulcehre_2014].  

[^8.2]: The kinds of accounts that might do this can be found in science and technology studies, particularly in actor-network theory versions, as well as in recent social and cultural theory that, for instance, draws on the work of the 19th century French sociologist, Gabriele Tarde [@Tarde_1902; @Borch_2005]. n

Worse, at a deeper level, the techniques often engage with the world in somewhat predictable ways. Their intuitions of shape are largely governed by a set of fairly basic practical forms. The phenomenologist, Edmund Husserl, describes something similar in _The Origin of Geometry_:

>First to be singled out from the thing-shapes are surfaces -- more or less "smooth," more or less perfect surfaces; edges, more or less rough or fairly "even"; in other words, more or less pure lines, angles, more or less perfect points; then again, among the lines, for examples, straight lines are especially preferred, and among surfaces, the even surfaces. ... Thus the production of even surfaces and their perfection (polishing) always plays its role in praxis [@Derrida_1989, 178]

Husserl here refers is attempting to describe something of the way in which forms such as circles, triangles, squares, lines,and points became objects of geometrical practice, but a similar polishing and smoothing of surfaces is certainly taking place today in the thing-shapes we call data.[^8.1] The strong alignment to lines and smooth surfaces plays out in many of the optimisation techniques, and in the graphs of machine performance. Whatever else is happening in the functional mappings traced out in the algorithms, lines, curves and surfaces bring with them powerful and predictable order to the operation of the techniques.

That said,  machine learning is not entirely predictable. The techniques, as we have seen, are often seen as too variable, too biased, they overfit, they underfit, they fail to generalise, so the meta-techniques of cross-validation or ROC curves attempt to restore order. The advent of specific techniques (support vector machine, neural network, decision tree, random forest, logistic regression or expectation maximization, MCMC) are not entirely unanticipated, but in themselves they are not direct imitations. These techniques are inventions that arise from re-combined repetitions of existing practices or procedures. Neural networks are like a massively proliferating nest of perceptrons.  Moreover, machine learning techniques often repeat something familiar by very different means (think of how `kittydar` treats photographs, or how a decision tree is legible but often unfamiliar). The event, then, resides less in either something intrinsic to the techniques understood as algorithmic models, or in something about the domains and places in which those techniques increasingly find themselves taken up (biomedicine, state security and intelligence agencies, finance, business, commerce, science, etc). Perhaps it is a rather more minimal event in which the tending of abstractions through practices of estimation,  optimisation, high-dimensional vectorisation, probabilistic mixing of latent and feature  variables, and imputation begins to replace existing ontological and epistemic norms of verification, objectification, and attribution. The growing saturation by abstractions, the introjection of many different associations and relations into the data. 

## What to do about predictability?

In writing about machine learning practice, two alternatives present themselves. One would be to write about the predictability of prediction today. That is, it would be seek to understand how this massive replicative movement centred on machine learning techniques has taken shape. In some respects, the kinds of abstractions that I have been describing in this book fit the mold of abstraction as characterised by much philosophical and critical thought during the course of the last century. We need to re-think what abstraction has become. Whether abstraction is understood phenomenologically (as Husserl or Arendt did), whether it is mapped into economic processes (as Marx, and more recent notions of immaterial and cognitive labour do), or whether it is understood (as Isabelle Stengers, and I think cybernetic-influenced thinkers such as Donna Haraway, Bruno Latour have suggested), as a reordering of relations between people and things, the interest lies less in contextualising abstraction in a given social-economic-political context, and more in devising ways to intensify the relevance and connection of abstraction to various aspects of experience. 

I have been less interested in treating these techniques as the predictable re-animation of alienated reason, and more inclined to look for those elements in machine learning that diagrammatically abstract away from structures of representations, subjectification or indeed physuicalization associated with platforms, services and products (for instance, the interminable implementations of document classifiers, sentiment analyses, or image labelling, or handwritten digit recognition, or autonomous navigation, etc.). Instead, the prime directive here is: reality is multiple if not removed the practices that sustain it [@Mol_2003, 6]. What is the reality multiple in machine learning? In writing a praxiography of machine learning, a latent model of this multiple might begin to take shape. Remember that the aim of praxiography is to act in the name of the multiple. 'Praxiography,' I suppose like ethnography or psychogeography, or indeed any writing practice, is meant to attend to the practices that keep reality multiples going.  And it is meant to be an act of sustaining the multiple of reality by treating the practices that make it multiple. In the frame of machine learning, if this book were a predictive model, it would be a kind of generative model that seeks to maximise the likelihood of millions of implementations of  machine learning techniques as they jump or slide into different settings. This reality maximisation might be entropic rather than convergent. It might not result in expanded or $N=all$-type infrastructures. In such a model, a model that would learn machine learning in order to intensify the abstraction , the features would figure less as techniques than as diagrams and diagrammatic process. We say machine learning 'techniques,' that technique obviously suffers a lot of the same problems of 'tool': it presupposes a doer, a maker, a user, or a technical expert who wields it. As we include the many techniques, the many meta-techniques (cross-validation, boosting, bootstrapping, ensemble methods), the manifold configuration work, the overlapping and often cantilevered software implementations, and the different people (programmers, web developers, scientists and engineers of many different ilk, social scientists, biomedical experts, economists, marketers and advertisers, etc), the notion of technique starts to become very strained in any case. Would an account of machine learning as technique encompass the thousands of random variables, and the temporal dynamics, the multiple superimposed time series of different domains? A diagram might but a tool or technique would not. Such a model is clearly not constructible, even with the deepest learning, or with the most advanced and comprehensive compute capacity. (At the moment, the state of the art in topic modelling, for instance, struggles to make sense of the shifts in topics in a well-defined collections of scientific papers, let alone the seething distribution of practice associated with machine learning).Here we might return to A.N. Whitehead's accounts of abstraction. The point is not to do away with abstraction but to preserve the abstraction with 'its adequate relevance to the concrete sense of value-attainment from which it is derived' [@Whitehead_1958, 169]. In the context of machine learning, this might be done in different ways, but it seems to me that without letting ourselves be affected by the practices of abstraction, then our concrete sense of what is happening there will lack vividness and depth. Hence,  the point of suggesting that a book might be a model is rather to pose the problem of how thinking today at the intersections and overlaps between writing code and writing prose might change it.

[^8.1]: In the essay on the origin of geometry, Husserl goes on to develop an account of how geometry opens up the possibility of a universality and infinite progress in knowledge constitutive of European civilization. Much of this sounds very problematic today, even if practically speaking the expansion of the styles of thought and practice Husserl was describing seems to confirm his account. The problem, to state it as plainly as possible, is that hardly anyone today thinks that geometry is a universal, not least because of the proliferation of mathematical thought and its production of alternative geometries. 


## What such a model could not do: the image of post-cybernetic thought

Social scientists always advocate more empirical work. Understandably, they think that new things that need to be described. What needs to be described in relation to machine learning? If there is something happening with machine learning to existing modes of abstraction, if the forms of optimisation and estimation it propels into so many domains take root, then what will need to be understood? There are different ways to put it or different vocabularies that might be used, but the core concern here been with an image of thought today. Sometimes I think of this in terms of movement. Borrowing from Foucault, the anthropologist Paul Rabinow advocates a 'specific relationship' to events in a hypercomplex world. He describes 'composition as motion' [@Rabinow_2003, 71] as a process that 'brings inquiry and equipment into a common discursive space' (79). Remember that the 'equipment' Rabinow refers to here are largely modelled on the notion of 'true discourse ... that have been assimilated so thoroughly'  (10) that they function as 'principles of action.' If machine learning can function as equipment in this sense, [^8.3] then what kind of features can we expect machine learning to display, and how should they be understood?

[^8.3]: Graham Harman's account of equipment as global in [@Harman_2002] would doubtless bring a completely different register to this discussion. The only problem is that I can't see a way of accessing the specificity of machine learning processes through Harman's 'tool-being,' much as though I like the general claim about the ontological provocations of equipment more generally. 


In  the three settings discussed in Part II of this book -- epidemiological modelling, biomedical prediction and machine learning competitions -- the differences between predictions were largely the result of the different ways in which the techniques were practiced. In some ways, these differences could be understood in terms of scale and variety of data. The Google search data used in flu trend prediction is massively homogeneous in its focus on short texts (Google search queries) while the data used in the UK Public Health Authority modelling of H1N1 is much more mixed. Similarly, and perhaps with much greater range of practices, the ways in which competitors in data mining and machine learning competitions  work with data and the ways in which industry and academic researchers work with data differ greatly. The infrastructural massification we saw in genomics was also occasioned by something that the machine learning techniques relate to: improvements in predictivity depend on deep reorganization of infrastructures, instruments, devices, and knowledge practices. Some of these contrasts in practice probably matter more than others.  If, however, we wish to understand machine learning as a practice of the multiple, as a way of sustaining the reality of the multiple, then we should pay attention to these differences. The version of machine learning pursued by a large media platform, a credit rating agency or by a government security agency on their data may have many affinities, may even draw on some common data, but be organised in very different ways. 

A different set of contrasts lie within the machine learning literature itself. The tensions between the 'learning' approach to machine learning, mainly derived from the computer science project of  artificial intelligence, and the 'function fitting' perspectives sometimes are sometimes hard to detect in practice, but deeply imbue much of what is done with the techniques. While the many techniques comprising machine learning intersect and overlap in different ways, and sometimes the same techniques have been invented with different names in different places , the different treatments of these techniques as algorithms or models, as computational or statistical processes runs like cracks or faultlines through much of the practice. In different academic disciplines and in different operational settings, whether a particular technique is algorithmically or statistically can make rather important differences in what results. We saw for instance that the use of  Naive Bayes classifiers in email spam classification differs quite markedly from the use of support vector machines to classify images. The differences largely stem from the intuitions underlying the techniques. The intuition beneath the support vector machine largely focuses on the idea that it is possible to construct some version of the data in vector space in which different kinds will be best separated. The intuition animating the Naive Bayes classifier and Latent Dirichlet Allocation topic models is that differences arise from events in the world, and those events can best be described en masse in terms of probability distributions.


## What is 'into the data' about?

This book has sought to provide an account of several dozen techniques, algorithms and procedures that run deeply through contemporary sciences, government, media and business. Many tensions and forces run through these techniques since they are said to be crucial to knowledge, to value and to power. The account offered in the preceding pages has not sought to list or delineate the plurality of applications, setting, implementations and variations in machine learning operates. Typically, humanities and social science research does focus on specific cases, and sometimes a list of cases, and it usually offers these cases as examplars, as a kind of training set that can be used for the purpose of generalization. The neural networks, the support vector machines, the linear regressions, the random forests and Naive Bayes classifiers are multiplying and generalizing. The several hundred thousand scientific articles that run underneath my discussion embody the process of generalizing the techniques. They constitute a dynamic substrate on which the techniques multiply and reproduce. These articles are no doubt only a limited institutional sample of the effervescing foam that bears these techniques further into everyday infrastructures of media, energy, government, commerce and manufacture. (The great virtue of scientific publication, even in a time where science is so bound up with technoscientific  enterprises, remains in the visibility its publications afford.) The point here, however, is that such lists of applications and implementations would be both symptomatic and inadequate in accounting for the movement of these techniques, perhaps in the same way that a list of recommended products on Amazon.com's website both evokes associations and also covers over the ways in which recommendations come about.

When we think of data as a place (a _topos_) or as a corpus (a body), then  machine learning techniques either constitute forms of movement that recognise patterns and associations, and that attribute classifications through dividing or clustering. The three main modes of this movement -- drawing lines through high-dimensional clouds of points, finding rules that divide and separate mixtures of features, or alternately, constructing models that generate the data using shaped flows of random numbers  -- entwine with each other over time. They make a tangled bank of techniques that ramble over many different slopes and surfaces.  These forms of movement together  comprise a moving substratum of techniques on which other forms of organization, scientific, economic, cultural and political life take root. The techniques are generative, I have suggested, of unfamiliar forms, patterns and associations. What they generate, however, may not be easily recognisable because they are not operating entirely within existing allocations or distributions of power.

The indications of these generative processes are legion. Amongst the mundane ones are simply the re-shaping of datasets, from the few columns and hundreds of rows of Fisher's `iris` dataset to the hundreds of millions or billions of rows found in contemporary datasets. If for Fisher, measurements of an iris sepal and an iris petal in a field yielded data that would pose challenging classification problems, today it is more likely that a million digital camera images of irises would be the dataset that a classifier algorithm would have to move through. The much-discussed changes in the extent of data do not tell us much about the forms of movement that accompany and arguably propel the re-scaling. The virtualization of computational processes in cloud computing, the increasingly parallel architectures of data processing, the expansive and accommodating options for storage of data in almost any form: many of these development bear the traces of the machine learning techniques, and the vectorised spaces they rely on. The ongoing transformation of digital infrastructures and perhaps the way in which they are taking shape increasingly as data processes stems from the techniques and the movements they impel. Numbers, words, images and things have long been arranged in tables, but the forms of order now running across those tables are no longer the comparisons and contrasts of the classical tabulations found in the early modern Europe, nor the statistical tabulations that accompany the growth of the nation-states or the accounting tables that accompany the rise of corporations in the nineteenth century, or even the lists, tables and arrays that routed flows of information and signals during the electronic age with its media. While arrays have grown in size, the actual number of rows is less important than the ways in which machine learning techniques traverse them as pluri-dimensional manifolds. They make 'inner products.'

## Is there any hope for machine learning?

Back in an earlier chapter, I posed the question: by unwinding some of the convoluted multiplications that generate contemporary multiples do we get closer to the concrete value-situations (the term is A.N. Whitehead's) that connect value and feeling? The question here, and I find it troubling, is whether it really makes sense to treat experience literally as a topological space of transformations (as [@Massumi_2002, 184] suggests we might, and whether those transformations occur in the realm of the techniques we have been discussing. At various points in the preceding pages, various types of transformation have been described. These transformations, sometimes written in the form of mathematics, sometimes written in `python` or `R` code or shown in data graphics, have been very typical ones. They mostly project numbers into high-dimensional vector-spaces, and then  move through those spaces along lines or across surfaces defined by mappings or functions such as probability distributions, such as metrics of similarity or purity, or optimisation procedures such as gradient descent or least squares.

No matter matter how inventive, unexpected or indeed predictable these forms of movement might be, the question remains how they intersect or encounter the affectively charged encounters with others, with things, with authority, with sensation, with authority, with experts, with knowledge, with decisions or with violence. The powerful movements of the techniques through data renders them the province of science, corporations, media platforms and  governments. But the fact that they can be written and practiced, even in close proximity to the relatively infrastructurally-disconnected form of a book suggests that their movement is not limited to or confined to the power-laden scenes of many of their contemporary usages. The fact  that the USA's  National Security Agency can handle so many phone calls and internet traffic in its data centres by use of machine learning [@Hickins_2013] should not deflect attention away from the many mundane appearances of machine learning in the world. The existence of `kittydar`, the cat face recognition demonstration, is not easily dismissed. In short, the increasingly praxiographic mode of existence of these techniques potentially generates more diverse forms of relationality in the same way that the working-through of symptoms can work in therapeutic settings to reactivate forgotten or excluded possibilities. 

When we glimpse something of the infrastructural contortions occasioned by, for instance, the need to produce inner products (matrix multiplication) on a large scale, then the ## What to do Three observations then about the significance of machine learning: -  its ubiquity in science, business and government renders it a key contemporary control practice, that differs in powerful yet subtle ways from existing ways of knowing, predicting, anticipating or controlling - its mobility as a cluster of methods has strongly performative effects -- parts of the world are heavily re-configured through it, whether in the banal forms of intelligence apparent in smart devices (text prediction, gesture recognition, voice recognition, etc), or in the financial torsions induced by algorithmic trading. - the mobility of its methods has no clear boundaries. The spread of machine learning into social sciences and humanities has started. Much of our research already implicitly depends on it (google search, etc). The question is how we move in relation to machine learning. It could be the cultural analytics route as propounded by Lev Manovich and others in the digital humanities; it could be the big data computational social science of US political scientists such as Gary King; or it could be more like the abundant hackathons that try to repurpose adn reinvest data with meaning or insights for the benefit of NGOs and charities. - Machine learning is a hard field to get into in some ways for social science and humanities researchers. There are lots of statistical, mathematical and infrastructural subleties to deal with. On the one hand, it is becoming enormously available and increasingly as Thrift would say, part of the contemporary time-space signature. Myriad mundane examples could be given. On the other hand, to get into it, to occupy the fluxing dimensionality of the data is  technically difficult, and conceptually challenging and sometimes boring. It takes very significant investments in time and attention (for instance, going through 27 hours of Youtube lectures with lots of equations written on blackboards is not trivial). - But I am suggesting that getting into it analytically and practically by whatever might be worthwhile, at least for some people for several reasons: 1. It offers  a way of accompanying the fluxing dimensionality of data. In _Modes of Thought_, Alfred North Whitehead writes: >Perhaps our knowledge is distorted unless we can comprehend its essential connection with happenings which involve spatial relationships of fifteen dimensions [@Whitehead_1958, 78] In this passage, Whitehead's choice of 15 is arbitrary. I guess it just refers to a spatiality that it is hard for to imagine, even though we no doubt inhabit it from time. From the standpoint of machine learning, we often do move through high dimensional spaces. Many machine learning techniques seek to reduce the dimensionality of spatial relationships in data. These would include the many dimensional reduction strategies. But others seek to expand the dimensionality of data. And the feild as a whole tends to augment rather than diminish data dimensionality. Certain techniques artificially inject infinite dimensional spaces into the models in order to find hyperplanes that separate data. 2. Like all sciences and technologies, machine learning must contain zones of slippage, inconsistency or friction where things can happen. While it might not be us as researchers who occupy or can identify those zones most easily, in making sense of machine learning all the way down, and pointing to the structures, processes or relations at play, we help free up the possibilities of gaming the models. And actually, who else is going to do it? That is, machine learning provides a way to contest the asymmetrical distributions of agency admidst re-dimensioned infrastructures.  
3. To some extent, as researchers we are encountering a version of the quandary I posed at the outset: what happens if you have all the data? The question is how we are going to comprehend 15D happenings, especially when a good number of those dimensions are occluded from us.  A couple of scenarios occur to me here:

- using machine learning to impute what kind of machine learning is going on in a given setting - these could involve either labelling what techniques are likely being used, or actively experimenting with ways of perturbing the model (as for instance, the many machine learning-based attempts to do search engine optimizination do). 'Gaming the model' means figuring out how it is likely to work, and then using that to perturb it. 
- using machine learning techniques as a way of thinking about differences, movement, shape and change

4. Bowker and Star suggested that we needed a kind of science to deal with classification systems: 

> The sheer density of the collisions of classification schemes in our lives calls for a new kind of science, a new set of metaphors, linking traditional social science and computer and information science. We need a topography of things such as the distribution of ambiguity.  ...  It will also use the best of object-oriented programming and other areas of computer science to describe this territory. [@Bowker_1999, 31]






