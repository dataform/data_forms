
\chapter{Conclusion: Out of the Data}
\label{ch:conclusion}


```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=12, tidy=TRUE, fig.height=8, 
                      echo=FALSE,  results='hide', warning=FALSE, message=FALSE, dev='pdf')

library(RSQLite)
con <- dbConnect(RSQLite::SQLite(),'../ml_lit/all_refs.sqlite3')
res = dbGetQuery(con, statement ="select * from basic_refs limit 10;")
library(ggplot2)
library(stringr)
library(xtable)
options(xtable.comment = FALSE)
simpleCap <- function(x) {
    s <- strsplit(x, " |-")[[1]]
    return (paste(toupper(substring(s, 1,1)), tolower(substring(s, 2)), sep="", collapse=" "))
  }
```
> These diagrams of the diagrammatic domains,  they kernel together in localization. 

> In this contrusion of major forms of invention in natures in machine learning techniques, inter-places, leveraged in and distributed.

The sentences above are the products of a generative model \index{model!generative} trained on the raw text of this book. Without any model of syntax, any dictionary of words or terms, relying purely on character sequences as probability distributions, the recurrent neural network that sampled these sentences out of its own unsupervised model of the book vectorised as data was primed with starting text of '`If `.' 'Diagrams of the diagrammatic domains,' kernelling together in localization, a 'contrusion' of major forms of invention in natures, in machine learning techniques, leveraged in and distributed in inter-places: all of that has been put quite well by the generative model, a two-layer 'long short term memory' neural net [@Karpathy_2016].  

If a book could be a machine learner or a generative model, then I hope this one might kernelize or multiply the range of practices it observes in the data. In such a model, a model that would learn machine learning in order to diagram an abstraction, the predictions would figure less as statements that rank, order and classify, than as a technology of the self, a means of effecting a certain number of operations on conduct, thinking and ways of being in order to transform oneself in order to attain some state of critical understanding. 

The work of at least 230,800 human machine learners (the number of unique authors listed in the corpus of machine learning research literature I have been drawing on)\index{machine learner!number of}, a new kind of operational formation jells in machine learning. People and things, knowledge and power, combine in novel forms. Understanding the distribution and production of elements that make up this emerging common space of decision, classification, prediction and anticipation matters contemporary critical thought in its engagement with power, production, conduct, communication, ways of being and thinking, materiality and experience.

Let us take 146,000 scientific articles, publications and books as statements concerning operations occurring in a variety of sites, modes, and settings connected in the operational formation we are discussing.\index{operational formation}. In contrast to Foucault's discursive formations, operational formations function  less by reference to the statements of a subject (the expert, the engineer, the doctor, the patient, the judge, the teacher, the student), and with greater reference to the statements generated in a human-machine interaction (for instance, the sentences generated by the RNN model above). The machine-human mixing in operational formations is highly variable, dynamic and mutable, sometimes planing through code, sometimes diagrammed in visible forms such as graphs and tables, and often ramifying through infrastructures. \index{statement!human-machine}.  

## Summary of the argument

Let me resume the argument of the book, an argument that comprising seven major facets or planes. In chapter \ref{ch:diagram}, I suggested that we should consider both the formal, mathematical abstraction  and certain transformations in the production of software associated with machine learning as diagrammatic processes that organise and assemble relations amidst a great accumulation of techniques, constructs, datasets and code implementations derived from many settings. The positivity \index{positivity} of machine learning, its specific forms of accumulation, regularity and rarety do not attest to the power of algorithms but rather lend  liveliness to the field by concentrating expressions from many regions. 

Chapter \ref{ch:vector} examined the practices of vectorising data, situating machine learners themselves in an organised, dimensioned space accommodating an increasing repertoire of movements and transformations operating on vectors. Vectorisation subsumes many differences in data. It is a practice of working with data \index{vectorisation} to accommodate all differences within an expanding dimensional space. Both in terms of infrastructure and epistemic cultures, the vector space NEED MORE HERE.

In chapter \ref{ch:function}, we discussed how learning might be understood as an experimental process that relays between operation and observation in optimising functions that predict and classify.  Viewed as an experimental relay, optimisation includes partial observers. NEED MORE HERE

Having all the data, chapter \ref{ch:probability} suggested, is not the principal stake in contemporary data cultures. Instead, the probabilisation \index{probabilisation} of both data and machine learners in populations suggests some of the axes of power-knowledge developing in machine learning. 

What happens to differences amidst vectorisation, learning as optimisation, probabilisation and the generalized diagrammatic abstraction of machine learning? \index{differences}. Treated  as pattern, chapter \ref{ch:pattern}, differences bifurcate between infinitesimal graduation and rigid decision boundaries, sometimes blurring or overlapping, and sometimes distributed into inaccessibly high-dimensional inner data spaces.

Rather than any new materiality, I have pointed to transformations in referentiality associated with machine learning. The topic of chapter \ref{ch:genome} was  a particular contemporary scientific object, genomes. I argued there that as a data form, genomic sequence data provoke re-use, transcription and transmission of classifications and predictions. 

Finally, chapter \ref{ch:subject} explored the subject position of machine learners. The argument here concerned human-machine differences and the dispersion of subject positions through operations that alter those differences. NEED MORE HERE 

HERE  As an operational formation, machine learning does not determine anything.  Under what conditions would that practice maximisation be divergent rather than convergent? It might not result in infinitely expanded $N=all$-type infrastructures subsuming all data.  in the closing pages of _The Archaeology of Knowledge_, Foucault writes:

>the positivities that I have tried to establish must not be understood as a set of determinations imposed from the outside on the thought of individuals, or inhabiting it from the inside, in advance as it were; they constitute rather the set of conditions in accordance with which a practice is exercised, in accordance with which that practices gives rise to partially or totally new statements, and in accordance with which it can be modified. These positivities are no so much limitations imposed on the initiative of subjects as the field in which that initiative is articulated [@Foucault_1972, 208-209]. \index{positivity}

Here Foucault refers to the restricted freedom that discursive practices and formations open for us. It is increasingly difficult for science, media, government and business to think and act outside data. The generalization of machine learning frames statements and makes things visible today. And yet Foucault is quite clear that amidst the positivities of knowledge production, knowing the conditions, setting out the rules, and identifying the relations that striate the density and complexity of practice is a pre-condition to any transformations in practice. 

I've read articles and books, downloaded data and software libraries, watched Youtube lectures and presentations, configured and written bits of code and text, made plots and diagrams, and done much configuration work across various platforms (Github.com, linux, Google Compute, `R`, `python` and `ipython`).   Amidst all of this data practice, there is no reason to assume that learning machine learning is something that a person does as a conscious subject. When we look at an equation, when we try to comply with the machine learning injunction to 'find a useful approximation $\hat(f)(x)$ to the function $f(x)$ that underlies the predictive relationship between input and output' [@Hastie_2009, 28], the 'learning' may not be entirely that of a proper human subject. The material-semiotic dynamic does not readily map onto the forms of subjectivity, sense-making and experience that we typically take as anchor points for our life in the world. 

I see writing about that learning as a practice of diagrammatically mapping the re-iterative drawing of human-machine relations.  Moving into the data like or as a machine learner perhaps allows writing to become more diagrammatic. 'Between the figure and the text we must admit a whole series of crisscrossings' wrote Foucault [@Foucault_1972, 66]. For present purposes, I have been treating datasets as a kind of texts and the machine learning as the crisscrossing that move data into various figures, moving the data into various forms of visibility and across epistemological, infrastructural, referential and experiential thresholds. \index{diagrammatic!writing}

After a couple of hundred weeks spending time with machine learners, some of which time has been mired in technical articles or the mathematics entries in Wikipedia, some of which has lost in a kind of dazzled and maybe naive immersion in technical configuration work, and some of which has been attempting to look to such techniques as ways of re-figuring contemporary forms of knowledge, power, value and experience, it seems to me that machine learning is an uneasy mixture of massively repeated and familiar forms, and something that is not easily understood. On the one hand, the level of imitation, duplications, copying and reproduction associated with the techniques suggests that a process of remaking the world according to particular forms is in process (for instance, in chapter \ref{ch:probability}  we saw how Naive Bayes classifiers are almost demonstrated on spam classification problems.) The scientific and engineering literature, with its really frequent variations on similar themes, suggests that imitation and copying are very much  at the heart of the movements I have been describing. This is nothing new. It would be strange of these techniques were not subject to imitation and emulation. That imitation is predictable, we expect it and can account for it sociologically.[^8.2] Some symptoms of these imitative fluxes can be found in the scientific and engineering literature.  As we have seen, work on image and video classification, on text and speech, on gene interaction prediction or above all, on predictions of relations or associations between people and things (usually commodities, but not always) is striking in its persevering homogeneity. Moreover, the powerful aspirations evident amongst large media platforms such as Baidu, Google  and Facebook to re-ground machine learning in the project of artificial intelligence amidst social media or web page-related data  in many ways continues business as usual for computer scientists [@Gulcehre_2014].  

At a deeper level, the techniques often engage with the world in somewhat predictable ways. Their intuitions of shape are largely governed by a set of fairly basic practical forms. The phenomenologist, Edmund Husserl, describes something similar in _The Origin of Geometry_:

>First to be singled out from the thing-shapes are surfaces -- more or less "smooth," more or less perfect surfaces; edges, more or less rough or fairly "even"; in other words, more or less pure lines, angles, more or less perfect points; then again, among the lines, for examples, straight lines are especially preferred, and among surfaces, the even surfaces. ... Thus the production of even surfaces and their perfection (polishing) always plays its role in praxis [@Derrida_1989, 178] \index{Husserl, Edmund!on surface thing-shape}

Husserl attempts to describe something of the way in which forms such as circles, triangles, squares, lines,and points became objects of geometrical practice, but a similar polishing and smoothing of surfaces is certainly taking place today in the thing-shapes we call data.[^8.1] The strong alignment to lines and smooth surfaces plays out in many of the optimisation techniques, and in the graphs of machine learning performance. Whatever else is happening in the functional mappings traced out in the algorithms, lines, curves and surfaces bring with them powerful and predictable order to the operation of the techniques.

As a data practice, however, machine learning is not entirely predictable. Machine learners,  as we have seen, vary too much, they are biased, they overfit, they underfit, or they fail to generalise.  The meta-techniques of cross-validation, bagging or ROC curves attempt to restore order. New machine learners arise from diagrammatic superimposition of existing practices or procedures. Neural networks are like a massively proliferating nest of perceptrons.  Moreover, machine learning techniques often repeat something familiar by very different means (think of how `kittydar` treats photographs, or how a decision tree is legible but often unfamiliar). The event, then, resides less in either something intrinsic to devices operating as algorithmic models, or in something about the domains and places in which the devices operate (biomedicine, state security and intelligence agencies, finance, business, commerce, science, etc). Perhaps it is a rather more modest event in which the tending of abstractions through estimation,  optimisation, high-dimensional vectorisation, probabilistic mixing of latent and feature  variables, and imputation unevenly replace existing ontological and epistemic norms of verification, objectification, and attribution. \index{machine learning!unpredictable operation of}

I have been less interested in treating these techniques as the predictable re-animation of alienated reason, and more inclined to look for those elements in machine learning that diagrammatically abstract away from structures of representations, subjectification or indeed physicalization associated with platforms, services and products (for instance, the interminable implementations of document classifiers, sentiment analyses, or image labelling, or handwritten digit recognition, or autonomous navigation, etc.). Like Anne-Marie Mol's 'praxiography,' which seeks to maintain reality multiples in describing practice [@Mol_2003, 6], the description of machine learning as data practice intends to  sustain the multiple of reality by identifying the practices that make it multiple. \index{Mol, Anne-Marie!on praxiography} \index{data practice!as multiple} 

[^8.1]: In the essay on the origin of geometry, Husserl goes on to develop an account of how geometry opens up the possibility of a universality and infinite progress in knowledge constitutive of European civilization. Much of this sounds very problematic today, even if practically speaking the expansion of the styles of thought and practice Husserl was describing seems to confirm his account. The problem, to state it as plainly as possible, is that hardly anyone today thinks that geometry is a universal, not least because of the proliferation of mathematical thought and its production of alternative geometries. \index{geometry!Husserl on universality of}

[^8.2]: Accounts that might do this can be found in science and technology studies, particularly in actor-network theory versions, as well as in recent social and cultural theory that, for instance, draws on the work of the 19th century French sociologist, Gabriele Tarde [@Tarde_1902; @Borch_2005].
