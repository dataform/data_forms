<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Machine Learning: Archaeology of a Data Practice</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="Machine Learning: Archaeology of a Data Practice">
  <meta name="generator" content="bookdown 0.3 and GitBook 2.6.7">

  <meta property="og:title" content="Machine Learning: Archaeology of a Data Practice" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Machine Learning: Archaeology of a Data Practice" />
  
  
  

<meta name="author" content="Adrian Mackenzie">


<meta name="date" content="2016-12-10">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="47-limiting-differences.html">
<link rel="next" href="49-differences-blur.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="1-acknowledgments.html"><a href="1-acknowledgments.html"><i class="fa fa-check"></i><b>1</b> Acknowledgments</a></li>
<li class="chapter" data-level="2" data-path="2-preface.html"><a href="2-preface.html"><i class="fa fa-check"></i><b>2</b> Preface</a></li>
<li class="chapter" data-level="3" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html"><i class="fa fa-check"></i><b>3</b> Introduction: Into the Data</a><ul>
<li class="chapter" data-level="3.1" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#three-accumulations-settings-data-and-devices"><i class="fa fa-check"></i><b>3.1</b> Three accumulations: settings, data and devices</a></li>
<li class="chapter" data-level="3.2" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#who-or-what-is-a-machine-learner"><i class="fa fa-check"></i><b>3.2</b> Who or what is a machine learner?</a></li>
<li class="chapter" data-level="3.3" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#algorithmic-control-to-the-machine-learners"><i class="fa fa-check"></i><b>3.3</b> Algorithmic control to the machine learners?</a></li>
<li class="chapter" data-level="3.4" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#the-archaeology-of-operations"><i class="fa fa-check"></i><b>3.4</b> The archaeology of operations</a></li>
<li class="chapter" data-level="3.5" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#asymmetries-in-common-knowledge"><i class="fa fa-check"></i><b>3.5</b> Asymmetries in common knowledge</a></li>
<li class="chapter" data-level="3.6" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#what-cannot-be-automated"><i class="fa fa-check"></i><b>3.6</b> What cannot be automated?</a></li>
<li class="chapter" data-level="3.7" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#different-fields-in-machine-learning"><i class="fa fa-check"></i><b>3.7</b> Different fields in machine learning?</a></li>
<li class="chapter" data-level="3.8" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#the-diagram-in-critical-thought"><i class="fa fa-check"></i><b>3.8</b> The diagram in critical thought</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-we-dont-have-to-write-programs.html"><a href="4-we-dont-have-to-write-programs.html"><i class="fa fa-check"></i><b>4</b> ‘We don’t have to write programs’?</a></li>
<li class="chapter" data-level="5" data-path="5-the-elements-of-machine-learning.html"><a href="5-the-elements-of-machine-learning.html"><i class="fa fa-check"></i><b>5</b> The elements of machine learning</a></li>
<li class="chapter" data-level="6" data-path="6-who-reads-machine-learning-textbooks.html"><a href="6-who-reads-machine-learning-textbooks.html"><i class="fa fa-check"></i><b>6</b> Who reads machine learning textbooks?</a></li>
<li class="chapter" data-level="7" data-path="7-r-a-matrix-of-transformations.html"><a href="7-r-a-matrix-of-transformations.html"><i class="fa fa-check"></i><b>7</b> <code>R</code>: a matrix of transformations</a></li>
<li class="chapter" data-level="8" data-path="8-the-obdurate-mathematical-glint-of-machine-learning.html"><a href="8-the-obdurate-mathematical-glint-of-machine-learning.html"><i class="fa fa-check"></i><b>8</b> The obdurate mathematical glint of machine learning</a></li>
<li class="chapter" data-level="9" data-path="9-cs229-2007-returning-again-and-again-to-certain-features.html"><a href="9-cs229-2007-returning-again-and-again-to-certain-features.html"><i class="fa fa-check"></i><b>9</b> CS229, 2007: returning again and again to certain features</a></li>
<li class="chapter" data-level="10" data-path="10-the-visible-learning-of-machine-learning.html"><a href="10-the-visible-learning-of-machine-learning.html"><i class="fa fa-check"></i><b>10</b> The visible learning of machine learning</a></li>
<li class="chapter" data-level="11" data-path="11-the-diagram-of-an-operational-formation.html"><a href="11-the-diagram-of-an-operational-formation.html"><i class="fa fa-check"></i><b>11</b> The diagram of an operational formation</a></li>
<li class="chapter" data-level="12" data-path="12-vectorisation-and-its-consequences.html"><a href="12-vectorisation-and-its-consequences.html"><i class="fa fa-check"></i><b>12</b> Vectorisation and its consequences}</a></li>
<li class="chapter" data-level="13" data-path="13-vector-space-and-geometry.html"><a href="13-vector-space-and-geometry.html"><i class="fa fa-check"></i><b>13</b> Vector space and geometry</a></li>
<li class="chapter" data-level="14" data-path="14-mixing-places.html"><a href="14-mixing-places.html"><i class="fa fa-check"></i><b>14</b> Mixing places</a></li>
<li class="chapter" data-level="15" data-path="15-truth-is-no-longer-in-the-table.html"><a href="15-truth-is-no-longer-in-the-table.html"><i class="fa fa-check"></i><b>15</b> Truth is no longer in the table?</a></li>
<li class="chapter" data-level="16" data-path="16-the-epistopic-fault-line-in-tables.html"><a href="16-the-epistopic-fault-line-in-tables.html"><i class="fa fa-check"></i><b>16</b> The epistopic fault line in tables</a></li>
<li class="chapter" data-level="17" data-path="17-surface-and-depths-the-problem-of-volume-in-data.html"><a href="17-surface-and-depths-the-problem-of-volume-in-data.html"><i class="fa fa-check"></i><b>17</b> Surface and depths: the problem of volume in data</a></li>
<li class="chapter" data-level="18" data-path="18-vector-space-expansion.html"><a href="18-vector-space-expansion.html"><i class="fa fa-check"></i><b>18</b> Vector space expansion</a></li>
<li class="chapter" data-level="19" data-path="19-drawing-lines-in-a-common-space-of-transformation.html"><a href="19-drawing-lines-in-a-common-space-of-transformation.html"><i class="fa fa-check"></i><b>19</b> Drawing lines in a common space of transformation</a></li>
<li class="chapter" data-level="20" data-path="20-implicit-vectorization-in-code-and-infrastructures.html"><a href="20-implicit-vectorization-in-code-and-infrastructures.html"><i class="fa fa-check"></i><b>20</b> Implicit vectorization in code and infrastructures</a></li>
<li class="chapter" data-level="21" data-path="21-lines-traversing-behind-the-light.html"><a href="21-lines-traversing-behind-the-light.html"><i class="fa fa-check"></i><b>21</b> Lines traversing behind the light</a></li>
<li class="chapter" data-level="22" data-path="22-the-vectorised-table.html"><a href="22-the-vectorised-table.html"><i class="fa fa-check"></i><b>22</b> The vectorised table?</a></li>
<li class="chapter" data-level="23" data-path="23-machines-finding-functions.html"><a href="23-machines-finding-functions.html"><i class="fa fa-check"></i><b>23</b> Machines finding functions}</a></li>
<li class="chapter" data-level="24" data-path="24-learning-functions.html"><a href="24-learning-functions.html"><i class="fa fa-check"></i><b>24</b> Learning functions</a></li>
<li class="chapter" data-level="25" data-path="25-supervised-unsupervised-reinforcement-learning-and-functions.html"><a href="25-supervised-unsupervised-reinforcement-learning-and-functions.html"><i class="fa fa-check"></i><b>25</b> Supervised, unsupervised, reinforcement learning and functions</a></li>
<li class="chapter" data-level="26" data-path="26-which-function-operates.html"><a href="26-which-function-operates.html"><i class="fa fa-check"></i><b>26</b> Which function operates?</a></li>
<li class="chapter" data-level="27" data-path="27-what-does-a-function-learn.html"><a href="27-what-does-a-function-learn.html"><i class="fa fa-check"></i><b>27</b> What does a function learn?</a></li>
<li class="chapter" data-level="28" data-path="28-observing-with-curves-the-logistic-function.html"><a href="28-observing-with-curves-the-logistic-function.html"><i class="fa fa-check"></i><b>28</b> Observing with curves: the logistic function</a></li>
<li class="chapter" data-level="29" data-path="29-the-cost-of-curves-in-machine-learning.html"><a href="29-the-cost-of-curves-in-machine-learning.html"><i class="fa fa-check"></i><b>29</b> The cost of curves in machine learning</a></li>
<li class="chapter" data-level="30" data-path="30-curves-and-the-variation-in-models.html"><a href="30-curves-and-the-variation-in-models.html"><i class="fa fa-check"></i><b>30</b> Curves and the variation in models</a></li>
<li class="chapter" data-level="31" data-path="31-observing-costs-losses-and-objectives-through-optimisation.html"><a href="31-observing-costs-losses-and-objectives-through-optimisation.html"><i class="fa fa-check"></i><b>31</b> Observing costs, losses and objectives through optimisation</a></li>
<li class="chapter" data-level="32" data-path="32-gradients-as-partial-observers.html"><a href="32-gradients-as-partial-observers.html"><i class="fa fa-check"></i><b>32</b> Gradients as partial observers</a></li>
<li class="chapter" data-level="33" data-path="33-the-power-to-learn.html"><a href="33-the-power-to-learn.html"><i class="fa fa-check"></i><b>33</b> The power to learn</a></li>
<li class="chapter" data-level="34" data-path="34-probabilisation-and-the-taming-of-machines.html"><a href="34-probabilisation-and-the-taming-of-machines.html"><i class="fa fa-check"></i><b>34</b> Probabilisation and the Taming of Machines}</a></li>
<li class="chapter" data-level="35" data-path="35-data-reduces-uncertainty.html"><a href="35-data-reduces-uncertainty.html"><i class="fa fa-check"></i><b>35</b> Data reduces uncertainty?</a></li>
<li class="chapter" data-level="36" data-path="36-machine-learning-as-statistics-inside-out.html"><a href="36-machine-learning-as-statistics-inside-out.html"><i class="fa fa-check"></i><b>36</b> Machine learning as statistics inside out</a></li>
<li class="chapter" data-level="37" data-path="37-distributed-probabilities.html"><a href="37-distributed-probabilities.html"><i class="fa fa-check"></i><b>37</b> Distributed probabilities</a></li>
<li class="chapter" data-level="38" data-path="38-naive-bayes-and-the-distribution-of-probabilities.html"><a href="38-naive-bayes-and-the-distribution-of-probabilities.html"><i class="fa fa-check"></i><b>38</b> Naive Bayes and the distribution of probabilities</a></li>
<li class="chapter" data-level="39" data-path="39-spam-when-foralln-is-too-much.html"><a href="39-spam-when-foralln-is-too-much.html"><i class="fa fa-check"></i><b>39</b> Spam: when <span class="math inline">\(\forall{N}\)</span> is too much?</a></li>
<li class="chapter" data-level="40" data-path="40-the-improbable-success-of-the-naive-bayes-classifier.html"><a href="40-the-improbable-success-of-the-naive-bayes-classifier.html"><i class="fa fa-check"></i><b>40</b> The improbable success of the Naive Bayes classifier</a></li>
<li class="chapter" data-level="41" data-path="41-ancestral-probabilities-in-documents-inference-and-prediction.html"><a href="41-ancestral-probabilities-in-documents-inference-and-prediction.html"><i class="fa fa-check"></i><b>41</b> Ancestral probabilities in documents: inference and prediction</a></li>
<li class="chapter" data-level="42" data-path="42-statistical-decompositions-bias-variance-and-observed-errors.html"><a href="42-statistical-decompositions-bias-variance-and-observed-errors.html"><i class="fa fa-check"></i><b>42</b> Statistical decompositions: bias, variance and observed errors</a></li>
<li class="chapter" data-level="43" data-path="43-does-machine-learning-construct-a-new-statistical-reality.html"><a href="43-does-machine-learning-construct-a-new-statistical-reality.html"><i class="fa fa-check"></i><b>43</b> Does machine learning construct a new statistical reality?</a></li>
<li class="chapter" data-level="44" data-path="44-patterns-and-differences.html"><a href="44-patterns-and-differences.html"><i class="fa fa-check"></i><b>44</b> Patterns and differences</a></li>
<li class="chapter" data-level="45" data-path="45-splitting-and-the-growth-of-trees.html"><a href="45-splitting-and-the-growth-of-trees.html"><i class="fa fa-check"></i><b>45</b> Splitting and the growth of trees</a></li>
<li class="chapter" data-level="46" data-path="46-differences-in-recursive-partitioning.html"><a href="46-differences-in-recursive-partitioning.html"><i class="fa fa-check"></i><b>46</b> 1984: Differences in recursive partitioning</a></li>
<li class="chapter" data-level="47" data-path="47-limiting-differences.html"><a href="47-limiting-differences.html"><i class="fa fa-check"></i><b>47</b> Limiting differences</a></li>
<li class="chapter" data-level="48" data-path="48-the-successful-dispersion-of-the-support-vector-machine.html"><a href="48-the-successful-dispersion-of-the-support-vector-machine.html"><i class="fa fa-check"></i><b>48</b> The successful dispersion of the support vector machine</a></li>
<li class="chapter" data-level="49" data-path="49-differences-blur.html"><a href="49-differences-blur.html"><i class="fa fa-check"></i><b>49</b> Differences blur?</a></li>
<li class="chapter" data-level="50" data-path="50-bending-the-decision-boundary.html"><a href="50-bending-the-decision-boundary.html"><i class="fa fa-check"></i><b>50</b> Bending the decision boundary</a></li>
<li class="chapter" data-level="51" data-path="51-instituting-patterns.html"><a href="51-instituting-patterns.html"><i class="fa fa-check"></i><b>51</b> Instituting patterns</a></li>
<li class="chapter" data-level="52" data-path="52-regularizing-and-materializing-objects.html"><a href="52-regularizing-and-materializing-objects.html"><i class="fa fa-check"></i><b>52</b> Regularizing and materializing objects}</a></li>
<li class="chapter" data-level="53" data-path="53-genomic-referentiality-and-materiality.html"><a href="53-genomic-referentiality-and-materiality.html"><i class="fa fa-check"></i><b>53</b> Genomic referentiality and materiality</a></li>
<li class="chapter" data-level="54" data-path="54-the-genome-as-threshold-object.html"><a href="54-the-genome-as-threshold-object.html"><i class="fa fa-check"></i><b>54</b> The genome as threshold object</a></li>
<li class="chapter" data-level="55" data-path="55-genomic-knowledges-and-their-datasets.html"><a href="55-genomic-knowledges-and-their-datasets.html"><i class="fa fa-check"></i><b>55</b> Genomic knowledges and their datasets</a></li>
<li class="chapter" data-level="56" data-path="56-the-advent-of-wide-dirty-and-mixed-data.html"><a href="56-the-advent-of-wide-dirty-and-mixed-data.html"><i class="fa fa-check"></i><b>56</b> The advent of ‘wide, dirty and mixed’ data</a></li>
<li class="chapter" data-level="57" data-path="57-cross-validating-machine-learning-in-genomics.html"><a href="57-cross-validating-machine-learning-in-genomics.html"><i class="fa fa-check"></i><b>57</b> Cross-validating machine learning in genomics</a></li>
<li class="chapter" data-level="58" data-path="58-proliferation-of-discoveries.html"><a href="58-proliferation-of-discoveries.html"><i class="fa fa-check"></i><b>58</b> Proliferation of discoveries</a></li>
<li class="chapter" data-level="59" data-path="59-variations-in-the-object-or-in-the-machine-learner.html"><a href="59-variations-in-the-object-or-in-the-machine-learner.html"><i class="fa fa-check"></i><b>59</b> Variations in the object or in the machine learner?</a></li>
<li class="chapter" data-level="60" data-path="60-whole-genome-functions.html"><a href="60-whole-genome-functions.html"><i class="fa fa-check"></i><b>60</b> Whole genome functions</a></li>
<li class="chapter" data-level="61" data-path="61-propagating-subject-positions.html"><a href="61-propagating-subject-positions.html"><i class="fa fa-check"></i><b>61</b> Propagating subject positions}</a></li>
<li class="chapter" data-level="62" data-path="62-propagation-across-human-machine-boundaries.html"><a href="62-propagation-across-human-machine-boundaries.html"><i class="fa fa-check"></i><b>62</b> Propagation across human-machine boundaries</a></li>
<li class="chapter" data-level="63" data-path="63-competitive-positioning.html"><a href="63-competitive-positioning.html"><i class="fa fa-check"></i><b>63</b> Competitive positioning</a></li>
<li class="chapter" data-level="64" data-path="64-a-privileged-machine-and-its-diagrammatic-forms.html"><a href="64-a-privileged-machine-and-its-diagrammatic-forms.html"><i class="fa fa-check"></i><b>64</b> A privileged machine and its diagrammatic forms</a></li>
<li class="chapter" data-level="65" data-path="65-varying-subject-positions-in-code.html"><a href="65-varying-subject-positions-in-code.html"><i class="fa fa-check"></i><b>65</b> Varying subject positions in code</a></li>
<li class="chapter" data-level="66" data-path="66-the-subjects-of-a-hidden-operation.html"><a href="66-the-subjects-of-a-hidden-operation.html"><i class="fa fa-check"></i><b>66</b> The subjects of a hidden operation</a></li>
<li class="chapter" data-level="67" data-path="67-algorithms-that-propagate-errors.html"><a href="67-algorithms-that-propagate-errors.html"><i class="fa fa-check"></i><b>67</b> Algorithms that propagate errors</a></li>
<li class="chapter" data-level="68" data-path="68-competitions-as-examination.html"><a href="68-competitions-as-examination.html"><i class="fa fa-check"></i><b>68</b> Competitions as examination</a></li>
<li class="chapter" data-level="69" data-path="69-superimposing-power-and-knowledge.html"><a href="69-superimposing-power-and-knowledge.html"><i class="fa fa-check"></i><b>69</b> Superimposing power and knowledge</a></li>
<li class="chapter" data-level="70" data-path="70-ranked-subject-positions.html"><a href="70-ranked-subject-positions.html"><i class="fa fa-check"></i><b>70</b> Ranked subject positions</a></li>
<li class="chapter" data-level="71" data-path="71-conclusion-out-of-the-data.html"><a href="71-conclusion-out-of-the-data.html"><i class="fa fa-check"></i><b>71</b> Conclusion: Out of the Data}</a></li>
<li class="chapter" data-level="72" data-path="72-machine-learners.html"><a href="72-machine-learners.html"><i class="fa fa-check"></i><b>72</b> 250,000 machine learners</a></li>
<li class="chapter" data-level="73" data-path="73-a-summary-of-the-argument.html"><a href="73-a-summary-of-the-argument.html"><i class="fa fa-check"></i><b>73</b> A summary of the argument</a></li>
<li class="chapter" data-level="74" data-path="74-in-situ-hybridization.html"><a href="74-in-situ-hybridization.html"><i class="fa fa-check"></i><b>74</b> In-situ hybridization</a></li>
<li class="chapter" data-level="75" data-path="75-critical-operational-practice.html"><a href="75-critical-operational-practice.html"><i class="fa fa-check"></i><b>75</b> Critical operational practice?</a></li>
<li class="chapter" data-level="76" data-path="76-obstacles-to-the-work-of-freeing-machine-learning.html"><a href="76-obstacles-to-the-work-of-freeing-machine-learning.html"><i class="fa fa-check"></i><b>76</b> Obstacles to the work of freeing machine learning</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning: Archaeology of a Data Practice</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="the-successful-dispersion-of-the-support-vector-machine" class="section level1">
<h1><span class="header-section-number">48</span> The successful dispersion of the support vector machine</h1>
<p>In growing and pruning decision trees, and even more markedly in support vector machine, patterns play out in dispersion and discontinuity rather than in regular geometry.  While machine learners order differences, that ordering becomes increasingly difficult to see as it is dispersed. Take the case of the support vector machine. The second most highly cited reference in the last few decades of machine learning literature is a paper from 1995 by Corinna Cortes and Vladimir Vapnik of AT &amp; T Bell Labs in New Jersey, USA entitled ‘Support Vector Networks’ <span class="citation">[@Cortes_1995]</span>. Few women’s names appear prominently in the machine learning literature.    The computing science and statistics departments at Stanford and Berkeley, the laboratories at Los Alamos and AT&amp;T Bell between the 1960s and the 1980s were, it seems, not overly popular or populated with women scientists and engineers. Some prominent machine learning researchers at the time of writing are women (I return to this in chapter ), but Cortes is perhaps pre-eminent both as head of Google Research in New York (2014) and as recipient with Vapnik of an Association for Computing Machine award in 2008 for work on the support vector machine algorithm.<a href="#fn71" class="footnoteRef" id="fnref71"><sup>71</sup></a>   </p>

<p>The rapid rise to popularity of the support vector machine can be seen in the machine learning research literature, a very small slice of which appears in Table . A substantial fraction of the overall research publication since the mid-1990s accumulates around this single technique, and as usual ranges across credit analysis, land cover prediction, protein structures, brain states and face recognition. The support vector machine spans the normal biopolitical triangle of life, labour and language. The influence of the technique can also be seen in overlapping fields such as pattern recognition and data mining, where <span class="citation">[@Cortes_1995]</span> and similar papers rank near the top-cited papers.<a href="#fn72" class="footnoteRef" id="fnref72"><sup>72</sup></a> This kind of growth betokens high levels of interest, identification and investment on the part of the researchers, and presumably more widely.</p>

<p>I suggested above that the classification tree (and then random forests) illustrate an enunciative modality anchored in a tension between recursively partitioned differences and classificatory stability.  The support vector machine shown in Figure  demonstrates a different change in what counts as pattern. The decision boundaries in the sub-graphs have different contours, contours that suggest a more mobile construction. While the name ‘support vector machine’ is somewhat forbiddingly technical compared to more familiar terms such as ‘decision tree’ or even ‘neural network,’ the underlying intuition of the technique is much older, and can be found in the models developed by the British statistician R. A. Fisher during the 1930s. Fisher developed the ‘first pattern recognition algorithm’ <span class="citation">[@Cortes_1995, 273]</span>, the ‘linear discriminator function’ <span class="citation">[@Fisher_1936]</span>, to deal with problems of classification, and demonstrated its efficacy on the taxonomic problem of discriminating or classifying the irises observed in W.E. Anderson’s irises in <code>iris</code> dataset (see above).   </p>
<p>In his 1936 article in the <em>Annual Review of Eugenics</em>, Fisher comments on similar classification work carried out in craniometry and other related settings: so-called ‘discriminant functions’ had been successfully used to distinguish populations. Fisher wrote: ‘when two or more populations have been measured in several characters, … special interest attaches to certain linear functions of measurements by which the populations are best discriminated’ <span class="citation">[@Fisher_1936, 179]</span>.  The discriminant functions divide the vector space into ‘a collection of regions labeled according to classification’ <span class="citation">[@Hastie_2009, 101]</span>. ‘Decision boundaries’ (or sometimes ‘decision surfaces’  often appear as straight lines that divide the vector space into regions of constant classification.  These long-standing discriminant functions were reconstructed during the 1990s in the form of the support vector machine, giving rise to new statements about differences, statements that can be glimpsed in table  in the range of things, facts and beings running through the titles of the papers.</p>
</div>
<div class="footnotes">
<hr />
<ol start="71">
<li id="fn71"><p>The support vector machine is distinctive in its transformations of data, and this owes something to history, politics and geography. Vapnik trained and worked of decades in the former USSR as a mathematician and statistician. His writings on the problems of pattern recognition contrast greatly with other engineers, statisticians and computer scientists in their robustly theoretical formalism. A highly cited 1971 publication with Alexey Chervonenkis ‘On the uniform convergence of relative frequencies of events to their probabilities’ (published in Russian in 1968 ) <span class="citation">[@Vapnik_1971]</span> sets the formal tone of this work. In ensuing publications in Russian and then in English after Vapnik moved from Moscow to AT&amp;T’s New Jersey Bell Labs in 1990, Vapnik’s work remains quite formally mathematical. Although it pertains to ‘learning machines,’ machine here are understood mathematically simply as ‘the implementation of a set of functions’ <span class="citation">[@Vapnik_1999, 17]</span>.  The way that Vapnik develops a theory of learning owes little visible debt to actual attempts to work with data or experience in doing statistics in any particular domain. This contrasts greatly for instance with the work of statisticians like Breiman or Friedman or even computer scientists like Quinlan or Le Cun whose work lies much closer to fields of application. Vapnik’s work, like that of the Russian mathematician Andrey Kolmogorov he draws on, differs from many other contributions to machine learning partly by virtue of this formality and its efforts to derive insight into machine learning by theorising learning. The <em>Vapnik-Chervonenkis dimension</em>( VC dimension), a very widely used way of defining the capacity of a particular machine learning technique to recognise patterns in data dates from his work in the 1960s and underpins a general theory of ‘learning.’ Vapnik writes in 1995, </p><blockquote><p>The VC dimension of the set of functions (rather than the number of parameters) is responsible for the generalization ability of learning machines. This opens remarkable opportunities to overcome the “curse of dimensionality <span class="citation">[@Vapnik_1999, 83]</span>.</p></blockquote><p>As we will see in this chapter, Vapnik’s attempts to overcome dimensionality also re-shape what counts as pattern. <a href="48-the-successful-dispersion-of-the-support-vector-machine.html#fnref71">↩</a></p></li>
<li id="fn72"><p><em>Elements of Statistical Learning</em> also devotes a chapter to support vector machines <span class="citation">[@Hastie_2009, Chapter 14]</span><a href="48-the-successful-dispersion-of-the-support-vector-machine.html#fnref72">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="47-limiting-differences.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="49-differences-blur.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
