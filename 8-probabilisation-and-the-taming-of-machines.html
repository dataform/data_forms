<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Machine Learning: Archaeology of a Data Practice</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="Machine Learning: Archaeology of a Data Practice">
  <meta name="generator" content="bookdown 0.3 and GitBook 2.6.7">

  <meta property="og:title" content="Machine Learning: Archaeology of a Data Practice" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Machine Learning: Archaeology of a Data Practice" />
  
  
  

<meta name="author" content="Adrian Mackenzie">


<meta name="date" content="2016-12-10">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="7-the-power-to-learn.html">
<link rel="next" href="9-patterns-and-differences.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="1-acknowledgments.html"><a href="1-acknowledgments.html"><i class="fa fa-check"></i><b>1</b> Acknowledgments</a></li>
<li class="chapter" data-level="2" data-path="2-preface.html"><a href="2-preface.html"><i class="fa fa-check"></i><b>2</b> Preface</a></li>
<li class="chapter" data-level="3" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html"><i class="fa fa-check"></i><b>3</b> Introduction: Into the Data</a><ul>
<li class="chapter" data-level="3.1" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#three-accumulations-settings-data-and-devices"><i class="fa fa-check"></i><b>3.1</b> Three accumulations: settings, data and devices</a></li>
<li class="chapter" data-level="3.2" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#who-or-what-is-a-machine-learner"><i class="fa fa-check"></i><b>3.2</b> Who or what is a machine learner?</a></li>
<li class="chapter" data-level="3.3" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#algorithmic-control-to-the-machine-learners"><i class="fa fa-check"></i><b>3.3</b> Algorithmic control to the machine learners?</a></li>
<li class="chapter" data-level="3.4" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#the-archaeology-of-operations"><i class="fa fa-check"></i><b>3.4</b> The archaeology of operations</a></li>
<li class="chapter" data-level="3.5" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#asymmetries-in-common-knowledge"><i class="fa fa-check"></i><b>3.5</b> Asymmetries in common knowledge</a></li>
<li class="chapter" data-level="3.6" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#what-cannot-be-automated"><i class="fa fa-check"></i><b>3.6</b> What cannot be automated?</a></li>
<li class="chapter" data-level="3.7" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#different-fields-in-machine-learning"><i class="fa fa-check"></i><b>3.7</b> Different fields in machine learning?</a></li>
<li class="chapter" data-level="3.8" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#the-diagram-in-critical-thought"><i class="fa fa-check"></i><b>3.8</b> The diagram in critical thought</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html"><i class="fa fa-check"></i><b>4</b> Diagramming machines}</a><ul>
<li class="chapter" data-level="4.1" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#we-dont-have-to-write-programs"><i class="fa fa-check"></i><b>4.1</b> ‘We don’t have to write programs’?</a></li>
<li class="chapter" data-level="4.2" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#the-elements-of-machine-learning"><i class="fa fa-check"></i><b>4.2</b> The elements of machine learning</a></li>
<li class="chapter" data-level="4.3" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#who-reads-machine-learning-textbooks"><i class="fa fa-check"></i><b>4.3</b> Who reads machine learning textbooks?</a></li>
<li class="chapter" data-level="4.4" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#r-a-matrix-of-transformations"><i class="fa fa-check"></i><b>4.4</b> <code>R</code>: a matrix of transformations</a></li>
<li class="chapter" data-level="4.5" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#the-obdurate-mathematical-glint-of-machine-learning"><i class="fa fa-check"></i><b>4.5</b> The obdurate mathematical glint of machine learning</a></li>
<li class="chapter" data-level="4.6" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#cs229-2007-returning-again-and-again-to-certain-features"><i class="fa fa-check"></i><b>4.6</b> CS229, 2007: returning again and again to certain features</a></li>
<li class="chapter" data-level="4.7" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#the-visible-learning-of-machine-learning"><i class="fa fa-check"></i><b>4.7</b> The visible learning of machine learning</a></li>
<li class="chapter" data-level="4.8" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#the-diagram-of-an-operational-formation"><i class="fa fa-check"></i><b>4.8</b> The diagram of an operational formation</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html"><i class="fa fa-check"></i><b>5</b> Vectorisation and its consequences}</a><ul>
<li class="chapter" data-level="5.1" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html#vector-space-and-geometry"><i class="fa fa-check"></i><b>5.1</b> Vector space and geometry</a></li>
<li class="chapter" data-level="5.2" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html#mixing-places"><i class="fa fa-check"></i><b>5.2</b> Mixing places</a></li>
<li class="chapter" data-level="5.3" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html#truth-is-no-longer-in-the-table"><i class="fa fa-check"></i><b>5.3</b> Truth is no longer in the table?</a></li>
<li class="chapter" data-level="5.4" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html#the-epistopic-fault-line-in-tables"><i class="fa fa-check"></i><b>5.4</b> The epistopic fault line in tables</a></li>
<li class="chapter" data-level="5.5" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html#surface-and-depths-the-problem-of-volume-in-data"><i class="fa fa-check"></i><b>5.5</b> Surface and depths: the problem of volume in data</a></li>
<li class="chapter" data-level="5.6" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html#vector-space-expansion"><i class="fa fa-check"></i><b>5.6</b> Vector space expansion</a></li>
<li class="chapter" data-level="5.7" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html#drawing-lines-in-a-common-space-of-transformation"><i class="fa fa-check"></i><b>5.7</b> Drawing lines in a common space of transformation</a></li>
<li class="chapter" data-level="5.8" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html#implicit-vectorization-in-code-and-infrastructures"><i class="fa fa-check"></i><b>5.8</b> Implicit vectorization in code and infrastructures</a></li>
<li class="chapter" data-level="5.9" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html#lines-traversing-behind-the-light"><i class="fa fa-check"></i><b>5.9</b> Lines traversing behind the light</a></li>
<li class="chapter" data-level="5.10" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html#the-vectorised-table"><i class="fa fa-check"></i><b>5.10</b> The vectorised table?</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html"><i class="fa fa-check"></i><b>6</b> Machines finding functions}</a><ul>
<li class="chapter" data-level="6.1" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html#learning-functions"><i class="fa fa-check"></i><b>6.1</b> Learning functions</a></li>
<li class="chapter" data-level="6.2" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html#supervised-unsupervised-reinforcement-learning-and-functions"><i class="fa fa-check"></i><b>6.2</b> Supervised, unsupervised, reinforcement learning and functions</a></li>
<li class="chapter" data-level="6.3" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html#which-function-operates"><i class="fa fa-check"></i><b>6.3</b> Which function operates?</a></li>
<li class="chapter" data-level="6.4" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html#what-does-a-function-learn"><i class="fa fa-check"></i><b>6.4</b> What does a function learn?</a></li>
<li class="chapter" data-level="6.5" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html#observing-with-curves-the-logistic-function"><i class="fa fa-check"></i><b>6.5</b> Observing with curves: the logistic function</a></li>
<li class="chapter" data-level="6.6" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html#the-cost-of-curves-in-machine-learning"><i class="fa fa-check"></i><b>6.6</b> The cost of curves in machine learning</a></li>
<li class="chapter" data-level="6.7" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html#curves-and-the-variation-in-models"><i class="fa fa-check"></i><b>6.7</b> Curves and the variation in models</a></li>
<li class="chapter" data-level="6.8" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html#observing-costs-losses-and-objectives-through-optimisation"><i class="fa fa-check"></i><b>6.8</b> Observing costs, losses and objectives through optimisation</a></li>
<li class="chapter" data-level="6.9" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html#gradients-as-partial-observers"><i class="fa fa-check"></i><b>6.9</b> Gradients as partial observers</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="7-the-power-to-learn.html"><a href="7-the-power-to-learn.html"><i class="fa fa-check"></i><b>7</b> The power to learn</a></li>
<li class="chapter" data-level="8" data-path="8-probabilisation-and-the-taming-of-machines.html"><a href="8-probabilisation-and-the-taming-of-machines.html"><i class="fa fa-check"></i><b>8</b> Probabilisation and the Taming of Machines}</a><ul>
<li class="chapter" data-level="8.1" data-path="8-probabilisation-and-the-taming-of-machines.html"><a href="8-probabilisation-and-the-taming-of-machines.html#data-reduces-uncertainty"><i class="fa fa-check"></i><b>8.1</b> Data reduces uncertainty?</a></li>
<li class="chapter" data-level="8.2" data-path="8-probabilisation-and-the-taming-of-machines.html"><a href="8-probabilisation-and-the-taming-of-machines.html#machine-learning-as-statistics-inside-out"><i class="fa fa-check"></i><b>8.2</b> Machine learning as statistics inside out</a></li>
<li class="chapter" data-level="8.3" data-path="8-probabilisation-and-the-taming-of-machines.html"><a href="8-probabilisation-and-the-taming-of-machines.html#distributed-probabilities"><i class="fa fa-check"></i><b>8.3</b> Distributed probabilities</a></li>
<li class="chapter" data-level="8.4" data-path="8-probabilisation-and-the-taming-of-machines.html"><a href="8-probabilisation-and-the-taming-of-machines.html#naive-bayes-and-the-distribution-of-probabilities"><i class="fa fa-check"></i><b>8.4</b> Naive Bayes and the distribution of probabilities</a></li>
<li class="chapter" data-level="8.5" data-path="8-probabilisation-and-the-taming-of-machines.html"><a href="8-probabilisation-and-the-taming-of-machines.html#spam-when-foralln-is-too-much"><i class="fa fa-check"></i><b>8.5</b> Spam: when <span class="math inline">\(\forall{N}\)</span> is too much?</a></li>
<li class="chapter" data-level="8.6" data-path="8-probabilisation-and-the-taming-of-machines.html"><a href="8-probabilisation-and-the-taming-of-machines.html#the-improbable-success-of-the-naive-bayes-classifier"><i class="fa fa-check"></i><b>8.6</b> The improbable success of the Naive Bayes classifier</a></li>
<li class="chapter" data-level="8.7" data-path="8-probabilisation-and-the-taming-of-machines.html"><a href="8-probabilisation-and-the-taming-of-machines.html#ancestral-probabilities-in-documents-inference-and-prediction"><i class="fa fa-check"></i><b>8.7</b> Ancestral probabilities in documents: inference and prediction</a></li>
<li class="chapter" data-level="8.8" data-path="8-probabilisation-and-the-taming-of-machines.html"><a href="8-probabilisation-and-the-taming-of-machines.html#statistical-decompositions-bias-variance-and-observed-errors"><i class="fa fa-check"></i><b>8.8</b> Statistical decompositions: bias, variance and observed errors</a></li>
<li class="chapter" data-level="8.9" data-path="8-probabilisation-and-the-taming-of-machines.html"><a href="8-probabilisation-and-the-taming-of-machines.html#does-machine-learning-construct-a-new-statistical-reality"><i class="fa fa-check"></i><b>8.9</b> Does machine learning construct a new statistical reality?</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="9-patterns-and-differences.html"><a href="9-patterns-and-differences.html"><i class="fa fa-check"></i><b>9</b> Patterns and differences</a><ul>
<li class="chapter" data-level="9.1" data-path="9-patterns-and-differences.html"><a href="9-patterns-and-differences.html#splitting-and-the-growth-of-trees"><i class="fa fa-check"></i><b>9.1</b> Splitting and the growth of trees</a></li>
<li class="chapter" data-level="9.2" data-path="9-patterns-and-differences.html"><a href="9-patterns-and-differences.html#differences-in-recursive-partitioning"><i class="fa fa-check"></i><b>9.2</b> 1984: Differences in recursive partitioning</a></li>
<li class="chapter" data-level="9.3" data-path="9-patterns-and-differences.html"><a href="9-patterns-and-differences.html#limiting-differences"><i class="fa fa-check"></i><b>9.3</b> Limiting differences</a></li>
<li class="chapter" data-level="9.4" data-path="9-patterns-and-differences.html"><a href="9-patterns-and-differences.html#the-successful-dispersion-of-the-support-vector-machine"><i class="fa fa-check"></i><b>9.4</b> The successful dispersion of the support vector machine</a></li>
<li class="chapter" data-level="9.5" data-path="9-patterns-and-differences.html"><a href="9-patterns-and-differences.html#differences-blur"><i class="fa fa-check"></i><b>9.5</b> Differences blur?</a></li>
<li class="chapter" data-level="9.6" data-path="9-patterns-and-differences.html"><a href="9-patterns-and-differences.html#bending-the-decision-boundary"><i class="fa fa-check"></i><b>9.6</b> Bending the decision boundary</a></li>
<li class="chapter" data-level="9.7" data-path="9-patterns-and-differences.html"><a href="9-patterns-and-differences.html#instituting-patterns"><i class="fa fa-check"></i><b>9.7</b> Instituting patterns</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="10-regularizing-and-materializing-objects.html"><a href="10-regularizing-and-materializing-objects.html"><i class="fa fa-check"></i><b>10</b> Regularizing and materializing objects}</a><ul>
<li class="chapter" data-level="10.1" data-path="10-regularizing-and-materializing-objects.html"><a href="10-regularizing-and-materializing-objects.html#genomic-referentiality-and-materiality"><i class="fa fa-check"></i><b>10.1</b> Genomic referentiality and materiality</a></li>
<li class="chapter" data-level="10.2" data-path="10-regularizing-and-materializing-objects.html"><a href="10-regularizing-and-materializing-objects.html#the-genome-as-threshold-object"><i class="fa fa-check"></i><b>10.2</b> The genome as threshold object</a></li>
<li class="chapter" data-level="10.3" data-path="10-regularizing-and-materializing-objects.html"><a href="10-regularizing-and-materializing-objects.html#genomic-knowledges-and-their-datasets"><i class="fa fa-check"></i><b>10.3</b> Genomic knowledges and their datasets</a></li>
<li class="chapter" data-level="10.4" data-path="10-regularizing-and-materializing-objects.html"><a href="10-regularizing-and-materializing-objects.html#the-advent-of-wide-dirty-and-mixed-data"><i class="fa fa-check"></i><b>10.4</b> The advent of ‘wide, dirty and mixed’ data</a></li>
<li class="chapter" data-level="10.5" data-path="10-regularizing-and-materializing-objects.html"><a href="10-regularizing-and-materializing-objects.html#cross-validating-machine-learning-in-genomics"><i class="fa fa-check"></i><b>10.5</b> Cross-validating machine learning in genomics</a></li>
<li class="chapter" data-level="10.6" data-path="10-regularizing-and-materializing-objects.html"><a href="10-regularizing-and-materializing-objects.html#proliferation-of-discoveries"><i class="fa fa-check"></i><b>10.6</b> Proliferation of discoveries</a></li>
<li class="chapter" data-level="10.7" data-path="10-regularizing-and-materializing-objects.html"><a href="10-regularizing-and-materializing-objects.html#variations-in-the-object-or-in-the-machine-learner"><i class="fa fa-check"></i><b>10.7</b> Variations in the object or in the machine learner?</a></li>
<li class="chapter" data-level="10.8" data-path="10-regularizing-and-materializing-objects.html"><a href="10-regularizing-and-materializing-objects.html#whole-genome-functions"><i class="fa fa-check"></i><b>10.8</b> Whole genome functions</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="11-propagating-subject-positions.html"><a href="11-propagating-subject-positions.html"><i class="fa fa-check"></i><b>11</b> Propagating subject positions}</a><ul>
<li class="chapter" data-level="11.1" data-path="11-propagating-subject-positions.html"><a href="11-propagating-subject-positions.html#propagation-across-human-machine-boundaries"><i class="fa fa-check"></i><b>11.1</b> Propagation across human-machine boundaries</a></li>
<li class="chapter" data-level="11.2" data-path="11-propagating-subject-positions.html"><a href="11-propagating-subject-positions.html#competitive-positioning"><i class="fa fa-check"></i><b>11.2</b> Competitive positioning</a></li>
<li class="chapter" data-level="11.3" data-path="11-propagating-subject-positions.html"><a href="11-propagating-subject-positions.html#a-privileged-machine-and-its-diagrammatic-forms"><i class="fa fa-check"></i><b>11.3</b> A privileged machine and its diagrammatic forms</a></li>
<li class="chapter" data-level="11.4" data-path="11-propagating-subject-positions.html"><a href="11-propagating-subject-positions.html#varying-subject-positions-in-code"><i class="fa fa-check"></i><b>11.4</b> Varying subject positions in code</a></li>
<li class="chapter" data-level="11.5" data-path="11-propagating-subject-positions.html"><a href="11-propagating-subject-positions.html#the-subjects-of-a-hidden-operation"><i class="fa fa-check"></i><b>11.5</b> The subjects of a hidden operation</a></li>
<li class="chapter" data-level="11.6" data-path="11-propagating-subject-positions.html"><a href="11-propagating-subject-positions.html#algorithms-that-propagate-errors"><i class="fa fa-check"></i><b>11.6</b> Algorithms that propagate errors</a></li>
<li class="chapter" data-level="11.7" data-path="11-propagating-subject-positions.html"><a href="11-propagating-subject-positions.html#competitions-as-examination"><i class="fa fa-check"></i><b>11.7</b> Competitions as examination</a></li>
<li class="chapter" data-level="11.8" data-path="11-propagating-subject-positions.html"><a href="11-propagating-subject-positions.html#superimposing-power-and-knowledge"><i class="fa fa-check"></i><b>11.8</b> Superimposing power and knowledge</a></li>
<li class="chapter" data-level="11.9" data-path="11-propagating-subject-positions.html"><a href="11-propagating-subject-positions.html#ranked-subject-positions"><i class="fa fa-check"></i><b>11.9</b> Ranked subject positions</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="12-conclusion-out-of-the-data.html"><a href="12-conclusion-out-of-the-data.html"><i class="fa fa-check"></i><b>12</b> Conclusion: Out of the Data}</a><ul>
<li class="chapter" data-level="12.1" data-path="12-conclusion-out-of-the-data.html"><a href="12-conclusion-out-of-the-data.html#machine-learners"><i class="fa fa-check"></i><b>12.1</b> 250,000 machine learners</a></li>
<li class="chapter" data-level="12.2" data-path="12-conclusion-out-of-the-data.html"><a href="12-conclusion-out-of-the-data.html#a-summary-of-the-argument"><i class="fa fa-check"></i><b>12.2</b> A summary of the argument</a></li>
<li class="chapter" data-level="12.3" data-path="12-conclusion-out-of-the-data.html"><a href="12-conclusion-out-of-the-data.html#in-situ-hybridization"><i class="fa fa-check"></i><b>12.3</b> In-situ hybridization</a></li>
<li class="chapter" data-level="12.4" data-path="12-conclusion-out-of-the-data.html"><a href="12-conclusion-out-of-the-data.html#critical-operational-practice"><i class="fa fa-check"></i><b>12.4</b> Critical operational practice?</a></li>
<li class="chapter" data-level="12.5" data-path="12-conclusion-out-of-the-data.html"><a href="12-conclusion-out-of-the-data.html#obstacles-to-the-work-of-freeing-machine-learning"><i class="fa fa-check"></i><b>12.5</b> Obstacles to the work of freeing machine learning</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning: Archaeology of a Data Practice</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="probabilisation-and-the-taming-of-machines" class="section level1">
<h1><span class="header-section-number">8</span> Probabilisation and the Taming of Machines}</h1>
<p></p>
<p>In the final pages of <em>The Taming of Chance</em>, Ian Hacking describes the work of the philosopher Charles Sanders Peirce in terms of a twin affirmation of chance. First, Peirce, following the work of the psychophysicist Gustav Fechner and before him the astronomer-sociologist Adolphe Quetelet, re-enacts the normal curve.<a href="#fn47" class="footnoteRef" id="fnref47"><sup>47</sup></a> The ‘personal equation,’ the variation in measurements made by any observer, becomes ‘a reality underneath the phenomena of consciousness’ <span class="citation">[@Hacking_1990, 205]</span>.  Peirce’s belief in absolute chance or a stochastic ontology,  ‘a universe of chance’ as Hacking puts it, continued a series of ‘realizations’ of curves, in which astronomical, social, biological and finally psychological variations were all understood as generated by processes of chance. Second, and in order to show the underlying reality of the normal curve, ‘Peirce deliberately used the properties of chance devices to introduce a new level of control into his experimentation. Control not by getting rid of chance fluctuations, but by adding some more’ <span class="citation">[@Hacking_1990, 205]</span>. In the century or so since, what happened to the thorough-going affirmation of statistical thought and probabilistic practice epitomised by Peirce?  Hacking stresses that he does not understand Peirce as the precursor or the innovator of twentieth century statistical thought (Hacking’s <em>Taming of Chance</em> ends at 1900), but rather as ‘the first philosopher to conceptually internalize the way chance had been tamed in the nineteenth century’ (215). What would the equivalent philosopher-machine learner internalize today? What would such persons, working in science or media or government, hold firm in relation to chance, probability and statistics? </p>
<p>In the opening lines of the preface to the First Edition of <em>Elements of Statistical Learning</em>, Hastie, Tibshirani and Friedman describe the altered situation of statistics:</p>
<blockquote>
<p>The field of Statistics is constantly challenged by the problems that science and industry brings to its door. In the early days, these problems often came from agricultural and industrial experiments and were relatively small in scope <span class="citation">[@Hastie_2009, xi]</span> </p>
</blockquote>
<p>(At the end of the preface, they also cite, we might note in passing, Hacking’s work: ‘The quiet statisticians have changed our world’ <span class="citation">[@Hastie_2009, xii]</span>.) One of the challenges science and industry has brought to the door of statistics in recent years has not only been more data but also machine learners. What difference do the ‘vast amounts of data … generated in many fields’ (xi) make to the field of statistics? Statistics has, I will suggest in this chapter, gradually <em>probabilised</em> machine learners, or injected a substratum of chance that flows directly from their operation.  To grasp this , we need to determine what role randomness, change and the probabilistic distribution of elements and events play in machine learning. These questions of how worlds becomes thinkable through machine learning can be addressed partly by contrasting the ‘taming of chance’ achieved by eighteenth and nineteenth century statistics and the taming of data – and machines – in statistical practices of machine learning today. </p>
<div id="data-reduces-uncertainty" class="section level2">
<h2><span class="header-section-number">8.1</span> Data reduces uncertainty?</h2>
<p>The broadest claim associated with machine learning hinges on the simple expression shown:</p>

<p>In Equation , <span class="math inline">\(N\)</span> refers to the number of observations (and hence the size of the dataset), the logical operator <span class="math inline">\(\forall\)</span> means ‘all’ since this is the level of inclusion which many fields of knowledge in science, government, media, commerce and industry envisage, and <span class="math inline">\(\boldsymbol{X}\)</span> refers to the data itself arrayed in vector space. Note that this expression leaves some things out. <span class="math inline">\(Y\)</span>, the response variable, for instance, may or may not be known or part of the data <span class="math inline">\(\boldsymbol{X}\)</span>.  While both the expansion of data in the vector space and the machine learners that transform and observe it have appeared in previous chapters, I focus here on changes in probability practices associated with machine learning, and in particular, <span class="math inline">\(N = \forall\boldsymbol{X}\)</span>, the claim that with all the data, the production of knowledge fundamentally changes. </p>
<p>The claim that with <span class="math inline">\(N=\forall\boldsymbol{X}\)</span> the nature of knowledge changes has been widely discussed.<a href="#fn48" class="footnoteRef" id="fnref48"><sup>48</sup></a> Viktor Mayer-Schönberger and Kenneth Cukier’s <em>Big Data: A Revolution That Will Transform How We Live, Work and Think</em> present this shift in many different settings in the course of the vignettes and teeming comparisons that have become typical of the data revolution genre. In a chapter entitled ‘More,’ they sketch the transition from data practices reliant on sampling to data practices that deal with all the <a href="data:\index%7Bdata!sampling!limits" class="uri">data:\index{data!sampling!limits</a> of}</p>
<blockquote>
<p>Using all the data makes it possible to spot connections and details that are otherwise cloaked in the vastness of the information. For instance, the detection of credit card fraud works by looking for anomalies, and the best way to find them is to crunch all the data rather than a sample <span class="citation">[@Mayer-Schonberger_2013, 2013, 27]</span></p>
</blockquote>
<p>In the several hundred pages that follow in <em>Big Data</em>, the problem of how to ‘crunch all the data’ is not a major topic. Machine learning remains almost completely invisible as a practice of transforming data in the name of knowledge.   While they mention the role of social network theory (30), ‘sophisticated computational analysis’ (55), ‘predictive analytics’ (58) and ‘correlations’ (7), and they observe that ‘the revolution’ is ‘about applying math to huge quantities of data in order to infer probabilities’ (12), any further consideration of a change in data practices is largely confined to a business-oriented contrast between having some of the data and having all the data (that is, businesses often have all the data on their customers).</p>
<p>Without a sense of how statistical practices animate and configure key features of ‘crunching the data’ to make predictions, it becomes hard to see how the ‘revolution’ takes place. Just as nineteenth century statistics transformed many measurements into population attributes (for instance, mean as the ideal or abstract property of a population), the shift between <span class="math inline">\(n\)</span> and <span class="math inline">\(\forall\boldsymbol{X}\)</span>, between some and all,  a shift very much dependent on machine learning, internalizes, I will suggest, population attributes into the operations of machine learners. This is a statistical event akin to the advent of the Normal distribution (and indeed, <span class="math inline">\(\mathnormal{N}\)</span> is a standard symbol for the Normal distribution in statistics textbooks) as a way of knowing and controlling populations <span class="citation">[@Hacking_1975, 108]</span>. To signal its continuity with the invention of probability, I term it ‘probabilisation,’ a pleonasm that refers that facet of the operational formation that renders knowledge in terms of probabilities. </p>
</div>
<div id="machine-learning-as-statistics-inside-out" class="section level2">
<h2><span class="header-section-number">8.2</span> Machine learning as statistics inside out</h2>
<p>The argument mimics Hacking’s. In <em>The Taming of Chance</em>, Hacking argues that modern statistical thought transposed a way of calculating errors in experimental measurements and astronomical observations into the real and constitute attributes of populations understood as processes of reproductive growth.  This transposition or inversion relied on four intermediate steps passing through the development of a probability calculus (particularly the work of Jacob Bernoulli and the binomial or heads-tails probability distribution in the 1690s <span class="citation">[@Hacking_1975, 143]</span>), the accumulation of large numbers of measurements (the most famous being the chest measurements of soldiers in Scottish regiments, but these were only one flurry amidst an avalanche of numbers in the 1830-1840s), the emergence of the idea of multiple, minute independent causes producing events (particularly as developed in medicine but also in studies of crime), and the ‘law of errors’ applying to measurements made by, amongst others, astronomers <span class="citation">[@Hacking_1990, 111-112]</span>.   As Hacking observes, coins, suicides, crime, chest measurements, and astronomical observations all pile up in a statistical aggregate which remains, although somewhat altered, indelible in contemporary statistical knowledges, particularly in its frequent recourse to notions of population, probability and distribution. In this entanglement, observers and the observed changed places. The distribution of errors made by astronomers measuring the position of stars or planets became a distribution or variation inherent in a population. </p>
<p>Machine learning reverse-engineers the invention of modern statistical thinking. It takes back the ‘real quantities’ – probabilities – that modern statistics had attributed to the populations in the world and distributes them to devices, to machine learners that people then observe, monitor and indeed measure again in many ways. The direct swapping between uncertainty in measurement and variation in real attributes that statistics achieved now finds itself re-routed and intensified as machine learners measure the errors, the bias and the variance of devices. Although it relies heavily on probability distributions, machine learning is a fat-tailed distribution of probability.</p>
<p>The swapping or re-distribution is not a simple mirror-image reversal, as if machine learners mistake devices for a population. Machine learning constantly takes statistical thinking as a basic condition for its operations and devices. When <em>Elements of Statistical Learning</em> states that (as we saw in the previous chapter) ‘our goal is to find a useful approximation <span class="math inline">\(\hat(f)(x)\)</span> to the function <span class="math inline">\(f(x)\)</span> that underlies the predictive relationship between input and output’ <span class="citation">[@Hastie_2009, 28]</span>, they invoke the ‘real quantities’ first elaborated and articulated by proto-statisticians such as Quetelet grappling with population and sample parameters. The major structuring operational practices in machine learning as a field of knowledge-practice show the marks of increasingly strong commitment to the reality of the statistical, and to the ongoing probabilisation of machine learners. </p>
<p>What is probabilisation in practice? Reading and working with machine learning techniques usually means encountering and responding to apparatus drawn from statistics, but the apparatus is not typically the statistical tests of significance or variation. In contrast to a statistics textbook such as the widely used <em>Basic Practice of Statistics</em> <span class="citation">[@Moore_2009]</span> or a more advanced guide such as <em>All of Statistics</em> <span class="citation">[@Wasserman_2003]</span>, where statistical tests (t-test, chi-squared test, etc.), hypothesis testing, and analysis of uncertainties (confidence intervals, etc.) order the exposition,  machine learning textbooks rely on a conceptual apparatus curiously stripped of statistical tests and measurements. Statistical underpinnings may be fundamental, but this does not mean that machine learners simply automate statistics.</p>

<p>Instead, a basic set of contrasts or indeed oppositions that owe much to probabilistic thinking order, compose, associate and link the statements of machine learners.  The contrasts shown in Table  all have a statistical facet and anchoring to them. Some refer to errors that affect how a machine learner refers to data (bias and variance; see discussion below); some designate an underlying statistical intuition about how particular machine learners treat data (does the model seek to generate the data or classify – discriminate – it; e.g. Naive Bayes or Latent Dirichlet Allocation are  models whereas logistic regression or support vector machines are <em>discriminative</em>);   parametric and non-parametric describe the role of probability distributions in the model;  and others indicate different kinds of statistical knowledge practice (prediction seeks to anticipate while inference seeks to interpret, etc.; also see discussion below).  These broad structuring differences reach down deeply into the architecture, the diagrams, the practices, statements and visual objects and computer code associated with <span class="math inline">\(N=\forall\boldsymbol{X}\)</span>. Because they anchor basic operations of machine learning in probability, formalisms derived from statistics have in the last two decades increasingly populated the field, furnishing and rearranging its diagrammatic references  to the worlds of industry, agriculture, earth science, genomics, etc., but also, crucially, triggering ontological mutations in machine learners themselves. </p>
</div>
<div id="distributed-probabilities" class="section level2">
<h2><span class="header-section-number">8.3</span> Distributed probabilities</h2>
<p>While these structuring differences deeply shape practice in machine learning, the underlying operator that allows swapping between knowledge and the world, between events and devices, is probability, and in particular, functions that describe variations in populations, probability distributions.  Probability distributions both map population variations and, as we will see, multiply the number of things that count as populations.  </p>
<p>The normal distribution pervades nineteenth century statistical thinking as it affects populations across law, medicine, agriculture, finance and not least, sociology as a domain of knowledge. Normal distributions appear in countless variations in scientific, government and institutional settings as functions that map events, measurements, observations and records to evidential probability quantities.<a href="#fn49" class="footnoteRef" id="fnref49"><sup>49</sup></a></p>

<p>The function shown in equation () expresses the probability of a given value of the variable <span class="math inline">\(x\)</span> given a population whose variations (with respect to <span class="math inline">\(x\)</span>) can be expressed in terms of two parameters, <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>, the mean and variance. This is the so-called normal or Gaussian distribution.<a href="#fn50" class="footnoteRef" id="fnref50"><sup>50</sup></a> Its mathematics were intensively worked over during the late eighteenth and early nineteenth centuries in what has been termed ‘one of the major success stories in the history of science’ <span class="citation">[@Stigler_1986, 158]</span>. It has a power-laden biopolitical history closely tied with knowledges and governing of populations in terms of morality, mortality, health, and wealth (see <span class="citation">[@Hacking_1975, 113-124]</span>.  The key parameters here include <span class="math inline">\(\mu\)</span>, the mean and <span class="math inline">\(\sigma\)</span>, the variance, a number that describes the dispersion of values of the variable, <span class="math inline">\(x\)</span> are. These two parameters together describe the shape of the curve. Given knowledge of <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>, the normal or Gaussian probability distribution maps all outcomes to probabilities (or numbers in the range <span class="math inline">\(0\)</span> to <span class="math inline">\(1\)</span>). Put statistically, functions such as the Gaussian distribution probabilise events as random variables. Every variable potentially becomes a function: ‘a random variable is a mapping that assigns a real number to each outcome’ <span class="citation">[@Wasserman_2003,19]</span>.  </p>
<p>The possibility of treating population variations as random variables, that is, as probability distributions, was a significant historical achievement, one that continues to develop and ramify.<a href="#fn51" class="footnoteRef" id="fnref51"><sup>51</sup></a> Random variables distribute probability in the world. When conceptualised as real quantities in the world rather than epiphenomenal by-products of inaccuracies in our observations or measuring devices, probability distributions weave directly into the productive operations of power.  Distribution in the sense of locating, positioning, partitioning, sectioning, serialising or queuing operations has received much more attention in critical thought (particularly in the many uses of Foucault’s concept of disciplinary power <span class="citation">[@Foucault_1977]</span>), but in almost every setting, distribution in the sense of counting, apportioning and weighting of different outcomes also operates. This constant interweaving of spatial, architectural, logistical and functional processes has energised statistical thought for several centuries.<a href="#fn52" class="footnoteRef" id="fnref52"><sup>52</sup></a>  For instance, given the normal distribution, it is possible, under certain circumstances, to effectively subjectify someone on the spot. If an educational psychologist indicates to someone that their intelligence lies towards the left-hand side of the normal curve peak (and hence less than the population mean), they quickly assign them to a potentially institutionally and economically consequential trajectory. Since its inception in the social physics of Adolphe Quetelet as a way of referring to a property of populations, the normal curve has not only described but modulated and re-shaped populations (in terms of health, morality and wealth). </p>
<p>If functions such as equation () have persisted for so long as elements of population governmentality or biopolitics,  what happen to them in machine learning? The pages of a book such as <em>Elements of Statistical Learning</em> show many signs of an ongoing invocation of probability distributions. We could simply observe their abundance. Hastie and co-authors invoke probability distributions. They speak of ‘Gaussian mixtures,’ ‘bivariate Gaussian distributions,’ standard Gaussian,‘’Gaussian kernels,’ ‘Gaussian assumptions,’ ‘Gaussian errors,’ ‘Gaussian noise,’ ‘Gaussian radial basis function,’ ‘Gaussian variables,’ ‘Gaussian densities,’ ‘Gaussian process,’ and so forth. (The term ‘normal’ appears in an even wider spectrum of similar guises.) Events, things, properties, operations, functions, and attributes all associate with probability distributions. </p>
<p>The multiple invocations of probability distributions attests to the variety of events (occurrence of cancer, occurrence of the word ‘Viagra’ in an email, a click on a hyperlink, etc.) map to real numbers. Despite the sometimes dense mathematical diagrammaticism, the term <em>distribution</em> emphasises a tangible and practically resonant way of thinking about how events or possible outcomes shift about as the parameters of a function vary.<a href="#fn53" class="footnoteRef" id="fnref53"><sup>53</sup></a> Whatever inferences and predictions become possible, probability distributions are a crucial control surface for machine learning understood as a form of movement through data.  In contrast to the endowment of living aggregates such as populations with probability that we see in the biopolitical history of statistics (and later in natural sciences such as physics and biology), statistical machine learning increasingly constitutes devices as populations via probability distributions. </p>
</div>
<div id="naive-bayes-and-the-distribution-of-probabilities" class="section level2">
<h2><span class="header-section-number">8.4</span> Naive Bayes and the distribution of probabilities</h2>
<p>How could machine learners become a population? The mathematical expression for one of the most popular of all machine learning classifiers, the Naive Bayes classifier, stands out for its probabilistic simplicity and seeming lack of ‘moving parts’. </p>

<blockquote>
<p><span class="citation">[@Hastie_2009, 211]</span></p>
</blockquote>
<p>Some machine learners are so simple that they can be implemented in a few lines of code. Along with the perceptron, linear regression, and <em>k</em> nearest neighbours, the function shown in equation () is one of the simplest one to be found in most machine textbooks yet easily adapts for high dimensional data, the kind of data associated with contemporary network infrastructures, scientific instruments, online communications and <span class="math inline">\(N = \forall\boldsymbol{X}\)</span> in general.<a href="#fn54" class="footnoteRef" id="fnref54"><sup>54</sup></a> Even though the Naive Bayes classifier is one of the most popular machine learning algorithms, it is more than 50 years old <span class="citation">[@Hand_2001]</span>.</p>
<p>The key diagrammatic elements of the classifier in the equation are <span class="math inline">\(\prod\)</span>, an operator that multiplies all the values of the matrix of <span class="math inline">\(X\)</span> values (from <span class="math inline">\(1\)</span> to <span class="math inline">\(p\)</span>) to generate a product. What product does the Naive Bayes classifier produce? The expression <span class="math inline">\(f_j(X)\)</span> refers to a probability density; that is, it describes the probability that a particular thing (a document, an image, an email message, a set of URLs, etc.) belongs to the class of things <span class="math inline">\(j\)</span>.  In constructing an estimate of the probability that a given message, image or event is an instance of class <span class="math inline">\(j\)</span>, <span class="math inline">\(p\)</span> different features are taken into account. ( The subscript <span class="math inline">\(k\)</span> indexes the <span class="math inline">\(p\)</span> dimensions of the vector space.) The subscripts <span class="math inline">\(k=1\)</span> on the <span class="math inline">\(\prod\)</span> operator, and <span class="math inline">\(k\)</span> on the data <span class="math inline">\(X_k\)</span> indicate that the Naive Bayes classifier makes use of a series of features or variables in calculating the overall probability that a given thing or observation belongs to a specific class. Put in the language of probability calculus, the classifier produces a probability density <span class="math inline">\(f_j(X)\)</span> by calculating the <em>joint probability</em> of all the <em>conditional</em> probabilities of the features or predictor variables in <span class="math inline">\(X\)</span> for the class <span class="math inline">\(j\)</span>. As <em>Elements of Statistical Learning</em> rather tersely puts it, ‘each of the class densities are products of the marginal densities’ <span class="citation">[@Hastie_2009,108]</span>.</p>
<p>The Naive Bayes classifier directly invokes probability (including its name, with its reference to the Bayes Theorem, an important late eighteenth century concept), yet there is little obvious connection to statistics in its modern form of tests of significance.  As Drew Conway and John Myles-White write in <em>Machine Learning for Hackers</em>,</p>
<blockquote>
<p>At its core, [Naive Bayes] … is a 20th century application of the 18th century concept of <em>conditional probability</em>. A conditional probability is the likelihood of observing some thing given some other thing we already know about <span class="citation">[@Conway_2012, 77]</span>  </p>
</blockquote>
<p>They point to the application of ‘conditional probability,’ a probability conditioned on the probability of something else. Conditional probability lies at the heart of many of the data transformation associated with prediction or pattern recognition since it links a class to the occurrence of combinations of variables or features. Naive Bayes links variables by simply multiplying probabilities.<a href="#fn55" class="footnoteRef" id="fnref55"><sup>55</sup></a>  As any of the many accounts of the technique will explain, the name comes from Bayes Theorem, one of the most basic yet widely used results in probability theory (again dating from the eighteenth century), yet Naive Bayes does not even fully embrace Bayes Theorem as the principle of its operation. The classifier has a simple architecture based on the concepts of conditional probability and joint probability; it calculates a probability density function <span class="math inline">\(f_j(X)\)</span> or probability distribution for each possible class of things as a combination of the probabilities of all the many features or attributes of populations that come together in data. It makes a drastically naïve assumption that features or variables are statistically independent of each other, where ‘independent’ means that they do not affect each other, or that they have no relation to each other. We will see below that dramatic simplifications such as independence do not necessarily weaken the referential grasp of machine learners on the world, but in certain ways allow them to reconfigure the operations of machine learners as a population of learners.</p>
</div>
<div id="spam-when-foralln-is-too-much" class="section level2">
<h2><span class="header-section-number">8.5</span> Spam: when <span class="math inline">\(\forall{N}\)</span> is too much?</h2>
<p><span class="math inline">\(\forall{N}\)</span> can be a bother. In <em>Doing Data Science</em>, Rachel Schutt and Cathy O’Neill furnish a <code>bash</code> script (that is, command line instructions) to download the well-known<code>Enron</code> email dataset  and build a Naive Bayes classifier that labels email as spam or not. In many ways, this is canonical machine learner pedagogy. For Naive Bayes, email spam detection has become the standard example (Andrew Ng uses it in CSS229, Lecture 5 <span class="citation">[@Ng_2008]</span>).  In this setting, machine learners operate as filters coping with too much communication. </p>
<p>A typical spam email in the <code>Enron</code> dataset, a dataset that derives from the U.S Federal Energy Regulatory Commission’s investigation into Enron Corporation <span class="citation">[@Klimt_2004]</span>, looks like this:</p>
<blockquote>
<p>Subject: it’s cheating, but it works ! can you guess how old she is ? the woman in this photograph looks like a happy teenager about to go to her high school prom, doesn’t she ? she’ s an international, professional model whose photographs have appeared in hundreds of ads and articles whenever a client needs a photo of an attractive, teenage girl.but guess what ? this model is not a teenager ! no, she is old enough to have a 7-year-old daughter.. she also says, &quot; if it weren’t for this amazing new cosmetic cream called ‘deception,’ i would lose hundreds of modeling assignments…because…there is no way i could pass myself off as a teenager.&quot; service dept 9420 reseda blvd # 133 northridge, ca 91324</p>
</blockquote>
<p>The text of a typical non-spam email like this:</p>
<blockquote>
<p>Subject: industrials suggestions…… ———————-forwarded by kenneth seaman / hou / ect on 01 / 04 / 2000 12 : 47 pm————————– - pat clynes @ enron 01 / 04 / 2000 12 : 46 pm to : kenneth seaman / hou / ect @ ect, robert e lloyd / hou / ect @ ect cc : subject : industrials ken and robert, the industrials should be completely transitioned to robert as of january 1, 2000.please let me know if this is not complete and what else is left to transition . thanks, pat</p>
</blockquote>
<p>Such communications, with their mixture of solicitation and imperative are familiar to anyone who uses email. How does Naive Bayes probablise their differences?  How do they become <span class="math inline">\(X\)</span> or even <span class="math inline">\(f_j(X)\)</span> in the Naive Bayes classifier? The code that <em>Doing Data Science</em> supplies is instructive:</p>

<p><span class="citation">[@Schutt_2013, 105-106]</span></p>
<p>The script draws out something of how the joint probability function in equation () probabilises a single word.<a href="#fn56" class="footnoteRef" id="fnref56"><sup>56</sup></a>  Not all machine learning models are so simple that they can be conveyed in 30 lines of code (including downloading the data and comments),  but the script signals that nothing that occurring in probabilisation  is intrinsically mysterious, elusive or indeed particularly abstract.<a href="#fn57" class="footnoteRef" id="fnref57"><sup>57</sup></a> On the contrary, the power of classifiers operates through the accumulated counting, adding, multiplying (that is, repeated adding) and dividing (that is, multiplying by parts or fractions) constrained by the joint probability distribution. Probability re-distributes things such as emails or documents as, in this case, events in a population of words. The Naive Bayes classifies endows every word in the <code>Enron</code> dataset with a probability density function. The classification of each email becomes a matter of estimating a conditional probability based on the joint probability distribution that quantifies the chance of all the words in that email appearing together. Probabilities are always between <code>0</code> and <code>1</code>, and classification entails selected a cutoff or dividing line. For instance, greater than <code>0.5</code> might result in a classification as <code>spam</code>. In the <code>enron</code> dataset, ‘finance’ has a NaN chance of being spam, while ‘sexy’ has a chance of NaN. Ironically, like the Naive Bayes classifier’s own reliance on seventeenth and eighteenth century probability calculus, the frequent application of this machine learner to document classification and retrieval echoes the seventeenth century thinking that first conceived of the very notion of ‘probability’ in relation to the evidential weight of documents <span class="citation">[@Hacking_1975, 85]</span>. </p>
</div>
<div id="the-improbable-success-of-the-naive-bayes-classifier" class="section level2">
<h2><span class="header-section-number">8.6</span> The improbable success of the Naive Bayes classifier</h2>
<p>There is something quite artificial at work in the construction of these populations and their associated probability distributions.  They are intentionally artificial and limited. They do not correspond or refer directly to what we know, for instance, of how language works, but instead to a rather different set of concerns. Like most machine learning techniques encountering complex realities, classifiers such as Naive Bayes ignore many obvious structural or semiotic features of emails as documents (for instance, word order, or co-occurrences of words). Yet this very artificiality or limitation in their reference to the world allows machine learners to appear in many different guises. Despite their simple architecture, Naive Bayes classifiers have been surprisingly successful. Many machine learners transform vectorised data into probability distributions populated by fields of random variables in process of change. They render all things as populations.</p>

<p>The altered relation between modern statistical and machine learning practice starts to appear in Naive Bayes from the early 1990’s as statisticians begins to generalize and re-diagram Naive Bayes by examining its statistical properties more carefully. Table  shows 30 of the most cited Naive Bayes-related scientific publications.<a href="#fn58" class="footnoteRef" id="fnref58"><sup>58</sup></a> The list of titles sketches a double movement. On the one hand, we see the typical diagonal forms of accumulation or positivity  of a machine learner across disciplines – computer science, statistics, molecular biology (especially of cancer), software engineering, internet portal construction, sentiment classification, and image ‘keypoint’ recognition. On the other hand, highly cited papers such as <span class="citation">[@Friedman_1997]</span> and <span class="citation">[@Hand_2001]</span> point to an intensified statistical treatment of machine learners during these years, an intensified probabilisation of machine learners that strongly affects their ongoing development (leading, for instance, to the much more document-oriented, heavily probabilistic topic models appearing in the following decade <span class="citation">[@Blei_2003]</span>). </p>
<p>In <em>Elements of Statistical Learning</em>, Hastie, Tibshirani and Friedman characterise the Naive Bayes classifier in terms of its capacity to deal with high dimensional data:</p>
<blockquote>
<p>It is especially appropriate when the dimension <span class="math inline">\(p\)</span> of the feature space is high, making density estimation unattractive. The naive Bayes model assumes that given a class <span class="math inline">\(G = j\)</span>, the features <span class="math inline">\(X_k\)</span> are independent <span class="citation">[@Hastie_2009, 211]</span>. </p>
</blockquote>
<p>Similar formulations can be found in most of the machine learning books and instructional materials. This appropriateness relates directly to <span class="math inline">\(\forall{\boldsymbol{X}}\)</span>, and the expansion of the vector space. As we saw above in equation (), <span class="math inline">\(p\)</span> stands for the number of different dimensions or variables in the data set. In the spam classifier, the number of dimensions balloon hundreds of thousands because every unique word adds a new dimension to the vector space. Compared to the complications of logistic regression, neural networks or support vector machines,  seems incredibly simple. How is it that a simple multiplication of probabilities and the assumption that ‘features … are independent’ can, as Hastie and co-authors write: ‘often outperform far more sophisticated alternatives’ <span class="citation">[@Hastie_2009, 211]</span>?</p>
<p>The answer to this conundrum of success does not lie in the increasing availability of data to train machine learners on. I want to explore two other contrasts as ways of viewing the probabilising processes at work in Naive Bayes. The first way to view this success is in terms of <em>ancestral communities</em> of probabilisation. The second concerns the statistical decomposition of machine learners in terms of their sources of error.   </p>
</div>
<div id="ancestral-probabilities-in-documents-inference-and-prediction" class="section level2">
<h2><span class="header-section-number">8.7</span> Ancestral probabilities in documents: inference and prediction</h2>
<p>Why is the Naive Bayes classifier is almost always demonstrated on the problem of filtering spam email <span class="citation">[@Conway_2012; @Schutt_2013, 93-113; @Kirk_2014, 53; @Lantz_2013, 92-93; @Flach_2012; @Ng_2008b]</span>, and in particular dealing with the abundance of spam emails mentioning a drug for erectile dysfunction sold under the tradename ‘Viagra’ (a drug that was itself the byproduct of the clinical trial for hypertension and heart disease)?  What are we to make of this regularity in production of statements? Admittedly spam, and spam trying to sell Viagra in particular, has been a very familiar part of most email since 1997 when Viagra was approved for sale, and of all the documents that machine learners mundanely encounter in quantity in those years, email might be the most numerous as well as one of the mundanely shared. Naive Bayes classifiers and variations of them also became practical devices in managing email traffic for most people, whether they know it or not, during the mid-1990s (see for instance, <a href="http://spamassassin.apache.org/">SpamAssassin</a>.(The other would be scientific publications. Many more recent machine learners train as classifiers on scientific publications <span class="citation">[@Blei_2007]</span> )</p>
<p>From an archaeological standpoint, the reiteration of email spam filtering using Naive Bayes is the effect of another process, a process akin to the attribution of probability distributions to populations in the nineteenth century. Like many machine learners, Naive Bayes has one important lineage derived from the problem of classifying and retrieving documents amidst archives. The operational practice of document classification is specified in the element of the archive.  Genealogical affiliation with a particular problem such as document classification (or image recognition) generates many re-iterations and versions of machine learners over time. As Lucy Suchman and Randall Trigg wrote in their study of work on artificial intelligence,</p>
<blockquote>
<p>rather than beginning with documented instances of situated inference … researchers begin with … postulates and problems handed down by the ancestral communities of computer science, systems engineering, philosophical logic, and the like <span class="citation">[@Suchman_1992,174]</span>. </p>
</blockquote>
<p>While Bayes Theorem dates from the 18th century, the highly successive use of Naive Bayes classifiers in email spam filtering in recent decades effectively draws on an ancestral community of document classification and information retrieval methods reaching back to the mid-20th century.<a href="#fn59" class="footnoteRef" id="fnref59"><sup>59</sup></a> </p>
<p>Early attempts to use what is now called Naive Bayes in the early 1960s re-iterated engagements with the evidential weight of documents that accompanied the emergence of probabilistic thinking as a quantification of belief in the seventeenth century <span class="citation">[@Hacking_1975, 35-49]</span>.  Working at the RAND Corporation in the early 1960s, M.E. Maron described how ‘automatic indexing’ of documents – Maron used papers published in computer engineering journals – could become ‘probabilistic automatic indexing.’ The necessary statistical assumption was:</p>
<blockquote>
<p>The fundamental thesis says, in effect, that statistics on kind, frequency, location, order, etc., of selected words are adequate to make reasonably good predictions about the subject matter of documents containing those words <span class="citation">[@Maron_1961, 406]</span> </p>
</blockquote>
<p>This thesis has remained somewhat fundamental in text classification and information retrieval applications, as well as many other machine learning approaches since. Maron’s work focused on a collection of several hundred abstracts of papers published in the March and June 1959 issues of the <em>IRE Transactions on Electronic Computers</em>. As in contemporary supervised learning, these abstracts were divided into two groups, a training and a test set (‘group 1’ and ‘group 2’ in Maron’s terminology <span class="citation">[@Maron_1961, 407]</span>), and the training set was classified according to 32 different categories that had already been in use by the Professional Group on Electronic Computers, the publishers of the <em>IRE Transactions</em>. Given these classifications, word counts for all distinct words in the abstracts were made, the most common terms (‘the’, ‘is’, ‘of’, ‘machine’, ‘data’, ‘computer’) and the most uncommon words removed, and the remaining set of around 1000 words were actually used for classification.</p>
<p>This treatment of the abstracts as documents, then as lists of words, and then as frequencies of terms, and finally as a filtered list of most information rich terms continues in much document and text classification work today.  A typical contemporary information retrieval textbook such as <span class="citation">[@Manning_2008]</span> devotes a chapter to the topic, including the canonical discussion of how simplifying assumptions about language and meaning do not vitiate the Naive Bayes classifier. Whenever machine learners announce the unlikely efficacy of classifiers, we might attend to the ways in which previous ‘ancestral probabilisations’ and archival constitution of the domain in question prepare the ground for that success.  </p>
</div>
<div id="statistical-decompositions-bias-variance-and-observed-errors" class="section level2">
<h2><span class="header-section-number">8.8</span> Statistical decompositions: bias, variance and observed errors</h2>
<p>Even with an eye on the ancestral communities that constantly accompany and heavily shape the indexical diagram of machine learning in the world, we still need a way of accounting for the artificiality of Naive Bayes.  The classifiers generates highly arbitrary probabilities of document class membership, yet these arbitrary probabilities still allow effective classification. Machine learners view the persistence of manifest artifice (in the case of Naive Bayes, a model that eschews any modelling of relations between things in the word such as words) in terms of another of the structuring differences of machine learning: the so-called <em>bias-variance_decomposition</em><span class="citation">[@Hastie_2009,24]</span>. </p>
<p>The terms ‘bias’ and ‘variance’ stem from the long history of statistical interest in errors (as Hacking’s account of the transposition of measurement errors into population norms illustrates).  The  and  of ‘estimators’ – the estimates of the parameters of the models usually written as <span class="math inline">\(\hat{\beta}\)</span> or <span class="math inline">\(\hat{\theta}\)</span>– feature heavily in machine learning discussions of prediction errors. The terms point to tensions that all machine learners experience. On the one hand, <em>variance</em> refers to the inevitable reliance of a machine learner on the data it ‘learns.’ To put it more formally, ‘variance refers to the amount by which <span class="math inline">\(\hat{f}\)</span> would change if we estimated it using a different training data set’ <span class="citation">[@James_2013,34]</span>. On the other hand, <em>bias</em> ‘refers to the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model’ <span class="citation">[@James_2013, 35]</span>.</p>
<p>These two sources of error, one which results from sampling and the other arising from the structure of the model or approximating function, can be reduced or at least subject to trade-off in what <em>Elements of Statistical Learning</em> terms ‘the bias-variance decomposition’ <span class="citation">[@Hastie_2009, 223]</span>.<a href="#fn60" class="footnoteRef" id="fnref60"><sup>60</sup></a> From the standpoint of the bias-variance decomposition, every machine learner makes a trade-off between the errors deriving from differences between samples, and errors due to the difference between the approximating function and the actual process that generated the data. Note that both sources of error in the bias-variance decomposition derive from transformations of the data. Variance affects how the model encounters the world (as a set of small samples or as, at the other end, a massive <span class="math inline">\(N=\forall{\boldsymbol{X}}\)</span> dataset). Bias relating to how the model ‘apprehends’ the data (as a set of almost coin-toss like independent events, as a geometrical problem of finding a line or curve that runs through a cloud of points, etc.).</p>
<p>Even with all the data, machine learning cannot fully circumvent the tensions between the different errors at work in the bias-variance decomposition.  Yet, sources of error do not always prove harmful.  The success of Naive Bayes (and <em>k</em> nearest neighbours classifier) runs counter to the long standing trend in statistics to construct increasingly sophisticated models of the domains they encounter. Writing in 1997, Jerome Friedman describes how very simple classifiers perform surprisingly well:</p>
<blockquote>
<p>Certain types of (very high) bias can be canceled by low variance to produce accurate classification <span class="citation">[@Friedman_1997,55]</span> </p>
</blockquote>
<p>A rather elaborate set of concepts and techniques address the bias-variance decomposition in the context of data availability. These techniques focus on managing the <em>test</em> or <em>generalization</em> error, the difference between the actual and predicted values produced by the machine learner when it encounters a fresh, hitherto unseen data sample. Machine learners in such settings still encounter the bias-variance trade-off as they select some data for training and some data for testing. This trade-off has to deal with the fact that training errors – the observed difference between what the model predicts and what the training data actually shows – are not a good guide to test or generalization error.   The process of fitting a model or finding a function (see previous chapter) will tend to reduce the training error by fitting the function more and more closely to the shape of the training data, but when it encounters fresh data that function might no longer fit well. In other words, a more sophisticated function may well reduce the bias but increase the variance. ‘Richer collections of models’ <span class="citation">[@Hastie_2009, 224]</span> reduce bias, but tend to increase variance. Conversely, models that cope well with fresh data (and Naive Bayes is a good example of such a machine learner), display low variance but high bias.</p>
<p>The trade-offs between bias and variance shift markedly between different types of models, and generates many different conceptual analyses of error in machine learning literature (‘optimism of the training error rate’ (228), ‘estimates of in-sample prediction error’ (230), ‘Bayesian information criterion’ (233), ‘Vapnik-Chervonenkis dimension’ (237), ‘minimum description length’ (235)) and technical methods of estimating prediction error (‘cross-validation’ (241), ‘bootstrap methods’ (249), ‘expectation-maximization algorithm’ (272), ‘bagging’ (282), or ‘Markov Chain Monte Carlo (MCMC)’ (279)), many of which date from the 1970s (e.g. cross-validation <span class="citation">[@Stone_1974]</span>, bootstrap <span class="citation">[@Efron_1979]</span>, expectation-maximization <span class="citation">[@Dempster_1977]</span>).  </p>
<p>A daunting field of concepts, themes, techniques and methods all gravitate to the threshold of probabilisation.  They invoke in some cases sophisticated mathematical or statistical constructs. They also very often rely on computational iteration or infrastructural scale to optimise parameters in models whose underlying intuitions remain quite straightforward (as in a linear regression or Naive Bayes). In some cases, the implementation of a model may be very simple, but analysis of how the machine learner manages to curtail a source of error such as bias or variance entails much more sophisticated statistical understanding. Many analyses of how a model becomes a ‘useful approximation’ reconfigure treat the models themselves as members of a population whose variations and uncertainties, whose tendencies and predispositions must be sampled, tested and monitored. The bias-variance decomposition points to an irreducible friction in the way that machine learning structures differences in the world. </p>
</div>
<div id="does-machine-learning-construct-a-new-statistical-reality" class="section level2">
<h2><span class="header-section-number">8.9</span> Does machine learning construct a new statistical reality?</h2>
<p> Following a broadly Foucaultean line of argument, Hacking proposes that statistical thinking and practice in the nineteenth and early twentieth century ontologically re-configured things in terms of probability distributions (and the Gaussian distribution in particular). What happens in worlds where the statistical treatment of error – the bias-variance decomposition is a shorthand term for this – distributes probability throughout an operational formation?  I have suggested that an ancestral probabilisation of domains and the statistical decomposition of error come together in statistical machine learning. The bias-variance decomposition includes both tightly bound points and certainly relatively free or unbound points, as we saw in the case of the Naive Bayes classifier in its encounter with data. It generates highly erroneous probability estimates but performs well as a classifier.</p>
<p>Viewed diagrammatically, unbound points matter greatly to the relations of force at work in a knowledge-power conjunction. Probabilisation gives machine learning a relation to its own plurality, to the tendencies of its models to proliferate and vary.  Every attempt to construct a machine learner in a given setting draws on both the re-iteration of ancestral probabilities (that is, prior structuring of settings in conformity with some probability distribution) or on the many interactive adjustments, re-distributions and re-samplings of the data <em>and</em> transformations of the models associated with the bias-variance decomposition.</p>
<p>Mayer-Schönberger and Cukier argue that having much data or all data (<span class="math inline">\(N=\forall{\boldsymbol{X}}\)</span>) re-bases knowledge.  Versions of this claim can be found running through various scientific and business settings throughout the 20th century.<a href="#fn61" class="footnoteRef" id="fnref61"><sup>61</sup></a> In certain settings, <span class="math inline">\(N=all\)</span> has been around for quite a while (as for instance, in many document classification settings where the whole archive or corpus of documents have been electronically curated for decades). Mayer-Schönberger and Cukier rightly emphasize that the huge quantities of data sluicing through some contemporary infrastructures support wider inferences (11). Their discounting of statistical sampling as a concept ‘developed to solve a particular problem at a particular moment in time under specific technological constraints’ <span class="citation">[@Mayer-Schonberger_2013, 31]</span> does not, however, accommodate the operational practices of sampling that pervade machine learning, particularly in the forms of probabilisation.</p>
<p>Whether or not someone uses Naive Bayes, a topic model, neural networks or logistic regression, does not greatly alter the processes of probabilisation. Random variables, probability distributions, errors and model selection practices crowd in around and re-configure machine learners as members of a population generating statements.  In many ways, the Mayer-Schönberger and Cukier account bobs in the wake of the enterprise-wide accumulations of data. They pay so much attention to the capital potentials of data accumulation that they cannot easily attend to the question of how machine learners probabilise that data. Sampling, estimation, likelihoods, and a whole gamut of dynamic relationships between random variables in joint probability distributions reassert themselves amidst a population of models. The data may not be sampled, but models moving through the high-dimensional vector spaces opened up by having ‘all’ the data transform it probabilistically. While not all machine learners are strictly speaking probabilistic models,<a href="#fn62" class="footnoteRef" id="fnref62"><sup>62</sup></a> machine learners relate to themselves and the data as populations defined by probability distributions.</p>
<p>Machine learning inhabits a reality that had already introjected statistical realities at least a century earlier, whether through the social physics of Quetelet, the biopolitical norms of Francis Galton and his regression to the mean (the linear model of regression is probably the basic machine learning model) or later, in the probability functions of quantum mechanics in early twentieth century physics.    Assembling an aggregate reality of many devices, machine learning inverts probability distributions. In this inversion, probability distributions, which had become the operational statement and model of truth for many different kinds of populations, fold back or re-distribute themselves into devices such as machine learners whose variations and uncertainties become populations. Populations of models are sampled, measured, and aggregated in the ongoing production of statistical realities whose object is no longer a property of individual members of a population (their height, their life-expectancy, their chance of HIV/AIDS), but a population of models of populations.</p>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="47">
<li id="fn47"><p>The historian of statistics Stephen Stigler provides a lengthy account of Fechner’s work in <span class="citation">[@Stigler_1986, 239-259]</span>.<a href="8-probabilisation-and-the-taming-of-machines.html#fnref47">↩</a></p></li>
<li id="fn48"><p>Rob Kitchin provides a very useful overview of these claims in <span class="citation">[@Kitchin_2014]</span>. While I will not analyse the claims about ‘big data’ in specific cases in any great detail, the growing literature on this topic suggests that machine learning in its various operations – epistopic construction of vector space, function finding as association of partial observers and a re-internalisation of probability – generates considerable difficulties and challenges for knowledge, power and production. <a href="8-probabilisation-and-the-taming-of-machines.html#fnref48">↩</a></p></li>
<li id="fn49"><p>Statistical graphics have a rich history and semiology that I do not discuss here (see <span class="citation">[@Bertin_1983]</span>).<a href="8-probabilisation-and-the-taming-of-machines.html#fnref49">↩</a></p></li>
<li id="fn50"><p>Dozens of differently shaped probability distributions map continuous and discrete variations to real numbers. Other probability distributions — normal (Gaussian), uniform, Cauchy exponential, gamma, beta, hypergeometric, binomial, Poisson, chi-squared, Dirichlet, Boltzmann-Gibbs distributions, etc. (see <span class="citation">[@NIST_2012]</span> for a gallery of distributions) — functionally express widely differing patterns.  The queuing times at airport check-ins do not, for instance, easily fit a normal distribution. Statisticians model queues using a Poisson distribution, in which, unfortunately for travellers, distributes the number of events in a given time interval quite broadly. Similarly, it might be better to think of the probability of rain today in north-west England in terms of a Poisson distribution that models clouds in the Atlantic queuing to rain on the northwest coast of England. (Rather than addressing the question of whether it will rain or not, a Poisson-based model might address the question of how many times it will rain today.)<a href="8-probabilisation-and-the-taming-of-machines.html#fnref50">↩</a></p></li>
<li id="fn51"><p>The mapping that assigns numbers to outcomes (heads v. tails; cancer v. benign; spam v. not-spam) is a probability distribution. As I have argued in <span class="citation">[@Mackenzie_2015d]</span>, random variables have become much more widespread in statistical practice due to changes in computational techniques. <a href="8-probabilisation-and-the-taming-of-machines.html#fnref51">↩</a></p></li>
<li id="fn52"><p>‘Distribution’ pervades Foucault’s account of power and knowledge from <em>The Order of Things</em> <span class="citation">[@Foucault_1992]</span> onwards.  Foucault treats distributions in several different ways: as spatial or logistical techniques, as mathematical orderings of large numbers of people or things, and as a methodological and theoretical framing device. In <em>Discipline and Punish</em> <span class="citation">[@Foucault_1977]</span>, the spatial sense prevails, but in later works, the population or demographic sense of distribution takes precedence <span class="citation">[@Foucault_1998]</span>. Distribution certainly has theoretical primacy in his account of power: ‘relations of power-knowledge are not static forms of distribution, they are “matrices of transformations”’ <span class="citation">[@Foucault_1998, 99]</span>.<a href="8-probabilisation-and-the-taming-of-machines.html#fnref52">↩</a></p></li>
<li id="fn53"><p>Machine learners adjust these parameters in different ways. For instance, parametric and non-parametric models (see table ) differ in that the former have a limited number of parameters and the latter an undefined number of parameters (for instance, Naive Bayes, <em>k</em> nearest neighbours or support vector machine models). But both kinds assume that an underlying probability distribution – a function, ‘unobservable’ or not – operates, even if it changes with new data. A probability distribution under these assumptions becomes the closest reality we have to whatever process generated all the variations in data gathered through experiments and observations. From a probabilistic perspective, the task of machine learning is to estimate the parameters (the mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma\)</span> in the case of Gaussian curve) that shape of the curve of the probability distribution.<a href="8-probabilisation-and-the-taming-of-machines.html#fnref53">↩</a></p></li>
<li id="fn54"><p>The other contender for simplest machine learner would be the also very popular <em>k</em> nearest neighbours. As Hastie et. al. observe: ‘these classifiers are memory-based and require no model to be fit’ <span class="citation">[@Hastie_2009, 463]</span>. Like the Naive Bayes classifier, the equation for <em>k</em> nearest neighbours is simple:</p><p>where <span class="math inline">\(\textit{N}_{k}(x)\)</span> is the neighborhood of <span class="math inline">\(x\)</span> defined by the <span class="math inline">\(k\)</span> closest points <span class="math inline">\(x_{i}\)</span> in the training sample <span class="citation">[@Hastie_2009, 14]</span>.</p><p>In equation , a parameter appears: <span class="math inline">\(k\)</span>, the number of neighbours. This contrasts greatly with the linear models discussed in chapters  and  where the number of parameters <span class="math inline">\(p\)</span> usually equals the number of variables in the dataset or dimensions in the vector space. <a href="8-probabilisation-and-the-taming-of-machines.html#fnref54">↩</a></p></li>
<li id="fn55"><p>In <span class="citation">[@Mackenzie_2014c]</span>, I have suggested that the intensification of multiplication associated with probabilistic calculation may constitute an important mutation in the ontological and practical texture of numbers. The epidemiological modelling of H1N1 influenza in London 2009 involved multiplying a great variety of probability distributions in order to calculate the conditional probability of influenza over time.<a href="8-probabilisation-and-the-taming-of-machines.html#fnref55">↩</a></p></li>
<li id="fn56"><p>The input to the script is a single word such as ‘finance’ or ‘deal’. The model is so simple that it only classifies a single word as spam. The <code>bash</code> script carries out four different transformations of the data in building the model. It uses only command line tools such as <code>wc</code> (word count), <code>bc</code> (basic calculator), <code>grep</code> (text search using pattern matching) and <code>echo</code> (display a line of text). These tools or utilities are readily available in almost any UNIX-based operating system (e.g. Linux, MacOS, etc.). The point of using only these utilities is to illustrate the simplicity of the algorithmic implementation of the model. The first part of the code downloads the sample dataset of Enron emails (and I will discuss spam emails and their role in machine learning below). Note that this dataset has already been divided into two classes - ‘spam’ and ‘ham’ – and emails of each class have been placed in separate directories or folders as individual text files.  <a href="8-probabilisation-and-the-taming-of-machines.html#fnref56">↩</a></p></li>
<li id="fn57"><p>After fetching the dataset from a website, the code excerpted in  counts the number of emails in each category <code>spam</code> or <code>ham</code>, and then counts the number of times that the chosen word (e.g. ‘finance’ or ‘deal’) occurs in both the spam and non-spam or ham categories. In Part 2, using these counts the script estimates probabilities of any email being spam or ham, and then given that email is spam or ham, that the particular word occurs. (To estimate a probability means, in this case, to divide the word count for the chosen word by the count of the number of spam emails, and ditto for the ham emails.) In Part 3, the final transformation of the data, these probabilities are used to calculate the probability of any one email being spam given the presence of that word. Again, the mathematical operations here are no more complicated than adding, multiplying and dividing. The probability that the chosen word is a spam word is, for instance, the probability of occurrence of the word in a spam email multiplied by the overall probability that an email is spam. Finally, given that the probability of the chosen word occurring in the email dataset is the probability of it occurring in spam plus the probability of it occurring in ham, the overall probability that an email in the Enron data is spam given the presence of that word can be calculated. (It is the probability that the chosen word is a spam word divided by the probability of that word in general.)<a href="8-probabilisation-and-the-taming-of-machines.html#fnref57">↩</a></p></li>
<li id="fn58"><p>Citation counts, even from the more reliable Reuters-Thomson Web of Science database, are difficult to evaluate when moving between disciplines. Some fields, such as computer science and biology, publish huge numbers of papers compared to smaller disciplines such as astronomy or plant ecology.<a href="8-probabilisation-and-the-taming-of-machines.html#fnref58">↩</a></p></li>
<li id="fn59"><p>The other lineage descends from medical diagnosis. For instance, starting in 1960, Homer Warner, Alan Toronto and George Veasy, working at the University of Utah and Latter-day Saints Hospital in Salt Lake City, began to develop a probabilistic computer model for diagnosis of heart disease <span class="citation">[@Warner_1961; @Warner_1964]</span>. Their model used exactly the same ‘equation of conditional probability’ we see in equation  but now used to ‘express the logical process used by a clinician in making a diagnosis based on clinical data’ <span class="citation">[@Warner_1961, 177]</span>. Despite the mention of logic in this description, the diagnostic model was thoroughly probabilistic in the sense that the model itself has no representation of logic included in its workings. Rather it calculates the probability of a given type of heart disease given ‘statistical data on the incidence of symptoms’ <span class="citation">[@Warner_1964, 558]</span>. Somewhat ironically, as they point out, physicians involved in preparing and submitting data to the diagnostic program improved the accuracy in their own diagnoses. In 1964, N.J Bailey was taking the same approach to medical diagnosis <span class="citation">[@Bailey_1965]</span>.   Heart disease to a central topic in machine learning (see chapter  for discussion of the <code>South African Heart Disease</code> dataset ).<a href="8-probabilisation-and-the-taming-of-machines.html#fnref59">↩</a></p></li>
<li id="fn60"><p>Another source of error, the ‘irreducible error’ <span class="citation">[@Hastie_2009, 37]</span> is noise that no model can eliminate.<a href="8-probabilisation-and-the-taming-of-machines.html#fnref60">↩</a></p></li>
<li id="fn61"><p>Later chapters of this book will track several instances of having all the data in the sciences, in government and in business in order to show what having all the data entails in different settings. <a href="8-probabilisation-and-the-taming-of-machines.html#fnref61">↩</a></p></li>
<li id="fn62"><p>Machine learning textbooks written by computer scientists tend to define probabilistic models more narrowly. As Peter Flach suggests:</p><blockquote><p>Probabilistic models view learning as a process of reducing uncertainty using data. For instance, a Bayesian classifier models the posterior distribution <span class="math inline">\(P(Y|X)\)</span> (or its counterpart, the likelihood function <span class="math inline">\(P(X|Y)\)</span>) which tells me the class distribution <span class="math inline">\(Y\)</span> after observing the features values <span class="math inline">\(X\)</span> <span class="citation">[@Flach_2012, 47]</span></p></blockquote><p>But whether they are probabilistic in this sense or not, the evaluation and configuring of machine learners irreducibly depends on a statistical treatment of errors and their trade-offs. <a href="8-probabilisation-and-the-taming-of-machines.html#fnref62">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="7-the-power-to-learn.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="9-patterns-and-differences.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
