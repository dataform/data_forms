<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Machine Learning: Archaeology of a Data Practice</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="Machine Learning: Archaeology of a Data Practice">
  <meta name="generator" content="bookdown 0.3 and GitBook 2.6.7">

  <meta property="og:title" content="Machine Learning: Archaeology of a Data Practice" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Machine Learning: Archaeology of a Data Practice" />
  
  
  

<meta name="author" content="Adrian Mackenzie">


<meta name="date" content="2016-12-10">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="20-implicit-vectorization-in-code-and-infrastructures.html">
<link rel="next" href="22-the-vectorised-table.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="1-acknowledgments.html"><a href="1-acknowledgments.html"><i class="fa fa-check"></i><b>1</b> Acknowledgments</a></li>
<li class="chapter" data-level="2" data-path="2-preface.html"><a href="2-preface.html"><i class="fa fa-check"></i><b>2</b> Preface</a></li>
<li class="chapter" data-level="3" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html"><i class="fa fa-check"></i><b>3</b> Introduction: Into the Data</a><ul>
<li class="chapter" data-level="3.1" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#three-accumulations-settings-data-and-devices"><i class="fa fa-check"></i><b>3.1</b> Three accumulations: settings, data and devices</a></li>
<li class="chapter" data-level="3.2" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#who-or-what-is-a-machine-learner"><i class="fa fa-check"></i><b>3.2</b> Who or what is a machine learner?</a></li>
<li class="chapter" data-level="3.3" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#algorithmic-control-to-the-machine-learners"><i class="fa fa-check"></i><b>3.3</b> Algorithmic control to the machine learners?</a></li>
<li class="chapter" data-level="3.4" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#the-archaeology-of-operations"><i class="fa fa-check"></i><b>3.4</b> The archaeology of operations</a></li>
<li class="chapter" data-level="3.5" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#asymmetries-in-common-knowledge"><i class="fa fa-check"></i><b>3.5</b> Asymmetries in common knowledge</a></li>
<li class="chapter" data-level="3.6" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#what-cannot-be-automated"><i class="fa fa-check"></i><b>3.6</b> What cannot be automated?</a></li>
<li class="chapter" data-level="3.7" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#different-fields-in-machine-learning"><i class="fa fa-check"></i><b>3.7</b> Different fields in machine learning?</a></li>
<li class="chapter" data-level="3.8" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#the-diagram-in-critical-thought"><i class="fa fa-check"></i><b>3.8</b> The diagram in critical thought</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-we-dont-have-to-write-programs.html"><a href="4-we-dont-have-to-write-programs.html"><i class="fa fa-check"></i><b>4</b> ‘We don’t have to write programs’?</a></li>
<li class="chapter" data-level="5" data-path="5-the-elements-of-machine-learning.html"><a href="5-the-elements-of-machine-learning.html"><i class="fa fa-check"></i><b>5</b> The elements of machine learning</a></li>
<li class="chapter" data-level="6" data-path="6-who-reads-machine-learning-textbooks.html"><a href="6-who-reads-machine-learning-textbooks.html"><i class="fa fa-check"></i><b>6</b> Who reads machine learning textbooks?</a></li>
<li class="chapter" data-level="7" data-path="7-r-a-matrix-of-transformations.html"><a href="7-r-a-matrix-of-transformations.html"><i class="fa fa-check"></i><b>7</b> <code>R</code>: a matrix of transformations</a></li>
<li class="chapter" data-level="8" data-path="8-the-obdurate-mathematical-glint-of-machine-learning.html"><a href="8-the-obdurate-mathematical-glint-of-machine-learning.html"><i class="fa fa-check"></i><b>8</b> The obdurate mathematical glint of machine learning</a></li>
<li class="chapter" data-level="9" data-path="9-cs229-2007-returning-again-and-again-to-certain-features.html"><a href="9-cs229-2007-returning-again-and-again-to-certain-features.html"><i class="fa fa-check"></i><b>9</b> CS229, 2007: returning again and again to certain features</a></li>
<li class="chapter" data-level="10" data-path="10-the-visible-learning-of-machine-learning.html"><a href="10-the-visible-learning-of-machine-learning.html"><i class="fa fa-check"></i><b>10</b> The visible learning of machine learning</a></li>
<li class="chapter" data-level="11" data-path="11-the-diagram-of-an-operational-formation.html"><a href="11-the-diagram-of-an-operational-formation.html"><i class="fa fa-check"></i><b>11</b> The diagram of an operational formation</a></li>
<li class="chapter" data-level="12" data-path="12-vectorisation-and-its-consequences.html"><a href="12-vectorisation-and-its-consequences.html"><i class="fa fa-check"></i><b>12</b> Vectorisation and its consequences}</a></li>
<li class="chapter" data-level="13" data-path="13-vector-space-and-geometry.html"><a href="13-vector-space-and-geometry.html"><i class="fa fa-check"></i><b>13</b> Vector space and geometry</a></li>
<li class="chapter" data-level="14" data-path="14-mixing-places.html"><a href="14-mixing-places.html"><i class="fa fa-check"></i><b>14</b> Mixing places</a></li>
<li class="chapter" data-level="15" data-path="15-truth-is-no-longer-in-the-table.html"><a href="15-truth-is-no-longer-in-the-table.html"><i class="fa fa-check"></i><b>15</b> Truth is no longer in the table?</a></li>
<li class="chapter" data-level="16" data-path="16-the-epistopic-fault-line-in-tables.html"><a href="16-the-epistopic-fault-line-in-tables.html"><i class="fa fa-check"></i><b>16</b> The epistopic fault line in tables</a></li>
<li class="chapter" data-level="17" data-path="17-surface-and-depths-the-problem-of-volume-in-data.html"><a href="17-surface-and-depths-the-problem-of-volume-in-data.html"><i class="fa fa-check"></i><b>17</b> Surface and depths: the problem of volume in data</a></li>
<li class="chapter" data-level="18" data-path="18-vector-space-expansion.html"><a href="18-vector-space-expansion.html"><i class="fa fa-check"></i><b>18</b> Vector space expansion</a></li>
<li class="chapter" data-level="19" data-path="19-drawing-lines-in-a-common-space-of-transformation.html"><a href="19-drawing-lines-in-a-common-space-of-transformation.html"><i class="fa fa-check"></i><b>19</b> Drawing lines in a common space of transformation</a></li>
<li class="chapter" data-level="20" data-path="20-implicit-vectorization-in-code-and-infrastructures.html"><a href="20-implicit-vectorization-in-code-and-infrastructures.html"><i class="fa fa-check"></i><b>20</b> Implicit vectorization in code and infrastructures</a></li>
<li class="chapter" data-level="21" data-path="21-lines-traversing-behind-the-light.html"><a href="21-lines-traversing-behind-the-light.html"><i class="fa fa-check"></i><b>21</b> Lines traversing behind the light</a></li>
<li class="chapter" data-level="22" data-path="22-the-vectorised-table.html"><a href="22-the-vectorised-table.html"><i class="fa fa-check"></i><b>22</b> The vectorised table?</a></li>
<li class="chapter" data-level="23" data-path="23-machines-finding-functions.html"><a href="23-machines-finding-functions.html"><i class="fa fa-check"></i><b>23</b> Machines finding functions}</a></li>
<li class="chapter" data-level="24" data-path="24-learning-functions.html"><a href="24-learning-functions.html"><i class="fa fa-check"></i><b>24</b> Learning functions</a></li>
<li class="chapter" data-level="25" data-path="25-supervised-unsupervised-reinforcement-learning-and-functions.html"><a href="25-supervised-unsupervised-reinforcement-learning-and-functions.html"><i class="fa fa-check"></i><b>25</b> Supervised, unsupervised, reinforcement learning and functions</a></li>
<li class="chapter" data-level="26" data-path="26-which-function-operates.html"><a href="26-which-function-operates.html"><i class="fa fa-check"></i><b>26</b> Which function operates?</a></li>
<li class="chapter" data-level="27" data-path="27-what-does-a-function-learn.html"><a href="27-what-does-a-function-learn.html"><i class="fa fa-check"></i><b>27</b> What does a function learn?</a></li>
<li class="chapter" data-level="28" data-path="28-observing-with-curves-the-logistic-function.html"><a href="28-observing-with-curves-the-logistic-function.html"><i class="fa fa-check"></i><b>28</b> Observing with curves: the logistic function</a></li>
<li class="chapter" data-level="29" data-path="29-the-cost-of-curves-in-machine-learning.html"><a href="29-the-cost-of-curves-in-machine-learning.html"><i class="fa fa-check"></i><b>29</b> The cost of curves in machine learning</a></li>
<li class="chapter" data-level="30" data-path="30-curves-and-the-variation-in-models.html"><a href="30-curves-and-the-variation-in-models.html"><i class="fa fa-check"></i><b>30</b> Curves and the variation in models</a></li>
<li class="chapter" data-level="31" data-path="31-observing-costs-losses-and-objectives-through-optimisation.html"><a href="31-observing-costs-losses-and-objectives-through-optimisation.html"><i class="fa fa-check"></i><b>31</b> Observing costs, losses and objectives through optimisation</a></li>
<li class="chapter" data-level="32" data-path="32-gradients-as-partial-observers.html"><a href="32-gradients-as-partial-observers.html"><i class="fa fa-check"></i><b>32</b> Gradients as partial observers</a></li>
<li class="chapter" data-level="33" data-path="33-the-power-to-learn.html"><a href="33-the-power-to-learn.html"><i class="fa fa-check"></i><b>33</b> The power to learn</a></li>
<li class="chapter" data-level="34" data-path="34-probabilisation-and-the-taming-of-machines.html"><a href="34-probabilisation-and-the-taming-of-machines.html"><i class="fa fa-check"></i><b>34</b> Probabilisation and the Taming of Machines}</a></li>
<li class="chapter" data-level="35" data-path="35-data-reduces-uncertainty.html"><a href="35-data-reduces-uncertainty.html"><i class="fa fa-check"></i><b>35</b> Data reduces uncertainty?</a></li>
<li class="chapter" data-level="36" data-path="36-machine-learning-as-statistics-inside-out.html"><a href="36-machine-learning-as-statistics-inside-out.html"><i class="fa fa-check"></i><b>36</b> Machine learning as statistics inside out</a></li>
<li class="chapter" data-level="37" data-path="37-distributed-probabilities.html"><a href="37-distributed-probabilities.html"><i class="fa fa-check"></i><b>37</b> Distributed probabilities</a></li>
<li class="chapter" data-level="38" data-path="38-naive-bayes-and-the-distribution-of-probabilities.html"><a href="38-naive-bayes-and-the-distribution-of-probabilities.html"><i class="fa fa-check"></i><b>38</b> Naive Bayes and the distribution of probabilities</a></li>
<li class="chapter" data-level="39" data-path="39-spam-when-foralln-is-too-much.html"><a href="39-spam-when-foralln-is-too-much.html"><i class="fa fa-check"></i><b>39</b> Spam: when <span class="math inline">\(\forall{N}\)</span> is too much?</a></li>
<li class="chapter" data-level="40" data-path="40-the-improbable-success-of-the-naive-bayes-classifier.html"><a href="40-the-improbable-success-of-the-naive-bayes-classifier.html"><i class="fa fa-check"></i><b>40</b> The improbable success of the Naive Bayes classifier</a></li>
<li class="chapter" data-level="41" data-path="41-ancestral-probabilities-in-documents-inference-and-prediction.html"><a href="41-ancestral-probabilities-in-documents-inference-and-prediction.html"><i class="fa fa-check"></i><b>41</b> Ancestral probabilities in documents: inference and prediction</a></li>
<li class="chapter" data-level="42" data-path="42-statistical-decompositions-bias-variance-and-observed-errors.html"><a href="42-statistical-decompositions-bias-variance-and-observed-errors.html"><i class="fa fa-check"></i><b>42</b> Statistical decompositions: bias, variance and observed errors</a></li>
<li class="chapter" data-level="43" data-path="43-does-machine-learning-construct-a-new-statistical-reality.html"><a href="43-does-machine-learning-construct-a-new-statistical-reality.html"><i class="fa fa-check"></i><b>43</b> Does machine learning construct a new statistical reality?</a></li>
<li class="chapter" data-level="44" data-path="44-patterns-and-differences.html"><a href="44-patterns-and-differences.html"><i class="fa fa-check"></i><b>44</b> Patterns and differences</a></li>
<li class="chapter" data-level="45" data-path="45-splitting-and-the-growth-of-trees.html"><a href="45-splitting-and-the-growth-of-trees.html"><i class="fa fa-check"></i><b>45</b> Splitting and the growth of trees</a></li>
<li class="chapter" data-level="46" data-path="46-differences-in-recursive-partitioning.html"><a href="46-differences-in-recursive-partitioning.html"><i class="fa fa-check"></i><b>46</b> 1984: Differences in recursive partitioning</a></li>
<li class="chapter" data-level="47" data-path="47-limiting-differences.html"><a href="47-limiting-differences.html"><i class="fa fa-check"></i><b>47</b> Limiting differences</a></li>
<li class="chapter" data-level="48" data-path="48-the-successful-dispersion-of-the-support-vector-machine.html"><a href="48-the-successful-dispersion-of-the-support-vector-machine.html"><i class="fa fa-check"></i><b>48</b> The successful dispersion of the support vector machine</a></li>
<li class="chapter" data-level="49" data-path="49-differences-blur.html"><a href="49-differences-blur.html"><i class="fa fa-check"></i><b>49</b> Differences blur?</a></li>
<li class="chapter" data-level="50" data-path="50-bending-the-decision-boundary.html"><a href="50-bending-the-decision-boundary.html"><i class="fa fa-check"></i><b>50</b> Bending the decision boundary</a></li>
<li class="chapter" data-level="51" data-path="51-instituting-patterns.html"><a href="51-instituting-patterns.html"><i class="fa fa-check"></i><b>51</b> Instituting patterns</a></li>
<li class="chapter" data-level="52" data-path="52-regularizing-and-materializing-objects.html"><a href="52-regularizing-and-materializing-objects.html"><i class="fa fa-check"></i><b>52</b> Regularizing and materializing objects}</a></li>
<li class="chapter" data-level="53" data-path="53-genomic-referentiality-and-materiality.html"><a href="53-genomic-referentiality-and-materiality.html"><i class="fa fa-check"></i><b>53</b> Genomic referentiality and materiality</a></li>
<li class="chapter" data-level="54" data-path="54-the-genome-as-threshold-object.html"><a href="54-the-genome-as-threshold-object.html"><i class="fa fa-check"></i><b>54</b> The genome as threshold object</a></li>
<li class="chapter" data-level="55" data-path="55-genomic-knowledges-and-their-datasets.html"><a href="55-genomic-knowledges-and-their-datasets.html"><i class="fa fa-check"></i><b>55</b> Genomic knowledges and their datasets</a></li>
<li class="chapter" data-level="56" data-path="56-the-advent-of-wide-dirty-and-mixed-data.html"><a href="56-the-advent-of-wide-dirty-and-mixed-data.html"><i class="fa fa-check"></i><b>56</b> The advent of ‘wide, dirty and mixed’ data</a></li>
<li class="chapter" data-level="57" data-path="57-cross-validating-machine-learning-in-genomics.html"><a href="57-cross-validating-machine-learning-in-genomics.html"><i class="fa fa-check"></i><b>57</b> Cross-validating machine learning in genomics</a></li>
<li class="chapter" data-level="58" data-path="58-proliferation-of-discoveries.html"><a href="58-proliferation-of-discoveries.html"><i class="fa fa-check"></i><b>58</b> Proliferation of discoveries</a></li>
<li class="chapter" data-level="59" data-path="59-variations-in-the-object-or-in-the-machine-learner.html"><a href="59-variations-in-the-object-or-in-the-machine-learner.html"><i class="fa fa-check"></i><b>59</b> Variations in the object or in the machine learner?</a></li>
<li class="chapter" data-level="60" data-path="60-whole-genome-functions.html"><a href="60-whole-genome-functions.html"><i class="fa fa-check"></i><b>60</b> Whole genome functions</a></li>
<li class="chapter" data-level="61" data-path="61-propagating-subject-positions.html"><a href="61-propagating-subject-positions.html"><i class="fa fa-check"></i><b>61</b> Propagating subject positions}</a></li>
<li class="chapter" data-level="62" data-path="62-propagation-across-human-machine-boundaries.html"><a href="62-propagation-across-human-machine-boundaries.html"><i class="fa fa-check"></i><b>62</b> Propagation across human-machine boundaries</a></li>
<li class="chapter" data-level="63" data-path="63-competitive-positioning.html"><a href="63-competitive-positioning.html"><i class="fa fa-check"></i><b>63</b> Competitive positioning</a></li>
<li class="chapter" data-level="64" data-path="64-a-privileged-machine-and-its-diagrammatic-forms.html"><a href="64-a-privileged-machine-and-its-diagrammatic-forms.html"><i class="fa fa-check"></i><b>64</b> A privileged machine and its diagrammatic forms</a></li>
<li class="chapter" data-level="65" data-path="65-varying-subject-positions-in-code.html"><a href="65-varying-subject-positions-in-code.html"><i class="fa fa-check"></i><b>65</b> Varying subject positions in code</a></li>
<li class="chapter" data-level="66" data-path="66-the-subjects-of-a-hidden-operation.html"><a href="66-the-subjects-of-a-hidden-operation.html"><i class="fa fa-check"></i><b>66</b> The subjects of a hidden operation</a></li>
<li class="chapter" data-level="67" data-path="67-algorithms-that-propagate-errors.html"><a href="67-algorithms-that-propagate-errors.html"><i class="fa fa-check"></i><b>67</b> Algorithms that propagate errors</a></li>
<li class="chapter" data-level="68" data-path="68-competitions-as-examination.html"><a href="68-competitions-as-examination.html"><i class="fa fa-check"></i><b>68</b> Competitions as examination</a></li>
<li class="chapter" data-level="69" data-path="69-superimposing-power-and-knowledge.html"><a href="69-superimposing-power-and-knowledge.html"><i class="fa fa-check"></i><b>69</b> Superimposing power and knowledge</a></li>
<li class="chapter" data-level="70" data-path="70-ranked-subject-positions.html"><a href="70-ranked-subject-positions.html"><i class="fa fa-check"></i><b>70</b> Ranked subject positions</a></li>
<li class="chapter" data-level="71" data-path="71-conclusion-out-of-the-data.html"><a href="71-conclusion-out-of-the-data.html"><i class="fa fa-check"></i><b>71</b> Conclusion: Out of the Data}</a></li>
<li class="chapter" data-level="72" data-path="72-machine-learners.html"><a href="72-machine-learners.html"><i class="fa fa-check"></i><b>72</b> 250,000 machine learners</a></li>
<li class="chapter" data-level="73" data-path="73-a-summary-of-the-argument.html"><a href="73-a-summary-of-the-argument.html"><i class="fa fa-check"></i><b>73</b> A summary of the argument</a></li>
<li class="chapter" data-level="74" data-path="74-in-situ-hybridization.html"><a href="74-in-situ-hybridization.html"><i class="fa fa-check"></i><b>74</b> In-situ hybridization</a></li>
<li class="chapter" data-level="75" data-path="75-critical-operational-practice.html"><a href="75-critical-operational-practice.html"><i class="fa fa-check"></i><b>75</b> Critical operational practice?</a></li>
<li class="chapter" data-level="76" data-path="76-obstacles-to-the-work-of-freeing-machine-learning.html"><a href="76-obstacles-to-the-work-of-freeing-machine-learning.html"><i class="fa fa-check"></i><b>76</b> Obstacles to the work of freeing machine learning</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning: Archaeology of a Data Practice</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="lines-traversing-behind-the-light" class="section level1">
<h1><span class="header-section-number">21</span> Lines traversing behind the light</h1>
<p>How does the combination of algebraic vector space and vectorised code play out in data? ‘We fit a linear model’ write Hastie and co-authors, referring to one epistopic operation on <code>prostate</code> data in <em>Elements of Statistical Learning</em>. In <code>R</code> this might look like the code excerpt shown below:</p>


<p>Table  displays estimates of the coefficients or parameters $} that define the direction of a flat surface running through the vector space of the <code>prostate</code> data.<a href="#fn31" class="footnoteRef" id="fnref31"><sup>31</sup></a>  This new vector is a product of operations in the vectorized <code>prostate</code> data. Some vectorizing operations can be seen in <code>R</code> code in listing  (for instance, <code>as.matrix</code> or <code>scale(prostate_standard)</code>).</p>
<p>The ‘unique solution’ to the problem of fitting a linear model to a given dataset using the popular method of ‘least squares’ <span class="citation">[@Hastie_2009, 12]</span> is given by the operations we have seen in equation . This tightly coiled expression calculates the <span class="math inline">\(\hat{\beta}\)</span> parameters that set the slope and location of a flat surface or plane in nine-dimensional vector space using all of the <code>prostate</code> variables apart from one variable chosen as the response or predicted variable, in this case <code>lpsa</code>. <span class="math inline">\(X\)</span> and <span class="math inline">\(y\)</span> matrices are multiplied, transposed (a form of rotation that swaps rows for columns) and inverted (a more complex operation that finds another matrix) in a series of linear algebra transformations. Epitomising the implicitly vectorised code often seen in machine learning, calculating <span class="math inline">\(\hat{\beta}\)</span> for the <code>prostate</code> data only requires one line of <code>R</code> code: </p>

<p>The implicit vectorization of the <code>R</code> code in the code listing , the fact that it already concretely operates in the vector space, operationalizes the concise diagrammaticism of equation  as a machine process.  More importantly, the vectorised multiplication, transposition and inversion of data creates the new vector <span class="math inline">\(\hat{\beta}\)</span> whose variations can be explored, observed, graphed and varied in ways that go well beyond the statistical tests of significance, variation, and error reported in Table .  (We will have occasion to return to these statistical estimates in chapter .) The play of values that starts to appear even in fitting one linear model will become much more significant when fitting hundreds or thousands of models, as some machine learners do.<a href="#fn32" class="footnoteRef" id="fnref32"><sup>32</sup></a> </p>
</div>
<div class="footnotes">
<hr />
<ol start="31">
<li id="fn31"><p>From the epistopic viewpoint, the most obvious result of fitting a linear model is the production not of a line on a diagram or in a graphic. As we have seen, such lines cannot be easily rendered visible. Instead, the model generates a new column-vector of coefficients (see Table ) and some new numbers, <em>statistics</em>. This table is not as extensive as the original data, the <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(Y\)</span> vectors. But the names of the variables in the dataset appear as rows in the new table, a table that describes something of how a line has been fitted by the linear model to the data. The columns of the table now bear abbreviated and much more statistical names such as <code>estimate</code> (the estimated values of <span class="math inline">\(\hat{\beta}\)</span>, the key parameters in any linear model), <code>Std. Error</code>, <code>t value</code>, and the all important <em>p</em> values written as <code>Pr(|t|)</code>. The numerical values ranging along the rows mostly range from -1 to 1, but the final column includes values that are incredibly small: <code>1.47e-06</code> is a few millionths. Other statistics range around the outside the table: the <code>F-statistic</code>, the <code>R-squared</code> statistic, and the <code>Residual standard error</code>. The numbers of the table  become epistopic here, since they now appear as a set of standard errors, estimates, t-statistics, and <em>p</em> values, that together indicate how likely the estimated values of <span class="math inline">\(\beta\)</span> are, and therefore how well the diagonal line expresses the relations between different dimensions of the dataset in the vector space. <a href="21-lines-traversing-behind-the-light.html#fnref31">↩</a></p></li>
<li id="fn32"><p>This is an important differentiation: it is not typical machine learning practice to construct one model, characterised by a single set of statistics (F scores, R^2 scores, <em>t</em> values, etc.). In practice, most machine learning techniques construct many models, and the efficacy of some predictive techniques derives often from the multiplication or indeed proliferation of models. Techniques such as neural networks, cross-validation, bagging, shrinkage and subset selection, and random forests, to name a few, generate many statistics, and navigating the multiple or highly variable models that result becomes a major concern. An epistopic abundance will appear here – bias, variance, precision, recall, training error, test error, expectation, Bayesian Information Criteria, etc. as well as graphisms such as ROC (Receiver-Operator-Characteristics) curves. Put simply, the proliferation of models start to drive the dimensional expansion of the vector space. At the same time, the multiplicity of models multiplied by the machine learners becomes the topic of statistical analysis.<a href="21-lines-traversing-behind-the-light.html#fnref32">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="20-implicit-vectorization-in-code-and-infrastructures.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="22-the-vectorised-table.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
