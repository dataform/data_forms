
## from proposal

The introduction will begin with several relatively familiar  examples drawn from a variety of fields over the last decade or so -- handwriting recognition, face recognition, autonomous robots [@thrun_stanley_2006], credit card checks, and cancer prognosis. It will highlight these examples as symptoms of the wide-ranging investments in knowledge, control, prediction and decision-making associated with data flows, and at the same time, suggest how these tracking some of the transformations might elicit changes in how humanities and social science researchers understand their own work. 

These examples  will also provide a preliminary overview of the techniques of machine learning discussed in the book -- supervised and unsupervised learning, the differences between classification, regression, and clustering and important notions such as learning and prediction. They will also highlight  constrasts between disciplines such as computer science and statistics that develop machine learning techniques, as well as illustrate the overlaps  between data-mining, pattern recognition, knowledge discovery, artificial intelligence, machine learning etc. Practically, these examples will also implicitly present some fo the methods used in the subsequent chapters, including the role of databases, data structures, code constructs, diagrams, and algorithms  in typical scientific and industry practices of modelling.

These examples will also stage some of  wider questions in the book  about the promise of data. These include the oft-mentioned 'end of theory' prediction (Chris Anderson, _Wired_ magazine, 2008), and the many claims and controversies about data analytics, machine learning and the 'power of big data' in physical, life and social sciences, in business, government and industry.  Claims  about  power of data, and responses to these claims  -- ranging from downright skepticism to enthusiastic embrace --  will be discussed here with an eye on what these debates about data  mean for research practices in the social sciences and humanities themselves in terms of their topics of research and how they do research.

Finally, the introduction will sketch the themes of 'in the data' and  'modes of machine thought,' drawing on a range of work drawn from pragmatist philosophers such as C.S. Peirce (abduction and diagrams), William James on experience [@james_essays_1996],  John Dewey on 'reconstruction' [@dewey_reconstruction_1957], Alfred N. Whitehead on 'abstraction' [@whitehead_modes_1958] and from recent social and cultural theory  such as Isabelle Stengers on experiment [@stengers_experimenting_2008]; Gilles Deleuze & Felix Guattari on scientific functions, and [@deleuze_what_1994]; Celia Lury on topology [@lury_introduction_2012]). In order to contextualise forms of data thought, the introduction will also sketch some points of departure drawn from software studies work on algorithms and databases, science studies work on calculation, statistics, number, device, image and diagram,  as well as accounts of subjectivity, experience [@berlant, 2007] or [@murphie, 2010] and materiality cross-cutting all of the above. This spectrum of work from across disciplines provide  scaffolding and departure points for much of the book. 


## overview

- 'into': different senses of that - to go into, to put into, a transition, going in, an interiority
- the triple dimensional space -- vector-space/function-finding/probability-prediction
- action-promise-prediction vs making/labour/work

## to do
- bring in materials from warwick talk
- say why I am focusing on machine learning and not data more generally -- databases, infrastructures, visualization, etc. - i.e. it is essential but opaque; it is the most slow-moving and tectonic aspect of what is happening with data; it has high levels of abstraction and generates many different potentials; it is a challenge for any re-thinking of what social sciences and critical humanities will do with data.
- recursiveness of this text
- the role of industry -- machine learning as industrial
- the difference between ML and IR, data-mining, - use IR book, use mining massive datasets; use db books to show this
- make the distinction between supervised and unsupervised, classifiers and predictions
- the literature on digital sociology; methods
- the form of writing; finding oneself experimenting with code and data
- trying to find experiments and experiences
-  the ml literature -- how the terms takes shape -- use ipython notebooks for that. machine learning as very loose assemblage; how it coalesces; etc. 
-  I can't cover all the algorithms, techniques, settings
-  FICO, DunneRaby -- huge amounts happening in these commercial spaces that I haven't investigate. Could be done through patent literature .... 
-  the development of machine learning out of AI -- shift from rule-based experts (cd Suchman; Collins  on this); visible in early textbooks (Tom Mitchell); decisive role of pattern recognition approaches focused on images (Bishop 1996 on this); Donald Michie for even earlier
-  the identificatino of ML and data-mining and predictive analytics
-  provenance of _iris_ dataset - -Fisher (1936) and Gaspe Peninsula, W.E. Anderson
- order of things -- big picture stuff on transdisciplinary episteme
- section on the digital humanities debate -- use Liu, and articles from _Differences_ 2014 to do this; + Hall; key points here in notes on Galloway's article. Contrast this with the attempts to conceptualise new forms of number and value in Latour, in Tarde, in STS, in Deleuze, etc.  -- I have PDFs of all these
- Arendt on statistics and praxis (nothing can happen the more predictive we become) vs Virno on the introjection of communicative praxis into poiesis and production vs ... SEE Notes files on both of these

## quotes to use
it follows that if one's object is an anthropological account of a problematization, then one's informants will differ from each other. The challenge lies in finding an experiential and experimental site that would provide a contemporary instance. 87 Rabinow

What I am attempting to do is to reflect on how it might be possible to transfigure elements of the equipment of modern method into a  form of modern meditation, and to bring the benefits and effects of that transformation to bear on enquiry. Rabinow 12 -  used this already in ch2 intro
source code
source code itself often offers inroads in alien phenomenology -- particular when carpentered to reveal the internal experience of withdrawn units, Bogost

the challenge, which I deem a materialist challenge, is that whatever the mess and perplexity that may result, we should resist the temptation to pick and choose among practices Stengers, wondering, 2011, 379

"The sheer density of the collisions of classification schemes in our lives calls for a new kind of science, a new set of metaphors, linking traditional social science and computer and information science. We need a topography of things such as the distribution of ambiguity.  ...  It will also use the best of object-oriented programming and other areas of computer science to describe this territory" [@bowker_sorting_1999, 31].


Quotes from Lanier



## examples to use
- [@le_building_2011] -- paper on the cat videos 
- the facebook paper
- the breast cancer paper


## Quotes from technical sources

_Definition_: A computer program is said to **learn** from experience $E$ with respect to some class of tasks $T$ and performance measure $P$, if its performance at tasks in $T$, improves with experience $E$ 2.  [@Mitchell_1997, 2]

In the past fifteen years, the growth in algorithmic modeling applications and methodology has been rapid. It has occurred largely outside statistics in a new community—often called machine learning—that is mostly young computer scientists (Section 7). The advances, particularly over the last five years, have been startling. [@breiman_statistical_2001, 200]

# Introduction

>_Definition_: A computer program is said to **learn** from experience $E$ with respect to some class of tasks $T$ and performance measure $P$, if its performance at tasks in $T$, improves with experience $E$ 2.  [@Mitchell_1997, 2]


A relatively new set of scientific-technical objects have taken shape in the last three decades. The scientific objects in question here are known by various names -- machine learning, pattern recognition, knowledge discovery, data mining -- and they are scattered across scientific disciplines, business and commercial applications, industry, engineering and government. They are found in breast cancer research, in autonomous vehicles, in insurance risk modelling, in credit transaction processing, in computer gaming, in face and handwriting recognition systems, in astronomy, ornithology, finance and surveillance. Sometimes these objects are understood as _scientific models_, and sometimes they are understood as _operational algorithms_. In very many scientific fields, publications mention or describe these objects as part of their analysis of some experimental or observational data (as in the logistic regression classification models found in a huge number of biomedical papers). They often become quasi-automatic devices, lying somewhere quite deeply embedded in other systems or device (as in the decision tree models used in some computer game consoles). The flexibility or plasticity of these objects, their proliferation and propagation in the world, and the epistemic-operational value accruing to them are the concerns of this book.

The names of these techniques can be found in many textbooks, instructional courses, website tutorials and descriptions: linear regression, logistic regression, neural networks, linear discriminant analysis, support vector machines, k-means clustering, decision trees, random forests, principal component analysis, or naive Bayes classifier. These names refer to predictive models and to computational algorithms of various ilk. They are shadowed by a wider panoply of modelling and data preparation techniques -- normalization, regularization, cross-validation, feature engineering, feature selection -- that augment their use in practice. The techniques, algorithms and models have arisen in the course of a century of work in mathematics, statistics, computer science as well as various scientific fields ranging from anthropology to zoology. Powerful mathematical abstractions such as linear algebra, vector space, differential calculus and probability theory run through them. But these formidable abstractions encounter today a novel situation in their direct applications to domains that lie far afield of the laboratory or engineering settings in which they first took shape.

I am focusing on machine learning techniques and not data practices or algorithms more generally -- databases, infrastructures, visualization, etc. - i.e. for several reasons. Enormous agency has been imputed to algorithms, often with very little discussion of their specific modes of existence. Databases and information infrastructures have been discussed widely in various settings, but often without reference to underlying computational abstractions. Moreover, the techniques of machine learning have received little critical attention, even though they have been pervasively threaded through infrastructures, devices and data practices. This threading  is rather opaque. It is the most slow-moving and tectonic aspect of what is currently happening with data, yet these techniques entail relatively high levels of abstraction. It would be possible to understand many changes in data-driven businesses, infrastructures and science in terms of the potentials implicit to these techniques. For instance, the seemingly obvious trend to collect and analyse more data is not simply a case of more is better. The performance of certain models and algorithms, as measured in terms of error rates, depends on the amount of data they have to work with. Attempts to optimise algorithmic performance, as we will see, generate demand for more data. More generally, the transformations associated with these techniques, I will suggest,  generates many different potentials, and lays down challenges for any re-thinking of what social sciences and critical humanities will do with data.

## The historical and social lives of abstractions

A cultural saturation of algorithms and an economically loaded awareness of their operation is taking shape in some quarters. [MIGHT NEED TO EXEMPLIFY SOME.] It is still very difficult, however, to move between a general awareness of the algorithmic saturation of science, business, commerce, government and media and the specificities of different settings in which they might operate. It is hard, moreover, to disentangle what belongs to the algorithms from what belongs to the domain in which they operate. The real problem here is that machine learning techniques, like many other algorithmic constructs, are often quite generic and can be mobilised in very different settings without essentially changing. The potency of algorithmic practice attests less to some algorithmic essence and more to the mobility afforded by their generic character. I see the problem of understanding the power of algorithms or models such as neural networks as primarily a matter of delineating how this generic character has taken shape.  What we lack is some way of moving between the abstract and generic character of the algorithms themselves and their mundane and farflung concrete implementations and applications. 

Several forms of movement might be possible here. The conventional one, at least for social science and humanities, is to identify the sources of their potency, currency, and particularly their apparent transcendence or universality. These sources vary according to different philosophical starting points. For twentieth century European philosophers such as Martin Heidegger or Hannah Arendt, decisive transformations in the underpinnings of Western thought occurred in the Scientific Revolution. The advent of algebra and experiment together generated shifts in ontology that profoundly configure modern science and technology. For instance, in _The Human Condition_, Hannah Arendt wrote:

> What is decisive is the entirely un-Platonic subjection of geometry to algebraic treatment, which discloses the modern idea of reducing terrestrial sense data and movements to mathematical symbols. Without this non-spatial symbolic language Newton would not have been able to unite astronomy and physics into a single science, or to put it another way, to formulate a law of gravitation where the same equation will cover the movements of heavenly bodies in the sky and motion of terrestrial bodies on earth. Even then it was clear that modern mathematics, in an already breathtaking development, had discovered the amazing human faculty to grasp in symbols those dimensions and concepts which at most had been thought of as negations and limitations of the mind, because their immensity seemed to transcend the minds of mere mortals, ... Yet even more significant than this possibility -- to reckon with entities which could not be "seen" by the eye of mind -- was the fact that the new mental instrument, in this respect even newer and more significant than all the scientific tools it helped to devise, opened the way for altogether novel mode of meeting and approaching nature in experiment [@Arendt_1998, 265]

Here Arendt frames most of modern science and technology as a combination of algebra and experimental practice engaging with the world ('terrestrial sense data and movement'). Similar formulations can be found in Edmund Husserl, Martin Heidegger, Maurice Merleau-Ponty, and Theodore Adorno; much social, cultural and political thought shares this treatment of the Scientific Revolution even if they are not explicitly engaged with problems of science and technology. While many not startlingly novel, many of Arendt's descriptions of how geometry was re-constituted algebraically, and how algebraic manipulations opened new ground for experimental encounters with natural phenomena, strongly resonate with machine learning techniques. In those techniques too, as we will, constant conversions between geometric, algebraic and statistical or experimental processes occur. Moreover,  Arendt's account of how algebraic reformulation of geometry barely mention calculus or statistics or indeed many other important mathematical developments, but the underlying argument that a kind of turning-inside-out of the 'structure of the human mind' also resonates with machine learning. There is an interesting explanation of all this: 

>With the rise of modernity, mathematics does not simply enlarge its content or reach out into the infinite to become applicable to the immensity of an infinite and infinitely growing, expanding universe, but ceases to be concerned with appearances at all. ... [it] becomes instead the science of the structure of the human mind [@Arendt_1998, 266]

Arendt suggests here that the ongoing development of increasingly counter-intuitive abstractions characteristic of modern mathematical thought can be understood as a turning inward from the world into the 'structure of the human  mind,' 'eye of mind' or 'mental instrument.' (This slightly odd formulation points to the phenomenology's influences on  Arendt.)  'Mathematics succeeded', she writes, 'in reducing and translating all that man is not into patterns which are identical with human, mental structures' (266). The assumption here, and it remains broadly speaking phenomenological in character, is that mathematical abstractions derive from distance, from wrenching away of experience from proximity and entwining with the world, and its replacement by the flattened product of distance: 'under this condition of remoteness, every assemblage of things is transformed into a mere multitude' (267). [TBA - linkage on the pattern recogntion] But this remote perspective derives,on the other hand from an almost introspective fixation on patterns derived from 'human, mental patterns.' Some might say that machine learning and pattern recognitions do precisely that today.

Something similar can be found in the very different mode of explanation of the potency of mathematical abstractions in sociological thought. Pierre Bourdieu, for instance, situates mathematical abstractions in the space between embodied skills and knowledges (the habitus of mathematicians) and symbolic systems possessing forms of closure:

>The experience of the transcendence of scientific objects, especially mathematical ones, that essentialist theories invoke is the particular form of _illusio_ which arises in the relationship between agents possessing the habitus socially required by the field and symbolic systems capable of imposing their demands on those who perceive them and operate them, and endowed with an autonomy closely linked to that of the field (which explains why the sense of transcendent necessity rises with the capital of accumulated resources and the qualifications demanded for entry) [@Bourdieu_2000, 113-114].

While experience of transcendence are not heavily emphasised in machine learning, much of Bourdieu's account of transcendence again resonates. Unlike more mundane techniques and technical configurations, these techniques involve some relatively formidable mathematical abstractions, and these abstractions, as we will discuss below, promise certain kinds of autonomy or quasi-autonomous action (for instance, they 'learn'). At the same time, the habitus -- the embodied skills in judging, perceiving, acting -- necessary to develop and apply these techniques is a matter of much discussion and debate (for instance in the many perseverant pronouncements about the need for people who can analyse the data), depends on people internalising 'symbolic systems' (linear algebra, probability and statistics, etc) that 'demand' the world be approached in some ways and not others (for instance, as a problem whose solutions can be found through processes of optimization). As in Arendt's account of the efficacy of mathematics in the world as an effect of distance, and of putting oneself at such a distance from things that mathematical functions can be seen operating everywhere, Bourdieu's account of mathematical transcendence -- its capacity to operate in many different times and places without changing -- depends on a relation between agents and symbolic systems
The problem of moving between the algorithms and their culturally saturated feedback loops matters because of the powerful grip these techniques have on contemporary data practice. While many of the intuitions underlying these techniques are relatively simple (for instance, the idea of drawing a line through a group of points as a way of describing the main tendency in a dataset), there are many differences in how these techniques work. 

>When an object – a geometrical space, for example – is scientifically constructed by functions, its philosophical concept, which is by no means given in the function, must still be discovered [@Deleuze_1994, 117].

In what ways do what computers do with data today change how we think about what we do? This slightly loopy question invites many possible responses. But I'm principally interested here in an increasingly widespread form of computation known variously as machine learning, pattern recognition, knowledge discovery, data mining or predictive analytics. Each of these terms comes from slightly different quarters, and they overlap with each other at many points. In general, however, all of them attest to the growth of a set of techniques concerned with working with data. These techniques are almost the sole focus of this book. The techniques assembled under the name machine learning are hardly new. Some of them date back more than a century, and the bulk of them are decades old. The breadth and scale of their application in recent years is somewhat astonishing. In the last few years, they have quickly become staple techniques in science, business, government, entertainment and media. 

