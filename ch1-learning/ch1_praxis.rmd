# A praxiography of machine learning

## Overview

- reconstruction vs problematization -- Rabinow
- data: its giveness, abstraction and actuality
    - the case of iris (science), digits (transactions), spam (media) and kittens -- in that order
    - the first function from Hastie has to go in
- practice and writing about practice - developing Mol
    - observant participation -- developing Wacquant; participating by observing
    - writing recursively -- developing Kelty w.r.t subjects
    - implementation as a practice -- the executable paper 
    - James on feeling of transition - and practicing radical empiricism - -see Massumi
- Implementing machine learning
    - the case of R -- the scientific
    - the case of Python -- the business/industry
    - the case of javascript -- popular culture
- computation -- psychic engagement - Wilson
- convergence and learning: for fitting a line; but then for choosing which features to use to learn
- learning about learning -- coursera, youtube, textbooks, the pile of books (across the gamut)
- things to include:
    - my R books, libraries, papers, competitions, blogs, talking, courses
    - the architecture of CRAN - DONE
    - knitr/ipython notebooks - executable - some examples from python notebooks
    - the notion of repl -- read-evaluate-print-loop
    - folding/plying
    - Add in Animation and Automation – The Liveliness and Labours of Bodies and Machines Body & Society March 2012 18: 1-46, 

## Quotes to use

    >The main point of this description is the concept of actuality as something that matters, by reason of its own self-enjoyment, which includes enjoyment of others and transitions towards the future. Whitehead, Modes, 161

    >But this enhancement of energy presupposes that the abstraction is preserved with its adequate relevance to the concrete sense of value-attainment from which it is derived. In this way, the effect of the abstraction stimulates the vividness and depth of the whole of experience. 169

    >Matter-of-fact is an abstraction, arrived at by confining thought to purely formal relations which then masquerade as the final reality. 25

    >A feeling in which the forms exemplified in the datum concern geometrical, straight, and flat loci will be called a 'strain.' In a strain, qualitative elements, other than the geometrical forms, express themselves as qualities implicated in those forms Whitehead,  PR, 310

    >Sometimes machines are the very means by which we can stay alive psychically, and they can  just as readily be a means for affective expansion and amplification as for affective attenuation. This is especially the case of computational machines. 30

## Introduction -- the data-driven mode

This chapter stands at the confluence of two broad tendencies shaping what is happening to data today. On the one hand, ways of working with scientific and other data have shifted substantially over the last decade or so due to the growth in open source programming languages and networked platforms. Many scientists use programming languages such as Python and R to do their work, and scientific research, which has long relied on counting and calculation, increasingly uses data more intensively. (Scientific computing languages such as FORTRAN have long underpinned scientific research in various more mathematical fields.) On the other hand, several decades of research and application of statistical and machine learning algorithms in science, government, industry and commerce have gradually developed fairly powerful ways of automating the construction of models that classify and predict events and associations between things, people, processes, etc. Programming and software development has long handled data and algorithms, and conversely, machine learning has always relied on programming. But they are coalescing in data-driven research and in various domains of data practice in ways that remain somewhat ambivalent. Can we locate anything in the proliferation machine learning that is more than plying data into a tightly woven matrix of commercial-scientific-governmental surveillance? 

I don't answer that question directly here, but instead explore some of the shifts in practices of working with data that can be observed around data. These practices are both the object of analysis, and in  certain cases, provide support for a shift in the way in which we might carry out research on changes in data practices. 


## Reading machine learning

The way that I approach data has been heavily shaped  by a single highly technical and compendious textbook, _Elements of Statistical Learning: Data Mining, Inference, and Prediction_ [@hastie_elements_2009]. Like the online machine learning courses and programming books I discuss below, _Elements of Statistical Learning_ is combines statistical techniques with various programming techniques to 'learn from data' [@hastie_elements_2009, 1]. The 18 chapters of the book range across various kinds of data (spam email, Californian forest fires, blood pressure measurements, handwritten digit recognition), and various machine learning techniques, methods and algorithms (linear regression, k-nearest neighbours, neural networks, support vector machines, the Google Page Rank algorithm, etc). While these techniques come from different places, in _Elements of Statistical Learning_, they are all framed by a notion of learning based on the predictive models. A 'learner' in machine learning is a model that predicts outcomes. 

While 'learning' is briefly discussed on the first page of _Elements of Statistical Learning_, the book hardly ever returns to the topic explicitly. The notion of learning in machine learning derives from the field of artificial intelligence. The broad project of artificial intelligence, at least as envisaged in its 1960s-1970s heydey, is today largely regarded as a failure. There is no general, well-rounded artificial intelligence in existence. But in the course of its failure, many interesting problems were generated. [TBA - references on the history of AI] The field of machine learning might be seen as one such offshoot. The so-called 'learning problem' and the theory of learning machines was developed largely by researchers in the 1960-1970s based on work already done in the 1950s on learning machines such as the perceptron, the  neural network model developed by the psychologist Frank Rosenblatt in the 1950s [@rosenblatt_perceptron:_1958]. Drawing on McCulloch-Pitts model of the neurone, Rosenblatt implemented the perceptron, which today would be called a single-layer neural network on a computer at the Cornell University Aeronautical Laboratory in 1957. As Vladimir Vapnik, a leading machine learning theorist, observes: 'the perceptron was constructed to solve pattern recognition problems; in the simplest case this is the problem of constructing a rule for separating data of two different categories using given examples' [@vapnik_nature_1999, 2]. While computer scientists in artificial intelligence of the time, such as Marvin Minsky and Seymour Papert, were sceptical about the capacity of the perceptron model to distinguish  or 'learn' different patterns [@minsky_perceptron:_1969], later work showed that perceptrons could 'learn universally.' For present purposes, the key point is not that neural networks have turned out several decades later to be extremely powerful algorithms in learning to distinguish patterns, and that intense research in neural networks has led to their ongoing development and increasing sophistication in many 'real world' applications (see for instance, for their use in sciences [@hinton_reducing_2006], or in commercial applications such as drug prediction[@dahl_deep_2012]). Rather, the important point is that it began to introduce learning machines as an ongoing project in which trying to understand what machines can learn, and to predict how they will classify or predict became central concerns. At the same time, and less visibly, the proliferation of implementations and applications of techniques derived from artificial intelligence and adjacent scientific disciplines gathered pace. 

In describing these techniques, I'm not attempting to provide any detailed history of their development. For the most part, I leave controversies about the techniques to one side. Also, in describing these developments, I focus mainly on what happens from the 1980s onwards. Rather than history or controversies, I focus on key methods, and the many configurational shifts associated with their implementations. While many of the machine learning techniques I discuss have much longer lineages (in some cases, runnning back to the 1930s), machine learning techniques begin to circulate much more widely in the 1980s as a result of personal computers, and then in the mid-1990s, the internet. The proliferation of programming languages such as FORTRAN, C, C++, Pascal, then Perl, Java, Python and R, and computational scripting environments such as Matlab, multiplied the paths along which  implementation of machine learning techniques could proceed. It would be impossible for anyone to read the vast machine learning research literature, or follow the the propagation of techniques across domains of science, business and goverment. For instance, a  perceptron that 'learns' the binary logical operation NAND (Not-AND) is  expressed in twenty lines of python code on the Wikipedia 'Perceptron' page [@perceptron_2013]. 

```{r perceptron, echo=TRUE, cache=TRUE, message=FALSE, warning=FALSE, comment=NA, size='smallsize', results='markup', engine='python' }
    
    threshold = 0.5
    learning_rate = 0.1
    weights = [0, 0, 0]
    training_set = [((1, 0, 0), 1), ((1, 0, 1), 1), ((1, 1, 0), 1), ((1, 1, 1), 0)]
     
    def dot_product(values):
        return sum(value * weight for value, weight in zip(values, weights))
     
    while True:
        print '-' * 60
        error_count = 0
        for input_vector, desired_output in training_set:
            print weights
            result = dot_product(input_vector) > threshold
            error = desired_output - result
            if error != 0:
                error_count += 1
                for index, value in enumerate(input_vector):
                    weights[index] += learning_rate * error * value
        if error_count == 0:
            break
 ```

While perceptrons and neural networks are the topic of a later chapter, the typical features of this code as the implementation of a machine learning algorithm are the presence of the 'learning rate', a 'training_set,' 'weights', an 'error count', a loop function that  multiplies values ('dot_product'). Some of the terms present in the code bear the marks of the theory of learning machines that we will discuss. But much of the code here is much more familiar, generic programming. It defines variables, sets up data structures (lists of numerical values), checks conditions, loops through statements or prints results. Executing this code (by pasting it into a python terminal) produces several dozen lines of numbers that are initially different to each other, but that gradually converge on the same values (see printout). These numbers are the 'weights' of the nodes of the perceptron as it iteratively learns to recognise patterns in the input values. None of the workings of the perceptron need concern us at the moment. It is a typical machine learning algorithm, almost always included in ML textbooks and usually taught in introductory ML classes.  Perhaps more strikingly than its persistence as an algorith over half a century, the implementation of the whole algorithm in twenty lines of code on a Wikipedia page suggests the mobility of these methods. What required the resources of a major research engineering laboratory in the 1950s can be re-implemented by a cut and paste operation between wikipedia pages and a terminal window on a laptop in 2013. This is now a familiar observation, and perhaps not very striking at all. But the question of who implements perceptrons or neural networks, and where they implement them today is rather more interesting. 


## Notion of praxiography: where practices are the feature

What would we learn by studying the proliferation of implementations of artificial intelligence or machine learning algorithms rather than their history or the controversies associated with them? The question that guides much of this book is how to live with machine learning? This is a largely affirmative question, rather than a critical one. I am looking for ways of thinking with machine learning. Several general  possibilities present themselves here. 

Science studies scholars such as Anne-Marie Mol and John Law have long urged the need to keep practice together with ontology. Towards the beginning of _The Body Multiple: Ontology in Medical Practice_[@mol_body_2003], Mol writes:

>If it is not removed from the practices that sustain it, reality is multiple. This may be read as a description that beautifully fits the facts. But attending to the multiplicity of reality is also an act. It is something that may be done – or left undone [@mol_body_2003, 6]

Mol's work offers a cogent case for developing accounts of what is real steeped in the practices that make it real. Similar affirmations of the sustaining role of practice can be found in many parts of social sciences and humanities. So for the first part, looking at implementations and the flow of implementations is a way of keeping practices in the picture, and therefore, a heuristic for learning reality as multiple. This already suggests that describing machine learning in terms of practices could be an act that attends to their multiplicity. Mol's notion of *praxiography*, a variant on ethnography, as an act of describing practice in the name of preserving their multiple-making value, has particular resonance for work with data, which is itself always heavily entwined in writing and reading practices. Could we praxiographically go into data?

A second  and related tack comes from the work of Alfred North Whitehead. Whitehead's work on how abstractions are embodied is highly relevant to thinking about machine learning. While Whitehead is comprehensively critical of certain tendencies in modern science (the fallacy of misplaced concreteness; the reduction of space-time to discrete locations, etc), he is very enthusiastic about the potential for better abstractions. While voiced in terminology that takes some getting used to, the broad affirmative point can be grasped:

> But this enhancement of energy presupposes that the abstraction is preserved with its adequate relevance to the concrete sense of value-attainment from which it is derived. In this way, the effect of the abstraction stimulates the vividness and depth of the whole of experience. [@whitehead_modes_1958,169]

The enhancement of energy Whitehead talks about here is concerned with the 'fortunate use of abstractions' (169). machine learning and  much work with data today can be seen as a form of abstraction (as can much of science). The crucial question for Whitehead, however, is how to abstract well. Like Mol, he advocates keeping abstractions together ('adequate relevance') with something like practice ('the concrete sense of value-attainment from which it is derived'). Like Mol too, the virtue of this bundling together of abstractions with their concrete sense affects the 'whole of the experience.' Abstractions can make experience more vivid or deep if they are aligned and assembled connectively. Here I pin some methodological hope on Whitehead's work as a way to experiment with abstraction as stimulant and enrichment, not as reduction as pale imitation.

A final and perhaps much higher level connection here comes from the Elizabeth Wilson's study, _Affect and Artificial Intelligence_ [@wilson_affect_2010]. Drawing on a combination of psychoanalytic, psychological and archival materials, Wilson discusses the work of key figures in the early history of artificial intelligence such as Alan Turing, Warren McCulloch and Walter Pitts on neural nets, and recent examples of affective computing and robots such as Kismet. The framing of her project is interesting:
    
    > Sometimes machines are the very means by which we can stay alive psychically, and they can  just as readily be a means for affective expansion and amplification as for affective attenuation. This is especially the case of computational machines. 30 

Under what conditions do machines and for present purposes, computational machines, become 'the very means we can stay alive psychically'? She  addresses this question by positing 'some kind of intrinsic affinity, some kind of intuitive alliance between the machinic and the affective, between calculation and feeling' (31), and suggesting that the 'one of the most important challenges will be to operationalize affectivity in ways that facilitate pathways of introjection between humans and machines' (31). Introjection, the process of bringing the world within self is, according to psychoanalytic accounts of subjectivity, crucial to the formation of 'a stable subject position' (25). Wilson envisages introjection of machine processes as a good, not as a failure or attenuation of relation to the world. 

These three takes on how to think about practices, abstraction and machines work on somewhat different levels, but they broadly share an interest or even a commitment to multiple, plural or surprising conjunctions between what people do, what the world is, and the many adjustments, alignments and slippages that connect them through data. It might be possible to treat them as offering guidance or methodological hints on how to work with data, with models and with machine learning somewhat differently. The question is how would one practically explore such insights.

## Mobility of statistical methods in the 1990s: the case of R

- The number of packages
- The history of the language
- The increase in popularity

Nearly all of the examples in _The Elements of Statistical Learning_ are implemented in a single programming language, R. R is a well-known and widely used statistical programming language and environment  [@r_development_core_team_r_2010]. An open source programming language, according to surveys of business and scientific users, R is replacing popular software packages such as SPSS, SAS and Stata as the statistical and data analysis tool of choice for many people in business, government, and sciences ranging from political science to genomics, from quantitative finance to climatology [@rexer_analytics_rexer_2010]. Developed in New Zealand in the mid-1990s, and like many open source software projects, emulating S, a commercialised predecessor developed at AT&T Bell Labs during the 1980s, R is now extremely widely used across life and physical sciences, as well as quantitative social sciences. Many undergraduate and graduate students learn R as a basic tool for statistics. Skills in R are often seen as essential pre-requisite for scientific researchers, especially in the life sciences. Estimates of its number of users range between 250000 and 2 million. Increasingly, R is integrated into commercial services and products (for instance, SAS, a widely used business data analysis system now has an R interface; Norman Nie, one of the original developers of the SPSS package heavily used in social sciences, now leads a business, Revolution, devoted to commercialising R; R is heavily used at Google, at FaceBook, and by quantitative traders in hedge funds, etc.).  R is an interestingly diffuse entity. Some ways of working with data are way more clearly focused on equations and calculation. For instance, in order to pursue number practices in physical sciences and engineering, maybe MATLAB or Mathematica would be better. In business or government, many ways of working with data are more focused on ordering and searching. For instance, in order to the look at the organisation of large aggregates of data, relational databases, query languages, data-centre architectures, and perhaps the techniques of aggregating and disaggregating data en-masse would be worth studying. 

Why then choose R, a statistical programming language, a language developed largely by academic statisticians and research scientists rather than computer scientists, software engineers or hackers? The primary ground is that the development, demonstration, proliferation and adoption of machine learning has occurred in R. The R packages such as 'ElemStatLearn' [@halvorsen_elemstatlearn:_2012], for instance, encapsulates some of the main datasets and methods discussed in _The Elements of Statistical Learning_.  Research articles and texbooks in statistics commonly both use R to demonstrate methods and techniques, and create R packages to distribute the techniques and sample data. For instance, an article published in 2007 by the Princeton computer scientist David Blei on 'correlated topic models' for classification of documents [@blei_correlated_2007] leads to a package 'topicmodels' available from the 'Comprehensive R Archive Network (CRAN)' a few years later [@cran_comprehensive_2010]. CRAN itself is an important infrastructure in the life of R. 

```{r mirrors, echo=TRUE, cache=TRUE, message=FALSE, warning=FALSE, comment=NA, size='smallsize', results='markup' } 
    library(utils)
    mirrors = getCRANmirrors(all = FALSE, local.only = FALSE)
    head(mirrors)

```
A global set of `r nrow(mirrors)` mirror sites in `r length(unique(mirrors$Country))` countries distributes the latest R platform and R packages to many different users. These mirror sites are largely run by universities and public institutions, alongside a few commercial users (such as Revolution, a company I discuss in a later chapter). So, in terms of machine learning practice, R is important because it is a staple programming language in statistics research and teaching. It is practically circulated in ways that make it globally available. 

R has also attracted  mainstream media attention  -- an article in the _New York Times_ in 2009 described its importance [@vance_data_2009], but at that time the _New York Times_ was heavily promoting its idea of data journalism.  More broadly, R is an evocative object, to use the psychoanalyst Christopher Bollas' term [@bollas_evocative_2009], an object through which run many different ways of thinking. Standing somewhere at the intersection of statistics and computing, modelling and programming, many different disciplines, techniques, domains and actors intersect in R. Bollas suggests that 'our encounter, engagement with, and sometimes our employment of, actual things is a _way_ of thinking' [@bollas_evocative_2009, 92].  R embodies something of plurality of practices of working with measurements, numbers, text, images, models and equations, with techniques for sampling and sorting, with probability distributions and random numbers. It engages immediately, practically and widely with  words, numbers, images, symbols, signals, sensors, forms, instruments and above all abstract forms such as mathematical functions like probability distributions and many different architectural forms (vectors, matrices, arrays, etc), as it employs data. By virtue of thousands of packages that flow across boundaries between nature and culture, between aesthetic, epistemic and pragmatic domain, R embodies a wide-with data economies, cultures, sciences, politics and technologies. Somewhere between calculation and searching, R channels counting and sorting, in the estimation of likelihoods.

How would we describe the R-based practices in ways that deepen our perception of their plurality, their relevance, and potential amplification of affect? Note that Bollas distinguishes encounter, engagement and employment of things. Employment, in his terms, is less likely to lead become a way of thinking than encounter and engagement. On balance, I'd have to admit that the employment dimension of R dominates.  It is easy today to find an employment-centric view of data practice. The 'chief economist' at Google, Hal Varian in 2009 made a widely quoted prediction about data:

> The ability to take data — to be able to understand it, to process it, to extract value from it, to visualize it, to communicate it — that’s going to be a hugely important skill in the next decades, not only at the professional level but even at the educational level for elementary school kids, for high school kids, for college kids. Because now we really do have essentially free and ubiquitous data. So the complimentary [sic] scarce factor is the ability to understand that data and extract value from it.” Hal Varian, chief economist at Google: [@mckinsey_&_company_hal_2009]

While cited here in a report prepared by the global business consultancy, McKinsey & Co, I have seen this quote or selections from it in many different contexts, ranging from government reports on higher education to the student noticeboard in the statistics department at the university. What Varian presents as an 'important skill'  also has a 'scarce factor':  the 'ability to understand that data.' While Varian presents understanding data as the prelude to 'extract[ing] value from it,' understanding might take different forms, depending on the kind of encounter or engagement that precedes it. [TBA: the singaporean government injunction]

Encounters with R are by no means limited to employment. A somewhat broader framing of the value of data practice can be found associated with R. Some people – admittedly a small group – explicitly promote R in association with the wider growth of data democracy or open data. I say somewhat, because the promotion of R as a valuable software object is linked to individual entrepreneurship. Norman Nie, the original developer of the widely-used SPSS social science statistics software (see [uprichard_spss_2008]), is a proponent of R. Here is Norman Nie interviewed in _Forbes_, a leading business news magazine, evangelising for R:

> Everyone can, with open-source R, afford to know exactly the value of their house, their automobile, their spouse and their children--negative and positive  … It's a great power equalizer, a Magna Carta for the devolution of analytic rights [@hardy_power_2010].

While strictly speaking referring to R for employment purposes, Nie, the founder of Revolution Analytics, a company that provides software and support services to R users, promotes R as a way of protect individual liberties to know what they own. In the context of the global financial crisis of 2007, it may be that the value of houses became much more problematic, and understandably, harder to know. And the reference to 'spouse and their children' must be flippant. Yet the 'devolution of analytic rights' that Nie speaks here, even as he frames it in terms of entrepreneurial individualism, is important. While Nie's company  Revolution Analytics promotes the incorporation of  R into  powerful and pervasive business and government prediction systems (such as SAS and IBM's Pure Data [@ibm_ibm_2013]), Nie here points to the problem of 'manipulation and control' of customers associated with just that incorporation. The 'analytic rights' he advocates in the form of R are meant to help individuals counterbalance the power of large scale data analysis conducted by corporations and governments. 

Nie's advocacy of R points towards a surfeit or overflowing sense of possibility associated with R. It would be possible to cite many other instances of this belief and desire in the potential of R. A good indication of this overflow is the aggregate blog, [R-bloggers](R-bloggers.com). R-bloggers brings together several hundred R-related blogs in one place. The range of topics discussed there on any one day – Merrill-Lynch Bond Return indices, how to run R on an iPhone, using US Department of Agriculture commodity prices, analysing human genetic variation using PLINK/SEQ via R, using the 'Rhipe' (sic!) package to program big data applications on Map-Reduce cloud architectures, doing meta-analysis of phylogenetic trees, etc., etc (R-bloggers 2011) – suggest how widely desires/beliefs in R range. Many of the applications, experiments, or techniques reported there can be reduced to employment or to individual uses of R. They might, as we will see, suggest the potential for encounters and engagements. 

## Vectorial encounters with  data

If we were to try to describe these encounters and engagements in which R as an actual thing is also a way of thinking, how would we go about it? The act of describing the practices of working with R might, at least this is my hope, be a way of staging an encounter with those aspects of data practice that exceed employment, and that instead do something like vivify experience. But one problem is that there are so many different ways to do things with data.  In many cases, data practice begins not from data, but from books, articles and websites. This a fairly banal observation, but acquiring, generating, finding or, to use the term often used by database architects, 'discovering' data is not all that common in practice. It is perhaps far more typical to encounter data in the context of learning how to manipulate, sort, model or display it. These settings are multiple. We could turn to the programmer Q&A site [Stackoverflow](http://www.stackoverflow.com), which has many questions tagged with R. There are also many online manuals, guides and tutorials relating to R [@wikibooks_r_2013]. For present purposes, I draw mainly on semi-popular books such as _R in a Nutshell_ [@adler_r_2010], _The Art of R Programming_ [@matloff_art_2011], or _R Cookbook_ [@teetor_r_2011]. These books are not written for academic audiences, although academics often write them. They are largely made up of illustrations and examples of how to do different things with different types of data. Given a certain predisposition, these books and other learning materials make for enjoyable reading. It is sometimes pleasing to see  the economical way in which they solve common data problems. This genre of writing about programming specialises in posing a problem and solving it directly. In these settings, learning takes place largely through following or emulating what someone else has done with either a well known dataset (Fisher's 'iris') or a toy dataset generated for demonstration purposes.  The many frictions, blockages and compromises that often affect data practice are largely deleted here in the interests of demonstrating the application of specific techniques.

In order to demonstrate both the costs and benefits of approaching R through such materials, rather than through ethnographic observation of people using R, it will be necessary to stage encounters here with data that has not been completely transformed or cleaned in the interests of demonstrating the power of a technique. While there are many different ways to encounter data, let us imagine it happens by chance, as when walking down the street we notice something about a building for the first time. Some feature, old or new, attracts our attention. Something similar happens in discovering data. For instance,  there are many potentially interesting, relevant or important datasets to be found, for instance, in the [World Bank Data catalog](http://data.worldbank.org/developers/data-catalog-api). The World Bank publishes much data on global economic activity. Much of this data can be retrieved from its Application Programmer Interface (API) using R code. It could also be downloaded using a web browser, and then imported into a spreadsheet, but in this case, since I'm interested in how a programming language affects encounters with data, the R code is more relevant. 

```{r worldbank, echo=TRUE, cache=TRUE, message=FALSE, warning=FALSE, comment=NA, size='smallsize', results='markup' } 

    library(RCurl)
    library(rjson)
    url = 'http://api.worldbank.org/countries?incomeLevel=OEC&format=json'
    high_income_countries = fromJSON(getURL(url))
    countries = unlist(sapply(high_income_countries[[2]], '[', 'name'), use.names=FALSE)

```

It still surprises me to see how tersely data can be gathered in code. The five lines R code shown here request a list of high-income countries (members of the OECD) from the WorldBank databases. The World Bank database returns a nested list describing `r length(countries)` countries. The list is hardly surprising (`r paste(countries[1:7],collapse=', ')` ... ). But the point of this code is to show something of the economy of movement and reshaping of data associated with R.  In this code, the first line loads an R library (the 'RCurl' package) to make requests to internet servers. The second line loads an R library  (the 'rjson' package) that can process data in the JSON (Javascript Standard Object Notation) format, a data format commonly used by servers to transfer data. Lines 3-4 provide the internet address of the World Bank API, request the data, receive the response, and convert the JSON data into a typical R data structure, a list. The final line selects from the list, which has many sub-lists nested within it, just the names of the countries.  Although this trivial code vignette does not do any of the understanding or extracting of value that Hal Varian propounds, it shows something about the state of data practice. Brevity in code suggests very well travelled paths of practice. Like the how-to book with their often just-so demonstrations of problem solving, the compactness or briefness code suggests that many obstacles, detours and blockages have already been encountered by others and dealt with by them. This is not to deny any agency on my part.  I did some 'processing'  here in, for instance, choosing a path into the World Bank datasets via the selection of the query terms (`r cat(url)`). It also includes the re-formatting operations that first transform JSON data into an R list, and then traverse the nested list structure, selecting only the items of data labelled 'name.' These kinds of selection operations, and transformations in the format of data are vital in contemporary data. Whether in the form of well-established relational database query language SQL, in the requests made to one of the legion of APIs that render data on various internet platforms such as Youtube, WorldBank or Pubmed, modifications in the shapes and sorts of data are a constant concern. These include transformations in types of data. For instance, numerical data might binned into categories such as high, medium, low or some other form of ranking; text might be split into keywords; the presence or absence of some value might be counted; etc. 

```{r world_bank_flat, echo=TRUE, cache=TRUE, message=FALSE, warning=FALSE, comment=NA, size='smallsize', results='markup' }

    countries_table = do.call('rbind', high_income_countries[[2]])
    df=do.call('rbind', apply(countries_table, MARGIN=2, 
        FUN =unlist, use.names=FALSE))
    countries_table_complete = t(df)
    head(df)
 ```

Some of these transformations matter more than others. The further lines of code produce a table from the WorldBank data. Tables, arrays and matrixes -- the rectangular form of data that typically puts one example per row, with different columns representing different measurements, attributes or properties of the examples. In R, such matrixes and dataframes are heavily used for reasons that go deep in machine learning algorithms and data practices more generally. These kinds of table-making operations are often highlighted in books, documentations and tutorials on R since they deeply affected how code runs. In this case,   the code vignette turns the list, with its sublists, into a flat table or matrix. This is a kind of topological transformation since the nested list of countries with their economic indicators has a different spatio-data structure than the table with its `r ncol(countries_table_complete)` columns.  

The code for these transformations is ugly and difficult to read, and perhaps could be written more cleanly. Nevertheless, at three points  -- the two `do.call` statements, and the `apply` function -- it demonstrates a distinctive aspect of R, and other programming languages designed for data practice (Octave, Matlab, Python's NumPy, or C++ Armadillo): vectorised transformations of data.

```{r vectorisation, echo=TRUE, cache=TRUE, message=FALSE, warning=FALSE, comment=NA, size='smallsize', results='markup' } 
 
     vector1 <- c(0,1,2,3,4,5,6,8,9,10)
     vector2 <- c(0,1,2,3,4,5,6,8,9,10)

     # looped addition
     result_looped = vector()
     for (i in vector1) {
        result_looped[i] = vector1[i] + vector2[i]
     }
     result_looped

     # vectorised addition
     result_vectorised  <- vector1 + vector2
     result_vectorised

```
In mainstream programming languages, transformations of data are often done using loops and array constructs in which some operation is successively repeated on each element of a data structure. In vectorised languages such as R, transformations of a data structure usually expressed in one line of code that seems to simultaneously work on all the elements of the code. As the widely used _R Cookbook_ puts it, 'many functions [in R] operate on entire vectors, too, and return a vector result' [@teetor_r_2011, 38]. Or as _The Art of R Programming: A Tour of Statistical Software Design_ by Normal Matloff puts it, 'the fundamental data type in R is the _vector_' [@matloff_art_2011, 24], and indeed in R, all data is vector. There are no individual data types,  only varieties of vectors in R. Again, as _R in a Nutshell_  puts the point directly: 'in R, any number that you enter ... is interpreted as a vector' or as 'an ordered collection of numbers' [@adler_r_2010, 17]. The practical difference is illustrated in the code vignette above in which two 'vectors' of numbers are added together, in the first case using a classic `for`-loop construct, and in the second case using an implicitly vectorised arithmetic operation `+`. The difference in speed between adding $1 ... 10$ using a loop or vector arithmetic is completely trivial here, but when millions of numbers are handled arithmetically, these implementation differences significantly affect the work of both programmers and computing machinery.  This simultaneity is only apparent, since somehow the underlying code has to deal with all the individual elements, but vectorised programming languages take advantage of hardward optimisations or carefully-crafted low-level libraries. 

While the examples of vectorised computation that I have shown are relatively trivial -- turning a nested list into a table, multiplying sequences of numbers -- these vectorised operations are carried out on a largely scale in many domains of contemporary computation. They are, for instance, especially important in computer graphics, where constant transformations of matrices of numbers support animation. They are extensively used in statistical modelling and machine learning, where 'fitting a model' to data is often literally implemented through the multiplication of matrices containing data in order to calculate parameters of a model. So when Brian Massumi writes that 'the space of experience is really, literally, physically a topological hyperspace of transformation' [@massumi_parables_2002, 184], he may have been describing the heavily vectorised operations on which much machine learning, statistical modelling and prediction depend. 

## Reshaping of data and the problem of spatiality

R and other numerical computing environments are not only reliant on vectorised operations. The dimensionality of data practice is something that we hardly think of when we think of models, algorithms, predictions and smart devices. But in terms of multiplying matrices, dimensionality both constrains and enables many aspects of the prediction.[^1]. As a programming language, R is striking for its `-ply` constructs. There are several in the core language and many to be found in packages (especially the popular 'plyr' package).  These include `apply`, `sapply`, `tapply`, `lapply`, `mapply`. All of the `-ply` constructs have a common feature: they take some collection of things (it may be ordered in many different ways – as a list, as a table, as an array, etc), do something to it, and return something else. This hardly sounds startling. But while most programming languages in common use offer constructs to help deal with collections of things sequentially (for instance, by accessing each element of data set in turn and doing something), R offers ways of dealing with  them all at once. The `-ply` constructs ultimately derive from the functional logic developed by the mathematician Alonzo Church in the 1930s [@church_note_1936; @church_introduction_1996], R presents difficulties for programmers used to so-called procedural programming languages. The functional programming style of applying functions to functions seems strangely abstract. Once you get a feel for them, these -ply constructs they are very convenient to write, and the code runs much faster. Like the vectorised operations described above, they implicitly parallelise operations on data in ways that adapts to the increasingly parallel contemporary chip architectures. The `-ply` operations reduce both data and computational frictions. The real stake in -plying data, however, is not speed but transformation. -Plying makes working with data less like iteration (number, text, table, list, etc), and more like folding a pliable material. Such shifts in feeling for data are very mundane yet crucial to the flow of data.  Plying, in the sense of plying trade, seems much closer to the kinds of work done on data than the figures of data flow or data deluge. 

People reshape data for different ends. 

> Almost any programming language you use will have great linear algebra libraries. And they will be high optimised to do that matrix-matrix multiplication very efficiently including taking advantage of any parallelism your computer. So that you can very efficiently make lots of predictions of lots of hypotheses [Ng, lecture 10.50]


```{r matrix, echo=TRUE, cache=TRUE, message=FALSE, warning=FALSE, comment=NA, size='smallsize', results='markup' } 
    
    a_vector = c(1,2,3)
    a_matrix = matrix(c(1,2,3,4,5,6,7,8,9), nrow=3)
    matrix_product = a_matrix %*% a_vector
    
    cat('Matrix:', a_matrix, '\n')
    cat('Vector:', a_vector, '\n')
    cat('Matrix times vector:', matrix_product, '\n')


[HERE] -- write about matrix manipulations, their architecture and why they are essential to prediction, and house prices ... 

- multiplying
- commutative
- associative
- identity matrix
- inverse - singular/degenerate
- transpose


## Datasets as evocative objects

Let's return to the example of house prices mentioned by Norman Nie. This is possibly the worst example for my case, since house prices seem so webbed into employment, salaries, class position, etc. It will be interesting to see if it is possible to write about predictive modelling of house prices in a way that responds to what Mol, Whitehead and Wilson propose. The house price dataset I use comes from around San Francisco, prior to the global financial crisis, but I will say more about how I encountered this dataset below. The dataset is fairly small and simple. Only the first few lines are shown below. 

```{r house_price, echo=TRUE, cache=TRUE, message=FALSE, warning=FALSE, comment=NA, size='smallsize', results='markup' }

    house_prices = read.csv('data/ex1data2.txt', header=FALSE)
    names(house_prices) = c('size', 'bedrooms', 'price')
    head(house_prices)
    summary(house_prices)
 ```
Two architectures come together here. On the one hand, around fifty houses are described in terms of the number of bedrooms, their total floor area and the price they sold for. This is a very sparse description of architecture, and it reduces the encounter with the psychically rich form of houses to a cognitive minimum. On the other hand, this description has its own architecture -- the  very familiar form of the table, grid or matrix. Whatever else happens to the data in machine learning, the matrix form pervades much data practice. As we will see below, in modelling this data, certain parts of the matrix will be separated out and treated differently. For the moment, only the bare architecture of the matrix frame concerns us.

How do we encounter such things? Bollas describes some of the ways in which we encounter things in the world as a mixture of determining and selecting processes: 

> the ability to move freely in the object world, to use its thing-ness as a matrix for thinking-by-action, pivots around whether the aleatory object determines - whether we move on quickly due to the dynamic of our last encounter with the object - or whether we select objects because we are unconsciously grazing: finding food for thought that only retrospectively could be seen to have a logic. [@bollas_evocative_2009, 93]

Although he refers to 'thing-ness as matrix', Bollas was probably not thinking about houses presented as a matrix of numbers. However, the house price dataset is an aleatory object for me in the sense that I encountered it by chance, and the question is, can it be used praxiographically, or for what Bollas calls, 'thinking-by-action'? 


## Learning to predict online

I'm not choosing the house price example at random. It is a surprisingly common teaching or demonstration example in machine learning, alongside other iconic datasets including R.A. Fisher's 'iris', the MNIST handwritten digits dataset [@lecun_mnist_2012]  (derived from U.S. National Institute of Standards and Technology (NIST)), or the cat photo dataset discussed in the previous chapter. In the course of writing this book, as well as reading the academic textbooks, popular how-to books, software manuals and blog-how-to posts,  I attended graduate courses in Bayesian statistics, genomic data analysis, data mining, and missing data. Significantly, I also participated in the online machine learning courses and some machine learning competitions [^4]. Typicallym, a house price dataset might be used for teaching purposes, as in Professor Andrew Ng's course 'Machine Learning' CS229 at Stanford (http://cs229.stanford.edu/) and also delivered on [Coursera.org](https://class.coursera.org/ml-003/class/index). These two  courses, and especially CS229, are fairly typical recent expositions of machine learning. In these lectures, a dataset derived from house prices in San Francisco. Given the course was running before 2007, it is likely that this dataset does not reflect the financial crisis that began in 2007 and led to drastic reductions in house prices from 2008 onwards. After sitting through several dozen hours of Ng's online lectures, and attempting  some of the review questions and programming exercises, including implementing well-known algorithms using R, I have some sense of how predictive models are constructed, and what kinds of algorithmic architectures and forms of data are preferred in machine learning. Importantly, these courses have been viewed by many other people. Ng's machine learning course on Coursera (of which he is co-founder, along with Daphne Koller, another machine learning scientist from Stanford), has a typical enrolment of 100,000. Youtube videos of his Stanford University lectures have viewing figures of around 500,000. My participation in these courses was a kind of arduous experiment. Like the hundreds of thousands of other students, I was trying to learn about machine learning. At the same time, however, I was trying to keep open a set of potentials that much of the course was trying to minimise or shutdown, even as instructor's such as Ng from Stanford or Pedro Domingo from the University of Washington sought to convey their excitement about the content of their courses. In both the Coursera  and Stanford courses, Ng uses the house price dataset to demonstrate certain techniques of predictive modelling. 

How would someone with a somewhat transdisciplinary background (undergraduate degrees in science and philosophy; postgraduate in philosophy) learn about machine learning online? I first encountered machine learning in reading recent scientific articles on genomics. Scholars interested in genomics have often discussed the growth of DNA sequence data in contemporary life sciences. They have carefully studied sequence and other biological databases, but less often attended to the ways in which data from those databases has been modelled or used predictively [^2]. Understanding predictive modelling in genomic science is difficult, since both the biology and the statistics are technically challenging. The possibility of reconstructing or implementing genomic models are more limited. Although I have attended genomic data analysis courses, even in these classes, where my classmates where either postgraduate research students or scientists, only a narrow selection of techniques for working with genomic data are covered. The online machine courses are, by contrast, quite generic and the examples are broad-ranging.  

Both the textbooks and the online university courses on machine learning have a similar structure. They nearly always begin with linear regression models (fitting a line to the data), then move to logistic regression (a way of using line fitting to classify outcomes), and afterwards move to some selection of neural networks, decision trees, support vector machine and clustering algorithms. They add in some decision theory, techniques of optimization, and ways of selecting predictive models (especially the bias-variance tradeoff). Whether in the textbook or the online course, examples such as the house price dataset form part of the learning of machine learning. They differ, however, in one crucial regard. Reading the _Elements of Statistical Learning_ textbook or one of machine learning books written for programmers (for example, _Programming Collective Intelligence_ or _Machine Learning for Hackers_ [@segaran_programming_2007; conway_machine_2012]) does directly imbricate the reader in a machine learning process. By contrast, doing a Coursera course on machine learning brings with it an ineluctable sense of being machine-learned. The learners on Coursera as the target of machine learning. Daphne Koller and Andrew Ng are leading researchers in the field of machine learning. They are also co-founded  the online learning site [Coursera](http://coursera.org).  As experts in machine learning, it is hard to imagine how they would not treat courses as learning problems. And indeed, Daphne Koller sees things this way:

    There are some tremendous opportunities to be had from this kind of framework. The first is that it has the potential of giving us a completely unprecedented look into understanding human learning. Because the data that we can collect here is unique. You can collect every click, every homework submission, every forum post from tens of thousands of students. So you can turn the study of human learning from the hypothesis-driven mode to the data-driven mode, a transformation that, for example, has revolutionized biology. You can use these data to understand fundamental questions like, what are good learning strategies that are effective versus ones that are not? And in the context of particular courses, you can ask questions like, what are some of the misconceptions that are more common and how do we help students fix them? [@koller_daphne_2012]

Whether the turn from 'hypothesis-driven mode to the data-driven mode' has 'revolutionized biology' is debatable (I return to this in a later chapter). And whether or not the data generated by my participation in Coursera's courses on machine learning generates data supports understanding of fundamental questions, that seems also improbable. Nevertheless, the loopiness of this description interests and appeals to me. I learn about machine learning, a way for computer models to optimise their predictions on the basis of 'experience'/data, but at the same time, my learning is learned by machine learners. This is not something that could happen very easily with a printed text, although versions of it happen all the time as teachers work with students on reading texts. 

## Writing code about code: scientific software and executable papers

I've been writing code for decades, most sporadically. But writing code was always something that lay at a distance from my academic work.  Only recently, owing mainly to developments in ways of analysing and publishing scientific data, have I found ways to write about code while coding. This looping between writing code and writing about code is the main way that this book has been written. Not all of the code I've written in implementing machine learning models or in reconstructing certain data practices has been included in this text, just as not all of the prose text I've written in trying to construct arguments or think about data practices has been included. Much has been cut away and left on the ground. At many points in researching the book, I digressed a long way into quite technical domains of statistical inference, probability theory, linear algebra, dynamic models as well as database design and data standards. Much of this exploration also remains invisible, since this is neither a textbook or a how-to cookbook. Nevertheless, the several years I have spent  writing about data practice has felt substantially different to any other project by virtue of a strongly recursive invocation of code in text, and text in code, made possible by working on code and text within the the same file, in the same text editor.  Switching between writing  R and  Python code to retrieve data, to transform it, to produce graphics, to construct models or some kind of graphic image, and within the same file be writing academic prose, is one way to write more praxiographically.

The mixture of text, code and images depends on an ensemble of software tools that differ somewhat from the usual collection of word processor, bibliographic software, image editor and web browser. In particular, it relies on software packages in R such as the 'knitr' [@xie_knitr_2013; @xie_new_2012,] and in python, the 'ipython' notebook environment [@perez_ipython:_2007] that have been developed by scientists and statisticians in the name of 'reproducible research.' These packages are designed to allow a combination of code written in R, python or other programming languages, scientific text (including mathematical formula) and images to be included, and importantly, executed together. In order to do this, they typically combine some form of text formatting or 'markup,' that ranges from very simple formatting conventions (for instance, the 'Markdown' format used in this book is much less complicated than HTML, and uses markup conventions readable as plain text and modelled on email [@gruber_markdown:_2004];) to the highly technical (LaTeX, the de-facto scientific publishing format or 'document preparation system' [@lamport_document_1986] elements of which are also used here to convey mathematical expressions). They add to that blocks of code and inline code fragments that are executed as the text is formatted in order to produce results that are shown in the text or inserted as figures in the text. 

There are different ways of weaving together text, computation and images together. In ipython, a scientific computing project dating from 2005 [@perez_ipython:_2007], and used across a range of scientific settings, interactive visualization and plotting, as well as access to operating system functions are brought together. Especially in using the ipython notebook, where editing text and editing code is all done in the same window, and the results of changes to code can be seen immediately, practices of working with data can be directly woven together with writing about practice. By contrast, knitr composes documents that have been written by interleaving chunks of code and text. When knitr runs, it executes the code and inserts the results in the flow of text. Practically, this means that a text editor, the software used to write code and text, remains somewhat separate from the software that executes the code. While ipython focuses on interactive computation, knitr focuses on bringing together scientific document formatting and computation. Given that both can include code written in other languages(that is, python code can be processed by knitr, and R code executed in ipython), the differences are not crucially important [^3]. 

[^3]:In combining Markdown and LaTex, I also make use of the [Pandoc utility](http://johnmacfarlane.net/pandoc/README.html), a command line tool that converts between different document formats.

[TBC -- some examples of ipython notebooks]

## Where does this stuff go?

Actually, this is a key problem for me – having just said that desires/beliefs in R travel widely, I'm not sure what I mean by 'widely' or 'travel.' Maybe it would be better to say they that they do not travel very far, but undergo some kind of modulation that reduces data and computational friction (Edwards, 2010), and allows them to slip around the corners that separate things. As a preliminary take on this question, I would say that frictions are handled R in several ways: 
1. Habit formation. Because R is a  programming language, it allow scripts to be written and run very on everything from laptops to cloud computing. In this it differs greatly from statistics software applications such as SPSS, SAS, or STATA that, arguably, change less often and under the control of business or scientific needs. Learning R is learning to read, write and run scripts, rather than how to carry out operations using the standard interfaces of menus, buttons and windows found in more conventional software applications. There is also much about its connection to programming and computing cultures (Unix operating system, open source scripting languages such as Perl, scientific programming languages such as FORTRAN) that mobilises R in a different way to statistics software applications. Scripts are much more fragmented, mobile elements than software applications. Their mutability and malleability means they can make and move data across many different interstices. At the same time, learning to use and write R opens pathways to habit-forming investments. 
2. Believing is seeing. Given that seeing something in data, finding patterns, showing connections or regularities, or locating important or relevant parameters animates all work with data, the endless variations in visualization found in R play an important role. The many forms of visualization it allows are very closely connected to debates, trends, fashions concerning how data should be made visible – as table, scatterplot, network diagram, tag-cloud, histogram, heatmap, map, etc. The associated minutiae of visual grammars also connect R directly into the literary technologies of many scientific, government, and business knowledges. It would be hard to over-estimate the role played by 'low science' graphing and diagramming in some of these domains.  
3. The latest and greatest. The latest statistical and algorithmic techniques and models developed by statisticians, domain scientists and computer scientists are often published as R packages (or libraries), especially on CRAN, the Comprehensive R Archive Network (CRAN 2010).  The deposit of an R package  - a collection of functions, data structures, data sets or interfaces to data sources – into CRAN often accompanies journal or book publication in certain fields. The quick rollout of techniques in the form of packages means that people resort to R to keep up. To understand what precipitates the data deluge, how people handle it, and what kinds of resistances, we might think more about the different things that become a package. Maybe it would be worth thinking about packages as monads, as world-making inscriptive-perspectives. This question is approached below in the discussion of the 'Bayesian revolution.'

All of this might indicate how R becomes believable, or how people are prepared to believe in R. But it doesn't say how belief actually is actually worked on in R. Here I think my position could move in two directions – either heading towards an analysis of some localised practices of working with data in R or towards an analysis of how the grounds on which data are made and analysed shift. This is returning to my broader position or hunch that a fairly subtle but important change in what counts as  empirically credible is occurring in many domains. This change troubles the negotiated settlement between beliefs and events achieved in the last 200-300 years of statistical reasoning practice. Any such change is inevitably hard to infer, especially as it is taking place. How could we apprehend something whose forms, practices and position are fluxing? 
What counts as data is subject to constant reshaping. Taking data and changing its form through various rearrangements has become a matter of central economic, commercial, political and scientific importance. While data has long been gathered and published in tables, in files and in databases, the injunction to re-use and re-analyse generates re-shaping or 'data munging' imperatives. We could approach these imperatives from the perspective of various discourses (transparency, sharing, accountability, democracy, Mertonian science, etc), or from the perspective of the world of analytics, data-mining and machine learning where data is seen as a valuable material whose inherent patterns promise new forms of economic, aesthetic or epistemic value. All of this deserves further analysis and comment. However, between the strategies of openness or pattern-finding and the data sources lies a terrain where data is re-shaped, transformed and plied into forms and patterns. The practices of plying, multiplying and applying data seem to me crucial in understanding belief in data. 

## Python: connectivity of practices

- The place of python amongst languages
- Python as symptom of industry, business and science more broadly
- The rise of industrial machine learning


```



## Irises, digits, spam and kittens, titanic survivors: iterating through data


## Reconstruction and problematization

Dewey on the need for reconstruction
Foucault on problematization
'Problematic implementation'

## Conclusion

- return to the psychic life and lived abstraction issue
- data is experience -- some James?

## Notes

[^1]: The dimensionality of data is the topic of a later chapter. There I discuss how machine learning re-dimensions data in various ways, sometimes reducing dimensions and at other times, multiplying dimensions.

[^2]: Machine learning in genomics is also the topic of a later chapter. Recent life sciences are not alone in their resort to computational techniques, but in contrast to commercial, industry or government data practices, it is easier to track how data is transformed, reshaped and re-dimensioned in producing biological knowledges. The scientific publications, the public databases and open source software work together to allow this. 

[^4]: I discuss the competitions and forms of machine learning subjectification in a later chapter. 

## References

    