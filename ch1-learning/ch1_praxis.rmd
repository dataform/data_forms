# Associating with data: a praxiography of machine learning

## Overview

- reconstruction vs problematization -- Rabinow -- on equipment
- data: its giveness, abstraction and actuality
    - observant participation -- developing Wacquant; participating by observing
    - writing recursively -- developing Kelty w.r.t subjects
    - James on feeling of transition - and practicing radical empiricism - -see Massumi
- Implementing machine learning
    - the case of R -- the scientific
    - the case of Python -- the business/industry
    - the case of javascript -- popular culture
- convergence and learning: for fitting a line; but then for choosing which features to use to learn
- learning about learning --
    - the Literature - landeker and kelty on treating literature as the informant
- things to include:
    - my R books, libraries, papers, competitions, blogs, talking, courses
    - knitr/ipython notebooks - executable - some examples from python notebooks
    - the notion of repl -- read-evaluate-print-loop
    - Add in Animation and Automation – The Liveliness and Labours of Bodies and Machines Body & Society March 2012 18: 1-46, 
    - machine learning textbooks -- hastie, williams, manning, see document archive
    - the github version of the book

## Todo

Bogost on carpentering, and the problems of writing -- I have a take on that ... 

## Introduction -- the data-driven mode

Many different tendencies shape what is happening to data today, but two of them impinge directly on my practice sense of the potentials of data. On the one hand, ways of working with  data have shifted substantially over the last decade or so due to the growth in open source programming languages and networked platforms. Scientists use programming languages such as Python and R to do their work, and scientific research, which has long relied on counting and calculation, increasingly organises and processes data more comprehensively because workflows, datasets, algorithms and databases can be quickly and flexibily assembled in code. (Scientific computing languages such as FORTRAN have long underpinned scientific research in various more mathematical fields.) On the other hand, several decades of research and application of statistical and machine learning algorithms in science, government, industry and commerce have gradually developed fairly powerful ways of automating the construction of models that classify and predict events and associations between things, people, processes, etc. Programming and software development has long handled data and algorithms, and conversely, machine learning has always relied on programming. But they are powerfully coalescing in data-driven research and in various domains of data practice in ways. Can we locate anything in the proliferation of machine learning that is more than plying data into a tightly woven matrix of commercial-scientific-governmental knowing/surveillance? I don't answer that question directly here, but my basic answer is affirmative. I think we can and we should  explore some of the shifts in practices of working with data that can be observed around data. These practices are both the object of analysis, and in  certain cases, provide support for a shift in the way in which we might carry out research on changes in media, publics, sciences, commerce, mobilities and  health. 

## Reading machine learning

The way that I approach data has been heavily influenced  by a single highly technical and compendious textbook, _Elements of Statistical Learning: Data Mining, Inference, and Prediction_ [@hastie_elements_2009], now in its third edition. Like the online machine learning courses and programming books I discuss below, _Elements of Statistical Learning_  combines statistical techniques with various algorithms to 'learn from data' [@hastie_elements_2009, 1]. The 18 chapters of this often tersely written and somewhat mathematical book range across various kinds of data (spam email, Californian forest fires, blood pressure measurements, handwritten digit recognition), and various machine learning techniques, methods and algorithms (linear regression, k-nearest neighbours, neural networks, support vector machines, the Google Page Rank algorithm, etc). While these techniques come from different places, in _Elements of Statistical Learning_, they are all framed by a notion of learning based on predictive models. The predictive models in statistical machine learning are  underpinned by a single prediction technique, linear regression or fitting a line to some points, that has been subjected to countless variations, iterations and modifications. The authors of the book, Jeff Hastie, Rob Tibshirani and Jerome Friedman are statisticians working at Stanford and Columbia University. (Statisticians and computer scientists from Stanford University loom large in the world of machine learning, perhaps due to their promixity to Silicon Valley.)  Their own research spans several decades and a range of topics. The book collates and borrows from a panoply of scientific publications  and datasets in fields of statistics, artificial intelligence and computer science. It is not the only book of its kind. I could just have well cleaved to Alpaydin's _Introduction to Machine Learning_[@alpaydin_introduction_2010] (a more computer science-base account), to Christopher Bishop's pattern-recognition focused and highly mathematical text [@bishop_pattern_2006] or Tom Mitchell's  earlier artificial intelligence centred volume [@mitchell_machine_1997].  These and quite a few other recent machine learning textbooks display a range of emphases, ranging from the highly theoretical to the very practical, from an orientation to statistical inference to an emphasis on computational processes, from  science to commercial applications. 

While 'learning' is briefly discussed on the first page of _Elements of Statistical Learning_, the book hardly ever returns to the topic explicitly. We can read on page two that a 'learner' in machine learning is a model that predicts outcomes.  The notion of learning in machine learning derives from the field of artificial intelligence. The broad project of artificial intelligence, at least as envisaged in its 1960s-1970s heydey, is today largely regarded as a failure. There is no general, well-rounded artificial intelligence in existence. But in the course of its failure, many interesting problems were generated. [TBA - references on the history of AI] The field of machine learning might be seen as one such offshoot. The so-called 'learning problem' and the theory of learning machines was developed largely by researchers in the 1960-1970s, based on work already done in the 1950s on learning machines such as the perceptron, the  neural network model developed by the psychologist Frank Rosenblatt in the 1950s [@rosenblatt_perceptron:_1958]. Drawing on McCulloch-Pitts model of the neurone, Rosenblatt implemented the perceptron, which today would be called a single-layer neural network on a computer at the Cornell University Aeronautical Laboratory in 1957. As Vladimir Vapnik, a leading machine learning theorist, observes: 'the perceptron was constructed to solve pattern recognition problems; in the simplest case this is the problem of constructing a rule for separating data of two different categories using given examples' [@vapnik_nature_1999, 2]. While computer scientists in artificial intelligence of the time, such as Marvin Minsky and Seymour Papert, were sceptical about the capacity of the perceptron model to distinguish  or 'learn' different patterns [@minsky_perceptron:_1969], later work showed that perceptrons could 'learn universally.' For present purposes, the key point is not that neural networks have turned out several decades later to be extremely powerful algorithms in learning to distinguish patterns, and that intense research in neural networks has led to their ongoing development and increasing sophistication in many 'real world' applications (see for instance, for their use in sciences [@hinton_reducing_2006], or in commercial applications such as drug prediction [@dahl_deep_2012]). Rather, the important point is that it began to introduce learning machines as an ongoing project in which trying to understand what machines can learn, and to predict how they will classify or predict became central concerns. At the same time, and less visibly,  implementations and applications of techniques derived from artificial intelligence and adjacent scientific disciplines became more statistically sophisticated. That is, the theory of machine learning alongside decision theory was interwoven with a set of concepts, techniques and language drawn from statistics. Just as humans, crops, habitats, particles and economies had been previously, learning machines became entwined with statistical methods. Not only in their reliance on the linear model as a common starting point, but in theories of machine learning, statistical terms such as bias, error, likelihood and indeed an increasingly thorough-going probabilistic framing of learning machines emerged. 

In describing the entwined elements of machine learning techniques, I'm not attempting to provide any detailed history of their development. For the most part, I leave controversies about the techniques to one side. Also, in describing these developments, I focus mainly on what happens from the 1980s onwards. My knowledg of these developments is not properly hierarchical. It derived much more either following citations back out of the textbooks, or looking and experimenting with software implementations of the techniques. Rather than history or controversies, I focus on key methods, and the many configurational shifts associated with their implementations. While many of the machine learning techniques I discuss have much longer lineages (in some cases, runnning back to the 1930s), machine learning techniques begin to circulate much more widely in the 1980s as a result of personal computers, and then in the mid-1990s, the internet. The proliferation of programming languages such as FORTRAN, C, C++, Pascal, then Perl, Java, Python and R, and computational scripting environments such as Matlab, multiplied the paths along which  implementation of machine learning techniques could proceed. It would be impossible for anyone to read the vast machine learning research literature, or follow the the propagation of techniques across domains of science, business and goverment. For instance, a  perceptron that 'learns' the binary logical operation NAND (Not-AND) is  expressed in twenty lines of python code on the Wikipedia 'Perceptron' page [@perceptron_2013]. 

```{r perceptron, echo=TRUE, cache=TRUE, message=FALSE, warning=FALSE, comment=NA, size='smallsize', results='markup', engine='python' }
    
    threshold = 0.5
    learning_rate = 0.1
    weights = [0, 0, 0]
    training_set = [((1, 0, 0), 1), ((1, 0, 1), 1), ((1, 1, 0), 1), ((1, 1, 1), 0)]
     
    def dot_product(values):
        return sum(value * weight for value, weight in zip(values, weights))
     
    while True:
        print '-' * 60
        error_count = 0
        for input_vector, desired_output in training_set:
            print weights
            result = dot_product(input_vector) > threshold
            error = desired_output - result
            if error != 0:
                error_count += 1
                for index, value in enumerate(input_vector):
                    weights[index] += learning_rate * error * value
        if error_count == 0:
            break
 ```

What does this code show? First of all, we should note the novelty of patches of code in Wikipedia pages. In citing this code, I'm not turning to a technical publication or scientific literature as such, just to a Wikipedia page. Second, while perceptrons and neural networks are the topic of a later chapter, the typical features of this code as the implementation of a machine learning algorithm are the presence of the 'learning rate', a 'training_set,' 'weights', an 'error count', a loop function that  multiplies values ('dot_product'). Some of the terms present in the code bear the marks of the theory of learning machines that we will discuss. But much of the code here is much more familiar, generic programming. It defines variables, sets up data structures (lists of numerical values), checks conditions, loops through statements or prints results. Executing this code (by copying and pasting it into a python terminal, for instance) produces several dozen lines of numbers that are initially different to each other, but that gradually converge on the same values (see printout). These numbers are the 'weights' of the nodes of the perceptron as it iteratively learns to recognise patterns in the input values. None of the workings of the perceptron need concern us at the moment. It is a typical machine learning algorithm, almost always included in ML textbooks and usually taught in introductory ML classes.  Perhaps more strikingly than its persistence as an algorithm over half a century, the implementation of the whole algorithm in twenty lines of code on a Wikipedia page suggests the mobility of these methods. What required the resources of a major research engineering laboratory in the 1950s can be re-implemented by a cut and paste operation between wikipedia pages and a terminal window on a laptop in 2013. This is now a familiar observation, and perhaps not very striking at all. But the question of who implements perceptrons or neural networks, and where they implement them today is rather more interesting. 

## Notion of praxiography: where practices are the features

What would we learn by studying the proliferation of implementations of artificial intelligence or machine learning algorithms rather than their history or the controversies associated with them? The question that guides much of this book is how to live with machine learning.  This is a largely affirmative question, rather than a critical one. I am looking for ways of thinking with machine learning, and this has meant learning to do machine learning in some measure. Several general  directions of travel present themselves here. 

Science studies scholars such as Anne-Marie Mol and John Law have long urged the need to keep practice together with ontology. Towards the beginning of _The Body Multiple: Ontology in Medical Practice_[@mol_body_2003], Mol writes:

    >If it is not removed from the practices that sustain it, reality is multiple. This may be read as a description that beautifully fits the facts. But attending to the multiplicity of reality is also an act. It is something that may be done – or left undone [@mol_body_2003, 6]

Mol's work offers a cogent case for developing accounts of what is real steeped in the practices that make it real. Similar affirmations of the sustaining role of practice can be found in many parts of social sciences and humanities. So for the first part, looking at implementations and the flow of implementations is a way of keeping practices in the picture, and therefore, a heuristic for learning reality as multiple. This already suggests that describing machine learning in terms of practices could be an act that attends to their multiplicity. Mol's notion of *praxiography*, a variant on ethnography, as an act of describing practice in the name of preserving their multiple-making value, has particular resonance for work with data, which is itself always heavily entwined in writing and reading practices. Could we praxiographically go into data and machine learning?

A second  and related tack comes from the work of Alfred North Whitehead. Whitehead's work on how abstractions are embodied is highly relevant to thinking about machine learning. While Whitehead is comprehensively critical of certain tendencies in modern science (the fallacy of misplaced concreteness; the reduction of space-time to discrete locations, etc), he is very enthusiastic about the potential for better abstractions. While voiced in terminology that takes some getting used to, the broad affirmative point can be grasped:

    > But this enhancement of energy presupposes that the abstraction is preserved with its adequate relevance to the concrete sense of value-attainment from which it is derived. In this way, the effect of the abstraction stimulates the vividness and depth of the whole of experience. [@whitehead_modes_1958,169]

The enhancement of energy Whitehead talks about here is concerned with the 'fortunate use of abstractions' (169). Machine learning and  much work with data today can be seen as a form of abstraction (as can much of science). The crucial question for Whitehead, however, is how to abstract well. Like Mol, he advocates keeping abstractions together ('preserved with its adequate relevance') with something like practice (' to the concrete sense of value-attainment from which it is derived'). Like Mol too, the virtue of this bundling together of abstractions with their concrete sense affects the 'whole of the experience.' Abstractions can make experience more vivid or deep if they are aligned and assembled connectively. Here I pin some methodological hope on Whitehead's work as a way to experiment with abstraction as stimulant and enrichment, not as reduction or pale imitation. 

A final and again different kind of approach here comes from the Elizabeth Wilson's study, _Affect and Artificial Intelligence_ [@wilson_affect_2010]. Drawing on a combination of psychoanalytic, psychological and archival materials, Wilson discusses the work of key figures in the early history of artificial intelligence such as Alan Turing, Warren McCulloch and Walter Pitts on neural nets, and recent examples of affective computing and robots such as Kismet. The framing of her project is interesting:
    
    > Sometimes machines are the very means by which we can stay alive psychically, and they can  just as readily be a means for affective expansion and amplification as for affective attenuation. This is especially the case of computational machines. 30 

Under what conditions do machines and for present purposes, computational machines, become 'the very means we can stay alive psychically'? She  addresses this question by positing 'some kind of intrinsic affinity, some kind of intuitive alliance between the machinic and the affective, between calculation and feeling' (31), and suggesting that the 'one of the most important challenges will be to operationalize affectivity in ways that facilitate pathways of introjection between humans and machines' (31). Introjection, the process of bringing the world within self is, according to psychoanalytic accounts of subjectivity, crucial to the formation of 'a stable subject position' (25). Wilson envisages introjection of machine processes as a good, not as a failure or attenuation of relation to the world. 

These three takes on how to think about practices, abstraction and machines work on somewhat different levels, but they broadly share an interest or even a commitment to multiple, plural or surprising conjunctions between what people do, what the world is, and the many adjustments, alignments and slippages that connect them through models, abstractions, computation and data. I regard them as  offering guidance or methodological steers on how to work with data, with models and with machine learning somewhat differently. The question is how would one practically explore such insights.

## Mobility of statistical methods in the 1990s: the case of R

Nearly all of the examples in _The Elements of Statistical Learning_ are implemented in  a single programming language, R. R is a well-known and widely used statistical programming language and environment  [@r_development_core_team_r_2010]. An open source programming language, according to surveys of business and scientific users, R is replacing popular software packages such as SPSS, SAS and Stata as the statistical and data analysis tool of choice for many people in business, government, and sciences ranging from political science to genomics, from quantitative finance to climatology [@rexer_analytics_rexer_2010]. Developed in New Zealand in the mid-1990s, and like many open source software projects, emulating S, a commercialised predecessor developed at AT&T Bell Labs during the 1980s, R is now extremely widely used across life and physical sciences, as well as quantitative social sciences. Many undergraduate and graduate students learn R as a basic tool for statistics. Skills in R are often seen as essential pre-requisite for scientific researchers, especially in the life sciences. Estimates of its number of users range between 250000 and 2 million. Increasingly, R is integrated into commercial services and products (for instance, SAS, a widely used business data analysis system now has an R interface; Norman Nie, one of the original developers of the SPSS package heavily used in social sciences, now leads a business, Revolution, devoted to commercialising R; R is heavily used at Google, at FaceBook, and by quantitative traders in hedge funds, etc.).  R is an interestingly diffuse entity. Some ways of working with data are way more clearly focused on equations and calculation. For instance, in order to pursue number practices in physical sciences and engineering, maybe MATLAB or Mathematica would be better. In business or government, many ways of working with data are more focused on ordering and searching. For instance, in order to the look at the organisation of large aggregates of data, relational databases, query languages, data-centre architectures, and perhaps the techniques of aggregating and disaggregating data en-masse would be worth studying. 

Why then choose R, a statistical programming language, a language developed largely by academic statisticians and research scientists rather than computer scientists, software engineers or hackers? The primary ground is that the development, demonstration, proliferation and adoption of machine learning has occurred in R. The R packages such as 'ElemStatLearn' [@halvorsen_elemstatlearn:_2012], for instance, encapsulates some of the main datasets and methods discussed in _The Elements of Statistical Learning_.  Research articles and texbooks in statistics commonly both use R to demonstrate methods and techniques, and create R packages to distribute the techniques and sample data. For instance, an article published in 2007 by the Princeton computer scientist David Blei on 'correlated topic models' for classification of documents [@blei_correlated_2007] leads to a package 'topicmodels' available from the 'Comprehensive R Archive Network (CRAN)' a few years later [@cran_comprehensive_2010]. CRAN itself is an important infrastructure in the global life of R. 

```{r mirrors, echo=TRUE, cache=TRUE, message=FALSE, warning=FALSE, comment=NA, size='smallsize', results='markup' } 
    
    library(utils)
    mirrors = getCRANmirrors(all = FALSE, local.only = FALSE)
    head(mirrors)

```
A set of `r nrow(mirrors)` mirror sites in `r length(unique(mirrors$Country))` countries distributes the latest R platform and R packages to many different users. These mirror sites are largely run by universities and public institutions, alongside a few commercial users (such as Revolution, a company I discuss in a later chapter). So, in terms of machine learning practice, R is important because it is a staple programming language in statistics research and teaching. It is practically circulated in ways that make it globally available. 

R has also attracted  mainstream media attention.  An article in the _New York Times_ in 2009 highlighted its practical importance in data analysis [@vance_data_2009], but at that time the _New York Times_ was heavily promoting its idea of data journalism.  More broadly, R is an evocative object, to use the psychoanalyst Christopher Bollas' term [@bollas_evocative_2009], an object through which run many different ways of thinking. Standing somewhere at the intersection of statistics and computing, modelling and programming, many different disciplines, techniques, domains and actors intersect in R. Bollas suggests that 'our encounter, engagement with, and sometimes our employment of, actual things is a _way_ of thinking' [@bollas_evocative_2009, 92].  R embodies something of plurality of practices of working with measurements, numbers, text, images, models and equations, with techniques for sampling and sorting, with probability distributions and random numbers. It engages immediately, practically and widely with  words, numbers, images, symbols, signals, sensors, forms, instruments and above all abstract forms such as mathematical functions like probability distributions and many different architectural forms (vectors, matrices, arrays, etc), as it employs data. By virtue of thousands of packages that flow across boundaries between nature and culture, between aesthetic, epistemic and pragmatic domain, R embodies a wide-with data economies, cultures, sciences, politics and technologies. Somewhere between calculation and searching, R channels counting and sorting, in the estimation of likelihoods.

How would we describe the R-based practices in ways that deepen our perception of their plurality, their relevance, and potential amplification of affect? Note that Bollas distinguishes encounter, engagement and employment of things. Employment, in his terms, is less likely to lead become a way of thinking than encounter and engagement. On balance, I'd have to admit that the employment dimension of R dominates.  It is easy today to find an employment-centric view of data practice. The 'chief economist' at Google, Hal Varian in 2009 made a widely quoted prediction about data:

    > The ability to take data — to be able to understand it, to process it, to extract value from it, to visualize it, to communicate it — that’s going to be a hugely important skill in the next decades, not only at the professional level but even at the educational level for elementary school kids, for high school kids, for college kids. Because now we really do have essentially free and ubiquitous data. So the complimentary [sic] scarce factor is the ability to understand that data and extract value from it.” Hal Varian, chief economist at Google: [@mckinsey_&_company_hal_2009]

While cited here in a report prepared by the global business consultancy, McKinsey & Co, I have seen this quote or selections from it in many different contexts, ranging from government reports on higher education to the student noticeboard in the statistics department at the university. Unsurprisingly, it ripples across the globe. What Varian presents as an 'important skill'  also has a 'scarce factor':  the 'ability to understand that data.' While Varian presents understanding data as the prelude to 'extract[ing] value from it,' understanding might take different forms, depending on the kind of encounter or engagement that precedes it. 

Encounters with R are by no means limited to employment. A somewhat broader framing of the value of data practice can be found associated with the language. Some people – admittedly a small group – explicitly promote R in association with the wider growth of data democracy or open data. I say somewhat, because the promotion of R as a valuable software object is linked to individual entrepreneurship. Norman Nie, the original developer of the widely-used SPSS social science statistics software (see [uprichard_spss_2008]), is a proponent of R. Here is Norman Nie interviewed in _Forbes_, a leading business news magazine, evangelising for R:

    > Everyone can, with open-source R, afford to know exactly the value of their house, their automobile, their spouse and their children--negative and positive  … It's a great power equalizer, a Magna Carta for the devolution of analytic rights [@hardy_power_2010].

While strictly speaking referring to R for employment purposes, Nie, the founder of Revolution Analytics, a company that provides software and support services to R users, promotes R as a way of protect individual liberties to know what they own. In the context of the global financial crisis of 2007, it may be that the value of houses became much more problematic, and understandably, harder to know. And the reference to 'spouse and their children' must be flippant. Yet the 'devolution of analytic rights' that Nie speaks of here, even as he frames it in terms of entrepreneurial individualism, is important. While Nie's company  Revolution Analytics promotes the incorporation of  R into  powerful and pervasive business and government prediction systems (such as SAS and IBM's Pure Data [@ibm_ibm_2013]), Nie here points to the problem of 'manipulation and control' of customers associated with just that incorporation. The 'analytic rights' he advocates in the form of R are meant to help individuals counterbalance the power of large scale data analysis conducted by corporations and governments. 

Let us stick with the example of house prices. This is possibly a bad example, since house prices seem so webbed into employment, salaries, class position, and business as usual. It will be interesting to see if it is possible to write about predictive modelling of house prices in a way that responds to what Mol, Whitehead and Wilson propose. The house price  come from San Francisco, prior to the global financial crisis 2007. As discussed  below, the dataset comes from an online machine learning course. The dataset is fairly small and simple and can be loaded, summarised and plotted using a few lines of R code. 

```{r house_price, echo=TRUE, cache=TRUE, message=FALSE, warning=FALSE, comment=NA, size='smallsize', results='markup' }

    house_prices = read.csv('data/ex1data2.txt', header=FALSE)
    names(house_prices) = c('size', 'bedrooms', 'price')
    head(house_prices)
    summary(house_prices)
    attach(house_prices)
    plot(size, price)
 ```
Two architectures come together here. On the one hand, around fifty houses  in a city, San Francisco, are described in terms of the number of bedrooms, their total floor area and the price they sold for. This is a very sparse description of urban space, and it reduces the encounter with the psychically rich form of houses/apartments to a cognitive minimum. On the other hand, the description of this encounter has its own architecture -- the  very familiar form of the table, grid or matrix. Whatever else happens to the data in machine learning, the matrix form pervades much data practice. As we will see below, in modelling this data, certain parts of the matrix will be separated out and treated differently. For the moment, only the bare architecture of the matrix frame concerns us.

Nie's advocacy of R points towards a surfeit or overflowing sense of possibility associated with R. It would be possible to cite many other instances of this belief and desire in the potential of R. A good indication of this overflow is the aggregate blog, [R-bloggers](R-bloggers.com). R-bloggers brings together several hundred R-related blogs in one place. The range of topics discussed there on any one day – Merrill-Lynch Bond Return indices, how to run R on an iPhone, using US Department of Agriculture commodity prices, analysing human genetic variation using PLINK/SEQ via R, using the 'Rhipe' (sic!) package to program big data applications on Map-Reduce cloud architectures, doing meta-analysis of phylogenetic trees, etc., etc (R-bloggers 2011) – suggest how widely desires/beliefs in R range. To take just one fairly recent example, 'R Analysis Shows How UK Health System could save £200 million' claims a recent post on the site. While many of the applications, experiments, or techniques reported there can be reduced to employment or to individual uses of R, they also, as we will see, suggest the potential for encounters and engagements.     

If we were to try to describe these encounters and engagements in which R as an actual thing is also a way of thinking, how would we go about it? The act of describing the practices of working with R might, at least this is my hope, be a way of staging an encounter with those aspects of data practice that exceed employment, and that instead do something like vivify experience. But one problem is that there are so many different ways to do things with data. In many cases, data practice begins not from data, but from books, articles and websites. This a fairly banal observation, but acquiring, generating, finding or, to use the term often used by database architects, 'discovering' data is not all that common in practice. It is perhaps far more typical to encounter data in the context of learning how to manipulate, sort, model or display it. These settings are multiple. We could turn to R-blogger's aggregation or the programmer Q&A site [Stackoverflow](http://www.stackoverflow.com), which has many questions tagged with R. There are also many online manuals, guides and tutorials relating to R [@wikibooks_r_2013]. For present purposes, I draw mainly on semi-popular books such as _R in a Nutshell_ [@adler_r_2010], _The Art of R Programming_ [@matloff_art_2011], or _R Cookbook_ [@teetor_r_2011]. These books are not written for academic audiences, although academics often write them and use them in their work. They are largely made up of illustrations and examples of how to do different things with different types of data, and their examples are typically oriented towards business or commercial settings, where, presumably, the bulk of the readers work, or aspire to work. Given a certain predisposition (that is, geekiness), these books and other learning materials can make for enjoyable reading. While they vary highly in quality, it is sometimes pleasing to see  the economical way in which they solve common data problems. This genre of writing about programming specialises in posing a problem and solving it directly. In these settings, learning takes place largely through following or emulating what someone else has done with either a well known dataset (Fisher's 'iris') or a toy dataset generated for demonstration purposes.  The many frictions, blockages and compromises that often affect data practice are largely occluded here in the interests of demonstrating the neat application of specific techniques. Yet are not those frictions, neat compromises and strains around data and machine learning precisely what we need to cleave to praxiographically? In order to demonstrate both the costs and benefits of approaching R through such materials, rather than through ethnographic observation of people using R, it will be necessary to stage encounters here with data that has not been completely transformed or cleaned in the interests of neatly demonstrating the power of a technique. One way to do this is by writing about code while coding. 

## Learning to predict online

Nie's recommendation of R as a way of learning house prices is not unusual. This mention of house price prediction is not a random example. It is a surprisingly common teaching or demonstration example in machine learning, alongside other iconic datasets including R.A. Fisher's 'iris', the MNIST handwritten digits dataset [@lecun_mnist_2012]  (derived from U.S. National Institute of Standards and Technology (NIST)), or the cat photo dataset discussed in the previous chapter. In the course of writing this book, as well as reading the academic textbooks, popular how-to books, software manuals and blog-how-to posts,  I attended graduate courses in Bayesian statistics, genomic data analysis, data mining, and missing data. Significantly, I also participated in the online machine learning courses and some machine learning competitions [^4]. Typically, a house price dataset might be used for teaching purposes, as in Professor Andrew Ng's course 'Machine Learning' CS229 at Stanford (http://cs229.stanford.edu/) or the shortened version of this course delivered under the title 'Machine Learning' on [Coursera.org](https://class.coursera.org/ml-003/class/index). These two  courses, and especially CS229, are fairly typical recent expositions of machine learning. In these lectures, a dataset derived from house prices in San Francisco. Given the course was running before 2007, it is likely that this dataset does not reflect the financial crisis that began in 2007 and led to drastic reductions in house prices from 2008 onwards. After sitting through several dozen hours of Ng's online lectures, and attempting  some of the review questions and programming exercises, including implementing well-known algorithms using R, I have some sense of how predictive models are constructed, and what kinds of algorithmic architectures and forms of data are preferred in machine learning. Importantly, these courses have been viewed by many other people. Ng's machine learning course on Coursera (of which he is co-founder, along with Daphne Koller, another machine learning scientist from Stanford), has a typical enrolment of 100,000. Youtube videos of his Stanford University lectures have viewing figures of around 500,000. My participation in these courses was a kind of arduous experiment. Like the hundreds of thousands of other students, I was trying to learn about machine learning. At the same time, however, I was trying to keep open a set of potentials that much of the course was trying to minimise or shutdown, even as instructor's such as Ng from Stanford or Pedro Domingo from the University of Washington sought to convey their excitement about the content of their courses. In both the Coursera  and Stanford courses, Ng uses the house price dataset to demonstrate certain techniques of predictive modelling. 

How would someone with a somewhat transdisciplinary background (undergraduate degrees in science and philosophy; postgraduate in philosophy) learn about machine learning online? I first encountered machine learning in reading recent scientific articles on genomics. Scholars interested in genomics have often discussed the growth of DNA sequence data in contemporary life sciences. They have carefully studied sequence and other biological databases, but less often attended to the ways in which data from those databases has been modelled or used predictively [^2]. Understanding predictive modelling in genomic science is difficult, since both the biology and the statistics are technically challenging. The possibility of reconstructing or implementing genomic models are more limited. Although I have attended genomic data analysis courses, even in these classes, where my classmates where either postgraduate research students or scientists, only a narrow selection of techniques for working with genomic data are covered. The online machine courses are, by contrast, quite generic and the examples are broad-ranging.  

Both the textbooks and the online university courses on machine learning have a similar topic structure. They nearly always begin with 'linear models' (fitting a line to the data), then move to logistic regression (a way of using line fitting to classify binary outcomes; for example, spam/not-spam; malignant/benign; cat/non-cat), and afterwards move to some selection of neural networks, decision trees, support vector machine and clustering algorithms. They add in some decision theory, techniques of optimization, and ways of selecting predictive models (especially the bias-variance tradeoff). Whether in the textbook or the online course, examples such as the house price dataset form part of the learning of machine learning. They differ, however, in one crucial regard. Reading the _Elements of Statistical Learning_ textbook or one of machine learning books written for programmers (for example, _Programming Collective Intelligence_ or _Machine Learning for Hackers_ [@segaran_programming_2007; conway_machine_2012]) does not directly enrol the reader in a machine learning process. By contrast, doing a Coursera course on machine learning brings with it an ineluctable sense of being machine-learned. The learners on Coursera are the target of machine learning. Daphne Koller and Andrew Ng are leading researchers in the field of machine learning. They are also co-founded  the online learning site [Coursera](http://coursera.org).  As experts in machine learning, it is hard to imagine how they would not treat courses as learning problems. And indeed, Daphne Koller sees things this way:

    There are some tremendous opportunities to be had from this kind of framework. The first is that it has the potential of giving us a completely unprecedented look into understanding human learning. Because the data that we can collect here is unique. You can collect every click, every homework submission, every forum post from tens of thousands of students. So you can turn the study of human learning from the hypothesis-driven mode to the data-driven mode, a transformation that, for example, has revolutionized biology. You can use these data to understand fundamental questions like, what are good learning strategies that are effective versus ones that are not? And in the context of particular courses, you can ask questions like, what are some of the misconceptions that are more common and how do we help students fix them? [@koller_daphne_2012]

Whether the turn from 'hypothesis-driven mode to the data-driven mode' has 'revolutionized biology' is debatable (I return to this in a later chapter). And whether or not the data generated by my participation in Coursera's courses on machine learning generates data supports understanding of fundamental questions, that seems also improbable. Nevertheless, the loopiness of this description interests and appeals to me. I learn about machine learning, a way for computer models to optimise their predictions on the basis of 'experience'/data, but at the same time, my learning is learned by machine learners. This is not something that could happen very easily with a printed text, although versions of it happen all the time as teachers work with students on reading texts. 

## Writing code about code: scientific software and executable papers

I've been writing code for several decades. But writing code was always something that lay at a remove from my academic work.  Only recently, owing mainly to developments in ways of analysing and publishing scientific data, have I found ways to write about code while coding. This looping between writing code and writing about code is the main way that this book has been written. Not all of the code I've written in implementing machine learning models or in reconstructing certain data practices is included in this text, just as not all of the prose text I've written in trying to construct arguments or think about data practices has been included. Much has been cut away and left on the ground. So, like the recipe books, cookbooks, how-tos, tutorials and other documentations I have read, things are heavily tidied up here. At many points in researching the book, I digressed a long way into quite technical domains of statistical inference, probability theory, linear algebra, dynamic models as well as database design and data standards. Much of this exploration is lost on me. And since this is neither a textbook or a how-to cookbook, I don't need to retrace my path through all that. Nevertheless, the several years I have spent  writing about data practice has felt substantially different to any other project by virtue of a strongly recursive invocation of code in text, and text in code, made possible by working on code and text within the the same file, in the same text editor. Switching between writing  R and  Python code (about which I say more below) to retrieve data, to transform it, to produce graphics, to construct models or some kind of graphic image, and within the same file be writing academic prose, is one way to write more praxiographically.

The mixture of text, code and images depends on an ensemble of software tools that differ somewhat from the usual collection of word processor, bibliographic software, image editor and web browser. In particular, it relies on software packages in R such as the 'knitr' [@xie_knitr_2013; @xie_new_2012,] and in python, the 'ipython' notebook environment [@perez_ipython:_2007]. Both have been developed by scientists and statisticians in the name of 'reproducible research.' These packages are designed to allow a combination of code written in R, python or other programming languages, scientific text (including mathematical formula) and images to be included, and importantly, executed together. In order to do this, they typically combine some form of text formatting or 'markup,' that ranges from very simple formatting conventions (for instance, the 'Markdown' format used in this book is much less complicated than HTML, and uses markup conventions readable as plain text and modelled on email [@gruber_markdown:_2004];) to the highly technical (LaTeX, the de-facto scientific publishing format or 'document preparation system' [@lamport_document_1986] elements of which are also used here to convey mathematical expressions). They add to that blocks of code and inline code fragments that are executed as the text is formatted in order to produce results that are shown in the text or inserted as figures in the text. 

There are different ways of weaving together text, computation and images together. In ipython, a scientific computing project dating from 2005 [@perez_ipython:_2007], and used across a range of scientific settings, interactive visualization and plotting, as well as access to operating system functions are brought together. Especially in using the ipython notebook, where editing text and editing code is all done in the same window, and the results of changes to code can be seen immediately, practices of working with data can be directly woven together with writing about practice. By contrast, knitr composes documents that have been written by interleaving chunks of code and text. When knitr runs, it executes the code and inserts the results (calculations, text, images) in the flow of text. Practically, this means that a text editor, the software used to write code and text, remains somewhat separate from the software that executes the code. While ipython focuses on interactive computation, knitr focuses on bringing together scientific document formatting and computation. From the perspective of praxiography, given that both can include code written in other languages(that is, python code can be processed by knitr, and R code executed in ipython), the differences are not crucially important [^3]. This whole book could have been written using just Python, since Python is a popular general purpose programming language, and many statistical, machine learning and data analysis libraries have been written for Python. Modules such as NumPy, SciPy, Scikit-learn, open-cv or Pandas [TBA : references needed here] mean that almost anything that could be done in R could also be done in Python. But I write using both R and Python in order to highlight the tensions between the more research-oriented R and the practical implementations typical of Python. 
 
Writers are always drawn into what they write about to some extent. So while I have read textbooks on machine learning and statistics, how-to books on data analysis, data-mining and machine learning, as well as myriad online documents, research papers, and help files, the praxiographic contact zone for me has been the attempt to bring the writing of code and writing about code into proximity, indeed to maximise the degree of mixture. Already at various points in this text, mixture of written materials has figured. The question is how this mode of writing answers the methodological injunctions of keeping the practices present (Mol), maintaining the 'concrete sense of value-attainment' (Whitehead) or allowing 'affective expansion' (Wilson). It seems as though, unusually, it brings a world of practices directly into a written text. Rather than simply describing a situation, executable texts form part of the situation. They belong to the situation to the extent that the code in the text invokes processes, or navigates architectures and infrastructures. Executable text is differently performant, but in ways that are not always controllable.  

## Vectoral encounters

While there are many different ways to encounter data, let us imagine it happens somewhat by chance, as when walking down a city street we notice something about a building for the first time. Some feature, old or new, attracts our attention. What is happening in such situations? From a psychoanalytic perspective, as Christopher Bollas writes, 

> the ability to move freely in the object world, to use its thing-ness as a matrix for thinking-by-action, pivots around whether the aleatory object determines - whether we move on quickly due to the dynamic of our last encounter with the object - or whether we select objects because we are unconsciously grazing: finding food for thought that only retrospectively could be seen to have a logic [@bollas_evocative_2009, 93].

Something similar happens in encountering data. There are many potentially interesting, relevant or important datasets out there. Our encounters with them are necessarily 'aleatory' due to their extent. Huge public scientific databases across the gamut of disciplines, social media datasets, government statistical data, data from various environmental sensors, financial market and other price data (so much of this!), let alone all the media forms -- images, text, video -- that can be rendered into data, congregate on the internet. Even researchers within specific fields struggle to 'discover' the data  in their field.   

For instance, via the [World Bank Data catalog](http://data.worldbank.org/developers/data-catalog-api), the World Bank publishes much data on global economic activity. Much of this data can be retrieved from its Application Programmer Interface (API) using R code. The World Bank Data is a thing. How can its thing-ness be used a 'matrix for thinking-by-action'? This data could also be downloaded using a web browser, and then imported into a spreadsheet, but in this case, since I'm interested in how a programming language affects encounters with data, the R code helps us slow down the dynamics of the encounter a bit. The lines of code, unlike a series of mouse movements, pointing, clicking and typing into a spreadsheet, make it possible to track the movements and gestures involved much more closely. Ironically, coding by hand slows down practice in some ways. Sometimes code is criticised for its unreadability or inaccessibility. Actually, in many ways, code makes it possible to follow practice much more closely.

```{r worldbank, echo=TRUE, cache=TRUE, message=FALSE, warning=FALSE, comment=NA, size='smallsize', results='markup' } 

    library(RCurl)
    library(rjson)
    url = 'http://api.worldbank.org/countries?incomeLevel=OEC&format=json'
    high_income_countries = fromJSON(getURL(url))
    countries = unlist(sapply(high_income_countries[[2]], '[', 'name'), use.names=FALSE)

```

There is an element of retrospective logic in looking at these lines. It still surprises me to see how tersely whole infrastructures can be marshalled in code. I have written lines like the first  five lines of R code shown here many times. They superimpose internet protocols (supplied via the RCurl library), data standards (rjson), internet servers (api.worldbank.org), databases (the 'countries?incomeLevel...' is a database query), data structures and algorithms ('unlist'). In this case they request a list of high-income countries (members of the OECD) from the WorldBank databases. The World Bank database returns a nested list describing `r length(countries)` countries. The list is hardly surprising (`r paste(countries[1:7],collapse=', ')` ... ). But the point of this code is to show something of the economy of movement and reshaping of data associated with R.  In this code, the first line loads an R library (the 'RCurl' package) to make requests to internet servers. The second line loads an R library  (the 'rjson' package) that can process data in the JSON (Javascript Standard Object Notation) format, a data format commonly used by servers to transfer data. Lines 3-4 provide the internet address of the World Bank API, request the data, receive the response, and convert the JSON data into a typical R data structure, a list. The final line selects from the list, which has many sub-lists nested within it, just the names of the countries.  Although this trivial code vignette does not do any of the understanding or extracting of value that Hal Varian propounds, it shows something about the state of data practice. Brevity in code suggests very well travelled paths of practice. In terms of urban architecture, the packages and modules that support this brevity are like escalators that quickly move people between levels and floors in a city.  Like the how-to book with their often just-so demonstrations of problem solving, the compactness or briefness code suggests that many obstacles, detours and blockages have already been encountered by others and dealt with by them. This is not to deny any agency on my part.  I did some 'processing'  here in, for instance, choosing a path into the World Bank datasets via the selection of the query terms (`r cat(url)`). It also includes the re-formatting operations that first transform JSON data into an R list, and then traverse the nested list structure, selecting only the items of data labelled 'name.' These kinds of selection operations, and transformations in the format of data are vital in contemporary data. Whether in the form of well-established relational database query language SQL, in the requests made to one of the legion of APIs that render data on various internet platforms such as Youtube, WorldBank or Pubmed, modifications in the shapes and sorts of data are a constant concern. These include transformations in types of data. 

For instance, numerical data might binned into categories such as high, medium, low or some other form of ranking; text might be split into keywords; the presence or absence of some value might be counted; etc. 

```{r world_bank_flat, echo=TRUE, cache=TRUE, message=FALSE, warning=FALSE, comment=NA, size='smallsize', results='markup' }

    countries_table = do.call('rbind', high_income_countries[[2]])
    df=do.call('rbind', apply(countries_table, MARGIN=2, 
        FUN =unlist, use.names=FALSE))
    countries_table_complete = t(df)
    head(df)
 ```

Some of these transformations matter more than others. The further lines of code produce a table from the WorldBank data. Tables, arrays and matrixes -- the rectangular form of data that typically puts one example per row, with different columns representing different measurements, attributes or properties of the examples. In R, such matrixes and dataframes are heavily used for reasons that go deep in machine learning algorithms and data practices more generally. These kinds of table-making operations are often highlighted in books, documentations and tutorials on R since they deeply affected how code runs. In this case,   the code vignette turns the list, with its sublists, into a flat table or matrix. This is a kind of topological transformation since the nested list of countries with their economic indicators has a different spatio-data structure than the table with its `r ncol(countries_table_complete)` columns.  

The code for these transformations is ugly and difficult to read, and perhaps could be written more cleanly. Nevertheless, at three points  -- the two `do.call` statements, and the `apply` function -- it demonstrates a distinctive aspect of R, and other programming languages designed for data practice (Octave, Matlab, Python's NumPy, or C++ Armadillo): vectorised transformations of data.

```{r vectorisation, echo=TRUE, cache=TRUE, message=FALSE, warning=FALSE, comment=NA, size='smallsize', results='markup' } 
 
     vector1 <- c(0,1,2,3,4,5,6,8,9,10)
     vector2 <- c(0,1,2,3,4,5,6,8,9,10)

     # looped addition
     result_looped = vector()
     for (i in vector1) {
        result_looped[i] = vector1[i] + vector2[i]
     }
     result_looped

     # vectorised addition
     result_vectorised  <- vector1 + vector2
     result_vectorised

```
In mainstream programming languages, transformations of data are often done using loops and array constructs in which some operation is successively repeated on each element of a data structure. In vectorised languages such as R, transformations of a data structure usually expressed in one line of code that seems to simultaneously work on all the elements of the code. As the widely used _R Cookbook_ puts it, 'many functions [in R] operate on entire vectors, too, and return a vector result' [@teetor_r_2011, 38]. Or as _The Art of R Programming: A Tour of Statistical Software Design_ by Normal Matloff puts it, 'the fundamental data type in R is the _vector_' [@matloff_art_2011, 24], and indeed in R, all data is vector. There are no individual data types,  only varieties of vectors in R. Again, as _R in a Nutshell_  puts the point directly: 'in R, any number that you enter ... is interpreted as a vector' or as 'an ordered collection of numbers' [@adler_r_2010, 17]. The practical difference is illustrated in the code vignette above in which two 'vectors' of numbers are added together, in the first case using a classic `for`-loop construct, and in the second case using an implicitly vectorised arithmetic operation `+`. The difference in speed between adding $1 ... 10$ using a loop or vector arithmetic is completely trivial here, but when millions of numbers are handled arithmetically, these implementation differences significantly affect the work of both programmers and computing machinery.  This simultaneity is only apparent, since somehow the underlying code has to deal with all the individual elements, but vectorised programming languages take advantage of hardward optimisations or carefully-crafted low-level libraries. 

While the examples of vectorised computation that I have shown are relatively trivial -- turning a nested list into a table, multiplying sequences of numbers -- these vectorised operations are carried out at scale in many domains of contemporary computation. They are, for instance, especially important in computer graphics, where constant transformations of matrices of numbers support animation. They are extensively used in statistical modelling and machine learning, where 'fitting a model' to data is often literally implemented through the multiplication of matrices containing data in order to calculate parameters of a model. So when Brian Massumi writes that 'the space of experience is really, literally, physically a topological hyperspace of transformation' [@massumi_parables_2002, 184], he may have been describing the heavily vectorised operations on which much machine learning, statistical modelling and prediction depend. 

## Reshaping data densities

I am describing this style of working with data in vectors not only because they are conducive to statistical computation.  R and other numerical computing environments are not only reliant on vectorised operations for the purposes of efficiency or speed of computation. The vectoral treatment of data taps into and is interwoven the dimensionality of data more generally. Later chapters will discuss various ways in the dimensionality of data fluxes in machine learning.   The dimensieonality of data practice is something that we hardly think of when we think of models, algorithms, predictions and smart devices. But in terms of multiplying matrices, dimensionality both constrains and enables many aspects of the prediction [^1]. Perhaps on the grounds of data dimensionality alone, we should attend to dimensionality practices in  R. 

There is another more praxiographic reason to follow them. Often data is  represented as if it is an homogeneous mass, but in reality it takes many different shapes and has many different _densities_.  I borrow the term 'density' from statistics, where *probability density functions* are often used to describe the distribution of probabilities of different values of a variable.   Data values are spaced out in many different density shapes, depending on how the data has been generated or created. Whatever the starting point (a measuring instrument, a device, a random number generator, etc.), it is quite likely that a particular machine learning, data visualisation or statistical test will need the data to be in specific linear shapes (vectors, arrays, tables, etc), and fit within a certain volume.    The process of re-shaping data for statistical, visual, predictive or even storage purposes, links the concrete situation, the praxiographic contact zone, to more abstract forms I will discuss later, such as models. If, as I have been suggesting, we sought to keep in contact with the practices, or the concrete value-situation, then the reshaping and reflowing of densities matters greatly.   This forming and reforming of data is evidence of  implicated relations. These practices attest to what Whitehead called 'strain': 'a' feeling in which the forms exemplified in the datum concern geometrical, straight, and flat loci will be called a 'strain.' In a strain, qualitative elements, other than the geometrical forms, express themselves as qualities implicated in those forms' [@whitehead_process_1960, 310]. In most data practice, the exemplified forms are straight, flat or geometrical.  Yet many different practices seeks to elicit relations that strain  -- in both senses of the term, to apply force and to filter out -- the data.  Put differently, many data practices working in the name of linear forms effectively trace strain feelings. These feelings are the affective connectors between the abstractions and the concrete-value situations in which data arises. They are vital to the psychic life of data, and the ongoing reality-multiples it figures. 

For example, as a programming language, R is striking for its vectorised `-ply` constructs.  There are several in the core language and many to be found in packages (the popular 'plyr' package; versions of 'ply' can also be found in recent Python data analysis libraries such as Pandas).  These include `apply`, `sapply`, `tapply`, `lapply`, `mapply`. All of the `-ply` constructs have a common feature: they take some collection of things (it may be ordered in many different ways – as a list, as a table, as an array, etc), do something to it, and return something else. This hardly sounds startling. But while most programming languages in common use offer constructs to help deal with collections of things sequentially (for instance, by accessing each element of data set in turn and doing something), R offers ways of dealing with  them all at once. The `-ply` constructs ultimately derive from the functional logic developed by the mathematician Alonzo Church in the 1930s [@church_note_1936; @church_introduction_1996], R presents difficulties for programmers used to so-called procedural programming languages. The functional programming style of applying functions to functions seems strangely abstract. Once you get a feel for them, these -ply constructs they are very convenient to write, and the code runs much faster. Like the vectorised operations described above, they implicitly parallelise operations on data in ways that adapts to the increasingly parallel contemporary chip architectures. The `-ply` operations reduce both data and computational frictions. The real stake in -plying data, however, is not speed but transformation. -Plying makes working with data less like iteration (number, text, table, list, etc), and more like folding a pliable material. Such shifts in feeling for data are very mundane yet crucial to the flow of data.  Plying, in the sense of plying trade, seems much closer to the kinds of work done on data than the figures of data flow or data deluge. 

People reshape data for different ends. Data can be folded together in order to contain it or reduce its dimensionality. At other times, much work goes into expanding the dimensionality of data in order to find ways of eliciting relations that remain somewhat latent or hidden in it. But for the moment, the question for us is what to make of those practices. How does the folding or plying of sheets of data into different shapes and densities matter to us? One of the stakes in following what happens to vectors, lists, matrixes, arrays, dictionaries, sets, dataframes, or series or tuples in data, is to get a sense of how these practices transform a concrete situation into something more abstract. These are the practices of abstraction, and any abstract experience in this domain must process along these lines. More concretely, as we will see in later chapters, the predictive models, the supervised and unsupervised learners, the classifiers, the decision trees and the neural networks in machine learning, to name a few techniques, will be largely implemented as transformations that fold and refold matrices and vectors  along  certain now well-established lines.  

Matrix multiplication loom large in writing about machine learning texts.  In articles and textbooks, matrix manipulations are taken for granted. 
The first full mathematical expression in _The Elements of Statistical Learning_ is shown below, and I quote the surrounding text as well 

    > The linear model has been a mainstay of statistics for the past 30 years and remains one of our most important tools. Given a vector of inputs
    $X^T = (X_1 , X_2, . . . , X_p)$, we predict the output $Y$ via the model
    >$\hat{Y} = \hat{\beta_0}  + \sum^p_(j=1) X_j \hat{\beta_j)$
    >The term $\hat{\beta_0}$ is the intercept, also known as the _bias_ in machine learning [@hastie_elements_2009, 11].

This is such an important expression in machine learning that learning to read it is almost indispensable to thinking about machine learning more generally.  This model is the 'simple but powerful linear model' (11) in which $Y$, the so-called 'response variable' is modelled on some additive combination (the $\sum$ operator means adding them) of input variables $X_j$ and a value of the intercept $\beta_0$.  Even if this does not make much sense, I am hoping that you can see that the linear model relies deeply on the spatial forms of vectors and matrices.  The subscripts $j=1$ refer to individual input variables.   Importantly, as Hastie and co-authors go on to say, 

    > Often it is convenient to include the constant variable 1 in $X$, include $\hat{\beta_0)$ in the vector of coefficients $\hat{\beta}$, and then write the linear model in the vector form as an inner product:
    $\hat{Y} = X^T \hat{\beta}$
    > where $X^T$ denotes vector or matrix transpose [@hastie_elements_2009, 12-13].

We will need further explanation to make more sense of these expressions, but for the moment, the only thing that needs to come across is the heavy reliance on vectors and matrices, which means that entire models can be expressed highly concisely as matrix multiplication operations. In this case, with the convenient convention of making the intercept into a constant input variable, linear models can be written simply as an 'inner product', that is,  as a matrix-matrix multiplication. Learning machine learning, as I say, and learning to implement machine learning techniques, is largely a matter of implementing series of matrix multiplications. As Andrew Ng advises his students,

> Almost any programming language you use will have great linear algebra libraries. And they will be high optimised to do that matrix-matrix multiplication very efficiently including taking advantage of any parallelism your computer. So that you can very efficiently make lots of predictions of lots of hypotheses [Ng, lecture 10.50]

This is one case where implementation does not facilitate learning. Ng advises his learners against implementing their matrix handling code.   In other parts of his teaching, and indeed throughout the practice exercises and assignments, Ng stresses the value of implementing machine learning techniques for both understanding them and using them properly.   They should instead  use the 'great linear algebra libraries' found in 'almost any programming language.' 'Linear algebra libraries' multiply, transpose, decompose, invert and generally transform matrices and vectors. They will be 'highly optimised' not because every programming language has been prepared for the advent of machine learning on a large scale, but rather more likely because matrix operations are just so widely used in image and audio processing.  Happily, Ng observes, that means that 'you can make lots of predictions.'  It seems that generating predictions and hypotheses outweighs the value of understanding how things work on this point. 

How do we 'efficiently make lots of predictions' using matrices? Let's return to the example of house prices mentioned by Norman Nie. In this example, house price is generally treated as the response variable ($Y$), and the number of bedrooms and overall size in square feet are the input variables ($X_1$, $X_2$).  A linear model seeks to find a line (or a plane, or as we will see, a hyperplane) that best fits the points. While I will discuss in the next chapter how such a line can be found, here I want to only show one solution to this problem because it takes the form of a matrix multiplication. As Hastie and co-authors write, the 'unique solution' to the problem of fitting a linear model to a given dataset using the most popular method of 'least squares' [@hastie_elements_2009, 12] is given by:

>$\hat{\beta} = (\mathbf{X}^T\mathbf{X})^-1\mathbf{X}^T\mathbf{y}$

This is quite a tightly coiled expression of how to calculate the parameters $\hat{\beta}$ for a linear model. The two matrices involved are the $X$ input variables (house size and number of bedrooms) and a 1-dimensional matrix $y$, the house price. These matrices are multiplied, transposed (a form of rotation that swaps rows for columns) and inverted (a more complex operation that finds another matrix) Implementing it for  the San Francisco house price data only requires a few extra lines of code:

```{r house_price_predict, echo=TRUE, cache=TRUE, message=FALSE, warning=FALSE, comment=NA, size='smallsize', results='markup' } 
 
    library(MASS)
    houses = read.csv('data/ex1data2.txt', header=FALSE)
    names(houses) = c('size', 'bedrooms', 'price')
    X = as.matrix(cbind(1, houses[,1:2]))
    y = houses$price
    beta_hat = ginv(t(X) %*% X) %*% t(X) %*% y
    
    #making a prediction
    rooms =3
    size = 2000
    predicted_price = beta_hat[1] + beta_hat[2]*size + beta_hat[3]*rooms
    plot(houses$size, houses$price)
    abline(beta_hat[1], beta_hat[2])
```

Given the values of $\hat{\beta}$, we can make predictions of house prices for any given number of bedrooms and house area. For instance, a house with 4 bedrooms and area of 1000 sq. feet should cost around $`'r predicted_price`. As the plot of the house price data shows, albeit without showing all the variables, the line that has been fitted passes through many of the points. This line or surface is the predictive element of the function. We don't need to grasp all the details of the matrix operations working to fit this prediction surface. As in the case of the infrastructural concision shown in the code that fetches data from the World Bank databases, I want to point here to the tightly coiled matrices  that produce predictions.  

The basic question is whether by unwinding some of these operations, we get closer to the vitality of the multiple/multiplying concrete value-situations that connection calculation and feeling.  My feeling is that the praxiographic implementation of the machine learning techniques provides a way begin to name the processes of knowing, predicting and deciding on which many aspects of the turn to data rely.  As the next chapter will suggest, the matrix operations we have just been viewing are themselves organised by other layers of intuition that explore shape, movement and surfaces in much more convoluted forms. 


[^1]: The dimensionality of data is the topic of a later chapter. There I discuss how machine learning re-dimensions data in various ways, sometimes reducing dimensions and at other times, multiplying dimensions.

[^2]: Machine learning in genomics is also the topic of a later chapter. Recent life sciences are not alone in their resort to computational techniques, but in contrast to commercial, industry or government data practices, it is easier to track how data is transformed, reshaped and re-dimensioned in producing biological knowledges. The scientific publications, the public databases and open source software work together to allow this. 

[^3]:In combining Markdown and LaTex, I also make use of the [Pandoc utility](http://johnmacfarlane.net/pandoc/README.html), a command line tool that converts between different document formats.

[^4]: I discuss the competitions and forms of machine learning subjectification in a later chapter. 
    
## References
