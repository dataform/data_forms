<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Machine Learning: Archaeology of a Data Practice</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="Machine Learning: Archaeology of a Data Practice">
  <meta name="generator" content="bookdown 0.3 and GitBook 2.6.7">

  <meta property="og:title" content="Machine Learning: Archaeology of a Data Practice" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Machine Learning: Archaeology of a Data Practice" />
  
  
  

<meta name="author" content="Adrian Mackenzie">


<meta name="date" content="2017-03-25">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="5-vectorisation-and-its-consequences.html">
<link rel="next" href="7-probabilisation-and-the-taming-of-machines.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="1-acknowledgments.html"><a href="1-acknowledgments.html"><i class="fa fa-check"></i><b>1</b> Acknowledgments</a></li>
<li class="chapter" data-level="2" data-path="2-preface.html"><a href="2-preface.html"><i class="fa fa-check"></i><b>2</b> Preface</a></li>
<li class="chapter" data-level="3" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html"><i class="fa fa-check"></i><b>3</b> Introduction: Into the Data</a><ul>
<li class="chapter" data-level="3.1" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#three-accumulations-settings-data-and-devices"><i class="fa fa-check"></i><b>3.1</b> Three accumulations: settings, data and devices</a></li>
<li class="chapter" data-level="3.2" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#who-or-what-is-a-machine-learner"><i class="fa fa-check"></i><b>3.2</b> Who or what is a machine learner?</a></li>
<li class="chapter" data-level="3.3" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#algorithmic-control-to-the-machine-learners"><i class="fa fa-check"></i><b>3.3</b> Algorithmic control to the machine learners?</a></li>
<li class="chapter" data-level="3.4" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#the-archaeology-of-operations"><i class="fa fa-check"></i><b>3.4</b> The archaeology of operations</a></li>
<li class="chapter" data-level="3.5" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#asymmetries-in-common-knowledge"><i class="fa fa-check"></i><b>3.5</b> Asymmetries in common knowledge</a></li>
<li class="chapter" data-level="3.6" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#what-cannot-be-automated"><i class="fa fa-check"></i><b>3.6</b> What cannot be automated?</a></li>
<li class="chapter" data-level="3.7" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#different-fields-in-machine-learning"><i class="fa fa-check"></i><b>3.7</b> Different fields in machine learning?</a></li>
<li class="chapter" data-level="3.8" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#the-diagram-in-critical-thought"><i class="fa fa-check"></i><b>3.8</b> The diagram in critical thought</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html"><i class="fa fa-check"></i><b>4</b> Diagramming machines</a><ul>
<li class="chapter" data-level="4.1" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#we-dont-have-to-write-programs"><i class="fa fa-check"></i><b>4.1</b> ‘We don’t have to write programs’?</a></li>
<li class="chapter" data-level="4.2" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#the-elements-of-machine-learning"><i class="fa fa-check"></i><b>4.2</b> The elements of machine learning</a></li>
<li class="chapter" data-level="4.3" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#who-reads-machine-learning-textbooks"><i class="fa fa-check"></i><b>4.3</b> Who reads machine learning textbooks?</a></li>
<li class="chapter" data-level="4.4" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#r-a-matrix-of-transformations"><i class="fa fa-check"></i><b>4.4</b> <code>R</code>: a matrix of transformations</a></li>
<li class="chapter" data-level="4.5" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#the-obdurate-mathematical-glint-of-machine-learning"><i class="fa fa-check"></i><b>4.5</b> The obdurate mathematical glint of machine learning</a></li>
<li class="chapter" data-level="4.6" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#cs229-2007-returning-again-and-again-to-certain-features"><i class="fa fa-check"></i><b>4.6</b> CS229, 2007: returning again and again to certain features</a></li>
<li class="chapter" data-level="4.7" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#the-visible-learning-of-machine-learning"><i class="fa fa-check"></i><b>4.7</b> The visible learning of machine learning</a></li>
<li class="chapter" data-level="4.8" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#the-diagram-of-an-operational-formation"><i class="fa fa-check"></i><b>4.8</b> The diagram of an operational formation</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html"><i class="fa fa-check"></i><b>5</b> Vectorisation and its consequences</a><ul>
<li class="chapter" data-level="5.1" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html#vector-space-and-geometry"><i class="fa fa-check"></i><b>5.1</b> Vector space and geometry</a></li>
<li class="chapter" data-level="5.2" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html#mixing-places"><i class="fa fa-check"></i><b>5.2</b> Mixing places</a></li>
<li class="chapter" data-level="5.3" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html#truth-is-no-longer-in-the-table"><i class="fa fa-check"></i><b>5.3</b> Truth is no longer in the table?</a></li>
<li class="chapter" data-level="5.4" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html#the-epistopic-fault-line-in-tables"><i class="fa fa-check"></i><b>5.4</b> The epistopic fault line in tables</a></li>
<li class="chapter" data-level="5.5" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html#surface-and-depths-the-problem-of-volume-in-data"><i class="fa fa-check"></i><b>5.5</b> Surface and depths: the problem of volume in data</a></li>
<li class="chapter" data-level="5.6" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html#vector-space-expansion"><i class="fa fa-check"></i><b>5.6</b> Vector space expansion</a></li>
<li class="chapter" data-level="5.7" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html#drawing-lines-in-a-common-space-of-transformation"><i class="fa fa-check"></i><b>5.7</b> Drawing lines in a common space of transformation</a></li>
<li class="chapter" data-level="5.8" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html#implicit-vectorization-in-code-and-infrastructures"><i class="fa fa-check"></i><b>5.8</b> Implicit vectorization in code and infrastructures</a></li>
<li class="chapter" data-level="5.9" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html#lines-traversing-behind-the-light"><i class="fa fa-check"></i><b>5.9</b> Lines traversing behind the light</a></li>
<li class="chapter" data-level="5.10" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html#the-vectorised-table"><i class="fa fa-check"></i><b>5.10</b> The vectorised table?</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html"><i class="fa fa-check"></i><b>6</b> Machines finding functions</a><ul>
<li class="chapter" data-level="6.1" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html#learning-functions"><i class="fa fa-check"></i><b>6.1</b> Learning functions</a></li>
<li class="chapter" data-level="6.2" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html#supervised-unsupervised-reinforcement-learning-and-functions"><i class="fa fa-check"></i><b>6.2</b> Supervised, unsupervised, reinforcement learning and functions</a></li>
<li class="chapter" data-level="6.3" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html#which-function-operates"><i class="fa fa-check"></i><b>6.3</b> Which function operates?</a></li>
<li class="chapter" data-level="6.4" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html#what-does-a-function-learn"><i class="fa fa-check"></i><b>6.4</b> What does a function learn?</a></li>
<li class="chapter" data-level="6.5" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html#observing-with-curves-the-logistic-function"><i class="fa fa-check"></i><b>6.5</b> Observing with curves: the logistic function</a></li>
<li class="chapter" data-level="6.6" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html#the-cost-of-curves-in-machine-learning"><i class="fa fa-check"></i><b>6.6</b> The cost of curves in machine learning</a></li>
<li class="chapter" data-level="6.7" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html#curves-and-the-variation-in-models"><i class="fa fa-check"></i><b>6.7</b> Curves and the variation in models</a></li>
<li class="chapter" data-level="6.8" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html#observing-costs-losses-and-objectives-through-optimisation"><i class="fa fa-check"></i><b>6.8</b> Observing costs, losses and objectives through optimisation</a></li>
<li class="chapter" data-level="6.9" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html#gradients-as-partial-observers"><i class="fa fa-check"></i><b>6.9</b> Gradients as partial observers</a></li>
<li class="chapter" data-level="6.10" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html#the-power-to-learn"><i class="fa fa-check"></i><b>6.10</b> The power to learn</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="7-probabilisation-and-the-taming-of-machines.html"><a href="7-probabilisation-and-the-taming-of-machines.html"><i class="fa fa-check"></i><b>7</b> Probabilisation and the Taming of Machines}</a><ul>
<li class="chapter" data-level="7.1" data-path="7-probabilisation-and-the-taming-of-machines.html"><a href="7-probabilisation-and-the-taming-of-machines.html#data-reduces-uncertainty"><i class="fa fa-check"></i><b>7.1</b> Data reduces uncertainty?</a></li>
<li class="chapter" data-level="7.2" data-path="7-probabilisation-and-the-taming-of-machines.html"><a href="7-probabilisation-and-the-taming-of-machines.html#machine-learning-as-statistics-inside-out"><i class="fa fa-check"></i><b>7.2</b> Machine learning as statistics inside out</a></li>
<li class="chapter" data-level="7.3" data-path="7-probabilisation-and-the-taming-of-machines.html"><a href="7-probabilisation-and-the-taming-of-machines.html#distributed-probabilities"><i class="fa fa-check"></i><b>7.3</b> Distributed probabilities</a></li>
<li class="chapter" data-level="7.4" data-path="7-probabilisation-and-the-taming-of-machines.html"><a href="7-probabilisation-and-the-taming-of-machines.html#naive-bayes-and-the-distribution-of-probabilities"><i class="fa fa-check"></i><b>7.4</b> Naive Bayes and the distribution of probabilities</a></li>
<li class="chapter" data-level="7.5" data-path="7-probabilisation-and-the-taming-of-machines.html"><a href="7-probabilisation-and-the-taming-of-machines.html#spam-when-foralln-is-too-much"><i class="fa fa-check"></i><b>7.5</b> Spam: when <span class="math inline">\(\forall{N}\)</span> is too much?</a></li>
<li class="chapter" data-level="7.6" data-path="7-probabilisation-and-the-taming-of-machines.html"><a href="7-probabilisation-and-the-taming-of-machines.html#the-improbable-success-of-the-naive-bayes-classifier"><i class="fa fa-check"></i><b>7.6</b> The improbable success of the Naive Bayes classifier</a></li>
<li class="chapter" data-level="7.7" data-path="7-probabilisation-and-the-taming-of-machines.html"><a href="7-probabilisation-and-the-taming-of-machines.html#ancestral-probabilities-in-documents-inference-and-prediction"><i class="fa fa-check"></i><b>7.7</b> Ancestral probabilities in documents: inference and prediction</a></li>
<li class="chapter" data-level="7.8" data-path="7-probabilisation-and-the-taming-of-machines.html"><a href="7-probabilisation-and-the-taming-of-machines.html#statistical-decompositions-bias-variance-and-observed-errors"><i class="fa fa-check"></i><b>7.8</b> Statistical decompositions: bias, variance and observed errors</a></li>
<li class="chapter" data-level="7.9" data-path="7-probabilisation-and-the-taming-of-machines.html"><a href="7-probabilisation-and-the-taming-of-machines.html#does-machine-learning-construct-a-new-statistical-reality"><i class="fa fa-check"></i><b>7.9</b> Does machine learning construct a new statistical reality?</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="8-patterns-and-differences.html"><a href="8-patterns-and-differences.html"><i class="fa fa-check"></i><b>8</b> Patterns and differences</a><ul>
<li class="chapter" data-level="8.1" data-path="8-patterns-and-differences.html"><a href="8-patterns-and-differences.html#splitting-and-the-growth-of-trees"><i class="fa fa-check"></i><b>8.1</b> Splitting and the growth of trees</a></li>
<li class="chapter" data-level="8.2" data-path="8-patterns-and-differences.html"><a href="8-patterns-and-differences.html#differences-in-recursive-partitioning"><i class="fa fa-check"></i><b>8.2</b> 1984: Differences in recursive partitioning</a></li>
<li class="chapter" data-level="8.3" data-path="8-patterns-and-differences.html"><a href="8-patterns-and-differences.html#limiting-differences"><i class="fa fa-check"></i><b>8.3</b> Limiting differences</a></li>
<li class="chapter" data-level="8.4" data-path="8-patterns-and-differences.html"><a href="8-patterns-and-differences.html#the-successful-dispersion-of-the-support-vector-machine"><i class="fa fa-check"></i><b>8.4</b> The successful dispersion of the support vector machine</a></li>
<li class="chapter" data-level="8.5" data-path="8-patterns-and-differences.html"><a href="8-patterns-and-differences.html#differences-blur"><i class="fa fa-check"></i><b>8.5</b> Differences blur?</a></li>
<li class="chapter" data-level="8.6" data-path="8-patterns-and-differences.html"><a href="8-patterns-and-differences.html#bending-the-decision-boundary"><i class="fa fa-check"></i><b>8.6</b> Bending the decision boundary</a></li>
<li class="chapter" data-level="8.7" data-path="8-patterns-and-differences.html"><a href="8-patterns-and-differences.html#instituting-patterns"><i class="fa fa-check"></i><b>8.7</b> Instituting patterns</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="9-regularizing-and-materializing-objects.html"><a href="9-regularizing-and-materializing-objects.html"><i class="fa fa-check"></i><b>9</b> Regularizing and materializing objects}</a><ul>
<li class="chapter" data-level="9.1" data-path="9-regularizing-and-materializing-objects.html"><a href="9-regularizing-and-materializing-objects.html#genomic-referentiality-and-materiality"><i class="fa fa-check"></i><b>9.1</b> Genomic referentiality and materiality</a></li>
<li class="chapter" data-level="9.2" data-path="9-regularizing-and-materializing-objects.html"><a href="9-regularizing-and-materializing-objects.html#the-genome-as-threshold-object"><i class="fa fa-check"></i><b>9.2</b> The genome as threshold object</a></li>
<li class="chapter" data-level="9.3" data-path="9-regularizing-and-materializing-objects.html"><a href="9-regularizing-and-materializing-objects.html#genomic-knowledges-and-their-datasets"><i class="fa fa-check"></i><b>9.3</b> Genomic knowledges and their datasets</a></li>
<li class="chapter" data-level="9.4" data-path="9-regularizing-and-materializing-objects.html"><a href="9-regularizing-and-materializing-objects.html#the-advent-of-wide-dirty-and-mixed-data"><i class="fa fa-check"></i><b>9.4</b> The advent of ‘wide, dirty and mixed’ data</a></li>
<li class="chapter" data-level="9.5" data-path="9-regularizing-and-materializing-objects.html"><a href="9-regularizing-and-materializing-objects.html#cross-validating-machine-learning-in-genomics"><i class="fa fa-check"></i><b>9.5</b> Cross-validating machine learning in genomics</a></li>
<li class="chapter" data-level="9.6" data-path="9-regularizing-and-materializing-objects.html"><a href="9-regularizing-and-materializing-objects.html#proliferation-of-discoveries"><i class="fa fa-check"></i><b>9.6</b> Proliferation of discoveries</a></li>
<li class="chapter" data-level="9.7" data-path="9-regularizing-and-materializing-objects.html"><a href="9-regularizing-and-materializing-objects.html#variations-in-the-object-or-in-the-machine-learner"><i class="fa fa-check"></i><b>9.7</b> Variations in the object or in the machine learner?</a></li>
<li class="chapter" data-level="9.8" data-path="9-regularizing-and-materializing-objects.html"><a href="9-regularizing-and-materializing-objects.html#whole-genome-functions"><i class="fa fa-check"></i><b>9.8</b> Whole genome functions</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="10-propagating-subject-positions.html"><a href="10-propagating-subject-positions.html"><i class="fa fa-check"></i><b>10</b> Propagating subject positions}</a><ul>
<li class="chapter" data-level="10.1" data-path="10-propagating-subject-positions.html"><a href="10-propagating-subject-positions.html#propagation-across-human-machine-boundaries"><i class="fa fa-check"></i><b>10.1</b> Propagation across human-machine boundaries</a></li>
<li class="chapter" data-level="10.2" data-path="10-propagating-subject-positions.html"><a href="10-propagating-subject-positions.html#competitive-positioning"><i class="fa fa-check"></i><b>10.2</b> Competitive positioning</a></li>
<li class="chapter" data-level="10.3" data-path="10-propagating-subject-positions.html"><a href="10-propagating-subject-positions.html#a-privileged-machine-and-its-diagrammatic-forms"><i class="fa fa-check"></i><b>10.3</b> A privileged machine and its diagrammatic forms</a></li>
<li class="chapter" data-level="10.4" data-path="10-propagating-subject-positions.html"><a href="10-propagating-subject-positions.html#varying-subject-positions-in-code"><i class="fa fa-check"></i><b>10.4</b> Varying subject positions in code</a></li>
<li class="chapter" data-level="10.5" data-path="10-propagating-subject-positions.html"><a href="10-propagating-subject-positions.html#the-subjects-of-a-hidden-operation"><i class="fa fa-check"></i><b>10.5</b> The subjects of a hidden operation</a></li>
<li class="chapter" data-level="10.6" data-path="10-propagating-subject-positions.html"><a href="10-propagating-subject-positions.html#algorithms-that-propagate-errors"><i class="fa fa-check"></i><b>10.6</b> Algorithms that propagate errors</a></li>
<li class="chapter" data-level="10.7" data-path="10-propagating-subject-positions.html"><a href="10-propagating-subject-positions.html#competitions-as-examination"><i class="fa fa-check"></i><b>10.7</b> Competitions as examination</a></li>
<li class="chapter" data-level="10.8" data-path="10-propagating-subject-positions.html"><a href="10-propagating-subject-positions.html#superimposing-power-and-knowledge"><i class="fa fa-check"></i><b>10.8</b> Superimposing power and knowledge</a></li>
<li class="chapter" data-level="10.9" data-path="10-propagating-subject-positions.html"><a href="10-propagating-subject-positions.html#ranked-subject-positions"><i class="fa fa-check"></i><b>10.9</b> Ranked subject positions</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="11-conclusion-out-of-the-data.html"><a href="11-conclusion-out-of-the-data.html"><i class="fa fa-check"></i><b>11</b> Conclusion: Out of the Data}</a><ul>
<li class="chapter" data-level="11.1" data-path="11-conclusion-out-of-the-data.html"><a href="11-conclusion-out-of-the-data.html#machine-learners"><i class="fa fa-check"></i><b>11.1</b> 250,000 machine learners</a></li>
<li class="chapter" data-level="11.2" data-path="11-conclusion-out-of-the-data.html"><a href="11-conclusion-out-of-the-data.html#a-summary-of-the-argument"><i class="fa fa-check"></i><b>11.2</b> A summary of the argument</a></li>
<li class="chapter" data-level="11.3" data-path="11-conclusion-out-of-the-data.html"><a href="11-conclusion-out-of-the-data.html#in-situ-hybridization"><i class="fa fa-check"></i><b>11.3</b> In-situ hybridization</a></li>
<li class="chapter" data-level="11.4" data-path="11-conclusion-out-of-the-data.html"><a href="11-conclusion-out-of-the-data.html#critical-operational-practice"><i class="fa fa-check"></i><b>11.4</b> Critical operational practice?</a></li>
<li class="chapter" data-level="11.5" data-path="11-conclusion-out-of-the-data.html"><a href="11-conclusion-out-of-the-data.html#obstacles-to-the-work-of-freeing-machine-learning"><i class="fa fa-check"></i><b>11.5</b> Obstacles to the work of freeing machine learning</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="12-references.html"><a href="12-references.html"><i class="fa fa-check"></i><b>12</b> References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning: Archaeology of a Data Practice</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="machines-finding-functions" class="section level1">
<h1><span class="header-section-number">6</span> Machines finding functions</h1>
<p></p>
<blockquote>
<p>Because of a gradient that no doubt characterizes our cultures, discursive formations are constantly becoming epistemologized <span class="citation">(Foucault <a href="#ref-Foucault_1972">1972</a>, 195)</span> </p>
</blockquote>
<p>‘All knowledge,’ hypothesises Pedro Domingos, ‘past, present,and future can be derived from data by a single, universal learning algorithm’ <span class="citation">(Domingos <a href="#ref-Domingos_2015a">2015</a>, 25)</span>.   How will the ‘single, universal’ algorithm learn, how will it ‘epistemologize,’ to use Foucault’s term, ‘our cultures’?</p>
<p>In practice, the opening pages of machine learning textbooks often warn or enthuse about the profusion of techniques, algorithms, tools and machines.  ‘The first problem facing you’, cautions Domingos readers of the <em>Communications of the ACM</em>, ‘is the bewildering variety of learning algorithms available. Which one to use? There are literally thousands available, and hundreds more are published each year <span class="citation">(Domingos <a href="#ref-Domingos_2012">2012</a>, 1)</span>.  ’The literature on machine learning is vast, as is the overlap with the relevant areas of statistics and engineering’ echoes David Barber in <em>Bayesian Reasoning and Machine Learning</em><span class="citation">(Barber <a href="#ref-Barber_2011">2011</a>, 4)</span>; ‘statistical learning refers to a vast set of tools for understanding data’ writes James and co-authors in an <em>Introduction to Statistical Learning with R</em> <span class="citation">(James et al. <a href="#ref-James_2013">2013</a>, 1)</span>; or writing in in <em>Statistical Learning for Biomedical Data</em> the biostatisticians James Malley, Karen Malley and Sinisa Pajevic ‘freely admit that many machines studied in this text are somewhat mysterious, though powerful engines’ <span class="citation">(Malley, Malley, and Pajevic <a href="#ref-Malley_2011">2011</a>, 257)</span>. In <em>Thoughtful Machine Learning</em> Matthew Kirk exacerbates the situation: ‘flexibility is also what makes machine learning daunting. It can solve many problems, but how do we know whether we’re solving the right problem, or actually solving it in the first place?’ <span class="citation">(Kirk <a href="#ref-Kirk_2014">2014</a>, ix)</span>.  The prefatory comments from Domingos, Barber, James, Malley and Kirk suggest a rampant even weed-like abundance of machine learners, as does the 700 or so pages of <em>Elements of Statistical Learning</em>. Much learning of machine learning work, at least for machine learners, concerns not so much implementation of particular techniques (neural network, decision tree, support vector machine, logistic regression, etc.), but rather navigating the maze of methods and variations that might be relevant to a particular situation.  How does this dual effect of profuse accumulation and the ideal a single, universal machine learner arise and hold together?</p>
<p>The machine learners I have just cited present that profusion as a problem of the piling up of techniques. As the authors of textbooks and how-to-manuals, they attempt to manage it by providing, indexes, maps and guides to the bewildering variety of machine learners. <em>Elements of Statistical Learning</em> deploys tables, overviews, theories of statistical modelling, model assessment and comparison techniques to aid in navigating them.</p>
<div class="figure"><span id="fig:mapping-functions"></span>
<img src="figure/ml_map_scikit.png" alt="`scikit-learn` map of machine learners" width="1381" />
<p class="caption">
Figure 6.1: <code>scikit-learn</code> map of machine learners
</p>
</div>
<p></p>
<p>Parallel and complementary mappings accompany software libraries. The visual map of machine learning techniques shown in Figure <a href="6-machines-finding-functions.html#fig:mapping-functions">6.1</a> comes from a machine learning library written in <code>Python,</code> <code>scikit-learn</code> <span class="citation">(Pedregosa et al. <a href="#ref-Pedregosa_2011">2011</a>)</span>.  This software library is widely used in industry, research and commerce. In contrast to the pedagogical expositions, theoretical accounts or guides to reference implementation, or the many overlapping packages in <code>R</code>, code libraries such as <code>scikit-learn</code> order the range of techniques by offering recipes and maps for the use of the <em>functions</em> the libraries supply.  The branches in the figure lay down paths through the profusion of techniques as a decision tree.<a href="#fn33" class="footnoteRef" id="fnref33"><sup>33</sup></a>  </p>
<p>The architecture of software libraries itself classifies and orders machine learners. <code>Scikit-learn</code> for instance comprises a number of sub-packages. Modules such as <code>lda</code> (linear discriminant analysis), <code>svm</code> (support vector machine) or <code>neighbors</code> (<em>k</em> nearest neighbours) point to well-known machine learners, whilst <code>cross-validation</code> or <code>feature_selection</code> refer to ways of testing models or transforming data respectively. These divisions, maps and classifications help order the techniques, but they obscure the process that first generates a competing profusion of machine learners.</p>
<p>If, as I have suggested earlier, we understand knowledge in terms of the radically re-conceptualised statements that Foucault described in <em>The Archaeology of Knowledge</em>, then statements comprise various units (sentences, series, tables, propositions, diagrams, equations, numbers) mapped to a field of objects, subject positions and domains of coordinations and reuse by an enunciative function <span class="citation">(Foucault <a href="#ref-Foucault_1972">1972</a>, 106)</span>. Confronted by a profusion of machine learners and the idea of a single, universal machine learning, an archaeological analysis attends to the enunciative function that multiplies meanings and operations.</p>
<p>We might understand the enunciative function  as the generative process that proliferates machine learners. The listing and mapping of accumulated techniques, whether in the form of textbooks such as <em>Elements of Statistical Learning</em> or a code library such as <code>scikit-learn</code>, together with the many attempts to unify them (Domingo’s ‘single, universal algorithm’, <code>scikit-learn</code>‘s map, <em>Elements of Statistical Learning</em>’s statistical theory) suggests a commonality in the production of statements. As I will argue in this chapter, there are so many techniques, algorithms and ways of deriving knowledge from data in machine learning because statements are actually rare in this operational formation. ’Because statements are rare,’ writes Foucault, ‘they are collected in unifying totalities, and the meanings to be found in them are multiplied’ <span class="citation">(Foucault <a href="#ref-Foucault_1972">1972</a>, 120)</span>. </p>
<div id="learning-functions" class="section level2">
<h2><span class="header-section-number">6.1</span> Learning functions</h2>

<p>The rarity of statements amidst the profusion of machine learners revolve around a single operator, the .<a href="#fn34" class="footnoteRef" id="fnref34"><sup>34</sup></a> Table @ref(tab:function_in_ml) shows the titles and author-supplied keywords of a sample of well-cited machine learning publications. In these randomly chosen publications, mathematical functions – ‘kernel function,’ ‘discriminant function,’ ‘radial basis function’ – mingle with biological and engineering functions – ‘protein-binding function,’ ‘intestinal motor function’, or ‘rules to control locomotion.’ Mathematical functions, however, dominate. Machine learners ‘find’, ‘estimate,’ ‘approximate,’ ‘analyse’ and sometimes ‘decompose’ mathematical functions. The primary mathematical sense of a function refers to a relation – a mapping – between sets of values or variables. (A variable is a symbol that can stand for a set of numbers or other values.) A function is one-to-one relation between two sets of values. For instance, a It maps a set of arguments (inputs) to a set of values (outputs, or to use slightly more technical language, it maps between a <em>domain</em> and a <em>co-domain</em>.) As we have already seen, mathematical functions are often written in formulae of varying degrees of complexity. They are of various genres, provenances, textures and shapes: polynomial functions, trigonometric functions, exponential functions, differential equations, series functions, algebraic or topological functions, etc. Various fields of mathematics have pursued the invention of functions. In machine learning and information retrieval, important functions include the logistic function (discussed below), probability density functions (PDF) for different probability distributions (Gaussian, Bernoulli, Binomial, Beta, Gamma, etc. See chapter <a href="#ch:probability"><strong>??</strong></a>),   and error, cost, loss or objective functions (these four are almost synonymous).  (The latter group I discuss below because they underpin many claims that machine learners learn.) </p>
<p>As we will see, from the perspective of the function, machine learning can be understood as a function-finding operation.  Implicitly or explicitly, machine learners find a mathematical expression – a function – approximating the social, technical, financial, transactional, biological, brain, heart or group process that flowed the data in question into vector space. Regardless of the application, no single mathematical function perfectly or uniques expresses data. Many if not infinite functions can approximate any given data. Even if there was a master algorithm, therefore, it would be concerned with a field of functions, and it would entail observation, classification and selection (finding, in short) in deriving knowledge from data. </p>
</div>
<div id="supervised-unsupervised-reinforcement-learning-and-functions" class="section level2">
<h2><span class="header-section-number">6.2</span> Supervised, unsupervised, reinforcement learning and functions</h2>
<p>The capacity of machine learners to learn is very closely linked to forms of observation that co-produce knowledge. The optics of this observation of machine learners vary but they are always partial or incomplete, partly because of the dimensionality of vector space and partly because of the domains in which machine learning operates. While the field is pragmatic in its commitment to classification and prediction (although in certain ways, curiously idealistic too in its constant reuse of well-worked datasets such as <code>iris</code> or <code>South African heart disease</code>), it distinguishes between three broadly different kinds of <em>learning</em> – supervised, unsupervised, and reinforcement – in terms of their observability.  <em>Elements of Statistical Learning</em> presents the distinction between supervised and unsupervised learning:</p>
<blockquote>
<p>With supervised learning there is a clear measure of success or lack thereof, that can be used to judge adequacy in particular situations and to compare the effectiveness of different methods over various situations. Lack of success is directly measured by expected loss over the joint distribution <span class="math inline">\(Pr(X,Y)\)</span>. This can be estimated in a variety of ways including cross-validation. In the context of unsupervised learning, there is no such direct measure of success. … This uncomfortable situation has led to heavy proliferation of proposed methods, since effectiveness is a matter of opinion and cannot be verified directly. <span class="citation">(Hastie, Tibshirani, and Friedman <a href="#ref-Hastie_2009">2009</a>, 486–7)</span></p>
</blockquote>
<p>Supervised learning in general terms constructs a model by training it on some sample data (the training data  ), and then evaluating the model’s effectiveness in classifying or predicting unseen test data  whose actual values are already known. The ‘clear measure of success’ in relation to in so-called ‘supervised learning’ is of relatively recent date.<a href="#fn35" class="footnoteRef" id="fnref35"><sup>35</sup></a>  Unsupervised machine learning techniques generally look for patterns in the data without any training or testing phases (for instance, <em>k</em>-means or principal component analysis do this, and both techniques have been heavily used for more than fifty years). In both supervised and unsupervised learning, machine learners observe how a function (or functions) changes as a model transforms, partitions or maps the data.  </p>
<p>Viewed as enunciative function, machine learning makes statements through operations that treat functions as .  At the same time, opacity – ‘no direct measure of success’ – is generative in machine learning.  <em>Elements of Statistical Learning</em> admits that success cannot be measured and that this inaccessible difficulty has led to proliferating methods, transformations and changes. If, as the first part of the quoted text puts it, supervised learning has a clear ‘measure of success,’ that success only seems to encourage further variations and comparisons that end up proliferating machine learners, their publications and their software implementations. </p>
</div>
<div id="which-function-operates" class="section level2">
<h2><span class="header-section-number">6.3</span> Which function operates?</h2>
<p> Differences between machine learners can be described using mathematical functions. That is, machine learners operate as functions and observations of those operations also constitute functions. Functions instantiate <em>both</em> the operations and the ordering of those operations.</p>
<p>For instance, classifiers, or machine learners that classify, are often identified directly with functions:</p>
<blockquote>
<p>A classifier … is a function <span class="math inline">\(d(\mathbf{x})\)</span> defined on <span class="math inline">\(\mathcal{X}\)</span> so that for every <span class="math inline">\(\mathbf{x}\)</span>, <span class="math inline">\(d(\mathbf{x})\)</span> is equal to one of the numbers <span class="math inline">\(1, 2, ..., J\)</span> <span class="citation">(Breiman et al. <a href="#ref-Breiman_1984">1984</a>, 156:4)</span>  </p>
</blockquote>
<p>Writing in the 1980s, the statistician Leo Breiman identifies classifiers – perhaps the key operational achievement of machine learning and certainly the catalyst of many applications – with functions. A classifier <em>is</em> a function <span class="math inline">\(d(\mathbf{x})\)</span> where is <span class="math inline">\(\mathbf{x}\)</span> is the data and <span class="math inline">\(d\)</span> ranges over numbers that map onto categories, rankings or or other forms of order and belonging (for instance, <code>cat</code> or <code>not cat</code> in the case of <code>kittydar</code>).</p>
<p>The identification of machine learning with functions appears in the first pages of most machine learning textbooks. Viewed operationally, learning in machine learning means finding a function that can identify or predict patterns in the data. As <em>Elements of Statistical Learning</em> formulates it,</p>
<blockquote>
<p>our goal is to find a useful approximation <span class="math inline">\(\hat{f}(x)\)</span> to the function <span class="math inline">\(f(x)\)</span> that underlies the predictive relationship between input and output <span class="citation">(Hastie, Tibshirani, and Friedman <a href="#ref-Hastie_2009">2009</a>, 28)</span>. </p>
</blockquote>
<p>This statement of learning compresses several layers in the function. It posits the existence of <em>the</em> function that generated the data as a foundation. This function figures as a ground truth existentially imputed to the world. It also refers to ‘finding … <span class="math inline">\(\hat{f}(x)\)</span>’, where the ‘^’ indicates a ‘useful’ approximation. A leading theorist of learning theory Vladimir Vapnik echoes the statement of learning as a function: ‘learning is a problem of <em>function estimation</em> on the basis of empirical data’ <span class="citation">(Vapnik <a href="#ref-Vapnik_1999">1999</a>, 291)</span>.<a href="#fn36" class="footnoteRef" id="fnref36"><sup>36</sup></a>  The use of the term ‘learning’ in machine learning displays affiliations to the field of artificial intelligence, but the ‘function-fitting paradigm’ as <span class="citation">(Hastie, Tibshirani, and Friedman <a href="#ref-Hastie_2009">2009</a>, 29)</span> terms it, emphasises this double layering of function as an observed approximation. Most importantly, <em>learning</em> here is understood as finding. Despite many differences in the framing of the techniques, all accounts of machine learning, even those such as <em>Machine Learning for Hackers</em> <span class="citation">(Conway and White <a href="#ref-Conway_2012">2012</a>)</span> that eschew any explicit recourse to mathematical formula, rely on the formalism and modes of thought associated with mathematical functions. Whether they are seen as forms of artificial intelligence or statistical models, the formalisms are directed to build ‘a good and useful approximation to the desired output’ <span class="citation">(Alpaydin <a href="#ref-Alpaydin_2010">2010</a>, 41)</span>, or, put more statistically, ‘to use the sample to find the function from the set of admissable functions that minimizes the probability of error’ <span class="citation">(Vapnik <a href="#ref-Vapnik_1999">1999</a>, 31)</span>. </p>
<p>The superimposed or doubling of function as operation and observer is hardly ever explicitly mentioned by machine learners.  The pages of <span class="citation">(Hastie, Tibshirani, and Friedman <a href="#ref-Hastie_2009">2009</a>)</span> present a series of ‘functions’: quadratic function, likelihood function, sigmoid function, loss function, regression function, basis function, activation function, penalty functions, additive functions, kernel functions,step function, error function, constraint function, discriminant function, probability density function, weight function, coordinate function, neighborhood function, and the list goes on. This mixed list draws from a pool of several hundred mathematical functions commonly used in science and engineering.<a href="#fn37" class="footnoteRef" id="fnref37"><sup>37</sup></a> Clearly neither machine learners or critical researchers can expect to understand the functioning of all these functions in any great detail. While this prickly list of terms begins confirms the salience of functions in machine learning (as perhaps in many other science and engineering disciplines), certain basic difference between functions might be a way to map the interplay of operational and observational functions. We can already see in this list that function are diverse. Sometimes, the function refers to a mathematical form – ‘quadratic,’ ‘coordinate’, ‘basis’ or ‘kernel’; sometimes it refers to statistical considerations – ‘likelihood’, ‘regression’, ‘error,’ or ‘probability density’; and sometimes it refers to some other concern that might relate to a particular modelling device or diagram – ‘activation,’ ‘weight’, ‘loss,’ ‘constraint,’ or ‘discriminant.’</p>
</div>
<div id="what-does-a-function-learn" class="section level2">
<h2><span class="header-section-number">6.4</span> What does a function learn?</h2>
<p>We wish to know: in what sense does a machine learner learn? This question can now be re-framed: how to machine learners find functions? For critical thought, this is a vexing question, for if function-finding agency inheres in machines and devices, then the politics of human-machine relations, and the practices of knowledge production shift.  The philosopher of science Isabelle Stengers sets tight limits on functions:  </p>
<blockquote>
<p>No function can deal with learning, producing, or empowering new habits, as all require and achieve the production of different worlds, non-consensual worlds, actively diverging worlds <span class="citation">(Stengers <a href="#ref-Stengers_2005">2005</a>, 162)</span></p>
</blockquote>
<p>If they cannot learn ‘new habits,’ what can functions learn? In some ways, Stengers would, on this reading, be taking a fairly conventional position on mathematical functions. They cannot learn or produce anything, only reproduce patterns implicit in their structure. Similar statements might be found in many philosophical writings on science and on mathematics in particular.<a href="#fn38" class="footnoteRef" id="fnref38"><sup>38</sup></a> But throughout in her writing Stengers explicitly affirms <em>experimental practice</em>, much of which depends on functions and their operations <span class="citation">(Stengers <a href="#ref-Stengers_2008">2008</a>)</span>. It might be better to say that she limits the agency of functions in isolation in order to highlight their specific power in science: ‘celebrating the exceptional character of the experimental achievement very effectively limits the claims made in the name of science’ <span class="citation">(Stengers <a href="#ref-Stengers_2011">2011</a>, 376)</span>. (Limiting claims made for science might save it from being totally re-purposed as a techno-economic innovation system. ) </p>
<p>The connection between a given function and a given concrete experimental situation is highly contingent or indeed singular. Stengers argues that mathematical functions impinge on matters of fact via experimentally constructed relays:</p>
<blockquote>
<p>The reference of a mathematical function to an experimental matter of fact is neither some kind of right belonging to scientific reason nor is it an enigma, but actually the very meaning of an experimental achievement <span class="citation">(Stengers <a href="#ref-Stengers_2005">2005</a>, 157)</span>. </p>
</blockquote>
<p>The generic term ‘reference’ here harbours a multitude of relations. The experimental achievement, the distinctive power of science, works through a tissue of relations that connect people, things, devices, facts (statements) and mathematical functions in a heterogeneous weave.<a href="#fn39" class="footnoteRef" id="fnref39"><sup>39</sup></a> Given that learning is not radically innate to machines, it might better be understood as an experimental achievement. When a biomedical researcher uses seeks to ‘estimate the probability that a critically-ill lupus patient will not survive the first 72 hours of an initial emergency hospital visit’ <span class="citation">(Malley, Malley, and Pajevic <a href="#ref-Malley_2011">2011</a>, 5)</span>, they might estimate and evaluate their predictions using classical statistical approaches (analysis of variance, correlations, regression analysis, etc.). The question from Stengers’ standpoint is this: what happens to the structure of referrals through experiments and the existing knowledge when functions are said to learn?  In order to address this question, we need to delineate how functions function in machine learning. </p>
<p>At first glance, machine learning as a field is not very experimental (even if it radically influences the conduct of experiments in many scientific fields; see chapter <a href="#ch:genome"><strong>??</strong></a>). It lacks the apparatus, the instruments, the laboratories, field sites or clinics of experimental practice. Experimentation takes place principally in the form of rendering diagrammatically the relays or referrals between different functions as they traverse data.  They appear in graphic forms as plots. The diagrammatic entanglement of operation and observation in functions is not surprising. The historical invention of the term ‘function’ and a notation for writing functions by the philosopher G.W. Leibniz in the 17th century pertained to the problem of describing continuous variations in curves.  Functions for Leibniz describe variations in response to other changes (<span class="math inline">\(y\)</span> may change in response to a change in <span class="math inline">\(x\)</span>), but they can also describe tendencies in functions (as a derivative function  describes the sensitivity or rate of change of the slope of curve). Identifying and locating important tendencies or changes in functions – <em>singularities</em> in curves also preoccupies the function-finding done in machine learning. In contrast to the vector space that expands to accommodate all transformations, the many observational elements such as graphic objects stage observations of tendencies or change points. The experimental relay or referral, the power to confer on things the power to confer on the machine learner (human-machine) the power to speaker in their name <span class="citation">(Stengers <a href="#ref-Stengers_2000">2000</a>, 89)</span>,  pivots around the double layer of functions. An operational function transforms the vector space and an observational function generates statements concerning degrees and rates of success, fit or error. </p>
<p>While we have yet to see how a function can observe, we can readily see some of the effect of the coupling between operational and observational functions. In the several hundred colour graphic plots in <em>Elements of Statistical Learning</em>, a striking mixture of network diagrams, scatterplots, barcharts, histograms, heatmaps, boxplots, maps, contour plots, dendrograms and 3D plots exhibit different aspects of this tension between operation and observation. Many of these graphic forms are common in statistics as statements of variation or tendency in data (histograms and boxplots). Others relate specifically to machine learning (for instance, ROC – Receiver Operating Curve – or regularization path plots). A significant proportion of these graphics do not display data from experiments or measurements, but diagram variations in the operational function that transforms the data in relation to some criteria of observations (for instance, prediction errors or purity of classification). </p>
</div>
<div id="observing-with-curves-the-logistic-function" class="section level2">
<h2><span class="header-section-number">6.5</span> Observing with curves: the logistic function</h2>
<p>How can a function observe? As we have already seen, machine learners often learn by ‘fitting’, as well as ‘over-fitting’ and ‘under-fitting,’ <em>Fitting</em> is a way of bringing functions into the data. As we saw in the previous chapter, the vector space cannot be fully seen and its operational transformation into lines, planes, or smooth surfaces often remain occluded.  Graphic plots and statistical summaries offer perspectival views on those transformations, but machine learners observe many of those transformations by adding a feedback loop between the transformations (fitting a line, building a decision tree, adjusting the weights in a neural net, etc.) and observed outcome.</p>
<p>Take the example of <em>sigmoid</em> functions.  These quite simple functions underpin many classifiers and animate many of the operations of neural network, including their recent re-incarnations in ‘deep learning’ <span class="citation">(Hinton and Salakhutdinov <a href="#ref-Hinton_2006">2006</a>; Mohamed et al. <a href="#ref-Mohamed_2011">2011</a>)</span>.   As operational functions in machine learning, they illustrate a transformation of discrete values into continuous values. As observational functions, they exemplify observability, as we will see, in the form of their differentiability.  An example of a sigmoid function, the logistic function, can be written as:</p>

<div class="figure"><span id="fig:logistic-curve"></span>
<img src="_main_files/figure-html/logistic-curve-1.png" alt="Logistic or sigmoid function for 2 different values of the parameter $k$" width="1152" />
<p class="caption">
Figure 6.2: Logistic or sigmoid function for 2 different values of the parameter <span class="math inline">\(k\)</span>
</p>
</div>
<p>The logistic function (shown as Equation  and as two curves in Figure <a href="6-machines-finding-functions.html#fig:logistic-curve">6.2</a>), as we will see, is very important in many classification and decision settings partly due to the <em>non-linear</em> shape that constrains vertical movement within the values (0 to 1), and partly because of the range of shapes opened up by the parameter <span class="math inline">\(K\)</span>. How does a function such as the sigmoid function ‘observe’ anything? Here the curve itself and even the name ‘sigmoid’ is the best guide. The S-shape of the sigmoid curve is a good guide to operations associated with curves. The logistic function has quite a long history in statistics since that curve diagrams growth and change in various ways.  (As the historian of statistics J.S. Cramer writes: ‘The logistic function was invented in the nineteenth century for the description of the growth of organisms and populations and for the course of autocatalytic chemical reactions’ <span class="citation">(Cramer <a href="#ref-Cramer_2004">2004</a>, 614)</span>.<a href="#fn40" class="footnoteRef" id="fnref40"><sup>40</sup></a>  In nearly all of these cases, the function was used to fit a curve to data on the growth of something: populations, reactions, tumours, tadpoles tails, oats and embryos. The reference of the curve to growth comes from its changing slope. Growth starts slowly, increases rapidly and then slows down again as it reaches a limit. In the second half of the twentieth century, it was widely used in economics. In all these settings and usages, the curve was a way of describing and predicting growth in populations.  Census data, clinical or laboratory measurements supplied the actual values of <span class="math inline">\(f(x)\)</span> at particular times, the <span class="math inline">\(x\)</span> values. The task of the demographer, physiologist or economist was to calculate out the values of parameters such as <span class="math inline">\(k\)</span> that controlled the shape of the curve.</p>
<p>Historically, then, the logistic function has a well-established biopolitical resonance.   But note that the curves showing in Figure @ref(fig:logistic_curve) plot the same data (<span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(y\)</span> values), but differ in their curvature. This diagrammatic variation derives from the parameter <span class="math inline">\(k\)</span>, which discreetly appears in the equation  next to <span class="math inline">\(x\)</span>. Such parameters are vital control points in function fitting and any learning associated with that. Varying these parameters and optimising their values is the basis of ‘useful approximation’ in machine learning.</p>
<p>Sometimes these parameters can be varied so much as to suggest entirely different functions.  In @ref(fig:logistic_curve) for instance, <span class="math inline">\(k=12\)</span> produces a much sharper curve, a curve that actually looks more like a qualitative change, range than a smooth transition from <span class="math inline">\(0\)</span> to <span class="math inline">\(1\)</span>. The sharp shape of the logistic curve when the scaling parameter <span class="math inline">\(k\)</span> is larger transforms the function into a classifier, into a function that, as Breiman puts it, is equal to one of the numbers <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span>. In this setting, the function <span class="math inline">\(f(x) = 1/(1+e^{-x})\)</span> maps continuously varying numbers (the <span class="math inline">\(x\)</span> values) onto a domain of discrete values. Because <span class="math inline">\(f(x)\)</span> tends very quickly to converge on values of <span class="math inline">\(1\)</span> or <span class="math inline">\(0\)</span>, it can be coded as ‘yes’/‘no’; ‘survived/deceased’, or any other binary difference.  The mapping between the <span class="math inline">\(x\)</span> values sliding continuously and the binary difference pivots on the combination of the exponential function (<span class="math inline">\(e^{-x}\)</span>), which rapidly tends towards zero as <span class="math inline">\(x\)</span> increases and rapidly tends towards <span class="math inline">\(\infty\)</span> as <span class="math inline">\(x\)</span> decreases, and the dividend <span class="math inline">\(\sfrac{1}{1+ ...}\)</span>, which converts high value denominators to almost zero and low value denominators to one. This mapping between variations in <span class="math inline">\(x\)</span> and the value of the function <span class="math inline">\(f(x)\)</span> is mathematically elementary, but typical of the relaying of references that allows functions to intersect with and constitute matters of fact and states of affairs such as <code>cat</code> and <code>not-cat</code>. This realisation – that a continuously varying sigmoid function can map discrete outcomes – forms the basis of many machine learning classifiers. </p>
</div>
<div id="the-cost-of-curves-in-machine-learning" class="section level2">
<h2><span class="header-section-number">6.6</span> The cost of curves in machine learning</h2>

<p>I have been suggesting that experimentality in machine learning consists in coupling operational and observational functions. If operational functions move through or transform the data, observational functions render the effects of those transformations visible. How does this take place practically? The logistic function appears frequently in machine learning literature, prominently as part of perhaps the most vernacular machine learner, the logistic regression model (see table @ref(tab:logistic_in_ml) for a sample of well-cited publications).  Descriptions of logistic regression models appear in nearly all machine learning tutorials, textbooks and training courses (see Chapter 4 in <span class="citation">(Hastie, Tibshirani, and Friedman <a href="#ref-Hastie_2009">2009</a>)</span>).  In biomedical research, ‘logistic regression is the default “simple” model for predicting a subject’s group status’ <span class="citation">(Malley, Malley, and Pajevic <a href="#ref-Malley_2011">2011</a>, 43)</span>. As Malley et.al. suggest, ‘it can be applied after a more complex learning machine has done the heavy lifting of identifying an important set of predictors given a very large list of candidate predictors’ (43). Especially in comparison to more complicated models, logistic regression models are relatively easy to interpret because they are superimpose the logistic function on the linear model that we have been discussing already (see figure @ref(fig:linreg_classifier_hastie) and also chapters <a href="#ch:diagram"><strong>??</strong></a> and ). As <em>Elements of Statistical Learning</em> puts it: ‘the logistic regression model arises from the desire to model the posterior probabilities of the <span class="math inline">\(K\)</span> classes via linear functions in <span class="math inline">\(x\)</span>, while at the same time ensuring that they sum to one and remain in <span class="math inline">\([0,1]\)</span>’ <span class="citation">(Hastie, Tibshirani, and Friedman <a href="#ref-Hastie_2009">2009</a>, 119)</span>. The logistic regression model predicts what class or category a particular instance is likely to belong to, but ‘via linear functions in <span class="math inline">\(x\)</span>.’</p>
<p>We see something of this predictive desire from the basic mathematical expression for logistic regression in a situation where there are binary responses or <span class="math inline">\(K=2\)</span>:</p>

<p><span class="citation">(Hastie, Tibshirani, and Friedman <a href="#ref-Hastie_2009">2009</a>, 119)</span></p>
<p>Equation  encapsulates lines in curves. That is, the linear model (the model that fits a plane to a scattering of points in vector space) appears as <span class="math inline">\(\beta_l0 + \beta_l^Tx\)</span>, where as usual <span class="math inline">\(\beta\)</span> refers to the parameters of the model and <span class="math inline">\(x\)</span> to the matrix of input values. The linear model has, however, now been relayed through the sigmoid function so that its output values no longer increase and decrease linearly. Instead they follow the curve of the logistic function, and range between a minimum of <span class="math inline">\(0\)</span> and a maximum of <span class="math inline">\(1\)</span>, a range of values that map onto probabilities (as discussed in the next chapter <a href="#ch:probability"><strong>??</strong></a>. As usual, small typographic conventions diagram some of this transformation.  In equation , some new characters appears: <span class="math inline">\(G\)</span> and <span class="math inline">\(K\)</span>. Previously, the response variable, the variable the model is trying to predict, appeared as <span class="math inline">\(Y\)</span>.  <span class="math inline">\(Y\)</span> refers to a continuous value whereas <span class="math inline">\(G\)</span> refers to membership of a group or class (e.g. survival vs. death; male vs female; etc.).<a href="#fn41" class="footnoteRef" id="fnref41"><sup>41</sup></a></p>
</div>
<div id="curves-and-the-variation-in-models" class="section level2">
<h2><span class="header-section-number">6.7</span> Curves and the variation in models</h2>
<p>Whether or not the logistic function is a useful approximation to ‘the function that underlies the predictive relationship between input and out’ depends on how it relates input and output. The way in which we have ‘learned’ the logistic function by taking a textbook formula expression of it, and plotting the function associated with it is not the way that machine learners typically ‘learns’ an approximation to the predictive relationship between the input data and the output or ‘response variable’. For a machine learner, finding a function means optimising function parameters on the basis of the data not deriving a formula. Machine learning is not a matter of mathematical analysis, but of algorithmic optimisation.<a href="#fn42" class="footnoteRef" id="fnref42"><sup>42</sup></a>   </p>
<p>If we turn just to the diagrammatic forms associated with logistic regression in <em>Elements of Statistical Learning</em>, something quite different and much more complicated than calculating the values of a known function presents itself there. For instance, in their analysis of the <code>South African coronary heart</code> disease data, Hastie and co-authors repeatedly model the risk of occurrence of heart disease using logistic regression.  They first apply logistic regression fitted by ‘maximum likelihood’, and then by ‘L1 regularized logistic regression’ <span class="citation">(Hastie, Tibshirani, and Friedman <a href="#ref-Hastie_2009">2009</a>, 126)</span>. The results of this function-finding work appear as tables of coefficients or as ‘regularization plots.’ As is often the case, <em>Elements of Statistical Learning</em> assumes that readers already understand conventional statistical usages of logistic regression. Discussion dwells instead on observing how values of the model parameter change as different variants of the model transform the data.</p>
<div class="figure"><span id="fig:heart-model"></span>
<img src="_main_files/figure-html/heart-model-1.png" alt="South African Heart disease regularization plot" width="1152" />
<p class="caption">
Figure 6.3: South African Heart disease regularization plot
</p>
</div>
<div class="figure"><span id="fig:heart-plane"></span>
<img src="_main_files/figure-html/heart-plane-1.png" alt="South African Heart disease decision plane" width="1152" />
<p class="caption">
Figure 6.4: South African Heart disease decision plane
</p>
</div>
<pre><code>##            Length Class     Mode     
## a0          58    -none-    numeric  
## beta       406    dgCMatrix S4       
## df          58    -none-    numeric  
## dim          2    -none-    numeric  
## lambda      58    -none-    numeric  
## dev.ratio   58    -none-    numeric  
## nulldev      1    -none-    numeric  
## npasses      1    -none-    numeric  
## jerr         1    -none-    numeric  
## offset       1    -none-    logical  
## classnames   2    -none-    character
## call         4    -none-    call     
## nobs         1    -none-    numeric</code></pre>
<p>Figure <a href="6-machines-finding-functions.html#fig:heart-model">6.3</a> shows a series of lines. Plotted after the model transforms the data 366 times, each line sets out the changing importance of a particular variable – <code>obesity, alcohol consumption, weight, age,</code> designated by numbers shown on the right hand side – as it is included in the logistic regression model in a different way. The learning or function-finding diagrammed in figure <a href="6-machines-finding-functions.html#fig:heart-model">6.3</a> concerns variations in parameters and ways of automating the variation of parameters beyond that undertaken by modelling experts such as statisticians and scientists when they fit models to data.<a href="#fn43" class="footnoteRef" id="fnref43"><sup>43</sup></a> </p>
<p>We saw that the classic statistical model of linear regression fits lines to the data through the linear algebra method of ordinary least squares (see chapter <a href="#ch:vector"><strong>??</strong></a>, equation ). Several obstacles hinder the construction of models using closed form approximate solutions. Unique ‘closed form’ or analytical solutions are quite unusual in machine learning. While they do exist for linear regression, they don’t exist for logistic regression nor for more complex machine learners. Equally problematically, the closed form solution is run once, and the model it produces is subject to no further variation. The parameters define the line of best fit. It can be interpreted by the modeller in terms of <em>p</em> or <span class="math inline">\(R^2\)</span> or other measures of the model’s fit. But the model itself does not generate variations.<a href="#fn44" class="footnoteRef" id="fnref44"><sup>44</sup></a></p>
</div>
<div id="observing-costs-losses-and-objectives-through-optimisation" class="section level2">
<h2><span class="header-section-number">6.8</span> Observing costs, losses and objectives through optimisation</h2>
<p>Faced with the impracticality of an analytical or mathematically closed form solution to the problem of finding a function, machine learners typically seek ways of observing how different models traverse the data. They replace the exactitude and precision of mathematically-deduced closed-form solutions with algorithms that generate varying solutions. A range of techniques search for optimal combinations of parameters. These optimisation techniques are the operational underpinning of machine learning.  Without their iterative processes, there is no machine in machine learning. They have names such as ‘batch gradient descent’, ‘stochastic gradient ascent,’ ‘coordinate descent,’ ‘coordinate ascent’ as well as the ‘Newtown-Raphson method’ or simply ‘convex optimisation’ <span class="citation">(Boyd and Vandenberghe <a href="#ref-Boyd_2004">2004</a>)</span>.  These techniques have a variety of provenances (Newton’s work in the 17th century, for instance, but more typically fields such as operations research that were the focus of intense research efforts during and after World War ; see <span class="citation">(Bellman <a href="#ref-Bellman_1961">1961</a>; Petrova and Solov’ev <a href="#ref-Petrova_1997">1997</a>; Meza <a href="#ref-Meza_2010">2010</a>)</span>). Much of the learning in machine learning occurs through these somewhat low-profile yet computationally intensive techniques of optimisation. </p>
<p>Optimisation is a practice of observation. ‘Science brings to light partial observers in relation to functions within systems of reference’ write Gilles Deleuze and Félix Guattari in their account of scientific functions <span class="citation">(Deleuze and Guattari <a href="#ref-Deleuze_1994">1994</a>, 129)</span>.<a href="#fn45" class="footnoteRef" id="fnref45"><sup>45</sup></a>   In many machine learning techniques, the search for an approximation to the function that generated the data is optimised by reference to another function called the ‘cost function’ (also known as the ‘objective function’ or the ‘loss function’; the terms are somewhat evocative of both economics and cybernetics). Machine learning problems are framed in terms of minimizing or maximising the cost function.  ‘Cost’ or ‘loss’ takes the form of errors, and minimizing the cost function implies minimizing the number of errors made by a machine learner. As we saw earlier, in his formulation of the ‘learning problem’, the learning theorist Vladimir Vapnik speaks of choosing a function that approximates to the data, yet minimises the ‘probability of error’ <span class="citation">(Vapnik <a href="#ref-Vapnik_1999">1999</a>, 31)</span>.</p>
<p>The  compares predictions generated by a machine learner to known values in the data set. Every cost function implies some measure of the difference or distance between the prediction and the values actually measured. Cost functions in common use include squared error, hinge loss, log-likelihood and cross-entropy.  In classifying outcomes into two classes (the patient survives versus patient dies; the user clicks versus user doesn’t click; etc.), the cost function has to express their either/or outcome. Crucially, if cost functions re-configure ‘the act of fitting a model to data as an optimization problem’ <span class="citation">(Conway and White <a href="#ref-Conway_2012">2012</a>, 183)</span>, function finding and hence machine learning in general occurs iteratively. Given a cost function, a machine learner can vary its parameters keeping in view – or partially observing – whether the cost function increases or decreases. Just as the logistic function wraps the linear regression model in a sigmoid curve that switches smoothly between binary values, the cost functions diagram model parameters (usually noted as <span class="math inline">\(\beta\)</span>) in relation to known responses or output values in the data. If there is learning here, it does derive from mathematic forms or higher abstraction. Cost functions diagram relations between models, and render their predictive reference through the negative feedback loops described by Norbert Wiener <span class="citation">(Wiener <a href="#ref-Wiener_1961">1961</a>)</span>.   Importantly, these feedback loops are not closed mechanisms but places from which variations can be viewed. </p>
<p>For instance, the log-likelihood function,  a typical and widely-used cost function associated with logistic regression is defined as:</p>
<span class="math display">\[\begin{equation}
\label {eq:cost_function_logreg}
J(\beta) = \sum_{i=1}^{m} y_i log{ h(x_i)} + (1 -  y_i) log{(1 - h(x_i)})
\end{equation}\]</span>
<p>where</p>
<p><span class="math display">\[h_{\beta}(x) = \frac{1}{1+e^{-\beta^T x}}\]</span></p>
<p>Equation  enfolds several manipulations and conceptual framings (particularly the Principle of Maximum Likelihood, a statistical principle; see chapter <a href="#ch:probability"><strong>??</strong></a>). But key terms stand out. First, the cost function <span class="math inline">\(J(\beta)\)</span> is a function of all the parameters (<span class="math inline">\(\beta\)</span>) of the model. They substitute in through the subsidiary function <span class="math inline">\(h_{\beta}(x)\)</span>, the logistic function function encapsulating a linear function <span class="math inline">\(\beta^T X\)</span> . Second, the function defines a goal of maximising the overall value of the expression, <span class="math inline">\(J(\beta)\)</span> as a function of variations in the parameters <span class="math inline">\(\beta\)</span>. The <span class="math inline">\(min\)</span> describes the results of the repeated application of the function. Third, the heart of the cost function is balancing of two tendencies: it adds (<span class="math inline">\(\sum\)</span>) all the values where the probability of the predicted class of a particular case <span class="math inline">\(h(x_i)\)</span> matches the actual class <span class="math inline">\(y_i\)</span>, and subtracts (<span class="math inline">\(1-y\)</span>) all the cases where the probability of the predicted class does not match the actual class. This so-called <em>log likelihood</em> function can be maximised through optimisation, but not solved in closed form. The optimal values for <span class="math inline">\(\beta\)</span>, the model parameters that define the model function need to be found through some kind of search. </p>
</div>
<div id="gradients-as-partial-observers" class="section level2">
<h2><span class="header-section-number">6.9</span> Gradients as partial observers</h2>
<p>We have some sense of how a function can be configured as an observer, but little sense of how they manage variations. Many optimization techniques rely on differential calculus and particularly the calculus of variations to maximise or minimise the value of a cost function. In fact, loss functions are often chosen on the basis of their differentiability.  One widely used optimisation algorithm called ‘gradient descent’ is quite easy to grasp intuitively. In neural nets and deep learning, gradient descent (or ascent) occurs on an increasingly vast scale.  As in many formulations of machine learning techniques, the framing of the problem is finding the parameters of the model/function that best approximates to the function that generated the data. It optimises the parameters of a model by searching for the maximum or minimum values of the objective function. The algorithm can be written using calculus style notation as:</p>
<span class="math display">\[\begin{equation}
\label{eq:stochastic_gradient_ascent}
Repeat until convergence:
\beta_{j} := \beta_j + \alpha (y_i - h_\beta(x_i))x_{\beta j}
\end{equation}\]</span>
<p>The version of the algorithm shown in algorithm  is called ‘stochastic gradient descent.’ Archaeologically, in presenting such formula, the point is not to read and understand them directly but to characterise the enunciative function that regulates them. Practical understanding would be the point in a machine learning course. Actually reading these formal expressions, and being able to follow the chain of references, and indexical signs that lead away from them in various directions depends very much on the diagrammatic processes described in chapter <a href="#ch:diagram"><strong>??</strong></a>. Many people who directly use machine learning techniques in industry and science would not often if ever need to make use of such expressions as they build models. They would mostly take them for granted, and simply execute via functions supplied by software libraries (e.g. <code>GradientDescentOptimizer</code> in the <code>TensorFlow</code> library or <code>StochasticGradient</code> in <code>torch</code>).</p>
<div class="figure"><span id="fig:gradient-ascent"></span>
<img src="figure/log_reg_stochastic_ascent.png" alt="Gradient ascent for logistic regression" width="330" />
<p class="caption">
Figure 6.5: Gradient ascent for logistic regression
</p>
</div>
<p>Given that equation  encapsulates the heart of a major optimisation technique, we might first of all be struck by its operational brevity. This is not an elaborate or convoluted algorithm. As Malley, Mally and Pajevic observe, ‘most of the [machine learning] procedures … are (often) nearly trivial to implement’ <span class="citation">(Malley, Malley, and Pajevic <a href="#ref-Malley_2011">2011</a>, 6)</span>. Note that this expression of the algorithm, taken from the class notes for [Lecture 3] of Andrew Ng’s ‘Machine Learning’ CS229 course at Stanford (<span class="citation">(A. Ng <a href="#ref-Ng_2008j">2008</a><a href="#ref-Ng_2008j">c</a>)</span>; see figure <a href="6-machines-finding-functions.html#fig:gradient-ascent">6.5</a>), mixes an algorithmic set of operations with function notation. We see this in several respects: the formulation includes the imperative ‘repeat until convergence’; it also uses the so-called ‘assignment operator’ <span class="math inline">\(:=\)</span> rather than the equality operator <span class="math inline">\(=\)</span>. The latter specifies that two values or expressions are equal, whereas the former specifies that the values on the right hand side of the expression should be assigned to the left. Both algorithmic forms – repeat until convergence, and assign/update values – owe more to techniques of computation than to mathematical abstraction.</p>
<div class="figure"><span id="fig:stochastic-gradient-ascent"></span>
<img src="figure/stochastic_gradient_handdrawn.png" alt="Stochastic gradient descent path" width="3056" />
<p class="caption">
Figure 6.6: Stochastic gradient descent path
</p>
</div>
<p>In gradient descent, we see functions acting as partial observers. The specification for the gradient descent algorithm brings us to the scene where ongoing transformation of data from irregular volume to plane can be observed. At the heart of this reshaping lies a different mathematical formalism: the , <span class="math inline">\(\frac {\partial }{\partial \beta_j}J(\beta_j)\)</span>. Like all derivatives in calculus, this expression can be interpreted as the rate at which one variable changes in relation to another; that is, as the rate at which the cost function <span class="math inline">\(J(\beta)\)</span> changes with respect to the different values of <span class="math inline">\(\beta\)</span>.<a href="#fn46" class="footnoteRef" id="fnref46"><sup>46</sup></a> Much learning in machine learning pivots on the observation of rates of change of a cost function in relation to its ‘arguments,’ the <span class="math inline">\(j\)</span>-dimensional vector space defined by <span class="math inline">\(\beta\)</span>, the model parameters. The partial derivatives in the gradient descent algorithm observe the direction in which the value of the cost function reduces. Each iteration of the algorithm reduces or increases the parameters <span class="math inline">\(\beta\)</span> of the model in the direction of reduced cost, and perhaps less error.  Importantly, the derivative of a sigmoid (or logistic function) <span class="math inline">\(\sigma(x)\)</span> is given by <span class="math inline">\(\sigma(x)(1-\sigma(x))\)</span>, which means that the partial derivatives of a cost function will be easy to compute. </p>
</div>
<div id="the-power-to-learn" class="section level2">
<h2><span class="header-section-number">6.10</span> The power to learn</h2>
<p>The power of machine learning to learn, its power to epistemologize, pivots around functions in disparate yet connected ways: the transformation of data through operational functions that map new sub-spaces in vector space and in the observational functions that algorithmically superimpose new constraints – cost, loss or objective functions – that direct an iterative process of optimisation.   Machine learning diagrammatically distributes learning in the operational human-machine formation. People look at curves for evidence of convergence, functions compress data into functions that support classification or predictions, and algorithms observe gradients or rates of error in relation to model parameters. In several senses, people and machines together move along curves. The logistic function folds the lines that best fit the data into a probability distribution that can be read in terms of classification. The cost functions, as they seek to minimize differences between the predicted values and the known values found in the vector space, control variations in the model. Every observer in this domain is partial, since the humans cannot see lines or curves in the multi-dimensional data, the functions that underpin models such as logistic regression or linear regression can transform data in the vector space, but can’t show how well they see it, and the processes of optimisation only see the results of the model and its errors, not anything in its referential functioning.  Universality (<em>apropos</em> master algorithms), whether fully supervised or completely unsupervised, is impossible here.</p>
<p>Amidst this endemic partiality, we can begin to understand the multiplication of machine learners and the mirage of universality. Machine learners are functions that transform data and observe the effects of those transformation in learning to classify, predict and rank. But the function that defines a ‘machine learner’ contracts a range of partial observers relaying values and changes to each other.  The operational power of machine learning depends on the diagrammatic and sometimes experimental relays between different practices of observing. Attending to specific mathematical functions in isolation – the logistic function, the Lagrangean, the Gaussian, the quadratic discriminant, etc. – will not tell us how the operational power of functions comes together in machine learning, but it may provide ways of mapping the diagrammatic connections, the enunciative function that connects different elements in the production of consequential classifications and predictions, generating operational statements in fields of knowledge. </p>
<p>We are in a slightly better position to understand now how there can be many machine learners but a relative sparsity in the production of statements.  Gradients – continuous variations in rate – are useful because they generate many functions, many approximations within one operational process, within one enunciative function. The profusion of machine learners, the ‘bewildering variety’ that Domingos and others celebrate and flag up, can be seen as the effect of an operational formation predicated on approximation through variation.</p>
<p>In his account of Foucault’s diagrams of power, Gilles Deleuze writes:</p>
<blockquote>
<p>every diagram is intersocial and constantly evolving. It never functions in order to represent a persisting world but produces a new kind of reality, a new model of truth. … It makes history by unmaking preceding realities and significations, constituting hundreds of points of emergence or creativity, unexpected conjunctions or improbable continuums <span class="citation">(Deleuze <a href="#ref-Deleuze_1988">1988</a><a href="#ref-Deleuze_1988">b</a>, 35)</span>  </p>
</blockquote>
<p>Functions in machine learning are ‘intersocial’ in the sense that they bring together very different mathematical, algorithmic, operational and observational processes. The sigmoid function switches the geometry of the linear model over into the calculation of probabilities and classification, but also figures heavily in the computability of partial derivatives. The cost functions re-craft statistical modelling as a quasi-iterative process of model generation and comparison. New kinds of realities arise in which the classifications and predictions generated by the diagonal connections between mathematical functions and operational processes of optimisation can constitute a ‘new model truth’ and can unmake ‘preceding realities and significations.’ And despite my deliberately narrow focus on a single set of relays that connect linear models, the logistic function, the cost function and gradient ascent, there are hundreds and perhaps and hundreds of thousands of ‘points of emergence’ associated with this diagram of functioning.</p>
<p>The machine learning diagram, like any functioning, harbours the potential for invention. Describing the application of machine learning to biomedical and clinical research, James Malley, Karen Malley and Sinisa Pajevic contrast it to more conventional statistical knowledges:</p>
<blockquote>
<p>working with statistical learning machines can push us to think about novel structures and functions in our data. This awareness is often counterintuitive, and familiar methods such as simple correlations, or slightly more evolved partial correlations, are often not sufficient to pin down these deeper connections. <span class="citation">(Malley, Malley, and Pajevic <a href="#ref-Malley_2011">2011</a>, 5–6)</span>  </p>
</blockquote>
<p>Novel structures and functions in ‘our data’ are precisely the functions that machine learning technique seek to learn. Could new habits or actively diverging worlds that Stengers calls for appear amidst this quasi-iterative pursuit of optimisation and convergence? This is a terrain for critical thought to explore. A function in isolation never learns. But when watched or observed, even virtually, divergence has some chance. To the extent that machine learners relay references experimentally between things and people, mobilising the production of statements and visibilities across different elements, divergence remain possible.</p>

</div>
</div>
<h3> References</h3>
<div id="refs" class="references">
<div id="ref-Foucault_1972">
<p>Foucault, Michel. 1972. <em>The Archaeology of Knowledge and the Discourse on Language</em>. Translated by Allan Sheridan-Smith. New York: Pantheon Books.</p>
</div>
<div id="ref-Domingos_2015a">
<p>Domingos, Pedro. 2015. <em>The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World</em>. New York: Basic Civitas Books.</p>
</div>
<div id="ref-Domingos_2012">
<p>Domingos, Pedro. 2012. “A Few Useful Things to Know About Machine Learning.” <em>Communications of the ACM</em> 55 (10): 78–87. <a href="http://dl.acm.org/citation.cfm?id=2347755" class="uri">http://dl.acm.org/citation.cfm?id=2347755</a>.</p>
</div>
<div id="ref-Barber_2011">
<p>Barber, David. 2011. <em>Bayesian Reasoning and Machine Learning</em>. Cambridge; New York: Cambridge University Press.</p>
</div>
<div id="ref-James_2013">
<p>James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. <em>An Introduction to Statistical Learning</em>. Springer. <a href="http://link.springer.com/content/pdf/10.1007/978-1-4614-7138-7.pdf" class="uri">http://link.springer.com/content/pdf/10.1007/978-1-4614-7138-7.pdf</a>.</p>
</div>
<div id="ref-Malley_2011">
<p>Malley, James D., Karen G. Malley, and Sinisa Pajevic. 2011. <em>Statistical Learning for Biomedical Data</em>. 1st ed. Cambridge University Press.</p>
</div>
<div id="ref-Kirk_2014">
<p>Kirk, Matthew. 2014. <em>Thoughtful Machine Learning: A Test-Driven Approach</em>. 1 edition. Sebastopol, Calif.: O’Reilly Media.</p>
</div>
<div id="ref-Pedregosa_2011">
<p>Pedregosa, F., G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, et al. 2011. “Scikit-Learn: Machine Learning in Python.” <em>Journal of Machine Learning Research</em> 12: 2825–30.</p>
</div>
<div id="ref-Hastie_2009">
<p>Hastie, Trevor, Robert Tibshirani, and Jerome H. 2009. <em>The Elements of Statistical Learning: Data Mining, Inference, and Prediction</em>. 2nd edition. New York: Springer.</p>
</div>
<div id="ref-Breiman_1984">
<p>Breiman, Leo, Jerome Friedman, Richard Olshen, Charles Stone, D. Steinberg, and P. Colla. 1984. <em>CART: Classification and Regression Trees</em>. Vol. 156. Belmont: Wadsworth.</p>
</div>
<div id="ref-Vapnik_1999">
<p>Vapnik, Vladimir. 1999. <em>The Nature of Statistical Learning Theory</em>. 2nd ed. 2000. Springer.</p>
</div>
<div id="ref-Conway_2012">
<p>Conway, Drew, and John Myles White. 2012. <em>Machine Learning for Hackers</em>. Sebastopol, CA: O’Reilly. <a href="http://search.ebscohost.com/login.aspx?direct=true&amp;scope=site&amp;db=nlebk&amp;db=nlabk&amp;AN=436647" class="uri">http://search.ebscohost.com/login.aspx?direct=true&amp;scope=site&amp;db=nlebk&amp;db=nlabk&amp;AN=436647</a>.</p>
</div>
<div id="ref-Alpaydin_2010">
<p>Alpaydin, Ethem. 2010. <em>Introduction to Machine Learning</em>. Cambridge, Massachusetts; London: MIT Press.</p>
</div>
<div id="ref-Stengers_2005">
<p>Stengers, Isabelle. 2005. “Deleuze and Guattari’s Last Enigmatic Message.” <em>Angelaki</em> 10 (1): 151–67.</p>
</div>
<div id="ref-Stengers_2008">
<p>Stengers, Isabelle. 2008. “Experimenting with Refrains: Subjectivity and the Challenge of Escaping Modern Dualism.” <em>Subjectivity</em> 22 (1): 38–59. doi:<a href="https://doi.org/10.1057/sub.2008.6">10.1057/sub.2008.6</a>.</p>
</div>
<div id="ref-Stengers_2011">
<p>Stengers, Isabelle. 2011. <em>Cosmopolitics II</em>. Translated by Robert Bononno. University of Minnesota Press.</p>
</div>
<div id="ref-Stengers_2000">
<p>Stengers, Isabelle. 2000. <em>The Invention of Modern Science</em>. Theory Out of Bounds ; V.19. Minneapolis ; London: University of Minnesota Press.</p>
</div>
<div id="ref-Hinton_2006">
<p>Hinton, Geoffrey E., and Ruslan R. Salakhutdinov. 2006. “Reducing the Dimensionality of Data with Neural Networks.” <em>Science</em> 313 (5786): 504–7. <a href="http://www.sciencemag.org/content/313/5786/504.short" class="uri">http://www.sciencemag.org/content/313/5786/504.short</a>.</p>
</div>
<div id="ref-Mohamed_2011">
<p>Mohamed, Abdel-rahman, Tara N. Sainath, George Dahl, Bhuvana Ramabhadran, Geoffrey E. Hinton, and Michael A. Picheny. 2011. “Deep Belief Networks Using Discriminative Features for Phone Recognition.” In, 5060–3. IEEE. doi:<a href="https://doi.org/10.1109/ICASSP.2011.5947494">10.1109/ICASSP.2011.5947494</a>.</p>
</div>
<div id="ref-Cramer_2004">
<p>Cramer, J. S. 2004. “The Early Origins of the Logit Model.” <em>Studies in History and Philosophy of Science Part C: Studies in History and Philosophy of Biological and Biomedical Sciences</em> 35 (4): 613–26.</p>
</div>
<div id="ref-Boyd_2004">
<p>Boyd, Stephen, and Lieven Vandenberghe. 2004. <em>Convex Optimization</em>. Cambridge ; New York: Cambridge university press. <a href="https://books.google.co.uk/books?hl=en&amp;lr=&amp;id=IUZdAAAAQBAJ&amp;oi=fnd&amp;pg=PR11&amp;dq=boyd+convex&amp;ots=HMIBdk5GAj&amp;sig=3zx9GlXT4pYe2THUwk5i-tRJSlE" class="uri">https://books.google.co.uk/books?hl=en&amp;lr=&amp;id=IUZdAAAAQBAJ&amp;oi=fnd&amp;pg=PR11&amp;dq=boyd+convex&amp;ots=HMIBdk5GAj&amp;sig=3zx9GlXT4pYe2THUwk5i-tRJSlE</a>.</p>
</div>
<div id="ref-Bellman_1961">
<p>Bellman, Richard. 1961. <em>Adaptive Control Processes: A Guided Tour</em>. Vol. 4. Princeton N.J.: Princeton University Press. <a href="http://www.getcited.org/pub/101191710" class="uri">http://www.getcited.org/pub/101191710</a>.</p>
</div>
<div id="ref-Petrova_1997">
<p>Petrova, Svetlana S., and Alexander D. Solov’ev. 1997. “The Origin of the Method of Steepest Descent.” <em>Historia Mathematica</em> 24 (4): 361–75. doi:<a href="https://doi.org/10.1006/hmat.1996.2146">10.1006/hmat.1996.2146</a>.</p>
</div>
<div id="ref-Meza_2010">
<p>Meza, Juan C. 2010. “Steepest Descent.” <em>Wiley Interdisciplinary Reviews: Computational Statistics</em> 2 (6): 719–22. doi:<a href="https://doi.org/10.1002/wics.117">10.1002/wics.117</a>.</p>
</div>
<div id="ref-Deleuze_1994">
<p>Deleuze, Gilles, and Félix Guattari. 1994. <em>What Is Philosophy?</em> Translated by Hugh Tomlinson. European Perspectives. New York; Chichester: Columbia University Press.</p>
</div>
<div id="ref-Wiener_1961">
<p>Wiener, Norbert. 1961. <em>Cybernetics, or, Control and Communication in the Animal and the Machine</em>. 2nd ed. Cambridge, MA: MITPress.</p>
</div>
<div id="ref-Ng_2008j">
<p>———, dir. 2008c. <em>Lecture 3 | Machine Learning (Stanford)</em>. <a href="https://www.youtube.com/watch?v=HZ4cvaztQEs&amp;feature=youtube_gdata_player" class="uri">https://www.youtube.com/watch?v=HZ4cvaztQEs&amp;feature=youtube_gdata_player</a>.</p>
</div>
<div id="ref-Deleuze_1988">
<p>Deleuze, Gilles. 1988b. <em>Foucault</em>. Translated by Seân Hand. Minneapolis: University of Minnesota Press.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="33">
<li id="fn33"><p>Similarly, for <code>R</code> code, the <em>Comprehensive R Archive Network</em> tabulates key libraries of <code>R</code> code in a machine learning ‘task view’ <span class="citation">(Hothorn <a href="#ref-Hothorn_2014">2014</a>)</span>. <a href="6-machines-finding-functions.html#fnref33">↩</a></p></li>
<li id="fn34"><p>I discuss the sense of function as operation or process in chapter <a href="#ch:genome"><strong>??</strong></a>. There I suggest that this important sense of function as operation or process, a sense that has underpinned transformations in life, social and clinical sciences may be shifting towards a different ordering. <a href="6-machines-finding-functions.html#fnref34">↩</a></p></li>
<li id="fn35"><p>Only in the mid-1980s were the first theories of algorithmic learning formalised <span class="citation">(Valiant <a href="#ref-Valiant_1984">1984</a>)</span>.<a href="6-machines-finding-functions.html#fnref35">↩</a></p></li>
<li id="fn36"><p>Vapnik is said to have invented the support vector machine, one of the most heavily used machine learning technique of recent years on the basis of his theory of computational learning. Chapter <a href="#ch:dimension"><strong>??</strong></a> discusses the support vector machine.<a href="6-machines-finding-functions.html#fnref36">↩</a></p></li>
<li id="fn37"><p>The U.S. National Institute of Standards published <em>The Handbook of Mathematical Functions</em> in 1965 <span class="citation">(Abramowitz <a href="#ref-Abramowitz_1965">1965</a>)</span>. This heavily cited volume, now also <a href="http://dlmf.nist.gov">versioned online</a> lists hundreds of functions organised in various categories ranging from algebra to zeta functions. While a number of the functions and operations catalogued there surface in machine learning, machine learners implement, as we will see, quite a narrow range of functions.<a href="6-machines-finding-functions.html#fnref37">↩</a></p></li>
<li id="fn38"><p>A major reference here would be Ernst Cassirer <span class="citation">(Cassirer <a href="#ref-Cassirer_1923">1923</a>)</span> who posited a philosophical-historical shift from ontologies of substance reaching back to Aristotle’s categories <span class="citation">(Aristotle <a href="#ref-Aristotle_1975">1975</a>)</span> to a functional ontology emerging in 19th century as the notion of function was generalized across many mathematical and scientific fields. (See <span class="citation">(Heis <a href="#ref-Heis_2014">2014</a>)</span> for a recent account of the <em>FunktionBegriff</em> in Cassirer’s philosophy)   In a recent article, Paolo Totaro and Domenico Ninno suggest that the transition from substance to function occurs practically in the form of the algorithm <span class="citation">(Totaro and Ninno <a href="#ref-Totaro_2014">2014</a>)</span>.  The idea of computable functions lies at the base of theoretical computer science and has been a topic of interest in some social and cultural theory (e.g. <span class="citation">(Parisi <a href="#ref-Parisi_2013">2013</a>)</span>; see also my <span class="citation">(Mackenzie <a href="#ref-Mackenzie_1997">1997</a>)</span>), but Totaro and Ninno’s suggest that algorithmic processes, as the social practice of the function, form contradictory hybrids with remnants of substance, in particular, categories and classification. . They see bureaucratic logic, for instance, as hopelessly vitiated by a contradiction between classification and function. Machine learning, I’d suggest, is an important counter-example. It hybridises function and classification without any obvious contradiction. <a href="6-machines-finding-functions.html#fnref38">↩</a></p></li>
<li id="fn39"><p>This point has often been made in the social studies of science; see <span class="citation">(Latour <a href="#ref-Latour_1993">1993</a>)</span> for a very high-level account.<a href="6-machines-finding-functions.html#fnref39">↩</a></p></li>
<li id="fn40"><p>The Belgian mathematician Pierre-François Verhulst designated the sigmoid function the ‘logistic curve’ in the 1830-40s <span class="citation">(Cramer <a href="#ref-Cramer_2004">2004</a>, 616)</span>. It was independently designated the ‘autocatalytic function’ by the German chemist Wilhelm Ostwald in the 1880s, and then re-invented under various names by biologists, physiologists and demographers during 1900-1930s (617). The term ‘logistic’ returns to visibility in the 1920s, and has continued in use as a way of describing the growth of something that reaches a limit. <a href="6-machines-finding-functions.html#fnref40">↩</a></p></li>
<li id="fn41"><p>What does this wrapping of the linear model in the curve of the sigmoid logistic curve do in terms of finding a function? Note that the shape of this curve has no intrinsic connection or origin in the data. The curve no longer corresponds to growth or change in size, as it did in its nineteenth century biopolitical application to the growth of populations. Rather, the curvilinear encapsulation of the linear model allows the left hand side of the expression to move into a different register. The left hand side of the expression is now a probability function, and defines the probability (<span class="math inline">\(Pr\)</span>) that a given response value (<span class="math inline">\(G\)</span>) belongs to one of the pre-defined classes (<span class="math inline">\(k = 1, ..., K-1\)</span>). In this case, there are two classes (‘yes/no’), so <span class="math inline">\(K=2\)</span>. Unlike linear models, that predict continuous <span class="math inline">\(y\)</span> values for a given set of <span class="math inline">\(x\)</span> inputs, the logistic regression model produces a probability that the instance represented by a given set of <span class="math inline">\(x\)</span> values belongs to a particular class. When logistic regression is used for classification, values greater than <span class="math inline">\(0.5\)</span> are usually read as class predictions of ‘yes’, ‘true’ or <code>1</code>. As a result, drawing lines through the vector space can effectively become a way of classifying things. Note that this increase in flexibility comes at the cost of a loss of direct connection between the data or features in the generalized vector space, and the output, response or predicted variables. They are now connected by a mapping that passes through the somewhat more mobile and dynamic operation of exponentiation <span class="math inline">\(exp\)</span>, a function whose rapid changes can be mapped onto classes and categories.<a href="6-machines-finding-functions.html#fnref41">↩</a></p></li>
<li id="fn42"><p>Even in machine learning, some function-finding through solving systems of equations occurs. For instance, the closed form or analytical solution of the least sum of squares problem for linear regression is given by <span class="math inline">\(\hat{\beta} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}\)</span>.  As we saw in the previous chapter, this expression provides a very quick way to calculate the parameters of a linear model given a matrix of input and output values. This formula itself is derived by solving a set of equations for the values <span class="math inline">\(\hat{\beta}\)</span>, the estimated parameters of the model. But how do we know whether a model is a good one, or that the function that a model proffers to us fits the functions in our data, or that it ‘minimises the probability of error’? One problem with closed-form or analytical solutions typical of mathematical problem-solving is precisely that their closed-form obscures the algorithmic processes needed to actually compute results. The closed form solution estimates the parameters of the linear model by carrying out a series of operations on matrices of the data. These operations include matrix transpose, several matrix multiplications (so-called ‘inner product’) and matrix inversion (the process of finding a matrix that when multiplied by the input matrix yields the identity matrix, a matrix with <span class="math inline">\(1\)</span> along the diagonal, and <span class="math inline">\(0\)</span> for all other values). All of these operations take place in the vector space. When the dataset, however, has a hundred or a thousand rows, these operations can be implemented and executed easily. But as soon as datasets become much larger, it is not easy to actually carry out these matrix operations, particularly the matrix inversion, even on fast computers. For instance, a dataset with a million rows and several dozen columns is hardly unusual today. Although linear algebra libraries are carefully crafted and tested for speed and efficiency, closed form solutions, even for the simplest possible structures in the data, begin to break down. <a href="6-machines-finding-functions.html#fnref42">↩</a></p></li>
<li id="fn43"><p>As we saw in chapter <a href="#ch:vector"><strong>??</strong></a>, the production of new tables of numbers that list the parameters of models actually transform the vectorised data into new subspaces (a line, plane, a surface).  Many of the plots and tables found in machine learning texts, practice and code offer nothing else but measurements of how the model parameters weight slightly different transformations of the vector space. In the case of logistic regression, the shape of the curve is determined using maximum likelihood. For present purposes, the statistical significance of this procedure is less important than the algorithmic implementation. This is the opposite to what might appear in a typical statistics textbook where the implementation of maximum likelihood would normally be quickly passed over. For instance, in <em>An Introduction to Statistical Learning with R</em>, a textbook focused on using <code>R</code> to implement machine learning techniques, the authors write: ‘we do not need to concern ourselves with the details of the maximum likelihood fitting procedure’ <span class="citation">(James et al. <a href="#ref-James_2013">2013</a>, 133)</span>. <a href="6-machines-finding-functions.html#fnref43">↩</a></p></li>
<li id="fn44"><p>As soon as we move from the more theoretical or expository accounts of function-finding into the domain of practice, instruction and learning of machine learning, a second sense of function comes to the fore. The second sense of function comes from programming and computer science. A function there is a part of the code of a program that performs some operation, ‘a self-contained unit of code,’ as Derek Robinson puts it <span class="citation">(Robinson <a href="#ref-Robinson_2008">2008</a>, 101)</span>.  The three lines of R code written to produce the plot of the logistic function are almost too trivial to implement as a function in this sense, but they show something of the transformations that occur when mathematical functions are operationalised in algorithmic form. The function is wrapped in a set of references. First, the domain of <span class="math inline">\(x\)</span> values is made much more specific. The formulaic expression <span class="math inline">\(f(x) = 1/(1+e^{-x})\)</span> says nothing explicitly about the <span class="math inline">\(x\)</span> values. They are implicitly real numbers (that is, <span class="math inline">\(x \in \mathbb{R}\)</span>) in this formula but in the algorithmic expression of the function they become a sequence of 20001 generated by the code. Second, the function itself is flattened into a single line of characters in code, whereas the typographically the mathematical formula had spanned 2-3 lines. Third, a key component of the function <span class="math inline">\(e^-x\)</span> itself refers to Euler’s number <span class="math inline">\(e\)</span>, which is perhaps the number most widely used in contemporary sciences due to its connection to patterns of growth and decay (as in the exponential function <span class="math inline">\(e^x\)</span> where <span class="math inline">\(e = 2.718282\)</span> approximately). This number, because it is ‘irrational,’ has to be computed approximately in any algorithmic implementation.<a href="6-machines-finding-functions.html#fnref44">↩</a></p></li>
<li id="fn45"><p>Its hard to know whether Deleuze and Guattari were aware of the extensive work done on problems of mathematical optimization during the 1950-1960s, but their strong interest in the differential calculus as a way of thinking about change, variation and multiplicities somewhat unexpectedly makes their account of functions highly relevant to machine learning. <a href="6-machines-finding-functions.html#fnref45">↩</a></p></li>
<li id="fn46"><p>The derivative <span class="math inline">\(\frac {\partial }{\partial \beta_j}J(\beta_j)\)</span> is <em>partial</em> because <span class="math inline">\(\beta\)</span> is a vector <span class="math inline">\(\beta_0, \beta_1 ... \beta_j\)</span>.<a href="6-machines-finding-functions.html#fnref46">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="5-vectorisation-and-its-consequences.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="7-probabilisation-and-the-taming-of-machines.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
