<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Machine Learning: Archaeology of a Data Practice</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="Machine Learning: Archaeology of a Data Practice">
  <meta name="generator" content="bookdown 0.3 and GitBook 2.6.7">

  <meta property="og:title" content="Machine Learning: Archaeology of a Data Practice" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Machine Learning: Archaeology of a Data Practice" />
  
  
  

<meta name="author" content="Adrian Mackenzie">


<meta name="date" content="2017-03-25">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="7-probabilisation-and-the-taming-of-machines.html">
<link rel="next" href="9-regularizing-and-materializing-objects.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="1-acknowledgments.html"><a href="1-acknowledgments.html"><i class="fa fa-check"></i><b>1</b> Acknowledgments</a></li>
<li class="chapter" data-level="2" data-path="2-preface.html"><a href="2-preface.html"><i class="fa fa-check"></i><b>2</b> Preface</a></li>
<li class="chapter" data-level="3" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html"><i class="fa fa-check"></i><b>3</b> Introduction: Into the Data</a><ul>
<li class="chapter" data-level="3.1" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#three-accumulations-settings-data-and-devices"><i class="fa fa-check"></i><b>3.1</b> Three accumulations: settings, data and devices</a></li>
<li class="chapter" data-level="3.2" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#who-or-what-is-a-machine-learner"><i class="fa fa-check"></i><b>3.2</b> Who or what is a machine learner?</a></li>
<li class="chapter" data-level="3.3" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#algorithmic-control-to-the-machine-learners"><i class="fa fa-check"></i><b>3.3</b> Algorithmic control to the machine learners?</a></li>
<li class="chapter" data-level="3.4" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#the-archaeology-of-operations"><i class="fa fa-check"></i><b>3.4</b> The archaeology of operations</a></li>
<li class="chapter" data-level="3.5" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#asymmetries-in-common-knowledge"><i class="fa fa-check"></i><b>3.5</b> Asymmetries in common knowledge</a></li>
<li class="chapter" data-level="3.6" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#what-cannot-be-automated"><i class="fa fa-check"></i><b>3.6</b> What cannot be automated?</a></li>
<li class="chapter" data-level="3.7" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#different-fields-in-machine-learning"><i class="fa fa-check"></i><b>3.7</b> Different fields in machine learning?</a></li>
<li class="chapter" data-level="3.8" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#the-diagram-in-critical-thought"><i class="fa fa-check"></i><b>3.8</b> The diagram in critical thought</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html"><i class="fa fa-check"></i><b>4</b> Diagramming machines</a><ul>
<li class="chapter" data-level="4.1" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#we-dont-have-to-write-programs"><i class="fa fa-check"></i><b>4.1</b> ‘We don’t have to write programs’?</a></li>
<li class="chapter" data-level="4.2" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#the-elements-of-machine-learning"><i class="fa fa-check"></i><b>4.2</b> The elements of machine learning</a></li>
<li class="chapter" data-level="4.3" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#who-reads-machine-learning-textbooks"><i class="fa fa-check"></i><b>4.3</b> Who reads machine learning textbooks?</a></li>
<li class="chapter" data-level="4.4" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#r-a-matrix-of-transformations"><i class="fa fa-check"></i><b>4.4</b> <code>R</code>: a matrix of transformations</a></li>
<li class="chapter" data-level="4.5" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#the-obdurate-mathematical-glint-of-machine-learning"><i class="fa fa-check"></i><b>4.5</b> The obdurate mathematical glint of machine learning</a></li>
<li class="chapter" data-level="4.6" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#cs229-2007-returning-again-and-again-to-certain-features"><i class="fa fa-check"></i><b>4.6</b> CS229, 2007: returning again and again to certain features</a></li>
<li class="chapter" data-level="4.7" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#the-visible-learning-of-machine-learning"><i class="fa fa-check"></i><b>4.7</b> The visible learning of machine learning</a></li>
<li class="chapter" data-level="4.8" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#the-diagram-of-an-operational-formation"><i class="fa fa-check"></i><b>4.8</b> The diagram of an operational formation</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html"><i class="fa fa-check"></i><b>5</b> Vectorisation and its consequences</a><ul>
<li class="chapter" data-level="5.1" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html#vector-space-and-geometry"><i class="fa fa-check"></i><b>5.1</b> Vector space and geometry</a></li>
<li class="chapter" data-level="5.2" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html#mixing-places"><i class="fa fa-check"></i><b>5.2</b> Mixing places</a></li>
<li class="chapter" data-level="5.3" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html#truth-is-no-longer-in-the-table"><i class="fa fa-check"></i><b>5.3</b> Truth is no longer in the table?</a></li>
<li class="chapter" data-level="5.4" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html#the-epistopic-fault-line-in-tables"><i class="fa fa-check"></i><b>5.4</b> The epistopic fault line in tables</a></li>
<li class="chapter" data-level="5.5" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html#surface-and-depths-the-problem-of-volume-in-data"><i class="fa fa-check"></i><b>5.5</b> Surface and depths: the problem of volume in data</a></li>
<li class="chapter" data-level="5.6" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html#vector-space-expansion"><i class="fa fa-check"></i><b>5.6</b> Vector space expansion</a></li>
<li class="chapter" data-level="5.7" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html#drawing-lines-in-a-common-space-of-transformation"><i class="fa fa-check"></i><b>5.7</b> Drawing lines in a common space of transformation</a></li>
<li class="chapter" data-level="5.8" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html#implicit-vectorization-in-code-and-infrastructures"><i class="fa fa-check"></i><b>5.8</b> Implicit vectorization in code and infrastructures</a></li>
<li class="chapter" data-level="5.9" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html#lines-traversing-behind-the-light"><i class="fa fa-check"></i><b>5.9</b> Lines traversing behind the light</a></li>
<li class="chapter" data-level="5.10" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html#the-vectorised-table"><i class="fa fa-check"></i><b>5.10</b> The vectorised table?</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html"><i class="fa fa-check"></i><b>6</b> Machines finding functions</a><ul>
<li class="chapter" data-level="6.1" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html#learning-functions"><i class="fa fa-check"></i><b>6.1</b> Learning functions</a></li>
<li class="chapter" data-level="6.2" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html#supervised-unsupervised-reinforcement-learning-and-functions"><i class="fa fa-check"></i><b>6.2</b> Supervised, unsupervised, reinforcement learning and functions</a></li>
<li class="chapter" data-level="6.3" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html#which-function-operates"><i class="fa fa-check"></i><b>6.3</b> Which function operates?</a></li>
<li class="chapter" data-level="6.4" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html#what-does-a-function-learn"><i class="fa fa-check"></i><b>6.4</b> What does a function learn?</a></li>
<li class="chapter" data-level="6.5" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html#observing-with-curves-the-logistic-function"><i class="fa fa-check"></i><b>6.5</b> Observing with curves: the logistic function</a></li>
<li class="chapter" data-level="6.6" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html#the-cost-of-curves-in-machine-learning"><i class="fa fa-check"></i><b>6.6</b> The cost of curves in machine learning</a></li>
<li class="chapter" data-level="6.7" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html#curves-and-the-variation-in-models"><i class="fa fa-check"></i><b>6.7</b> Curves and the variation in models</a></li>
<li class="chapter" data-level="6.8" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html#observing-costs-losses-and-objectives-through-optimisation"><i class="fa fa-check"></i><b>6.8</b> Observing costs, losses and objectives through optimisation</a></li>
<li class="chapter" data-level="6.9" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html#gradients-as-partial-observers"><i class="fa fa-check"></i><b>6.9</b> Gradients as partial observers</a></li>
<li class="chapter" data-level="6.10" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html#the-power-to-learn"><i class="fa fa-check"></i><b>6.10</b> The power to learn</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="7-probabilisation-and-the-taming-of-machines.html"><a href="7-probabilisation-and-the-taming-of-machines.html"><i class="fa fa-check"></i><b>7</b> Probabilisation and the Taming of Machines}</a><ul>
<li class="chapter" data-level="7.1" data-path="7-probabilisation-and-the-taming-of-machines.html"><a href="7-probabilisation-and-the-taming-of-machines.html#data-reduces-uncertainty"><i class="fa fa-check"></i><b>7.1</b> Data reduces uncertainty?</a></li>
<li class="chapter" data-level="7.2" data-path="7-probabilisation-and-the-taming-of-machines.html"><a href="7-probabilisation-and-the-taming-of-machines.html#machine-learning-as-statistics-inside-out"><i class="fa fa-check"></i><b>7.2</b> Machine learning as statistics inside out</a></li>
<li class="chapter" data-level="7.3" data-path="7-probabilisation-and-the-taming-of-machines.html"><a href="7-probabilisation-and-the-taming-of-machines.html#distributed-probabilities"><i class="fa fa-check"></i><b>7.3</b> Distributed probabilities</a></li>
<li class="chapter" data-level="7.4" data-path="7-probabilisation-and-the-taming-of-machines.html"><a href="7-probabilisation-and-the-taming-of-machines.html#naive-bayes-and-the-distribution-of-probabilities"><i class="fa fa-check"></i><b>7.4</b> Naive Bayes and the distribution of probabilities</a></li>
<li class="chapter" data-level="7.5" data-path="7-probabilisation-and-the-taming-of-machines.html"><a href="7-probabilisation-and-the-taming-of-machines.html#spam-when-foralln-is-too-much"><i class="fa fa-check"></i><b>7.5</b> Spam: when <span class="math inline">\(\forall{N}\)</span> is too much?</a></li>
<li class="chapter" data-level="7.6" data-path="7-probabilisation-and-the-taming-of-machines.html"><a href="7-probabilisation-and-the-taming-of-machines.html#the-improbable-success-of-the-naive-bayes-classifier"><i class="fa fa-check"></i><b>7.6</b> The improbable success of the Naive Bayes classifier</a></li>
<li class="chapter" data-level="7.7" data-path="7-probabilisation-and-the-taming-of-machines.html"><a href="7-probabilisation-and-the-taming-of-machines.html#ancestral-probabilities-in-documents-inference-and-prediction"><i class="fa fa-check"></i><b>7.7</b> Ancestral probabilities in documents: inference and prediction</a></li>
<li class="chapter" data-level="7.8" data-path="7-probabilisation-and-the-taming-of-machines.html"><a href="7-probabilisation-and-the-taming-of-machines.html#statistical-decompositions-bias-variance-and-observed-errors"><i class="fa fa-check"></i><b>7.8</b> Statistical decompositions: bias, variance and observed errors</a></li>
<li class="chapter" data-level="7.9" data-path="7-probabilisation-and-the-taming-of-machines.html"><a href="7-probabilisation-and-the-taming-of-machines.html#does-machine-learning-construct-a-new-statistical-reality"><i class="fa fa-check"></i><b>7.9</b> Does machine learning construct a new statistical reality?</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="8-patterns-and-differences.html"><a href="8-patterns-and-differences.html"><i class="fa fa-check"></i><b>8</b> Patterns and differences</a><ul>
<li class="chapter" data-level="8.1" data-path="8-patterns-and-differences.html"><a href="8-patterns-and-differences.html#splitting-and-the-growth-of-trees"><i class="fa fa-check"></i><b>8.1</b> Splitting and the growth of trees</a></li>
<li class="chapter" data-level="8.2" data-path="8-patterns-and-differences.html"><a href="8-patterns-and-differences.html#differences-in-recursive-partitioning"><i class="fa fa-check"></i><b>8.2</b> 1984: Differences in recursive partitioning</a></li>
<li class="chapter" data-level="8.3" data-path="8-patterns-and-differences.html"><a href="8-patterns-and-differences.html#limiting-differences"><i class="fa fa-check"></i><b>8.3</b> Limiting differences</a></li>
<li class="chapter" data-level="8.4" data-path="8-patterns-and-differences.html"><a href="8-patterns-and-differences.html#the-successful-dispersion-of-the-support-vector-machine"><i class="fa fa-check"></i><b>8.4</b> The successful dispersion of the support vector machine</a></li>
<li class="chapter" data-level="8.5" data-path="8-patterns-and-differences.html"><a href="8-patterns-and-differences.html#differences-blur"><i class="fa fa-check"></i><b>8.5</b> Differences blur?</a></li>
<li class="chapter" data-level="8.6" data-path="8-patterns-and-differences.html"><a href="8-patterns-and-differences.html#bending-the-decision-boundary"><i class="fa fa-check"></i><b>8.6</b> Bending the decision boundary</a></li>
<li class="chapter" data-level="8.7" data-path="8-patterns-and-differences.html"><a href="8-patterns-and-differences.html#instituting-patterns"><i class="fa fa-check"></i><b>8.7</b> Instituting patterns</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="9-regularizing-and-materializing-objects.html"><a href="9-regularizing-and-materializing-objects.html"><i class="fa fa-check"></i><b>9</b> Regularizing and materializing objects}</a><ul>
<li class="chapter" data-level="9.1" data-path="9-regularizing-and-materializing-objects.html"><a href="9-regularizing-and-materializing-objects.html#genomic-referentiality-and-materiality"><i class="fa fa-check"></i><b>9.1</b> Genomic referentiality and materiality</a></li>
<li class="chapter" data-level="9.2" data-path="9-regularizing-and-materializing-objects.html"><a href="9-regularizing-and-materializing-objects.html#the-genome-as-threshold-object"><i class="fa fa-check"></i><b>9.2</b> The genome as threshold object</a></li>
<li class="chapter" data-level="9.3" data-path="9-regularizing-and-materializing-objects.html"><a href="9-regularizing-and-materializing-objects.html#genomic-knowledges-and-their-datasets"><i class="fa fa-check"></i><b>9.3</b> Genomic knowledges and their datasets</a></li>
<li class="chapter" data-level="9.4" data-path="9-regularizing-and-materializing-objects.html"><a href="9-regularizing-and-materializing-objects.html#the-advent-of-wide-dirty-and-mixed-data"><i class="fa fa-check"></i><b>9.4</b> The advent of ‘wide, dirty and mixed’ data</a></li>
<li class="chapter" data-level="9.5" data-path="9-regularizing-and-materializing-objects.html"><a href="9-regularizing-and-materializing-objects.html#cross-validating-machine-learning-in-genomics"><i class="fa fa-check"></i><b>9.5</b> Cross-validating machine learning in genomics</a></li>
<li class="chapter" data-level="9.6" data-path="9-regularizing-and-materializing-objects.html"><a href="9-regularizing-and-materializing-objects.html#proliferation-of-discoveries"><i class="fa fa-check"></i><b>9.6</b> Proliferation of discoveries</a></li>
<li class="chapter" data-level="9.7" data-path="9-regularizing-and-materializing-objects.html"><a href="9-regularizing-and-materializing-objects.html#variations-in-the-object-or-in-the-machine-learner"><i class="fa fa-check"></i><b>9.7</b> Variations in the object or in the machine learner?</a></li>
<li class="chapter" data-level="9.8" data-path="9-regularizing-and-materializing-objects.html"><a href="9-regularizing-and-materializing-objects.html#whole-genome-functions"><i class="fa fa-check"></i><b>9.8</b> Whole genome functions</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="10-propagating-subject-positions.html"><a href="10-propagating-subject-positions.html"><i class="fa fa-check"></i><b>10</b> Propagating subject positions}</a><ul>
<li class="chapter" data-level="10.1" data-path="10-propagating-subject-positions.html"><a href="10-propagating-subject-positions.html#propagation-across-human-machine-boundaries"><i class="fa fa-check"></i><b>10.1</b> Propagation across human-machine boundaries</a></li>
<li class="chapter" data-level="10.2" data-path="10-propagating-subject-positions.html"><a href="10-propagating-subject-positions.html#competitive-positioning"><i class="fa fa-check"></i><b>10.2</b> Competitive positioning</a></li>
<li class="chapter" data-level="10.3" data-path="10-propagating-subject-positions.html"><a href="10-propagating-subject-positions.html#a-privileged-machine-and-its-diagrammatic-forms"><i class="fa fa-check"></i><b>10.3</b> A privileged machine and its diagrammatic forms</a></li>
<li class="chapter" data-level="10.4" data-path="10-propagating-subject-positions.html"><a href="10-propagating-subject-positions.html#varying-subject-positions-in-code"><i class="fa fa-check"></i><b>10.4</b> Varying subject positions in code</a></li>
<li class="chapter" data-level="10.5" data-path="10-propagating-subject-positions.html"><a href="10-propagating-subject-positions.html#the-subjects-of-a-hidden-operation"><i class="fa fa-check"></i><b>10.5</b> The subjects of a hidden operation</a></li>
<li class="chapter" data-level="10.6" data-path="10-propagating-subject-positions.html"><a href="10-propagating-subject-positions.html#algorithms-that-propagate-errors"><i class="fa fa-check"></i><b>10.6</b> Algorithms that propagate errors</a></li>
<li class="chapter" data-level="10.7" data-path="10-propagating-subject-positions.html"><a href="10-propagating-subject-positions.html#competitions-as-examination"><i class="fa fa-check"></i><b>10.7</b> Competitions as examination</a></li>
<li class="chapter" data-level="10.8" data-path="10-propagating-subject-positions.html"><a href="10-propagating-subject-positions.html#superimposing-power-and-knowledge"><i class="fa fa-check"></i><b>10.8</b> Superimposing power and knowledge</a></li>
<li class="chapter" data-level="10.9" data-path="10-propagating-subject-positions.html"><a href="10-propagating-subject-positions.html#ranked-subject-positions"><i class="fa fa-check"></i><b>10.9</b> Ranked subject positions</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="11-conclusion-out-of-the-data.html"><a href="11-conclusion-out-of-the-data.html"><i class="fa fa-check"></i><b>11</b> Conclusion: Out of the Data}</a><ul>
<li class="chapter" data-level="11.1" data-path="11-conclusion-out-of-the-data.html"><a href="11-conclusion-out-of-the-data.html#machine-learners"><i class="fa fa-check"></i><b>11.1</b> 250,000 machine learners</a></li>
<li class="chapter" data-level="11.2" data-path="11-conclusion-out-of-the-data.html"><a href="11-conclusion-out-of-the-data.html#a-summary-of-the-argument"><i class="fa fa-check"></i><b>11.2</b> A summary of the argument</a></li>
<li class="chapter" data-level="11.3" data-path="11-conclusion-out-of-the-data.html"><a href="11-conclusion-out-of-the-data.html#in-situ-hybridization"><i class="fa fa-check"></i><b>11.3</b> In-situ hybridization</a></li>
<li class="chapter" data-level="11.4" data-path="11-conclusion-out-of-the-data.html"><a href="11-conclusion-out-of-the-data.html#critical-operational-practice"><i class="fa fa-check"></i><b>11.4</b> Critical operational practice?</a></li>
<li class="chapter" data-level="11.5" data-path="11-conclusion-out-of-the-data.html"><a href="11-conclusion-out-of-the-data.html#obstacles-to-the-work-of-freeing-machine-learning"><i class="fa fa-check"></i><b>11.5</b> Obstacles to the work of freeing machine learning</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="12-references.html"><a href="12-references.html"><i class="fa fa-check"></i><b>12</b> References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning: Archaeology of a Data Practice</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="patterns-and-differences" class="section level1">
<h1><span class="header-section-number">8</span> Patterns and differences</h1>
<p></p>
<blockquote>
<p>The notion of pattern involves the concept of different modes of togetherness <span class="citation">(Whitehead <a href="#ref-Whitehead_1956">1956</a>, 195–6)</span>.  </p>
</blockquote>
<blockquote>
<p>Algorithms for pattern recognition were therefore from the very beginning associated with the construction of linear decision surfaces <span class="citation">(Cortes and Vapnik <a href="#ref-Cortes_1995">1995</a>, 273–4)</span> </p>
</blockquote>
<p>Do machine learners generate new patterns of difference? Should we hold machine learners accountable for their claims to recognise patterns in data in the same way we hold experimental scientists accountable for their factual claims?<a href="#fn63" class="footnoteRef" id="fnref63"><sup>63</sup></a>  This chapter explores two major machine learning treatments of pattern dating from the last decades of the twentieth century from the standpoint of differences. I suggest that what counts as pattern changes in machine learning over time. While much machine learning strains to identify differences in terms of differences of degree, the practice of pattern-finding itself harbours differences of kind.  For critical thought, the connection between pattern and differences is particularly important, since if machine learning changes what counts as pattern, this will also affect the recognition or articulation of differences.  We have seen the emergence of the vector space and its vectorised transformations, the multiplication of operational functions and their associated partial observers, and then the probabilisation that distributes machine learners into populations of error-sensitive learners. What in this diagram and in the forest-like growth of techniques, projects, applications and proponents, allows us to make sense of what happens to differences in machine learning?</p>
<p>Across vectors, functions and populations, the diagram of machine learning weaves and knots many points of emergence, continuity and conjunction. I view the formidable accumulations of infrastructure, devices and expertise accrediting around machine learning as multi-faceted abstractions, where abstraction is understood diagrammatically as a concretising entanglement of references.  Three highly developed and heavily used machine learners – decision trees, support vector machines and neural nets – more or less mesmerised machine learning between 1980-2000. They initiated relatively novel and somewhat heterogeneous diagrammatic movements into data.    These diagrammatic movements, which we might characterise as <em>splitting</em>, and <em>marginalising</em> not only animate subsequent machine learners in producing newer techniques, they re-configure what counts as pattern.  Since machine learning has no fixed idea of pattern (a term lacks much operational definition), then claims that machine learners uncover hidden patterns in data might be better grounded in the operational practices of working with differences.<a href="#fn64" class="footnoteRef" id="fnref64"><sup>64</sup></a></p>
<p>As machine learners of recent decades, the decision tree and support vector machine embody a new , a way of describing, locating and perceiving differences in which differences of degree and differences of kind are re-mapped.  Every machine learner generates statements, but from different places, by somewhat different individuals, and from the different situations they ‘occupy in relation to the various domains or groups of objects’ <span class="citation">(Foucault <a href="#ref-Foucault_1972">1972</a>, 52)</span>.<a href="#fn65" class="footnoteRef" id="fnref65"><sup>65</sup></a> Practically, decision trees and support vector machines loom large in various contemporary accounts of machine learning as a way of knowing (for instance, in popular machine learning books such as <em>Machine Learning for Hackers</em> <span class="citation">(Conway and White <a href="#ref-Conway_2012">2012</a>)</span> or <em>Doing Data Science</em> <span class="citation">(Schutt and O’Neil <a href="#ref-Schutt_2013">2013</a>)</span>). The machine learning research published in statistics, computer science, mathematics, artificial intelligence and a swathe of related scientific fields during 1980-2010 bristles with references to decision trees and support vector machine, as well as neural networks.<a href="#fn66" class="footnoteRef" id="fnref66"><sup>66</sup></a></p>
<p>Rather than seeing pattern as something discovered in data, the notion of enunciative modality suggests we should examine the diagrammatic operations that configure differences in the practice of machine learning, giving rise to a field of patterns attributed to objects or subject positions. The two machine learners that anchor this chapter are perhaps the most distinctive data mining, pattern recognition and predictive modelling achievements of the late twentieth century (at least judging by the citations and usage they attract). They differ greatly in how they move through data. At certain times, they come together (for instance, in machine learning competitions discussed in chapter <a href="#ch:subjects"><strong>??</strong></a>; or in certain formalizations such as machine learning theory or in graphs of the bias-variance decomposition discussed in chapter ; or in the pedagogy of machine learning discussed in chapter ).</p>
<div id="splitting-and-the-growth-of-trees" class="section level2">
<h2><span class="header-section-number">8.1</span> Splitting and the growth of trees</h2>
<blockquote>
<p>Mastering the details of tree growth and management is an excellent way to understand the activities of learning machine generally <span class="citation">(Malley, Malley, and Pajevic <a href="#ref-Malley_2011">2011</a>, 118)</span>. </p>
</blockquote>
<p>Decision trees promise an understanding of machine learning.  The enunciative modality of the decision tree concerns the observability and comprehensibility of machine learning.  As we will see, not all machine learners readily support observation or comprehension. The cost of comprehensibility, however, is a certain highly restricted framing of differences in transforming. As <em>Elements of Statistical Learning</em> puts it: ‘tree-based methods partition the feature space into a set of rectangles, and then fit a simple model (like a constant) in each one. They are conceptually simple yet powerful’ <span class="citation">(Hastie, Tibshirani, and Friedman <a href="#ref-Hastie_2009">2009</a>, 305)</span>. </p>

<p>Tree-based methods are supervised learners as they require the data to either be labelled with a class or to have some outcome value.  The variable types in the feature space (or vector space of the data)  can be mixed. Because the method cuts the vector space into a tiled surface (see figure <a href="#fig:rpart"><strong>??</strong></a>), the features or data variables can be continuous or discontinuous. The ‘simple models’ that tree methods construct each define one of the rectangular regions or partitions of the feature space. In Figure <a href="#fig:rpart"><strong>??</strong></a>, the different regions or partitions produced by a decision tree are labelled <span class="math inline">\(R1, R2\)</span> etc.</p>

<p> Work on classification and regression techniques using decision trees goes back to the early 1960s when social scientists James Morgan and John Sonquist at the University of Michigan’s Institute for Social Research were attempting to analyse increasingly large social survey datasets <span class="citation">(Morgan and Sonquist <a href="#ref-Morgan_1963">1963</a>)</span>. As Dan Steinberg describes in his brief history of decision trees <span class="citation">(Steinberg and Colla <a href="#ref-Steinberg_2009">2009</a>, 180)</span>, the ‘automatic interaction detector’ (<code>AID</code>) as it was known, sought to automate the practice of data analysts looking for interactions between different variables. The variety and sheer optimism of subsequent applications of these prototype decision tree techniques is striking. In the 1960s and 1970s, papers that drew on the AID paper or use AID techniques can be found, as table  shows, in education, politics, economics, population control, advertising, mass media and family planning.  </p>

<p>A decade after the initial work, <code>AID</code> was the object of trenchant criticism by statisticians and others, not for the classifications it used (see Figure @ref(fig:aid_tree)), but for its ‘pure’ empiricism. Writing in the 1970s, statisticians in the behavioural sciences such as Hillel Einhorn at the University of Chicago castigated the use of such techniques. The criticisms stemmed from a general distrust of ‘purely empirical methods’, and scepticism focused on their positivity:</p>
<blockquote>
<p>The purely empirical approach is particularly dangerous in an age when computers and packaged programs are readily available, since there is temptation to substitute immediate empirical analysis for more analytic thought and theory building. It is also probably too much to hope that a majority of researchers will take the time to find out how and why a particular program works. The chief interest will continue to be in the output-the results-with as little delay as possible <span class="citation">(Einhorn <a href="#ref-Einhorn_1972">1972</a>, 368)</span> </p>
</blockquote>
<p>Einhorn discusses AID alongside other techniques such as factor analysis and multi-dimensional scaling (both still widely used) before concluding ‘it should be clear that proceeding without a theory and with powerful data analytic techniques can lead to large numbers of Type I errors’ <span class="citation">(Einhorn <a href="#ref-Einhorn_1972">1972</a>, 378)</span>. His statistical objections to AID are particularly focused on the problematic power of the technique: ‘it may make sense out of “noise”’ (369). Consequently, researchers easily misuse the technique: they ‘overfit’ the data, and do not pay enough attention to issues of validation (369-370). Similarly the British marketing researcher Peter Doyle, criticising the use of AID in assessing store performance and site selection by operations researchers, complained that searching for patterns in data using data sets was bound to lead to spurious results and the decision trees, although intuitively appealing (that is, they could be easily interpreted), were afflicted with arbitrariness: ‘a second variable may be almost as discriminating as the one chosen, but if the program is made to split on this, quite a different tree occurs’ <span class="citation">(Doyle <a href="#ref-Doyle_1973">1973</a>, 465–66)</span>.<a href="#fn67" class="footnoteRef" id="fnref67"><sup>67</sup></a>   Most of these criticisms can be seen as expressing conventional statistical caution in response to threats to validity, but they also address the core issues of pattern and difference: did the trees render differences arbitrarily? </p>
</div>
<div id="differences-in-recursive-partitioning" class="section level2">
<h2><span class="header-section-number">8.2</span> 1984: Differences in recursive partitioning</h2>
<p>As Einhorn expected, it was too much to hope that all researchers would take time to investigate how a particular program works. Some researchers however did take time in the following decade to investigate how decision trees work. Writing around 2000, Hastie, Tibshirani and Friedman, who could hardly by accused of not understanding decision trees, happily recommend decision trees as the best ‘off-the-shelf’ classifier: ‘of all the well-known learning methods, decision trees comes closest to meeting the requirements for serving as an off-the-shelf procedure for data-mining’ <span class="citation">(Hastie, Tibshirani, and Friedman <a href="#ref-Hastie_2009">2009</a>, 352)</span>. We might wonder here, however, whether they damn with faint praise, since ‘off-the-shelf’ suggests pre-packaged, and commodified, and the term ‘data-mining’ itself is not without negative connotations. As for its commercial realities, in 2013, Salford Systems, the purveyors of the leading contemporary commercial decision tree software, <code>CART</code>, could claim:</p>
<blockquote>
<p>CART is the ultimate classification tree that has revolutionized the entire field of advanced analytics and inaugurated the current era of data mining. CART, which is continually being improved, is one of the most important tools in modern data mining. Others have tried to copy CART but no one has succeeded as evidenced by unmatched accuracy, performance, feature set, built-in automation and ease of use. <a href="http://www.salford-systems.com/products/cart">Salford Systems</a> </p>
</blockquote>
<p>What happened between 1973 and 2013? Decision trees somehow stepped out of the statistically murky waters of social science departments and business schools in the early 1970s to inaugurate the ‘current era of data mining’ (which the scientific literature indicates starts in the early 1990s).  This was not only a commercial innovation. As the earlier citation from U.S. National Institutes of Health biostatisticians Malley, Malley and Pajevic indicates, decision trees enjoy high regard even in biomedical research, a setting where statistical rigour is highly valued for life and death reasons. The happy situation of decision trees four decades on suggests some kind of threshold was crossed in which the epistemological, statistical, or algorithmic (‘built-in automation’) power of the technique altered substantially. </p>
<p>The third author of <em>Elements of Statistical Learning</em>, Jerome Friedman, worked at the U.S. Department of Energy’s Stanford Linear Accelerator during the late 1970s. Friedman was instrumental in rescuing decision trees from the ignominy of profligate ease of use and pure empiricism they had endured since the late 1960s.  The reorganisation and statistical retrofitting of the decision tree was not a single or focused effort. During the 1980s, statisticians such as Friedman and Leo Breiman renovated the decision tree as a statistical tool <span class="citation">(Breiman et al. <a href="#ref-Breiman_1984">1984</a>)</span>. At the same time, computer scientists such as Ross Quinlan in Sydney were re-implementing decision trees guided by an artificial intelligence-based formalisation as rule-based induction technique <span class="citation">(Quinlan <a href="#ref-Quinlan_1986">1986</a>)</span>.<a href="#fn68" class="footnoteRef" id="fnref68"><sup>68</sup></a>   This uneasy parallel effort between computer science and statistics still somewhat strains relations in machine learning today. Statisticians and computer scientists do and use the same techniques, but often with the computer scientists focusing on optimisation and algorithmic scale and the statisticians inventing novel statistical formalizations and abstractions. The fateful embrace of statistics and computer science, the disciplinary binary that vectorizes machine learning, has been generative in the retrieval of the decision tree. </p>
<p>An initial symptom of the transformation of the technique appears in a name change. The term ‘decision tree,’ although still widely used in the research literature and machine learner parlance was supplanted by ‘classification and regression tree’ during the late 1970s and 1980s. The terms ‘classification and regression tree’ is sometimes contracted to ‘CART,’ and that term strictly speaking refers to a computer program described in <span class="citation">(Breiman et al. <a href="#ref-Breiman_1984">1984</a>)</span> as well as the title of that highly-cited monograph, <em>Classification and Regression Trees</em>.   As we have seen in previous chapters, classification and regression (predictive modelling using estimates of relations between variables)  stage the two main sides of machine learning practice. Their concatenation with ‘tree’ attests to a renovation of existing machine learning approaches behind a single facade.</p>
<p>The implementation of machine learning techniques in <code>R</code> accentuates the statistical side of decision tree practice, but that has certain forensic virtues not offered by commercial or closed-source software often produced by computer scientists. The name of one long-standing and widely-used <code>R</code> package itself attests to something: <code>rpart</code> is a contraction of ‘recursive partitioning’ and this term generally describes how the decision tree algorithm works to partition the vector space into the form shown in Figure <a href="#fig:rpart"><strong>??</strong></a> <span class="citation">(Therneau, Atkinson, and Ripley <a href="#ref-Therneau_2015">2015</a>)</span>.  ‘CART,’ on the other hand, is a registered trademark of Salford Systems, the software company mentioned above, who sell the leading commercial implementation of classification and regression trees. Hence, the <code>R</code> package <code>rpart</code> cannot call itself the more obvious name <code>cart,</code> and instead invokes the underlying algorithmic process: recursive partitioning.<a href="#fn69" class="footnoteRef" id="fnref69"><sup>69</sup></a>  </p>

<p>R.A. Fisher’s <code>iris</code> dataset, which contains 150 measurements made in the 1930s of petal and sepal lengths of <em>iris virginica, iris setosa</em> and <em>iris versicolor</em> is a standard instructional example for decision trees <span class="citation">(Fisher <a href="#ref-Fisher_1938">1938</a>)</span>.<a href="#fn70" class="footnoteRef" id="fnref70"><sup>70</sup></a>  The code shown here loads the <code>iris</code> data (the dataset is routinely installed with many data analysis tools), loads the <code>rpart</code> decision tree library, and builds a decision to classify the irises by species. What has happened to the iris data in this decision tree? The <code>R</code> code that invokes the recursive partitioning algorithm is so brief <code>iris_tree =rpart(Species ~ ., iris)</code> that we can’t tell much about how the data has been ‘recursively partitioned.’ We know that the <em>iris</em> has 150 rows, and that there are equal numbers of the three iris varieties.</p>
<p>Code brevity indicates a great deal of formalization of practice has accrued around decision trees.  Some of this formalization was described in the landmark <em>Classification and Regression Trees</em> monograph <span class="citation">(Breiman et al. <a href="#ref-Breiman_1984">1984</a>)</span>. Classification in decision trees operates by splitting each of the dimensions of vector space into two parts (as we saw in figure <a href="#fig:rpart"><strong>??</strong></a>). These splits institute branches along which differences are hierarchically ordered in a tree structure. The recursive splitting algorithm draws a diagram of hierarchical differences. The problem here is that many splits are possible. What is a good split or ordering of differences? </p>
<blockquote>
<p>The first problem in tree construction is how to use <span class="math inline">\(\mathcal{L}\)</span> to determine the binary splits of <span class="math inline">\(\mathcal{X}\)</span> into smaller pieces. The fundamental idea is to select each split of a subset so that the data in each of the descendant subsets are “purer” than the data in the parent subset <span class="citation">(Breiman et al. <a href="#ref-Breiman_1984">1984</a>, 156:23)</span>.</p>
</blockquote>
<p>Tree construction hinges on the notion of purity or more precisely ‘node impurity’, a function that measures to what extent data labelled as belonging to different classes are mixed together at a given branch or node in a decision tree: ‘that is, the node impurity is largest when all classes are equally mixed together in it, and smallest when the node contains only one class’ <span class="citation">(Breiman et al. <a href="#ref-Breiman_1984">1984</a>, 156:24)</span>. As Malley and co-authors note, ‘the collection of purity measures is still a subject of research’ <span class="citation">(Malley, Malley, and Pajevic <a href="#ref-Malley_2011">2011</a>, 123)</span>, but Breiman, Friedman, Olshen and Stone promoted a particular form of impurity measure for classification trees known as ‘Gini index of diversity’ <span class="citation">(Breiman et al. <a href="#ref-Breiman_1984">1984</a>, 156:38)</span>.  Like the planar decision surface used in classifiers such as the perceptron or linear regression model, recursive partitioning combined with measures of node impurity transforms data by cuts or divides. Whereas in linear model-based machine learners, the intuition motivating the function-finding or learning was ‘find the line that best expresses the distribution of the data,  here the intuition is more like: ’find the cuts that minimize mixing’. Good splits decrease the level of impurity in the tree. In a tree with maximum purity, each terminal node – the nodes at the base of the tree – would contain a single class.</p>
<div class="figure">
<img src="_main_files/figure-html/iris_tree_plot-1.png" alt="Decision tree on _iris_ dataset" width="1152" />
<p class="caption">
(#fig:iris_tree_plot)Decision tree on <em>iris</em> dataset
</p>
</div>

<p>In Figure @ref(fig:iris_tree_plot), the plot on the left shows the decision tree and the plot on the right shows just <em>setosa</em> and <em>versicolor</em> plotted by petal and sepal widths and lengths. Decision trees are read from the top down, left to right. The top level of this tree can be read, for instance, as saying, if the length of petal is less 2.45, then the iris is <em>setosa</em>.  As the plot on the right shows, most of the measurements are well clustered. Only the <em>setosa</em> petal lengths and widths seem to vary widely. All the other measurements are tightly bunched. A decision tree has little trouble ordering differences between species of iris.</p>
<p>Like logistic regression models, neural networks, support vector machines or any other machine learning technique, decision trees order differences in terms of specific qualities and logics. Recursive partitioning split and sub-divide the vector space to capture every minor difference between cases, and thereby achieve a ever-closer fit to the individual or sub-individual variations.  Although the partitioning or splitting rules have strong statistical justifications, they do not at all eliminate the problem of instability or variance in trees.  For instance, they easily end up ‘overfitting’ the data. Overfitting is a problem for all machine learning techniques. Algorithms sometimes find it hard to know when to stop identifying differences. During construction of a decision tree, recursive partitioning splits features in the data into smaller and smaller groups. ‘The goodness of the split’, wrote Breiman and co-authors, ‘is defined to be the decrease in impurity’ <span class="citation">(Breiman et al. <a href="#ref-Breiman_1984">1984</a>, 156:25)</span>. Under this definition of goodness, the terminal nodes or leaves of the tree can end up containing a single case, or a single class of cases. </p>
<p>The decision tree targets the differences of the individual case to such a degree that it could end up seeing categorical differences everywhere. Operating to maximise the purity of the partitions it creates, it leans too heavily on data it has been trained on to see relevant similarities when fresh data appears. Trees that branch too much are sensitive to differences and generalize poorly (that is, they suffer from generalization error \index{error!generalization}). Such a model will almost always <em>overfit</em>, since slight variations in the values of variables in a fresh case are likely to yield widely differing predictions. In the terminology of machine learning, such a decision tree may have low bias but high variance.  </p>
</div>
<div id="limiting-differences" class="section level2">
<h2><span class="header-section-number">8.3</span> Limiting differences</h2>
<p>Given this problem of unstable difference, much of the development of decision tree did not revolve around how to construct them, but how to limit their growth so as to manage tensions between pure but unstable differences and impure but stable classification. As <em>Elements of Statistical Learning</em> puts the problem in its account of classification and regression trees:</p>
<blockquote>
<p>How large should we grow the tree? Clearly a very large tree might overfit the data, while a small tree might not capture the important structure. Tree size is a tuning parameter governing the model’s complexity, and the optimal tree size should be adaptively chosen from the data. One approach would be to split tree nodes only if the decrease in sum-of-squares due to the split exceeds some threshold. This strategy is too short-sighted, however, since a seemingly worthless split might lead to a very good split below it. The preferred strategy is to grow a large tree <span class="math inline">\(T_0\)</span>, stopping the splitting process only when some minimum node size (say 5) is reached. Then this large tree is pruned using cost-complexity pruning <span class="citation">(Hastie, Tibshirani, and Friedman <a href="#ref-Hastie_2009">2009</a>, 307–8)</span>.</p>
</blockquote>
<p>Growing a maximum decision tree, and then cutting back its branches using a cost-function  optimises the decision tree as a machine learner.   ‘Cost complexity pruning’ extends the optimisation we have already discussed in relation to linear regression and logistic regression models (in chapter <a href="#ch:function"><strong>??</strong></a>). As in these techniques, the definition of a cost function controlling the ‘complexity’ of a tree – how many branches and leaves/nodes it contains, combined with measures of how well it classifies or predicts – iteratively observes and tests different versions of tree against each other. ‘We define the cost complexity criterion,’ write Hastie and co-authors, as:</p>

<p>The idea is ‘to find, for each <span class="math inline">\(\alpha\)</span>, the subtree <span class="math inline">\(T_\alpha \subseteq T_0\)</span> to minimize <span class="math inline">\(C_\alpha(T)\)</span>’ <span class="citation">(Hastie, Tibshirani, and Friedman <a href="#ref-Hastie_2009">2009</a>, 308)</span>. For present purposes, we need only recognise that the cost complexity function re-configures a large decision tree (<span class="math inline">\(T_0\)</span> in equation ) by cutting or pruning it back through optimisation that balances between the complexity of the tree and its stability. Tree construction is as an optimisation problem, in which the variation of a parameter (<span class="math inline">\(\alpha\)</span>) allows minimization of a derived value (the cost <span class="math inline">\(C_\alpha\)</span>). </p>
<p>While the graphic form of the decision tree was, by virtue of the long-standing diagrammatic practice of tree-drawing, easy to interpret, observation of decision trees had no way of gauging the instability or variability of any given tree.  Hastie and co-authors write: ’one major problem with trees is their high variance. Often a small change in the data can result in a very different series of splits, making interpretation somewhat precarious. The major reason for this instability is the hierarchical nature of the process: the effect of an error in the top split is propagated down to all of the splits below it <span class="citation">(Hastie, Tibshirani, and Friedman <a href="#ref-Hastie_2009">2009</a>, 312)</span>. The very diagrammatic form that allows decision trees to be observed and interpreted is also the source of their instability. Regardless of this instability, the diagrammatic composition of the tree through splitting and pruning negotiates between two different ways of doing difference.</p>
<p>The shift from <code>AID</code> to <code>CART</code> enunciates a change in how patterns of difference become visible. The decision tree algorithm superimposes recursive partitioning and cost-complexity pruning to configure a mode of enunciation of differences. It creates a new rules of differentiation of individuals, facts, things and relations.[^5.101] Differences in a decision tree – the combination of purity and density that comes from recursive partitioning and cost-complexity pruning – re-configure what counts as pattern. </p>
<p>Decision trees have been heavily used in credit risk assessment as well as many biomedical models. Does their popularity stem from the legibility of the statements they produce, even if those patterns prove unstable? Or is the success of the decision tree perhaps better understood as a change in the differentiation of patterns more generally, their mode of enunciation, in which case, decision trees would only be one instance among many?  If we understand machine learners as generating populations of statements, the transformation and re-modelling of the decision tree as classification and regression trees suggests a subtle, non-localizable discontinuity. The later development of the decision tree and its subsequent transmogrification into random forests <span class="citation">(Breiman <a href="#ref-Breiman_2001">2001</a><a href="#ref-Breiman_2001">b</a>)</span>,  that grow a myriad of small decision trees disperses kaleidoscopic fragments of classificatory order with only partial or provisional stabilisation in visible pattern. In such developments – and we could also consider here techniques, models and methods of ‘boosting,’ ‘bagging,’ or the ‘ensemble learning’ that conducts ‘supervised search in a high-dimensional space of weak learners’ <span class="citation">(Hastie, Tibshirani, and Friedman <a href="#ref-Hastie_2009">2009</a>, 603)</span>, pattern has an operational rather than visible mode of togetherness. </p>
</div>
<div id="the-successful-dispersion-of-the-support-vector-machine" class="section level2">
<h2><span class="header-section-number">8.4</span> The successful dispersion of the support vector machine</h2>
<p>In growing and pruning decision trees, and even more markedly in support vector machine, patterns play out in dispersion and discontinuity rather than in regular geometry.  While machine learners order differences, that ordering becomes increasingly difficult to see as it is dispersed. Take the case of the support vector machine. The second most highly cited reference in the last few decades of machine learning literature is a paper from 1995 by Corinna Cortes and Vladimir Vapnik of AT &amp; T Bell Labs in New Jersey, USA entitled ‘Support Vector Networks’ <span class="citation">(Cortes and Vapnik <a href="#ref-Cortes_1995">1995</a>)</span>. Few women’s names appear prominently in the machine learning literature.    The computing science and statistics departments at Stanford and Berkeley, the laboratories at Los Alamos and AT&amp;T Bell between the 1960s and the 1980s were, it seems, not overly popular or populated with women scientists and engineers. Some prominent machine learning researchers at the time of writing are women (I return to this in chapter <a href="#ch:subjects"><strong>??</strong></a>), but Cortes is perhaps pre-eminent both as head of Google Research in New York (2014) and as recipient with Vapnik of an Association for Computing Machine award in 2008 for work on the support vector machine algorithm.<a href="#fn71" class="footnoteRef" id="fnref71"><sup>71</sup></a>   </p>

<p>The rapid rise to popularity of the support vector machine can be seen in the machine learning research literature, a very small slice of which appears in Table @ref(tab:svm_lit). A substantial fraction of the overall research publication since the mid-1990s accumulates around this single technique, and as usual ranges across credit analysis, land cover prediction, protein structures, brain states and face recognition. The support vector machine spans the normal biopolitical triangle of life, labour and language. The influence of the technique can also be seen in overlapping fields such as pattern recognition and data mining, where <span class="citation">(Cortes and Vapnik <a href="#ref-Cortes_1995">1995</a>)</span> and similar papers rank near the top-cited papers.<a href="#fn72" class="footnoteRef" id="fnref72"><sup>72</sup></a> This kind of growth betokens high levels of interest, identification and investment on the part of the researchers, and presumably more widely.</p>

<p>I suggested above that the classification tree (and then random forests) illustrate an enunciative modality anchored in a tension between recursively partitioned differences and classificatory stability.  The support vector machine shown in Figure @ref(fig:svm_iris) demonstrates a different change in what counts as pattern. The decision boundaries in the sub-graphs have different contours, contours that suggest a more mobile construction. While the name ‘support vector machine’ is somewhat forbiddingly technical compared to more familiar terms such as ‘decision tree’ or even ‘neural network,’ the underlying intuition of the technique is much older, and can be found in the models developed by the British statistician R. A. Fisher during the 1930s. Fisher developed the ‘first pattern recognition algorithm’ <span class="citation">(Cortes and Vapnik <a href="#ref-Cortes_1995">1995</a>, 273)</span>, the ‘linear discriminator function’ <span class="citation">(Fisher <a href="#ref-Fisher_1936">1936</a>)</span>, to deal with problems of classification, and demonstrated its efficacy on the taxonomic problem of discriminating or classifying the irises observed in W.E. Anderson’s irises in <code>iris</code> dataset (see above).   </p>
<p>In his 1936 article in the <em>Annual Review of Eugenics</em>, Fisher comments on similar classification work carried out in craniometry and other related settings: so-called ‘discriminant functions’ had been successfully used to distinguish populations. Fisher wrote: ‘when two or more populations have been measured in several characters, … special interest attaches to certain linear functions of measurements by which the populations are best discriminated’ <span class="citation">(Fisher <a href="#ref-Fisher_1936">1936</a>, 179)</span>.  The discriminant functions divide the vector space into ‘a collection of regions labeled according to classification’ <span class="citation">(Hastie, Tibshirani, and Friedman <a href="#ref-Hastie_2009">2009</a>, 101)</span>. ‘Decision boundaries’ (or sometimes ‘decision surfaces’  often appear as straight lines that divide the vector space into regions of constant classification.  These long-standing discriminant functions were reconstructed during the 1990s in the form of the support vector machine, giving rise to new statements about differences, statements that can be glimpsed in table @ref(tab:svm_lit) in the range of things, facts and beings running through the titles of the papers.</p>
</div>
<div id="differences-blur" class="section level2">
<h2><span class="header-section-number">8.5</span> Differences blur?</h2>
<p>Decision boundaries change in two ways in support vector machines. They blur and bend, again affecting what counts as pattern.  The support vector machine addresses the problem of how to model differences when differences are blurred. An oft-repeated illustration of how the support vector machine transforms data appears in Cortes and Vapnik’s initial publication simply entitled ‘Support Vector Networks’ <span class="citation">(Cortes and Vapnik <a href="#ref-Cortes_1995">1995</a>)</span>. They demonstrate how the support vector machine classifies handwritten digits drawn from a dataset supplied by the US Postal Service <span class="citation">(LeCun and Cortes <a href="#ref-LeCun_2012">2012</a>)</span>. Like <code>iris</code>, the US Postal Service digits and a larger version from the US National Institute of Standard (<code>mnist</code>) are standard machine learning dataset. They have been frequently used to measure the performance of competing learning algorithms.  In contrast to <code>iris</code>, the <code>mnist</code> is high dimensional. Each digit in the dataset is stored as a 16x16 pixel image. Image classification typically treats each pixel as a feature or variable in the input space. So each digit as represented by 16x16 pixels amounts to a 256 dimensional input space. By comparison, <code>iris</code> has five dimensions. Unsurprisingly, there are also many more digits in the US Postal Service Database than in flowers in <code>iris</code>. The <code>mnist</code> dataset has around 70,000. Aside from this dimensional growth, the handwritten digits aptly convey the blurring of differences. On the one hand, many people can easily recognise slight variations in handwritten digits with few errors. This is despite the many variations in handwriting that skew, morph and distort the ideal graphic forms of numbers.<a href="#fn73" class="footnoteRef" id="fnref73"><sup>73</sup></a></p>

<p>In their experiments with digit recognition (shown in figure @ref(fig:postal_digits), Cortes and Vapnik contrast the error rates of decision trees (<code>CART</code> and <code>C4.5</code>), neural networks and the support vector machine working at various level of dimensionality. Support vector machines deal with blurred differences or continuous variations by superimposing two operations: ‘soft margins’ and ‘kernelisation.’ Nearly all expositions of the support vector machine including <span class="citation">(Cortes and Vapnik <a href="#ref-Cortes_1995">1995</a>)</span> highlight the ‘soft margin’ that runs in parallel to the solid decision boundary. The support vector machine develops Fisher’s linear discriminant analysis since it searches for a separating hyperplane in the data.  While linear discriminant analysis constructs a hyperplane by finding the most likely linear boundary between classes based on all the data, the support vector machine searches for a hyperplane resting only on those cases in the data that lie near the boundary. It introduces the intuition that the best hyperplane differentiating classes will run near to the cases – the <em>support vectors</em> – that are most difficult to classify. Hard-to-classify cases become the ‘support vectors’  whose relative proximities tilt the decision surface in various directions. In contra-distinction to the <span class="math inline">\(N=\forall{\boldsymbol{X}}\)</span> proposition (discussed in chapter <a href="#ch:probability"><strong>??</strong></a>), the machine learner discards much of the data. In contrast to linear discriminant analysis, as Ethem Alpayadin writes, ‘we do not care about correctly estimating the densities [probability distributions of variables] inside class regions; all we care about is the correct estimation of the <em>boundaries</em> between the class regions’ <span class="citation">(Alpaydin <a href="#ref-Alpaydin_2010">2010</a>, 210)</span>.  </p>
<p><img src="_main_files/figure-html/svm_margins-1.pdf" width="1152" /></p>

<p>Figure @ref(fig:svm_margins) has appeared in many slight variations in the last two decades. Such figures diagram classes by different point shapes, and the diagrammatic work of the classifier takes the shape of diagonal lines, the solid line marking the decision surface or hyperplane and the dotted lines marking the soft margins that separate the two classes. In Figure @ref(fig:svm_margins), the dotted lines represent a margin on either side of a hyperplane (the solid line). The support vector machine finds the hyperplane for which that margin or perpendicular distance between the margins is greatest. Of all the slightly different planes that might run between the two classes shown in that figure, the maximum separating hyperplane lies at the greatest distance from all the points of the different classes. The support vector classifier modifies the idea of the optimal separating hyperplane by accommodating inseparable or overlapping classes. This is something that other machine learners (for instance, the perceptron) cannot do. </p>
<p>While the geometrical intuition here is that some data points (cases or observations) will lie on the opposite side of the decision surface to where they should be, the distance they lie on the wrong side of the separating hyperplane will be as small as possible. How are the lines showing in Figure @ref(fig:svm_margins) calculated? Locating the optimal separating hyperplane and a limited number of permitted mis-classifications presents a complicated optimisation problem. As <em>Elements of Statistical Learning</em>, following Cortes and Vapnik’s formulations, formalizes it, the problem can be stated in terms of minimization:</p>

<p><span class="citation">(Hastie, Tibshirani, and Friedman <a href="#ref-Hastie_2009">2009</a>, 420)</span></p>
<p>In equation , the optimisation problem is to minimize <span class="math inline">\(L_P\)</span>, the Lagrange primal function with respect to <span class="math inline">\(\beta, \beta_0\)</span> and <span class="math inline">\(\xi_i\)</span> <span class="citation">(Hastie, Tibshirani, and Friedman <a href="#ref-Hastie_2009">2009</a>, 420)</span>.  In this complicated optimisation problem (one that is difficult to understand without extensive mathematical background), familiar elements include the parameters <span class="math inline">\(\beta\)</span> in the linear form of <span class="math inline">\(x_i^T\beta + \beta_0\)</span>, which is the equation defining the hyperplane, as well as triple operations of addition (<span class="math inline">\(\sum\)</span>) of all the values <span class="math inline">\(\xi_i\)</span>, which calculate the distance that each case is on the wrong side of the margin. Correctly classified cases will, therefore, have <span class="math inline">\(x_i =0\)</span>.</p>
<p>As always with mathematical functions, their diagrammatic relations, and the way in which they contain both the generalizing regularities (algebraic icons and indexes, the linear equation, the repeated summation of all values, the shaping parameters) and forms of variation (the presence of the misclassification measures <span class="math inline">\(\xi_i\)</span>) should focus our attention. What if elements that lie on the wrong side of the hyperplane were allowed? If that were possible, the support vector machine could deal with in-separable or overlapping classes, and hence with blurred patterns of difference. Given that support vector machine permits instances that lie on the wrong side of the separating hyperplane, irregular differences no longer function as errors (as they would appear in most linear classifiers such as linear discriminant analysis and logistic regression), but as elements in a ‘soft margin’ designed to accommodate inseparability and indistinctness. Equations such as equation  connect the diagrammatic intuition of a separating hyperplane with a set of steering movements controlled by parameters such as <span class="math inline">\(C\)</span>, which effectively controls the size of the margin, and <span class="math inline">\(\alpha\)</span>, which effectively bounds the proportion by which a predicted instance can be on the wrong side of the margins that define the hyperplane. In other words, as we have seen previously in cost-function optimisation (see chapter <a href="#ch:function"><strong>??</strong></a>), the learning in the machine consists in finding a way of transforming data into differences according to constraints. </p>
</div>
<div id="bending-the-decision-boundary" class="section level2">
<h2><span class="header-section-number">8.6</span> Bending the decision boundary</h2>
<p>The support vector machine reinstates a linear decision boundary as the enunciative mode of difference. Yet it transforms that boundary. In the abstract of their 1995 paper, Cortes and Vapnik briefly describe the how the support vector machine revises the linear decision surface:</p>
<blockquote>
<p>The support-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed <span class="citation">(Cortes and Vapnik <a href="#ref-Cortes_1995">1995</a>, 273)</span> </p>
</blockquote>
<p>Another example of vector space transformation (discussed in chapter <a href="#ch:vector"><strong>??</strong></a>), this ‘very high-dimension feature space’ is explicitly made to support ‘a linear decision surface,’ just as Fisher’s linear discriminant analysis had. But this linear decision surface is now located amidst a non-linear mapping of the data.<a href="#fn74" class="footnoteRef" id="fnref74"><sup>74</sup></a> Cortes and Vapnik’s support vector machine constructs a new domain - ‘a very high dimension feature space’ – where inseparable differences start to disentangle themselves. The constructed dimensions do not index new sources or kinds of data. Instead, the support vector machine transforms the vector space into a much higher dimension.</p>
<p>As Vapnik writes in the preface to the second edition of <em>The Nature of Statistical Learning Theory</em> <span class="citation">(Vapnik <a href="#ref-Vapnik_1999">1999</a>, vii)</span>, ‘in contrast to classical methods of statistics where in order to control performance one decreases the dimensionality of a feature space, the SVM dramatically increases dimensionality’ (vii).  From the standpoint of pattern recognition, this often vastly augmented vector space should make it harder to locate patterns. A linear or planar decision surface in high dimensional space maps onto a curving even labyrinthine decision boundary when projected back onto the original vector space (see the curving decision boundaries in figure @ref(fig:svm_iris)). In certain cases, machine learners multiply dimensions in data in the name of differentiation, classification, and prediction. Many of the techniques that have accumulated or been gathered into machine learning flatten variations and differences into lines and planes, but not always by reducing them. In fact, random forests, neural networks and support vector machines exemplify a counter-movement that maximises variety in the name of differentiation.<a href="#fn75" class="footnoteRef" id="fnref75"><sup>75</sup></a> Research in machine learning, whether it has been primarily statistical, mathematical or computational, countenances and addresses problems of non-linear classification through <em>dimensional expansion</em>. </p>
<p>The powerful augmentation characteristic of the support vector machine works through diagrammatic substitution.  Consider the expression shown below in equation :</p>

<p><span class="citation">(Hastie, Tibshirani, and Friedman <a href="#ref-Hastie_2009">2009</a>, 423)</span></p>
<p>In equation , a re-mapping of equation  occurs particularly through the substitution of a product <span class="math inline">\(\langle h(x_i), h(x_i^&#39;)\rangle\)</span> for <span class="math inline">\(x\)</span>. All of the data <span class="math inline">\(x\)</span> is re-mapped using some function <span class="math inline">\(h(X)\)</span> into a new higher dimensional space. What would be the value of a more complicated space? As Leo Breiman writes in his account of the development of the support vector machine:</p>
<blockquote>
<p>In two-class data, separability by a hyperplane does not often occur. However, let us increase the dimensionality by adding as additional predictor variables all quadratic monomials in the original predictor variables. … A hyperplane in the original variables plus quadratic monomials in the original variables is a more complex creature. The possibility of separation is greater. If no separation occurs, add cubic monomials as input features. If there are originally 30 predictor variables, then there are about 40,000 features if monomials up to the fourth degree are added. <span class="citation">(Breiman <a href="#ref-Breiman_2001a">2001</a><a href="#ref-Breiman_2001a">a</a>, 209)</span> </p>
</blockquote>
<p>The extravagant dimensionality released in the shift from 30 to 40,000 variables vastly expands the number of possible decision surfaces or hyperplanes that might be instituted in the vector space. The support vector machine, however, corrals and manages this massive and sometimes infinite generation of differences  at the same time by only allowing this expansion to occur along particular lines marked out by <em>kernel functions</em>. While the support vector machine maintains a commitment to the separating hyperplane, a linear form albeit with soft margins, it re-constitutes that plane in newly created vector spaces constrained by certain key structural features that render them computationally tractable. On the one hand, a promise of infinite expansion and associated freedom from the rigidity of lines, and on the other hand, a mode of expansion can only countenance a limited range of movements prescribed by the kernel functions (polynomial, radial, etc.). </p>
<p><em>Elements of Statistical Learning</em> puts it this way:</p>
<blockquote>
<p>We can represent the optimization problem and its solution in a special way that only involves the input features via inner products. We do this directly for the transformed feature vectors <span class="math inline">\(h(x_i)\)</span>. We then see that for particular choices of <span class="math inline">\(h\)</span>, these inner products can be computed very cheaply <span class="citation">(Hastie, Tibshirani, and Friedman <a href="#ref-Hastie_2009">2009</a>, 423)</span></p>
</blockquote>
<p>The terminology here takes us back the vector space (see chapter <a href="#ch:vector"><strong>??</strong></a>) that machine learning inhabits. The ‘inner product’ or ‘the convolution of the dot-product’ described by Cortes and Vapnik come from this space, in which the distances or alignments between whatever can be rendered as a vector can be calculated <em>en masse</em>. </p>
<p><em>Top 10 Algorithms for Data Mining</em> <span class="citation">(Wu et al. <a href="#ref-Wu_2008">2008</a>)</span>, a widely cited computer science account of data mining, justifies the operation in relation to entangled differences:</p>
<blockquote>
<p>The kernel trick is another commonly used technique to solve linearly inseparably problems. This issue is to define an appropriate kernel function based on the <em>inner product</em> between the given data, as a nonlinear transformation of data from the input space to a feature space with higher (even infinite) dimension in order to make the problems linearly separable. The underlying justification can be found in <em>Cover’s theorem</em> on the separability of patterns; that is, a complex pattern classification problem case in a high-dimensional space is <em>more likely</em> to be linearly separable than in a low dimensional space <span class="citation">(Wu et al. <a href="#ref-Wu_2008">2008</a>, 42)</span>. </p>
</blockquote>
<p>The ‘kernel trick’ that overcomes inseparability remaps an already transformed vector space – the inner product of all the vectors in the data – into a higher dimensional space defined by functions such as <span class="math inline">\(f(x) = x_i^2 + x_i^3\)</span>. The trick is no simple technical trick, since as Cortes and Vapnik point out it relies on substantial mathematical developments in the 1960s. ‘The idea of constructing support-vector networks comes from considering general forms of the dot-product in a Hilbert space (Anderson &amp; Bahadur, 1966)’, write Cortes and Vapnik <span class="citation">(Cortes and Vapnik <a href="#ref-Cortes_1995">1995</a>, 283)</span>. It is a trick, however, in the sense that it is ‘can be computed very cheaply.’<a href="#fn76" class="footnoteRef" id="fnref76"><sup>76</sup></a></p>
<p>What does the transformed feature space combined with the computational short cut of the inner product do in practice? Describing the generalization error – the errors made when a model classifies hitherto unseen data  – Cortes and Vapnik highlight the growth in dimensionality introduced by the technique of the support vector in classifying the handwritten numbers of the <code>mnist</code> data. They recount how the technique exponentially increases the dimensionality of the feature space and how the error rate on difficult-to-classify handwritten digits drops correspondingly. When the feature space has 256 dimensions (the given dimensions of the 16x16 pixel digits), the error rate is around 12%. As the dimensionality grows to 33,000, then a million, a billion, a trillion and so forth (up to <span class="math inline">\(1 x 10^{16}\)</span> dimensions), the error rate drops to just over 4%, close to the errors made by ‘human performance’ (2.5%) <span class="citation">(Cortes and Vapnik <a href="#ref-Cortes_1995">1995</a>, 288)</span>. </p>
</div>
<div id="instituting-patterns" class="section level2">
<h2><span class="header-section-number">8.7</span> Instituting patterns</h2>
<p> The engineered movement of various machine learners do not simply discover differences. They construct, identify and optimise distributions or patterns of difference. They do it in different ways. Sometimes they take for granted the very possibility of identifying differences in data, as if all differences must be visible and divisible given the right partition. At other times, intrinsic inseparability is taken into account as part of the pattern. The power of support vector machine to do this is limited, but instructive. It can deal with various forms of inseparability by taking the difficult-to-classify boundary cases as the basis of the model. It deals with problems of non-linearity by increasing the dimensionality of the data, and looking for separations in the higher-dimensional space.</p>
<p>What do we learn about differences from decision trees and their development into random forests, or from linear discriminant analysis and its re-formalization as the support vector machine in some of their operational specificities? First, patterns are multiple in machine learning. We could also have tracked the movement between the perceptron <span class="citation">(Rosenblatt <a href="#ref-Rosenblatt_1958">1958</a>)</span> and the ‘deep learning’ convolutional neural networks <span class="citation">(Hinton and Salakhutdinov <a href="#ref-Hinton_2006">2006</a>)</span> of more recent practice (see chapter <a href="#ch:subjects"><strong>??</strong></a>). Second, each of these shifts bears witness, I have been suggesting, to the emergence of a new enunciative mode that disperses patterns as the visible form of difference into a less visible but nevertheless operational space.  A single decision tree becomes thousands in a random forest. A relatively small number of dimensions in the vector space becomes potentially infinite in the convolutional dot products and kernel functions of support vector machines. Models that sought to encompass or fit everything in the data including the outliers within a single probability distribution instead dwell on the difficult-to-classify, the erroneous or borderline instances amidst the massive normalized accumulations of event.</p>
<p>What counts as pattern today? The visually interpretable shape of a decision tree cascades into the statistically observable trade-offs between fine-grained classification and cost-complexity considerations, between recursive differentiation and general sparsity. The separating lines and planes that allow linear models to become classifiers in the ‘classic’ techniques such as linear discriminant analysis find themselves displaced into hyper-planes, into newly constructed and sometimes inordinately-dimensioned feature spaces that can only be traversed by virtue of the kernel functions, and their computationally tractable inner products.</p>
<p>What does it matter if pattern disperses into operations? From the standpoint of critical thought, it might be that learning to find dispersed patterns only intensifies a tendency ‘to see differences in degree where there are differences in kind’ <span class="citation">(Deleuze <a href="#ref-Deleuze_1988a">1988</a><a href="#ref-Deleuze_1988a">a</a>, 21)</span>  It would, however, be relatively pointless to assert primacy of differences in kind. A more constructive and experimental challenge lies in exploring differences in kind within the computed differences of degree active in machine learning.  Despite their quite different ways of partitioning, separating or propagating differences, support vector machines and decision trees define possibilities of grouping and spacing differences, sometimes through purifying, sometimes through bending and blurring, and sometimes through multiplying. These groupings and spacings attract, generate and accumulate propositions. This grouping-spacing, despite its commonalities, is not an homogeneous field. It does not have the coherence of a science, it uses different systems of formalization (the cross-entropy measures of the decision tree, the tunable soft margins and kernel functions of the support vector machine), and disperses in different ways across knowledge practice (the decision tree with its commercial uptake in data mining versus the support vector machine’s heavy use in image recognition and classification).</p>

</div>
</div>
<h3> References</h3>
<div id="refs" class="references">
<div id="ref-Whitehead_1956">
<p>Whitehead, Alfred North. 1956. <em>Modes of Thought; Six Lectures Delivered in Wellesley College, Massachusetts, and Two Lectures in the University of Chicago</em>. New York, Cambridge University Press.</p>
</div>
<div id="ref-Cortes_1995">
<p>Cortes, C., and V. Vapnik. 1995. “Support-Vector Networks.” <em>Machine Learning</em> 20 (3): 273–97. doi:<a href="https://doi.org/10.1023/A:1022627411411">10.1023/A:1022627411411</a>.</p>
</div>
<div id="ref-Foucault_1972">
<p>Foucault, Michel. 1972. <em>The Archaeology of Knowledge and the Discourse on Language</em>. Translated by Allan Sheridan-Smith. New York: Pantheon Books.</p>
</div>
<div id="ref-Conway_2012">
<p>Conway, Drew, and John Myles White. 2012. <em>Machine Learning for Hackers</em>. Sebastopol, CA: O’Reilly. <a href="http://search.ebscohost.com/login.aspx?direct=true&amp;scope=site&amp;db=nlebk&amp;db=nlabk&amp;AN=436647" class="uri">http://search.ebscohost.com/login.aspx?direct=true&amp;scope=site&amp;db=nlebk&amp;db=nlabk&amp;AN=436647</a>.</p>
</div>
<div id="ref-Schutt_2013">
<p>Schutt, Rachel, and Cathy O’Neil. 2013. <em>Doing Data Science</em>. Sebastopol, Calif.: O’Reilly &amp; Associates Inc.</p>
</div>
<div id="ref-Malley_2011">
<p>Malley, James D., Karen G. Malley, and Sinisa Pajevic. 2011. <em>Statistical Learning for Biomedical Data</em>. 1st ed. Cambridge University Press.</p>
</div>
<div id="ref-Hastie_2009">
<p>Hastie, Trevor, Robert Tibshirani, and Jerome H. 2009. <em>The Elements of Statistical Learning: Data Mining, Inference, and Prediction</em>. 2nd edition. New York: Springer.</p>
</div>
<div id="ref-Morgan_1963">
<p>Morgan, James N., and John A. Sonquist. 1963. “Problems in the Analysis of Survey Data, and a Proposal.” <em>Journal of the American Statistical Association</em> 58 (302): 415–34. <a href="http://amstat.tandfonline.com/doi/full/10.1080/01621459.1963.10500855" class="uri">http://amstat.tandfonline.com/doi/full/10.1080/01621459.1963.10500855</a>.</p>
</div>
<div id="ref-Steinberg_2009">
<p>Steinberg, Dan, and Phillip Colla. 2009. “CART: Classification and Regression Trees.” <em>The Top Ten Algorithms in Data Mining</em>, 179–201. <a href="http://books.google.co.uk/books?hl=en&amp;lr=&amp;id=_kcEn-c9kYAC&amp;oi=fnd&amp;pg=PA179&amp;dq=dan+steinberg+cart&amp;ots=eQ7jtfUODm&amp;sig=Xs8kegu_D4DcrPhT6TUkB0LCV1A" class="uri">http://books.google.co.uk/books?hl=en&amp;lr=&amp;id=_kcEn-c9kYAC&amp;oi=fnd&amp;pg=PA179&amp;dq=dan+steinberg+cart&amp;ots=eQ7jtfUODm&amp;sig=Xs8kegu_D4DcrPhT6TUkB0LCV1A</a>.</p>
</div>
<div id="ref-Einhorn_1972">
<p>Einhorn, Hillel J. 1972. “Alchemy in the Behavioral Sciences.” <em>Public Opinion Quarterly</em> 36 (3): 367–78. doi:<a href="https://doi.org/10.1086/268019">10.1086/268019</a>.</p>
</div>
<div id="ref-Doyle_1973">
<p>Doyle, Peter. 1973. “The Use of Automatic Interaction Detector and Similar Search Procedures.” <em>Operational Research Quarterly</em>, 465–67.</p>
</div>
<div id="ref-Breiman_1984">
<p>Breiman, Leo, Jerome Friedman, Richard Olshen, Charles Stone, D. Steinberg, and P. Colla. 1984. <em>CART: Classification and Regression Trees</em>. Vol. 156. Belmont: Wadsworth.</p>
</div>
<div id="ref-Quinlan_1986">
<p>Quinlan, J. Ross. 1986. “Induction of Decision Trees.” <em>Machine Learning</em> 1 (1): 81–106. <a href="http://link.springer.com/article/10.1023/A:1022643204877" class="uri">http://link.springer.com/article/10.1023/A:1022643204877</a>.</p>
</div>
<div id="ref-Therneau_2015">
<p>Therneau, Terry, Beth Atkinson, and Brian Ripley. 2015. <em>Rpart: Recursive Partitioning and Regression Trees</em>. <a href="http://CRAN.R-project.org/package=rpart" class="uri">http://CRAN.R-project.org/package=rpart</a>.</p>
</div>
<div id="ref-Fisher_1938">
<p>Fisher, R.A. 1938. “The Statistical Utilization of Multiple Measurements.” <em>Annals of Human Genetics</em> 8 (4): 376–86.</p>
</div>
<div id="ref-Breiman_2001">
<p>Breiman, Leo. 2001b. “Statistical Modeling: The Two Cultures (with Comments and a Rejoinder by the Author).” <em>Statistical Science</em> 16 (3): 199–231. <a href="http://projecteuclid.org/euclid.ss/1009213726" class="uri">http://projecteuclid.org/euclid.ss/1009213726</a>.</p>
</div>
<div id="ref-Fisher_1936">
<p>Fisher, Ronald A. 1936. “The Use of Multiple Measurements in Taxonomic Problems.” <em>Annals of Eugenics</em> 7 (2): 179–88. <a href="http://onlinelibrary.wiley.com/doi/10.1111/j.1469-1809.1936.tb02137.x/full" class="uri">http://onlinelibrary.wiley.com/doi/10.1111/j.1469-1809.1936.tb02137.x/full</a>.</p>
</div>
<div id="ref-LeCun_2012">
<p>LeCun, Yann, and Corinna Cortes. 2012. “MNIST Handwritten Digit Database, Yann LeCun, Corinna Cortes and Chris Burges.” <a href="http://yann.lecun.com/exdb/mnist/" class="uri">http://yann.lecun.com/exdb/mnist/</a>.</p>
</div>
<div id="ref-Alpaydin_2010">
<p>Alpaydin, Ethem. 2010. <em>Introduction to Machine Learning</em>. Cambridge, Massachusetts; London: MIT Press.</p>
</div>
<div id="ref-Vapnik_1999">
<p>Vapnik, Vladimir. 1999. <em>The Nature of Statistical Learning Theory</em>. 2nd ed. 2000. Springer.</p>
</div>
<div id="ref-Breiman_2001a">
<p>Breiman, Leo. 2001a. “Random Forests.” <em>Machine Learning</em> 45 (1): 5–32.</p>
</div>
<div id="ref-Wu_2008">
<p>Wu, X., V. Kumar, J. Ross Quinlan, J. Ghosh, Q. Yang, H. Motoda, G. J McLachlan, et al. 2008. “Top 10 Algorithms in Data Mining.” <em>Knowledge and Information Systems</em> 14 (1): 1–37.</p>
</div>
<div id="ref-Rosenblatt_1958">
<p>Rosenblatt, F. 1958. “The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain.” <em>Psychological Review</em> 65 (6): 386–408. doi:<a href="https://doi.org/10.1037/h0042519">10.1037/h0042519</a>.</p>
</div>
<div id="ref-Hinton_2006">
<p>Hinton, Geoffrey E., and Ruslan R. Salakhutdinov. 2006. “Reducing the Dimensionality of Data with Neural Networks.” <em>Science</em> 313 (5786): 504–7. <a href="http://www.sciencemag.org/content/313/5786/504.short" class="uri">http://www.sciencemag.org/content/313/5786/504.short</a>.</p>
</div>
<div id="ref-Deleuze_1988a">
<p>Deleuze, Gilles. 1988a. <em>Bergsonism</em>. New York: Zone Books.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="63">
<li id="fn63"><p>Many authors have suggested that algorithms should be the focus of more attention. The sociologist Mike Savage, in his account of the growth of ‘descriptive assemblages’ based around large scale data mining of transactions, administrative records and social media practice concludes:</p><p>It follows that a core concern might be to scrutinize how pattern is derived and produced in social inscription devices, as a means of considering the robustness of such derivations, what may be left out or made invisible from them, and so forth. We need to develop an account which seeks to criticize notions of the descriptive insofar as this involves the simple generation of categories and groups, and instead focus on the fluid and intensive generation of potential <span class="citation">(Savage <a href="#ref-Savage_2009">2009</a>, 171)</span> <a href="8-patterns-and-differences.html#fnref63">↩</a></p></li>
<li id="fn64"><p><em>Elements of Statistical Learning</em> uses the term ‘pattern’ only occasionally. The term appears 33 times there, and mainly in the bibliography. Apart from Brian Ripley’s <em>Pattern Recognition and Neural Networks</em> <span class="citation">(Ripley <a href="#ref-Ripley_1996">1996</a>)</span>, statisticians largely eschew the term. Computer scientists like it more, and particularly in work on the classification of images (see Christopher Bishop <em>Pattern Recognition and Machine Learning</em> <span class="citation">(Bishop <a href="#ref-Bishop_2006">2006</a>)</span>. Hastie, Tibshirani and Friedman, as statistical machine learners, confine their use of pattern to the term ‘pattern recognition’. <a href="8-patterns-and-differences.html#fnref64">↩</a></p></li>
<li id="fn65"><p>While Foucault tends to retain a decoupled subject-object relation in the production of statements, I tend to see these enunciative modalities as distributed across people and things. As always, machine learner is a composite term for this distribution.<a href="8-patterns-and-differences.html#fnref65">↩</a></p></li>
<li id="fn66"><p>The top 20 most cited publications in the field include Ross Quinlan and Leo Breiman’s papers on decision trees <span class="citation">(Quinlan <a href="#ref-Quinlan_1986">1986</a>; Breiman et al. <a href="#ref-Breiman_1984">1984</a>)</span>, Vladimir Vapnik and Corinna Cortes’ support vector machines papers <span class="citation">(Vapnik <a href="#ref-Vapnik_1999">1999</a>; Cortes and Vapnik <a href="#ref-Cortes_1995">1995</a>)</span>, an early textbook written by a computer scientist on machine learning <span class="citation">(Mitchell <a href="#ref-Mitchell_1997">1997</a>)</span>, a textbook and software package on data mining using Java <span class="citation">(Witten and Frank <a href="#ref-Witten_2005">2005</a>)</span>; a textbook on pattern recognition dating from the 1970s <span class="citation">(Duda, Hart, and Stork <a href="#ref-Duda_2012">2012</a>)</span>, a tutorial on an error control technique (ROC - Receiver Operating Characteristics, first developed by the US military during WWII) and somewhat lower, another well-known textbook, this time on neural networks and pattern recognition <span class="citation">(Bishop <a href="#ref-Bishop_2006">2006</a>)</span>. <a href="8-patterns-and-differences.html#fnref66">↩</a></p></li>
<li id="fn67"><p>These objections and resistances to early decision trees echo today in discussions around pattern recognition, knowledge discovery and data-mining in science and commerce. The problem of what computers do to the analysis of empirical data is long-standing.<a href="8-patterns-and-differences.html#fnref67">↩</a></p></li>
<li id="fn68"><p>Quinlan’s papers and book on versions of the decision tree (<code>ID3</code> and <code>c4.5</code>) are both amongst the top ten the most highly cited references in the machine literature itself. Google Scholar reports over 20,00 citations of the Quinlan’s book <em>C4.5: Programs for Machine Learning</em> <span class="citation">(Quinlan <a href="#ref-Quinlan_1993">1993</a>)</span> (although far fewer appear in Thomson Reuters Web of Science). Several years ago, <code>C4.5</code> was voted the top data mining algorithm <span class="citation">(Wu et al. <a href="#ref-Wu_2008">2008</a>)</span>. While I don’t discuss Quinlan’s work in much detail here, we should note as a computer scientist, Quinlan takes a much more rule-based approach to decision tree that Breiman and co-authors.  <a href="8-patterns-and-differences.html#fnref68">↩</a></p></li>
<li id="fn69"><p>Other <code>R</code> packages such as <code>party</code> <span class="citation">(Hothorn, Hornik, and Zeileis <a href="#ref-Hothorn_2006">2006</a>)</span> and <code>tree</code> <span class="citation">(Ripley <a href="#ref-Ripley_2014">2014</a>)</span> also use recursive partitioning, but with various tweaks and optimisations that I leave aside here.  <a href="8-patterns-and-differences.html#fnref69">↩</a></p></li>
<li id="fn70"><p><code>iris</code> is a very small dataset, a pre-computational miniature.  That diminutive character makes it diagrammatically mobile. It supports a rhizomatic ecosystem of examples scattered across the machine learning literature. The usual framing of the classification problem is how to decide whether a given iris blossom is of the species <em>virginica</em>, <em>setosa</em> or _versicolor. These irises don’t grow in forests – they are more often found in riverbanks and meadows – but they do offer a variety of illustrations of how machine learning classifiers are brought to bear on classification problems. Here the classification problem is taxonomic - the <em>iris</em> genus has various sub-genera, and sections within the sub-genera.Setosa, <em>virginica</em> and <em>versicolor</em> all belong to the sub-genus <em>Limniris</em>. This botanical context is routinely ignored in machine learning applications. In machine learning textbooks and tutorials, <code>iris</code> typically would be used to demonstrate how cleanly a classifier can separate the different kinds of irises. <a href="8-patterns-and-differences.html#fnref70">↩</a></p></li>
<li id="fn71"><p>The support vector machine is distinctive in its transformations of data, and this owes something to history, politics and geography. Vapnik trained and worked of decades in the former USSR as a mathematician and statistician. His writings on the problems of pattern recognition contrast greatly with other engineers, statisticians and computer scientists in their robustly theoretical formalism. A highly cited 1971 publication with Alexey Chervonenkis ‘On the uniform convergence of relative frequencies of events to their probabilities’ (published in Russian in 1968 ) <span class="citation">(Vapnik and Chervonenkis <a href="#ref-Vapnik_1971">1971</a>)</span> sets the formal tone of this work. In ensuing publications in Russian and then in English after Vapnik moved from Moscow to AT&amp;T’s New Jersey Bell Labs in 1990, Vapnik’s work remains quite formally mathematical. Although it pertains to ‘learning machines,’ machine here are understood mathematically simply as ‘the implementation of a set of functions’ <span class="citation">(Vapnik <a href="#ref-Vapnik_1999">1999</a>, 17)</span>.  The way that Vapnik develops a theory of learning owes little visible debt to actual attempts to work with data or experience in doing statistics in any particular domain. This contrasts greatly for instance with the work of statisticians like Breiman or Friedman or even computer scientists like Quinlan or Le Cun whose work lies much closer to fields of application. Vapnik’s work, like that of the Russian mathematician Andrey Kolmogorov he draws on, differs from many other contributions to machine learning partly by virtue of this formality and its efforts to derive insight into machine learning by theorising learning. The <em>Vapnik-Chervonenkis dimension</em>( VC dimension), a very widely used way of defining the capacity of a particular machine learning technique to recognise patterns in data dates from his work in the 1960s and underpins a general theory of ‘learning.’ Vapnik writes in 1995, </p><blockquote><p>The VC dimension of the set of functions (rather than the number of parameters) is responsible for the generalization ability of learning machines. This opens remarkable opportunities to overcome the “curse of dimensionality <span class="citation">(Vapnik <a href="#ref-Vapnik_1999">1999</a>, 83)</span>.</p></blockquote><p>As we will see in this chapter, Vapnik’s attempts to overcome dimensionality also re-shape what counts as pattern. <a href="8-patterns-and-differences.html#fnref71">↩</a></p></li>
<li id="fn72"><p><em>Elements of Statistical Learning</em> also devotes a chapter to support vector machines <span class="citation">(Hastie, Tibshirani, and Friedman <a href="#ref-Hastie_2009">2009</a>, Chapter 14)</span><a href="8-patterns-and-differences.html#fnref72">↩</a></p></li>
<li id="fn73"><p>Neural network researchers have heavily used the MNIST dataset. I discuss some of that work in chapter <a href="#ch:subjects"><strong>??</strong></a>. The handwritten MNIST also appear in <em>Elements of Statistical Learning</em> , where they are used to compare the generalization error (see previous chapter) of a <em>k</em> nearest neighbours, convolutional neural network, and a ‘degree-9 polynomial’ support vector machine <span class="citation">(Hastie, Tibshirani, and Friedman <a href="#ref-Hastie_2009">2009</a>, 408)</span>.  What about the handwritten digits attracts so many machine learning techniques? The logistics of the US Postal Service aside (since the <code>mnist</code> datasets continue to be used by machine learners well after the problem of scrawl on letters has been sorted), the variations, the regularities, and the banal everydayness of these digits furnish a referential locus, whose existence as a facts, things or events in the world is less important than the relations of similarity and differences it poses.  The field of digitals becomes a site of differentiation not only of digits – the machine learners attempt to correctly classify the digits – but of the authority of different machine learning techniques and approaches. They become ways of announcing and delimiting the authority, the knowledge claims or ‘truth’ associated with the machine. The many uses of the<code>mnist</code> data documented by <span class="citation">(LeCun and Cortes <a href="#ref-LeCun_2012">2012</a>)</span> suggests something of the ancestral probabilisaton  of such datasets.<a href="8-patterns-and-differences.html#fnref73">↩</a></p></li>
<li id="fn74"><p>As we have seen on several occasions, the vector space invites a certain form of classification based on the search for the best line, the line of best fit, or the most discriminating line, the line that best divides things from each other. Linear regression is not called ‘linear’ for no reason. And Fisher’s ‘discriminant functions’ were later called ‘linear discriminant analysis’ for the same reason: they divide the vector space into different regions (‘decision regions’) separated by ‘linear decision boundaries’ <span class="citation">(Alpaydin <a href="#ref-Alpaydin_2010">2010</a>, 53)</span>. Almost all machine learners are aware of and try to address the idealism or abstraction of the line or plane. <a href="8-patterns-and-differences.html#fnref74">↩</a></p></li>
<li id="fn75"><p>Despite the in-principle commitment to any form of function, machine learning strongly prefers forms that can either be visualised on a plane (using the visual grammar of lines, dots, axes, labels, colours, shapes, etc.), or can be computed in form of matrix or vectorised calculations focused on planes.  Many of the techniques that grapple with complicated datasets seek to reduce their dimensionality so that lines, planes and regular curves can be applied to them: multi-dimensional scaling (MDS), factor analysis, principal component analysis (PCA), or self-organising maps (SOM) are just a few examples of this.    <a href="8-patterns-and-differences.html#fnref75">↩</a></p></li>
<li id="fn76"><p>This cheapness appeared already in the cat machine learner discussed in the introduction. Heather McArthur’s<code>kittydar</code> cat image classifier implemented a support vector machine in Javasript that runs in a browser. Cats are classified cheaply there. <a href="8-patterns-and-differences.html#fnref76">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="7-probabilisation-and-the-taming-of-machines.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="9-regularizing-and-materializing-objects.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
