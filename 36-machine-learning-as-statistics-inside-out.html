<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Machine Learning: Archaeology of a Data Practice</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="Machine Learning: Archaeology of a Data Practice">
  <meta name="generator" content="bookdown 0.3 and GitBook 2.6.7">

  <meta property="og:title" content="Machine Learning: Archaeology of a Data Practice" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Machine Learning: Archaeology of a Data Practice" />
  
  
  

<meta name="author" content="Adrian Mackenzie">


<meta name="date" content="2016-12-10">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="35-data-reduces-uncertainty.html">
<link rel="next" href="37-distributed-probabilities.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="1-acknowledgments.html"><a href="1-acknowledgments.html"><i class="fa fa-check"></i><b>1</b> Acknowledgments</a></li>
<li class="chapter" data-level="2" data-path="2-preface.html"><a href="2-preface.html"><i class="fa fa-check"></i><b>2</b> Preface</a></li>
<li class="chapter" data-level="3" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html"><i class="fa fa-check"></i><b>3</b> Introduction: Into the Data</a><ul>
<li class="chapter" data-level="3.1" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#three-accumulations-settings-data-and-devices"><i class="fa fa-check"></i><b>3.1</b> Three accumulations: settings, data and devices</a></li>
<li class="chapter" data-level="3.2" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#who-or-what-is-a-machine-learner"><i class="fa fa-check"></i><b>3.2</b> Who or what is a machine learner?</a></li>
<li class="chapter" data-level="3.3" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#algorithmic-control-to-the-machine-learners"><i class="fa fa-check"></i><b>3.3</b> Algorithmic control to the machine learners?</a></li>
<li class="chapter" data-level="3.4" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#the-archaeology-of-operations"><i class="fa fa-check"></i><b>3.4</b> The archaeology of operations</a></li>
<li class="chapter" data-level="3.5" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#asymmetries-in-common-knowledge"><i class="fa fa-check"></i><b>3.5</b> Asymmetries in common knowledge</a></li>
<li class="chapter" data-level="3.6" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#what-cannot-be-automated"><i class="fa fa-check"></i><b>3.6</b> What cannot be automated?</a></li>
<li class="chapter" data-level="3.7" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#different-fields-in-machine-learning"><i class="fa fa-check"></i><b>3.7</b> Different fields in machine learning?</a></li>
<li class="chapter" data-level="3.8" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#the-diagram-in-critical-thought"><i class="fa fa-check"></i><b>3.8</b> The diagram in critical thought</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-we-dont-have-to-write-programs.html"><a href="4-we-dont-have-to-write-programs.html"><i class="fa fa-check"></i><b>4</b> ‘We don’t have to write programs’?</a></li>
<li class="chapter" data-level="5" data-path="5-the-elements-of-machine-learning.html"><a href="5-the-elements-of-machine-learning.html"><i class="fa fa-check"></i><b>5</b> The elements of machine learning</a></li>
<li class="chapter" data-level="6" data-path="6-who-reads-machine-learning-textbooks.html"><a href="6-who-reads-machine-learning-textbooks.html"><i class="fa fa-check"></i><b>6</b> Who reads machine learning textbooks?</a></li>
<li class="chapter" data-level="7" data-path="7-r-a-matrix-of-transformations.html"><a href="7-r-a-matrix-of-transformations.html"><i class="fa fa-check"></i><b>7</b> <code>R</code>: a matrix of transformations</a></li>
<li class="chapter" data-level="8" data-path="8-the-obdurate-mathematical-glint-of-machine-learning.html"><a href="8-the-obdurate-mathematical-glint-of-machine-learning.html"><i class="fa fa-check"></i><b>8</b> The obdurate mathematical glint of machine learning</a></li>
<li class="chapter" data-level="9" data-path="9-cs229-2007-returning-again-and-again-to-certain-features.html"><a href="9-cs229-2007-returning-again-and-again-to-certain-features.html"><i class="fa fa-check"></i><b>9</b> CS229, 2007: returning again and again to certain features</a></li>
<li class="chapter" data-level="10" data-path="10-the-visible-learning-of-machine-learning.html"><a href="10-the-visible-learning-of-machine-learning.html"><i class="fa fa-check"></i><b>10</b> The visible learning of machine learning</a></li>
<li class="chapter" data-level="11" data-path="11-the-diagram-of-an-operational-formation.html"><a href="11-the-diagram-of-an-operational-formation.html"><i class="fa fa-check"></i><b>11</b> The diagram of an operational formation</a></li>
<li class="chapter" data-level="12" data-path="12-vectorisation-and-its-consequences.html"><a href="12-vectorisation-and-its-consequences.html"><i class="fa fa-check"></i><b>12</b> Vectorisation and its consequences}</a></li>
<li class="chapter" data-level="13" data-path="13-vector-space-and-geometry.html"><a href="13-vector-space-and-geometry.html"><i class="fa fa-check"></i><b>13</b> Vector space and geometry</a></li>
<li class="chapter" data-level="14" data-path="14-mixing-places.html"><a href="14-mixing-places.html"><i class="fa fa-check"></i><b>14</b> Mixing places</a></li>
<li class="chapter" data-level="15" data-path="15-truth-is-no-longer-in-the-table.html"><a href="15-truth-is-no-longer-in-the-table.html"><i class="fa fa-check"></i><b>15</b> Truth is no longer in the table?</a></li>
<li class="chapter" data-level="16" data-path="16-the-epistopic-fault-line-in-tables.html"><a href="16-the-epistopic-fault-line-in-tables.html"><i class="fa fa-check"></i><b>16</b> The epistopic fault line in tables</a></li>
<li class="chapter" data-level="17" data-path="17-surface-and-depths-the-problem-of-volume-in-data.html"><a href="17-surface-and-depths-the-problem-of-volume-in-data.html"><i class="fa fa-check"></i><b>17</b> Surface and depths: the problem of volume in data</a></li>
<li class="chapter" data-level="18" data-path="18-vector-space-expansion.html"><a href="18-vector-space-expansion.html"><i class="fa fa-check"></i><b>18</b> Vector space expansion</a></li>
<li class="chapter" data-level="19" data-path="19-drawing-lines-in-a-common-space-of-transformation.html"><a href="19-drawing-lines-in-a-common-space-of-transformation.html"><i class="fa fa-check"></i><b>19</b> Drawing lines in a common space of transformation</a></li>
<li class="chapter" data-level="20" data-path="20-implicit-vectorization-in-code-and-infrastructures.html"><a href="20-implicit-vectorization-in-code-and-infrastructures.html"><i class="fa fa-check"></i><b>20</b> Implicit vectorization in code and infrastructures</a></li>
<li class="chapter" data-level="21" data-path="21-lines-traversing-behind-the-light.html"><a href="21-lines-traversing-behind-the-light.html"><i class="fa fa-check"></i><b>21</b> Lines traversing behind the light</a></li>
<li class="chapter" data-level="22" data-path="22-the-vectorised-table.html"><a href="22-the-vectorised-table.html"><i class="fa fa-check"></i><b>22</b> The vectorised table?</a></li>
<li class="chapter" data-level="23" data-path="23-machines-finding-functions.html"><a href="23-machines-finding-functions.html"><i class="fa fa-check"></i><b>23</b> Machines finding functions}</a></li>
<li class="chapter" data-level="24" data-path="24-learning-functions.html"><a href="24-learning-functions.html"><i class="fa fa-check"></i><b>24</b> Learning functions</a></li>
<li class="chapter" data-level="25" data-path="25-supervised-unsupervised-reinforcement-learning-and-functions.html"><a href="25-supervised-unsupervised-reinforcement-learning-and-functions.html"><i class="fa fa-check"></i><b>25</b> Supervised, unsupervised, reinforcement learning and functions</a></li>
<li class="chapter" data-level="26" data-path="26-which-function-operates.html"><a href="26-which-function-operates.html"><i class="fa fa-check"></i><b>26</b> Which function operates?</a></li>
<li class="chapter" data-level="27" data-path="27-what-does-a-function-learn.html"><a href="27-what-does-a-function-learn.html"><i class="fa fa-check"></i><b>27</b> What does a function learn?</a></li>
<li class="chapter" data-level="28" data-path="28-observing-with-curves-the-logistic-function.html"><a href="28-observing-with-curves-the-logistic-function.html"><i class="fa fa-check"></i><b>28</b> Observing with curves: the logistic function</a></li>
<li class="chapter" data-level="29" data-path="29-the-cost-of-curves-in-machine-learning.html"><a href="29-the-cost-of-curves-in-machine-learning.html"><i class="fa fa-check"></i><b>29</b> The cost of curves in machine learning</a></li>
<li class="chapter" data-level="30" data-path="30-curves-and-the-variation-in-models.html"><a href="30-curves-and-the-variation-in-models.html"><i class="fa fa-check"></i><b>30</b> Curves and the variation in models</a></li>
<li class="chapter" data-level="31" data-path="31-observing-costs-losses-and-objectives-through-optimisation.html"><a href="31-observing-costs-losses-and-objectives-through-optimisation.html"><i class="fa fa-check"></i><b>31</b> Observing costs, losses and objectives through optimisation</a></li>
<li class="chapter" data-level="32" data-path="32-gradients-as-partial-observers.html"><a href="32-gradients-as-partial-observers.html"><i class="fa fa-check"></i><b>32</b> Gradients as partial observers</a></li>
<li class="chapter" data-level="33" data-path="33-the-power-to-learn.html"><a href="33-the-power-to-learn.html"><i class="fa fa-check"></i><b>33</b> The power to learn</a></li>
<li class="chapter" data-level="34" data-path="34-probabilisation-and-the-taming-of-machines.html"><a href="34-probabilisation-and-the-taming-of-machines.html"><i class="fa fa-check"></i><b>34</b> Probabilisation and the Taming of Machines}</a></li>
<li class="chapter" data-level="35" data-path="35-data-reduces-uncertainty.html"><a href="35-data-reduces-uncertainty.html"><i class="fa fa-check"></i><b>35</b> Data reduces uncertainty?</a></li>
<li class="chapter" data-level="36" data-path="36-machine-learning-as-statistics-inside-out.html"><a href="36-machine-learning-as-statistics-inside-out.html"><i class="fa fa-check"></i><b>36</b> Machine learning as statistics inside out</a></li>
<li class="chapter" data-level="37" data-path="37-distributed-probabilities.html"><a href="37-distributed-probabilities.html"><i class="fa fa-check"></i><b>37</b> Distributed probabilities</a></li>
<li class="chapter" data-level="38" data-path="38-naive-bayes-and-the-distribution-of-probabilities.html"><a href="38-naive-bayes-and-the-distribution-of-probabilities.html"><i class="fa fa-check"></i><b>38</b> Naive Bayes and the distribution of probabilities</a></li>
<li class="chapter" data-level="39" data-path="39-spam-when-foralln-is-too-much.html"><a href="39-spam-when-foralln-is-too-much.html"><i class="fa fa-check"></i><b>39</b> Spam: when <span class="math inline">\(\forall{N}\)</span> is too much?</a></li>
<li class="chapter" data-level="40" data-path="40-the-improbable-success-of-the-naive-bayes-classifier.html"><a href="40-the-improbable-success-of-the-naive-bayes-classifier.html"><i class="fa fa-check"></i><b>40</b> The improbable success of the Naive Bayes classifier</a></li>
<li class="chapter" data-level="41" data-path="41-ancestral-probabilities-in-documents-inference-and-prediction.html"><a href="41-ancestral-probabilities-in-documents-inference-and-prediction.html"><i class="fa fa-check"></i><b>41</b> Ancestral probabilities in documents: inference and prediction</a></li>
<li class="chapter" data-level="42" data-path="42-statistical-decompositions-bias-variance-and-observed-errors.html"><a href="42-statistical-decompositions-bias-variance-and-observed-errors.html"><i class="fa fa-check"></i><b>42</b> Statistical decompositions: bias, variance and observed errors</a></li>
<li class="chapter" data-level="43" data-path="43-does-machine-learning-construct-a-new-statistical-reality.html"><a href="43-does-machine-learning-construct-a-new-statistical-reality.html"><i class="fa fa-check"></i><b>43</b> Does machine learning construct a new statistical reality?</a></li>
<li class="chapter" data-level="44" data-path="44-patterns-and-differences.html"><a href="44-patterns-and-differences.html"><i class="fa fa-check"></i><b>44</b> Patterns and differences</a></li>
<li class="chapter" data-level="45" data-path="45-splitting-and-the-growth-of-trees.html"><a href="45-splitting-and-the-growth-of-trees.html"><i class="fa fa-check"></i><b>45</b> Splitting and the growth of trees</a></li>
<li class="chapter" data-level="46" data-path="46-differences-in-recursive-partitioning.html"><a href="46-differences-in-recursive-partitioning.html"><i class="fa fa-check"></i><b>46</b> 1984: Differences in recursive partitioning</a></li>
<li class="chapter" data-level="47" data-path="47-limiting-differences.html"><a href="47-limiting-differences.html"><i class="fa fa-check"></i><b>47</b> Limiting differences</a></li>
<li class="chapter" data-level="48" data-path="48-the-successful-dispersion-of-the-support-vector-machine.html"><a href="48-the-successful-dispersion-of-the-support-vector-machine.html"><i class="fa fa-check"></i><b>48</b> The successful dispersion of the support vector machine</a></li>
<li class="chapter" data-level="49" data-path="49-differences-blur.html"><a href="49-differences-blur.html"><i class="fa fa-check"></i><b>49</b> Differences blur?</a></li>
<li class="chapter" data-level="50" data-path="50-bending-the-decision-boundary.html"><a href="50-bending-the-decision-boundary.html"><i class="fa fa-check"></i><b>50</b> Bending the decision boundary</a></li>
<li class="chapter" data-level="51" data-path="51-instituting-patterns.html"><a href="51-instituting-patterns.html"><i class="fa fa-check"></i><b>51</b> Instituting patterns</a></li>
<li class="chapter" data-level="52" data-path="52-regularizing-and-materializing-objects.html"><a href="52-regularizing-and-materializing-objects.html"><i class="fa fa-check"></i><b>52</b> Regularizing and materializing objects}</a></li>
<li class="chapter" data-level="53" data-path="53-genomic-referentiality-and-materiality.html"><a href="53-genomic-referentiality-and-materiality.html"><i class="fa fa-check"></i><b>53</b> Genomic referentiality and materiality</a></li>
<li class="chapter" data-level="54" data-path="54-the-genome-as-threshold-object.html"><a href="54-the-genome-as-threshold-object.html"><i class="fa fa-check"></i><b>54</b> The genome as threshold object</a></li>
<li class="chapter" data-level="55" data-path="55-genomic-knowledges-and-their-datasets.html"><a href="55-genomic-knowledges-and-their-datasets.html"><i class="fa fa-check"></i><b>55</b> Genomic knowledges and their datasets</a></li>
<li class="chapter" data-level="56" data-path="56-the-advent-of-wide-dirty-and-mixed-data.html"><a href="56-the-advent-of-wide-dirty-and-mixed-data.html"><i class="fa fa-check"></i><b>56</b> The advent of ‘wide, dirty and mixed’ data</a></li>
<li class="chapter" data-level="57" data-path="57-cross-validating-machine-learning-in-genomics.html"><a href="57-cross-validating-machine-learning-in-genomics.html"><i class="fa fa-check"></i><b>57</b> Cross-validating machine learning in genomics</a></li>
<li class="chapter" data-level="58" data-path="58-proliferation-of-discoveries.html"><a href="58-proliferation-of-discoveries.html"><i class="fa fa-check"></i><b>58</b> Proliferation of discoveries</a></li>
<li class="chapter" data-level="59" data-path="59-variations-in-the-object-or-in-the-machine-learner.html"><a href="59-variations-in-the-object-or-in-the-machine-learner.html"><i class="fa fa-check"></i><b>59</b> Variations in the object or in the machine learner?</a></li>
<li class="chapter" data-level="60" data-path="60-whole-genome-functions.html"><a href="60-whole-genome-functions.html"><i class="fa fa-check"></i><b>60</b> Whole genome functions</a></li>
<li class="chapter" data-level="61" data-path="61-propagating-subject-positions.html"><a href="61-propagating-subject-positions.html"><i class="fa fa-check"></i><b>61</b> Propagating subject positions}</a></li>
<li class="chapter" data-level="62" data-path="62-propagation-across-human-machine-boundaries.html"><a href="62-propagation-across-human-machine-boundaries.html"><i class="fa fa-check"></i><b>62</b> Propagation across human-machine boundaries</a></li>
<li class="chapter" data-level="63" data-path="63-competitive-positioning.html"><a href="63-competitive-positioning.html"><i class="fa fa-check"></i><b>63</b> Competitive positioning</a></li>
<li class="chapter" data-level="64" data-path="64-a-privileged-machine-and-its-diagrammatic-forms.html"><a href="64-a-privileged-machine-and-its-diagrammatic-forms.html"><i class="fa fa-check"></i><b>64</b> A privileged machine and its diagrammatic forms</a></li>
<li class="chapter" data-level="65" data-path="65-varying-subject-positions-in-code.html"><a href="65-varying-subject-positions-in-code.html"><i class="fa fa-check"></i><b>65</b> Varying subject positions in code</a></li>
<li class="chapter" data-level="66" data-path="66-the-subjects-of-a-hidden-operation.html"><a href="66-the-subjects-of-a-hidden-operation.html"><i class="fa fa-check"></i><b>66</b> The subjects of a hidden operation</a></li>
<li class="chapter" data-level="67" data-path="67-algorithms-that-propagate-errors.html"><a href="67-algorithms-that-propagate-errors.html"><i class="fa fa-check"></i><b>67</b> Algorithms that propagate errors</a></li>
<li class="chapter" data-level="68" data-path="68-competitions-as-examination.html"><a href="68-competitions-as-examination.html"><i class="fa fa-check"></i><b>68</b> Competitions as examination</a></li>
<li class="chapter" data-level="69" data-path="69-superimposing-power-and-knowledge.html"><a href="69-superimposing-power-and-knowledge.html"><i class="fa fa-check"></i><b>69</b> Superimposing power and knowledge</a></li>
<li class="chapter" data-level="70" data-path="70-ranked-subject-positions.html"><a href="70-ranked-subject-positions.html"><i class="fa fa-check"></i><b>70</b> Ranked subject positions</a></li>
<li class="chapter" data-level="71" data-path="71-conclusion-out-of-the-data.html"><a href="71-conclusion-out-of-the-data.html"><i class="fa fa-check"></i><b>71</b> Conclusion: Out of the Data}</a></li>
<li class="chapter" data-level="72" data-path="72-machine-learners.html"><a href="72-machine-learners.html"><i class="fa fa-check"></i><b>72</b> 250,000 machine learners</a></li>
<li class="chapter" data-level="73" data-path="73-a-summary-of-the-argument.html"><a href="73-a-summary-of-the-argument.html"><i class="fa fa-check"></i><b>73</b> A summary of the argument</a></li>
<li class="chapter" data-level="74" data-path="74-in-situ-hybridization.html"><a href="74-in-situ-hybridization.html"><i class="fa fa-check"></i><b>74</b> In-situ hybridization</a></li>
<li class="chapter" data-level="75" data-path="75-critical-operational-practice.html"><a href="75-critical-operational-practice.html"><i class="fa fa-check"></i><b>75</b> Critical operational practice?</a></li>
<li class="chapter" data-level="76" data-path="76-obstacles-to-the-work-of-freeing-machine-learning.html"><a href="76-obstacles-to-the-work-of-freeing-machine-learning.html"><i class="fa fa-check"></i><b>76</b> Obstacles to the work of freeing machine learning</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning: Archaeology of a Data Practice</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="machine-learning-as-statistics-inside-out" class="section level1">
<h1><span class="header-section-number">36</span> Machine learning as statistics inside out</h1>
<p>The argument mimics Hacking’s. In <em>The Taming of Chance</em>, Hacking argues that modern statistical thought transposed a way of calculating errors in experimental measurements and astronomical observations into the real and constitute attributes of populations understood as processes of reproductive growth.  This transposition or inversion relied on four intermediate steps passing through the development of a probability calculus (particularly the work of Jacob Bernoulli and the binomial or heads-tails probability distribution in the 1690s <span class="citation">[@Hacking_1975, 143]</span>), the accumulation of large numbers of measurements (the most famous being the chest measurements of soldiers in Scottish regiments, but these were only one flurry amidst an avalanche of numbers in the 1830-1840s), the emergence of the idea of multiple, minute independent causes producing events (particularly as developed in medicine but also in studies of crime), and the ‘law of errors’ applying to measurements made by, amongst others, astronomers <span class="citation">[@Hacking_1990, 111-112]</span>.   As Hacking observes, coins, suicides, crime, chest measurements, and astronomical observations all pile up in a statistical aggregate which remains, although somewhat altered, indelible in contemporary statistical knowledges, particularly in its frequent recourse to notions of population, probability and distribution. In this entanglement, observers and the observed changed places. The distribution of errors made by astronomers measuring the position of stars or planets became a distribution or variation inherent in a population. </p>
<p>Machine learning reverse-engineers the invention of modern statistical thinking. It takes back the ‘real quantities’ – probabilities – that modern statistics had attributed to the populations in the world and distributes them to devices, to machine learners that people then observe, monitor and indeed measure again in many ways. The direct swapping between uncertainty in measurement and variation in real attributes that statistics achieved now finds itself re-routed and intensified as machine learners measure the errors, the bias and the variance of devices. Although it relies heavily on probability distributions, machine learning is a fat-tailed distribution of probability.</p>
<p>The swapping or re-distribution is not a simple mirror-image reversal, as if machine learners mistake devices for a population. Machine learning constantly takes statistical thinking as a basic condition for its operations and devices. When <em>Elements of Statistical Learning</em> states that (as we saw in the previous chapter) ‘our goal is to find a useful approximation <span class="math inline">\(\hat(f)(x)\)</span> to the function <span class="math inline">\(f(x)\)</span> that underlies the predictive relationship between input and output’ <span class="citation">[@Hastie_2009, 28]</span>, they invoke the ‘real quantities’ first elaborated and articulated by proto-statisticians such as Quetelet grappling with population and sample parameters. The major structuring operational practices in machine learning as a field of knowledge-practice show the marks of increasingly strong commitment to the reality of the statistical, and to the ongoing probabilisation of machine learners. </p>
<p>What is probabilisation in practice? Reading and working with machine learning techniques usually means encountering and responding to apparatus drawn from statistics, but the apparatus is not typically the statistical tests of significance or variation. In contrast to a statistics textbook such as the widely used <em>Basic Practice of Statistics</em> <span class="citation">[@Moore_2009]</span> or a more advanced guide such as <em>All of Statistics</em> <span class="citation">[@Wasserman_2003]</span>, where statistical tests (t-test, chi-squared test, etc.), hypothesis testing, and analysis of uncertainties (confidence intervals, etc.) order the exposition,  machine learning textbooks rely on a conceptual apparatus curiously stripped of statistical tests and measurements. Statistical underpinnings may be fundamental, but this does not mean that machine learners simply automate statistics.</p>

<p>Instead, a basic set of contrasts or indeed oppositions that owe much to probabilistic thinking order, compose, associate and link the statements of machine learners.  The contrasts shown in Table  all have a statistical facet and anchoring to them. Some refer to errors that affect how a machine learner refers to data (bias and variance; see discussion below); some designate an underlying statistical intuition about how particular machine learners treat data (does the model seek to generate the data or classify – discriminate – it; e.g. Naive Bayes or Latent Dirichlet Allocation are  models whereas logistic regression or support vector machines are <em>discriminative</em>);   parametric and non-parametric describe the role of probability distributions in the model;  and others indicate different kinds of statistical knowledge practice (prediction seeks to anticipate while inference seeks to interpret, etc.; also see discussion below).  These broad structuring differences reach down deeply into the architecture, the diagrams, the practices, statements and visual objects and computer code associated with <span class="math inline">\(N=\forall\boldsymbol{X}\)</span>. Because they anchor basic operations of machine learning in probability, formalisms derived from statistics have in the last two decades increasingly populated the field, furnishing and rearranging its diagrammatic references  to the worlds of industry, agriculture, earth science, genomics, etc., but also, crucially, triggering ontological mutations in machine learners themselves. </p>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="35-data-reduces-uncertainty.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="37-distributed-probabilities.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
