
# 8. Optimising machine learners to learn on their own

## to do

- 'neural nets were the best for many years' [@Ng_2008b]. 
- 'generalization error is what we care about' [@Ng_2008f]
- deep neural nets [@Hassabis_2013] should appear here
- the Higgs Boson challenge
- MF in d&p writes about how the individual internalises the disciplinary mechanism -- we are all machine learners now?
- the masculinity of machine learning -- how to deal with that? some prominent women, but massively masculinist -- takes me back to 1996 publication - also use SI of angelaki on geophilosophy of masculinity. See zotero masculinity folder
- my experience with vlad, sitting apart from everyone working on the neural nets -- such a shift
- the people change alongside the data; their sense of the power of data has a cost for them too
- put in Perlich stuff about data leakage -- really important to focus on competition as a way of showing how people do things
- we finally reach people -- why so late? And so what?
- use Lazzarato here -- semiotics, etc
- MF from archaeology of knowledge on the subject in the discursive formation

## Introduction

> It is the privileged machine in this context that creates its marginalized human others [@Suchman_2006, 269]

> If a proposition, a sentence, a group of signs can be called 'statement' , it is not therefore because, one day, someone happened to speak them or put them into some concrete form of writing; it is because the position of the subject can be assigned [@Foucault_1972,  95]

In almost every machine learning class, neural nets make some appearance. They have an ambivalent status. In some ways, they renew long-standing hopes in the biological, and particularly, brains and cognition as models of computation. In other ways, they gain traction as a ways of dealing with changing computational infrastructures, and the difficulties of capitalising on infrastructure that is powerful yet difficult to manage. For instance, David Ackley, Geoffrey Hinton, an important figure in the inception of neural nets, and Terrence Sejnowski wrote in the early 1980s: 

> Evidence about the architecture of the brain and the potential of the new VLSI technology have led to a resurgence of interest in “connectionist” sys-terns ...  that store their long-term knowledge as the strengths of the connections between simple neuron-like processing elements. These networks are clearly suited to tasks like vision that can be performed efficiently in parallel networks which have physical connections in just the places where processes need to communicate. ... The more difficult problem is to discover parallel organizations that do not require so much problem-dependent information to be built into the architecture of the network. Ideally, such a system would adapt a given structure of processors and communication paths to whatever problem it was faced with [@Ackley_1985, 147-148].

This convergence between brain and 'new VSLI [Very Large Scale Integrated] technology' -- semiconductor chips -- sought to implement the plasticity of neuronal networks in the parallel distributed processing enabled by very densely packed semiconductor circuits. The problem here was how to organize these connections without having hardwire them into 'the architecture of the network.' How could the architectures adapt to the problem in hand?

We saw in \ref{ch:diagram} that the psychologist Frank Rosenblatt's perceptron [@Rosenblatt_1958] first implemented the cybernetic vision of neurones as models of computation [@Edwards_1996]. While the perceptron did not weather the criticism of artificial intelligence experts such as Marvin Minsky (Minsky famously showed that a perceptron cannot learn the logical exclusive OR or `XOR` function; [@Minsky_1969]), cognitive psychologists such as David Rumelhart, Geoffrey Hinton and Ronald Williams returned to work with perceptrons, seeking to generalize their operations. In the mid-1980s, they developed the back propagation algorithm \index{back propagation algorithm} [@Rumelhart_1985; @Hinton_1989], a way of adjusting the connections between nodes (neurones) in the network in response to features in the data. The back propagation algorithm did begin to address the problem of discovering parallel network organizations without reliance on problem-specific architectures. Effectively, an architecture of generalization was implemented. While cognition, and the idea that machines would be cognitive (rather than say, mechanical, calculative, or even algorithmic) constantly organised research work in artificial intelligence for several decades, the development of the back propagation algorithm as a way for a set of connected computational nodes to learn also had strong infrastructural resonances. These resonances continue to echo today. Like the advent of VSLI in the early 1980s, the vast concentrations of computers in contemporary data centres (hundreds of thousands of cores as we saw in the case of Google Compute in the previous chapter \ref{ch:genomic}) pose the problem of organizing infrastructure so that processes can communicate with each other. 

The back propagation algorithm will return below, but for the moment, this overlap between cognition and infrastructures itself suggests another way of thinking about how 'long-term knowledge' takes shape today. At the same time as infrastructural reorganization takes place around learning, and around the production of statements by machine learners, both human and non-human machine learners are assigned new positions. These positions show a good deal of dispersion, hierarchy and distribution. The subject position in machine learning is a highly relational one, connected to transforms in infrastructure, variations in referentiality (such as we have seen in the construction of the common vector space), and competing forms of authority (as we have seen in the accumulations of different techniques).

How would we describe the figure of human machine learner in this setting? In _The Archaeology of Knowledge_, Michel Foucault refers to the 'position of the subject' as an anchor point for the power-laden, epistemically conditioning enunciative functions called 'statements.' When a subject position can be assigned, propositions, diagrams, numbers, models, calculations and data structures can come together in statements, as 'the specific forms of an accumulation' [@Foucault_1972, 125]. But this anchor point is not a unifying point grounded in interiority, in intentionality or even in  single speaking position or voice (that of the  machine learning expert, for instance). On the contrary, 'various enunciative modalities manifest his [sic] dispersion' [@Foucault_1972, 54]. In the mist of this dispersion (a dispersion that is the main focus of this chapter), the position of subject is linked to operations that determine statements that become a kind of law for the subject. As Foucault puts it, in a formulation that effectively anticipates accounts of performativity that gain currency elsewhere several decades later,  

>in each case the position of the subject is linked to the existence of an operation that is both determined and present; in each case, the subject of the statement is also the subject of the operation (he who establishes the definition of a straight line is also he who states it; he who posits the existence of a finite series is also, and at the same time, he who states it) ; and in each case, the subject links, by means of this operation and the statement in which it is embodied , his future statements and operations (as an enunciating subject, he accepts this statement as his own law) [@Foucault_1972, 94-95]. 

It is indeed fitting that Foucault's examples here include subjects who say things like 'I call straight any series of points that ...,' since these are just the kinds of sentences that operate in machine learning. The _operation_, however, is crucial, since the operation opens onto many different practices and techniques (function finding, optimisation, transformation of data into the common vector space, mobilisation of probability distributions as a kind of rule of existence for learnable situations, etc.) that accompany, ornament, armour and diagram the statement. 

Neural nets receive uneven attention in the machine learning literature. Andrew Ng's Stanford CS229 lectures from 2007, they receive somewhat short shrift: around 30 minutes of discussion in Lecture 6, in between Naive Bayes classifiers and several lectures on support vector machines [@Ng_2008b]. As he introduces a video of an autonomous vehicle steered by a neural net after a 20 minute training session with a human driver, Ng comments that 'neural nets were the best for many years.' The lectures quickly moves on to the successor, support vector machines.  In _Elements of Statistical Learning_, a whole chapter appears on the topic, but prefaced by a discussion of the antecedent statistical method of 'projection pursuit regression.' The inception of 'projection pursuit' is dated to 1974, and thus precedes the 1980s work on neural nets that was to receive so much attention. In _An Introduction to Statistical Learning with Applications in R_, a book whose authors include Hastie and Tibshirani, neural nets are not discussed and indeed not mentioned [@James_2013]. Textbooks written by computer scientists such as Ethem Alpaydin's _Introduction to Machine Learning_ do usually include at least a chapter on them, sometimes under different titles such as 'multi-layer perceptrons' [@Alpaydin_2010].  Willi Richert and Luis Pedro Coelho's _Building Machine Learning Systems with Python_ likewise does not mention them. Cathy O'Neil and Rachel Schutt's _Doing Data Science_ mention them but don't discuss them [@Schutt_2013], whereas both Brett Lantz's _Machine Learning with R_ [@Lantz_2013] and Matthew Kirk's _Thoughtful Machine Learning_ [@Kirk_2014] devote chapters to them. In the broader cannon of machine learning texts, the computer scientist Christopher Bishop's heavily cited books on pattern recognition dwell extensively on neural nets [@Bishop_1995; @Bishop_2006]. Amongst statisticians, Brian Ripley's _Pattern Recognition and Neural Networks_ [@Ripley_1996], also highly cited, placed a great deal of emphasis on them.


The somewhat uneven presence of neural nets amongst machine learning literature finds parallels in the fortunes of machine learners. Yann LeCun's work on optical character recognition during 1980-1990s, and the implementations in `LeNet` led many machine learning  competitions. In 2007, Andrew Ng is casually observing that neural nets _were_ the best, but in  2014, LeCun find himself working on machine learning at Facebook. Similarly, the cognitive psychologist Geoffrey Hinton's involvement in the early 1980s work on connectionist learning procedures in neural nets and subsequently on 'deep learning nets' [@Hinton_2006] delivers him to Google in 2013. These trajectories between academic research and industry are not unusual. Many of the techniques in machine learning have been incorporated into companies later acquired by other larger companies. Even if there is no spin-off company to be acquired, machine learners themselves have been assigned key positions in many industry settings. Corinna Cortes, co-inventor with Vladimir Vapnik of the  support vector machine, heads research at Google New York. Ng himself in 2014 began work as chief scientist for the Chinese search engine, Baidu leading a team of AI researchers specializing in 'deep learning,' the contemporary incarnation of neural nets [@Hof_2014]. In 2011, Ng led a neural net-based project at Google that had, among other things, detected cats  in millions of hours of Youtube videos.[^8.1]

In recent years, (2012-2015), [TBA]

What accounts for the somewhat uneven fortunes of the neural net amongst machine learners? The unevenness of their performance, from curiosity to best, from second best to spectacular promise, suggests that some powerful dynamics or becomings are in play around them. These dynamics are not easily understood in terms of celebrity machine learners (human and non-human) suddenly rising to prominent or privileged positions in the research departments of social media platforms.[^8.2]  Nor does it make sense to attribute the rising fortunes of the neural net to the algorithms themselves. The algorithms used in neural net have not, as we will see, been radically transformed in their core operations. There have been important changes in scale (similar to those described in the previous chapter in the case of the `RF-ACE` algorithm and Google Compute). While machine learners in their machine form can be assigned a privileged position in the transformations of knowledge and action today, human machine learners are not exactly marginalized, at least in high profile cases such as Ng, LeCun, Hinton and others. Rather, the scale of machine learning seems to be changing both for the algorithms and for the human computer scientists, programmers and engineers. 

[^8.1]: Unlike the cats detected by `kittydar,` the software discussed in the introduction to this book, the Google experiment did not use supervised learning. The deep learning approach was unsupervised [@Markoff_2012]. That is, the neural nets were not given images in which cats were labelled to train on. 

[^8.2]: In any case, social media and search engines cannot be understood apart from the machine learning techniques that have been thoroughly woven through them since their inception.  Hence _Elements of Statistical Learning_ devotes several pages Google's famous _PageRank_ algorithm, describing it as an unsupervised learner [@Hastie_2009, 576-578].


## What do machine learners need to do?

'The central idea,' write Hastie and co-authors, 'is to extract linear combinations of the inputs as derived features, and then model the target as a nonlinear function of these features. The result is a powerful learning method, with widespread applications in many fields' [@Hastie_2009, 389]. 

>The big picture is that given data, a real-world classification problem, and constraints, you need to determine:
    1. Which classifier to use
    2. Which optimization method to employ
    3. Which loss function to minimize
    4. Which features to take from the data
    5. Which evaluation metric to use
    [@Schutt_2013, 116]

## People at the intersection or in a hierarchy

-- cf Ng, Hassabis, Hinton, Le Cun, etc. with others

## Competition and the work of optimisation

## Programming as intersectional practice

## The privilege of machine learning 

## Conclusion
