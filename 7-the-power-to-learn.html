<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Machine Learning: Archaeology of a Data Practice</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="Machine Learning: Archaeology of a Data Practice">
  <meta name="generator" content="bookdown 0.3 and GitBook 2.6.7">

  <meta property="og:title" content="Machine Learning: Archaeology of a Data Practice" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Machine Learning: Archaeology of a Data Practice" />
  
  
  

<meta name="author" content="Adrian Mackenzie">


<meta name="date" content="2016-12-10">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="6-machines-finding-functions.html">
<link rel="next" href="8-probabilisation-and-the-taming-of-machines.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="1-acknowledgments.html"><a href="1-acknowledgments.html"><i class="fa fa-check"></i><b>1</b> Acknowledgments</a></li>
<li class="chapter" data-level="2" data-path="2-preface.html"><a href="2-preface.html"><i class="fa fa-check"></i><b>2</b> Preface</a></li>
<li class="chapter" data-level="3" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html"><i class="fa fa-check"></i><b>3</b> Introduction: Into the Data</a><ul>
<li class="chapter" data-level="3.1" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#three-accumulations-settings-data-and-devices"><i class="fa fa-check"></i><b>3.1</b> Three accumulations: settings, data and devices</a></li>
<li class="chapter" data-level="3.2" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#who-or-what-is-a-machine-learner"><i class="fa fa-check"></i><b>3.2</b> Who or what is a machine learner?</a></li>
<li class="chapter" data-level="3.3" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#algorithmic-control-to-the-machine-learners"><i class="fa fa-check"></i><b>3.3</b> Algorithmic control to the machine learners?</a></li>
<li class="chapter" data-level="3.4" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#the-archaeology-of-operations"><i class="fa fa-check"></i><b>3.4</b> The archaeology of operations</a></li>
<li class="chapter" data-level="3.5" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#asymmetries-in-common-knowledge"><i class="fa fa-check"></i><b>3.5</b> Asymmetries in common knowledge</a></li>
<li class="chapter" data-level="3.6" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#what-cannot-be-automated"><i class="fa fa-check"></i><b>3.6</b> What cannot be automated?</a></li>
<li class="chapter" data-level="3.7" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#different-fields-in-machine-learning"><i class="fa fa-check"></i><b>3.7</b> Different fields in machine learning?</a></li>
<li class="chapter" data-level="3.8" data-path="3-introduction-into-the-data.html"><a href="3-introduction-into-the-data.html#the-diagram-in-critical-thought"><i class="fa fa-check"></i><b>3.8</b> The diagram in critical thought</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html"><i class="fa fa-check"></i><b>4</b> Diagramming machines}</a><ul>
<li class="chapter" data-level="4.1" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#we-dont-have-to-write-programs"><i class="fa fa-check"></i><b>4.1</b> ‘We don’t have to write programs’?</a></li>
<li class="chapter" data-level="4.2" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#the-elements-of-machine-learning"><i class="fa fa-check"></i><b>4.2</b> The elements of machine learning</a></li>
<li class="chapter" data-level="4.3" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#who-reads-machine-learning-textbooks"><i class="fa fa-check"></i><b>4.3</b> Who reads machine learning textbooks?</a></li>
<li class="chapter" data-level="4.4" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#r-a-matrix-of-transformations"><i class="fa fa-check"></i><b>4.4</b> <code>R</code>: a matrix of transformations</a></li>
<li class="chapter" data-level="4.5" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#the-obdurate-mathematical-glint-of-machine-learning"><i class="fa fa-check"></i><b>4.5</b> The obdurate mathematical glint of machine learning</a></li>
<li class="chapter" data-level="4.6" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#cs229-2007-returning-again-and-again-to-certain-features"><i class="fa fa-check"></i><b>4.6</b> CS229, 2007: returning again and again to certain features</a></li>
<li class="chapter" data-level="4.7" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#the-visible-learning-of-machine-learning"><i class="fa fa-check"></i><b>4.7</b> The visible learning of machine learning</a></li>
<li class="chapter" data-level="4.8" data-path="4-diagramming-machines.html"><a href="4-diagramming-machines.html#the-diagram-of-an-operational-formation"><i class="fa fa-check"></i><b>4.8</b> The diagram of an operational formation</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html"><i class="fa fa-check"></i><b>5</b> Vectorisation and its consequences}</a><ul>
<li class="chapter" data-level="5.1" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html#vector-space-and-geometry"><i class="fa fa-check"></i><b>5.1</b> Vector space and geometry</a></li>
<li class="chapter" data-level="5.2" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html#mixing-places"><i class="fa fa-check"></i><b>5.2</b> Mixing places</a></li>
<li class="chapter" data-level="5.3" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html#truth-is-no-longer-in-the-table"><i class="fa fa-check"></i><b>5.3</b> Truth is no longer in the table?</a></li>
<li class="chapter" data-level="5.4" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html#the-epistopic-fault-line-in-tables"><i class="fa fa-check"></i><b>5.4</b> The epistopic fault line in tables</a></li>
<li class="chapter" data-level="5.5" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html#surface-and-depths-the-problem-of-volume-in-data"><i class="fa fa-check"></i><b>5.5</b> Surface and depths: the problem of volume in data</a></li>
<li class="chapter" data-level="5.6" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html#vector-space-expansion"><i class="fa fa-check"></i><b>5.6</b> Vector space expansion</a></li>
<li class="chapter" data-level="5.7" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html#drawing-lines-in-a-common-space-of-transformation"><i class="fa fa-check"></i><b>5.7</b> Drawing lines in a common space of transformation</a></li>
<li class="chapter" data-level="5.8" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html#implicit-vectorization-in-code-and-infrastructures"><i class="fa fa-check"></i><b>5.8</b> Implicit vectorization in code and infrastructures</a></li>
<li class="chapter" data-level="5.9" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html#lines-traversing-behind-the-light"><i class="fa fa-check"></i><b>5.9</b> Lines traversing behind the light</a></li>
<li class="chapter" data-level="5.10" data-path="5-vectorisation-and-its-consequences.html"><a href="5-vectorisation-and-its-consequences.html#the-vectorised-table"><i class="fa fa-check"></i><b>5.10</b> The vectorised table?</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html"><i class="fa fa-check"></i><b>6</b> Machines finding functions}</a><ul>
<li class="chapter" data-level="6.1" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html#learning-functions"><i class="fa fa-check"></i><b>6.1</b> Learning functions</a></li>
<li class="chapter" data-level="6.2" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html#supervised-unsupervised-reinforcement-learning-and-functions"><i class="fa fa-check"></i><b>6.2</b> Supervised, unsupervised, reinforcement learning and functions</a></li>
<li class="chapter" data-level="6.3" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html#which-function-operates"><i class="fa fa-check"></i><b>6.3</b> Which function operates?</a></li>
<li class="chapter" data-level="6.4" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html#what-does-a-function-learn"><i class="fa fa-check"></i><b>6.4</b> What does a function learn?</a></li>
<li class="chapter" data-level="6.5" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html#observing-with-curves-the-logistic-function"><i class="fa fa-check"></i><b>6.5</b> Observing with curves: the logistic function</a></li>
<li class="chapter" data-level="6.6" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html#the-cost-of-curves-in-machine-learning"><i class="fa fa-check"></i><b>6.6</b> The cost of curves in machine learning</a></li>
<li class="chapter" data-level="6.7" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html#curves-and-the-variation-in-models"><i class="fa fa-check"></i><b>6.7</b> Curves and the variation in models</a></li>
<li class="chapter" data-level="6.8" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html#observing-costs-losses-and-objectives-through-optimisation"><i class="fa fa-check"></i><b>6.8</b> Observing costs, losses and objectives through optimisation</a></li>
<li class="chapter" data-level="6.9" data-path="6-machines-finding-functions.html"><a href="6-machines-finding-functions.html#gradients-as-partial-observers"><i class="fa fa-check"></i><b>6.9</b> Gradients as partial observers</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="7-the-power-to-learn.html"><a href="7-the-power-to-learn.html"><i class="fa fa-check"></i><b>7</b> The power to learn</a></li>
<li class="chapter" data-level="8" data-path="8-probabilisation-and-the-taming-of-machines.html"><a href="8-probabilisation-and-the-taming-of-machines.html"><i class="fa fa-check"></i><b>8</b> Probabilisation and the Taming of Machines}</a><ul>
<li class="chapter" data-level="8.1" data-path="8-probabilisation-and-the-taming-of-machines.html"><a href="8-probabilisation-and-the-taming-of-machines.html#data-reduces-uncertainty"><i class="fa fa-check"></i><b>8.1</b> Data reduces uncertainty?</a></li>
<li class="chapter" data-level="8.2" data-path="8-probabilisation-and-the-taming-of-machines.html"><a href="8-probabilisation-and-the-taming-of-machines.html#machine-learning-as-statistics-inside-out"><i class="fa fa-check"></i><b>8.2</b> Machine learning as statistics inside out</a></li>
<li class="chapter" data-level="8.3" data-path="8-probabilisation-and-the-taming-of-machines.html"><a href="8-probabilisation-and-the-taming-of-machines.html#distributed-probabilities"><i class="fa fa-check"></i><b>8.3</b> Distributed probabilities</a></li>
<li class="chapter" data-level="8.4" data-path="8-probabilisation-and-the-taming-of-machines.html"><a href="8-probabilisation-and-the-taming-of-machines.html#naive-bayes-and-the-distribution-of-probabilities"><i class="fa fa-check"></i><b>8.4</b> Naive Bayes and the distribution of probabilities</a></li>
<li class="chapter" data-level="8.5" data-path="8-probabilisation-and-the-taming-of-machines.html"><a href="8-probabilisation-and-the-taming-of-machines.html#spam-when-foralln-is-too-much"><i class="fa fa-check"></i><b>8.5</b> Spam: when <span class="math inline">\(\forall{N}\)</span> is too much?</a></li>
<li class="chapter" data-level="8.6" data-path="8-probabilisation-and-the-taming-of-machines.html"><a href="8-probabilisation-and-the-taming-of-machines.html#the-improbable-success-of-the-naive-bayes-classifier"><i class="fa fa-check"></i><b>8.6</b> The improbable success of the Naive Bayes classifier</a></li>
<li class="chapter" data-level="8.7" data-path="8-probabilisation-and-the-taming-of-machines.html"><a href="8-probabilisation-and-the-taming-of-machines.html#ancestral-probabilities-in-documents-inference-and-prediction"><i class="fa fa-check"></i><b>8.7</b> Ancestral probabilities in documents: inference and prediction</a></li>
<li class="chapter" data-level="8.8" data-path="8-probabilisation-and-the-taming-of-machines.html"><a href="8-probabilisation-and-the-taming-of-machines.html#statistical-decompositions-bias-variance-and-observed-errors"><i class="fa fa-check"></i><b>8.8</b> Statistical decompositions: bias, variance and observed errors</a></li>
<li class="chapter" data-level="8.9" data-path="8-probabilisation-and-the-taming-of-machines.html"><a href="8-probabilisation-and-the-taming-of-machines.html#does-machine-learning-construct-a-new-statistical-reality"><i class="fa fa-check"></i><b>8.9</b> Does machine learning construct a new statistical reality?</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="9-patterns-and-differences.html"><a href="9-patterns-and-differences.html"><i class="fa fa-check"></i><b>9</b> Patterns and differences</a><ul>
<li class="chapter" data-level="9.1" data-path="9-patterns-and-differences.html"><a href="9-patterns-and-differences.html#splitting-and-the-growth-of-trees"><i class="fa fa-check"></i><b>9.1</b> Splitting and the growth of trees</a></li>
<li class="chapter" data-level="9.2" data-path="9-patterns-and-differences.html"><a href="9-patterns-and-differences.html#differences-in-recursive-partitioning"><i class="fa fa-check"></i><b>9.2</b> 1984: Differences in recursive partitioning</a></li>
<li class="chapter" data-level="9.3" data-path="9-patterns-and-differences.html"><a href="9-patterns-and-differences.html#limiting-differences"><i class="fa fa-check"></i><b>9.3</b> Limiting differences</a></li>
<li class="chapter" data-level="9.4" data-path="9-patterns-and-differences.html"><a href="9-patterns-and-differences.html#the-successful-dispersion-of-the-support-vector-machine"><i class="fa fa-check"></i><b>9.4</b> The successful dispersion of the support vector machine</a></li>
<li class="chapter" data-level="9.5" data-path="9-patterns-and-differences.html"><a href="9-patterns-and-differences.html#differences-blur"><i class="fa fa-check"></i><b>9.5</b> Differences blur?</a></li>
<li class="chapter" data-level="9.6" data-path="9-patterns-and-differences.html"><a href="9-patterns-and-differences.html#bending-the-decision-boundary"><i class="fa fa-check"></i><b>9.6</b> Bending the decision boundary</a></li>
<li class="chapter" data-level="9.7" data-path="9-patterns-and-differences.html"><a href="9-patterns-and-differences.html#instituting-patterns"><i class="fa fa-check"></i><b>9.7</b> Instituting patterns</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="10-regularizing-and-materializing-objects.html"><a href="10-regularizing-and-materializing-objects.html"><i class="fa fa-check"></i><b>10</b> Regularizing and materializing objects}</a><ul>
<li class="chapter" data-level="10.1" data-path="10-regularizing-and-materializing-objects.html"><a href="10-regularizing-and-materializing-objects.html#genomic-referentiality-and-materiality"><i class="fa fa-check"></i><b>10.1</b> Genomic referentiality and materiality</a></li>
<li class="chapter" data-level="10.2" data-path="10-regularizing-and-materializing-objects.html"><a href="10-regularizing-and-materializing-objects.html#the-genome-as-threshold-object"><i class="fa fa-check"></i><b>10.2</b> The genome as threshold object</a></li>
<li class="chapter" data-level="10.3" data-path="10-regularizing-and-materializing-objects.html"><a href="10-regularizing-and-materializing-objects.html#genomic-knowledges-and-their-datasets"><i class="fa fa-check"></i><b>10.3</b> Genomic knowledges and their datasets</a></li>
<li class="chapter" data-level="10.4" data-path="10-regularizing-and-materializing-objects.html"><a href="10-regularizing-and-materializing-objects.html#the-advent-of-wide-dirty-and-mixed-data"><i class="fa fa-check"></i><b>10.4</b> The advent of ‘wide, dirty and mixed’ data</a></li>
<li class="chapter" data-level="10.5" data-path="10-regularizing-and-materializing-objects.html"><a href="10-regularizing-and-materializing-objects.html#cross-validating-machine-learning-in-genomics"><i class="fa fa-check"></i><b>10.5</b> Cross-validating machine learning in genomics</a></li>
<li class="chapter" data-level="10.6" data-path="10-regularizing-and-materializing-objects.html"><a href="10-regularizing-and-materializing-objects.html#proliferation-of-discoveries"><i class="fa fa-check"></i><b>10.6</b> Proliferation of discoveries</a></li>
<li class="chapter" data-level="10.7" data-path="10-regularizing-and-materializing-objects.html"><a href="10-regularizing-and-materializing-objects.html#variations-in-the-object-or-in-the-machine-learner"><i class="fa fa-check"></i><b>10.7</b> Variations in the object or in the machine learner?</a></li>
<li class="chapter" data-level="10.8" data-path="10-regularizing-and-materializing-objects.html"><a href="10-regularizing-and-materializing-objects.html#whole-genome-functions"><i class="fa fa-check"></i><b>10.8</b> Whole genome functions</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="11-propagating-subject-positions.html"><a href="11-propagating-subject-positions.html"><i class="fa fa-check"></i><b>11</b> Propagating subject positions}</a><ul>
<li class="chapter" data-level="11.1" data-path="11-propagating-subject-positions.html"><a href="11-propagating-subject-positions.html#propagation-across-human-machine-boundaries"><i class="fa fa-check"></i><b>11.1</b> Propagation across human-machine boundaries</a></li>
<li class="chapter" data-level="11.2" data-path="11-propagating-subject-positions.html"><a href="11-propagating-subject-positions.html#competitive-positioning"><i class="fa fa-check"></i><b>11.2</b> Competitive positioning</a></li>
<li class="chapter" data-level="11.3" data-path="11-propagating-subject-positions.html"><a href="11-propagating-subject-positions.html#a-privileged-machine-and-its-diagrammatic-forms"><i class="fa fa-check"></i><b>11.3</b> A privileged machine and its diagrammatic forms</a></li>
<li class="chapter" data-level="11.4" data-path="11-propagating-subject-positions.html"><a href="11-propagating-subject-positions.html#varying-subject-positions-in-code"><i class="fa fa-check"></i><b>11.4</b> Varying subject positions in code</a></li>
<li class="chapter" data-level="11.5" data-path="11-propagating-subject-positions.html"><a href="11-propagating-subject-positions.html#the-subjects-of-a-hidden-operation"><i class="fa fa-check"></i><b>11.5</b> The subjects of a hidden operation</a></li>
<li class="chapter" data-level="11.6" data-path="11-propagating-subject-positions.html"><a href="11-propagating-subject-positions.html#algorithms-that-propagate-errors"><i class="fa fa-check"></i><b>11.6</b> Algorithms that propagate errors</a></li>
<li class="chapter" data-level="11.7" data-path="11-propagating-subject-positions.html"><a href="11-propagating-subject-positions.html#competitions-as-examination"><i class="fa fa-check"></i><b>11.7</b> Competitions as examination</a></li>
<li class="chapter" data-level="11.8" data-path="11-propagating-subject-positions.html"><a href="11-propagating-subject-positions.html#superimposing-power-and-knowledge"><i class="fa fa-check"></i><b>11.8</b> Superimposing power and knowledge</a></li>
<li class="chapter" data-level="11.9" data-path="11-propagating-subject-positions.html"><a href="11-propagating-subject-positions.html#ranked-subject-positions"><i class="fa fa-check"></i><b>11.9</b> Ranked subject positions</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="12-conclusion-out-of-the-data.html"><a href="12-conclusion-out-of-the-data.html"><i class="fa fa-check"></i><b>12</b> Conclusion: Out of the Data}</a><ul>
<li class="chapter" data-level="12.1" data-path="12-conclusion-out-of-the-data.html"><a href="12-conclusion-out-of-the-data.html#machine-learners"><i class="fa fa-check"></i><b>12.1</b> 250,000 machine learners</a></li>
<li class="chapter" data-level="12.2" data-path="12-conclusion-out-of-the-data.html"><a href="12-conclusion-out-of-the-data.html#a-summary-of-the-argument"><i class="fa fa-check"></i><b>12.2</b> A summary of the argument</a></li>
<li class="chapter" data-level="12.3" data-path="12-conclusion-out-of-the-data.html"><a href="12-conclusion-out-of-the-data.html#in-situ-hybridization"><i class="fa fa-check"></i><b>12.3</b> In-situ hybridization</a></li>
<li class="chapter" data-level="12.4" data-path="12-conclusion-out-of-the-data.html"><a href="12-conclusion-out-of-the-data.html#critical-operational-practice"><i class="fa fa-check"></i><b>12.4</b> Critical operational practice?</a></li>
<li class="chapter" data-level="12.5" data-path="12-conclusion-out-of-the-data.html"><a href="12-conclusion-out-of-the-data.html#obstacles-to-the-work-of-freeing-machine-learning"><i class="fa fa-check"></i><b>12.5</b> Obstacles to the work of freeing machine learning</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning: Archaeology of a Data Practice</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="the-power-to-learn" class="section level1">
<h1><span class="header-section-number">7</span> The power to learn</h1>
<p>The power of machine learning to learn, its power to epistemologize, pivots around functions in disparate yet connected ways: the transformation of data through operational functions that map new sub-spaces in vector space and in the observational functions that algorithmically superimpose new constraints – cost, loss or objective functions – that direct an iterative process of optimisation.   Machine learning diagrammatically distributes learning in the operational human-machine formation. People look at curves for evidence of convergence, functions compress data into functions that support classification or predictions, and algorithms observe gradients or rates of error in relation to model parameters. In several senses, people and machines together move along curves. The logistic function folds the lines that best fit the data into a probability distribution that can be read in terms of classification. The cost functions, as they seek to minimize differences between the predicted values and the known values found in the vector space, control variations in the model. Every observer in this domain is partial, since the humans cannot see lines or curves in the multi-dimensional data, the functions that underpin models such as logistic regression or linear regression can transform data in the vector space, but can’t show how well they see it, and the processes of optimisation only see the results of the model and its errors, not anything in its referential functioning.  Universality (<em>apropos</em> master algorithms), whether fully supervised or completely unsupervised, is impossible here.</p>
<p>Amidst this endemic partiality, we can begin to understand the multiplication of machine learners and the mirage of universality. Machine learners are functions that transform data and observe the effects of those transformation in learning to classify, predict and rank. But the function that defines a ‘machine learner’ contracts a range of partial observers relaying values and changes to each other.  The operational power of machine learning depends on the diagrammatic and sometimes experimental relays between different practices of observing. Attending to specific mathematical functions in isolation – the logistic function, the Lagrangean, the Gaussian, the quadratic discriminant, etc. – will not tell us how the operational power of functions comes together in machine learning, but it may provide ways of mapping the diagrammatic connections, the enunciative function that connects different elements in the production of consequential classifications and predictions, generating operational statements in fields of knowledge. </p>
<p>We are in a slightly better position to understand now how there can be many machine learners but a relative sparsity in the production of statements.  Gradients – continuous variations in rate – are useful because they generate many functions, many approximations within one operational process, within one enunciative function. The profusion of machine learners, the ‘bewildering variety’ that Domingos and others celebrate and flag up, can be seen as the effect of an operational formation predicated on approximation through variation.</p>
<p>In his account of Foucault’s diagrams of power, Gilles Deleuze writes:</p>
<blockquote>
<p>every diagram is intersocial and constantly evolving. It never functions in order to represent a persisting world but produces a new kind of reality, a new model of truth. … It makes history by unmaking preceding realities and significations, constituting hundreds of points of emergence or creativity, unexpected conjunctions or improbable continuums <span class="citation">[@Deleuze_1988, 35]</span>  </p>
</blockquote>
<p>Functions in machine learning are ‘intersocial’ in the sense that they bring together very different mathematical, algorithmic, operational and observational processes. The sigmoid function switches the geometry of the linear model over into the calculation of probabilities and classification, but also figures heavily in the computability of partial derivatives. The cost functions re-craft statistical modelling as a quasi-iterative process of model generation and comparison. New kinds of realities arise in which the classifications and predictions generated by the diagonal connections between mathematical functions and operational processes of optimisation can constitute a ‘new model truth’ and can unmake ‘preceding realities and significations.’ And despite my deliberately narrow focus on a single set of relays that connect linear models, the logistic function, the cost function and gradient ascent, there are hundreds and perhaps and hundreds of thousands of ‘points of emergence’ associated with this diagram of functioning.</p>
<p>The machine learning diagram, like any functioning, harbours the potential for invention. Describing the application of machine learning to biomedical and clinical research, James Malley, Karen Malley and Sinisa Pajevic contrast it to more conventional statistical knowledges:</p>
<blockquote>
<p>working with statistical learning machines can push us to think about novel structures and functions in our data. This awareness is often counterintuitive, and familiar methods such as simple correlations, or slightly more evolved partial correlations, are often not sufficient to pin down these deeper connections. <span class="citation">[@Malley_2011, 5-6]</span>  </p>
</blockquote>
<p>Novel structures and functions in ‘our data’ are precisely the functions that machine learning technique seek to learn. Could new habits or actively diverging worlds that Stengers calls for appear amidst this quasi-iterative pursuit of optimisation and convergence? This is a terrain for critical thought to explore. A function in isolation never learns. But when watched or observed, even virtually, divergence has some chance. To the extent that machine learners relay references experimentally between things and people, mobilising the production of statements and visibilities across different elements, divergence remain possible.</p>

</div>
            </section>

          </div>
        </div>
      </div>
<a href="6-machines-finding-functions.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="8-probabilisation-and-the-taming-of-machines.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
