# Finding functions: lines and curves

## techniques
- logistic regression - on what?
- *k-nn* 
- decision tree

## examples
- housing price prediction; 
- cancer prognosis; 
- digit recognition; 
- credit scoring

## overview
- connection to matrix/vector manipulations
- machine learning as finding an approximation to the function that generated the data
    - the linear model
- can functions learn?
- can we learn functions?
	- diff  notions of function
	- translation between 2 senses of function
	- finding functions through solving vs through approximation - gradient descent
- the visual forms of data -- nearly always a curve or line to find
    - Tim Ingold on lines
    - probability density functions as curves
- Deleuze and other on  functions
- gradient descent
    - cost functions and optimisation
    - partial observers

## quotes to use

The problematization of classifications, practices, things is an event.  Rabinow

##  D & G

science brings to light partial observers  in relation to functions within systems of reference.129

### Whitehead

Thus beyond all questions of quantity, there lie questions of pattern, which are essential for the understanding of nature. Apart from a presupposed pattern, quantity determines nothing. Indeed quantity itself is nothing other than analogy of functions within analogous patterns 195

It is the reason why ... in our direct apprehension of the world around us we find that curious habit of claiming a two-fold unity with the observed data. We are in the world and the world is in us. W, MoT, 227

'Value' is the world I use for the intrinsic reality of an event.  ... We have only to transfer to the very texture of realisation in itself that value which we recognise so readily in terms of human life. SMW, 116

The aboriginal data in terms of which pattern weaves itself are the shapes, of sense-objects, and of other eternal objects whose self-identity is not dependent on the flux of things SMW, 187-8

### Stengers:

the challenge, which I deem a materialist challenge, is that whatever the mess and perplexity that may result, we should resist the temptation to pick and choose among practices Stengers, wondering, 2011, 379

I propose as a materialist motto: we never get a relevant answer if our practices have not enabled us to produce a relevant question Stengers, wondering, 373
Celebrating the exceptional character of the experimental achievemnt very effectively limits the claims made in the name of science. Stengers, wondering, 376

Taking seriously the singularity of experimental practices aslo leads us to understand the strong possibility of their destruction by the coming knowledge economy. The points is not that the scientific enterprise would lose a neutrality it never had.  … What is at risk is rather the very social fabric of scientific reliability, that is, the constitutive relation between an epxerimental achievmenet and the gathering of what can be called 'competent colleagues' Stengers, wondering, 377

## Introduction

## The scientific function and its data

Machine learning  treats data as examples from which something might be learned.  Learning in machine learning means finding a function that can identify or predict patterns in the data. As _Elements of Statistical Learning_ puts it, ' our goal is to find a useful approximation $\hat{f}(x)$ to the function $f(x)$ that underlies the predictive relationship between input and output' [@hastie_elements_2009, 28].  Or as a leading theorist of learning theory Vladimir Vapnik puts it, 'learning is a problem of _function estimation_ on the basis of empirical data' [@vapnik_nature_1999, 291]. The use of the term 'learning' in machine learning displays affiliations to the field of artificial intelligence, but the  attempt to find a 'useful approximation' -- the 'function-fitting paradigm' as [@hastie_elements_2009, 29] terms it -- stems mainly from statistics.  Not all accounts of machine learning frame the techniques in terms of function fitting. Some retain a much more explicit commitment to the notion of intelligent machines (see for example, [@alpaydin_introduction_2010, xxxvi] who writes: 'we do not need to come up with new algorithms if machines can learn themselves'). Despite any differences in the  framing of the techniques, all accounts of machine learning, even those such as _Machine Learning for Hackers_ that eschew any explicit recourse to mathematical formula,  depend on the  formalism and modes of thought associated with mathematical functions. Whether they are seen as forms of artificial intelligence or statistical models, the formalisms are directed to build 'a good and useful approximation to the desired output' [@alpaydin_introduction_2010, 41], or 'to use the sample to find the function from the set of admissable functions that minimizes the probability of error' [@vapnik_nature_1999, 31]. The linear  regression model that fits a line to a set of points ($\hat{Y} = X^T \hat{\beta}$) is just such a useful approximation to  the actual function that generated the data. The kind of visual pattern it identifies is really elementary: a straight line. The lengths to which machine learning is prepared to go to fit lines to situations is, as we will see, quite extraordinary.  The linear model undergoes some drastic deformations as lines stretch and fold into planes, hyperplanes,  and various curved and  fitted surfaces. 

Whether the function that 'underlies the predictive relationship' takes the form of a line, a plane, a curve, a tree, a forest, an ensemble, or a map as its models matters less than the  way in which an approximation is found or made. In making an approximation, machine learning practitioners make various decisions.  A basic decision, described in the first pages of any machine learning textbook, concerns whether the model they construct will do   supervised or unsupervised learning. Most machine learning uses supervised learning: it uses 'the presence of the outcome variable to guide the learning process' [@hastie_elements_2009, 2]. Less important yet still widely used, unsupervised learning has 'no measurements of the outcome' and seeks rather 'to describe how the data are organized and clustered' (2). (_k_-means is a widely used clustering algorithm). Whether  supervised or unsupervised, every machine learner implements some kind of model, and the choice of model cannot be formalised or algorithmically decided. It is impossible in principle to known whether a decision tree, a random forest, a neural network, a support vector machine or least angle regression model will work best. In practice people have favourite models, and have varying degrees of experience in implementing those models. In pattern recognition problems (for instance, in learning to recognise faces), neural networks are popular because they can be tuned to handle the many variations in patterns associated with images. In biomedical research, logistic regression and decision trees are widely used because they are easier to interpret.  While there is some agreement in the academic machine learning and data-mining communities based on the outcome of annual competitions between algorithms on difficult problems (for instance, the Association of Computing Machinery SIGKDD - Special Interest Group Knowledge Data Discovery runs the 'KDD Cup' annually [@kdd_call_2013]; Chapter 4 discusses machine learning competitions in some detail), new techniques or combinations of techniques constantly change the rule of thumb for model selection. 

A couple of critical questions present themselves when it comes to the practices of function finding and pattern recognition in machine learning. The first of these is fundamental: how can a function  learn? The philosopher of science Isabelle Stengers writes:

 > No function can deal with learning, producing, or empowering new habits, as all require and achieve the production of different worlds, non-consensual worlds, actively diverging worlds [@stengers_deleuze_2005, 162]

I think this might not be quite right, or at least, it might  strictly limit our relation to functions.  In some ways, it is a fairly conventional position to take on mathematical functions.  They cannot learn or produce anything, only reproduce patterns that we already recognise.  Similar statements might be found in many philosophical writings on science. But unlike many philosophers of science,  in her writing Stengers explicitly affirms the achievements of experimental practice in order to defend science against being engulfed by the demands of the knowledge economy. She limits the claims made about science by seeking to highlight the specific power of science: 'celebrating the exceptional character of the experimental achievement very effectively limits the claims made in the name of science' [@stengers_wondering_2011, 376]. Limiting claims made for science might save  it from being totally re-purposed as a techno-economic innovation system. 

The problem here is that connection between a function and a given concrete experimental situation is highly contingent. Stengers argues that the ways in which mathematical functions impinge on matters of fact depends on a reference between a function and matter of fact constructed experimentally.   On this point, Stengers is justifiably adamant: 

>The reference of a mathematical function to an experimental matter of fact is neither some kind of right belonging to scientific reason nor is it an enigma, but actually the very meaning of an experimental achievement [@stengers_deleuze_2005, 157].

The generic term 'reference' here harbours a multitude of relations. The experiment achievement, the distinctive power of science, works through a tissue of relations that connect people, things, facts and mathematical functions in a highly heterogeneous weave. (This point has often been made in the social studies of science; see[@TBA]). When a biomedical experiment uses the  statistical procedure of _logistic regression_ to  'estimate the probability that a critically ill lups patient will not survive the firest 72 hours of an initial emergency hospital visit' [@malley_statistical_2011, 5],  they are doing machine learning, and the value of their predictions is not captured by classical statistical approaches (analysis of variance, correlations, regression analysis, etc). As machine learning techniques and the underpinning mathematics of probabilistic learning theory circulate more widely across different scientific disciplines (geography, ecology, astronomy, epidemiology, genomics, chemistry, communication engineering), in each setting the experimental achievement consists in constructing references between the mathematical functions and the matters of fact generated by instruments, observations and measurements and cantilevered by previous experiments.  The question for Stengers is whether structure of referrals through experiments and the accumulated knowledge will be maintained if functions are said to learn. 

From  this perspective, claiming that mathematical functions can learn, or that algorithmic implementations of functions can learn (as in machine learning) might seem  tantamount to putting mathematics solely in the hands of the people who make predictions for finance, for customer relations management or for surveillance purposes, or in other words, those who have the technical capacity to collect large amounts of data and to build models based on it.  But from the standpoint of experimental practice, the problem here is not so much who uses the techniques, but whether or not the systems of reference that connects the function to a state of affairs is experimental. In many of the cases I've just mentioned, the system of reference is not very experimental. The learning, if there is any, is much more focused on re-making worlds such that they can be described, approximated or predicted by models.  When machine learning techniques become functions that classify who should have a bank account and who shouldn't, who should be offered a cheap deal on their next purchase and who shouldn't, or who should be allowed into the country and who shouldn't, the margins of experiment are tightly limited, tend to exclude the possibility of surprising results or objections from other interested parties. Much of what I discuss in this chapter will be seeking to establish a different relation to functions, a relation in which it becomes harder to say who or what is learning. That would be useful in that it neither says that functions learn or that humans learn, but together some learning might occur. It might also be useful in keeping open some space in which surprising realtions to the machine learning models remains possible. At least as I see it, the possibility of relating to  the functions differently is indispensable if we want the models and their mathematical functions to generate something we want to learn about. 

## Learning functions: 'read it, cover it, and do it yourself'

The second critical question is related to the first, and seemingly easier: how can we learn functions? There are two main senses of function in machine learning, and I have already been using both of them implicitly. The first mathematical sense refers to a relation between sets of values or variables. (A variable is a symbol that can stand for a set of numbers or other values.) A function is one-to-one relation between two sets of values. It maps a set of arguments (inputs) to a set of values (outputs, or to use slightly more technical language, it maps between a _domain_ and a _co-domain_. As we have already seen, mathematical functions are often written in formulae of varying degress of complexity. They are of various genres, provenances, textures and shapes: polynomial functions, trigonometric functions, exponential functions, differential equations, series functions, algebraic or topological functions, etc. Various fields of mathematics have pursued the invention of functions. In machine learning and information retrieval, important functions would include the logistic function (discussed below), probability density functions (PDF) for different probability distributions (Gaussian, Bernoulli, Binomial, Beta, Gamma; I will discuss these in greater depth in Chapter 5-6), cost functions, Langrangian functions, etc. Not all functions take numbers as inputs or outputs. Letters, words or almost any other symbol can be values in a function. 

While functions are often written in formulae, they can be written in different formulae and expressed in different graphic or sometimes geometric forms. Take the example of the logistic (or sigmoid) function. It can be written as:

$f(x) = 1/(1+e^{-x})$

It can be graphed as:

```{r logistic, echo=TRUE, cache=TRUE, message=FALSE, warning=FALSE, comment=NA, size='smallsize', results='markup'}
	
	x = seq(-100, 100, 0.01)
	y = 1/(1+exp(-x))
	plot(x,y)

```
This curviness of this function, as we will see, is very important in many classification and decision settings. How does it get into these settings? The second sense of function comes from programming and computer science. A function there is a part of the code of a program that performs some operation. The three lines of R code written to produce the plot of the logistic function are almost too trivial, but they show something of the transformations that occur when mathematical functions are operationalised in algorithmic form. The function is wrapped in a set of references. First, the domain of $x$ values is made much more specific. The formulaic expression $f(x) = 1/(1+e^{-x})$ says nothing explicitly about the $x$ values. They are implicitly real numbers (that is, $x \in \mathbb{R}$) in this formula but in the algorithmic expression of the function they become a sequence of `r length(x)` generated by the code. Second, the function itself is flattened into a single line of characters in code, whereas the typographically the mathematical formula had spanned 2-3 lines. Third, a key component of the function $e^-x$ itself refers to Euler's number $e$, which is perhaps the number most widely used in contemporary sciences due to its connection to patterns of growth and decay (as in the exponential function $e^x$ where $e = 2.718282$ approximately).  This number, because it is 'irrational,' has to be computed approximately in the algorithmic implementation. Finally, the plot of the function invokes a whole set of spatial and graphic conventions. For instance, it shows the $x$ values along aa horizontal axis, with negative values on the left and positive values on the right, and the $y$ values on a vertical axis at right angles to the $x$ axis, etc. These transformation between the formula expression of the function, the algorithmic and the graphic form are very mundane, mostly taken for granted in contemporary data practice. But in certain cases, they become much problematic and unstable. 

This description of the differences between functions in a mathematical sense as a mapping and functions in an algorithmic sense as an implementation of some operations that might express a mathematical function is meant to highlight a key issue in learning functions. As we move from the mathematical formula to the three lines of R code that produces a plot of the function what has happened?  This is a kind of implementation of a function, and perhaps we learn something about the logistic function, that for instance, that it the $y$ values change very rapidly, almost decisively between $0$ and $1$ across a very brief interval of $x$ values. Unlike the linear functions we saw in the house-price models (see previous chapter), logistic functions switch between $1$ and $0$, or other values such as  `yes` and `no`.  This rapid change in value will, as we see, prove incredibly useful machine learning. But the way in which we have learned the logistic function  by taking a textbook formula expression of it, implementing it in code and then plotting the function is not the way that machine learning typically learns the function that maps between the input data and the output variables (the so-called 'response variable'). It is not the way that mathematicians typically learn functions.  Much mathematical practice takes the form of finding a function that satisfies a set of constraints or limits. Typically, functions are learned by solving a problem posed in terms of equations. Mathematics textbooks are replete with demonstrations of this problem-solving activity, and learning mathematics is in large part becoming practiced in solving problems by finding a functions. Even in machine learning, some function-finding through solving systems of equations occurs.  For instance, the closed form solution of the least sum of squares problem for linear regression is given by  $\hat{\beta} = (\mathbf{X}^T\mathbf{X})^-1\mathbf{X}^T\mathbf{y}$. As we saw in the previous chapter, this expression provides a very quick way to calculate the parameters of a linear model given a matrix of input and output values.  This formula itself is derived by solving a set of equations for the values $\hat{\beta}$, the estimated parameters of the model. In Week 2 of  CS229 'Machine Learning' lectures of 2008, Andrew Ng writes the steps needed to derive this closed form solution on the blackboard in the Stanford lecture theatre. He advises students to  learn these derivations. To learn machine learning, he advocates, he should 'read it, cover over the derivation and then do it yourself' [@stanforduniversity_lecture_2008]. The transitions between mathematically formalised functions, the lines of code, graphic plots, and the deductively derived solutions that we have just discussed are neither, as Stengers points out, evidence of scientific reason or enigmatic correspondences. The paths between these different practices are quite convoluted, and for any particular machine learning situation, they have to be negotiated and composed.

## Functions: at what cost?

As we saw earlier, in his formulation of the 'learning problem', Vladimir Vapnik speaks of choosing a function that approximates to the data, yet  minimises the 'probability of error' [@vapnik_nature_1999, 31]. Unique 'closed form' solutions are quite unusual in machine learning. In machine learning, functions are normally learned through a process of approximation and optimisation that has little resemblance to the deductive solving of equations.  Even the apparently simplest data modelling procedure of fitting a line to a set of points is usually implemented differently in machine learning settings. This is a point where machine learning differs substantially from conventional statistical techniques. The use of approximation, optimisation and heuristic learning approaches is much more prevalent for reasons that have to do with the situations in which machine learning is put work.  For instance, describing the  application of machine learning to biomedical and clinical research, James Malley, Karen Malley and Sinisa Pajevic  contrast it to more conventional statistical approaches:

> working with statistical learning machines can push us to think about novel structures and functions in our data. This awareness is often counterintuitive, and familiar methods such as simple correlations, or slightly more evolved partial correlations, are often not sufficient to pin down these deeper connections. [@malley_statistical_2011, 5-6]

The novel structures and functions in 'our data' are  precisely the functions that machine learning technique seek to learn. As we will see, these structures and functions take various forms and shapes (lines, trees, curves, peaks, valleys, forests,  boundaries, neighbourhoods, etc.), and they can identify 'deeper connections' than the correlations we have saw in the house price or iris datasets.  

But how do we know whether a model is a good one, or that the function that a model proffers to us fits the functions in our data. (If it is not already obvious, in the world of machine learning it is simply taken as read that functions are in the world; 'Nature' has been thoroughly mathematised here.) One problem with  closed-form or analytical solutions typical of mathematical problem-solving is precisely their closed-form. To continue with key example of the closed form solution for linear regression ($\hat{\beta} = (\mathbf{X}^T\mathbf{X})^-1\mathbf{X}^T\mathbf{y}$), we see that it estimates the parameters of the linear model by carrying out a series of operations on matrices of the data. These operations include matrix transpose, several matrix multiplications (so-called 'inner product') and matrix inversion (the process of finding a matrix that when multiplied by the input matrix yields the identity matrix, a matrix with $1$ along the diagonal, and $0$ for all other values).  When the dataset has a hundred or a thousand rows, these operations can be implemented and executed easily. But as soon as datasets become much larger, it is not easy to actually carry out these matrix operations even on fast computers. For instance, a dataset with a million rows and several dozen columns is hardly unusual today. Although linear algebra libraries are  carefully crafted and tested for speed and efficiency, there is no way that they can quickly carry out matrix multiplication (inner products) on million row datasets in a reasonable time. The closed form solution, even for the simplest possible structures in the data, begins to break down in this situation.  If, for instance, instead of working with the San Francisco house price dataset used in the previous chapter, we used much older Boston house price dataset available from the University of California Irvine machine learning data repository (http://archive.ics.uci.edu/ml/datasets/Housing), and tried to calculate the parameters of the model using the closed form solution, problems of scale arise immediately. With only 500 rows of data, and  12 variables, calculation of the parameters for a linear model to predict house prices is still workable, but with 5000 rows, my laptop starts to struggle for  minutes at a time.  With hundreds of variables and millions of rows, the closed form solution becomes increasingly unworkeable.

```{r name, echo=TRUE, cache=TRUE, message=FALSE, warning=FALSE, comment=NA, size='smallsize', results='markup', engine='python' } 

	import sklearn.datasets
	import numpy as np
	boston = sklearn.datasets.load_boston()
	boston.data.shape
	x = boston.data[:,0:10]
	y = boston.data[:,11]
	beta_hat = np.linalg.pinv(x.dot(x.transpose())).dot(x.transpose().dot(y))
```
There is another problem with the closed form approximation. The closed form solution is run once, and the model it produces is subject to no further computation, elaboration or variation. That is, the closed form solution  based on the so-called 'normal equations' delivers a fixed solution. The parameters $\hat{beta}$ define the line of best fit for the datasets on which they are based. A more typical machine learning approach  is to find a way of modelling how well the model has dealt with the data. It replaces the exactitude and precision of mathematically-deduced closed-form solutions with algorithms that search for a good solution, not 'the' solution. The combination of all the variables can be imagined as a surface whose contours include peaks and valleys. As we have seen, a linear model tries to find a line or plane or hyperplane (a higher dimensional plane) that fits this topography. Many different planes more or less fit the contours, but how do we choose the best one?  If we can't produce an exact answer to this problem, we could spend time trying out different parameters, varying some, and keeping the others the same until we find a set of parameters that seems to fit well.This is a classic machine learning scenario, where the machine is meant to learn something that programmers, engineers, scientists or statisticians cannot. How would the machine learning approach the line/plane of best fit?


In many machine learning techniques (especially in the so-called 'supervised learning'), the search for an approximation to the function that generated the data is guided by another function called the 'cost function' (also known as the 'objective function' or the 'loss function'; both terms are somewhat evocative). There are various ways of learning functions, but cost functions are an essential component of many machine learning models. Deciding on the cost function means thinking about how the predictions relate to the values in the data set. Every cost function implicitly has some measure of the difference or distance between the prediction and the values actually measured. In supervised learning, cost functions work with the known values. Cost functions stage 'the act of fitting a model to data as an optimization problem' [@conway_machine_2012, 183]. Having defined a cost function, the learning algorithm can fit many models to the data, and use the cost function to decide which fits best. The cost functions themselves only provide a way of testing whether a given model performs better than another model in terms of changes in the parameters. So, the kind of model or type of prediction it performs does not change radically. If there is learning here, it is not some enigmatic form or a higher form of scientific reason. Just the opposite, the cost function does something more like create a place from which variations in  models can be viewed. 

A typical and widely cost function associated with regression is defined as: 

> $J(\beta) := min_(\beta) \frac{1}{m} \sum\limits^m(h_\beta(x^{(i)} - y^{(i)}))^2$

Again, the reading this kind of formula expression of a function is not easy. But several key terms stand out. First, the cost function $J(\beta)$ is a function of all the parameters of the model. Second, the function is defined in terms of the goal of minimizing the overall value of the expression. The $min$ describes the results of the repeated application of the function. Third, the heart of the function is a kind of average: it adds ($\sum$) all the  differences between the values of $y$ predicted by the model and the known values of $y$, and divides them by the number of values of y ($\frac{1}{m}$). This  is the so-called 'mean squared error' measure of prediction, a core measure of prediction in machine learning, and the basis of many standard statistical procedures. Mean squared error (MSE) is so taken for granted that machine learning textbooks such as _The Elements of Statistical Learning_ [@hastie_elements_2009] often rely on it without any further explanation. 

Importantly, the cost function itself does not say anything about how the values of the parameters are to be found. They could be generated randomly, or perhaps could be just a range of values  (e.g. 0 to 100). 

```{r gradient, echo=TRUE, cache=TRUE, message=FALSE, warning=FALSE, comment=NA, size='smallsize', results='markup' } 

	url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data'
	boston_house =read.delim(url, sep='', header=FALSE)
	X = as.matrix(boston_house[, 1:13])
	Y = as.matrix(boston_house[,14])

	//normalise the data
	X_norm =cbind(X[,1], apply(X[,2:3], MARGIN=2, function(X) {(X-mean(X))/sd(X)}))
	Y_norm = (Y - mean(Y))/sd(Y)

	X_means = colMeans(X)
	X_sd = apply(X=X,MARGIN=2,FUN=sd )

	Y_mean = mean(Y)
	Y_sd = sd(Y)

	beta = matrix(0, nrow = ncol(X_norm))

	computeCost <- function(X,Y, beta) {
	    m = nrow(X)
	    h = X %*% beta
	    cost = 0.5 /m * sum( (h-Y)^2)
	    return(cost)
	}

```
 In the code fragment above, the R function `computeCost` takes as inputs the matrix of $X$ , the vector of $Y$ values (the outputs), and a vector of current values of   $\beta$, the model parameters. It predicts the $Y$ values by multiplying $X$ by $/beta$ (this is a linear model in which predictions are generated by multiplying variables by their respective parameters and then adding the result). In the cost line of code, it calculates the differences between all the predictions and known actual values, adds them, and averages them  to yield the 'cost' of these particular values of the parameters. In practice, the cost function will be calculated for each change in the model's parameters, and the calculated cost or loss will be checked to see if is lower or higher than previous values. If  lower, then the new model parameters are producing better predictions than previous values of the parameters. This is good because it means that the approximation to the function that generated the data is better.  Measures of error, or closeness and distance are crucial to machine learning. If we can learn functions non-deductively and if functions can learn functions, it is partly by virtue of the kinds of view on variation set up in the cost functions. 

## Gradient descent and the search for a partial observer

'Science brings to light partial observers  in relation to functions within systems of reference' wrote Gilles Deleuze and Feliz Guattari in their account of scientific functions [@deleuze_what_1994, 129]. I'm not sure how much Deleuze and Guattari knew about mathetmatical optimization, but their strong interest in the mathematics of differential calculus actually and somewhat unexpectedly makes their account of functions highly relevant to machine learning. Many of the techniques of optimisation underlying machine learning techniques rely on differnetial calculus. 

[HERE] -- say more about calculus and how they think about it; put the partial differentials for grad desc in. 

How then are a stream values of the parameters generated for the cost function to be minimized? This is a key problem since lowering the MSE or any cost/loss/risk function implies a way to find better values. If the values of the model parameters were generated randomly, there would be little guarantee that any model was going to be better than the last one. Perhaps after trying enough random values, a good fit might appear. Or perhaps a good model might never appear. A more ordered approach might be to try a 'grid search' [@conway_machine_2012, 185], in which all the possible combinations of values for the model parameters are tested successively, just like a search party systematically combing some terrain for a missing person.   But with more than a few parameters, the matrix of values quickly becomes vast (100 different values for 10 parameters means calculating and comparing $100^10$ or $10^20$ values). It is impossible to move through large fields  of values without missing some important features. Both random and grid search are likely to fail because they do not actually take into account any structure in the data. If implemented, they might minimize the cost function or they might not.
 
Hence, a third decision has to be made in any machine learning setting. Having chosen a type of model and a cost function, it is also necessary to choose an optimisation method. [HERE - say more about where optimisation comes from; link more to Stengers on this]. 
One widely used optimisation algorithm called 'gradient descent' is quite easy to grasp intuitively and it illustrates the process of searching for an approximation to the function that  produced the data. The term 'gradient' refers to the direction and steepness of values that can be represented by a slope. In linear models, each variable usually has numbers that can be graphed as curves.  Gradient descent is an optimisation technique that takes some of the structure of the data into account as it tries to find the minimum value of the cost function. It explores this surface looking for the lowest points.  It is a low-level algorithm in the sense that it does not predict or classify anything directly, but searches for values of the linear model paramaters ($\hat{\beta}) that minimise the differences between the actual values and the predicted values. Effectively, gradient descent starts by drawing a line somewhere through the points, and then changes some of the parameters controlling slope and position of the line in order to reduce the difference between the predicted values and the actual values. Rather than calculating a single line that best fits the data, the gradient descent algorithm explores gradients or tendencies in the data. There is nothing magical about this. 

```{r gradient_algorithm, echo=TRUE, cache=TRUE, message=FALSE, warning=FALSE, comment=NA, size='smallsize', results='markup' } 

	gradientDescentMulti <- function(X,Y, beta, alpha, iterations) {

	    m = nrow(X)
	    par(mfrow = c(1,2))
	    beta = as.matrix(beta, ncol=1)
	    beta_temp = c()
	    J = c()
	    beta_all = matrix(nrow=iterations, ncol=length(beta))

	    for (i in 1:iterations) {
	        # compute beta
	        h = X%*%beta
	        beta_temp = beta - alpha/m *sum ( apply(X, MARGIN=2, FUN = function(x) {x*(h-Y)}))

	        # update beta
	        beta = beta_temp
	        cat('beta current: ', beta, '\n')
	        cost = computeCost(X,Y, beta)
	        cat('cost: ',cost, '\n')
	        J[i] = cost
	        beta_all[i,]= beta
	    }
	    return(list(beta=beta_all,J=J))    
	}


```

Functions pervade all data practices. That is, there is no raw data. All other data is preformed, collected, ordered and analysed through functions. Functions are data’s necessary prosthesis. 

How do functions work? A wide variety of mathematical functions exist.  At the same time, functions exist as operational processes, as for instance, when a programmer write a function to sort records by date. This means that operate on data in the world, and introduce procedures into its many states of affairs. Both kinds of functions share some features. They both invoke numbers and symbols. They are both written in formal ways (equations, code). They both have arguments or variables. They both have various kinds of operators (+, -, =,etc.) that articulate variables together, usually expressed in the form of symbols. 

The question is how we can relate these two different kinds of functions to data?  Does a mathematical or scientific function have the same mode of existence as an operation that proceduralises a state of affairs? Do the matters of facts map onto states of affairs through functions? I would suggest that although the two kind of functions are entwined, they do quite different things, and that comprehending their differences is vital to thinking through contemporary forms of data. 

According to Deleuze and Guattari in *What is Philosophy*, the function is the central inventive process in science. Functions are not demonstrated or deduced, but created (cf. Manuel Delanda on this). Their account of scientific functions offers one way of understanding how these two different kinds of functions relate. The key accomplishment of functions for them consists in a slowing down of chaos (or what we might call, following William James, ‘pure experience’). A function is  ‘a fantastic slowing down, and it is by slowing down that matter, as well as the scientific thought able to penetrate it with propositions, is actualized. A function is a Slow-motion’ {Deleuze, 1994,118}.  They see functions as particular way of constructing references within chaos or noise. Here chaos is understood as) formlessness that arises from speed, from the fact that forms appear and disappear rapidly. While it is hardly a commensense account of a world, we might think of it as the degree zero on knowing or thinking, the background noise or dark screen against which all form, differentiation, pattern or order eventuates. 

As Isabelle Stengers writes:

>Deleuze and Guattari defined the “creation of scientific function by science’s own specific means” they certainly did not agree with the old bearded-face explanation, but they nevertheless asked us to relate science as creation with science’s “own specific means,” which are associated, one way or the other, {Stengers, 2005, 154}

The specific means through scientific functions are created is different to the ‘functions of the lived’ that map onto perceptions and affections. Functions of the lived have ‘consensual perceptions and affections’ as their arguments {Stengers, 2005, 154}, and to that extent, are not creative. They are complicated forms of recognition. By contrast, insofar as they are creative, scientific functions diverge from functions of the lived, and their implicitly consensual recognition. By means of experiment, they transform functions of the lived into matters of fact articulated through a mathematical function, or as Stengers write, into a scientific matter of fact correlated to a scientific mathematical function’ (156).’Correlated’ here is the key term. What does it mean to correlate something a mathematical function? 

In referring to an experimental situation or to matters of fact, variables re-articulate what perceived state of affairs in terms of a function. The function, as a relation, allows these variables to vary, and indeed suggests that they can vary in more or less constrained ways. The relation of variables to each other can be tested, and it can produce further matters of fact. The scientific function, then, is a kind of convention that remakes the ground on which it stands. As Stengers is at pains to point out, here and elsewhere in her work, the scientific function and matter of fact needs a specific kind of collective, animated by objections, in order to survive. The scientific collective raises objections to the convention and put it to the test. The scientific function is powerful to the extent that it leaves itself open to objections.  Matters of fact bifurcate states of affairs and functions of the lived. They make a difference between knowledge as matters of fact and consensual perception or discourse. Scientific functions become vulnerable to takeover by functions of the lived to the extent that they are not open to objection. 

    This is why the power of such a convention changes in nature as soon as it leaves its birthplace and concerns human affairs where all protagonists are not enabled to object, where some are a priori defined as not mattering. The effectuation of the event, the meaning of the ‘‘it works,’’ radically changes as soon as the tent is pitched on the settled ground of interests and power, and as soon as the habit of the public to accept being addressed as powerless, mere opinion, unable to object and propose, is an ingredient in its concept (Stengers, 2005, 162).

## Conclusion

In their analysis of the commons, Michael Hardt and Antonio Negri   recommend analysis of 'spectres of the commons':

	> Specters of the common appear throughout capitalist society, even if in veiled and mystified forms. Despite its ideological aversion, capital cannot do without the common , and today in increasingly explicit ways. To track down these specters of the common , we will need to follow the path of productive social cooperation and the various modes of abstraction that represent it in capitalist society [@hardt_commonwealth_2009, 153].

I have sought in this chapter to describe a mode of abstraction that powerfully organises contemporary social cooperation today. The core practices of learning associated with machine learning understood . As in much of this book, the  starting intuition here is that machine learning, like many other contemporary social and technical processes is a highly focused if somewhat misted and opaque lens on relationality or on how we live together. That is, the very processes of abstraction, generationalisation, modelling and theorisation, although they often seem reductive, minimal, or simply vastly inadequate to the plurality of experience. Machine learning seeks to identify, classify and predict certain salient aspects of experience and relationality. Subject to machine learning,  'experience now flows as if shot through with adjectives and nouns and prepositions and conjunctions' as William James puts it [@james_essays_1996,94]. to what Willam James would term the 'proportional amount of unverbalized sensation which it still embodies'  